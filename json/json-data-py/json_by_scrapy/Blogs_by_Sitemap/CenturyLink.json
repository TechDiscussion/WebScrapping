[
{"website": "CenturyLink", "title": "hacking-for-a-cause", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/hacking-for-a-cause", "abstract": "GlobalHack VI is an annual event that brings together software developers, graphic designers, technologists, and entrepreneurs from around the world to solve a single civic technology problem. This years project is centered around the issue of homelessness in our communities. Hundreds of participants across youth, collegiate, and pro divisions will work in teams to solve this problem by building software solutions over the course of an entire weekend. Under an open-source software license, winning solutions will be made free and available for anyone to use and for software developers to improve upon. To ensure a level playing field for all hackathon participants, full details of the challenge will not be released until the evening of the event. $1 million in cash prizes will be awarded to the top teams, including $250,000 in follow-on funds that will be used to take the most promising prototypes generated at the event to market. GlobalHack VI will be held Friday, October 21 to Sunday, October 23, 2016, at Chaifetz Arena in St. Louis, MO , on the campus of Saint Louis University . CenturyLink is excited to be an Innovator Sponsor for this important community event. In the fall of 2013, three St. Louis entrepreneurs, Drew Winship, CEO & Co-Founder, Juristat ; Travis Sheridan, Executive Director, Venture Café ; and Gabe Lozano, CEO & Co-Founder, LockerDome launched GlobalHack. GlobalHack, a nonprofit organization, spawned from the group's frustration of an irresponsible focus on business plan competitions and ineffective mentoring programs as methods for starting tech companies. They felt that the best way to improve the tech community in St. Louis was to show entrepreneurs that the first step in growing successful tech companies starts with building great products. Startup communities are not built on ideas alone. By developing a thriving ecosystem of individuals who are focused on building solutions for real-world problems, more founders will move past their initial concepts to create viable startup companies of their own. Since GlobalHack launched in 2013, there have been five hackathons that have awarded $275,000 in total cash prizes. The over 700 event participants have gone on to collectively produce 117 functional, software prototypes. In 2016, GlobalHack launched their youth programs in an effort to help build the next generation of tech talent in St. Louis and beyond. Through the youth programs, students are provided with hands-on computer science experiences. The courses create an environment for students to become excited about computer programming by showing them how fun, creative, and empowering coding can be. It also shows students that they can make an impact in the world via technology. Homelessness is a global problem begging for a solution. GlobalHack VI will work to solve this issue by challenging teams to create software that helps agencies like the St. Patrick Center and other service providers better answer the question -- how can we do more with less? If you are looking to learn more about GlobalHack events or to register, visit their website . Read the blog, Populating the St. Louis Development Center , to learn more about the technology growth in the St. Louis area. Immerse yourself in tutorials , Knowledge Base articles, and more .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "containers-service-discovery-soa-ansible", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/containers-service-discovery-soa-ansible", "abstract": "Reading Time : about 7 minutes. Service-oriented architecture (SOA) is a method of deploying and running software by using multiple smaller services connected over a network. This network can be local to a machine. SOA typically takes the form of virtualized containers communicating to each other over a local network bridge. More and more, software capabilities are being delivered as services. Software running in a SOA needs to utilize these services, which may be changing locations regularly across clouds and networks as resources, are automatically provisioned or decommissioned through failure or horizontal scaling. The process of locating and connecting to services is called service discovery, and it's a growing, changing part of the SOA landscape. Locating services involves two steps. The first step is service registration. When a new container or software capability is provisioned it needs to announce its availability via some form of registry. This registration also includes the configuration parameters needed to use the service, such as host name, port, and API versions. The second step is service discovery. This step involves clients querying the central registration to find available services that match specific requirements, such as network, cloud location, capabilities, or capacity. In this article, we will look at different options for container clustering and discovery using Docker. Docker has become a very popular method to distribute and deploy Linux applications. It is a lightweight container system that wraps an application in a complete file system containing everything that it needs to function. One of the biggest benefits of this approach is that distributed software will always run with the same dependencies, configuration, and environment across any host system, whether the host system is bare metal, virtual server, or cloud. While Docker containers provide services that should work in a SOA, they don't automatically publish and broadcast the services they provide. In an environment that may automatically scale containers horizontally as they are needed, this can be painstaking, and sometimes even a headache. So let's take a look at some of the services and solutions that solve the issue. A discovery backend is a service that registers other services. Usually, it consists of some sort of replicated key/value database with multiple nodes. The redundancy provided by replication means that the discovery backend is always available. In fact, a SOA with service discovery needs to have a readily available discovery backend. Here are a few options for discovery backend services. Etcd is distributed and maintained as part of the CoreOS project. It is billed as \"a distributed, reliable key-value store for the most critical data of a distributed system.\" It expects to be part of a cluster of several nodes, but can run as a standalone service as well. Typically, Etcd containers are small. For example, the Microbox Docker image for Etcd is only 17MB. One nice element of Etcd is that it is a RESTful service. Once the Docker image is running, you can test the installation using curl or other command-line clients. Check-out this article by Nitish Tiwari for details. Note: When deploying servers to host Etcd, you will need to open up ports 4001 and 7001 in your firewall. Consul is a more sophisticated discovery backend than Etcd in that it provides not only a RESTful key/value store, but a DNS-based service registry. The DNS interface can allow some applications to deploy to a SOA architecture with little or no code change. Instead of needing to add RESTful key/value lookups to find services, for instance, an application can use a DNS query. Consul also provides built-in support for SOA clusters operating across multiple data centers. This is essential in certain hybrid cloud infrastructures and geographically distributed systems. The official Docker Consul image is very well-documented, but because of its additional features, it requires a little more work to setup. Note: When deploying servers to host Consul, keep ports 8300 through 8600 open. Also, remember that DNS can be a UDP protocol. So plan your firewall accordingly. There is no single route to container discovery, even when you're just looking at a limited set of tools. Many methods for service discovery with Docker use Docker Swarm . This is a native clustering solution for Docker that uses a collection of machines to emulate a single virtual Docker host server. It uses the standard Docker remote API , which allows any tool that normally communicates with Docker to use the Swarm installation to perform standard tasks. The recommended Swarm configuration requires five different virtual servers, consisting of two swarm management servers, two nodes, and a Consul server. The Docker Swarm documentation includes a tutorial for deploying an implementation . It uses Consul as the discovery backend and can easily be adapted to deploy on CenturyLink Compute servers. Note: A quick-and-easy Docker Swarm tutorial is also available on GitHub Gist . This implementation uses an Etcd discovery backend and only requires two virtual servers. To deploy Docker Swarm, you will need multiple virtual servers. While it is possible to configure and provision them manually one at a time using the CenturyLink Cloud Control Portal , the approach is more time consuming than need be. Instead, it is faster to use an automation tool like Ansible to handle provisioning and configuration for us. In this tutorial, we will use Ansible to provision five virtual servers that can be used to host Docker Swarm. CenturyLink Cloud Compute servers are high-performance cloud servers. These enterprise-grade virtual machines are easy to deploy and manage from the CenturyLink Cloud Console automatically via our powerful API. Ansible is a tool for configuring and managing clouds and servers. It handles software deployment, cloud resource provisioning, task execution, and configuration management. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to access CenturyLink Cloud Compute servers. Because automatically deploying and configuring virtual servers can be a complicated topic, you might want to check-out our comprehensive collection of knowledge base articles on virtual servers . You will also want to install Ansible on your machine. Most Linux distributions have Ansible packages available. For example, on Ubuntu or Debian you would use the following command: For other distributions and operating systems, follow the installation instructions from the Ansible documentation. To create an Ansible playbook for automatically provisioning and deploying CenturyLink Cloud Compute servers, follow these instructions: With a text editor, create a file called deploy_docker_swarm.yml and edit the file to look like the following example. Pay close attention to the task definition. Check the Ansible clc_server module documentation for more options. You can change any of the settings in the vars: section to customize your deployment. Be sure to change the value of server_password to the password to be used for your new virtual servers. Execute the playbook using the following command. Replace <your-ctl-username> with your CenturyLink Cloud username, and <your-ctl-password> with your CenturyLink Cloud password. It can take a little while to provision multiple servers. Once you have provisioned a collection of servers, deploy a SOA discovery backend using the tutorial in the Docker Swarm manual . Automatic service discovery is still a developing field, and dozens of different success stories and configurations are available on the web. Still, with this tutorial you are on the road to having automatic horizontal scaling and container discovery for your Docker-backed applications. Docker offers many options to deploy and manage software on CenturyLink Cloud Compute servers. Check-out our other articles about Docker for more ideas and inspiration. Sign-up for our Developer-focused newsletter CODE. Designed hands-on by developers, for developers. Keep up to date on topics of interest including tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "security-doesnt-have-to-be-scary", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/security-doesnt-have-to-be-scary", "abstract": "October is National Cyber Security Awareness month (NCSAM), the time when businesses often audit their security strategy. With new security threats emerging daily, it is imperative for businesses large and small to be continuously evolving their security measures and standards. With proper planning and processes in place security doesn't have to be scary. Whether you are the CEO trying to ensure that your companies data is secure or a developer concerned about writing secure code, embedding security into the fundamental core of your daily workflow can help to minimize security threats. Below is a compilation of security topics to consider when revisiting your security posture, as well as solutions and resources to strengthen your overall security strategy. If your code isn't secure, then your applications won't be either. Studies show that 60% of all vulnerabilities are due to coding error , or bugs. And while not all developers need to be security experts, all developers do need to be aware of their responsibility in writing code that reduces the chance of exploitable vulnerabilities showing up in applications. Ensuring that your development team build security into the infrastructure of their coding processes can vastly increase the chances of finding insecure code early on in the development cycle, thereby saving time and money. Regular training sessions can help ensure that developers are consistently in the know regarding the latest vulnerabilities and software available to help test for secure code. To learn more on building secure code into the foundation of your process, or secure coding fundamentals in general visit the links below. Making Security Agile Secure Coding Fundamentals Secure Coding Fundamentals Part II Ensuring that your data is secure is an important part of an overall security plan. CenturyLink and Server General have partnered together to make encryption fast and easy. Server General allows customers to encrypt a cloud database or file server quickly and easily using key management. Using this service, installing, configuring, and encrypting a database can take as little as 30 minutes. The service works with most database servers like MongoDB, CouchDB, MySQL, PostgreSQL, Apache, or Samba, including ones that store sensitive and regulated information. To learn more about encrypting your sensitive data visit our Knowledge Base article. While Docker and container based technology continues to grow, so does the security risks behind utilizing them. While pushing to public Docker repositories can be convenient, it can also cause security issues. Dealing with passwords, private keys, and API tokens in Docker containers can be tricky. Just a few wrong moves, and you'll accidentally expose private information in the Docker layers that make up a container. For more information on ways to securely use Docker, you might want to check out the tutorial below. Getting hacked can negatively effect your business in various ways. It can cost your business money due to downtime, loss of secure data, legalities, and damage the reputation of the business. One way to secure your sensitive data is via strengthening authentication policies. By adding multi-layered authentication options that require user interaction before granting access, organizations have the ability to add significant levels of security to protect their data from hackers and prevent unauthorized access. FlexSecure has integrated their authentication technology on the CenturyLink Platform . FlexSecure is a context-based Authentication-as-a-Service API platform, providing passwordless and pin-based authentication. This enables organizations of all sizes to choose, or mix and match appropriate user authentication methods to protect and secure their data sources on their cloud, mobile, and IT infrastructures. Watch this short video to learn how you can enhance your cyber security with FlexSecure . There are many aspects to a robust security strategy. Even the most robust security plan needs to be reviewed and updated often. Training, sharing knowledge, implementing security into daily practices, and planning are all keys to a successful security future for your business. For more information on security, visit these resources. Increase Security Not Complexity Tips for a Successful Security Strategy", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "ansible-hybrid-cloud-to-on-premise", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/ansible-hybrid-cloud-to-on-premise", "abstract": "Virtual servers are a huge boon to software development. Having rapidly deployed and recycled server instances can not only speed up development, but reduce the workload and bottleneck of DevOps procedures. With a pre-made development server blueprint, developers can quickly launch configured sandboxes and development machines complete with libraries, development environments, and database servers. However, private and enterprise deployment may have requirements that virtual servers just don't meet. For instance, private deployment might have strict network security rules or bandwidth and connectivity requirements that can't be met by cloud-hosted virtual servers. In this case, it is necessary to have methods to not only quickly launch and configure cloud-hosted virtual servers for developers, but also for robust long-lasting instances for enterprise production. In this tutorial, you will learn how to build a hybrid cloud software deployment solution, beginning with a rapid deployment development system using virtual cloud servers. We will then explore options available for deploying systems in staging and production environments on a private data center. CenturyLink Cloud Compute servers are high-performance cloud servers. These enterprise-grade virtual machines are easy to deploy and manage from the CenturyLink Cloud Control Portal and via our powerful API. Ansible is a tool for configuring and managing clouds and servers. It handles software deployment, cloud resource provisioning, task execution, and configuration management. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to access CenturyLink Cloud Compute servers. And, because automatically deploying and configuring virtual servers can be a complicated topic, you might want to check out our comprehensive collection of Knowledge Base articles on virtual servers . Install Ansible on your machine. Most Linux distributions have available Ansible packages. For example, on Ubuntu or Debian you would use the following command: For other distributions and operating systems, follow the installation instructions from the Ansible documentation . To use the Ansible module for provisioning and controlling CenturyLink Cloud products, you will need to install the Python SDK. Run the following command. Development environments differ from project to project. Sometimes they consist of just one server with relevant services, an IDE, and development tools installed. However, more complex projects that need to mirror production environments accurately can get very large. Some might need eight or more individual servers to provide a complete environment. Deploying a development environment for each developer can be time consuming and expensive. Virtual servers have a lower overhead, both in terms of cost and manpower, and thus make a lot of sense for this sort of development environment. Automatically deployed and configured virtual servers make even more sense. In this tutorial, we will use Ansible to automate deployment of virtual private servers. Before setting up Ansible to provision and deploy a development environment, you will need to spend some time setting up and configuring one or more virtual servers as a template development environment. This process varies widely depending on your environment and requirements. Explore our other tutorials for information on deploying popular IDEs and database servers on CenturyLink Cloud Compute. For instance, check out our tutorial on deploying Eclipse Che in the cloud . Once you have a fully configured development environment to convert to templates, follow the instructions below for each virtual server in the environment: Note: You can see a list of all server templates in the Templates server group. To create an Ansible playbook for automatically provisioning and deploying CenturyLink Cloud Compute servers, follow these instructions: With a text editor, create a file called deploy_development_servers.yml and edit the file to look like the following example. Execute the playbook using the following command. Replace \"your-ctl-username\" with your CenturyLink Cloud username, and \"your-ctl-password\" with your CenturyLink Cloud password. You can also use Ansible to move developed applications from the cloud to on-premises private servers. There are many different approaches, but all of them should include staging environment to test deployment procedures. This is where automatic provisioning techniques are useful. If there is an existing deployment plan that mimics the production environment, this can be automated to provision a staging environment. Staging environments allow a team to practice deploying new versions of software as well as configuration changes, database schema and data updates, and package updates. A staging environment should match the production environment as closely as possible. This enables you to catch any obstacles or speed bumps and correct them before they cause an IT disaster. There are three main areas to plan for when deploying to your staging and production environments: package management, server provisioning, and database changes. Every operating system and programming language has at least one packaging system. For example, Linux has deb, RPM, and several others. PHP uses composer; Node.js uses npm; and Ruby uses gem. Each of these provides an interface that downloads and installs packages from a remote package repository. Using a private package repository gives you the benefit of full dependency management and fail safes during installation. Take advantage of these valuable features. Software can also be distributed via version control systems. For example, Git's branches and tags can mark software as ready for deployment. Note that while this can be a convenient way to distribute changes, it has its risks. As the size of your DevOps or development team grows, the chances of somebody accidentally doing the wrong thing to a source code repository grows exponentially. Hopefully, you will catch these inevitable problems in your staging environment, but they will slow you down. Once your team is large enough, move to a private package repository and save yourself the pain. Virtual private servers offer benefits even when they're running on bare metal in a private data center. The ability to quickly distribute, provision, and manage containers simplifies software deployment and upgrades. In addition, the isolated environments offered by virtual servers keep runaway software from damaging the entire deployment environment. Ansible provides many options for provisioning private virtual servers in private data centers as well as in the cloud. It controls virtual machines using hundreds of different modules . Here are a few to get you started: Once again, Ansible comes to the rescue. Check-out its extensive list of database modules to find out about available capabilities. Database change management is tricky, and there isn't a consensus on the best way to handle substantial differences during deployment. Tools like DBdeploy and PHPmig reduce the pain of database change management. They make a good starting point for a solution that works in your production environment. This guide is just the beginning of developing a robust hybrid IT solution for software development and deployment. After you get new developers started in the cloud while deploying changes safely to production, it's time to explore other automation solutions offered by Ansible. Be sure to look into the possibilities offered by Runner , the CenturyLink Cloud Ansible-as-a-Service offering. It provides powerful tools for IT automation and infrastructure management. Since it's offered as a cloud service, it's available anywhere. Sign-up for our Developer-focused newsletter CODE. Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "populating-the-stl-development-center", "author": ["\n              David Cormier - PhD\n            "], "link": "https://www.ctl.io/developers/blog/post/populating-the-stl-development-center", "abstract": "Recently, Dynatrace and CenturyLink hosted a CloudWalk designed to guide users in a hands-on environment through use cases and functionality that combine Dynatrace's software on the CenturyLink Platform . The event was hosted at the newly opened St.Louis CenturyLink Development Center . This CloudWalk is one in a series of scheduled events designed to help consolidate the growing St. Louis tech start-up community which includes Lockerdome , Hatchbuck , T-REX (a tech incubator), and Pixel Press . In the spirit of collaboration, the CTO of Lockerdome delivered opening remarks. CenturyLink wants to share opportunities, knowledge, and resources in the St. Louis area by hosting events like these that spark innovation and the cross-pollination of ideas and practices. The Development Center provides the environment in which like-minded people can come together, share ideas, and work through the tech-world problems that they face and find ways to solve them. Bob Stolzberg and Michael Villiger led the two instructional components. Bob covered the deployment of the Dynatrace client to the CenturyLink Public Cloud and how to use the application performance monitoring tools. Michael showcased the Dynatrace PurePath dashboard with its ability to gather and present detailed information about application use. CenturyLink loves hosting these types of events. They pull out all the stops to make the event both casual and informative. The event is well staffed. The Development Center is comfortable and inviting. The CenturyLink team is friendly and welcoming. Food, beer, and snacks are provided. Prizes are given out throughout the evening. I even won a Lootcrate raffle by accruing points for using the CenturyLink St. Louis Meetup user group mobile app . Throughout the presentation technical experts walk the floor offering help to guide users through installation and configuration, and making sure everyone is on track. The spirit of a CloudWalk is simplicity and accessibility. The CenturyLink Cloud is a showcase of on-demand flexibility and high performance. Bob Stolzberg describes CenturyLink’s Cloud software as having the “best user interface in the industry.” The idea is to make the problem solving potential of the cloud accessible and introduce that to a thriving tech ecosystem in St. Louis. This type of event is indicative of the larger Midwest start-up movement. St. Louis continues riding the top of that wave. Joe McKendric of Forbes, citing Tech Vision 2016 , argues convincingly that large non-tech companies might be the cloud industry’s next disruptors, innovators, and contributors. If that proves to be the case, then the large non-tech companies in the St. Louis area stand to benefit significantly from partnerships with companies like CenturyLink which, judging from this event and others that are planned, is ready to make those partnerships a reality. The CenturyLink Platform isn't simply an innovative or novel technology for tech start-ups. The cloud is likely to become an integral part of most businesses within a few years, whether those businesses are technology based or otherwise. Bob Muglia, HuffPost technology contributor, noted in his 2016 post on data walks and cloud battles: “this year looks to be another groundbreaking year in [cloud] technology.” In a 2014 article in Fortune magazine Erin Griffith asked the question “can St. Louis become the next tech hub?” She cited several examples of investors and entrepreneurs scouting the Midwest as a potential extension of the Silicon Valley spirit of innovation and risk. The infrastructure, though, needs development in order to support the growth. Her opinion is that the St. Louis \"tech ecosystem is young.\" CenturyLink and its St. Louis Development Center is changing the landscape. The Development Center brings that growing ecosystem one step closer to maturity. In addition, CenturyLink is listed as one of five visionary leaders in the field. The Dynatrace CloudWalk is part of a planned strategy to influence the Midwest in a dynamic, unpredictable, globalizing technology market. It might be a little early to say at this point. Still, if CenturyLink is successful in capturing the St. Louis tech market, it will continue to solidify itself as a market leader.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "flexsecure-cyber-security", "author": ["\n              Sid Prasanna\n            "], "link": "https://www.ctl.io/developers/blog/post/flexsecure-cyber-security", "abstract": "This video tutorial covers FlexSecure , a context-based Authentication-as-a-Service API platform. As a partner in the CenturyLink Cloud Marketplace Provider Program , FlexSecure provides passwordless and pin-based authentication methods for organizations of all sizes. Their technology enables companies to provide strong and flexible authentication for access to more sensitive resources and data, while relaxing the requirements for access to less sensitive content. Check out this video to learn how to create an account, systems, and users, and assign existing policies to your FlexSecure instance. In order to follow along, you will need a CenturyLink Cloud account. Just head over to our website and activate an account . You will also need to register for a FlexSecure account at the following link: FlexSecure Registration Form . FlexSecure Video Voice Over from CenturyLink Cloud on Vimeo . A written version of this tutorial can be found in our Knowledge Base . Be sure to check out the FlexSecure website to learn more about their solution, which can adjust to security authentication requirements in real-time, based on dynamic factors in the security equation. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "local-check-mk", "author": ["\n              Kevin White\n            "], "link": "https://www.ctl.io/developers/blog/post/local-check-mk", "abstract": "Note: While this article may still be relevant, there have been a number of updates since it was written in September 2015, including some rebranding.  Some links and text have been updated to reflect those changes, and anyone intersted in this content is encouraged to check out the newer features of Checkmk by tribe29 . Checkmk (OMD) is an open source performance and fault monitoring tool based on Nagios core , capable of both agent-based and agent-free monitoring. A library of plug-ins is available to monitor many types of applications, but sometimes you might need to write your own local checks. The intent of this article is to guide you through the steps necessary to create and deploy a custom health check. There are several reasons for to write your own checks: First, local checks require that the Checkmk agent is installed on the host to be monitored (in most cases). The agent scans a local directory on the target host and executes any scripts it finds. By default this directory is in /usr/lib/check_mk_agent/local on Linux. On Windows, the scripts are run from the “local” folder at the same installation path of the check_mk_agent.exe file; you'll have to create that folder manually. On Linux, you can find the local checks path by running the following command: Next, you can write the health check in the programming language of your choice, so long as you satisfy the languages dependencies and the output requirements of Checkmk. Remember, the script is run on the host where it is deployed. This example will be written in BASH. The agent expects a response in a timely manner. This means your check should finish before the next scheduled check, which is typically one or two minutes later by default. Don't fret though, as of version 1.2.3i1 there is a convention for checks that run longer. Under your local check folder, simply create a new folder with the number of seconds to cache your response. For our example, we installed the script in: Finally, you need to output four columns of text, separated by a single space.  The fourth column is the only one that allows additional spaces. Column 1: Nagios status – 0 for OK, 1 for Warning, 2 for Critical, and 3 for Unknown. Column 2: Check Name – This is the service name of the check. Column 3: Performance data – You don't have to provide performance data, but if you do this is the column. You set the key/pair values here as well as the warn, crit, min and max values. Warn, crit, min and max values are optional. Column 4: Check output, which will be displayed in the status detail column. There are some limitations though. On Windows the output must only contain ASCII characters. UTF-8 and Unicode are not allowed. In versions before 1.1.5 you can only send one performance variable. Lastly, warning and critical levels have to be configured and handled on the host itself. Here's an excerpt from our code success and fail options: Our use case required us to test the end-to-end health of our Intrusion Prevention product (IPS). The transactional nature of the check drove our decision to script this as a local check. We also decided it wasn't necessary to run the check on the target host because of the distributed nature of the application. Our check will: Now, lets look at the code that puts this check to work. First, we initialize some variables that will be used to setup our query: Next, we attack our target VM: Now that we've setup the attack, we watch our database. We expect one or more new records to appear in our database sometime within 15 minutes of the attack, in a worst case scenario. This indicates that: The code to watch the database for new messages is: That's all there is to it, besides installing it in the agent's local folder.  After a short time you should begin seeing performance data for the newly-configured check. There are many other components of the Checkmk Project to consider. For more detailed information, visit the new official Homepage of Checkmk.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tutorial-dr-failover-migration-safehaven", "author": ["\n              Jim Greene\n            "], "link": "https://www.ctl.io/developers/blog/post/tutorial-dr-failover-migration-safehaven", "abstract": "If your disaster recovery site sits on CenturyLink Cloud, a managed SafeHaven team is available to assist or do wide cloud implementation, test failovers, and failovers in case of a disaster. In addition to using SafeHaven for DR, you can also use SafeHaven for performing migration to any of these recovery or production sites — CenturyLink Cloud, Amazon Web Services (AWS), or Microsoft Azure — or any of these production sites: You can have multiple recovery instances on the Disaster Recovery site in AWS or Azure. If you perform a test failover or failover operation, you can have multiple recovery servers coming up on the DR site. In this tutorial, we’ll walk you through a test failover operation. In this test failover operation, the source site is in CenturyLink Cloud, the destination site is Azure, and the production workload is a Windows 2008 VM. It's already replicating and has checkpoints being taken. If you want to migrate to AWS or to CLC, the process is the same. At the end, you will click on migration. On the left, we have the navigation tree in the SafeHaven console. On the right, we have the Properties panel. Our production site is in CenturyLink Cloud, CA2. It's replicating to the DR site, which is AzureCanadaCentral. The source VM in CenturyLink Cloud is a Windows 2008 VM replicating to Azure. Since it's already replicating, this VM also has periodic checkpoints. If we click on Checkpoint History, we'll see it has 10 checkpoints. These checkpoints can be VSS or non-VSS. We can use any of them to perform a test failover or failover operation of the production workload to the DR site. Next, we enter data into the production environment by writing and saving a simple Paint file for a test failover operation to Azure. Now, we'll take a manual checkpoint by right-clicking the protection group and clicking on Create Manual Checkpoint to check that this data is replicated to the DR site. These changes will appear on the DR site as a checkpoint job at the bottom of the screen. Once it's finished, you'll see that it's completed and that there are also sub-jobs. If there is a problem in a job, you can see where it went wrong. In Checkpoint History, we see the manual checkpoint we just created. For this checkpoint, we also have a corresponding snapshot in Azure cloud. This snapshot can also be used to perform a recovery operation. To begin a test failover operation, right-click the protection group and then click on Test Failover. This will give us a list of checkpoints. Select the manual checkpoint we just created, then click Next. To input information about the recovery server that's going to be created in Azure cloud, select resource group, VM size, and primary network. In CLC, we have an option to isolate the network, so there are also options to select a security group and a subnet. You can select a security group that is completely isolated from your production environment. We are using DHCP, but you can provide a static IP or an IP from the production site here. Now it's going to spin up a recovery server in Azure cloud. It's in a running state, and the IP address is 172.25.1.5. In Azure cloud, this is the server that is created. It starts with test failover and has the name of our protection group. To check the boot diagnostics of this VM, click on Boot Diagnostics. It can take a few minutes for the VM to boot up and for you to have the IDP4 to be opened to log into the protection group. Log into the VM using the IP provided by SafeHaven console. The username and password should be same as the production server because it's the exact image of the production VM. Now we're in the VM and we can take a look at the file that we just created. This is the test failover to Azure Paint file that we created on the production server. This completes a successful test failover operation. The file is on the DR site in the recovery clone. Once testing is done and we are happy with the test failover, we can simply delete the recovery server. This also deletes it from the Azure cloud. For more insight into using SafeHaven for DR, failover and migration, check out these articles in our Knowledge Base.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-technology-changing-game-day", "author": ["\n              Jonathan Townsend\n            "], "link": "https://www.ctl.io/developers/blog/post/how-technology-changing-game-day", "abstract": "For the modern fan, there's more to sports than just following a game. In fact, investing time, energy, and emotion into sports is only part of the appeal. With all the technology available online, the avid sports can make watching a game or following a team a more interactive experience – as noted in this CenturyLink article . The conventional ways of watching sports are long gone. Today, many fans watch games on more than one screen, whether it's a television, computer, tablet, mobile device or any combination of those tools. However, there is more to watching than just the action on the screen. Fans expect to have access to game-specific data-sets on their game-day experience. They want to know more about team and individual performance than ever before, mainly through the use of tracking and analytics. To support real-time sports consumption, which continues to reach new heights, the right technology has to be available to fully-support the growing consumer demand. For example, RFID tracking chips completely personalize the game-day experience digitally by tracking player speeds, distance, and patterns on the field. In essence, technology has proven to be a massive game-changer (pardon the pun) for the sports industry. The NFL is one of many leagues working to integrate technological advances into physical stadium architecture. A few years ago, about 17 football stadiums installed the required equipment to receive, record, and log what the league calls “ Next Generation Stats ,” which are stored in the NFL’s cloud. The data cultivated from those logs is processed instantaneously for official use and made accessible for fans both in the stadium and for those watching on a device or the television. CenturyLink has made an impact in this arena by taking part in monumental conversations with both current and future sponsorship partners, with an aim to figure out how to implement best-in-class technology solutions to enhance the fan and corporate technology sponsorship experience. For example, the company's sponsorship deal with the Minnesota Vikings is focused on providing fans and viewers with an enhanced version of the game-day experience, and state-of-the-art stadium technology will help make that happen. Vikings fans will be able to enjoy the ultimate game-day connectivity via CenturyLink’s Wi-Fi networking infrastructure. With 1,300 Wi-Fi access points and 2,000 HD TVs connected stadium-wide, cloud-based technology and data analytics will customize the fan experience now and in the future. There are many emerging trends that have placed formerly-different industries on the same path and going forward, sports and technology are perfectly aligned. The use-cases and technical applications have positive implications as fans, teams, coaches, scouts, and analysts are using infrastructure-based connectivity capabilities to get more out of the on-field action. Globally, the Internet of Things (IoT) continues to re-shape the world in which we live. Technology has accelerated the world of sports outside the NFL, too. Around the world, coaches, fans, and athletes are utilizing the power of tech to access the myriad of resources and services available to them. Sites like Stats provide interactive solutions such as Data Feeds that have changed the way sports are coached, played, and enjoyed the world over. Additionally, CenturyLink is committed to providing the right services for a truly digital age. Not a CenturyLink Cloud customer? No problem. Designed for your business needs today and tomorrow, the CenturyLink Cloud is reliable, secure , robust , and global . If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tech-employer-of-choice", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/tech-employer-of-choice", "abstract": "When you think of tech-heavy ecosystems, St. Louis might be one of the last cities that comes to mind. However, the tech industry is growing exponentially in St. Louis and the surrounding areas. In the last decade, young people have moved to St. Louis in droves compared to the prior 30 years. The net migration of people between the ages of 25 - 29 grew from a loss of 28 people for every 100 in the 1970s to a gain of 35 people for every 100 in the 2000s. Tech-based jobs have flourished with this migration. The tech realm in St. Louis varies. Just a few of the local tech start-ups include: aisle411 , LockerDome , and Hatchbuck . The city is also home to heavy hitters like Square , Ungerboeck , and CenturyLink , all of which have major technology-based offices in the St. Louis area. With the growing tech ecosystem comes the challenge for employees to create a culture which attracts young, tech-savvy employees. How does a telecom, for example, become a leader in cloud? CenturyLink has provided the blueprint by transforming the way it does business. Along with strategizing its tech-based acquisitions and altering its culture, CenturyLink announced another milestone in its long-term tech vision by opening a Cloud and Managed Services Development Center in St. Louis on March 3, 2016. With this opening, CenturyLink created a collaborative, open work environment, which is home to an engineering-driven culture of innovation inspired by DevOps methodologies and Agile production cycles . When asked about the rationale behind the new space and the culture they wanted to generate, senior leadership said they believe that DevOps is not just about the culture, it also has to be complemented by the tools and the facilities. CenturyLink believes that making great technology services requires a commitment to collaborating through teamwork. So, they dismantled the interior of the building and tore it back to the studs. They rebuilt the former \"cubical farm\" to a layout one patterned on DevOps, one that enables teams to work collaboratively in groups. There is also plenty of room to host customers providing their input. Team rooms, organized by function. The majority of the space is dedicated to “team rooms” – large, open rooms where employees are grouped by work stream (platform team, application services team, service engineering, etc.). As teams build new features and push them to production, all the people they need to interact with are literally a few feet away. Desks suited to pairing. Pairing is a powerful tool for distributing knowledge across an organization – and wide desks are usually needed so that people can sit shoulder-to-shoulder and maximize the collaboration. Built-in collaboration spaces. Each team room is outfitted with touchscreen smartboards and whiteboard walls. That allocation of space gives people the opportunity to start drawing and illustrating an idea, problem, or scenario. They are even whiteboards in the hallways. Sometimes it's the hallway conversation that becomes the most important. Several scooters are on the premises for ease-of-navigation down the long hallways. Having fun is just as important as working. It’s easy for employers to say that work should be fun.  But how do they show it?  At the Center, team rooms have embedded speakers in the ceiling so everyone can listen to music during the day. There’s even satellite TV in each team room, as well as in the large shared spaces. And of course, there’s a dedicated gaming room and a small gym area when it’s time for a much-needed break. \"I work here because it’s the coolest place in St Louis to work.  We have a casual work environment, talented people, fun atmosphere, and individual team autonomy.\" --Lane Maxwell \"I have a special love affair with RobotCoffee.  Every day, I make two trips to visit it. Sure, there might be a line in the morning, but it's worth it. Aside from the opportunity to chat with co-workers, I'm rewarded with delicious coffee, the way I like it: freshly ground, robust, and black.\" --Ben Swoboda \"It's energizing to be surrounded by people who are great at what they do.\" --Dana Bowlin \"The new Developer Center provides a collaborative environment that breeds innovation, with everything you need to get through your day in one spot. And scooters...\" --Traci Yarbrough \"The company caters lunch twice a week (Tuesday and Thursday), we even have a Trello board for lunches. The other days of the week the choices are grab a bite out, go down to the cafe, bring your lunch, or grab something from the E-Mart, where the company loads each employee account with funds weekly. All that's needed to check out at the register is your thumb print. We work in team rooms, but we all eat in the dining area, which we've named (Joe's Tavern). This part of our culture is pretty cool. Having lunch together allows for collaboration and communication with others product teams (some of which your product may have dependencies on). And everyone likes free food. And cookies...there's almost always cookies!\" --Brian Felton \"We are literally changing the way businesses operate.\" --Darren Simmonds One of the more innovative ways that CenturyLink has tapped into the young tech pool is through hosting both technical and community events throughout St. Louis. CenturyLink focuses on educating customers in practices such as DevOps or how to use CenturyLink tools more efficiently by hosting Meetups and hands-on CloudWalk events, which enable a fluid environment for sharing and gaining technology-based knowledge. The company also hosts other community events to foster a learning environment for children in the community. We host Girl Scout events and monthly Coder DoJo meetups for kids.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "acceptance-tdd-cucumber", "author": ["\n              Kevin Wilde\n            "], "link": "https://www.ctl.io/developers/blog/post/acceptance-tdd-cucumber", "abstract": "Acceptance Test Driven Development (ATDD) is a process where every member of a project team may develop acceptance criteria for a particular feature. By using ATDD, a team can produce a feature from inception to implementation with minimal surprises regarding the final product. A developer following ATDD will use acceptance tests to iteratively drive the coding process until the feature is fully implemented. The practice of ATDD provides a self-documenting process that results in an automated test library describing the behavior of the system in a language that programmers and non-technical members alike can understand. Cucumber is the Behavior Driven Development (BDD) testing tool that will be used as examples to describe ATDD in this post. ATDD gives the power of traditional Test Driven Development (TDD) to the entire project team. The first step a team takes in the ATDD process is to collaborate and define the acceptance criteria for the feature to be developed. The acceptance criteria is written in the BDD Given When Then test scenario format that the Cucumber testing tool understands. Once the test can be executed against the implemented feature, your entire team can advertise with confidence that the feature is functioning as everyone on the team expects. Similar to a developer writing a unit test while practicing TDD, the first thing a team should do when developing an acceptance test is to determine the desired behavior that the test will verify. This process is an opportunity for the product owner to ensure that the stakeholders for your project will receive a feature that behaves exactly as they need it to. It is also the engineers' opportunity to clearly define the scope of the feature. The team will have a much better understanding of what implementing the feature will entail by crafting the acceptance criteria. It is entirely possible that everyone subscribing to the team's idea of what the feature is supposed to be will drastically change during the acceptance criteria crafting process. Determining acceptance criteria should be part of the story writing process in a story grooming session. If a team is having difficulty crafting the acceptance criteria for a feature, it could be a good indication there may be a problem with the feature. Perhaps the feature scope is too broad. Maybe the concept simply will not work in the real world. The beauty of having this discussion now is that problems are discovered before any lines of code have been written. The final result of acceptance criteria determination is a script that will serve as the actual acceptance test for a feature. Acceptance tests are  written in the BDD Given When Then format. For example, a test scenario for the CLC IPS Product Offering would be: The Given When Then format follows the typical approach taken for writing unit tests: A comprehensive library of acceptance tests that the entire team has collaborated on gives each member a warm, fuzzy feeling about the product they are producing. It also helps marketing to espouse the capabilities of the system without fear of making unfounded promises. The implemented acceptance tests coupled with a Continuous Integration (CI) system such as Jenkins gives the developers confidence that the product is still functioning as advertised. We can begin implementing our test now that we have a fully-vetted acceptance test scenario. Cucumber makes test implementation a breeze. Put your test scenario into a feature file: Note: Source for the examples is provided in GitHub . ~/workspace/atdd-cucumber/src/test/java/com/ctl/example/cucumber/feature/ips.feature Execute the test runner: ~/workspace/atdd-cucumber/src/test/java/com/ctl/example/cucumber/runner/RunCukesTest.java The source code for the step definitions is printed to the console: Add steps to a Step Definition Class: ~/workspace/atdd-cucumber/src/test/java/com/ctl/example/cucumber/step/IpsSteps.java Now, we can start writing our test. Start by adding the action that we are testing in the When clause, so that we learn what needs to be set up for the test. We surmise that we need an IPS install client to interact with the IPS API. This is where our acceptance test begins to drive the code to be developed. Note: Before the next step, you should develop the ipsInstallClient.install() using standard TDD. Next, we'll put the new version of ipsInstallClient.install() into our When step: After developing the ipsInstallClient.install() method we see that we need an ipsInstallConfiguration and bearerToken to execute our test. So, we make ipsInstallConfiguration and bearerToken members of our step class so that we can initialize them in our Given step. Now that we're setting up our test and executing the action that we want to test, we can verify that the install is functioning in the Then step. Here, we have to figure out how we will verify that the action in the When step is functioning. Let's execute the test to see if that will give us an idea for the verification. Running the test yields the following error (exception): When our feature is completed and working we should no longer receive the error. Let's gracefully catch the exception and set a member to that exception. Our Then step verifies success by asserting that the exception is null. Now when we run our test we get a nice expected failure: We can use this failing test to drive our next steps of development. Is there a problem with the installation client? Are we calling the client properly? The above testing and developing cycle provides an example of the iterative development process that an acceptance test can provide. We want the final state of the Then step to be something like this: Now that we have this high level acceptance test in place, we can change any part of the underlying implementation with confidence that the overall functionality remains. As can be seen from the above feature tests, we now have a document that anyone can read to understand how a particular feature works. A Given When Then test scenario can serve as the centerpiece for a demonstration of the current state of a feature. A library of feature tests gives a developer very clear feedback on the state of the entire system. Coupled with a continuous integration server such as Jenkins, a team can know very quickly if any part of their system is failing. Here is a sample of the type of report that can be automatically generated using the Jenkins Cucumber Report plug-in: Cucumber is one of many test tools that allows teams to create acceptance tests. No matter what tool is used, the most important aspect of ATDD is the collaboration it fosters between all team members. The image at the top of this post is a combination of two images with creative commons licenses, located here and here .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "10-commandments-for-developers", "author": ["\n              Jim Greene\n            "], "link": "https://www.ctl.io/developers/blog/post/10-commandments-for-developers", "abstract": "Almost 50 years ago, Gerald M. Weinberg authored The Psychology of Computer Programming . In it, he listed The Ten Commandments of Egoless Programming , which have remained relevant for programmers who were born well after the book was published in 1971. Weinberg is regarded as a pioneer in taking a people-oriented approach to computing, and his work endures as a guide to the intelligence, skill, teamwork, and problem-solving power of the developer. While his commandments have become more readily available with the advent of the internet, the commandments still are new to some developers. When they appear to inspire and instruct, we find that they can apply to just about every business area, and even to life itself. Topics include motivation, personality, problem-solving ability, team formation, and other factors that developers can use to avoid social problems in the programming environment and during any large project. With that in mind, here are the 10 important lessons developers, project managers, and stakeholders would do well to keep in mind during the project lifecycle. Still incredibly relevant. Maybe more than ever. Keep them close, and over the long haul they'll make you a better developer and co-worker — and perhaps a better person.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "managing-devops-with-cloud-application-manager-and-alm", "author": ["\n              Jim Greene\n            "], "link": "https://www.ctl.io/developers/blog/post/managing-devops-with-cloud-application-manager-and-alm", "abstract": "Application Lifecycle Management (ALM) describes the management of software application development and its three main phases -- requirements, development, and operations. Application development is managed from beginning to end in ALM, from development to deployment and decommissioning. ALM is not specific to the DevOps and Agile approach, so any organization can implement it in whatever software development environment they use, such as CenturyLink Cloud, Amazon Web Services (AWS), or something else. But, ALM complements DevOps in unique ways. DevOps is a combination of software development and IT operations; its practice emphasizes collaboration between developers, analysts, engineers, and other IT professionals when developing and automating software and applications. It also emphasizes iterative development as a way to create a fast turnaround time for deployment. At CenturyLink , our teams use this continuous integration and continuous deployment (CI/CD) approach, where the focus is on releasing a \"base\" or \"beta\" application, then continuously releasing features as they’re developed. Each release goes through quality assurance (QA) and testing. We use different tools to automate testing, builds, and releases , such as Jenkins , Github , Chef , etc., as automation is integral to DevOps. ALM supports the DevOps approach and Agile model by providing a universal platform with centralized visibility into the application and the entire development process. Specifically, the ALM framework provides a methodology for developers and engineers to control and record iterative development. Rapidly-changing requirements in IT development require systems that allow teams to have access to the infrastructure and the means to quickly view it at different stages across different providers. At CenturyLink, we use Cloud Application Manager to facilitate ALM by allowing teams to view all their cloud resources from a single interface. This cloud management functionality frees teams to innovate and create by reducing the time they spend managing the ALM process. Tasks like spinning up servers and other maintenance tasks that take up staff time can be automated so teams can focus on developing new features and discovering new ways to innovate in the market. With Managed Services Anywhere as a key component, Cloud Application Manager is a complete feature set for automating ALM – scaling, updating, migrating, managing – for rapid deployment and efficient, governed/standardized application deployment models. Cloud Application Manager supports a CI/CD approach, and ALM must also support CI/CD since our DevOps is modeled on a continuous deployment scale. Cloud Application Manager enables our teams to do their own provisioning, control cloud consumption, manage workloads from one page, and view, implement, and change cloud usage and amount spent on cloud resources. For example, it provides a view into your infrastructure by showing the type and location of each VM, which can then be provisioned (resource management) and configured (configuration management). Cloud Application Manager can manage direct deployments and automate the frequency of deployments, an important aspect for a DevOps ALM. It also integrates across clouds, so that all resources can be viewed in one space, reducing provisioning time through the ability to automate. Using Cloud Application Manager for ALM can enhance your DevOps strategy through automation of cloud deployments and management across platforms, allowing teams to focus on innovation. Ultimately, it provides a centralized visibility into the lifecycle process, an important aspect in DevOps transparency and collaboration.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "javascript-automated-code-testing", "author": ["\n              Steve Veit, Lead Developer. Contact ", "\n            "], "link": "https://www.ctl.io/developers/blog/post/javascript-automated-code-testing", "abstract": "Automated testing is a great way to improve the quality and speed throughout the  development of your code. In this article I'll explain some of the benefits of automated testing, show you how to add testing to an existing project, write tests, and then write code to make the tests pass.\n  This article is written with the assumption you are familiar with Node.js and Gulp . Adding automated tests to your code offers several benefits: In order to get the most out of this process, you want to add tasks to your Gulp file so that the tests are re-run as you add code. The testing framework that we'll use in this example is Jasmine . Install Jasmine: npm install --save-dev jasmine jasmine-core jasmine-spec-reporter gulp-jasmine Edit gulpfile.js and require the testing modules: Add the testing task: Add the following watcher task: This assumes that all tests are under the test directory and all server-side JavaScript files are under the js directory. We'll be testing a file named lib/helpers/acccountVerification.js . It is used to send an SMS message to a user registering for our service. The complete spec file is located here: test/accountVerification.spec.js Create a file under the test directory named test/accountVerification.spec.js . In the file add a describe : We require the file we're going to be testing and the third-party NPM module ( twilio-node ) that our code uses. The Q module will be described later. The first parameter to describe matches the name of the item being tested. Now add an it clause with an expectation: The description string passed into the it function should always start with \"should.\" The descriptions strings for the describe s and it s are combined; so make sure that they make sense. In this case, \"accountVerification should exist.\" The expect function takes as its sole argument the object being tested. There are many matchers. See the Jasmine Matchers . Now run gulp test:js . It runs the Jasmine specs and produces the following output: If you run gulp test:watch-js , the test will be re-run whenever the spec file is changed or a JavaScript file is changed. Now add tests for the sendVerificationCode function: Note how the combined description strings make sense: \"accountVerification sendVerificationCode should be a function\". We're using the beforeEach function. The function passed in will be executed before each it and describe function within the parent describe 's function. The phone and email parameters will be used when we call sendVerificationCode . Let's call the sendVerificationCode function and test its execution. Now it returns a promise . So we will have to provide functions for both success and failure. We do this by creating \"spy\" functions. These functions record their arguments and how any times they're called. We're writing a \"black box\" test. That means we can only test by calling the module's public API and mocking and checking the external functions that the module calls. We notice that sendVerificationCode calls validatePhoneNumber which is a private function. Notice that validatePhoneNumber calls new twilio.LookupsClient , then lookups.phoneNumbers and then lookups.phoneNumbers().get which are all external functions. So we'll need to mock these. We do this using the Jasmine functions createSpy and spyOn . createSpy creates a function that does nothing and spyOn replaces an existing function with a spy that does nothing. However, we can specify that a spy will return a value whenever it is called using either and.returnValue or and.callFake . In this case, we mock the LookupsClient constructor to set the property phoneNumbers . phoneNumbers itself is a spy that returns an object with a get property, which is another spy function. Finally, the get function returns a promise. We create a deferred object using the \"Q\" library and we return the promise of that deferred. Later on we can call resolve or reject on this promise, depending on whether we want to test the success or failure condition. We'll mock the LookupsClient API in the first beforeEach function since these mocks will be used throughout our tests. We test that the LookupsClient constructor function is called. Then, we test that the phoneNumbers function is called with the phone number we will be sending the message to. To do this we use the toHaveBeenCalledWith matcher. Finally, we test that the get function is called with the option to get carrier information on the phone number. If you think this looks pretty involved, you are correct. Most of the time external libraries are called with a single function, not a chain of constructors and functions. Now we want to test when the promise that the get function returns is resolved or rejected. In order to do this we need to know what is returned by the API call in each case. One way to find this out is to add console.log to the code. Another way is to check out the API's documentation. To resolve the get promise, we call phoneNumbersGetDefer.resolve passing the mock phone information. It is important to always call process.nextTick . The then function on the promise is not called until the next tick after it is resolved. Also, there is the done function, which is an optional parameter to beforeEach . It is used for asynchronous tests (like resolving promises). The it functions after the beforeEach are not executed until the done function is called. In this case, done is called after the next tick queue is processed and the then functions of resolved promises have been called. After get succeeds, the private function sendMessage is called which, in turn, calls the external Twilio API. We add tests for those calls too. Now sendMessage returns another promise. So we have to do the same thing as we did above. After the message is sent, the code wants to get the message's status until it has been delivered. Since we call the twilio.RestClient constructor a second time, we want to clear the calls using reset . Again, RestClient.accounts.messages.get returns a promise. So we have to call resolve on that. Since this is the last external API call, after it is resolved, then the promise returned from sendVerificationCode should be either resolved or rejected. We check this by testing whether or not the success and failure callbacks have been called. We also want to check that the correct value is returned. So we add tests for that. We can examine what was passed to the success callback using callsFor . Finally, we'll add a check for the failure case. We'll call reject passing the error value. Just like for resolve , we need to call process.nextTick and use the done function to make the beforeEach asynchronous. This is just a start. You can see how we'd need to add test cases for when messages().get fails and sendMessage fails. Also, we should test the case where messages().get returns a message status of \"undelivered\". Thanks for reading, and feel free to leave any questions in the comment section below.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "deploy-wordpress-janrain-social-login", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/deploy-wordpress-janrain-social-login", "abstract": "Keeping track of hundreds of different passwords for hundreds of different websites is not only a pain, but it leads to poor security practices. Protocols such as OAuth and OpenID have provided single sign-on support, which eventually expanded to use social media platforms such as LinkedIn, Twitter, and Facebook. Once you have single sign-on support across multiple websites and services, you have the added benefit of tracking users across platforms. Janrain offers a single sign-on solution complete with customer identity and access-management tools. They offer a range of products, but in this tutorial we will show you how to deploy WordPress with the Janrain Engage plugin on CenturyLink Cloud services. This provides you with a solid base on which to build a cross-platform social media customer identity solution. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to use CenturyLink Cloud tools. You will also need a Janrain account. Visit the Janrain sign-in page to create one. CenturyLink Cloud Compute servers are high-performance cloud servers. These enterprise-grade virtual machines are easy to deploy and manage from the CenturyLink Cloud Console and via our powerful API. CenturyLink Cloud MySQL is a cloud-hosted managed relational database solution. It is backed by 100% flash storage, and automatically scales to handle even the most demanding workloads. Our first step is to deploy a new CenturyLink Cloud virtual server. Follow the steps below. On the left side menu, click Infrastructure and then Servers . On the left-hand side of the server panel, click on the region for the server we will provision. Your server provisioning request will enter the queue. You can watch the progress of your request on the screen. Your server is provisioned when the status of all tasks in the queue is complete. After your new server is provisioned, in the CenturyLink control portal, click Infrastructure on the left side menu, then click Servers . Next, we will provision a new CenturyLink Cloud MySQL database. Follow the steps below. In the \"Create Database\" form, select \"MySQL\" for the engine. For \"location\", pick a data center close to the one you selected for your virtual server. This section is based on the WordPress Famous 5-Minute Install , but we will guide you through the configuration options for each step. To get Janrain and WordPress configured properly, you will need to create a DNS \"A\" record so you can refer to your server properly. There are too many ways to handle DNS configuration to cover in this tutorial, so we can't walk you through the whole process. However, you will need your virtual server's public IP address to configure DNS. Follow these steps to find it. From a shell or terminal on your local machine, connect to your virtual server with the following command. Replace \"YOUR.HOST.NAME\" with your server's hostname. Install the required Ubuntu packages with the following commands. Download and install the WordPress package. Next, we need to prepare the CenturyLink Cloud MySQL database that we created earlier. For this step, you will need the following configuration values from the \"Provision a CenturyLink MySQL Database\" section. Database name From the shell prompt on your virtual server, run ifconfig to get your server's private IP address. In the example below, it is \"10.80.109.12\", highlighted. Run the following command to configure the database. Replace \"CONNECTION_HOST\", \"CONNECTION_PORT\", \"USERNAME\", and \"PASSWORD\" with the configuration values for your CenturyLink Cloud MySQL database. At the \"mysql>\" prompt, run the following commands. Replace \"PRIVATE.VPS.IP\" with your private IP address. Replace \"USERNAME\" and \"PASSWORD\" with the configuration values for your CenturyLink MySQL database. Each command should produce a \"Query OK\" response. Next, we will configure WordPress to connect to the MySQL database we just created. The following commands will be run in the shell on your virtual server. Run the following command to make sure you're in the correct directory. In your favorite text editor, open wp-config.php . Look for the line that says /** MySQL settings - You can get this info from your web host **/ and edit the section under it to look like the following. Replace \"CONNECTION_HOST\", \"CONNECTION_PORT\", \"USERNAME\", and \"PASSWORD\" with the configuration values for your CenturyLink Cloud MySQL database. Run the following commands to make sure permissions are set correctly and the web server is running. To finish the configuration of your new WordPress installation, follow these steps. Before we add the WordPress plugin, we need to create a new Janrain Engage application to support social login. Follow these steps. Click Create an App next to \"Engage\". Follow the instructions on the app creation wizard. Finally, we will add the Janrain Engage plugin to your WordPress installation and configure it. Follow the steps below: Your WordPress application will now have social login and registration for users. You can see it in action on the admin login form or on blog post comment forms. Janrain offers a wide variety of powerful social user management tools. Once you have Janrain Engage working in WordPress, you can explore further integration in other applications. Leveraging the capabilities of the CenturyLink Cloud with social user management brings the full power of distributed cloud-based applications to your project. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "database-horror-stories-healthcare-gov", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/database-horror-stories-healthcare-gov", "abstract": "On a dark and stormy night, your phone buzzes. It’s 3 am, and you’re still awake. At wit’s end after your datacenter’s team told you, “We don’t know why the system is unavailable, but we’re working on it.” You tremble with helplessness as you pick up your phone and read, “The database crashed and it’s not coming back up.” After long enough, we all have horror stories: the calls we field at awful hours, the system failures that confound us, the vendors that let you down. But often the worst times can yield the best lessons. If you’re American, you may have followed the media circus around the healthcare.gov launch: In essence, all kinds of vendors failed the team through service outages and product failures. Even when products weren’t breaking, the coders building the site didn’t understand their stack, and built sketchy software on top of it as a result. As a result, even when Oracle’s products worked and Verizon’s datacenters weren’t on fire, the site could still barely handle demand. The quick fix, which they executed, was to throw more hardware at the problem. With this approach, code still consumes more hardware than budgeted for, a problem which only gets worse and more expensive as the project scales. So, MarkLogic threw a bunch of their own engineers at the problem, optimized queries, and reduced response times by as much as a four-fifths. Pages that took 1.5 seconds to load suddenly took only 300 ms. Not bad, so what can we learn from healthcare.gov’s mistakes? Coders building on a NoSQL system like MarkLogic with the same assumptions they’d hold for a SQL setup will run into troubles. In the absence of JOINs, for example, you need to think about how you’ll aggregate your data effectively and efficiently. orc-denorm is one solution for NoSQL systems like Orchestrate. Without similar tools, you should put in the time to grok the tech you rely on. Tech debt will hurt more than bad tech ever could. I have made much of my career helping folks migrate off Oracle tech. Everyone who has worked in enterprise has horror stories about them. Verizon, too, has been a little spooky . Do your research, no matter what Forbes tells you. Google “[vendor]+[critical verb of your choice]” and see what spine tingling tales come up around the campfire. Truth is, most tech will do the job. healthcare.gov could work on PostgreSQL as well as MarkLogic, but it would take different expertise. If you don’t want to develop that expertise, seek out and evaluate expert vendors you can rely on. No matter the system, it will break. If you don’t understand it, it will confound you, fail you, and wake you up in a cold sweat at 3am when your phone buzzes reading, “Something broke.” Only expertise will prevent nightmares. If you need to focus elsewhere, delegate to experts you can trust and rely on. Without systems like that we’d all be making our own underwear and administering clusters we built by hand. What a nightmare that would be.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "drupal-php-docker-containers", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/drupal-php-docker-containers", "abstract": "Reading Time : about 5 minutes Drupal is a powerful, popular content management system and web application framework written in PHP. It is extensible through plugins and a powerful API . It can power websites ranging from personal blogs to business collaboration applications. It has a flexible database interface which can utilize a number of different database backends, including MySQL. While it has a lot to offer, Drupal has been criticized for being difficult to install and too complex for beginners. Fortunately, there are steps that can be taken to mitigate some of the confusing aspects of a Drupal deployment. Drupal can be encapsulated in a Docker container very easily. Docker is a lightweight container system that keeps applications isolated and segregated. Its design makes it easy to distribute and deploy ready-to-go software packages, eliminating much of the effort and pain of installing and configuring large or confusing packages. In this tutorial, we will deploy Drupal and MySQL on a CenturyLink Cloud Computer virtual server using Docker containers. CenturyLink Cloud Compute servers are high-performance cloud servers. These enterprise-grade virtual machines are easy to deploy and manage from the CenturyLink Cloud Console and via our powerful API. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to access CenturyLink Cloud products. Our first step is to deploy a new CenturyLink Cloud virtual server. Follow the steps below. The steps below guide you through installing Docker on your new virtual server. From a shell or terminal on your local machine, connect to your new server with the following command. Note: Replace \"YOUR.VPS.IP\" with your server's public IP address. Note: You can also deploy a new virtual server with Docker through our Docker Engine Runner script . Before we install Drupal, we need to deploy MySQL. This is easily done with Docker by running the following command. In this command, replace \"MYSQL-NAME\" with a name for your MySQL Docker container. Also, replace \"MYSQL-PASSWORD\" with a new password for the container's MySQL instance. Record both of these for later. Our next step is to deploy a Drupal Docker container, connect it to your MySQL container, and finish configuring Drupal. Follow these steps. Drupal is a flexible system. There are a lot of options for configuring your new site. Visit the Get Started with Drupal guide for more ideas on where to go next. Once you have configured Drupal and installed the plugins and themes needed to get your new website running, you can save a snapshot of your Drupal Docker container for redeployment. This can be used for quick backups, horizontal scaling, or even blue-green deployment strategies. Docker offers many options to deploy and manage software on CenturyLink Cloud Compute servers. Check out our other articles about Docker for more ideas and inspiration. Sign-up for our Developer-focused newsletter CODE. Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tldr-lessons-learned-from-website-usability-testing", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/tldr-lessons-learned-from-website-usability-testing", "abstract": "Usability is about understanding what someone else needs and giving it to them, not how to get them to do what you want. When Orchestrate ‘s NoSQL database service for developers went into general availability earlier this year, we were optimistic we would have a successful launch. After all, our beta testing had revealed a lot of interest among developers for our product – despite a website one could charitably call “bare bones”  and a stripped down UX for the actual service. So when launch came, we figured a new website that told our story – the story of why we felt it important to build a database service as unique as ours – would vastly increase engagement and drive up our conversions. We thought we had made a great case for who we were, what we were doing, and why. By every indicator – traffic, conversions, paying customers – our launch was a success. For a few weeks we saw the number of visits and signups climbing rapidly and in lock step. However, beneath these numbers we were seeing another trend: half or more of our sign ups were not coming back after checking us out.  And while we were getting much higher numbers, our conversion rates had dipped. We were prepared for the post-surge slump many startups face after a big launch but what we hadn’t expected to see was a decrease in conversion rates. What could make people who had gone to the trouble of signing up take one look and shrug? We did surveys to try and unveil the root of the problem. People told us they loved our website, they thought our API was simple and straightforward, and they asked for even more emails reminding them to use the service. Many promised they really were interested and would come back some day. We weren’t satisfied with the answers, so we contacted a local usability expert who helped us design a study involving developers that reflected our best understanding of our most frequent visitors at the time.  We ran them through the website and product, and what we found opened our eyes to just how far off the mark we, and many others with “beautiful” Websites, turned out to be. It didn’t take long for us to identify the cause of our decreased conversion rates. Mere hours into the study, which felt like a cross between Mission Control for the Apollo 13 mission and a jury room after three weeks of deliberation, we were forced to face some facts.  “But our site is beautiful,” we muttered as we stumbled aimlessly around the remote testing room. “You all told us so. We have the survey results right here.” But the truth was, our target user wasn’t interested in what we had to tell them. At all. TL;DR. This is what we learned: Make code (or whatever the product is) the content . Developers skip text in favor of code – they consider it marketing. When faced with text that explained the code and product, developers always – without fail – choose code samples and ignore copy. Most customers – even non-developers – rarely want to be told why they should use a product and prefer to vet it for themselves. Context that converts a developer to a user can only come from combining documentation with actual examples of functionality. Get real about who a user really is . No one is a “user” until they use an API or product repeatedly. Until then, they are what they are: curious visitors. Words like “user” only bias you towards the unknown person visiting your site. Give them the product as soon as you can . Stop trying to convince them to use the thing you provide or sell or whatever  – just show them the product and throw them into the sandbox. We made a mistake by not allowing access to our product’s functionality up front. People signed up because they had to to actually experience the product to see for themselves, not because they wanted to. Better informed and knowing what to expect, low-quality leads will remove themselves from the pipeline and the right users will bubble to the surface, fast. Follow the Pareto principle . You’ve heard of the 80/20 rule – that a small selection of your user base will drive the majority of your revenue. So, profile actual users to find your sweet spot. “Personas” are plastic, produced, and likely not on target. It is difficult to capture the magnificent uniqueness of real users and their use cases – they are odd, contradictory, ambitious, friendly, generous, vain, and amazing. They are the 20%. Pay close attention to them. Simplify. Simplify. Simplify . Based on the usability testing, we decided to clear away text from the initial web experience, and move copy meant for product managers and executives under the Product tab. We cut out pages and non-product images. Instead of telling visitors something that feels important to you, or explaining what something does rather than show them, let them try your product firsthand. Remove anything that lengthens the path for a visitor between typing in your URL and deciding whether use the product. A final thought for those who chose to read this far: We are quickly becoming a nation of article-skimming bloviators with a half-formed opinion on everything, and it is not our fault. As I began this post, I came across an article by Karl Taro Greenfeld in the New York Times, Faking Cultural Literacy , which concludes we are essentially a culture of grandstanding skimmers who, having read a headline or a few bullet points in summary, feel no hesitation to offer strong opinions on the topics about which they have read only a little. It made me immediately think of a blog post we recently published that garnered over 7000 unique readers, dozens of Reddit comments full of refutations, accusations, and plaudits, and had an average visit time of 45 seconds. And hardly a single signup. This pattern repeated itself with other posts – lots of visits, lots of comments, a little controversy, and few actual readers. Had I read the Times article two months ago, I would have agreed with its author and dismissed developers as, like the rest of us, a bunch of cynical headline readers with one finger on the up or down vote and another on the tweet button. That was before we woke up to something startling – the only thing that stood between potential customers and adoption of our database API was us. Or more specifically, us trying to tell people why they really, really, REALLY needed to use Orchestrate. We learned a valuable lesson: TL;DR is a sign of respect for the potential user. And if you want people to use your product or service, give it to them to use. Don’t tell them about how great it is and why they will love using it. Maybe skimming is a defense against all the crap we throw at users, when we should really just show them the goods and get out of the way.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-to-use-different-docker-filesystem-backends", "author": ["\n              Chris Weyl\n            "], "link": "https://www.ctl.io/developers/blog/post/how-to-use-different-docker-filesystem-backends", "abstract": "Docker's underlying filesystem support options have expanded (AUFS, BRTFS, etc.), but what does that mean? How can you take advantage of different Docker filesystem options? One of the ways Docker makes containerization so easy is by managing an overlay-style filesystem , allowing containers and images to incrementally change the filesystem layout of the image without requiring large copies of multiple images kicking around. With one exception , this is a copy-on-write approach: parent layers are held read-only, and changes are reflected in the working layer. A similar analogy would be version control systems like SVN, that store versions between revisions in the form of the differences. Docker has always supported AUFS as its preferred storage driver, but now they offer multiple backends . They differ from one another in terms of longevity (AUFS), security (or lack thereof -- BRTFS does not support SELinux yet), and availability. While AUFS has been in Ubuntu kernels for quite some time, it is not in the mainline -- that is, upstream -- Linux kernel... Nor is it likely to be anytime soon. If you're like me, then you need a more recent kernel than the stock Ubuntu kernel in order to leverage stable firmware for your laptop's wireless device. Additionally, AUFS cannot be built as a dkms package, meaning the entire kernel must be rebuilt to support it. Faced with a choice between sticking with native AUFS support and horrific wireless stability, building a mainline kernel with AUFS support, and using a pre-built Ubuntu mainline kernel with the proper support but switching my Docker storage backend to devicemapper , well... That's an easy choice. Lossy wifi connectivity is enough to make anyone scream, and rebuilding kernels for little profit ceased to be fun years ago. For the purposes of this post, we're using a Ubuntu trusty (14.04 LTS) environment, with upstream Docker installed. You can follow along at home, intrepid reader, by firing up a Vagrant sandbox of the ubuntu/trusty64 image; that's what the exercises below will be based on. We're going to work from the simple case here, that you can start from scratch. When you change the storage driver you use with docker, you lose access to your older images and containers (for hopefully obvious reasons). For the more complex case, you can use Docker's save and load commands (and export, import) to handle creating tarballs of your containers and images. These tarballs contain the right information to be able to recreate everything across backend storage. This article gives a good overview, but remember, like anything else the process can be made far easier with a little careful scripting: The device-mapper backend leverages some pretty cool new conflations between the Linux Volume Manager (lvm2) and the device-mapper subsystem itself. The device-mapper subsystem, originally aimed at raid devices and the like, has become near-ubitiquous, enabling once-esoteric technologies like LVM and LUKS/dm-crypt block-level encryption to become commonplace. Without getting too detailed, I'll say that the device-mapper backend of Docker makes use of some of the newer parts of the lvm2 and device-mapper subsystems, specifically the \"thin\" provisioning and \"thin external snapshots\" capabilities. Let's fire up a scratch Vagrant machine, install Docker upstream, and stop the docker service: From here we need to install the \"lvm2\" and \"thin-provisioning-tools\" packages -- otherwise things will fail in all sorts of mysterious ways: Next, docker itself needs to be told to use the device-mapper backend; this is as simple as ensuring that /etc/default/docker contains the line: ...restart docker, and that should be all we need to do. Now, if we look under /dev/mapper , where device-mapper devices live, you'll note a couple special nodes, the control node and the docker storage pool: Let's try a simple approach. We'll pull down the latest Ubuntu image and fire up an interactive session: From our host, note what /dev/mapper looks like now: Inside the container, if we look at what it considers the root filesystem to be, we can see it's using the snapshot we saw just above: We see our control node again, as well as the docker pool... and the device containing the root filesystem for our running container. Installing a mainline kernel is left for another blog post.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "geeking-out-on-apis-with-wynn-netherland-from-github", "author": ["\n              Ross Jimenez\n            "], "link": "https://www.ctl.io/developers/blog/post/geeking-out-on-apis-with-wynn-netherland-from-github", "abstract": "Wynn Netherland Author, Full-stack Developer at Github Wynn is a full-stack developer, but prefers to play where the user meets the machine. He’s worked in huge companies and small startups, but prefers smaller sized teams of five or less. He currently works at GitHub on APIs and other things. He has co-authored or contributed to several books, including Sass and Compass in Action , Open Government , and ASP.NET Custom Controls . GitHub had an API wrapper called octokit , it was a really base wrapper. At the time there wasn't too many idiomatic wrappers for the GitHub API, and this was for the first version of the API not the one we currently support. I was building a good deal of mash-ups, I was involved in the Twitter gem, John Nunemaker's Twitter gem for pulling data out of the Twitter API. When I started poking at the GitHub API I didn't like any of the ActiveResource style wrappers that were out there, so that got me into building parts that I needed to consume the GitHub API. In 2012 I was invited to interview, I found that octokit was used inside of GitHub, had I known I probably would have put a little more love into it. I work on the platform team, so it involves the API but also involves the oAuth system that powers the authentication system. Dabble a little bit in hooks and other platform features, security enhancements and things of that sort. New features in the API, keeping the lights on. We put a lot of emphasis on REST and JSON and providing URL affordances for clients to discover what's in the API and make those calls. Honestly, there are still a lot of API's regardless if you are using traditional REST or XML-RPC, theres just a lot of HTTP plumbing that a lot of API's just don't do. For someone to consume your API there are some things you can do to make it easier for them is to lean into error codes. An on-going challenge is when you come up with a new state or condition trying to figure which HTTP code it maps to. JSON Schemas are another tool that are helping quite a bit. JSON is typed, but it really doesn't have a shape. We're used to XML having schemas that declare what are valid values for some of these types and JSON will give you primitives but it doesn't really tell you how big the integers can get, if something is nullable, things of that sort. JSON Schemas patches that hole, both on the input and output side, helping make an API more modern. First, the octokit project is a great community project, Erik Michaels-Ober and some other folks have done a great job of contributing to the project, I don't want to act like I wrote every line of code in that. But I have written a lot of wrappers, and consumed a lot of API's, for me making a wrapper idiomatic to the language you are writing it in is, far and away, the biggest thing you can do to make it easier for clients. If I'm writing Ruby I want it to feel like Ruby, I don't want to have to know the details of the ReST calls I am making behind the scenes. There are certain language constructs you can lean into to make it easier to feel at home in the language you are working in. Especially with Ruby you can create DSL's and nice chained methods that do a number of things that feel very \"Ruby-ish\" whereas if you are in Go you may want to keep it more error handling driven. If you are exposing an API snake-casing your JSON keys, for example, is one way you can help clients build idiomatic wrappers and not have the details of the API bleed through. There are so many wrappers that take an API method and simply wrap it instead of extract it, I think abstraction enables workflows and you can stitch together methods internally and not really worry about API method you are calling behind the scenes. Snake-casing is just using underscores to separate words, opposed to traditional \"camel-casing\" (which uses capital letters to separate words). We use camel-casing in a lot of languages for methods and variables, but in JSON snake-casing makes it easier to parse what those keys mean. If you are going to build an API you need to know curl. I put in the signature line of the GitHub support for the API \"curl -v or it didn't happen\". Which basically means send me some verbose curl output to verify that what your claiming actually happened. Mastery of curl will go a long way because it is the lingua franca of API's. Given that I use curl so much, some tools that I couldn't live without, one is jq . The tagline on the project is \"the real jQuery\", to install on a Mac just brew install jq . Once that is in place it is just a stream parser for JSON that makes it really easy to make a curl call and pipe it to jq and provide a filter, you can then pull back parts of the JSON. grc is another one. If you want to colorize output in the shell, I use this tremendously. I even have a curl alias that maps it through grc so that curl output comes back color coded, which makes it easy to see header values and header names and syntactic highlighted JSON results, which is pretty cool. I'm a pretty close to the metal sort of developer, in the past I have explored frameworks that supposedly made it easier to annotate attributes in your model and expose those via an API. One of the things that more senior developers at GitHub have taught me to appreciate, is decoupling your API from your actual website. That allows you to move independently in changing one side or the other. You can share certain layers of code, at GitHub we share the models and we share the lib folder, but we have different presentation layers. There's a Rails controller that serves up the web and we have an embedded Sinatra app that serves up the workflows for the API. This gives the best of both worlds, you don't have to be worried about ripping out features in the website affecting the API, we can leave the parts that we need to keep the lights on over on the API side. I just started to build some streaming stuff, just experimentation, in Go. I really love the concurrency model in Go. It's not Ruby, sometimes when you switch languages there's a whole mental paradigm shift that has to happen. I've written some thing to consume API's, in Ruby and JavaScript and Go. I just think in Ruby now, it is my first language by far, but I do love developing in Go. One of the reasons is that, because it is statically typed, it helps us remember that when we are building API's to keep those static clients in mind. It's so easy if you have a dynamic background, Ruby, Python, JavaScript to start returning nil for a field. Not thinking anything about it, and not realizing that statically typed languages, C#, Java, those have to take care in those types of values. The takeaway is that one of the greatest things you can do to build a great API is to dogfood it. One of the things I like about working on the GitHub API is that we use the GitHub API to build GitHub. So we have tremendous opportunity to dogfood the API and to figure out if this workflow makes sense for consumers because we are also a consumer. Simple goroutines and the way you can write to channels makes it extremely easy to have just enough concurrency without having to constantly track. It's almost like garbage collection in modern languages, you don't have to count references anymore. Goroutines brings that sort of programming model to concurrency. One thing I like about Go is its terseness, I guess that's a two-edged sword. There are certain things that rae more verbose in Go and certain things that are more terse. One of the things that took me some getting used to coming into Go from Ruby, in most object oriented languages which Go really isn't, it is object based with structs, in most languages your indentation level drives the \"where am I?\" question if you jump into a file. In Ruby it is usually two white spaces, based on that indentation level I know if I am inside a method, inside a module, inside a class. With Go, since you essentially create structs and then you graft on methods on the outside kind of like an exoskeleton you have to read the full function signature to figure out what am I adding this behavior to. And there is no enforcement in there that says this file should define this type and only one type. So, depending on the project you can come in and there is multiple types and multiple methods being grafted onto that particular type in the same file, it can get kind of hairy. Go is still learning what the conventions are, as great as Goformat is there are still some things that it doesn't catch from a style-guide standpoint. From a tool standpoint my background is in graphical user interface tools. There was a time when I could do anything without a right-click property pane. Microsoft was notorious for having things that were hidden away in some component manager, some MMC control, that you would have to open up and do some config setting. When I jumped into the Unix world with Ruby on Rails everything was driven from config files. It has been a natural progression, that you could configure these tools and most had an rc file or some sort of dot file that lived in your home directory that you could provide defaults to, when you started the tool you could provide your own preferences. Its essentially a preference pane stored as text inside of your home folder. Zach Holman, at GitHub, first turned me onto dotfiles because of his article Dotfiles are meant to be forked . This was several years ago and it got me thinking. I was part of a podcast, Changelog and we started interviewing folks that were prolific in the vim community and that got me into developing with vim. Now that I had my text editor in the terminal what I really wanted was to stay in the terminal all the time. When you start a new Rails project you need an editor, you need a shell terminal to execute commands, you need to crank up a server, you need a console for REPL, sometimes you want to tail a log, so you have 5 or 6 environments. I got weary of flipping the command tab merry-go-round to go between the editor and the browser and a SQL GUI tool so I was on a quest to get everything into the terminal. The holy grail for me was finding two things. tmux , which is a terminal multiplexer that stitches all of these terminals together so you can have one logical terminal session and have workspaces per project, but now you can create these statefully. Mislav, who works at GitHub, has a great hack that, its in my dotfiles if you want to check it out, that allows you the seamlessly navigate between tmux panes and vim panes. That really tied it all together so you can create your own IDE in the shell. Dotfiles are just a way to specify all those preferences and bring your own shell scripts to craft your own IDE. If you want to get started go to dotfiles.github.com and there are some frameworks that we feature there and some introductory material that will help you get started. If you are new to the shell, if you are a designer or front-end developer and are used to GUI tools and you want to jump into the shell, start with a framework. Janus, or something like that, and just playing with other people's curated defaults. Jenkins is at the top of the open source list as far as running continuous integration tests. I'm still surprised in talking to people how many development teams don't use even source control, CI is the next logical step to make sure your code is working with the rest of the team and you are not stuck running a 15 minute test suite on your local machine between changes, that can get very tedious. The idea with continuous integration is to let a server take care of that, you just push your branch to the repository and some scripts kick off, magic happens, and it tells you if it passes or fails. Janky is our tool that we have built on top of Jenkins, it adds a more simplified GUI on top of Jenkins. The GitHub API also has a couple of API's that power our implementation. One of them is the status API and this is where it began. The ability to mark a particular SHA, a particular commit in a git repo as passing or failing a series of integration checks. We have three or four environments that our source code has to deploy to and so we have to make sure they run in those three or four different environments, they all have to come back and vote as a unit so it is not done until it passes green in all the environments. There's also the deployment API that can also be configured to roll branches into particular environments and report back to GitHub that these particular bits have been deployed to these environments. Some services that will do this for you, Travis CI is probably the most popular especially for open source projects. CircleCI is very popular as well, for both open source and closed source. Hubot is a snarky little robot that the folks at GitHub built that hangs out in your chat room. He does some important work, some critical work, and then some just nonsense work. He listens just like an IRC bot for commands. We use it for everything from pulling up visualizations of metrics that we track to see how certain endpoints are performing, to track anything that we have instrumented in our code. We use it to pull up logs, investigate logs using Splunk, which is pretty cool. He can also do other really important things like pulling up animated GIF's and insult other people. He's got an attitude, so if you cross him he'll tell you to come at him. I'm working my way through Creative, Inc. the Pixar story. I'm a history buff so I love how that's framed from a historical perspective. From a technical book standpoint, some of my favorites lately have been Building Awesome Command-Line Applications in Ruby , Practical Vim is another one, there's also a great book out there for Tmux . For the most part I get most of my technical content from blogs. My form factor for consuming technical materials is pretty much blog format, I really dig that. The Thoughtbot blog is excellent. NShipster is great for Objective-C and now Swift. Coding Horror is definitely a classic. Inessential is really good. One of the blogs that I find interesting, less so on technical material, Adam Keys blog The Real Adam . His blog is usually full of softer topics and I think that is one of the things that as I age in development I am less technical focused and more people focused. I still dig tech, don't get me wrong, but I am more into what is stable than what is new. And I am finding patterns, from developers way smarter than me, have been around forever. So I have been on this trek to find what has been around, what's still used, and to use that more and more. Yeah, I subscribe to the GitHub Explore mailings and I find this is the best way to find cool open source projects because it will mail you a daily list of what's popular, what's trending, what the GitHub staff has starred, and what folks that you are following on GitHub have starred which is pretty cool. One of the things I have been playing with lately, and like any good open source project its like \"Hey I found this great tool and I'm not sure how you pronounce it\", is vedeu . It’s from Gavin Laking and its a cool DSL for creating command-line, stateful, ncurses style apps in the shell. As I mentioned before, I hang out in the shell quite frequently and so, with a 30 in monitor you just have tons of real estate to crank up a new tmux bay and put something over there, so I stream Twitter with Twitter command line clients and things like that. I'm interested in building a heads-up display for critical metrics from a workload standpoint, API bugs that are open, help tickets that need answering, things of that sort. I think web components are interesting. I have a healthy skepticism based on micro-formats and some other things that supposedly promised similar things years ago. They are a little bit more codified, a little bit more isolated, sandboxed, things of that sort. I remember back in the C# days of getting the component source catalog and drooling over gauge controls from third parties. Going to our manager and saying \"hey, we just need $4000 for this particular slider control so we don't have to write it\". Not sure if we will get there from the web standpoint but it is interesting to see how many times we are going to reinvent certain controls. Even to the point where we are still trying to style away browser specific styles for form controls. I'm interested to see with these more sophisticated types of controls how the community is going to build those and what that will turn into. Webcomponents.org is the website, I've been playing with some of the gallery controls, mostly just piddling. One of the things that does interest me is the upcoming spec for window.fetch which should simplify AJAX calls in the browser and have them consuming polyfill, that David Graham and others have created. There is a polyfill on GitHub that should conform to that API making AJAX calls easier for some simple cases. To be honest, in terms of trends, is figuring out more \"people hacks\" and the community. How do we find mentors and how do we bring along junior developers. Once you hit your mid-30's what does the road look like for you as a developer, if you want to stay creative, if you want to stay productive without being set out to pasture as a manager or something like that. I'm interested in finding folks that are further up the road, that have done that, and have stayed relevant and stayed happy. I think a big part about trends in tech is like life, it is the people involved. To not put your hope in projects and to view tech, like anything in life, as a medium for relationships. Work on the soft skills just as much as you work on your development skills. It's easy as a young developer to have this toxic combination of arrogance and ignorance, we were all there at some point, as you age a bit you figure out \"wow, I don't know anything\". Which is a healthy thing, now you know how to get off the happy path and how to keep others from failing spectacularly. Empathy has always served me well. To think about who is on the other side of this API call, on the other side of this screen I am building, why are we building this, what problem are we trying to solve with this particular page instead of just cramming more info on it. But also exercise some empathy. If I were consuming this API call, regardless that this API call has met the specs that we set out at the beginning of the sprint, can I do the meaningful work that we intended on the other side? If I am going to change this API call what impact will this have on other people's businesses? Being able to empathize with other is critically important. Communication skills. Being able to explain ideas and that just comes with seeking to teach. Don't be afraid to speak at a local meet-up or something like that, doing that will help you hone your thinking. One of the things that I've talked to people about the same issue, one of the things that I have found is that sometimes people feel like, \"oh, I'm a junior developer what can I teach anybody?\" To those people I always say, \"just write down your journey.\" It doesn't matter if you think you sound stupid because you are just learning. There's always going to be someone who comes after you who knows less than you. Think of yourself as writing notes to help the future version of yourself, you can begin honing your communication skills this way. That's great advice and if you find that you're not being stretched and you are the smartest person in the room, seek to get to somewhere that is stretching you. The flip side of that is impostor syndrome , its easy to get into a place and feel \"I really don't belong here\". Just realize that you have something valuable to share, that the specialized expert in this field probably doesn't know the ins and outs of what you were hired to do. Have some confidence and some humility and it'll work out.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "setting-up-a-simple-docker-dev-environment", "author": ["\n              Laura Frank\n            "], "link": "https://www.ctl.io/developers/blog/post/setting-up-a-simple-docker-dev-environment/", "abstract": "Using Docker, your applications sit atop an excellent platform for packing, shipping and running low-overhead, isolated execution environments -- so it's no wonder that many teams are choosing Docker for deployment. But your deployment choices directly affect your development workflow, because your Dev, Staging, and Prod environments should match each other as closely as possible. No matter where you are on the stack, if your apps are deployed inside a container, you should get to know how to use containers in your development environment. The great news is that Docker makes it easy. In this tutorial, we'll be working on developing a simple Sinatra application. If you'd like to try it yourself, you can pull down the code at https://github.com/rheinwein/hello-world-container-demo . Before getting started, make sure that you're able to run Docker in your environment. On Linux, simply install Docker via the official packages . On a Mac, I recommend using Boot2Docker , a tool that will fire up a Tiny Linux VM and make working with Docker extremely easy. There are two different ways of developing with Docker. First is what I refer to as the static container way: running code inside a container that does not need to be modified during the course of development. Most likely, you'll use this style for dependencies, and also during QA and testing. In this type of container, the code is packaged up and you can't modify it after the image is built and the container is running. The dynamic container way is what you'll use when working on an application under active development. Instead of running an image with the code packaged up, you'll link the folders from your dev environment to the container by mounting them as a volume. You can work on the file on your local system and see the changes propagate all the way through the container. For the sake of simplicity, we'll look at the simplest way first: the static way. These types of images, when run in a container, provide an immutable service. You can interact with it, but not change the code. You'll want this type of service for dependencies, and you'll likely build your application in this way when you're ready to test and ship. Here is a Dockerfile for a static Sinatra application that prints out \"Hello World\" in the browser. We'll break down the important parts of this Dockerfile: RUN mkdir -p /usr/src/app -- make a new directory ADD . /usr/src/app -- copy all of your current files into this directory WORKDIR /usr/src/app -- set this new directory as the working directory RUN bundle install -- execute your bundle install within the app directory The ADD instruction copies everything in the current directory into the /usr/src/app directory. Once the copying is done, you won't be able to modify this code. Running docker build -t hello-world . will build this image on your host. You will see each layer of the image being downloaded. You can check to see that your image was downloaded successfully by running docker images . Next, run this image inside a container. The -p flag sets up a port binding rule of -p host_port:container_port . If you're working with a VM (like Vagrant, etc.) remember to also set up a port forwarding rule on your VM in order to access the application from your host. The -p flag only creates a rule from the container to the container's host system, which, if you're not running Linux on your local machine, is most likely a small VM. If you're using Boot2Docker, you can either set up a port forwarding rule to access the application on localhost:XXXX, or you can use the Docker host address given to you during setup. After the port rules are set, you can access your application in your browser at localhost:4567. Alternatively, and probably the most common use case for the static way, you will pull down dependencies from the Docker Hub and run them in your application. Most common dependencies, like databases, can easily be download from the Docker Hub and run in your development environment. But what about the parts of your application that need to change? It's not practical to rebuild and run the image every time you make a change. Instead of packaging the code up inside the container, the dynamic way of creating a container will allow you to access and modify your code, and see those updates propogated to your browser. If you've already built the previous hello-world image, great news! You have a ruby-base image, which is all you need to run a container in this way. If you haven't built a Docker image yet, modify the Dockerfile so that it only has one line. Alternatively, you could just pull down centurylink/ruby-base:2.1.2 by saying docker pull centurylink/ruby-base:2.1.2 That's it! The only thing this image does is creates an Ruby environment for your application to run in, using version 2.1.2. To get the code into the container, there are a few config options that need to be set on the docker run string. The -it flag allocates a tty for interactive sessions, which we'll need in order to bundle install and fire up the application. Unlike the previous static way, which had the container start command (CMD) specified in the Dockerfile, we have specified /bin/bash as the container entrypoint command. When the container is up, you'll be dropped into a bash session automatically. Think of this dynamic container as just a normal dev environment inside a container -- you still need to execute all of the commands needed to get your application running. An important note about the volume mount: just like with the ports, there could be an extra hop in here if you're working on a VM. Make sure that /PATH/TO/CODE is the correct directory on the VM, not your local machine. In some cases, like Boot2Docker, the filepath will be identical. You'll see pretty quickly if you made a mistake as your container directory will be empty. Once inside the container (you'll see root@container_id as the prompt), cd into the code directory, in this case /var/app/hello-world. From here, bundle install. Then start the application with ruby hello_world.rb . If you're working with a VM (like Boot2Docker) remember to also set up a port forwarding rule on your VM in order to access the application from your local host. You have to make two jumps: one from the container to the VM (the -p flag) and then one from the VM to your host. Make sure to avoid any port conflicts from previous containers or projects (you may want to use a different port just to make it easier on yourself). Check out the application in your browser and you should see \"Hello World!\". On your local system, go modify public/index.html. You'll see the changes after a page refresh. That right there is you modifying code that's running in a container! This dynamic container concept can be applied to much larger projects as well, and even multiple projects running inside containers. For example, you could run both a UI and API project in a container -- using container linking, port forwarding, and environment variables to get them to communicate -- and actively develop against both projects.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "updating-the-gorc-the-go-client", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/updating-the-gorc-the-go-client", "abstract": "At Orchestrate we have been adding features like crazy recently ( geospacial , sorting , Multiple Clusters !) While each meant a new function, we decided to update the client to be more robust and future-proof to prevent confusion about which function should be used by default. The typical non-compatible update path for Go libraries is to create a new repo while leaving the old intact. This gives people the ability to switch on a schedule. Because of this, the old client ( gorc ) will continue to be updated as possible, but the new client ( gorc2 ) has support for all existing features. First you need go 1.1 or later. You can then download the package by running: Then you can use it in an application like this: Most of the calls from gorc2 have been back ported into gorc. This allows you to switch to the newer API incrementally. Once completely switched over, it should be possible to simply switch the import path. The only exception to this is that the Vent object from gorc2 was renamed Event2 in gorc in order to not interfere with the existing Event object. The largest code path change will be in dealing with search and list results. Rather than requiring the caller to manage most of the logic it has been wrapped up in an Iterator object that deals with the HTTP calls for you. There are examples on each of the calls that use this model explaining how to work with the object. Changes made in gorc2 will be backported into gorc in order to ensure that support for new features exists in both packages. Switch to gorc2 when convenient. Going forward, it will only get better tested and cleaner, more intuitive API. The complete go style documentation for the project can be found here . A new client was also a chance to integrate DVR testing directly into the package. This client has 100% test coverage !", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "the-future-of-continuous-integration-with-shippable-founder-avi-cavale", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/the-future-of-continuous-integration-with-shippable-founder-avi-cavale", "abstract": "This week we are talking about the future of Continuous Integration through Docker with Shippable founder Avi Cavale. Avi Cavale , Founder of Shippable Born in Bangalore, Avi studied Mechanical Engineering before earning a Masters Degree in Computer Science at Arizona State. I founded Shippable back in February 2013 and just integrated Docker in the last ten months. A little bit background on myself. I was born in Bangalore, India. Undergrad was mechanical engineering and then I did a masters at Arizona State in computer science and then joined Microsoft back 2000 and worked there for 10+ years for different groups, like Xbox and Kinect. Then worked on a Cloud Foundry based startup for about six-eight months before starting Shippable, where I saw the need for CI/CD as a service and was the inspiration behind and creating Shippable. Shippable is predominantly focused towards the developer cloud. Anybody into the whole agile and continues delivery concept should be thinking about using Shippable or products like Shippable out there. People have used Jenkins in the past and got they made Jenkins do things which it was not designed for but at the end of the day what Shippable is about, going and solving those fundamental problems of, how do I manage infrastructure and continuous delivery and make sure my code is always deployable. Using Docker instead of VMs made it 2.5X faster with just pure build time. But if you take a look at it from a more holistic way of, how long does it take to bring up a machine, how much time it takes to install stuff, it’s all that stuff. It's actually much, much more faster if you look at it as total cost of operation. So the reason why it’s faster, is one, we have our own proprietary platform build from scratch, which is optimized to take an advantage of what Docker brings to the table, as opposed to retrofitting an existing product server like Jenkins to do what Docker does. We initially tried that approach and we started having all kinds of issues in terms of scale as well as security so we decided to build this from scratch. The second thing it does is because Docker is portable, we can move around containers on Amazon machines. So, what typically people will not use as build machines, we can actually use those kind of machines and allow Docker containers to float around that gives us extra horsepower that we need. So you don't really get the noisy neighbor issues or queuing issues which is what we can do, where you can spin up machines but just a few API calls you can actually do a lot of things in a very, very cost-effective way. And that's the secret behind it. With Jenkins, you really getting into more operations of VMs and at some point of time it's a very transient workload, so you need to manage elasticity manually. Do I want to keep all these machines running? Do I need to shut it off? All of that stuff starts becoming part of what a developer job is. Docker allows us to kind of abstract those things out. First thing is, you can have standardized images through the Docker Hub. So you can kind of think of it all as it's all standardize images that we can bring get online within like two or three seconds without developers having to actually go in, monkey around on a lot of Debian package installs and all of that stuff. So what we do is at Shippable, there are two ways of how we use Docker. We use Docker containers on which our platform runs natively. So we use the Docker work flows and how our UI server is actually running on a Node Docker container and it all comes with our custom stuff. So that's one part where infrastructure Shippable itself runs on Docker. The second part is where our customer actually running on individual containers that are tailored for every single customer. So in terms of Jenkins, if you really want to run Jenkins as a service now we are sitting down and managing multiple VM images and all of that stuff. With Docker, I can kind of create a lowest-common-denominator image and then lay out whatever the customer needs, which is specific to them on top of that. So it allows us operational efficiency and that means we can run the service way, way more cost effective when we can pass the savings back to the customer. Because of the portability, you can move workloads from dev/test to production and ensure it is exactly the same. You can use Puppet or Chef or SaltStack to build your containers and then test them with Shippable. Shippable isn't great at enterprise stuff yet, we need to build a lot more governance, we need to build a lot more controls. It could be as simple as a container is sitting in your pipeline and you get a text message and it says or somebody in operation says, “yes, let this go into production.” I mean it could be as simple as that so we need to build a lot more of that. We’re being focused a lot on making developer workflows and developer life much more easier, we need to start focusing a little bit on how the whole operation is and how it works. And that's where some of the challenges I want to see for the next six months. The future of programming is in micro-services and 12-factor applications, and the future of CI/CD is testing combinations of micro-services together instead of testing monolithic applications. Older CI/CD systems were not build to test the interactions between many different micro-services and this is creating an opportunity to re-think CI/CD. Shippable gives you 5 private repositories for free. Our competition makes you pay an arm and a leg for that. Then $10/month is the cheapest paid plan, so we are very competitive. Over 4,500 people signed up so far and growing.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "cloud-coding-docker-remote-pairing", "author": ["\n              Don Petersen\n            "], "link": "https://www.ctl.io/developers/blog/post/cloud-coding-docker-remote-pairing", "abstract": "Dissatisfied with the screen share method of remote pairing, I've used Docker and a variety of other tools to make my coding environment portable and repeatable. Here's how. I'm part of an entirely remote team. I'm mobile, and so my most reliable Internet access is tethering to my phone. It's not always fast, and I pay by the gigabyte. I pair frequently, and traditional screen sharing has been consistently awful: bandwidth hungry, lagged, and hard on the eyes. I addressed these problems using a combination of cloud servers, Docker, SSH, and tmux. Aside from an improved pairing experience, there were unexpected additional benefits. Let's get this out of the way early: my method requires a text-based editor to realize all of the benefits. Emacs, nano, vim, any of them will work. It's possible to containerize GUI applications, so what I describe may still be applicable to you – particularly if you have bandwidth to spare, or you intend to automate an environment that you'll run locally. This post is not a technical dive into my particular environment, but an overview of how and why I used a container-based development environment, and particularly where Docker helped. I do link to the actual code that I use, so all of the gritty details are available to you if you're inclined to look further. Laura Frank previously published a blog post on containerizing your development environment. If you're new to Docker, that post explains basics that I will omit. Her focus is on running your application in a container, but I will expand that to include all of the tools you use to author the code, too. If it took you weeks or months to configure your local machine just right, it might seem daunting to automate. My very first attempt at this was an Amazon EC2 instance and a long shell script. It was awful in every way: difficult to author, fragile, and time-consuming to run. What you want is a solution that is easy to build, repeatable, and quick to deploy. Docker arrived at the right time. It provides a combination of power and simplicity that are perfect for this task. Containers are easy to spin up, test, and tear down. Dockerfiles have a low learning curve, and the layer caching feature means you can iterate repeatedly from scratch until everything works correctly. I split my development Docker images into two parts: a base of cross-language tools that I always need(shell, vim, git, etc.) with language-specific tools like a Ruby interpreter or Go compiler on top of that. As an example, here are the sources for my base and Go development images. The gist is that the base image is built on Ubuntu, installs software I always need, and starts an SSH server. You've probably read warnings that suggest keeping SSH out of your images, but that's not applicable in this case. The point of this image is to host a shell from which you can develop. Using SSH instead of just docker run /bin/sh is important because it allows you to use SSH agent forwarding to identify yourself. That way you won't need to copy your private key into the image. SSH will also come in handy later when you want to allow access to others. The Go image descends from the base image, adding Go-specific software like a compiler and development packages. Source code should exist on the host machine and be volume mounted so that it outlives any one container, as mentioned in that README . This setup didn't happen overnight. Docker is flexible enough to let you work on your automation in stages. You can docker run /bin/sh an Ubuntu container, manually run some shell commands, then docker commit the result. You don't have to stop everything and make it work perfectly from the outset, but try and be disciplined enough to jot down the manual steps so that you can automate them later when you have the time. You can use my Dockerfile as your starting point if you'd like, but that's _my_ config and you might hate it. If you look at the contents, what I've done just isn't that complicated. It's a dozen shell commands. You can do this yourself and make it yours. At this point you have something you can use locally, but it's only a containerized version of what you already had. This isn't terribly exciting until you think about the side-effects: everything you need to get work done is backed up, and it's divorced from the vagaries of your host operating system. If you've ever lost a laptop, or held on to an old one or an old version of an OS solely because you were terrified of breaking your development environment, this is good news. You can also fearlessly experiment with your environment, because anything you do can be undone by deleting the container. Your environment is now a portable Docker image that you access via SSH. Guess where this is going? Let's back up for a moment and talk about specific problems with screen share remote pairing. First, it's a bandwidth hog. You're transferring pictures of text, so the person on the remote end of the session is trading their usual crisp text for lagged, compressed images. The host is unaffected by all of this, while the remote client is at a permanent disadvantage. Second, if the host needs to take lunch or leaves for the day, the session is over and the remote either waits or pulls code down locally – unless the host forgot to push before they left, in which case you're stuck. Hosting the development environment we've just built on a cloud server offers a neutral site to hold your pairing session. Both people are on an equal footing. You're only transferring text, so there are no readability issues and significantly better response times. You can also use a dedicated VOIP client like Skype to communicate with one another. The bandwidth usage is minimal compared to screen sharing. So what does a pairing session with wemux look like? Here's a contrived side-by-side demo. For the host instance, I have lately chosen CoreOS. Many cloud providers offer pre-built CoreOS machines, and it comes with Docker and git, which are all I need to get the code and start the development container. Wemux provides an easy-to-use wrapper around tmux's session sharing feature, so that both pairs are seeing the same thing. Tmux's persistent sessions mean that pairs can detach from the session, then later return with everything exactly as they'd left it. This setup has great flexibility. When I started at CenturyLink and was waiting for my laptop to arrive, I worked from the only computer I had handy, a five-year-old Windows desktop. Last year when I had an equipment failure and was left with only an iPad , I was able to continue pairing. Any platform with Skype and an SSH client can be my development box. I've swarmed three developers on the same problem, and hosted multiple pairing sessions from the same cloud instance simultaneously. The sky's the limit, as long as it's text. I've glossed over some details about my specific image setup, but feel free to dive into the source code of my base and Go development images. These are exactly what I use every day to work, so if you're curious about specifics like how I version my dotfiles, or how I deal with SSH keys, it's all there. You can even try it out for yourself. It's reasonably well-documented, and if you already have Docker then you're a single command away. Better yet, spin up a Docker-ready server on a cloud provider, follow the instructions in my README , and code with someone else!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "career-path-of-a-programmer", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/career-path-of-a-programmer", "abstract": "The ugly unfortunate truth that many programmers don't want to face is that many programming careers have a peak and an inevitable decline. Eventually it will get harder to find and keep a job as a programmer. For many, they find out this truth unprepared and flat footed. Today, we are going to cover essential career information you should be thinking about so you can be prepared. TechCrunch wrote \" Silicon Valley's Dark Secret: It's All About Age \", which refers to a study that found the effective career timespan for a programmer is limited. The questions this raises are existential and serious. Everyone kind of knows that eventually programmers can turn into managers or leaders. But many programmers don't understand the expectations and job requirements of being a manager. Sure, we all have managers, but what does it mean to be a manager? What is expected? And what is the difference between a mid-level manager and a senior leader? In this blog post, I'd like to pull back the curtain and show a potential career path from the beginning of a technical career as a Junior Programmer to the apex as a CTO. Note: Many great careers stop at different points of the ladder and stay there through retirement. Some careers even skip rungs of the ladder. But management and leadership roles are not a fit for everyone, and you might find that your interest in management changes over the years. In your 20's you might hate the idea of being a manager, but in your 40's you might hate the idea of writing any more code. It is difficult to predict. However, it is always good to know and understand your options and their consequences. Below is a visual representation of the career opportunities for a programmer with average pay ranges. Note that there are some people making double or triple this much money being a programmer in certain verticals (banking, trading, etc.) so these numbers only represent an average range, not the full spectrum. When you are starting out in your programming career, it can be daunting and frustrating. Some days you feel out of your depth, unsure of how anyone could possibly write big complex applications. And then on other days, you wonder why you haven't been promoted to Senior Developer yet. You look at other more senior developers and think that you do basically the same work as they do. But the earmark of a junior developer is lack of experience. Even the smartest and fastest learning junior developers have not been exposed to enough code or edge cases to have the wisdom of a senior developer. In programming terms, one form of wisdom is known as software design patterns. Though you can read books on patterns, you need to write enough bad code in your career that breaks things and fails to truly and deeply understand the value of software patterns. \"Design Patterns\" by Gamma, et. al. On being a junior developer A senior developer is the typical role for people who get really good at building whole applications at scale. Much of a programmers career can be spent as a senior developer. In fact if you hate doing management and you just love to code, you may be a senior developer for all of your career. I have hired many senior developers of all ages, but this is also the role that gets harder to compete with as you get older. This role can also be a jumping off point for another rung on the ladder. Once you understand technology well enough to be a senior developer, you likely already have the technical know-how to be a technical founder or CTO of a startup. Being a founder or CTO at a startup involves a lot less coding and a lot more people skills. However a deep technical knowledge goes a long way and is essential to doing a good job in these roles. If after 7+ years of programming, if you know that management is not your thing, being an architect is the highest rank left on the technical career ladder. Architects sometimes write code, but more often they design complex systems that will be implemented by teams of senior and junior developers. An architect's job is to use his technical wisdom earned after years of experience (leaning programming patterns and anti-patterns) to create the structure for a successful software project. As new requirements come in, a software architect needs to know the right ways to build and scale all different kinds of applications. A lead developer is a senior developer that other junior and senior developers look to for guidance and direction. Though lead developers usually don't usually hire or fire programmers, they do a lot of similar work to managers. They co-ordinate what work needs to be done and are the decision makers for implementation decisions while writing code. Management is the traditional next rung up the ladder for engineers. There are different focuses for management. If you love keeping track of work-streams and are detail obsessed, then being a project manager is right up your ally. If you obsess about features and product enhancements, then being a product manager is a strong fit. However the most common manager that a programmer turns into is a developer manager. The typical role of the developer manager is to mediate the needs of the product manager and project manager with the personalities of the development team. This role requires strong people skills, talent at mediating conflicts, and frequently acting like the group shrink. The developer manager's job is not just to hire, but also to fire the developers when needed. This means it is hard to be friends with them. Being friends puts you in awkward positions. If you have ever known someone who became the developer manager of a team he used to be a developer on, you know that they seem to change almost overnight. This is why. It is a hard job, and often a thankless one. But it can also be very rewarding to lead a team to victory. If you want to buff up your leadership skills in preparation for a management role, read Phil Jackson's book. The obvious difference between a mid-level manager and a senior leader is that senior leaders are in charge of mid-level managers. However, managers often don't need to be managed. They need to be led. Mid-Level Managers need to know the high-level orientation they need to drive in, not be given turn-by-turn directions. A senior leader's job is to make high level decisions and inspire their workforce to go along with those decisions, to believe in the mission. The higher up the ladder you climb, the less you will end up programming. At the top, it is all about people. Mid-Level Managers still get to have fun dipping their toes in the guts of technology, for a Senior Leader must spend all of his or her time focused on people issues: inspiring, motivating, leading, and strategizing. If you do write code, it is often just side projects (unless you are Bill Gates, but almost nobody is Bill Gates, he's an extreme outlier). Phil Jackson's book is required reading at this level, not just a nice to have. The job as a senior leader is to make sure that everyone at the entire company is rowing in the same direction, making sure that direction leads to dry land, and making sure everyone knows why they are rowing that direction. It is a deceivingly difficult job. It is fraught with peril, heartache and booby traps. If you are up to the task, read \"The Hard Thing About Hard Things\" by Ben Horowitz . Senior leaders are never born naturally. They are made. It takes practice to be good about it. Read Ben Horowitz's blog about Making Yourself a CEO for some examples of this. The opportunities for programmers today are rich and plentiful. And programmers are in higher demand now than any time in history. It is truly a great time to be a programmer. But it is never too early to be planning your career. I hope this post gave you some guidance and food for thought that might come in handy.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "voices-sheri-dover", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/voices-sheri-dover", "abstract": "Sheri Dover (@ sheridover ) has organized PDX Startup Weekend for the past three years and recently founded a new developer school – Code Guild . She’s this week’s special guest on Voices . A long-time entrepreneur who’s passionate about empowering others and strengthening the local Oregon economy, Sheri also founded a Portland co-working space for small businesses, and an incubator. When I first came to Portland in 2011, the common wisdom was that if you wanted to have a successful startup, you had to move to the Bay area. It was understood that Portland was too isolated from any critical mass of talent, mentors and investors to be a valid place to found a startup, at least past the concept stage. By late 2012, the general consensus was that you could start in Portland, but when it came time to market it, you would have to move to Silicon Valley. In the last year, Portland has blossomed into a city where “everybody in Portland who deserves funding is getting funded” and the vibrant, booming tech scene it is today. We have a growing community that includes highly qualified mentors, and we have a supportive atmosphere that other tech communities are reputed to lack. We prepare our students to move directly into positions as junior Web developers. Our Board of Advisors gives us direct contacts with several potential employers, and we solicit contract work for our advanced students. The course capstone is a student-driven project that results in a published Web application. Our program is an immersive, hands-on boot camp running twelve weeks long. Our students learn practical skills with text editors, command shells, version control, and debugging. We teach Python as the core data-crunching language (with SQL for database use), with HTML/CSS for the view layer. JavaScript and Django provide the control “glue”. We are unique in that our classroom environment is a highly interactive lab, patterned after an industrial “bullpen” office. The instructor presents and expands on the course materials, adding practical tips from industrial experience. Students consistently share problems and solutions. The course also covers useful workplace skills, such as brainstorming, meeting protocol, design and code reviews, and interview preparation. Check here for our next Developer Bootcamp . We’re now taking new applicants. I’m not actually a developer – my background is Biology. Having been active in economic development and the tech startup scene, I was aware that there was a shortage of tech talent. I was considering going to a code school myself when an acquaintance who had a background in computer science and education asked me to help him start a code school. As an idealist and entrepreneur, founding PDX Code Guild was a great fit for me. I can use my creative drive to create a viable business while connecting people with great jobs, and connect the tech companies I adore with the talent they need. Try out an online programming course at Treehouse or Code Academy to see if you like it first. Talk to some developers you or your friends know; there’s nothing like practical knowledge of the everyday life. Once you decide you’re serious about a career as a developer, go to a developer training program such as PDX Code Guild. While there are a lot of free online programming classes, only about 4% of people who try to learn programming online succeed. A classroom program will give you a well-rounded learning environment and job placement service. Most schools are inexpensive enough that your first month or two of professional salary will be more than you paid in tuition. First, I want to say I find that the fact that there is a debate about women’s role in tech very encouraging. As a community, I sense that there is a growing awareness, and that positive change is happening. It’s been my personal experience that women do run into micro-aggressions from time to time, such as ideas not being heard, being encouraged to take a more stereotypical direction, or not getting as many job offers, promotions, or term sheets. I believe that it’s our duty to ourselves and to other women to push past that. We may have to occasionally lower the pitch of our voice and keep talking over someone who is interrupting us in a meeting, remind the group that the new cool idea was one that was ignored when a woman colleague brought up last month, apply for more jobs and ask for more promotions. No one is stopping us, we may simply just have to be more persistent. Sheri Dover, PDX Startup Weekend and PDX Code Guild", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "cron-in-production", "author": ["\n              \n            "], "link": "https://www.ctl.io/developers/blog/post/cron-in-production", "abstract": "Cron is a Unix tool for launching processes at given time intervals. It is incredibly useful, but the dangers that it presents can often be overlooked when selecting it as a solution. In this post I would like to spend a little time covering some of the pitfalls that I have seen using cron, as well as solutions to get around these issues. I am going to rely heavily on an example from a previous employer as it demonstrates several major shortcomings of cron, all in one pass, while also being a good example of cron’s power. We had an application that would sync user accounts across all of our production systems. This app would query the LDAP server to get a list of all expected users, and would then add, update, or delete users on the local system to match LDAP. This allowed us to run in production without the stability issues that linking directly to LDAP can cause, while removing the need to manually manage our cluster. This little tool was brilliant and solved lots of problems for us. Cron will email the output of a given job to the user that the job is running as. This can be very useful when some aspect of the job requires alerting the user. That is why this initial design approach was taken. You can spawn a job at midnight to count the total number of requests for the day and email it to you, etc. The problem is that very often you do not want emails from cron jobs. And usually when they start they can flood your inbox very fast. In our example app above, we made sure that the process wouldn’t ever output anything. Everything was running along smoothly right up until we took our LDAP server down for maintenance. This promptly caused an exception in Python which, in turn, caused every job to mail us every time it ran. Annoyingly this made our mailboxes unusable very fast (and worse, started causing delivery delays for all of our email) right in the middle of an important maintenance. In this case, it was the non 0 exit code that was causing the email. Cron supports setting MAILTO=”” in the command in order to disable email, or redirect it to a different account. That only helps if you are editing the direct command line. If you are running a helper script, you will need to redirect and control exit codes. This post was initially written to address scripts so I avoided using MAILTO here. Our solution was to force redirect the output (both stdout and stderr) to null, and to ensure that the command always returned a 0 exit code by running true afterwards, if the command failed. Solution: Prevent output and bad exit codes in the cron command line. Because the typical way to monitor cron jobs is via email it is often extremely hard to find out if a job has actually run successfully. Ideally, your monitoring shouldn’t involve people being awake and willing to read automatically generated emails, especially if the cron job is important. So then, how do you monitor cron jobs with something like Nagios? I use a simple tactic of touching a file that I can then check the age of at the end of the command. Keep in mind the general runtime of the cron job and how many you want to fail before alerting when using this approach. So for a job that finish every hour, I check that the file being touched is at most 3 hours and a few minutes old so that I know when the job has failed at least twice. For human readability I often write the output of date into the file, but that is not strictly required. Solution: Touch a file and alert on the file’s age. When combined with the above issue, the command should look like this: On of the biggest dangers when using cron in a clustered environment is the thundering herd problem. Specifically, when each machine wakes up and tries to query a backend all at the same time. Typically, machines in a cluster are synchronized via NTP , so the time on each machine is within milliseconds of each other, or better. In our account manager example, we had initially configured this tool to run at the top of the hour on each machine. This means that every machine in the cluster would start and query LDAP for a complete listing of all users within milliseconds. Soon enough this would cause the LDAP server to slow down, or even start failing requests. To solve this issue we first setup our machines to randomize which minute they ran at. For consistency we used the MD5 sum of the hostname modded by 60, so it would be consistent with an even distribution across the hour. This reduced the load on the LDAP server from 100% at the top of each hour, to (100/60)% at the top of each minute. Eventually, this too became a problem. So we had to add a sleep at the start of the command as well as the modded minute trick. As a side note, a friend told me that they were getting around thundering herd by using Redis , and within a week they had an issue where the cron jobs over ran the Redis instances listen queue, which caused a minor production issue. So never assume that you are immune to issues due to the cost of the query. Solution: Use minute offsets and sleep to avoid running all the jobs at the same time: When combined with the above issues, the command should look like this: This can be a form of thundering herd. Rather than competing for a shared cluster resource, the problem is that the cron jobs can start competing for shared system resources. By far the most common form of this problem is log rotation which compresses log files. This consumes I/O resources needed to read and write the files, as well as CPU resources to compress them. In my experience this goes unnoticed until it starts to become an issue. While debugging an issue where our site latency seemed to get wildly unpredictable I noticed that our hourly latency graph ended up looking like this: Note the hourly cycle in the graph. At the top of every hour our latency would drastically increase. For the remaining 59 minutes or so the latency would return to expected levels. After digging a bit, it became clear that running logrotate was causing latency to increase. The CPU it consumed would be better suited to serving user requests. This problem is not limited to logrotate. Any job that consumes resources can become part of the same problem. Unix provides some very nice solutions to this issue. The absolute best solution is still cgroups , but that is beyond the scope of this post. I will explain the far simpler approach. There is a tool called ‘nice’ on Linux (and Unix for the most part) that tells the operating system to force the job running to play nice. It gets whatever resources are left, after all the other applications run (though in reality it is given some CPU time to ensure that it eventually finishes.) There is also another tool, ionice, which does the same thing to io. But for this example I will stick to nice. Solution: Use the nice command: When combined with the above issues, the command should look like this: We thought that we would be smart and make our LDAP application retry until it was successful. This was an easy way to get around delays and crashes caused by upstream server outages, etc. It seemed like a really good idea until we were paged because machines had thousands of the processes running, all doing nothing but sleeping and retrying the tcp connection. A network blip had caused all the connections to fail and, soon enough, every machine was trying and retrying, which caused a thundering herd to the LDAP server. But even worse, the outage had continued long enough that several copies of our tool were running, which made the thundering herd worse, and they started spawning faster than they finished. It was not long before the machine was over run. Cron makes no promises that one, and only one job will be run at a time. It keeps spinning up new ones when the time matches its internal pattern. It is up to you as the user to ensure that you don’t allow more than one process to run at a time. Luckily, there is an easy way to do this using the flock command in Linux. We can lock a file exclusively, which will prevent a second process from starting while an existing process is running. Solution: Use a lock file and the flock command to prevent duplicate commands from running: When combined with the above issues, the command should look like this: Sometimes it's prudent to redirect the output of the program into a file. This is useful for capturing the errors, or other such events. The problem is that this usually is paired with the inability to establish exactly when something happened. Seeing a stack trace in the log doesn’t tell you much. For readability its often desirable to attach a time stamp to each line. Luckily, you can accomplish this with bash fairly easily. Solution: Use a shell wrapper to attach timestamps. When combined with the above issues, the command should look like this: The combined command line is anything but simple. Luckily, it's not too complicated to simplify it with a little script. It can do everything but setting the offset at the minute mark. Download cron_helper.sh and put it in /usr/local/bin . This would allow us to do the above complex definition like this:", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "explore-the-marvel-universe-with-mongodb-and-centurylink-cloud", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/explore-the-marvel-universe-with-mongodb-and-centurylink-cloud", "abstract": "Marvel recently opened up an API for its character and comic data, which includes all of the characters, what comics they appear in, and a lot more. After poking around with it for a little while, I decided I wanted a more visual way to explore the characters and the comics, and that this would be a great project to build off of MongoDB and CenturyLink Cloud. We could use graph search to explore the relationship between characters and comics, as well as full text search to explore the data. This project was pretty fast to build. It took a little time to import the data from the Marvel API and to do the design and frontend work, but the actual backend code and database work was really fast (less than a day or so). A node.js app for searching and viewing all the Marvel Comics data. Since Marvel’s API is rate limited, we’ll store all the data in MongoDB and periodically update it from the API. It took some time to get the data from Marvel’s API and I won’t go over that part here, but if you want to download the cached data, I’ve included a dump of it in the repo . View the code on GitHub . View the finished app here . If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You'll need it to access CenturyLink Cloud products. Our first step is to deploy a new CenturyLink Cloud virtual server. Follow the steps below. On the left side menu, click Infrastructure and then Servers . On the left-hand side of the server panel, click on the region for the server we will provision. Your server provisioning request will enter the queue. You can watch the progress of your request on the screen. Your server is provisioned when the status of all tasks in the queue is complete. After your new server is provisioned, in the CenturyLink control portal, click Infrastructure on the left side menu, and then click Servers . Type \"27017\" in the blank box to open up the MongoDB server port. Click add public ip address . With your favorite text editor, open /etc/mongod.conf . Look for the line that begins \"bind_ip\" and comment it out. The top of your file should now look like this: First we’ll start off with a basic Express app. We’ll create a web.js file for our express routes and a functions.js file for our (yup you guessed it) app functions. In the web.js we’ll start out with this: Let’s start off with viewing a specific character’s page. We’ll use the character id provided from the Marvel API as our key. Add this to your web.js file so we can respond to /character/123456 requests. Then in the functions.js file, create a new function that will get a character’s data from MongoDB. Getting data from MongoDB is super easy. This will return the data stored at the key we pass in. But we want to get more than just the character data. We also want to see the comics that the character appears in. You can see the getComicsByCharacter call. We’ll create the function that gets the comics. Now we can view a character, but how about searching? MongoDB supports full text search as of version 2.4. This allows us to do a full search across any text-based field we decide to index. To use this powerful feature, connect to the database server with this command from your server shell. Then, from the mongo client prompt, run these commands: Now let's add full-text search to the application. First create a new endpoint for the search in the web.js file. Then add a new function getCharacters() to functions.js : Back to our query, our search form will pass a few parameters. The getCharacters function is already set up to process different parameters using special MongoDB keywords like $regex . The process to get a comic or search through them is very similar. Take a look at the source code on GitHub to learn more about searching for comics. Now our app can view and search. We use full-text search to find comics and characters, but what else could we do? What about seeing which characters appear in comics together, or seeing which villains have appeared in the most comics? There’s a lot more we could do with this data. Enjoy!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "http-apis-test-code", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/http-apis-test-code", "abstract": "One of the challenges of using a remote API is not having a local instance to write tests against. However, when you're testing you do not want to test the database or the APIs that your code relies on. The goal should be to test your code, because... In March, we showed you how to test HTTP APIs using Python and VCR, but this time I want to show off using Mocha and Nock to automate API testing in Node.js projects. Mocha is a testing framework for Node.js and JavaScript projects. Using it often looks like this: What does testing that look like? Say you ran mocha -R nyan; this is what you'd see: Yep. That's a nyan cat, passing your tests. Mocha comes with a bunch of other reporters, including several for generating code coverage reports. Check out Mocha's docs for more. Nock is an HTTP mocking library that can record HTTP calls and play them back, so that you can either mock an API by hand, or just record actual HTTP calls and use their responses in future test runs. I'll show you how to do the latter. Nock works by intercepting any HTTP calls your application makes and recording their parameters. You can then record those parameters so future HTTP calls get the recorded responses, rather than making an HTTP request to an external service. This makes your tests fast and deterministic. To start recording, just put this in your code: Then, to work with the recorded responses, do this: By default, Nock records JavaScript code snippets, like this: Let's see this in action. GeoNames offers a free API for various geolocation operations, such as looking up latitude and longitude for a zip code or finding local weather stations. Sign up for a free account at their home page and we will use their public RESTful API to run tests using Mocha and Nock. Consider an application where you wanted to provide location-based services for a user. You might want to look up the user's geolocation based on a zip code. However, for testing purposes, remote API calls can be slow. We're going to look up a sequence of zip codes to determine their latitude and longitude. Then, we'll record those HTTP calls, save them, and see how much faster our tests run. I'll be using this code . To follow along, you'll need Node.js installed. Then, just replace \"your.geonames.username\" with the username for your GeoNames account in the following list of commands: How long did it take? Here's my console output: It took about 9 seconds. How about with recorded responses? 49 milliseconds . Dang! So, under the hood, what's happening? All the magic is in test/record.js : record.before checks whether you have fixtures already that should be read, or whether to record and save any HTTP calls made during tests. record.after writes any recorded HTTP calls to disk if you don't already have fixtures. record.js gives us a recorder object that we use to record HTTP calls, and then saves them for later use. We'll then use it in our mocha tests like this: Voila! Our HTTP calls are saved to disk. Now the next time we run npm test, our tests will use the recordings. If you want to force a re-recording, say if your tests change, just do NOCK_RECORD=1 npm test . You can use record.js in your own projects to your heart's content.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "wip-it-good", "author": ["\n              Natalie Simonsen\n            "], "link": "https://www.ctl.io/developers/blog/post/wip-it-good", "abstract": "Now that you are completely distracted with the Herculean effort it will take to get that song out of your head, let’s talk about Work in Progress , also known as WIP . Just the term alone can be misleading – after all, on the surface it seems like the more work we have in progress, the more work we should be getting done. So, in terms of Agile software development, why does WIP matter? Why do we want to limit work in progress? Once upon a time there was a team that appeared disjointed in their focus and skill sets. They were working on many disparate items and did not appear united in their goals. They were functioning, but they could be better. One of the indications of their need to improve was their cumulative flow showed a large number of stories in progress. So we played a game with them— GetKanban . This game simulates software development work using Kanban mechanics and processes.\nThere are many reasons to encourage a team to play GetKanban. Our focus for this particular team was to show them that swarming a few stories was more efficient, effective and productive than distributing a lot of work across the team. We played GetKanban at the beginning of their iteration. The metrics in the cumulative flow diagram below are telling. A cumulative flow diagram is a tool used in queuing theory . It is an area graph that depicts the quantity of work in a given state, showing arrivals, time in queue, quantity in queue, and departure. In this chart the blue/grey section represents work that is complete, the yellow represents work in progress and the red is work that has been committed to and ready to work. Looking at the trajectory at the beginning of this chart, you can see how unlikely the team is to complete the work. This assumption is based on the graph trends at the beginning of the iteration and the additional work added (indicated by the upswing in the red section) early in the sprint. The team actually reduced their focus at the same time additional work was added; this is shown by the yellow section (representing work in progress) narrowing and the red section expanding. Oftentimes, the concept of doing less work is difficult for people to grasp, particularly when we work in an industry rewarding those that do the most work. Through playing a game of GetKanban, the team saw that while at any given moment they were focusing on less work, they were also getting more work to final completion and ready for production. This up-tick in completed tasks is attributed to multiple people working on a limited amount of work. Work was not only produced faster, but it was produced with a higher level of quality. This was due to the freeing up of a wider talent pool to pull from with specific knowledge of the processes needed to complete the work. Who wouldn’t want to get value through the pipeline quicker and start earning a return sooner? Furthermore, like any productive team, this one is comprised of talented individuals with a wide variety of skill sets, personalities and background. While playing GetKanban, each team member felt the limiting impacts of having people on their team who were restricted to just one function in addition to feeling the frustration with too much work happening at the same time. So, what did the team do? They swarmed and paired. They started using and acting on those words, enforced those practices, encouraged team members to start evangelizing this practice, and the team internalized their blockers rather than blaming others for them. The collaborative group also become more united because they had overcome their obstacles and were making tangible progress together. The team also realized the advantage of having multiple eyes on code, which increased their code quality and knowledge base while growing the pool of people who could provide on-call support. More eyes on code meant more people were accountable for what gets pushed to production and the whole team was familiar with the code being pushed. I saw the benefits of this as they planned for their next sprint; they planned stories with the intent of making sure that all team members would learn, instead of just leaving certain types of work to the people who are comfortable in that particular domain. One comment that was made while we were playing the game that stuck out for me was, “There are too many cards in flight on the board.” The follow-on discussion led to team members expressing that they felt it was too hard to make a decision with all those cards; there was too much noise. The conclusion was to reduce the noise for the people doing the work, thereby keeping the task focused and relevant. Product Owners, you have a responsibility here: Don't clutter the team’s working board with features that are distant wishes. The team also started caring about how long the work was in progress from start to finish, or “cycle time.” This isn't something we've been actively pushing, (it's just a pleasant side effect) but it concerned the team that cards were “stale.” Stale can mean the work items are obsolete, potentially introducing the risk that the team is working on something that is no longer a priority. Product Owners need to step in here to evaluate the priority of the work and provide direction. At the end of this iteration we of course had a retrospective .  We went with Start/Stop/Keep. This is what individuals came up with for “Keep” All of these items are consistent with practices and benefits of limiting WIP. The team voted on items and agreed they wanted to keep focused on this practice. They went on to decide they wanted to put a firm limit on WIP. The team committed to a WIP limit of three because they understood the value! While we know our processes are still evolving, the progress made by understanding WIP is tremendous. The team is working on identifying the skills and access needed to be on-call support and developing a strategy around getting their team to a workable solution. After all, they are responsible for their results. To date, they have earned the most amount of money playing GetKanban than any other team. They are committed to continuous improvement and will continue to deliver. It’s not too late, to WIP it.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "could-server-side-dart-replace-nodejs", "author": ["\n              Luc Perkins\n            "], "link": "https://www.ctl.io/developers/blog/post/could-server-side-dart-replace-nodejs", "abstract": "Dart is a Google-produced programming language that has slowly been gaining traction, particularly since its 1.0 release last year. While Dart has thus far been seen as a promising alternative to JavaScript in the browser, I'd like to argue that like Node.js, the true promise of Dart is actually on the server side. I too am skeptical of Dart as a client-side language, but if you cross over to the server, Google is cooking up something extremely promising and ready to carve out a large niche for itself, attracting not just Node.js developers but even developers from the Java community and beyond. Since the earliest days of Dart in 2011, the language has provoked a great deal of skepticism from all sides, primarily because Google has advertised it as a successor to JavaScript on the client side—not just another compile-to-JavaScript option, mind you, but rather a new, full-fledged in-browser runtime suited for a bold new generation of thick client applications. Sounds good, eh? Well, yes, but the skepticism towards Dart is well founded: Nonetheless, there are some encouraging signs for client-side Dart. Markus Persson, aka Notch , the creator of Minecraft , has thrown his weight behind Dart and has even begun building games in it, and projects like AngularDart seem to be catching on. So the skeptics might be in for a surprise down the road. As I argued above, the skeptics are right to wonder how much of a foothold Dart can gain. But let's set aside the client-side Dart issue for a minute and take a look at sever-side Dart. That's when we start to see a much different and more immediately promising picture. The power of Java without the verbosity — Hopefully I've made it clear why Node.js devs should consider Dart. But I think that JVM folks should be paying attention as well, because Dart offers a lot of things that no Java developer could live without: abstract classes , declarative typing (which is optional ), generics , a real import system , metadata annotations , and variable types that are directly inspired by Java, including final and static variables. What this means is that Dart is frankly a better constructed language than JavaScript in almost every regard and a natural fit for JVM developers who want to try something that is less verbose but also targeted at Node.js-style architectures—and who want to do so without giving up certain coveted aspects of Java.\nIn terms of performance, Dart is still a ways off from JVM-caliber performance and probably will be for quite some time. But the core libraries are still coming together and the performance story could be markedly different in a year's time. Dart has already performed exceedingly well on some benchmarks . Node.js without the JavaScript baggage — I like Node.js because of the principles on which it was built: run things asynchronously whenever possible, use data streams whenever you can get away with it, and tie it all together with futures and callbacks. The problem with Node.js is that it's JavaScript. There's no distinction between types of numbers, working with arrays is always janky, NaN -infused wat moments abound, and so on.\nBut Dart gives you everything that's great about Node.js without the baggage of JavaScript clinging to the project like a nagging, alcoholic in-law. Dart has an incredibly robust stream API and top-notch futures support baked into the core dart:async library, plus a bunch of other stuff that you get for free, like WebSockets , SSL support , a decent math library (complete with ints , nums , and doubles out of the box), as well as the foundations for an Erlang-flavored actor system in the dart:isolate library.\nBasically, server-side Dart is trying to solve the same problems that Node.js was built to solve, i.e. problems around asynchronous, non-blocking I/O, but is doing so on a better VM and with a more robust and less flaky language. Another bit of good news is that Dart in practice is highly reminiscent of JavaScript, from the basic syntax to callback functions as first-class citizens, which means that the bridge from Node.js to Dart isn't a terribly precarious one. Other good stuff — Even though the language is young, Dart's package manager, pub , is simple and yet powerful, with clear debts to npm and its package.json -based system. If you're looking for IDE support, the makers of Dart provide their own IDE, the Dart Editor , which isn't too shabby at all on its own. I would recommend, however, having a look at WebStorm 's support for Dart, which is currently very strong. And when it comes to publishing and documenting code, pub.dartlang.org is a great resource, inspired by Rubygems and related sites, that makes it easy to publish projects. For example, I've been working on a Dart HTTP client for interacting with REST APIs called Mesh , and interacting with the main Pub repository has been a breeze. I'm surprised that Dart isn't yet being taken more seriously as a server-side option for a wide variety of use cases. The performance isn't quite up to JVM standards, but Dart beats scripting languages like Ruby, Python, and PHP in a variety of areas and is explicitly designed for the same architectures that Node.js is currently the best fit for. The next time you sit down to write a web server or a Markdown parser or an algorithm library, give Dart some real consideration.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "build-react-redux-frontend", "author": ["\n              Asa Miller, co-founder of standin.io\n            "], "link": "https://www.ctl.io/developers/blog/post/build-react-redux-frontend", "abstract": "This is Part 3 of 3 in the series. Here are the previous articles, if needed: Part 2: Build a Simple API Server to Search NPM and GitHub Data For the front end we’ll use some great technologies. Redux has different concepts from other Flux implementations, or straight React for that matter. These concepts have a learning curve associated with them, but offer a lot of benefits. The first part of the article explains a few of them. The entire state of your application is stored in a single place. This is a big departure from each component managing its own state. The benefit is that applications become more predictable. We can even reproduce an application at any point. Redux also comes with a nice developer tool that allows you to view all actions that have happened and jump to different states of the application. If we’re trying to find a bug, having the ability to move back in time to a point before the bug occurred and look at the state is very useful. This capability allows you to see what action and state change may have triggered the bug. In order to make all of the above \"fanciness\" happen, we need to be careful not change the state directly. With React we're already used to using this.setState({ name: ‘mulder’ }) , instead of this.state.name = ‘mulder’ . We also have to be careful not to change objects stored in state. For example, say our state looks like this: If we want to change the name, it might look like this: This would work fine in React, but it changes the this.state.user object directly. So, there’s no difference between the state before and after the setState command. Since state in Redux needs to be immutable (can’t be changed once it’s set) we need to return a new object for any state changes. For example, (in React not Redux) : In Redux you emit an action when you want something to happen. Then, your reducer handles the action and changes the state accordingly. An action is an event that happens in your application. This can be something simple like a name change, or more complex like hitting an API and retrieving data. Actions are simply a function that returns an object. Take the following for example: An Action Creator is a function that is called to create an action, like the following: Redux only has one store. This holds all of the application’s state. But luckily, you can map reducers to different parts of the store to keep different parts of the application separate. A reducer receives an action and makes a change to the state. Reducers don’t perform any action (like pulling data from an API). Reducers only handle the result of an action. That way, their function is very clear and they are easily testable. If we put X in we should always get Y out, no matter when and where it’s called. The first step is build our actions and reducers. That enables us to have the main logic of our application in place. After that, we can easily build out our components and have them work. For searching our API, we’ll start by setting up two actions: receiveSearch When the user enters a search term and presses the Enter key, that triggers the requestSearch action. That action tells our application to show the loader and tell the user that we’re searching. Now in our application, we can call fetchSearch({ term: ‘react’ }) . It  first checks that the search isn’t already fetching, then calls fetchSearchFromServer() , which dispatches the requestSearch action (so we show the user that we’re searching). Then, it makes the API call. Once the API call is finished, it dispatches the receiveSearch action, which handles the results. Let’s look at the receiveSearch action . It simply creates a new action object with the parts of the API response we want to pass to our reducer, which  actually updates the state. Now that we have those actions set up, let’s write our reducer. Reducers are simple functions that get passed an action and the current state. The reducer then returns a new state. A reducer doesn't perform any actions; so it can be run with any arbitrary data and will always return the same thing. Our packagesBySearch function will get called when an action happens. It’ll then look at the action and, if it’s RECEIVE_SEARCH , it updates the state for that search term. Our state now looks something like this: Now we can initial the search like this: Now that the basic search logic is set up we can start building our React components. All of the components except the root component will receive props for the full application state. We’ll start with the <ScoutApp /> component and use react-redux to bind the Redux store to our component’s props. React-redux exposes a connect method that we call with functions as arguments. Our state is then passed through those functions to props of our component as in the following: The component now has search , searchResults , and selectedPackages all as props. The props are then passed down the hierarchy to the other components. The rest of the components are considered “dumb”, meaning they just receive props and render accordingly. That's how you build a React frontend with Redux. What a combination! Bring together NMP, GitHub, React, and Redux -- you're off and running. You can view the full source here: asamiller/scoutjs-frontend . Don't have an account on CenturyLink Cloud? No problem. Just head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "seven-cms-applications", "author": ["\n              Jessica Weber\n            "], "link": "https://www.ctl.io/developers/blog/post/seven-cms-applications", "abstract": "There are an astonishing amount of Content Management Systems (CMSs) available on the web, especially if you include open-source offerings. CenturyLink offers Blueprints to easily deploy seven different CMSs to your servers: WordPress, Drupal, Joomla!, CMS Made Simple, Enano CMS, Refinery CMS, and Tiki Wiki CMS Groupware . WordPress is software that allows all user levels to build both simple and sophisticated websites, blogs and more. It's easily learned and used by DIY, non-technical folks and developers alike. If you're building your first website, WordPress is a good choice. This CMS is a good venue for developers because the functions are clear, the coding style is extremely transparent and the documentation is unparalleled. It's made up of several common programming languages: PHP, HTML, CSS and Javascript. WordPress is built primarily by volunteers. There are also thousands of plugins & themes that allow you to personalize your website and integrate deep functionality. Additionally, WordPress offers free access to their Codex , which offers extensive developer-related articles, tutorials and documentation of WordPress functions. CenturyLink offers WordPress as a Blueprint , which allows you to deploy your website in a few clicks. Drupal is a popular open-source CMS. It is written in PHP, which supports procedural and object-oriented approaches to programming for software development. As a CMS, it tends to be more conducive to enterprise and corporate environments that provide infrastructure and developer support. There are several important aspects of Drupal that developers need to understand: modules, hooks and themes. Modules are PHP code and supporting files that leverage the Drupal Core APIs and integrate new functional components into the framework. Included by default in any installation, Drupal Core Modules are maintained by the Drupal Product Development Team. Drupal Hooks allow modules to interact with the core code. They make it possible for a module to define new URLs and pages within the site, to add content to pages, to set up custom database tables and much more. A Drupal theme is a bundle of resources (including PHP templates, CSS, JavaScript, and images) that provides layout and style information for some or all of the Drupal content. Drupal is available on the CenturyLink Cloud as a Blueprint , making it quick and easy to deploy the CMS to cloud servers. Joomla! is another open-source CMS solution that enables you to build websites and online applications. Joomla is designed to be easy to install and set up even if you're not an advanced user. Since Joomla is so easy to use, a designer or developer can quickly build websites which are then turned over to clients or internal stakeholders. Many companies and organizations have requirements that go beyond what is available in the basic Joomla or CMS package. In those cases, Joomla's powerful application framework makes it easy for developers to create sophisticated add-ons that extend the power of Joomla into virtually unlimited directions. CenturyLink Cloud makes Joomla available to deploy via Blueprint . CMS Made Simple is an open-source option that works best for corporate or organization websites. It was created primarily for the use of experienced web developers who want a simple and collaborative site that they can make their own. The CMS also has hundreds of third party add-on tools to enhance the core package of features such as a WYSIWIG editor, search functionality, and page templates. This open-source CMS was built using PHP, the same language that the templating engine Smarty uses to provide the caching, templating and logic behind the site. Like most other open-source tools, CMS Made Simple provides documentation on the product as well as a built-in help system on the website. Many of the add-ons are supported by an online public forum. CenturyLink Cloud has developed a Blueprint that allows you to deploy CMS Made Simple with a few clicks. Built on PHP, the free GNU General Public License software Enano touts its offering as a customizable CMS that behaves like a wiki -- it's simple and easy to create content. The developers actually implemented the structure of a wiki into a CMS architecture. However, unlike a wiki, Enano provides a more robust application platform that allows developers to create plug-ins and extensible applications without worrying about the intense details, such as database abstraction, user management and caching. These plug-ins have the support to add or replace major subsystems of the software. The user-friendly API comes with full, updated documentation on creating plug-ins; there is also an Administrator's Handbook that covers every aspect of the software from installation to administration. Though Enano is structured like a wiki, it also has a selective wiki mode for users who don't want their site, or part of their site, to behave like one. You can disable any of the wiki features with the click of a button. To check out the Enano software, use our Blueprint to deploy it in your environment. This Ruby On Rails open-source CMS is available in over 30 different languages and allows contributions from anyone, anywhere, ensuring that the code and CMS stays as up-to-date as possible. The Refinery CMS is targeted towards the end user who may or may not have a lot of technical knowledge but wants to make their own edits to a site. Although anyone with basic computer skills can navigate the intuitive UI, the modular building blocks were designed to allow developers to create easy and quick extensions to customize their site. The entire code project for Refinery CMS is housed in Github as an open contribution repository. People can contribute to the code or point out small issues they run across. This allows many developers to collaborate on the code easily and fix issues fast. In addition to the developer documentation housed on the Github site, there are multiple guides for both easy and complex tasks, all with the end goal of helping the user get up and running as fast as possible. CenturyLink provides a Blueprint that allows any customer to easily deploy Refinery CMS to their servers. The Tiki Wiki CMS is an open-source CMS that boasts an impressive number of built-in features, meaning a developer may not need to spend much time building extensions or adding features to a site. In addition to the standard CMS offerings of a WYSIWYG editor and wiki, some of the more unique built-in features are: a shopping cart and payment capabilities, chat and social media options, office suite integration (spreadsheets, drawings, reports, etc.), and e-Learning integration. While a developer may not need to add any extensions to a site built with Tiki Wiki, if they do find issues with the code or want to make enhancements they can do so easily with the help of the developer section of the website. Code is housed in a repository on sourceforge.net and requires and SVN program for downloading and uploading. The process seems simple and well-documented, and is supported by hundreds of contributing developers. The documentation for developers is highly organized and seems to cover most of the issues or questions someone may have when working with the CMS. There is also extensive non-developer documentation for end users. If you would like to play with the Tiki Wiki CMS, you can use our Blueprint to easily deploy a version to your servers. You can check out more information on the CenturyLink CMS solutions by visiting our CMS page .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "agile-and-no-estimates", "author": ["\n              Tim Hauze\n            "], "link": "https://www.ctl.io/developers/blog/post/agile-and-no-estimates", "abstract": "No estimates? How can a software development team have no estimates? How can an Agile software development team not apply story points to a story? It goes against everything…or does it? We first need to explore why we estimate. On the surface, a software development team “estimates” so that it can predict when work will be complete. In an Agile/Scrum world, we estimate in story points so we can establish a velocity. From the established velocity and a prioritized backlog, we can predict when a set amount of work will be complete. So, there it is…we need to estimate to predict when we will complete a feature. But wait, is there a chance that this process can cause a distraction to the team? Estimates, whether in points or hours or both (equating points to hours), set an expectation. With that expectation, a team will focus on hitting the time window rather than the quality of the product. The intent of story points and velocity is an internal tool for the team to gauge the complexity of work that can be completed over a set period time. It is not intended to be used to formulate a deadline. Are there other, maybe more efficient, ways to predict the completion of work? Cycle time measures the time it takes to complete a story from the time work begins. Couldn’t that be used as a way to predict the completion of work? Throughput measures the number of stories completed for a given period of time. What if we just look at the number of cards we move across the board in a week and use that as our predictor? The graph below shows one of our team’s tracking of velocity and throughput, and indicates story points completed is virtually equivalent to cards completed. OK, so if I use cycle time or throughput, I don’t have to estimate stories? That sounds great! We don’t need estimates! All of those lengthy story sizing sessions can be thrown away because we don’t need to bother “wasting time” discussing the complexity and effort required to complete a story! We don’t need estimates! Cycle time and throughput saves us! But maybe we need to go beyond the surface as to why we really go through the estimation exercise. Can estimating the size of story be used for more than just predicting when a set amount of work will be complete? Many times, I’ve heard story sizing sessions are too long. And pointing sessions are painful. But why are they lengthy? An estimating session is used for the team to come to consensus on the complexity, risk, and effort of story. The team will relatively compare a story to another to determine the size. It all sounds simple, yet seemingly often this ends up turning into velocity killing sessions So, what are some reasons that cause these sessions to be lengthy? A story that is not estimable: What does that mean? The story lacks the detail necessary for a team to fully understand what it takes to complete it. Maybe it’s poorly written, or it lacks clear acceptance criteria. To be “ready for work”, the team needs to understand how to get it across the board. To be sure a story is estimable, you need an estimating exercise. A team struggles to come to consensus on the size of story: Why would that happen? Again, the story is too ambiguous and needs refinement. The team flashes their story size and the numbers are all over the map. The team goes through the discussion as to why the sizes are so different. The story is refined and, after some time, the team comes to a consensus. What other avenue besides an estimation session could help an entire team come to a consensus on what it takes to complete a story? Of course, over time, a team matures and can get consistent in their story writing in that stories have just enough detail for a collective understanding and are all relatively the same size. However, that is a journey that will vary from team to team -- and, it takes time. The story is too large: The team sizes a story that is an XL. They feel it is too big so they break the story down even further and then estimate the size of the resulting stories. While a mature team can easily identify when a story can be additionally broken down, a less-seasoned team will use the estimate as a trigger to further break down the story. So, where does that leave us? There are alternate methods (i.e., cycle time and throughput) that can act as predictors when work can be complete. If we can collect those metrics, we don’t need estimates. But how much value do we lose if we don’t go through the estimating sessions? Mature teams will come to a quick consensus the majority of the time. But if the sizing exercise clarifies the understanding of even one story, doesn’t it justify the value? In LEAN and Agile, we strive to eliminate waste. An estimating session in which the team comes to a collective understanding of a story is not waste. Working a story that isn’t clear and leads to a misinterpreted result is waste. So, if in the end you want no estimates attached to a story, go for it.\nHowever, keep the value of the discussions that occur in an estimation session in mind. And be sure those “consensus” discussions are happening in some form, even if it is during a story sizing session in which the estimates aren’t ultimately attached to the stories.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "live-demo-horror", "author": ["\n              Adam DuVander\n            "], "link": "https://www.ctl.io/developers/blog/post/live-demo-horror", "abstract": "Everything was set for my presentation. After a ten hour flight the day before, I was settled in a Swedish conference room, my slides projected on a screen. My presentation wouldn't start for 90 minutes, but I wanted to be prepared before joining attendees for lunch. As you've guessed, when I returned to the room, the projector refused to cooperate. Furious unplugging and rebooting were futile. At the organizer's suggestion, I began my talk without slides while he called in technical support. I drew diagrams on a large pad of paper, attempting to evoke the information in my slides. Finally and thankfully, the projector flickered on. My nightmare was over. If you've shared code live or presented at conferences, you likely know the feeling of horror when the Internet, your code, the audience, or some other entity doesn't work in your favor. It's more frightening than any Halloween costume. You aren't alone, as I discovered when I reached out to hear others' demo horror stories. Technology finds opportunities to get in the way, even when the projector is operating normally. Richard Seroter , CTL : \"I've had my share of demo issues. There was the time the screen wouldn't show 'presenter mode' and I hadn't memorized all my talking points, so the 60 minute keynote was done in about 30.\" Perhaps the most common and easiest way to put a presenter in panic mode is when the Internet is unavailable. Sometimes this can happen in unpredictable ways. Greg Bulmash , Avalara : \"We were doing a big Scratch project with Seattle CoderDojo, and it's a web-based IDE in Flash with a downloadable Adobe AIR version. A couple of hours before 100+ kids were supposed to arrive, I see a tweet that they're taking down their web site for maintenance at the exact moment we start our event\" Jordan Walsh , Whispir : \"I rehearsed a demo about 10 minutes before giving it and forgot to switch everything back to 'live' mode.. Triggered the demo and told the 40 odd people that awesomeness was about to happen... Nothing.. Tried again.. Nothing.. Well, thanks for coming everyone. Let's just say they didn't buy it.\" Of course, the classic demo fail involves programming under pressure. Nick Quinlan , MLH : \"Giving my first demo ever at a student Hackathon, I was typing away, and got to the place where the students emailed in. I waited a good fifteen seconds for people to shout that they got an email, but nothing was happening. I started freaking out a little, then a voice came from the front row, 'you forgot a semi-colon.'\" Hod Greeley , Samsung : \"Tons of prep for a lab, only to have the company post an update to the SDK which broke many things hours before the start without telling us. Attendees downloaded the latest on site and ... queue the fun.\" You may do everything right, but sometimes someone else has a different idea. Richard Seroter, CTL : \"At my previous job, whenever a teammate presented while keeping the Instant Messenger open (big no-no), I would send them inappropriate messages like 'You left your pants in my office' or 'Did you tell him he was fired yet?'\" Tony Blank , SendGrid : \"My favorite one was that hackathon that I paid to sponsor, and spent about $4,000 in travel and expenses, only to show up to be informed that they decided not to have demos... since 'they'd take a long time.'\" Brandon West , SendGrid : \"I went to Tel Aviv and my clock was off by an hour and I missed my demo. Fun stuff.\" Many times the biggest question mark is the audience itself. Maybe you've prepared for a beginner audience, but the crowd is technical--or vice-versa. Benjamin Swoboda , CTL : \"I was doing a live presentation in the Philippines. I was told everyone spoke perfect English, which they did, but I wanted to check if everyone was comprehending what I was talking about. So I starting calling on people to see if they were grasping the info and making jokes with them. Anyone I called on or made jokes with would smile and laugh! I was killing! By the time I was done, the room was in an uproar. Nailed it. Cloud nine. Walking ten feet tall.  When I left the room, my 'guide' (who had also been in the presentation) delicately informed me that a nuance of this particular culture is that when people are nervous, they react by smiling and laughing. I was offending the whole room.\" Lyza Latham , CTL : \"I once gave a webinar titled 'On Demand, The Musical.' It was supposed to be for the uber-non-technical crowd. I composed a couple dozen songs staged to familiar tunes, like On Demand, Why Not Take On Demand... (sung to All of Me...) And of course, I sang them out at staged intervals. Which was, of course, utterly delightful (to me) and fun (to me.) But about half-way through, a viewer who didn't realize they were NOT on mute said 'Ugh, I wish she'd just STOP singing...' I died laughing...\" Try as we might, some demos are just destined for the toilet, sometimes literally. Dana Bowlin , CTL : \"I worked on a construction site and I was giving a demo of a badge tracking system. Suddenly the system went down. Literally, the server fell into the toilet.     Why was the server there? Well, that was the office furthest away from where bad things could happen and someone was to grab the server and run if bad things happened.\" While it's rare to have a demo get that... dirty , Keen IO's Sarah-Jane Morris suggests we follow her lead and pray to the Demo Gods. Twilio's Rob Spectre offers another perspective: \"There is but one true Demo God, and Her name is Rehearsal.\" What's your demo horror story? Share it with the #demohorror hashtag", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "data-lock-in-a-qa-with-orchestrate-cto-ian-plosker", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/data-lock-in-a-qa-with-orchestrate-cto-ian-plosker", "abstract": "Throughout the development stages of Orchestrate, we spent a lot of time talking to developers about what THEY would want to see in a service like this. We conducted developer focus groups, took surveys, and had a lot of one-on-one conversations to make sure we were addressing as many concerns from the developer community as possible. It was no surprise to us that data portability and vendor lock-in came up time and time again. Our CTO, Ian Plosker, took a moment to address lock-in head on. Lock-in is, essentially, the inability to move your data from one service to another should you choose to. And users worry about this for good reason. With no easy way to get your data out, you might get stuck using (and paying for) a service that you’re unhappy with. Many service providers use lock in as a strategy for retaining users – but the tactic can resemble a hostage situation rather than reflect true user sentiment and loyalty. Even with an easy way to free your data, there is still “data gravity” to consider. The more massive your data set, the stronger a hold your provider has on it. Simply put, more data means there is more data to move. And finally, vendor-specific features can also yield a sort of lock-in. If you rely on a feature specific to one provider, moving to another can prove difficult. Data lock-in is a very legitimate concern. Here are a number of scenarios to consider: Without the ability to extract your data at any time, any number of scenarios could put your entire business or project at risk. First, Orchestrate wants its users to have complete control of backups. Today, we make daily backups of your data, allowing us to restore user data should disaster strike. Soon, we’ll be enabling daily dumps to a specified S3 bucket of your choice. With Orchestrate, you own your data, and you can take it at any time. In the future, we’d like to offer ways to stream updates to our users in real-time. What this means is that users could maintain an independent copy of their dataset in a solution of their choice, so that moving to an alternative would be as simple as flipping a bit in a configuration file. From the perspective of features and pricing, we’re doing everything we can to engender trust. Trust is hard to earn and easy to lose. It’s not a commodity, it’s not a currency, and it can’t be traded. Honesty is the only way to build trust. We strive to be honest in our communication and transparent with our customers and community. We will let users know where features are in the development pipeline, and when they can expect changes to our service. Assuming you had a “Plan B” ready that allowed you to take your backup and transfer it to an alternative solution, yes. Orchestrate recommends, as a best practice, that consumers of any and all cloud services have a “Plan B” in place should any aforementioned scenario arise. Take your backup and import it into the service, database, or databases of your choice. It’s that easy. That’s a possibility. Orchestrate is not a database. It’s a service that amalgamates the features of multiple database into a single API. Assuming you only use a subset of Orchestrate’s features that map well to a specific database or class of database, you could easily move to that. If you rely on a broad spectrum of our features, you will need to replicate those features, either in your code or by using multiple databases.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "our-asp-net5-dnx-experience", "author": ["\n              Brian Geihsler\n            "], "link": "https://www.ctl.io/developers/blog/post/our-asp-net5-dnx-experience", "abstract": "Engineers are always experimenting with new technologies. We embarked on the frontier of new and exciting technologies earlier this year when we formed the \"Tinman\" team to build our Bare Metal offering on CenturyLink Cloud. Bare Metal has a few moving parts, but the main piece is the \"Tinman\" web application that manages our pool of physical servers. Faced with the overwhelming decision about which technology should power this app, we ultimately decided to use Microsoft's new ASP.NET 5 + DNX runtime. We have been using this bleeding-edge technology for over six months and want to share our experience so far. This post is the first in a series of posts which will include: In this first post, we will cover the burning question:\n\"Why the heck did they pick ASP.NET 5?\" I'll answer that in two parts. I'll go over the factors we use when picking a project's technology, and how we utilized that data to ultimately choose ASP.NET 5. Teams have to weigh several factors when choosing a project's underlying technology. Some factors are technical and others are organizational. The technical factors include: The organizational factors include: So how did these factors influence our decision? Our intuition about many of these factors made ASP.NET 5 an attractive option to try versus other potential options like Node.JS or GO, which some of our other teams actively use and like. If I retro-fit our intuition into the rubric above, it would go something like this: Technical factors: Organizational factors: Given all of those factors, ASP.NET 5 provided an attractive, balanced option.  We could use the team's expertise, code on tools we love, learn something new, AND run on Linux! What's not to love? Stay tuned for our next post on how our wildest Linux dreams didn't come true.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-we-work-content-team", "author": ["\n              Content Team\n            "], "link": "https://www.ctl.io/developers/blog/post/how-we-work-content-team", "abstract": "Technology companies each handle content differently; and how your company produces and manages technical documentation has a direct impact on internal and external consumption of communication. Before we define who we are and what we do here at CenturyLink Cloud, let’s discuss why this team exists in the first place. Remember those annoying essays in college? You know, the ones you waited until the day before it was due to start writing – the ones that counted for 50% of your final grade? Some people shudder at the thought of writing; maybe it brings back bad memories. We operate in a digital age. Documentation is an essential part of a team’s success or failure, as the case may be. Teams are comprised of people spanning skill-sets and, oftentimes, what goes on day-to-day inside the team gets overlooked until it’s time to explain what your team does, what service it provides, what product it plans on releasing, how to use product tutorials, how it plans to implement new technologies, or how it builds relationships through collaborative efforts. Here at CenturyLink Cloud, the Content Team serves a myriad of functions that help product and platform teams, partners, contributing authors, and stakeholders communicate their message effectively, accurately, and in a timely manner. Grammatically correct, reviewed, and up-to-date articles, release notes, blogs, use cases, knowledge base pieces, procedures and tutorials that communicate to specific target audiences (note the plural), and educative and informative materials don’t just appear out of thin air – they are process-driven. At CenturyLink Cloud, everything we do is customer-driven. In the developer, web architecture, product-platform world, it’s a standard practice to use documentation as a means to guide and explain your work. Oftentimes, teams and their corresponding developers are supporting several applications at any given time. That makes having the right documentation in place even more crucial in order to track all phases and features of each application. Additionally, documentation helps development, on-going maintenance, and collaboration with others. Across the cloud technology industry, environments, applications, and servers usually have detailed rules that must be documented. At CenturyLink Cloud, the Content Team understands the importance of having such information at the ready, especially when setting up and deploying new application environments and for any on-going maintenance of current testing, development, staging and production environments. Tracking essential information for each environment, such as versioning, naming conventions, server locations, IP tracking/listings, directory paths, operating system requirements, end user account information, maintenance logs and URL directories must become standard practice. Part of the Content Team’s main function is documenting rules and processes for teams. It’s important to create documentation that articulates how and why a program or application runs a certain way. End users should be able to understand the requirements and details for products and services. The Content Team helps to facilitate the creation and publication of help documents, Frequently Asked Questions (FAQs), use cases, tutorials, contextual documents and product pages related to CenturyLink Cloud offerings. This leads us to other essential areas relating to documentation where the Content Team is best utilized and your company might prioritize these considerations as well. For starters, creating troubleshooting documents for development and production issues help bolster FAQs and expedite resolutions, workarounds and solutions for issues. Additionally, for more technical content, the Content Team helps fashion documents to specific target audiences to ensure the author’s intended message is on-point and the information is relevant. And let’s face it, having a guide at the ready not only helps end users and customers, but it ensures each team has a “plan” at all times. CenturyLink Cloud's Content Team understands that proficiency in a product is best gained through experience over time, but documentation can tremendously speed up that learning. On a more linear level, application installation and configuration documents are a must, especially when developers add, remove, and set-up new applications and environments. The Content Team maintains the technical accuracy of documents, organizes the materials and steps, and makes documentation publishable. We get it, writing documentation is time-consuming. Such a conundrum is common across the industry. Although engineers, product owners, and developers don’t need to write the next great American novel – you do, however, need to provide the Content Team with information, as you are the Subject Matter Experts (SMEs). For example, developers might be more interested in an article written from a fellow developer's perspective, but the Content Team can handle all the editing, content direction, and publication issues that tend to be far more time-consuming than the actual writing. In other words, documentation is the responsibility of each team – the Product Owner and the members of the Team. You own the message. The Content Team empowers other teams by taking care of the messy part, making sense of it all. We’re experienced writers. We’ve been doing this for a long time. The biggest pitfall we see is procrastination. At CenturyLink Cloud, we have found the best way around falling into the “pit of despair” is by each team building documentation into their monthly sprint cycle. That message isn’t new. Nobody enjoys the last minute scramble of throwing a story or a tutorial together. That kind of approach is ineffective for any cloud technology company. By working documentation into your sprint and prioritizing the relay of information to the Content Team, you’ll actually save yourselves a lot of time and headaches. Great question! CenturyLink Cloud's Content Team collaborates with product and platform teams, third-party partners, internal and external stakeholders and with other internal teams to produce quality content. This content shows up in several places - on the main ctl.io site, on our ctl.io/developers site and sometimes in other areas (for example, SDK documentation in Github). One more thing: turnaround time on a piece of content is largely dependent on the SME and team member that the content belongs to. More often than not, the Content Team can only work with the material we're given, so when individual teams put off documentation, the process bottlenecks (see introduction). At CenturyLink Cloud, the Content Team functions as a conduit between each of the product and platform teams and the outside world. The industry trend is based on recurred sharing and the ever-increasing demand for consumable and applicable information. By positioning team initiatives to include professional-grade documentation into your sprints, you avoid catch-up work and the last minute scramble. In the wider scope, think of the Content Team like any other product or platform team.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "our-take-apis-will-drive-a-profound-shift-in-enterprise-it", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/our-take-apis-will-drive-a-profound-shift-in-enterprise-it", "abstract": "When we started Orchestrate, we knew we wanted to make it easier to use databases together and we knew we wanted to build a service. We had no idea we were contributing in our way to a much larger movement that promises to profoundly reshape the way individuals and enterprises build software. This shift toward modular systems, ones built on API-fronted services one could easily integrate to compose new applications, captured not just the essence of what we are trying to do for databases, will have a broad technological, organizational and economic impact. Today, we revisit an article in which Orchestrate CEO Antony Falco explores the new “Composable Enterprise,” an idea first made popular by Warner Music’s EVP & CTO Jonathan Murray, and one that is a critical backstory on the creation of Orchestrate. Click here to view the story on GigaOm.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "5q05-tory-adams-from-game-clash-a-made-in-portland-game-discovery-and-reviews-platform", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/5q05-tory-adams-from-game-clash-a-made-in-portland-game-discovery-and-reviews-platform", "abstract": "In our ongoing 5Q series , Orchestrate catches up with interesting folks from around the Web to discuss a wide range of topics relevant to the developer community. Recent 5Qs have featured Mat Ellis , CEO of Cloudability; Adam DuVander , Developer Communications Director at SendGrid; Puneet Agarwal, General Partner at True Ventures; and Tim Prendergast , CEO of Evident.io. Today, long time Orchestrate user Tory Adams tells us a little bit about his own project Game Clash, and how he’s been using Orchestrate – and other Portland-made technologies – to build his gaming discovery and recommendation platform. Game Clash is going to be more than just one service. What we are launching in private beta at the end of Q2 (at the latest) is something we have dubbed “BLASTr.” The concept is simple – let’s say you’re shopping for a video game for yourself or someone else. With BLASTr, you’re able to find games based on different sets of criteria, such as single-player or co-op, by genre, by publisher, et cetera, as well as read reviews about the games from other gamers. You can sign up to be notified of the private beta now. We feel user reviews and scores on a video game carry more weight than a published review that is often influenced by corporate relationships. I myself like to know what my friends, and what their friends think of a game, and “BLASTr” will help with that. We will be launching a web site first, followed by a mobile app. I was always into building things on a computer when I was younger. Once I hit about 19, I ran with it. I had mentors and professors from my college that kept feeding my curiosity, and now I know a handful of languages. Building apps has always been fun – other than the database side of it all. I once wrote an app that was connected to both CouchDB and MySQL. Constantly syncing those two together, as well as updating an access DB…. well it was interesting, to say the least. I heard about Orchestrate from a friend that worked at CPUsage . He showed me a sticker and handed me a business card, and said “check them out.” Once I was accepted into the private beta, I started by just adding test documents, running test code, and I was hooked. I honestly decided to use Orchestrate to build my application because of the ease of use. Something that would take 2-3 services to build now only required one service, one API and one connection. At Game Clash, we have multiple features in the first “app”. We have search, scoring, reviews, following other users, and liking reviews. We use multiple features offered from the Orchestrate API. Game Clash uses the Orchestrate Graph API in a big way. We believe that gamers are far more social than people think, so we wanted to create a dialog system for users to follow the reviews of multiple users. Orchestrate also offers a great base search, for searching “Collections” of data. We use it as a starting point and built a small amount of functionality off the top of it. We also plan on using the events functionality, but that’s all I can say about that at this point in time. I personally have tested out the many features that Orchestrate offers and will say if you plan on building an app, try Orchestrate first, it will make your life much easier. That is true. Just as a quick backstory, we were using multiple services from everywhere: MongoHQ, MongoLab, Elastic Search, Remote MySQL companies, Heroku – the list goes on.  We started talking with CPUsage about some of the heavier data crunching that I need for building the next app for Game Clash. To put CPUsage simply: Imagine what you could with the processing power of thousands of virtual computers! I have no complaints, and only praise for these companies. They take pride in what they do, and I can relate to the passion they take in their product.  My new goal is to use only Portland startups to show people what you can build within Portland, Oregon all by itself.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-you-should-demand-from-apis", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/what-you-should-demand-from-apis", "abstract": "In a recent article on GigaOm, I talked about how the proliferation of APIs will empower developers to influence significant shifts within enterprise IT organizations. Aptly termed “composable” infrastructure by Jonathan Murray, EVP & Chief Technology Officer of Warner Music Group ( @adamalthus ), this new environment promises to rearrange not just architectures, but the entire power structures within enterprise IT organizations by pushing more power to developers at the point of prototyping. Not surprisingly, such an environment should stoke the furnace of innovation. More ideas can be tried, faster, and at very little cost beyond the cost of labor to bring them to life. As we rush toward this brave new world of innovation, it's important to expect the inevitable implications and limitations of composable infrastructure. Not all APIs are made the same. Brian Proffit covered some of the biggest potential pitfalls of mis-managed APIs on ReadWrite. However, are there some key indicators of a well managed API, too. If you are considering integrating a new API into your development environment, here are some important features – a Bill of Rights, as it were – to look for: A clear API policy and/or End User License Agreement (EULA) . It almost feels too silly to write, but if you start using a new API, it should always come with a clear API policy or EULA. It should outline not only your obligations, but your rights as a user and the responsibilities of the provider to protect those rights, or else you'll put your data and application at unknown risk. Terms of use, EULAs, and contracts should be available on-demand and easily understood without an attorney. And they should provide remedies to top issues that you can actually use. If you have to fly across the country to adjudicate a dispute, free or low cost composable service has just incurred a potentially major cost. An Open Roadmap . Restricting roadmap access and input to only the highest paying customers is a barrier that keeps most developers from making an informed choice about that service. An API's product vision should be shared to all users. The process by which features land in the production queue should be readily understandable. A service that plans to break backwards compatibility for a feature that a subset of users depend on should do so publicly, and as early as possible, so that those users can make the appropriate plans to modify or migrate. Operational transparency . Everything is groovy until you find out your service provider stores all their data in a single instance of SQL Lite, running on a Lenovo laptop with weekly backups to a thumb drive. Composable service providers need to provide ample insight into how they operate the infrastructure housing your critical data and services. Providers may claim that all you need to worry about is that their API responds within an SLA, but if you can't get basic answers that satisfy your concerns or curiosity, trust your gut and use a different provider. Providers should make public operational metrics like historical uptime, disaster mitigation and recovery plans and provide at least some information on the architecture behind the service. Performance data . While related to overall operational transparency, performance data doesn't necessarily apply to every system. When it does, a system that doesn't provide performance data and the details for how benchmarks can be reproduced should be approached with trepidation. While not a deal breaker for everything, it behooves you to have a firm understanding of the performance you can expect, and ensure that the service will willingly provide reproducible benchmarks. Security – As Tim Prendergrast, CEO of security firm Evident.io recently told us , you should evaluate your service provider's security across two dimensions – proactive and reactive security. On the proactive side, the service provider should disclose how they secure their systems, at least enough so that you can get a sense of whether or not security is an afterthought or a focus. Have they shared the standards with which they comply? Do they have anyone focused on security? What security and privacy policies have they published? On the reactive side, they should offer an easy way to report bugs. Do they run a bounty system that sends the message that they want help solving security problems? Do they promise to do post-mortems and inform you after an event? Have they ever failed to disclose a security breach ? Lock-in . If you are locked in, you are trading whatever convenience the service provides for future mobility, and hence freedom to pursue what is best for your project or business. Suffice to say, if the service fails any of the following criteria, you are in jeopardy of signing up to the infrastructure version of the Hotel California (you can check in any time you like, but you can never leave…) : You can cancel at any time. That is not to say that if you pay for a fixed term in return for a discount, you should be able to renege on that commitment, but there should always be an option that allows you to terminate service at will. Your data is your data – you should be able to leave a service at any time and take all of the data you put into that service with you. Otherwise, it isn't your data. Interoperability – even if you can get your data out, if you have nowhere else to put it, you are still locked in. Make sure you know when you are consuming features of a service that are proprietary. Simple pricing – You can't understand what you're getting into if pricing requires an MBA and MatLab to comprehend. You should know that if you take action X, it will consume service Y, and that service will cost you $Z. Free tier? There is no free tier. Even the easiest to use infrastructure requires your time and effort to try it out. A free (and easy) tier . Easy means two things – how long does it take to get started, and how easy is it to grok the service's utility and benefits? Are there clients for your preferred language? How long does it take you to open an account, read some basic documentation (yeah, right) and start curl'ing JSON objects into the API? One architecture for all user levels – Free and low cost models that include a multi-tenant architecture at the low end of the consumption scale, and dedicated systems at the high end, are just masquerading as open and composable architectures. If you have to pay extra to TRULY experience the full benefits of a system (including paying to migrate your data between tiers), if it involves dedicated infrastructure, and if those benefits (a.k.a. attendant problems) come at a premium to the Free-n-Easy tier, you might find yourself with few of the benefits of a modular system, but a whole lot more expense. You should be able to experience the full scope of a service and only pay more if you use more of the service or desire professional services. The list is certainly not exhaustive, but should serve as a good starting point for assessing API candidates for your project. They're rules we take to heart as an API provider, and attributes we expect from APIs that we use in our own applications.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "immutability", "author": ["\n              Originally posted on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/immutability", "abstract": "Immutability is the idea that data or objects should not be modified after they are created. This concept has permeated throughout computing, and it is a core tenant of functional programming languages such as Haskell, Erlang, and Clojure. Popular object oriented programming languages, like Java and Python, treat strings as immutable objects. Immutability has been put forth as a way to simplify parallel programming. Coordinating actors reading from and writing to mutable state is the primary challenge of parallelism. The version control system Git is another example of immutability in practice. Each version in a Git repo is effectively a tree of pointers to immutable blobs and diffs. New versions introduce new blobs and diffs, but those necessary to produce the previous version remain intact and unchanged. In the past few years, immutability has entered the conversation in databases and data storage. Pat Helland of Salesforce gives a great talk on how “ Immutability Changes Everything “. In a Hadoop Distributed File System (HDFS), once a file is closed, it cannot be modified. For Datomic , a database designed by Rich Hickey, the creator of Clojure, immutability is a core concept. Datomic is a time-oriented database, wherein the entire database is treated as a time-ordered series of immutable objects. This simplifies reasoning about the state of data as all queries are performed at a particular point-in-time, and at any point-in-time, the database is consistent and unchanging. When we first conceived of Orchestrate, Rich’s ideas around immutability were very much in our minds. We designed Orchestrate with immutability as a central tenant of the system. Every collection-key pair in Orchestrate identifies a time-ordered collection of immutable objects. This time-series represents the version history of that key. When you PUT a value to a key in Orchestrate, you will be provided with an identifier for that particular value. We call that identifier a ref . When you GET by key, you will be returned the latest value assigned to that key. If you were then to update that value and PUT it back to that key, that updated value becomes the head. Even as you change the head value of a collection-key pair, all previous versions remain available. You can retrieve old values by querying the ref provided when storing or retrieving those values. Even DELETE s are non-destructive. A DELETE sets the head version to null. All previous versions remain available. (Side note: we recently rolled out purge deletes which run counter to immutability, but allow users to ensure that data has been permanently removed from Orchestrate.) From the perspective of a user, Orchestrate is non-destructive. All your old data is available. If a mistake is made during an update, an older version can be retrieved and restored as the current value. Immutability offers a sort of conceptual simplicity in the face of our search subsystem being eventually consistent relative to key/value data. When search results are returned, each value is accompanied by its full qualified path, its collection, key, and ref. This fully qualified path is guaranteed to identify the same value that was returned by the search query. This comes in handy when performing updates. Should you update a value using a search result and you do a conditional PUT , a PUT where you tell Orchestrate which ref your update was applied to, it will reject the update if that ref being updated is no longer the latest value. Treating objects as immutable confers other benefits vis-à-vis other databases. Coordinating multiple actors updating the same value is as simple as using a conditional PUT. Comforted by the knowledge that old data remains available, you can provide your end users ways to track and restore old versions, even in the face of destructive edits. Further, Orchestrate is auditable by data governance professionals. They can see what was changed when and how it was changed. This concludes our initial discussion of immutability. Immutability will show up in future posts in more practical contexts such as backups, conditional updates, and search. By the way, we're live now in the CenturyLink Cloud . Sign up today for Orchestrate at orchestrate.io .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "dont-fall-for-false-choices", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/dont-fall-for-false-choices", "abstract": "As NoSQL databases mature and adoption widens, some in the NoSQL sector believe users will trade ease of use for reliability. At Orchestrate, we believe this is a false choice. The following excerpts are from JAX’s interview with Bob Wiederhold, CEO of Couchbase, on their strategy to compete with MongoDB. JAX: How have NoSQL market dynamics changed since Couchbase first launched? Wiederhold: In the first few years of the NoSQL market, it was really characterized by grassroots developer adoption. Developers were downloading the software, playing around with it, and generally using it on relatively small applications and somewhat rarely using it for business critical applications.\nThat really started to change in a big way in late 2012 and throughout 2013. What we started to see in a big way was big internet companies and enterprises starting to believe that NoSQL was really strategic to their infrastructure and they started doing deep technical evaluations of the various competitors and then they began to deploy NoSQL in a big way under business critical applications. Before Orchestrate, a number of us worked at Basho, which built a database called Riak. Riak came into the NoSQL scene in August of 2009. We’d even been told the year before by investors that “databases are a solved problem” and that no one would seriously consider databases other than Oracle and Microsoft and then proceeded to beat Oracle out on a number of huge projects. We watched the NoSQL market evolve. Based on this experience, I agree with Wiederhold on the history. Wiederhold then goes on to discuss the evolution of the NoSQL market to its current status: We started to see a very significant increase in this period, when the dynamics of the market really started to change. It that phase, it was a much heavier focus on ease of development, and Mongo was quite strong as a result during that first phase, and gained some significant popularity.\nWhat we’re seeing in this second phase, with the more business critical and mission critical applications, is that scalability and performance are dramatically more important because these apps are already operating at significant scale and they need higher performance. Our own experiences during this period with Riak (known to be horizontally scalable, resilient to failure, and not particularly simple to run) supports Wiederhold’s description of the evolving NoSQL. MongoDB is incredibly easy to get started. Most package managers have a one liner to install MongoDB and the UX of Mongo has always been top-notch. MongoDB’s makers succeeded in optimizing it for “ease of development.” Unfortunately, central to Wiederhold’s assertion that “second phase” use cases have been willing to sacrifice ease of use for scalability and reliability is the notion that such a choice is unavoidable, that one must either choose ease of use or the scalability, and that business critical applications make this choice always in favor of the latter properties. For the Orchestrate founders, we all felt the shift into this second phase as well, but we didn’t believe such a choice was necessary. In fact, we felt the opposite – that the second phase of scalability and performance, in order to succeed, had to maintain and master the lessons from the first phase on ease of development. Two trends shaped our belief: First came the rise of polyglot persistence . As we moved into the “second phase,” developers and operations teams had many new databases with which to build applications. Under pressure to add search, geo, social, and event functionality to their applications, developers could now play to the strengths and overcome the limitations of each database. Unfortunately, polyglot persistence meant operational complexity. Companies ran multiple databases in production. The move from the first to the second phase came with a lot of additional costs. Second and perhaps more importantly, we saw the rapid rise of cloud computing. Suddenly spinning up new server instances became easy and inexpensive. What we found important here was that expectations were changing. The developer no longer needed to tolerate long delays and high costs to use basic infrastructure. If basic services could be accessed and scale horizontally to vast numbers just by programming an API, why would developers both running their own corresponding versions. So, we saw developers wanting access to more query types and we saw API-driven services replace traditional infrastructure. Given those two trends, it seems silly to me to believe developers will resign themselves to the “second phase” of NoSQL software where they trade ease of development for scalability and performance, and it’s operational complexity. Scalability and performance should not get in the way of developers innovating, any more than spinning up a server or environment does (AWS, Heroku, etc). We believe the individual developer is critical for creation, innovation, and evolution. It is the “grassroots developers” that build businesses that explode. When those businesses grow, developers shouldn’t have to trade “ease of development” for “scalability and performance”. We believe in the future what was once heavy infrastructure will take seconds to sign up for and begin using. When users are asked to choose between scalable, high performance systems, or easy-to-use data infrastructure, they will choose a third option – one that combines both options.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-coreos-rocket-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-coreos-rocket-on-the-centurylink-cloud/", "abstract": "In December 2014, a key player in the Docker ecosystem, CoreOS, announced \"Rocket\", a competing container runtime to Docker. They spelled out their motivations for creating Rocket in this blog post . While this caused some controversy amongst the Linux Container community we are simply looking at how to deploy and evaluate Rocket in this tutorial. Rocket is an alternative to the Docker runtime, designed for server environments with the most rigorous security and production requirements. Rocket is oriented around the App Container specification, a new set of simple and open specifications for a portable container format. According to CoreOS the important factors in the design of a container are: As we have done previously with Docker, in this tutorial we will show how to install and run Rocket on the CenturyLink Cloud and use it to deploy containerized applications. Note that Rocket is in a very early and immature state so we need to install component by component, the CoreOS team is soliciting input and contributions by the community. CenturyLink Labs recently did a podcast with Brandon Philips, the CTO of CoreOS and you can hear his view on Rocket, Docker, and the future of containerization . We will create a standard server with the following characteristics: Standard default of 2 cores and 4GB of memory, with Ubuntu 14.04 (64 bit) installed as the base operating system. Create a public ip and open these standard ports: In addition open port 5000 , this is the port that our Rocket contained application will listen on. Login to your server and ensure that you are running the newest base OS bits: If you check the OS version version via: You should see something like: You now have a directory called \"go\" under your home directory. Move this to /opt which is the Go default (or else you need to adjust some of the base defaults). For everything to work like it should, you will need to do the following to complete your Go setup. Create two system variables called GOROOT and GOPATH. These two variables will be used by Golang itself when building your application. You will also need to create a directory to store any dependencies being pulled. Run the following to make these updates persistent after reboots: Create a working directory ~/rkttest and switch (cd) to it. In order to validate the installation we will create a simple \"Hello World\" program. Create a file called goworld.go and enter the following: After saving the file you can run the program by: You should see \" Hello World \" printed to the screen. Go ahead and delete the \"goworld.go\" program as we will be creating a more complex version of the app later. Move back to the home directory. Grab the Rocket release from GitHub: You can validate the Rocket install by: And you should see something like: Move back to the home directory. Next we will be downloading the tools necessary to build the containers that Rocket will deploy. The containers are defined by the _App Container Specification_ that can be found on GitHub here . The \"App Container\" defines an image format, image discovery mechanism and execution environment that can exist in several independent implementations. The core goals include: To achieve these goals this specification is split out into a number of smaller sections. Download the current App Container Specification (ACI) to get the actool program: Change to the new spec directory: Execute the following to build the tools: Add to the path and make persistent: Move back to the home directory. Next we will create yet another \"Hello World\" app in Go in the rkttest directory. This one is a bit more complex, as we will be making it accessible through HTTP. Create a file called _rktworld.go_ and enter the following: Build the static binary by: You will now have two files in your directory: rktworld.go and the binary rktworld . In order to run the container we need to build a manifest file. Create a file called mainifest.json and enter the following: Validate the newly created mainfest file by using the actool : You should see output like: ] Finally, we are going to build the actual container application. To do so we will execute the following steps: And you should see output that looks like: In order to start the rktworld container application, run the following: Validate by logging into a second terminal locally and running: You should see: Finally, point your browser to http://{PUBLIC_IP}:5000 and you should see: ] Congratulations! You are now able to create and run ACI container applications via CoreOS Rocket on the CenturyLink Cloud.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "agile-story-grooming", "author": ["\n              Trey Willis\n            "], "link": "https://www.ctl.io/developers/blog/post/agile-story-grooming", "abstract": "How an Agile or a DevOps team operates is largely defined by those working within the team. They typically develop working agreements and team charters to ensure that all members of the team are comfortable in their working environment and are aware of what they are working to build. The way in which the team interacts can be defined when the team forms, but it should also evolve and adapt to become more effective over time. Such is the purpose of using the ritual of the Retrospective within Agile methodology. At CenturyLink Cloud, we utilize different tactical approaches regarding how we perform the various ceremonies within the assorted feature teams. However, the end goal is the same. Aim to create automated, programmable, self-service features that are of the highest quality, all while providing a delightful customer experience. Story grooming is one of the practices in which our Load Balancer team has found a nice rhythm. As a team, we’ve found that not much beats a well-groomed story when you’re talking about creating team velocity. In this post, we talk about how one team grew into Continuous Planning over a few months, and how our team and product benefited as a result of this progression. When we initially formed the Load Balancer team in the spring of this year, we defaulted to using the typical approach to creating and grooming the backlog through the Product Owner (PO). Initially, the PO was almost always responsible for writing and grooming all of the stories. While this approach was functional, it was clear that there were a multitude of opportunities to operate more efficiently. From there, we moved to a \"Three Amigos\" type of approach where the Product Owner would still create the stories and groom them initially. Then the product owner would consult an engineering lead on the team for further dialogue and a deeper technical understanding of how to approach the story before we planned that work with the rest of the team. Over a relatively brief period of time, we subtly migrated to more of a full team-based approach for this discussion. In short order, we saw the distinct value in such a discussion with the whole team as long as we could control the focus and use this exercise to further develop a shared consciousness and direction of the team vision. Now we groom stories on a daily basis with the whole team. Had I heard about this 3 years ago, I wouldn’t have allowed it! I couldn’t help but assume that this approach would have wasted too much time across too many valuable resources. Through data-driven analysis of our productivity, we have found that we actually produce more, higher quality work in less time with this approach. Our team techniques that resulted in this outcome are identified below. For starters, we found that using the given, when, then formula to best define our acceptance criteria worked well for our team. We set that as the baseline expectation that each story had to have before we brought it to the team, and we only got better from there. As mentioned, we started to include the whole team in the discussion. When we did that, however, we added more opinions and much more discussion. So, while we greatly appreciated the benefits of the full team having a very solid perspective on what we were building, how we were approaching each piece of the puzzle, and how each puzzle piece effected the whole picture, we were forced to find a way to control that dialogue to be certain that we weren’t wasting more time than needed. As such, we started to focus on ensuring that we had time-boxed activities to control the discussion. We added another tool for our team members to use when the conversation started to stray from the specific story in question. The team decided that we could call “squirrel” when the discussion needed to be refocused to address the story at hand and the task within it directly. We use this technique to center our discussion in situations when we’re going too deeply into actually solving the technical design of the story, or when we start to discuss follow-on work that might occur after the story that we’re grooming is completed. While calling \"squirrel\" may sound a little silly, it has certainly helped our team to keep the planning and grooming conversation collectively focused and productive. Any tangential discussions can be held after the immediate need is addressed which keeps us moving through grooming a single story at a time. Daily grooming has several opportunities to add value to our grooming and planning practices. It certainly helps to spread out the planning and grooming ceremonies over a series of shorter sessions. That helps our team to avoid burning out from a long grooming session as we prep for a new iteration. It also creates a forum for our team to identify and to discuss new items on the backlog that the Product Owner has only recently added. This helps the whole team to be constantly aware of the work that’s coming and to stay tightly aligned with what we are building. Finally, daily planning gives the team a continuous opportunity to look at stories multiple times. We still review all cards with each iteration to ensure that we have the most current information and thought processes. We haven’t totally forgotten our discipline from where we started. We still perform all of the typical Agile rituals during each iteration, but the results of our evolution allow us to be very efficient with those practices. Every four iterations or so we still have a longer planning session due to new information or a subtle change in plans, but we use the time very effectively even when the discussion needs to run longer. We continue to time-box our grooming and planning sessions after each iteration, and we eventually evolved into grooming slightly more work each day. We further advanced into grooming and then estimating a small amount each day. Eventually, we had come so far in our common understanding of how to approach the work and what the relative sizing would be that we didn’t have a need for an iteration commitment, as we had organically grown into a continuous planning cycle. In our journey towards continuous planning, we achieved several objectives that contributed to the power of our team. We wanted the team velocity to get faster and more consistent over time. The team got tighter as a unit; thinking more as a group rather than as individuals. Our stories got better, both from the product owner and then in terms of how effectively they were groomed. Most importantly, the vision continues to get clearer to all members of the team in order to give each team member the greatest opportunity to offer their best contributions.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "asp-net5-linux-docker", "author": ["\n              Brian Geihsler\n            "], "link": "https://www.ctl.io/developers/blog/post/asp-net5-linux-docker", "abstract": "In our last post from this series, we covered why we chose ASP.NET 5 for Tinman, the core application managing CenturyLink's Bare Metal servers . In this post, we'll share how we failed to realize a wild dream: running a .NET web application inside Docker. Like any good story of failure, this one starts with excitement and optimism. We started the Bare Metal effort, created our new ASP.NET 5 project, and started having crazy thoughts. Up until this point, .NET development was tightly coupled to Windows. If you were writing .NET code, you were running it on Windows. But things were different now. With the new DNX runtime, Microsoft demonstrated a serious commitment to cross-platform .NET. Windows was no longer the only option for .NET applications. This was music to our ears. We prefer to run services on Linux. We also like C#. Linux and C# were no longer mutually exclusive - we could have both! We started asking questions that previously weren't valid. What if we ran Tinman on Linux? And even crazier, what if we ran it inside a Docker container? Great ops and C#? Does not compute! So we tried it. At the time we were using beta3, so Mono was the only option for Linux. We looked around online and found Mark Rendle's precursor to the Docker files listed at Microsoft's aspnet Docker repo . We built our own container on Mark's file and in a few days had the Tinman API running in Docker and servicing requests. We were pretty honking excited. This was a brave new world! The first few weeks with Docker seemed great. We enjoyed how easy Docker made deployments. We liked writing C#. We implemented zero-downtime deployments using NGINX + Docker. All appeared to be well. Except for one thing. Sometimes, albeit infrequently, Tinman would die. When infrequent and difficult-to-troubleshoot problems happen, the temptation is to chalk it up to random events and move on. We unfortunately succumbed to that temptation for a little while, but eventually saw the seriousness of this problem. After figuring out how to debug logs in Docker containers (a somewhat frustrating side-effect of working in Docker), we discovered Mono was crashing. Worse, there was no managed stack trace for the problem. It looked something like this: ​\nThis looked serious. A managed stack trace would mean that Tinman, or even something in the .NET framework, was crashing. But this was lower-level. Mono's guts were crashing. Our next step was to reproduce the problem. Our working theory was that HAPROXY's regular health checks against the API were overloading Mono. This theory seemed absurd because the health checks generate a negligible load, but we didn't have any better ideas. So we pointed Apache Benchmark at Tinman using a tiny load of 5 parallel requests a second. The results were disheartening. Average response times for a light-weight health check were between 500ms and 1000ms. After just a short time running the tool, Tinman would die. We could now consistently kill Tinman by running Apache Benchmark with a tiny amount of load. The inevitable question came up: should we move back to Windows? To answer that question, we deployed Tinman to a Windows machine and ran the same benchmarks. It wasn't even a contest. We quadrupled the load and saw requests returned in half the time. We ran it for a long time with no hint of crashing. The decision was obvious: Tinman should run on Windows. We got spoiled by our zero-downtime deployment running on Linux, so we used Ansible to implement zero-downtime deployments on Windows. Our next post will cover how we accomplished this. A question we still ask is whether we'll try Linux again. We eventually want to run Tinman on Linux, but we have a few conditions. We won't run on Mono given its track record. We will wait for Microsoft's production-ready cross-platform CoreCLR (Release Candidate set for November 2015 ). Since we will only run on CoreCLR, we also need our 3rd-party libraries to run on CoreCLR. This will take time, so we will be running on Windows for at least another six months. Dreams are Delayed, but Never Dashed!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "flash-sprints", "author": ["\n              Michael Quinn\n            "], "link": "https://www.ctl.io/developers/blog/post/flash-sprints", "abstract": "Flash Sprints are a technique used by seasoned Agile software development teams to focus attention on solving a specific issue. They are a full Agile Sprint executed in a very compressed period of time. Flash Sprints are useful for: •    Tackling tech debt that has reached critical mass. •    Bringing a team’s cumulative talent to bear on a particularly challenging or high-value feature. •    Addressing a defect or design shortfall impacting operational stability. A secondary, but noteworthy product of a Flash Sprint is the motivating influence on team morale and camaraderie. Developers live for the joy of exercising productive creativity. The intensity of a Flash Sprint can be highly energizing for a team in a “slump” or simply bored with the predictability of a cadence. Like any Agile project, whatever problem your Flash Sprint intends to conquer must be clearly articulated and understood. This requires that expectations be realistic and achievable within the context of your team’s capabilities. Flash Sprints require intense focus, team commitment, and attention to discipline. While rigor itself is not a recipe for guaranteed success, lack of rigor is a recipe for guaranteed failure. By definition there is little margin for wasted cycles and strict time boxing is a must. All of this intensity will take a toll, so treat the team to lunch and allocate an hour to relax and decompress before starting the next sprint. Alright – lets consider a use case. Our very busy development team has been working two week sprints for the last six months. They have a good reputation for delivering first class work, but the last two releases have been problematic, contributing to a glut of technical debt, an unstable application, and a team that is exhibiting signs of burnout. As part of the remediation plan, two Flash Sprints spanning a single business day are declared. Sprint 1: There is currently no effective way to extract and correlate log records associated with a particular business transaction. This lack of traceability makes troubleshooting extremely difficult and time consuming. The goal of this Flash Sprint is to refactor the application to generate and assign unique ID’s to groups of related operations. Morning Schedule (4 hours) 08:00 – 08:15 Planning 08:15 – 11:30 Execution 11:30 – 12:00 Demo / Retro Break:\n12:00 – 13:00 Lunch Break Sprint 2: An application is leaking TCP/IP connections that result in downtime if they are not detected and remediated in a timely fashion. The goal of this Flash Sprint is two-fold. One, determine the root cause of the leak. Two, create mechanization to detect the condition and automatically recover service. The team members will be distributed across these two efforts and work in parallel. Afternoon Schedule (4 hours) 01:00 – 01:15 Planning 01:15 – 04:30 Execution 04:30 – 05:00 Demo / Retro Flash Sprints are another tactic to add to your Agile problem solving tool box. The concepts are familiar to the seasoned agile team, but executed at a highly accelerated tempo. Flash Sprints are a great way to mix things up and revitalize a dispirited team, while injecting fresh energy into the resolution of problems.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "modern-wordpress-development", "author": ["\n              Ryan Brockman, Web Applications Product Owner\n            "], "link": "https://www.ctl.io/developers/blog/post/modern-wordpress-development", "abstract": "One of the greatest things about WordPress is that just about anyone can use it to build a website or blog. A user doesn't need to know how to write code in HTML, PHP, or CSS. All it takes is some basic knowledge of SFTP and the WordPress file structure to get a site installed. On top of that, by installing pre-built themes and plugins via the WordPress dashboard, often a site can be fully customized to the user's requirements. For non-technical users, it's often this low barrier to entry they value in selecting WordPress to run their new site. And it probably goes a long way towards explaining why WordPress runs 25% of all websites . However, WordPress also offers a foundation for software developers to build custom web applications. If you want to build your own custom themes and custom plugins to extend WordPress, you are going to need a more robust toolset than just the WordPress dashboard and an FTP client. The right tools will help you be more efficient in your work, collaborate with other developers, avoid costly mistakes, and recover from errors more quickly. Below, we describe a basic toolset for WordPress development that will help you get started. And the good news is that they won't bust your budget, as they are almost all free, open-source products. Version control tools track changes to the files that make up your WordPress site, and allow you to save versions of those files. Not happy with that change you just made to a CSS or PHP file on your site? A version control tool will allow you to undo that change and restore your site back to its previous state. Having trouble remembering what changes you made to a client's site six months ago? A version control tool has a complete history of those changes, along with a record of who made them. There are lots of different version control tools to choose from. One of the most popular tools is Git . Git originated in 2005 with the developers of the Linux kernel to support the open-source development of Linux. They created Git in an effort to provide a tool that was fast, efficient with large projects, and could handle a globally-distributed community of developers. Luckily for the rest of us, the tool they ended up creating also works incredibly well with WordPress. A full-fledged education in Git is beyond the scope of this article, but here are some great resources to help get you started: You can use pretty much any text editor to edit the source files within WordPress. There are many free ones available for download or that come bundled with your computer, such as TextEdit for Mac OS X. However, if you are doing more than just editing a few lines of code, you might want to consider a more robust commercial text editor made for software development. These editors include a lot of \"developer friendly\" features like code completion, coding standards and style checkers, and integrated debuggers that can help save time during development, and improve the quality of your WordPress code. Resources: If you are doing custom WordPress development, it's a best practice to avoid developing directly on your Production server. Instead, you can create a local development environment that runs WordPress on your personal computer. This prevents site visitors from seeing your development work in progress, and also protects against bugs making their way into your live production site (where they can have much more severe consequences). To make sure you catch as many bugs as possible while testing locally, it's important that your development environment matches production as closely as possible. This is called Dev-Prod Parity , and it will save you many headaches when it's time to deploy your application to Production.  One way to achieve this is to use a tool called Vagrant which runs inside a virtualized environment called VirtualBox . See our KB article for more information on how to get started using Vagrant. If you prefer to use a different local development tool, such as MAMP or Desktop Server , that's fine too. Just keep in mind that there may be differences between how those environments are configured and how our Production setup is configured, which can complicate your testing efforts. They are also not integrated with CenturyLink WordPress, so additional configuration may be required in order to successfully run WordPress. Resources: Finally, there is going to come a time when you are doing custom development and you need access to the WordPress database . Maybe you are digging into the standard WordPress tables to work with the data, or adding your own custom tables and fields to work within a plugin. You're going to need a SQL client that connects easily and securely to both your local database (in your local dev environment) as well as your Production database. You can use a tool like MySQL Workbench , which is free to download and offers a robust set of features. Our KB article describes how to get connected.  Alternatively, if you are using a premium text editor such as Coda or PHPStorm your editor may have a SQL client embedded within it that provides the same features. Resources: WordPress is the most popular CMS platform in the world for developing websites. If you want to create a site to showcase your blogs, Ecommerce, online community, school, or  business, WordPress makes the process fast and seamless for all levels of technical depth.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "rigorous-qa-of-marketplace-offerings", "author": ["\n              ", "\n            "], "link": "https://www.ctl.io/developers/blog/post/rigorous-qa-of-marketplace-offerings", "abstract": "Any public marketplace runs the risk of becoming an unpredictable wasteland to consumers. For example, consider eBay, Alibaba, or any other marketplace specifically designed to support Cloud Service providers. Given enough time and no supervision, all can revert to their natural state of chaos. In the cloud space this often resembles: With so much volume, abandonware quickly becomes the norm. In the CenturyLink Cloud Marketplace we've taken a different approach. Our hosting customers have always trusted the products and vendors we make available throughout our data center and cloud products, and a marketplace with third party offerings should be no different. To that end our goal is, and will continue to be: all integrations must pass a three-way test. CenturyLink's Marketplace Quality Team set out to develop a tool set to support this three-way test at scale. The end product was named BP Keeper to remind us of our ultimate goal - to build and keep customer trust throughout the Blueprint -based marketplace. This impressive tool set was started by building on top of existing tools like bpformation , which provides programmatic access to packages and blueprints. We combined this with the mature Python SDK for direct access into the CenturyLink Cloud IaaS. The testing process unfolds like this (with an emphasis on visual ops to keep the Quality team aware of the entire testing landscape): Automatically identify new public Blueprints. To keep abreast of changes the team's Slack channel is messaged in real time. Discovered Blueprints are run through an analysis funnel, which identifies what execution parameters are needed for a successful deployment. Many of these are automatically configured based on standard naming conventions while others are bubbled up to the Quality Team for rapid review prior to initial execution. If for any reason testing is paused, the Quality team is immediately notified. When a failure occurs the team is tagged on a message with specific error details and deep links to the individual deploy log, as well as the ongoing history and trend for the package in question. Anything above a 4% error threshold is escalated for remediation and removed from public view until testing again indicates success. This process - continuous testing of all Blueprint assets within the CenturyLink Cloud Marketplace - is no doubt resource-intensive, as shown by one of our execution summary messages below. That said, we've made a conscience choice as an organization to define and honor the three-way test. By that active effort our Marketplace will remain a trusted source for technologies that work with CenturyLink Cloud. @KeithResar believes success comes from Partners + Integration + Developer Engagement. He achieves this through developing partnerships and technology integrations between Software and Services organizations, and the CenturyLink Platform.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "using-bpformation-to-pipeline-your-automation", "author": ["\n              ", "\n            "], "link": "https://www.ctl.io/developers/blog/post/using-bpformation-to-pipeline-your-automation", "abstract": "Over the years we've seen hundreds of software integrations into the CenturyLink Cloud. Many introduced new challenges to us (arghh!), especially as we all work to follow industry-wide best practices. I am pleased to report that with increased volume came a common set of solutions that support these best practices and virtually eliminate challenges faced by end customers and partner integrators alike. Using the bpformation toolset a factory approach is achievable for even the smallest integration effort. Blueprints are an amazing tool for all consumers of the CenturyLink Cloud Infrastructure-as-a-Service (IaaS). True - by definition a Cloud vendor must maintain a set of orchestration that enables instant provisioning of resources like new servers or storage. CenturyLink is no different in this respect. What sets CenturyLink apart is extending this orchestration, called Blueprints , directly to end customers and service integrators. Long ago we realized that just deploying a server is of limited value. It's enabling a new service or business process through the development or installation of software that customers really want. By extending Blueprints, our end customers deploy a fully functioning service (which may be composed of multiple servers and applications) repeatably, consistently, all at the press of a single button. While great in theory, we found that many of the Blueprints are missing some key controls that have come to be regarded for good reason as commonplace: We all know when things aren't going smoothly. These are the times to lean in and align what's happening now with best practices across the industry as a whole or even from a specific Center of Excellence. To sum up a few of the commonalities: Achieving any of these using a hand-administered web interface is impossible. The solution we've implemented relies on the bpformation toolset. This is a  command line interface (CLI) with available Python extensions that allows create, read, update, and delete (CRUD) operations against Blueprints from a command line or programmatic endpoint. Using bpformation we can: Blueprint workflows are represented using JSON objects that can be clearly read, easily modified, and, being text-based, are natively supported by version control like Git. Use the bpformation command line upload-and-public to transfer a Blueprint Package to CenturyLink Cloud and publish for general availability. Very easily! Let's assume we have our credentials for a development, QA, and production environments. At CenturyLink Cloud in the Marketplace Provider Program we've found uses across the board -- from enabling a single engineer to build and maintain the integrations for a dozen products to supported automated QA and publishing of all partner provided Blueprints prior to public release.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "five-open-source-discussion-forums", "author": ["\n              Jessica Weber\n            "], "link": "https://www.ctl.io/developers/blog/post/five-open-source-discussion-forums", "abstract": "Open-source discussion forums and software. You may not think much about them until suddenly you're stuck and need an answer fast -- then, they might be all you can think about. These forums can have many forms. Some may be a simple board where a group of people post and track discussions. Others can be an entire website set up for enterprise communication and management. So what edge do these forums have over, for example, a discussion page hosted on a larger website? They are open-source, so they can be customized for the personal needs of the group using them. They provide private access - only members of the forum can contribute to the discussion. They can be set up to contain and track all conversations, for record-keeping, efficiency, etc. In an effort to provide as many services as possible, we've teamed up with five different open-source forums to create Blueprints within the CenturyLink Cloud Ecosystem. These Blueprints can be deployed to any cloud server environment with little effort on the part of the customer (you). Discourse , which provides a free 14-day trial, is organized with continuity and simplicity in mind. This API-based, JavaScript site updates conversations in real-time and displays all conversations as a continuous page. The continuous display allows users to view older conversations by scrolling down the page instead of clicking \"next page\" buttons. Users can also expand and retract the conversations on the board, which is helpful when trying to navigate different conversations. Notifications can also be enabled so that users receive alerts for, among other things, posts that mention them. Development for the site is done within Github and programmers wishing to contribute to the project are encouraged to fork the project here . This website is unique in that it was designed for a mobile screen, meaning you can view it comfortably on a mobile device without installing an app. Users seem to like the real-time integration of the site. Replies can be typed while a user views the conversation and receives updates from other users. All replies are also saved immediately - you can start typing a reply on one device and switch devices to finish it without missing a beat. MyBB is appealing for more reasons than just its price (free). This Linux-configured online forum has benefits for everyone, from the technologically challenged to seasoned developers. It provides features such as mass mailing capabilities, calendars, and extensive documentation and support for those working with their product. This site is very customizable, offering numerous plugins, templates, and themes. It was designed to be easy-to-use for everyone, so those with little to no development experience don't have to worry about a complicated set-up. However, their design also appeals to developers and programmers; they provide access to their code in github and allow free-reign of the development process (on your own fork, of course). This includes creating customized plugins over and above what has already been developed. Developers also have access to development forums, an IRC channel, and a lot of community-sourced documentation. As you might be able to derive from the name, phpBB is written in the PHP language and is adaptable to UTF-8, which makes it compatible with almost any other programming language. The site also provides a good amount of plugins, with the option to create custom ones with very little effort or code changing. The site organization is based around the different forums users create. There are also an unlimited number of subforums available in order to create more organization. The options for notifications seem to be robust and easy to deploy. Some of the more unusual features available within the forums are the ability to bookmark topics, the ability to display the most active topics at the top of the page, and the ability to create password-protected private forums. This site is structured around the ability to customize just about everything. There are even options to create different styles and template within each forum. If a user doesn't like any of the styles available to them, they can also create their own custom templates. OpenAtrium , also built and hosted on the Linux platform, provides a very developer-friendly environment that has an uncommon feature. Users are given a development sandbox where they can test updates and changes before releasing them to the environment. Drupal is the driving force behind this platform, and developers are encouraged to set up a Drupal environment and collaborate with other developers for fixes and upgrades. For those not in the development arena, there is also an option to submit issues to the development community. OpenAtrium was developed by the Drupal team, so all of the supporting docs and discussions are integrated into the Drupal community. That means you have access to a robust development community. The organization of the site is based around the idea of related microsites that build together to form a whole system. The site also provides the ability to develop a workflow and track documents in various stages. Notifications and the ability to send mass group emails also add to the functionality of the site. OSQA is an acronym that describes the main purpose of the site - Open Source Question and Answer. This totally free site hosted on the Linux platform is best suited for entry-level, small traffic sites with limited requirements. For users who decide they want more, there is also an option to upgrade to a paid version, called AnswerHub . This site was developed in the Python programming language. The developers of QSQA and AnswerHub have fully immersed themselves in the environments - all supporting documentation, wiki, and discussions are done using their own product. Developers provided a unique feature to this site when  they created a \"bootstrap\" mode. In this mode, newly-created communities allow posts, comments, votes, etc. Once a user base is developed, community members then have the option to impose restrictions on content and contributors. On top of this, the developers have made this site easy to integrate - users can login using Facebook or Twitter, by creating a username and password, or by using OpenID , which allows access without creating a specific username and password for their site. Users can also opt for a variety of notification processes and options for group email distributions. In addition to the unique features I listed in the above reviews, I also found a few traits that were similar across all the online forums. Some of them are features such as: the ability to set control roles for administrators or moderators, the ability to control who can join (and revoke privileges when necessary), search functionality, private message functionality, and the ability for users or administrators to \"rate\" other posters based on their content or behavior. At CenturyLink Cloud, we've worked hard to integrate these forums into our Blueprint offerings. Check out these and other open-source Blueprints available on our cloud platform .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "continued-development-part-1-websites", "author": ["\n              Chad Middleton\n            "], "link": "https://www.ctl.io/developers/blog/post/continued-development-part-1-websites", "abstract": "Why learn anything new? In the world of software development, a new tool, language, or version releases constantly. The ability to be agile in an Agile environment is crucial. One day you may be writing in Java and the next day in Ruby. There are many different approaches to stay up-to-date or learn something new. Things like open-source projects, free tools, training websites, and user groups are available to anyone who wants to learn. Just about every tool or language has a documentation section: Oracle, Node, Spring, etc. These documents can give the user a solid base of understanding. Most of the main tools used in coding have a simple programming example that consists of the \"Hello World!\" function. A good practice to increase knowledge of whichever tool you want to use is to implement a few different projects using the examples. Creating a list of 10 simple projects to code in every language can be very helpful to getting your initial understanding to a certain level. There are many different types of technologies out there, and thankfully there are a few that offer a significant amount of training for free. From databases, to websites, to going back to school, there are a lot of choices for learning. The shift towards open-source projects has granted users the ability to analyze working code. This gives people the opportunity to see an actual, functional application. By contributing to open-source projects, you can get a better understanding of the code. GitHub offers a section dedicated to users who want to look through or contribute to source code. If a developer wants to follow and contribute to a popular repository, the trending section gives them those projects. The explore section offers a list of different projects, allowing users to investigate other coding projects until they find one that intrigues them. If a software developer wants to take a more hands-on approach to learning, there are multiple websites that contain rankings. All of these are free as well! There are, of course, many more sites for continued learning. A simple Google search will easily reveal plenty of good options. Good use of a search is a weapon to be honed. If you don't see a site that you know and love listed here, please leave a comment with the web address and a quick message about why you like the site!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "aspnet5-on-windows", "author": ["\n              Nick Stetich\n            "], "link": "https://www.ctl.io/developers/blog/post/aspnet5-on-windows", "abstract": "In our last post from this series, we talked about how our dreams of running ASP.NET v5 in Linux seemed like they had become reality, until we discovered that Mono was not stable enough to support our application. As it turned out, the shortest route to getting a stable environment was to host the application in Windows instead. We had already written Ansible playbooks to deploy the Tinman application in Docker on Linux, but now our deployment story was changing completely. Fortunately, while Ansible was originally written to run over SSH on UNIX-like systems, it also supports a growing subset of functionality for Windows systems. Instead of communicating over SSH, for Windows remote hosts, Ansible uses Powershell remoting. There are also modules developed specifically for Windows . In order to support being controlled remotely by an Ansible agent, we had to first set up some prerequisites on our servers. After some trial and error, we found that the Ansible team actually provides a setup script that ended up working well for us. It allows remote access to WinRM with reasonable security defaults. Although there aren't as many modules for Windows as there are for Linux systems, we were able to get significant functionality between the following three modules: With this functionality, even if there wasn't a module to do exactly what we wanted, we could write our own Powershell scripts that would perform any of the logic that would be part of our deployment. As we mentioned in earlier posts , when we developed Tinman to run in Docker on Linux, we were using NGINX to route requests to the docker container. This allowed us to have multiple versions of our application running on a single machine, to give longer-running processes time to finish while also granting us the control to route API requests only to the latest version of our code. To turn our self-hosted dnx process into a service, we used NSSM . Starting in Ansible 2.0, support for NSSM comes as a built-in module, win_nssm . Because we were writing our deployment for Ansible 1.9, we controlled NSSM by issuing commands via the raw module. We created a batch file run.cmd that's parameterized using the win_template module. Then we told NSSM to run that batch file to start our application. NGINX support is better for Linux than it is for Windows, so it wasn't as attractive an option once we made the switch. Instead, we were inspired by a blog post and guided in our implementation by another blog post to use IIS as a reverse proxy to direct requests to our self-hosted application, thus using the URL Rewrite and Application Request Routing IIS modules. When a new version is deployed we configure IIS to direct requests to the new version instead of the old one by using the Set-WebConfigurationProperty commandlet. To maintain zero downtime, we: Because our deployment requires zero downtime, we can deploy with confidence that our customers will not experience any interruption. Deploying frequently as well as deploying during normal working hours, therefore, became much easier. We hope that one day we can return to a Linux-hosted solution as more of our third-party dependencies are made compatible with CoreCLR. In the meantime, we now have a Windows-hosted web service that reliably deploys in a few minutes. Running on Windows, we've found developing ASP.NET 5 to be a great experience.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "all-about-haskell", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/all-about-haskell", "abstract": "One of the key components of the CenturyLink Cloud Developer Center is sharing knowledge. In keeping with that spirit, we are embarking on a series around programming languages. Throughout the series we will be reaching out to Devs, Coders, and Engineers to find out what their favorite languages are, why they favor them over others, and identifying some use cases for each. Our first language in the series is Haskell . Haskell is a lazily-evaluated pure functional programming language named after mathematician Haskell Curry. Haskell is in the same family as Standard ML, OCaml, and F#, and has influenced many other languages. It has been in active community development for over 20 years and supports a large community of academic researchers and computer science professionals, who continue to iterate and improve on the language. Haskell offers a number of advantages for both researchers and software development professionals. The strength of the Haskell-type system, wide availability of libraries, and the fact that it compiles to native code makes Haskell suitable for developing high-performance applications where stability and correctness are important. The advanced type system offered by Haskell, in conjunction with a number of libraries and language extensions, makes Haskell a practical method for research and prototyping systems and generating proofs of correctness. Pros: • Haskell offers a rich type system that helps developers ensure the correctness of their applications. Programs that compile in Haskell have a very high likelihood of operating with fewer errors than applications in other languages. • Haskell is highly-expressive and allows developers to get more done with less code. • Haskell compiles to native code and can achieve performance comparable to C or C++. • Haskell possesses a large number of available libraries for many different tasks. • Haskell supports programming with abstract mathematical concepts (e.g. monads, functors, combinators, GADT, etc.). Cons • Lazy evaluation can make it difficult to reason about the performance characteristics of a specific program. • Many resources available for Haskell developers pre-suppose a high level of familiarity with category and type theoretical concepts that may be new to developers outside of the functional programming ecosystem. • The rapid release schedule of GHC and associated libraries means that long-term maintenance of applications can be difficult without external tools. The Haskell ecosystem has a rich and active community of contributors ranging from individual developers, academic institutions, and commercial entities that support Haskell development. There are many books, papers, videos, and articles on the Haskell wiki and elsewhere to aid a Developer in getting started with Haskell. If you are looking to learn Haskell, joining a Haskell Meetup is a great way to connect with Haskell fanatics in your area. At CenturyLink Cloud , we host the Saint Louis Haskell User Group at our St. Louis Developer Center. Stay tuned for more on our language series and sign up below to keep up to date on everything \"CODE\" with our Developer-focused newsletter.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "continued-development", "author": ["\n              Chad Middleton\n            "], "link": "https://www.ctl.io/developers/blog/post/continued-development", "abstract": "In Part 1 on Continued Development, we highlighted the use of various websites to help learn more about software development, new tools and languages, and version releases -- all of which are beneficial. In an effort to help users stay up-to-date while learning something new, we focused on available open-source projects, free tools, training websites, and user groups. If you've ever hit a wall in your learning and can't find your answer on Stack Overflow , you may have thought, \"There has to be a better way.\" Going to the Internet for everything is more than likely what every aspiring or current developer does. Problems can arise where the solution you are looking for is presented in a way you have no experience with, is solved six or more different ways, or there is no solution at all. Copying and pasting code you don't understand seems fast and easy, but that process can lead to missing testing, extra dependencies, or just misunderstood code. In this post I will give you multiple tools to use, both to help with issues you have collided with or simply increasing your knowledge. By taking advantage of tools such as books, mentors, or user groups you can achieve a better understanding of programming, techniques, and solve issues that others have already experienced. One of the current patterns in programing is that of \"Pair Programming\" . Pair programming is a style of development that has grown out of the Agile transformation of businesses. This allows a more junior developer to code with a more senior developer. Those in opposition to this technique usually state that the developers are working at half the speed. This mindset doesn't take into account the many advantages of the method; specifically for this blog post is the learning aspect. Being in the driver seat and actually typing out the code is a hands-on approach to learning, even if you do not understand all of what you're writing. Not fully understanding what is being written also will drive the less senior developer to open a dialog and ask questions about design pattern, implementation, or methods the junior developer has never encountered, as well as making the senior developer think about why that implementation is the best. Being able to process what you are being told into the actual code is a training aspect that is otherwise very difficult to achieve alone. User groups for different technologies are popping up everywhere. With different and relevant topics for each event, user groups serve many great functions. If you have a current job which requires you to use older tools, meet-ups offer a look at integrating new techniques with those tools or introduce new tools that could replace the older ones. You can also expand your network around the local area and open up options for the future. Meet-ups are another great opportunity to meet and perhaps identify a mentor among the programmers attending the meet up. Currently, publishers like O'Reilly offer a plentiful selection for programming. Books offer the ability to learn something new or gain expertise with a known skill-set. If you're just starting out, using the Head First series or the For Dummies series can gain a quick and easy base. These books generally contain multiple projects that you create to learn the lessons with a parallel hands-on approach. In addition to using books, utilizing or finding something like a book club of peers who are also reading the same book can be very helpful as well. Talking about topical books with others can spark ideas, present a different perspective, and answer questions from the book.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "traditional-vs-paas-hosting", "author": ["\n              \n            "], "link": "https://www.ctl.io/developers/blog/post/traditional-vs-paas-hosting", "abstract": "Comparing a platform-as-a-service (PaaS) to traditional hosting like VPS/shared hosting (e.g. DreamHost, Host Gator, GoDaddy) or infrastructure-as-a-service hosting (Iaas) (e.g. Amazon Web Services, Linode, CenturyLink Cloud is like comparing apples and oranges. One must look beyond hardware and price to get a true cost/value when picking a hosting provider. Shopping for traditional hosting is too much like shopping for breakfast cereal: many mediocre options, little differentiation, annoyances for up-sell. Traditional hosting… (aka “do it  yourself”) With traditional hosting developers have many responsibilities before they can even touch a line of code. Lets look at some of these responsibilities… As you can see, before you get to the code, you’ll have to spend hours getting your production environment in a state which is just barely good enough to host your application. If you want your application to be reliable, scalable, and resilient against various failures, you’ll have to deal with additional issues like setting up monitoring, alerting, load balancers, replication of application across multiple servers, deployment scripts, etc. None of these are trivial. Developers write code, not manage servers; why should they spend all this time managing servers?! PaaS hosting… As a developer your core competency is writing code, not managing servers. Enter the world of PaaS. With PaaS you get a whole lot more “out-of-the-box” than you could from a traditional hosting provider; here are just a few… Conclusion… If you need a fast, reliable and scalable host for your web application so you can write some code and you don’t want to deal with managing servers, then getting a PaaS is the answer. If you are confident at setting up servers, you love doing it, and you rather do that than writing code, than a traditional host is a better option.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "build-a-2-container-app-with-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/build-a-2-container-app-with-docker/", "abstract": "Docker is fundamentally changing the way people do infrastructure in the cloud. However many developers I know are still confused about how to use it to do anything beyond a simple \"Hello World\" or a WordPress-all-in-one container with everything bundled into a single container, Apache, MySQL, nginx, etc. The power of Docker is in being able to ship and scale individual components or \"containers\". However it is still a bit of a black art about how that works. There are a few awesome projects being started to make it easier to build multi-container apps like CoreOS , Serf , Flynn , Maestro , Deis , and Dokku . But what if you want to do it without any fancy tools? How do you do it from scratch? I am not going to say this is the best way, but I would like to walk you through how I recently built a 2-Container app (WordPress in one container and MySQL in the other). In future articles, we will go into more complex setups and walk you through how to use some of the cool Open Source tools out there. This tutorial will guide you through 5 somewhat easy steps ( Panamax is a new project we are working on to make this kind of tutorial unnecessary, you will be able to stitch together Docker containers in a simple web UI). First, setup docker on your machine. If you are on a Mac, this is a simple 3-step process: boot2docker which will install Docker on your Mac. Now that you have docker running on your development machine, let's do something interesting. Let's create your first single-container application. First, you go to the public Docker Index to find a container you want to use. Let's search for WordPress. You will find about 30 results, but you only want to use a Trusted Repository. NOTE: Most of the docker containers in the Docker Index are unsafe to use as-is. You have no idea how they were built, so you can't be sure there isn't malware on the images. If you want to use any public Docker images as a starting point, make sure to use a \"Trusted Repository\" and make sure to audit it's Dockerfile before you use it. One of the good trusted WordPress containers out there is managed by Tutum , a Docker-based hosting company that lets you run Docker containers and provides private docker registries and private docker images. The WordPress container you want to use is called tutum/wordpress and you can use it quickly and easily by doing the following: Congratulations! You are now running WordPress in a single container with MySQL on the same container. Easy right? Want to see WordPress running for yourself? Run docker-osx ssh , and then run curl 0.0.0.0:49163/readme.html Now how do you get MySQL to run in its own container? Now that you can use existing \"Trusted Images\", let me show you how to modify them. Instead of running in daemon mode with the -d flag, use the interactive and TTY flags ( -i -t ) and specify bash to get into the container as root. Ever wonder what /run.sh did in step 2's run of the docker command? Let's find out. Now that we don't want MySQL running on the same container as WordPress, let's modify this container to not run MySQL. In a new Terminal window, run docker ps again to find the container id of your interactive container. Now that we have the container id (dfa24049c264) we can commit our changes to the filesystem. Now we have our own version of tutum's WordPress with MySQL disabled. Let's try it out. First we will stop our other containers and then we will start a new one with our newly committed image, myuser/wordpress . Notice that the 3306 port does not show as open in the new container when it did in the previous one, but that you can still curl the url curl 0.0.0.0:49164/readme.html . Just like WordPress, you can get yourself a MySQL container from the Docker Index. Again, let's go in and check out what run.sh does. Look familiar? It should, the same tutum folks made this image. We don't need to remove any components out of this container, but we do want to make sure that the root username and password do not change. If you go back to your OS X terminal, we can find out what username and password were generated and commit the new container locally. You can now connect to this MySQL Server using: mysql -uadmin -pqa1N76pWAri9 -h -P Please remember to change the above password as soon as possible! OK now we know what the username and password are and have created our own container with a static root username and password. Let's stop the generic container and start our new custom MySQL server container. Congratulations, you have reached the last step! All you have to do now is bind the two containers together. In order for the two containers to know about each other, we will need to link them using the docker -link flag. Before we do that, we will need to make another modification to our WordPress container so that it can pull credentials from environment variables. Now you go back to another OS X terminal (keeping the bash container running) and commit these changes to the container. Finally, it is time to start the myuser/wordpress container with a special -link flag and the environmental variable for the MySQL DB password. We now have a 2-container setup that are bound together. To see it in action, go to: We can now create a 2-container application in Docker without even having to touch a Dockerfile. We can tie the containers together in a loosely coupled way. You can create multiple instances of the WordPress container that all talk to the same MySQL database. Note that this is not a production environment yet. The MySQL container's data is ephemeral and we did not cover how to persist it yet (hint: look at the docker run -v flag). Also note that in production we would want to docker push our myuser/wordpress and myuser/mysql images to a private Docker registry. We will show you how to run your own registry in an upcoming tutorial. In the next installment next week, we will talk about adding an nginx container to load balance and using Serf to automatically make all of the containers aware of each other. To automatically get an email when the next installment comes out, subscribe to our weekly newsletter that has Docker tips and news.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "building-complex-apps-for-docker-on-coreos-and-fig", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/building-complex-apps-for-docker-on-coreos-and-fig/", "abstract": "For Docker deployments, CoreOS is powerful but can be complex to setup. Fig is simple, but hard to scale to multiple servers. This blog post will show you how to bridge the gap between building complex multi-container apps using Fig and deploying those applications into a production CoreOS system. In last week's blog post, we covered building a 4-container app in Fig. To cut to the chase, here is the fig.yml that gets you a 4-container app: To show this 4-container system in action, you simply run fig up -d (which is the same command used to restart the running system). As you can see, Fig is super easy to get started with and use, but the #1 question people had was how do you scale this to multiple servers? CoreOS is not the easiest or most approachable system in the world. Understanding how to write systemd config files and getting everything configured just right can be time consuming and frustrating. Which is why I wrote a fig2coreos gem (github.com/centurylinklabs/fig2coreos), which converts fig.yml to CoreOS formatted systemd configuration files automatically. For some reason (I am not sure why yet), the default version of CoreOS shipped when you vagrant up is not the latest version of coreos, which means it does not have Fleet installed and has an old version of Docker (0.7.2). Once the vagrant coreos box stays up and running long enough to download the latest version of coreos, you can apply the update by either running sudo reboot in the coreos image, or by running vagrant reload --provision in the generated directory. The fig2coreos command parses your fig.yml file and generates a bunch of systemd config files. You can see the files in the directory that is created. Which looks complicated, but is the CoreOS version of this part of your fig.yml file: In addition to the start script for MySQL, there is also an etcd auto-registry script\nvia systemd in the db-discovery.1.service file. This registers the service into the etcd key/value store. In this example Fig application, we used Serf to make the Docker containers self-aware, but etcd is the standard CoreOS alternative to Serf available in all CoreOS containers. Do you need both Serf and etcd? No. The biggest difference between the two is where the daemons are run. In etcd, the etcd daemons are run outside of the Docker containers and it is up to a system like systemd to keep the etcd key/value pairs up to date. In Serf, the serf daemons are run inside the Docker containers, so when a container dies or goes offline, the peers of that container figure it out automatically. These are two very different philosophies for configuration and system management, but in the end result is essentially the same. Both Serf and etcd have the same ability to let Docker apps become more aware of their containers and the roles their containers play and interconnect. You can (in theory) use Fig on each one of your servers, each running independent Docker daemons. However linking containers becomes more complicated and network issues make it non-trivial, even with Serf. I am getting closer to showing how to deploy multi-host Docker in this series of blog posts, but this post won't show you exactly how just yet. Subscribe to get weekly updates and we will get there in due time. In the meantime, so that you know that I am not lying to you, watch this 2-minute YouTube Video about CoreOS Fleet to see an example of a real multi-host CoreOS deployment. You can see the power of CoreOS and Fleet in this simple video, but it doesn't connect the dots of how to put all the components together to re-create the demo. We are getting very close to showing you how to deploy production multi-host Docker apps in a cloud environment. Each week, I am getting you closer and closer to the answer by introducing slightly more complexity into the example. CoreOS and Fig are just two Docker technologies being built to manage complex applications. In the future we will show you other ways you may want to accomplish this same task with tools like Flynn and Deis .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "auto-loadbalancing-with-fig-haproxy-and-serf", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/auto-loadbalancing-with-fig-haproxy-and-serf/", "abstract": "In this series of blog posts, we have progressively built increasingly complex Docker-based applications. We started with a simple 2-container Docker app . Then we built a 3-container decentralized Docker app with Serf . ( Edit: Fig was purchased by Docker in mid-2014, and renamed Docker Compose. As such, all Fig references now apply to Docker Compose.) Today we are building a 4-container Docker app that uses Fig , HAProxy and Serf . When you start building apps this complex with Docker, maintaining the state of your containers can become very difficult. There are multiple ways to manage this complexity, but this we will cover just one: Fig from the makers of Orchard . Fig is a great way to keep all the state of your complex application in one place. Let's look at a fig configuration that is equivalent to the 2-container system we built in our first blog post . And to run this fig.yml file, you put it in your current working directory and run: Adding Serf is a great way to get your containers to auto-discover each other. Let's look at a fig configuration that is equivalent to the 3-container system we built in our second blog post . And to run this fig.yml file, you put it in your current working directory and run: Fig can automatically build the links between Docker containers and save you the time and hassle of writing docker run -link commands yourself. However Fig can not talk to the containers themselves, so if you add a new web server via Fig's scale command (e.g., fig scale web=3 ), you would need to restart any load balancers to take advantage of the new environmental variables which could mean downtime for your users. This is where Serf combined with Fig shines. The most powerful part of Serf are the event hooks, and the previous examples did not show off this power. To see the power of Serf's event hooks, you can create an haproxy container. Luckily I have created a ctlc/haproxy-serf Trusted Build Image with Serf built-in for you to use. This image has a different serf agent call than the other images. For reference, let's first look at the serf agent call in the WordPress container. You can see it on GitHub if you want to. Ok, simple enough. So then what does the haproxy serf look like? You can check it out on GitHub but here it is if you don't want to click: Clearly a lot more going on. Let's break it down. First let's look at serf-member-join.sh : This bash script will be called every time a new Serf agent is added to the network. The script looks for only the web servers and adds them to the haproxy config file. Then at the end, it gracefully restarts haproxy with no downtime. Cool, right? I won't paste the code for serf-member-leave.sh , but it un-does the above work. Now let's add haproxy to the fig.yml . Notice something interesting here, I actually replaced the DB image from ctlc/mysql-serf to ctlc/mysql and the app still works. I did this to make a point. The ctlc/mysql container doesn't have a Serf agent in it like ctlc/mysql-serf does, so in this example we use a hybrid of Serf detection for the load balancing and Docker linking for the database connection string. The upside to using the ctlc/mysql container is that you can set the MySQL database and root password through environmental variables instead of having to build a custom local image that has a random password assigned. To show this 4-container system in action, you simply run fig up -d (which is the same command used to restart the running system). Sticking HAProxy, Apache and MySQL in a single Docker container might be the easiest way to get a site up and running quickly, but it doesn't scale. Getting a Docker-based system to scale is not rocket science, but takes some careful thought and attention. The projects and tools being built around Docker today can make your life a lot easier if you know how to use them correctly. In the coming weeks we will look at multi-host Docker solutions and alternatives to Fig and Serf like CoreOS .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "decentralizing-docker-how-to-use-serf-with-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/decentralizing-docker-how-to-use-serf-with-docker/", "abstract": "In last week's post, we talked about how to create a 2-container application with Docker . Now we want to use a brand new project called Serf (from the makers of Vagrant) to have the containers learn about each other automatically. Serf uses gossip-based membership to create a decentralized solution for service discovery and orchestration that is lightweight, highly available, and fault tolerant. Sounds awesome, but how do you use it with Docker? First, let's setup serf agent on a Docker container. Well that was easy! Now let's try to connect to our serf agent from our Docker host machine to make sure it is working. Now that we have serf available on your host, you can connect to your Docker container's serf agent to see it in action. Sweet. We now have a decentralized solution for Docker containers with a mini two-system serf cluster. The next step is to launch a MySQL database Docker container with serf built-in. Conveniently, you can find a Trusted Image on the Docker Index called ctlc/mysql-serf . The final step is to launch a WordPress Docker container that has serf built-in. Luckily, I have built one already called ctlc/wordpress-serf and link it to the serf container we already named \"serf\". Congratulations! We now have created a distributed Docker container system that is self-aware. Every container can run docker members and get a list of the other members in the system and what their roles are. We can even see WordPress in action by curling the website. How does this work? By a special line in wp-config.php that makes the magic happen. Notice that the serf members -tag role=mysql call will automatically find the correct IP address for MySQL. Before we finish this week's blog post, let's start scaling out our WordPress by adding more instances of the ctlc/wordpress-serf container. You can see how easy it is to create a distributed application with Docker and Serf together, but this does not yet show the power of Serf. In next week's column , we will add an Nginx Docker container that will automatically detect when a new WordPress container comes into existence and HUP nginx to take advantage of it using serf event hooks.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "the-easy-way-to-deploy-java-tomcat-with-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/the-easy-way-to-deploy-java-tomcat-with-docker/", "abstract": "If you are trying to deploy a Java Tomcat app on a Docker system, you might find other tutorials which will take you hours of heartache to get just right. How about deploying a full custom Tomcat app in just 30 seconds? Don't believe me? Just try these 10 simple steps: The basic Heroku Java buildpack doesn't work with every Java app. If you run it from the jesperfj/webapp-with-jndi base directory it will fail. But if you use the jesperfj/buildpack-tomcat custom buildpack within the app's directory, everything works smoothly. This is just a simple example of how much better the developer tooling around Docker has become over the last year. With Docker 1.0 just around the corner, there has never been a better time to start incorporating it into your daily workflow. If you are ready to get your feet wet with CoreOS, try our Building Your First App on CoreOS tutorial.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "wordpress-in-a-docker-container-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/wordpress-in-a-docker-container-on-the-centurylink-cloud/", "abstract": "Installing and playing with Docker on an Ubuntu based server is interesting, but what we really want is to install and run containerized applications. A good example application is WordPress, as it consists of a web front-end backed by a MySQL database. First make sure you have your CenturyLink Cloud Server setup correctly with Docker installed as per this tutorial . Next go to the public Docker Index to find a container you want to use. Search for WordPress and you will find about 30 results. For the most part you only want to use a Trusted Repository. NOTE: Most of the docker containers in the Docker Index are unsafe to use as-is. You have no idea how they were built, so you can't be sure there isn't malware on the images. If you want to use any public Docker images as a starting point, make sure to use a \"Trusted Repository\" ( read more about trusted builds and make sure to audit it's Dockerfile before you use it. One of the good trusted WordPress containers out there is managed by Tutum , a Docker-based hosting company that lets you run Docker containers and provides private docker registries and private docker images. The WordPress container you want to use is called tutum/wordpress and you can use it quickly and easily by doing the following: And that is everything you need to know to set up Wordpress in a single Docker container. Just to ensure that everything is as it should be let's actually bring up WordPress. In order to do that we need to know the port that Docker has assigned to the application. Make a note of the returned port (e.g., 0.0.0.0:port number ). In the example above the returned port is 49154 and that is where WordPress is listening. Validate by pointing your browser at http://public ip:port number. This will bring up the Wordpress installation dialog and everything is good to go. This is a framework for downloading and installing containerized applications that have been designed to run everything in one container. Check out our Kubernetes Knowledge Base article. It will get you started using Ansible to create a Kubernetes cluster on CenturyLink Cloud - all by running a single script. Don't have a CenturyLink account? No problem. Just head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "flynn-vs-deis-the-tale-of-two-docker-micro-paas-technologies", "author": ["\n              Lucas Perkins\n            "], "link": "https://www.ctl.io/developers/blog/post/flynn-vs-deis-the-tale-of-two-docker-micro-paas-technologies/", "abstract": "Flynn and Deis are two Docker-related open source projects that I find incredibly promising, so much so that I included both of them in my list of the Top 10 open-source Docker projects at numbers 1 and 2, respectively, and I stand by that decision. What I love about both Flynn and Deis is that they're highly ambitious. Both openly describe themselves as PaaS platforms, but it's clear that they're not just taking the old PaaS paradigm and throwing Docker containerization into the mix. Instead, they're seeking to redefine the meaning of PaaS. Just as Sinatra redefined the idea of web frameworks by creating a micro-web framework, Flynn and Deis have created a micro-PaaS concept where anyone can run their own PaaS on their own hardware with very little complicated overhead. While this list is likely far from comprehensive, I take the following to be fundamental similarities between Flynn and Deis: There are surely other similarities -- such as the fact that both are written in Go -- but the ones listed above strike me as the most fundamental. What it comes down to is that both Flynn and Deis are attempting to provide a distributed scaffolding that enables IT departments to build potentially massive, containerized, service-oriented architectures. The robust systems layer is meant to make life easier for ops, and the Heroku-inflected deployment tools are intended to make life easier for devs. Today, the two projects have similar ambitions, architectures, and messaging. It would be highly beneficial to the Docker and PaaS communities to hear more from the project leaders themselves about what makes their respective visions fundamentally different. In the next few weeks, we will be interviewing the leaders of the Deis and Flynn projects to get this perspective right from the source. In the mean time, I'd like to invite anyone involved with any PaaS projects---not to mention any of our readers---to join the discussion in the comments or elsewhere. In particular, I'd love to get your thoughts on the following questions:", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "analyzing-dockers-new-oss-libchan-and-libswarm", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/analyzing-dockers-new-oss-libchan-and-libswarm/", "abstract": "This morning's keynote from Solomon Hykes was inspiring. Just after the keynote, Solomon told me he was really nervous. But if he was, it didn't show. The way they are building out their ecosystem is as equally inspiring as Solomon's speech this morning. Today, there were three new projects announced from Docker: And two \"secret\" Docker projects were hinted at too: This is really exciting news for people building tools around Docker, however it may be less interesting to people who are just using Docker for building apps. Let's review what this means for the Docker ecosystem: In Docker's words: Libswarm is a toolkit for composing network services . In my words: Libswarm is like an ORM or a type system for distributed systems . The biggest problem with Docker is multi-container and multi-server inter-container communication. What does that mean without buzz words? Imagine being able to run docker ps and it shows all your containers running on 5 different infrastructures (some public, some private, some on your laptop). That's what libswarm enables… a swarm of Docker containers available through a unified API. Here is a demo video shown at the DockerCon Keynote made by Ben Firshman of Orchard of what you can do with libswarm: The \"Distributed Docker Containers\" problem has lead to lot of open-source projects on top of Docker: The problem with so many choices is that they all have their own protocols and work in different ways. Once you adopt one of these tools, you effectively are locked into their solution because they don't share an API or an underlying communication method. libswarm exposes a standard interface to systems like Fleet, Mesos, Consul etc Libswarm is built to help provide a shared API between all of these orchestration tools. This is a brilliant move from the Docker guys. It simultaneously creates a common ground that allows people to collaborate on orchestration, but it also encourages healthy competition. When the orchestration tools are interoperable, it breaks vendor lock-in which allows people to move more freely back and forth and lets the best tool win . It levels the playing ground. This is a revolutionary building block for Docker ecosystem development. It keeps incumbents honest and gives up-and-comers a real chance to create something new and better. Solomon notes on Hacker News that this project is very rough and early, and not yet ready for production use yet. Like Go channels over the network In Docker's words: Libchan is an ultra-lightweight networking library which lets network services communicate in the same way that goroutines communicate using channels . It is for: In my words: Libchan is the underlying library that enables libswarm . It is like ZeroMQ or RabbitMQ for Docker. The difference is that the existing messaging tools are heavyweight and general purpose… libchan is built to support Docker and containerized apps in Docker. For network engineers and people who get excited about Unix sockets, this tool is very exciting. For an average developer, you will more likely interact with things built on libswarm than libchan. This was not the biggest news of the day, since libcontainer has been a part of Docker for a while now. It is simply spinning it out into its own repository. However it is a smart move and it will enable faster velocity in expanding support for more types of containers and more abstract support for things like managing VMs. The Docker community continues to evolve quickly and tackle big challenges. The new projects announced this morning will go a long way to addressing some of the biggest problems Docker has to get to mainstream adoption.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "top-10-startups-built-on-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/top-10-startups-built-on-docker/", "abstract": "Docker is a new technology gold rush. Docker, Inc. just raised another $15M in venture capital, and it seems like new Docker-based projects and startups are popping up every week. In this weeks post, I'm going to highlight the top 10 that have caught my eye. I have grouped them to emphasize the areas in common between them, but many do multiple things. All three of the Docker hoster startups listed here have SSD containers. 1. StackDock (by Copper.io) \"Docker hosting on blazing fast dedicated infrastructure\" StackDock sells SSD Docker-containers for $5/month. You do this by creating what they call a \"Docker Deck\" which allows you to specify a template with any number of technologies you can put into a Docker container. Then \"Drops\" are \"distilled Decks\" that give you an instance of the container. StackDock is a fast way to get going quickly with Docker. There is no free plan with StackDock. 2. Orchard \"Host your Docker containers in the cloud\" Orchard has a novel take on Docker hosting. Instead of providing push-button web interfaces for launching Docker containers like StackDock or Tutum, Orchard provides a pass-through CLI that lets you deploy Docker containers on the command line remotely with the same types of commands you use locally (just prepending their orchard CLI in front of the docker command you would usually use). For example: You can get $5 of credit for free with Orchard (effectively two weeks free for one 512MB container, paid containers start at $10/month charged hourly). Orchard also provides free private Docker registries for your apps which is a nice plus and persistent storage. One of the cool things Orchard has contributed to the Docker community is an open-source project called Fig which allows you to hook together code and databases really easily. 3. Tutum \"IaaS-like control, PaaS-like speed\" Tutum aims to differentiate with value-added services like load balancing and persistent storage. Their web console is the most advanced out of the three mentioned here and has a logging console and a nice management dashboard. Tutum also has the smallest available Docker containers, starting at $2/month for 62MB RAM in what they call XS plan. They offer one month of free hosting for one XS container, so it is easy to try before you buy with them. One of the cool things Tutum has done is created an excellent set of Trusted Repositories on the Docker index that you can use to quickly setup databases and technologies using multiple stacks. 4. Quay.io \"Secure hosting for private Docker repositories\" This is the closest thing to GitHub that Docker currently has. Although Orchard has private Docker registries as well, Quay focuses on organizations so that you can collaborate with your team on building the perfect linux containers. Prices range from $12/month to over $200/month. 5. Flynn \"The product that ops provides to developers\" Flynn is not a traditional startup in the sense of bootstrapping or venture funded business, rather it is an open source project that is sponsored by developers and corporations (including yours truly, CenturyLink). The promise of Flynn is a lightweight Docker-based PaaS that runs anywhere. Flynn provides a set of “PaaS Lego” including an API that orchestrates the management of containerized services across a cluster. For easy out-of-the-box deployment Flynn includes Heroku-style “git push” deployment and management tools that utilize buildpacks. Flynn was inspired by PaaSes like Heroku but is designed to run more than traditional 12 factor apps by embracing the problem of state. 6. CoreOS \"Linux for Massive Server Deployments\" CoreOS, a Y-Combinator alum, is a startup backed by top-tier VCs Andreessen Horowitz and Sequoia Capital. This is one project to keep a close eye on. Unfortunately the documentation is not great yet, but you can essentially think of CoreOS as 3 projects right now. When you combine those three technologies and chant the right incantation (hint: look at this page where the systemd conf file says \"Description=My Advanced Service\") you will get a distributed CoreOS platform for deploying and managing your large-scale distributed application requirements. Awesome! If you need more hands-on guidance, you can always pay them to step you through it themselves. 7. Serf (by HashiCorp, makers of Vagrant ) \"A decentralized solution for service discovery and orchestration that is lightweight, highly available, and fault tolerant\" One of my favorite new open-source projects came out of the guys who make Vagrant: it's called Serf. I wrote about it last week in Decentralizing Docker: How to Use Serf with Docker so you can get a great insight into how to use it with Docker there, but essentially it is a hammer you can use where CoreOS and etcd is a nail-gun. Serf is really easy to use outside of Docker and can be used in a lot of different ways where etcd and CoreOS are pretty specific tools that aren't nearly as flexible (though definitely very powerful). 8. Shippable Shippable is a Techstarts startup that just raised $2M from angels and VCs . Their claim to fame is that they use Docker to build containers that run your unit tests faster, more distributed, and safer than in other CI/CD environments. Since they TEST your code using Docker containers, the awesome thing that they can do is DEPLOY the containers that they just tested wherever you want. This feature is not in general availability yet, but once it is, it could change the game for CI/CD across the board. 9. Memcached as a Service Though today this appears like more of a hobby project than a startup, it still provides an awesome utility and shows of the speed and potential of Docker by spinning up memcached services for you in less than a second. A really cool project and Julien Barbier has posted some awesome slides about how he built this SaaS using Docker. 10. The Docker Book Last but certainly not least is the Docker Book. A book has more alike with a startup than you might realize at first. As an author, unless you are satisfied with selling 10 copies of your book (all to your mother), you have to figure out your go-to-market strategy and your product-market fit. Nathan Barry has written a great book about treating technical books like startups called Authority . If you measure the health of an open source project by the ecosystem created around it, I think it is fair to say that Docker is the picture of health. Next week we will spend more time looking at some of the top open source projects built using Docker. Make sure to subscribe to our newsletter to get updates. You can subscribe at the top or bottom of this page.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-drone-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-drone-on-the-centurylink-cloud/", "abstract": "Install a fully functioning CI platform on the CenturyLink Cloud , hook it up to GitHub, and demonstrate full CI automation through the use of Docker and Drone. Prerequisites: It is the fairly modern practice of merging developer changes into a single trunk version several times a day. Additionally, most teams using a CI methodology hook in their QA systems to allow the automation of unit and integration tests. By designing the CI pipeline to invoke QA processes prior to merging the changes to the mainline trunk, an organization can effectively have a “gated trunk” whereby all of the changes have passed tests before being merged, thereby guaranteeing that the trunk version is always runnable. Drone is a CI platform that has tight integration with GitHub (public and Enterprise) and BitBucket, with more code repos to come, I am sure. It is based on Docker and container technologies and is available through a SaaS model (at http://drone.io ) or via open source that you install yourself ( https://github.com/drone/drone ). Today we are going to install the open source Drone platform on the CenturyLink Cloud and demonstrate how easy it is to get going. To start, ensure the 3 pre-requisites (above) are in place. We will want to have 2 windows open at this time: From your terminal: And that is how easy it is to install Drone on a Docker-based CenturyLink Cloud server. From your browser navigate to http://public IP/install . Drone, by default, is listening on port 80 (of of the default ports we opened when installing the Docker server). Follow the steps to create the admin user. The second page will ask for a GitHub Client ID and secret. This has to be done from your GitHub account and is done by registering a new application. Open a second browser window, the link is here and it should look like: Enter a name for the application (eg. “Drone”). Enter http://public IP for your server under “Homepage URL”. An optional Application description can be entered. Set the “Callback URL” to http://public IP/auth/login/github . Push the button to register the application. In the upper right of the subsequent screen you will find the Client ID and Client Secret. Enter these into the original Drone install browser window. Scroll to the bottom of the page and you will find settings for email notifications. This is super helpful in keeping track of the CI process flow, so take a few minutes to enter these and then press the “Save” button. Look at the top of the page, see the “New Repository” button? Click this, and then the GitHub “Link Now” button on the next page. Press the “Authorize Application” button and the enter your GitHub name and repo. Congratulations! You have successfully installed and configured Drone and GitHub on the CenturyLink Cloud . You are ready to start adding code and feeling the power of build automation! Go Hello World Since our version of Ubuntu does not have the user ‘ubuntu’ we need to add it. This is (currently) required for Drone to execute correctly. Create a new user named 'ubuntu' by following these steps: At the prompt create a password for this user and then accept the default user information by pressing ENTER at the prompts. In order to give ubuntu the correct privileges we must add the user to the sudoers file. Add the new ubuntu user in the \"User privilege specification\" section. Once the file has been successfully edited press CTRL-x to exit the file; and Y to save the changes. The key thing to make Drone work with your GitHub repo is to add a .drone.yml file to your repo. This is the instruction manual to tell Drone what to do. We are going to build a simple Go application and our .drone.yml file looks like: Make sure when you commit this file it has the leading period. If you commit this file with no Go language files (as I just did) you will notice in your Drone browser window that a new file has been detected. Click on the commit number (eg. 1aaf85) and you can see that the commit has started. Why does it take so long (approx. 3 minutes for me) to complete, particularly since there is actually no application code to build? Since this is the first commit Drone is downloading and installing the base images (including Ubuntu) that the applications will run on. Go back to the README file, make a trivial change, and see how long the second commit takes. On my system it took less than a minute. Let’s add the Hello World application to the repo. Over at GitHub create a file called GoHello.go and add the following: Once you are done, commit the file. Quick... look at your Drone browser to see that the CI server has detected the new file and is processing it. Click on the commit number and you should see something like this: This shows a successful clone, build, test, and install CI pass! Now all you have to do is add some tests, rinse, and repeat!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "persistent-distributed-filesystems-in-docker-without-nfs-or-gluster", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/persistent-distributed-filesystems-in-docker-without-nfs-or-gluster/", "abstract": "Persistent filesystem is one of the hardest parts of running a CMS or blog in the cloud. If you have ever tried running WordPress or Drupal on Docker or a PaaS environment before, you know what I am talking about. Each container or dyno is ephemeral (goes away when the container goes away) and when the dyno goes away, all your uploaded content goes away too. You can't easily Docker-ize NFS or Gluster. There are tutorials out there, and even a few containers in the index that say they work. But I could not get any of them working in the latest version of Docker. Even if you could in theory run NFS or Gluster in Docker, would you really want to? Who hasn't had trouble with those technologies here? Whether it is kernel patches or TCP ports, it is one thing or another. It is never easy. Wouldn't it be great if you could use a syncing technology that didn't require you to open ports or patch your kernel? Like Dropbox, but without the central server so you don't have to worry about your code being compromised? Although BitTorrent Sync isn't open source, it does work really well to sync your servers for no cost and without a centralized server. A new open-source project called Syncthing is a promising alternative to BitTorrent Sync, but since it is not stable yet, we will show you how to use it after it becomes more stable. Here is how you can add syncing to your Docker app: And on any other computer, you can run the ctlc/btsync container with the secret from your first instance: You only need to run one ctlc/btsync container per virtual server. You can run as many containers that bind to that container's volumes through --volume-from as you want. That way all your containers can share their ephemeral data locally AND across systems. If you combine BitTorrent Sync with Docker, you can create a potent and powerful solution to the persistent filesystem problem without debugging NFS or kludging together an rsync solution. Data-only volumes are a great solution for shared filesystem within a single host, but when you want to build multi-host apps, using a solution like this can be an elegant way to scale.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "building-your-first-app-on-coreos", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/building-your-first-app-on-coreos/", "abstract": "I am sure you have heard of CoreOS , but have you ever actually deployed a real app on it? Not many people have. Building an app on CoreOS can be incredibly hard and frustrating. The documentation is scattered and you have to learn all about different technologies before you even get started (etcd, systemd, and docker). If you are lazy (like me) and want to just try out CoreOS with no fuss, I will guide you through the whole process from start to finish. We will create a simple WordPress application together running with a MySQL service in CoreOS. If you are on a Mac, you can install fleetctl and etcdctl natively to control your CoreOS clusters. Here is how: Getting a local clustered CoreOS up and running with Vagrant is pretty easy. Now you have a mini 3-system local CoreOS cluster running on your laptop. Easy enough, now let's check our local fleetctl . Awesome. It worked! Great, now you have a clustered CoreOS. What do you do with it? The fleetctl command lets you deploy your app into a cluster of CoreOS nodes, but writing service files for fleet sucks. Luckily, you don't have to write them yourself. You can use a simple yaml format to generate service files. The fleetctl client tool uses etcd's key/value store to understand the servers it has access to and needs access to the etcd servers running on each machine in the cluster. Here is how to deploy your app into the cluster. Now your app is running, but the services are not registered into etcd yet. Luckily, fig2coreos generated the discovery service files as well for us. That's it! You're done! In fact, if you are using Vagrant 1.5 with a Vagrant Cloud account, you can actually get to your WordPress app and see it in action. There is much more you can do with CoreOS, but at least you have now done the basics. If you are planning on leveraging multi-server CoreOS clusters in production, you will need to add ambassador containers to your system. In fact, you can even combine the etcd service discovery with ambassador containers, but we will leave that for another blog post next week.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "the-future-of-continuous-integration-with-shippable-founder-avi-cavale", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/the-future-of-continuous-integration-with-shippable-founder-avi-cavale/", "abstract": "This week we are talking about the future of Continuous Integration through Docker with Shippable founder Avi Cavale. Avi Cavale , Founder of Shippable Born in Bangalore, Avi studied Mechanical Engineering before earning a Masters Degree in Computer Science at Arizona State. I founded Shippable back in February 2013 and just integrated Docker in the last ten months. A little bit background on myself. I was born in Bangalore, India. Undergrad was mechanical engineering and then I did a masters at Arizona State in computer science and then joined Microsoft back 2000 and worked there for 10+ years for different groups, like Xbox and Kinect. Then worked on a Cloud Foundry based startup for about six-eight months before starting Shippable, where I saw the need for CI/CD as a service and was the inspiration behind and creating Shippable. Shippable is predominantly focused towards the developer cloud. Anybody into the whole agile and continues delivery concept should be thinking about using Shippable or products like Shippable out there. People have used Jenkins in the past and got they made Jenkins do things which it was not designed for but at the end of the day what Shippable is about, going and solving those fundamental problems of, how do I manage infrastructure and continuous delivery and make sure my code is always deployable. Using Docker instead of VMs made it 2.5X faster with just pure build time. But if you take a look at it from a more holistic way of, how long does it take to bring up a machine, how much time it takes to install stuff, it’s all that stuff. It's actually much, much more faster if you look at it as total cost of operation. So the reason why it’s faster, is one, we have our own proprietary platform build from scratch, which is optimized to take an advantage of what Docker brings to the table, as opposed to retrofitting an existing product server like Jenkins to do what Docker does. We initially tried that approach and we started having all kinds of issues in terms of scale as well as security so we decided to build this from scratch. The second thing it does is because Docker is portable, we can move around containers on Amazon machines. So, what typically people will not use as build machines, we can actually use those kind of machines and allow Docker containers to float around that gives us extra horsepower that we need. So you don't really get the noisy neighbor issues or queuing issues which is what we can do, where you can spin up machines but just a few API calls you can actually do a lot of things in a very, very cost-effective way. And that's the secret behind it. With Jenkins, you really getting into more operations of VMs and at some point of time it's a very transient workload, so you need to manage elasticity manually. Do I want to keep all these machines running? Do I need to shut it off? All of that stuff starts becoming part of what a developer job is. Docker allows us to kind of abstract those things out. First thing is, you can have standardized images through the Docker Hub. So you can kind of think of it all as it's all standardize images that we can bring get online within like two or three seconds without developers having to actually go in, monkey around on a lot of Debian package installs and all of that stuff. So what we do is at Shippable, there are two ways of how we use Docker. We use Docker containers on which our platform runs natively. So we use the Docker work flows and how our UI server is actually running on a Node Docker container and it all comes with our custom stuff. So that's one part where infrastructure Shippable itself runs on Docker. The second part is where our customer actually running on individual containers that are tailored for every single customer. So in terms of Jenkins, if you really want to run Jenkins as a service now we are sitting down and managing multiple VM images and all of that stuff. With Docker, I can kind of create a lowest-common-denominator image and then lay out whatever the customer needs, which is specific to them on top of that. So it allows us operational efficiency and that means we can run the service way, way more cost effective when we can pass the savings back to the customer. Because of the portability, you can move workloads from dev/test to production and ensure it is exactly the same. You can use Puppet or Chef or SaltStack to build your containers and then test them with Shippable. Shippable isn't great at enterprise stuff yet, we need to build a lot more governance, we need to build a lot more controls. It could be as simple as a container is sitting in your pipeline and you get a text message and it says or somebody in operation says, “yes, let this go into production.” I mean it could be as simple as that so we need to build a lot more of that. We’re being focused a lot on making developer workflows and developer life much more easier, we need to start focusing a little bit on how the whole operation is and how it works. And that's where some of the challenges I want to see for the next six months. The future of programming is in micro-services and 12-factor applications, and the future of CI/CD is testing combinations of micro-services together instead of testing monolithic applications. Older CI/CD systems were not build to test the interactions between many different micro-services and this is creating an opportunity to re-think CI/CD. Shippable gives you 5 private repositories for free. Our competition makes you pay an arm and a leg for that. Then $10/month is the cheapest paid plan, so we are very competitive. Over 4,500 people signed up so far and growing.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "announcing-panamax-docker-management-for-humans/?utm_source=CLC&amp;utm_medium=web&amp;utm_campaign=panamax_launch", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/announcing-panamax-docker-management-for-humans/?utm_source=CLC&amp;utm_medium=web&amp;utm_campaign=panamax_launch", "abstract": "\"Panamax allows developers to go from zero to pre-packaged, multi-container Docker environments in minutes. The web interface and overall user experience is impressive.” –Gabriel Monroy, creator of Deis Docker is an amazing tool, but it remains notoriously hard to use to build multi-container micro-service architected applications. That is until today . Today, I am proud to announce the release of the Apache 2 open-source platform we call: Panamax . Over 9 months in the making, CenturyLink has invested in a team of 11 killer seasoned engineers and designers to build a tool that we believe will dramatically lower the barrier of entry for developers to use Docker, CoreOS and other innovative technologies. To build complex clustered containerized applications before Panamax, one needed to chose a variety of new technologies, each with it’s own learning curve: fig, etcd, systemd, fleet, mesos, ambassadors, discoverd, consul, consulate, serf, registrator, skydns, libswarm and the list keeps growing. We have tried to keep up with all the changes on this blog, but it is easy to fall behind. What if there were just one project that applied Docker best practices and took care of stitching Docker containers together for you? That’s Panamax. That’s what I imagined 9 months ago. “Panamax is an exciting improvement on the Docker user experience.” –Jonathan Rudenberg, creator of Flynn Panamax has a beautiful and elegant UI component for deploying Dockerized applications with, but it is not just a Docker UI. First of all, CoreOS (an industry leading cutting edge operating system for running clustered Docker systems) is built into Panamax. Along with CoreOS, Panamax leverages Fleet. Built-in container orchestration and job scheduling from Fleet goes way beyond regular Docker UIs. But Panamax’s real killer feature is its Open-Source Application Template Library. Deeply inspired by Orchard’s fig format, Panamax took the idea one step further by creating a repository on GitHub that houses the configuration and architecture of these Dockerized apps. Anyone in the world running Panamax can use the Open-Source Application Template Library to reproduce complex multi-container Docker apps with a single click. \"We're pleased to see Panamax launched! It's an exciting project that shows the strength and diversity of the Docker ecosystem.\" –James Turnbull, VP at Docker Panamax is not a PaaS. It is a containerized cloud builder. In fact, you can (in theory) install any Docker PaaS on top of Panamax: Deis, Dokku, Flynn. We hope the community will create app templates for these PaaS that can be used to install any PaaS with one click. What makes Panamax different than PaaS? First of all, PaaS is inherently opinionated. You give a PaaS your code and it does all the rest. Panamax is a tool for building containerized clouds. If you don’t use an app template, you have total control over the entire architecture of your app as you build it. For example, in PaaS you have to use the router and load balancer that the PaaS choose for you. In Panamax, there is no opinionated router or load balancer. You search Docker Hub for nginx or haproxy, or build a container yourself and stitch it into your application. Panamax doesn’t force opinions. It gives you total control. “Docker containers are great, but you need more than just one to run your application. The Panamax solution brings application templates and a public repository to share and collaborate that is elegant and simple to use.” –Borja Burgos, founder of Tutum Edit: The Panamax Application Template Contest has concluded and the contest page listing the rules, prizes and winners is no longer available To celebrate the launch, we are giving away over $100,000 in cool gadgets. You can win one of 30 New Mac Pros, 30 New iPad Airs, a copy of the Docker Book and even be featured in this blog. All you have to do to win one is create the most elegant, clever and popular application templates for Panamax that you can. All application templates must be submitted within 2 weeks from today. To learn more about the rules and how to compete, go to Panamax.io. The launch is just the beginning. What comes next is up to you and the community that grows around Panamax. CenturyLink incubated this project with the intention of giving back to the open-source community. We did not just go and find some project nobody cared about and put the source code on GitHub and call it open-source. Our team of 11 has put their best foot forward trying to make clean and legible code so that it would be easy for people to join in and collaborate. Check out the source code on GitHub: https://github.com/centurylinklabs/ . We are very proud of it. Help us build features and functionality we only dream of. Join us in making complex Docker apps easier to build today. Introduction to Panamax from CenturyLink from cardmagic", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-is-flynn-an-open-source-docker-paas", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/what-is-flynn-an-open-source-docker-paas/", "abstract": "Flynn.io is one of the most popular and powerful open source Docker PaaS solutions around. Flynn believes that ops should be a product team, not consultants. Flynn is the single platform that ops can provide to developers to power production, testing, and development, freeing developers to focus. This week we talk to one of the creators of Flynn, Jonathan Rudenberg. Jonathan is a super smart guy who used to work at Shopify and has brings his experience with large distributed systems into Flynn. Jonathan Rudenberg , Flynn.io Creator Besides being one of the cofounders of Flynn, Jonathan is a developer, security consultant and researcher. He is also one of the Tent Protocol architects. Besides working on the future of Docker-oriented PaaS', Jonathan occasionally posts on his blog, Titanous.com . Flynn is an open source PaaS built from pluggable components that you can mix and match however you want. Out of the box, it looks a lot like Heroku (with git push functionality), but you can replace pieces and put whatever you need into Flynn. flynn-host The Flynn host service discoverd The Flynn service discovery system flynn-controller The Flynn Controller for the management of applications running on Flynn via an HTTP API flynn-bootstrap Bootstraps Flynn Layer 1 gitreceived An SSH server made specifically for accepting git pushes that will trigger an auth script and then a receiver script to handle the push. (This is a more advanced, standalone version of gitreceive .) flynn-cli Command-line Flynn HTTP API client flynn-receive Flynn’s git deployer slugbuilder A tool using Docker and Buildpacks to produce a Heroku-like slug given some application source. slugrunner A Docker container that runs Heroku-like slugs produced by slugbuilder . flynn-dev Flynn development environment in a VM strowger Flynn TCP/HTTP router shelf A simple, fast HTTP file service sdutil Service discovery utility for systems based on go-discover flynn-postgres Flynn PostgreSQL database appliance taffy Taffy pulls repos and deploys them to Flynn go-flynn go-discoverd rpcplus Docker is great for running containers on single servers, but not great at running distributed apps on many servers. Flynn helps you do that better. Many PaaS technologies mainly focus on scaling a stateless app tier. They may run one or two persistent services for you, but for the most part you are on your own to figure that part out. Flynn is really trying to solve the state problems, which is pretty unique. We use etcd from CoreOS in Flynn. Flynn can run on CoreOS, but CoreOS is essentially just a Linux distribution whereas Flynn is a PaaS You can deploy a Flynn cluster with a simple tool we built here: https://flynn.cupcake.io/ . There is also a Vagrant VM and you can bootstrap it yourself on your own environment. Flynn managed database services for you. Right now we only have Postgres support, but our goal is to have Flynn help you run the database services so you don't have to. Application filesystems are currently ephemeral still, but we are looking into solutions in the future that could provide persistent filesystems for app instances. Although there is currently not a direct tie in, you could build a CI tool on top of Flynn. It is very much not production ready. We are focusing on improving the production readiness of Flynn, but right now there are a lot of rough edges and a lot of bugs. The general timeline on 1.0 is around early Fall 2014. Pinkerton is a tool for manipulating Docker images outside of Docker. Containers have been around a long time, but still aren't perfect yet. They don't provide as much security isolation as a VM, which means containers are not ready for multi-tenant environments yet. The goal of Flynn is to change operations from a consulting team that goes off and writes Chef scripts for people to an actual product team that runs the platform that enables engineers. To enable something like IT-as-a-Service.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-does-apache-mesos-work", "author": ["\n              Chris Weyl\n            "], "link": "https://www.ctl.io/developers/blog/post/how-does-apache-mesos-work/", "abstract": "Managing multiple Dockers hosts is hard. Docker orchestration systems are built to address that problem. There are multiple options for Docker orchestration. Last week we talked about Google's Kubernetes , and this week we are going to discuss a cluster resource management solution that has recently become an Apache \"Top Level Project\" : Mesos . A project originally born from scaling Twitter, Apache Mesos is a datacenter-level project that aims to provide high-level, finely-grained, and collaborative locality-aware resource management to various computing frameworks. Mesos operates as a cluster manager, with a master controller that communicates with agents residing on each node (e.g. host machine) and \"framework schedulers\". Nodes run with a Mesos agent on them, which in turn is responsible for managing the local resources, any \"framework executors\" running on the nodes and coordinating with the master. \"Framework schedulers\" are responsible for telling the Mesos master what jobs they have ready to run and what resources those jobs require; \"framework executors\" in turn are handed jobs that the cluster is ready to have run on the right nodes with the right access to resources. Data locality is -- simplistically -- the concept of how \"expensive\" the data or resource you need is for you to access. Generally speaking, the cost here is latency or time needed to access a resource, but could be, say, machine or network use charges. A fundamental example of locality and cost can me found inside every IPv4/IPv6 attached computer: the routing table details how to send packets from your machine to a target ip, and when there's more than one acceptable route for a packet to take the kernel chooses the route (generally network interface) to send those packets over based on which route has the lowest \"metric\", the key indicator of cost. (You'll appreciate this on a well configured-laptop that has a wireless and wired interface both off the same network!) An even more tl;dr version of this can be found on -- where else -- Wikipedia, at Locality of Reference . To impress on the importance of leveraging data locality, let's imagine a Mesos cluster that runs on 4 nodes, two of which have direct access on node-attached storage of, say, a 500TB dataset (dataset A), and all of which have direct access to a 100GB dataset (B); three frameworks run inside the cluster, all of which run jobs that use either dataset A or B, but never both at the same time. When the framework schedulers communicate with the Mesos master as to what jobs they have and what resources those jobs need, all other things being equal Mesos will offer slots on the nodes with direct access to dataset A to the jobs that need it. This locality of access can result in drastic speed increases, as the cost of accessing data locally -- e.g. internal disk, SAN/SAS/SCSI-attached storage, etc -- is often significantly less than accessing data remotely, often over a network and through another host. Lower cost to access data/resources translates into faster processing and more efficient utilization of cluster resources. We are interviewing Florian Leibert, Founder and CEO of Mesosphere (the commercial company behind the Mesos project) on the CenturyLink Labs Podcast . Check back soon to get his background and history on the project in more detail. We also soon publish a follow-up post on how to get started using Apache Mesos. Until then, check out these great links...", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-to-migrate-legacy-applications-into-docker-containers", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/how-to-migrate-legacy-applications-into-docker-containers/", "abstract": "What if your application has been around a long time already? Is it too late for containers? Can you teach an old dog new tricks? Yes you can! Before you throw in the towel and loading up your fatty VM, let's consider what constraints 'containerizing' puts on the application. In this post, I will demonstrate containerizing legacy applications use an application written by Matthias Nehlsen called Birdwatch . His application was not designed for containers but with making a few alterations, Birdwatch can be deployed and configured within a Docker container. If the application relies on external services (i.e. database, nginx proxy, or message queue) these services should now be running within their own container. Of course, this step is not unique to containers; it's a common step for any complex application. The Birdwatch application has two major components. The application is built using the Play framework and relies on an ElasticSearch index. Therefore, I will design the architecture using two containers: one for the application and one for elastic search. This implementation has several advantages: An alternative would be to create a single image which includes all components. While this may appear simpler, it has the same drawbacks as creating a monolithic black box - any alterations to a component will require a new image of the entire application. The larger the image, the larger the download, which will have a negative impact on deployment. A search on the Docker Registry will provide a list of possible images upon which to base the application. The difficulty is determining which to use. The community can star an image, which could be used to determine the popularity of an image. The Registry also shows the number of times an image has been downloaded, but these measures are subjective therefore find images with the following characteristics: Using an image with a tag provides version control for the application. The latest tag is a rolling version consequently pulling the latest image and therefore may introduce a breaking change to the application. The Dockerfile is the complete image recipe to validate all the code and actions contained within the image. Having a common base image will improve the development cycle because they will leverage an image cache. Read Working with the Docker image cache to learn more. Our Birdwatch application needs an ElasticSearch image with version 1.1 or greater. Searching on the Registry for 'elasticsearch' provided 236 results. The first entry is a trusted image, has a Dockerfile, but only provides a latest tag. It also requires the Play2 framework. A Registry search only provides three possible candidates. Inspecting the FROM command in the Dockerfiles for each of the results show none of them have a common base with our trusted elastic search. Initially, I tried the mzkrelx/playframework2-dev but found its startup was slow. I finally settled on the reubenbond/playframework2 image. For an efficient production release of Birdwatch, I would consider creating a new Play framework image which was FROM the common dockerfile/java image used in the trusted ElasticSearch image. Applications require some configuration. This could include: An application running in a container will need the same configuration. This configuration data could be sealed within the container; or, a better approach is to provide the configuration at runtime. Currently, there are three methods to provide runtime configuration. The first is using environment variables . All environment variables provided on the run command are exposed to the image. Therefore any configurations which use environment variables can be set during the run command. The Birdwatch application uses a set of configurations defined in an application.conf file. Assigning environment variables to these items is the only step required to make them configurable. Each of these must be set by a -e option on the docker run command. Using multiple container's requires setting up a communication channel between them. Docker calls this linking . The links are presented to the container as environment variables using specific naming convention. Our Birdwatch application is going to be linked to elasticsearch using --link dockerfile_elasticsearch_latest:db This link command creates several environment variables based on the db alias provided in the link. The TCP address and port are used in the application.conf again to provide the link to the elastic search container. The final method for providing runtime configuration is through parameters to the Dockerfile entrypoint . Our Birdwatch application did not use this configuration process. The current best practice for logging is to write to the standard out. The journal and container logs capture this data and can be viewed in case of an application failure. This subject is far from concluded many discussions have been circling around the ability of a container to provide logging data. The Birdwatch application used system logging and a log file so no alterations to the codebase was needed to access its log data. A Dockerfile must be used to create an image. This file outlines all the steps required to build the image. This article will not discuss the optimization of images. A great discussion about image optimization can be found here . The key command for adding code is the ADD command. It has two parameters a src and dest. The src parameter is a valid path when building the image. The dest parameter is an absolute path within the container. If you want to ignore files or directories when adding files place a .dockerignore in the root directory. It works similar to a .gitignore file. Once you have an image created it can be added to the Docker Hub . Our Birdwatch application has a pretty involved build process. Each of the UI frameworks need to be built first and then the application can be constructed. This could be handled in a script which would be executed on build, but for this demonstration I chose to run those steps outside of the docker build process and only add the resulting compiled code. I added a .dockerignore file to ignore the .git and logs directories: The Dockerfile to assemble the image: The final image is on the Docker Hub: Birdwatch Image Search for Birdwatch and click install. We have shown with the Birdwatch application it is possible to 'containerize' an application after it has completed its development cycles. With some knowledge of how Docker will expose linked containers and using environment variable we also provided runtime controls to the application. Now the Birdwatch application can be deployed and configured for any topic using any twitter credentials. No more excuses. Go forth and Containerize!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "more-docker-image-cache-tips", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/more-docker-image-cache-tips/", "abstract": "In my previous article I described the Docker image cache and how Docker decides that a particular layer needs to be rebuilt when you do a docker build . Now, let's build on that knowledge and discuss some strategies for making the Dockerfile code/build/test cycle as fast and reliable as possible. This one should be pretty obvious by now, but as you're iterating on your Dockerfile you should try and keep the stable parts toward the top and make your additions at the bottom. If you know you need to install a bunch of OS packages in your image (which is typically one of the slower parts of building an image) put your package installation instructions toward the top of the Dockerfile . That way you only need to sit through the installation process for those packages once as you go through the code/build/test/repeat cycle for your image. Similarly, if you have a core set of instructions that you use across all of your images (like a MAINTAINER value you always use), it's best to keep those at the top of your Dockerfile and always in the same order. That way those cached layers can be shared between different images. When executing docker build the first line of output typically reads \"Sending build context to Docker deamon . . .\" The build context constitutes everything in your build directory (the directory that you pass to the docker build command) and is used by Docker so that you can inject local files into your image using the ADD and COPY instructions. This is the one place where the caching rules change slightly -- in addition to looking at the instruction and the parent image, Docker will also check to see if the file(s) being copied have changed. Let's create a simple Dockerfile that uses ADD to copy a file into our image: Now let's docker build the image: If we were to execute the docker build again we'd see that no new images are created since we haven't changed anything. However, let's update the README.md file and then build again: Note that a new image was generated for the ADD instruction this time (compare the image ID here to the one from the previous run). We didn't change anything inside the Dockerfile , but we did update the timestamp on the README.md file itself. For the most part, this is exactly the behavior we want when building images. If the file changes in some way, you would expect that the next build of the image would incorporate the changes to that file. However, things get a bit trickier when you start adding lots of files at once. A common pattern is to inject an application's entire codebase into an image using an instruction like: In this case we're injecting the entire build context into the image. If any single file changes in the entire build context, it will invalidate the cache and a new image layer will be generated on the next build. If your build directory happens to include things like log files or test reports that are updated frequently you may find that you're getting new image layers generated with every single docker build . You could work-around this by specifically ADD ing ONLY those files which are necessary for your application but if you have many files spread across a number of directories this can be pretty tedious. Luckily, Docker has a better solution in the form of the .dockerignore file. In much the same way that the .gitignore file works, the .dockerignore file allows you to specify a list of exclusion patterns. Any files/directories matching those patterns will be excluded from the build context. If you have files in your build directory that change often and are not required by your image, you should consider adding them to .dockerignore file. A good rule of thumb is that anything in your .gitignore is a good candidate for inclusion in your .dockerignore . One Catch-22 related to the use of ADD . is that the Dockerfile itself is also part of the build context -- so any changes you make to the Dockerfile result in a change to the build context, and you can't add the Dockerfile to the .dockerignore file because it needs to be part of the build context in order for Docker to read the build instructions. If you're using ADD . and making changes to your Dockerfile don't be surprised to see new image layers generated every time you do a build. For the most part, the image cache is incredibly helpful and can save you a lot of time while building your images. However, there are times when the caching can bite you if you aren't paying attention, so it's good to know how to selectively bust the cache. Let's say we have a Dockerfile which contains the following: When I build this the first time, I'm going to get exactly what I expect -- it'll clone my Git repo and checkout the v1.0.0 tag. Now imagine I push some changes to my repo and tag it as v1.1.0 . I'm going to update the Dockerfile to reference the new tag: When I go to build the image from the updated Dockerfile I get the following error: I definitely pushed a v1.1.0 tag to my repo, yet Git is telling me that no such tag is found. This is one of those times where the Docker image cache is being a little too helpful. In the output above note how the git clone step had already been cached from our previous build and was re-used in this run. When we get to the git checkout instruction we're still using a copy of the repo that doesn't have a v1.1.0 tag. This is quite different from the example with the build context above. In this case the contents of the git repo are not part of the build context -- as far as Docker is concerned, our git clone is just another instruction that happens to match one that already exists in the cache. The brute-force solution here is to simply run docker build with the --no-cache flag and force it to re-create all the layers. While that will work, it doesn't allow us to take advantage of any earlier instructions in the Dockerfile that were just fine to be pulled from the cache. A better approach is to refactor our Dockerfile a bit to ensure that any future changes to the tag will force a fresh git clone as well: Now we've combined the git clone and git checkout into a single instruction in the Dockerfile . If we later edit the file to change the tag reference it will invalidate the cache for that layer and we'll get a fresh clone when the new layer is generated. Note also that I moved the WORKDIR instruction so that the directory would be created before the cloning the repo. Then, by cloning into the current directory (note that . after the repo's URL), I was able to execute my clone and checkout without needing to switch directories in-between When building images based-off of Debian/Ubuntu you'll often see this same pattern applied to installing OS packages: Here the apt-get update is like the git clone in the previous example -- we want to ensure that we've got access to all the latest packages anytime we add another package or update the version of the vim.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "small-docker-images-for-go-apps", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/small-docker-images-for-go-apps/", "abstract": "Over the past year we've written a few articles about optimizing your Docker images -- usually with an emphasis on creating the smallest possible image. Unfortunately, when using an interpreted language like Ruby (which has been used for many of the CenturyLink projects) there is only so much fat you can trim from your images. To run a Ruby application in a container you still need the Ruby interpreter and a whole host of OS packages and Ruby Gems installed. Even with our best effort to optimize the images, our Docker-packaged Ruby applications often weigh-in at 400+ MBs. We've now started to use Go for some of our latest projects which, among other things, gives us the ability to package our apps into really compact images. With a Go application, you can compile your code into a self-contained, statically-linked binary that has absolutely no external dependencies. Your application code along with the Go runtime and any imported packages are all compiled together into one binary. The primary benefit of a statically-linked binary is that it allows you to deploy your application by simply copying the binary. When deploying with Docker this means you can build an image for your application that contains nothing but the app itself. You can go from using ubuntu (130 MB) or debian (85 MB) as your base image to something like busybox (2 MB) or even scratch (0 bytes). If you take your 4 MB, statically-linked Go binary and package it in a Docker container using scratch as your base you'll end-up with a 4 MB image -- that's 1/100th the size of our packaged Ruby app! After playing with a few Go applications and reading Adriaan de Jonge's blog post \" Create the Smallest Possible Docker Container \", we thought it would be interesting to see if we could create a general purpose tool that could take any Go project and turn it into a slim image. The result of that work is golang-builder . golang-builder is itself a containerized app that can compile Go code into a static binary and package it in a Docker image. Let's say we have a basic \"Hello World\" Go application: and a corresponding Dockerfile : To package this application in a Docker image, we would invoke the golang-builder as follows: Assuming that we're currently in the directory containing our hello.go source file and Dockerfile , the first -v flag will mount our project directory into the golang-builder container as /src . The script inside golang-builder is hard-coded to look for your Go code at /src . The second -v flag mounts the Docker API socket into the container. Since the golang-builder code needs to interact with the Docker API in order to build the final image, it needs access to /var/run/docker.sock . The golang-builder will set-up the appropriate GOPATH for your project, resolve your dependencies, compile your code and then issue a docker build against your project's Dockerfile . The end result should be a new image in your Docker image list: If you're only interested in the statically-linked binary, you can omit the volume mount for the Docker socket and golang-builder will stop after compiling your application: If you're interested in learning more take a look at the golang-builder page on the Docker Hub or check out the source on GitHub .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "ephemeral-containers-in-a-microservices-workflow", "author": ["\n              Ross Jimenez\n            "], "link": "https://www.ctl.io/developers/blog/post/ephemeral-containers-in-a-microservices-workflow/", "abstract": "One of the exciting patterns we have been utilizing and working on within the Panamax project is the ability to use Docker containers to perform a functional workflow or a series of asynchronous tasks in the background. Recently, I ran across this interesting post on the Iron.io blog: The Ephemeral Life of Dockerized Microservices , which inspired me to write this post. Although I was already familiar with the Iron.io Worker service it was great to see how they were using containers, specifically short lived or ephemeral containers, to accomplish their service. We have been using ephemeral containers as well, albeit somewhat differently, as a series of functional containers in a workflow. These functional workflows can be triggered on an event or scheduled and allow work to be done on-demand in the background. In our case, the workflow required initial steps to be completed and the output provided to later steps in order for the complete workflow to be successful. In the Panamax project, we want to help our users setup a remote cluster on a cloud provider so that they can easily deploy Panamax templates to them. Although we have the capability today to deploy templates to remote infrastructures, the setup of the infrastructure and the installation of our Remote Agent/Adapter are all separate processes. Lets run through an example of what it would take to automate this use case end-to-end. This is a simplification but assume there are three basic steps to accomplish this capability: Now let's say the logic for each one of these steps is encapsulated in a Docker container so that each container is responsible only for a specific step. We could fire up these containers in parallel, but are limited by the fact that each step needs to get some data from the previous step in order for it to execute successfully (e.g. we can’t install the orchestrator in step 2 until we know the IP address of the server created in step 1). In trying to address this use case, we set out to build a general purpose framework that we could use to orchestrate container-based workflows like the one described above. Among other things our framework would need to: One of the more interesting problems to solve was how to move data between containers. The solution that we settled on was to use a Unix piping model where we would attach to the standard output stream of the running container, capture any output, and then write that data to the standard input stream of the next container in the chain. As each subsequent step in the workflow is executed, the output of the previous step becomes the input to the next step. Initially we're going to bundle this framework into Panamax, but we'll soon be releasing it as a separate stand-alone project to help others implement this similar pattern. Let’s talk about some of the advantages of using Containers in a functional workflow: Given each step in our process is decoupled it has been easy for us to reuse each logical unit. This is one of the exciting aspects of Microservices architecture: the ability to have composable units of work that can be re-ordered, easily modified and extended. Although, we are writing all our worker containers in GoLang in reality each container is a black box and therefore developers can choose any implementation language they wish. This is the same concept as Panamax's Remote Agent/Adapter model , which is also encapsulated in containers. Hopefully, now you have some idea as to how such a pattern can be applied. Although the example that was discussed solves a very specific use case, there are much more exciting possibilities for the pattern. The ability to truly build on-demand sensor driven or event based architectures excites me tremendously and can have a very broad impact on how computing resources are consumed. If Docker containers allow you to start a container sub-second, then how solutions are architected can be totally rethought in many cases to be more event driven and on-demand. This means less resource intensive solutions, as compute resources would only be needed for the time the ephemeral container exists. There is also the potential to provide functional libraries of containers that could easily be visually composed into all sorts of interesting workflows by less-technical users. Sensor containers could query APIs where Actuator containers could do units of work. Finally, although we are only doing basic sequential workflows once you add in complex workflows and some simple logic/business rule definition along with the workflow schema you can truly create some unique solutions. Combine all these possibilities with the 'run anywhere' ability of a container itself and the possibilities are truly exciting. How are you using microservices? Have you been able to use containers as more than just holders of application stacks ? We would love to hear from you or please jump in and get involved in one of our many open source projects .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "creating-a-panamax-adapter-in-go", "author": ["\n              Gary Paige\n            "], "link": "https://www.ctl.io/developers/blog/post/creating-a-panamax-adapter-in-go/", "abstract": "Learn how to Create a Remote Adapter Our remote adapter model was designed so developers could use their preferred language, but so far, all of the remote adapters were written in Ruby. We recently released a Marathon Adapter using Go. After creating the Marathon Adapter, we split the codebase to provide developers an easy integration to their favorite orchestrator using Go. This article will explain the process of using the Panamax Adapter project to build a remote adapter. Everything you need to create an adapter is outlined in the Adapter Developer Guide . An adapter must implement the REST interface methods described in the developer guide and be deployed within a container so the remote agent can communicate with it. The Panamax Adapter library provides a server which already implements the adapter REST contract and an interface for developers code. Therefore, a development team using the Panamax Adapter has only two steps to create a working adapter. Within the Panamax Adapter library an interface is provided which has the CRUD operations defined in the developer guide. A structure which implements these methods enables the integration between the Panamax Service definition and the orchestration system. Once the interface has been implemented the final step is to hook it into the server. This simply is providing the adapter as a parameter to the server. Once the previous tasks are complete and the adapter build successfully its time to create an image for deployment. The standard process will create an image of at least 500MB size using a Go base image. However, you can create the smallest image possible (~5mb) using the Golang Builder process outlined in the article Small Docker Images for Go Apps . Building an adapter to any orchestration framework does not require learning a new programming language, because the architecture relies on containers. The labs team created the Marathon Adapter using Go to demonstrate this powerful architectural advantage. Using the Panamax Adapter library and the Golang Builder process it is very easy for teams to create a small efficient Golang container for the adapter they require. A sample using the points in this article can be found here: https://github.com/CenturyLinkLabs/sample-go-adapter . Don't have a CenturyLink account? No problem. Just head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "automated-deployment-endpoint-creation-with-panamax", "author": ["\n              Pat Cox\n            "], "link": "https://www.ctl.io/developers/blog/post/automated-deployment-endpoint-creation-with-panamax/", "abstract": "Panamax is happy to introduce a powerful new feature, automated remote agent cluster creation in version 0.2.12 of the UI and API projects. Using Dray (Look for a future blog post on Dray soon!), you can now create a remote cluster on a cloud provider directly from Panamax, while having the Panamax Remote Agent and appropriate service adapter pre-installed. By only collecting a few specifics around your cloud provider account, Panamax can create the cluster with the orchestrator pre-installed and ready for deployment – all in one click! Note: Based on the cloud provider, this process can take several minutes. Feel free to continue your work in Panamax and come back to this screen to check its progress . Note: If you see an error during the remote deployment process, be sure to check the logs to diagnose the error. Panamax does not delete any VMs created during the process, so please visit your cloud provider to delete any VMs or artifacts to avoid unwanted charges. Note: In order to reach your deployed application, additional ports may need to be opened on your cloud provider . In our initial release, Panamax supports CenturyLink Cloud with Kubernetes and AWS for cluster creation. In the coming weeks we plan on adding more cloud providers and support for additional orchestrators. Be sure to check out our upcoming builds for these additions.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "dockerfile-add-vs-copy", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/dockerfile-add-vs-copy/", "abstract": "This blog post will help you understand the differences between two similar Dockerfile instructions – ADD and COPY – how they became what they are today, and our recommendation on which instruction you should use.  (Hint: It's not ADD) When building Docker images from a Dockerfile you have two instructions you can choose from to add directories/files to your image: ADD and COPY. Both instructions follow the same basic form and accomplish pretty much the same thing: In both cases, directories or files (the <src> ) are copied and added to the filesystem of the container at the specified <dest> path. So if both instructions are equivalent, why do they both exist and which one should you use? Read on to find out. If you're not interested in the nuances of ADD and COPY and just want an answer to \"which one should I use?\", all you need to know is: use COPY. Unlike the COPY instruction, ADD was part of Docker from the beginning and supports a few additional tricks beyond simply copying files from the build context. The ADD instruction allows you to use a URL as the <src> parameter. When a URL is provided, a file is downloaded from the URL and copied to the <dest> . The file above will be downloaded from the specified URL and added to the container's filesystem at /tmp/main.go . Another form allows you to simply specify the destination directory for the downloaded file: Because the <dest> argument ends with a trailing slash, Docker will infer the filename from the URL and add it to the specified directory. In this case, a file named /tmp/bar.go will be added to the container's filesystem. Another feature of ADD is the ability to automatically unpack compressed files. If the <src> argument is a local file in a recognized compression format (tar, gzip, bzip2, etc) then it is unpacked at the specified <dest> in the container's filesystem. The command above would result in the contents of the foo.tar.gz archive being unpacked into the container's /tmp directory. Interestingly, the URL download and archive unpacking features cannot be used together. Any archives copied via URL will NOT be automatically unpacked. Clearly, there is a lot of functionality behind the simple ADD instruction. While this makes ADD quite versatile it does NOT make it particularly predictable. Here's a quote from an issue that was logged against the ADD command back in December of 2013: Currently the ADD command is IMO far too magical. It can add local and remote files. It will sometimes untar a file and it will sometimes not untar a file. If a file is a tarball that you want to copy, you accidentally untar it. If the file is a tarball in some unrecognized compressed format that you want to untar, you accidentally copy it. - amluto The consensus seemed to be that ADD tried to do too much and was confusing to the user. Obviously, no one wanted to break backward compatibility with existing usage of ADD, so it was decided that a new instruction would be added which behaved more predictably. When version 1.0 of Docker was released the new COPY instruction was included . Unlike ADD, COPY does a straight-forward, as-is copy of files and folders from the build context into the container. COPY doesn't support URLs as a <src> argument so it can't be used to download files from remote locations. Anything that you want to COPY into the container must be present in the local build context. Also, COPY doesn't give any special treatment to archives. If you COPY an archive file it will land in the container exactly as it appears in the build context without any attempt to unpack it. COPY is really just a stripped-down version of ADD that aims to meet the majority of the \"copy-files-to-container\" use cases without any surprises. In case it isn't obvious by now, the recommendation from the Docker team is to use COPY in almost all cases. Really, the only reason to use ADD is when you have an archive file that you definitely want to have auto-extracted into the image. Ideally, ADD would be renamed to something like EXTRACT to really drive this point home (again, for backward-compatibility reasons, this is unlikely to happen). OK, but what about fetching packages from remote URLs, isn't ADD still useful for that? Technically, yes, but in most cases you're probably better off RUNning a curl or wget . Consider the following example: Here we have an ADD instruction which retrieves a package from a URL followed by a RUN instruction which unpacks it, builds it and then attempts to clean-up the downloaded archive. Unfortunately, since the package retrieval and the rm command are in separate image layers we don't actually save any space in our final image (for a more detailed explanation of this phenomenon, see my Optimizing Docker Images article). In this case you're better off doing something like this: Here we curl the package and pipe it right into the tar command for extraction. This way we aren't left with an archive file on the filesystem that we need to clean-up. There may still be valid reasons to ADD a remote file to your image, but that should be an explicit decision and not your default choice. Ultimately, the rule is this: use COPY (unless you're absolutely sure you need ADD). Don't have a CenturyLink account? No problem. Just head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "is-from-scratch-the-root-of-all-docker-images", "author": ["\n              Gary Paige\n            "], "link": "https://www.ctl.io/developers/blog/post/is-from-scratch-the-root-of-all-docker-images/", "abstract": "ImageLayers, a new project from the team at CenturyLink Labs, gives you the ability to inspect all the layers of an image, including ancestors. And of course, all images must begin with some primordial layer at the base. Generally this image is the Docker scratch image but it can also be a custom image. Using the ImageLayers tool we can demonstrate the various types of root images found in the Docker ecosystem. You can load the example images in your browser using this example link . The most common root image is the Docker scratch image, which is pulled into a Dockerfile using the FROM scratch instruction. In the photo above, all of the images except one (mysql) are derived from this scratch image. But that fact may be hard to observe because Docker has recently changed the way it reports the FROM instruction. Prior to Docker 1.5 an image's history would look like the following: The final line shows an image with size 0 and no command. This is the Docker scratch image which is also reflected in ImageLayers as a 0 size layer with the instruction FROM scratch . A change was made in Docker 1.5.0 which changes the reporting of the scratch image, but it's still there. The golang:latest was created with Docker 1.5.0 or later because the FROM scratch image has been omitted. This is shown in the docker history golang:latest report: The root image remains the 0 byte scratch image but Docker 1.5.0 made the following change: \"Dockerfile FROM scratch instruction is now interpreted as a no-base specifier.\" Therefore images will not show the 0 byte no command scratch layer if they have been created using Docker 1.5.0 or greater. In the history and in ImageLayers.io the last reported instruction will be the one after FROM scratch. In the golang case above the last reported command is the ADD file:96977352301efe982e It is possible to create a custom base image. The appcontainers/mysql:latest image shows a 271mb unknown instruction in ImageLayers. This instruction is the result of Creating a Base Image . Having a custom base image may allow a single layer as a base to improve application deployment by using the shared base. However creating a custom base is akin to squashing an image. It could reduce the size and number of layers but it prevents incremental changes. The merging of the layers possibly creates a single optimized image, but the entire merged image must be updated on changes rather than a single layer. The most common base for Docker images will be the FROM scratch image even if it is reported as a no-base specifier. The history of images which constitute a single image can be found using the docker history command or shown visually using the ImageLayers.io tool. Knowing the ancestry of images may not be an everyday need but when optimizing your images it can be useful to see what additional baggage is brought along.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "zodiac-easy-container-deployment-rollback", "author": ["\n              Alex Welch\n            "], "link": "https://www.ctl.io/developers/blog/post/zodiac-easy-container-deployment-rollback/", "abstract": "With the recent launches of ImageLayers and Lorry, the CTL Labs team has been thinking a lot about the deployment of containerized applications. Over the past month we've prototyped a few ideas that tackle container deployment in different ways. Today we're happy to announce our first project, Zodiac. Built with small teams and single developers in mind, it is designed to be lightweight and plug into the existing Docker tooling. Zodiac makes it easy to deploy and rollback containerized applications. Our focus is on simplicity and ease of adoption, not features. We welcome your feedback in helping us to chart the future path for Zodiac. At this point Zodiac is more or less a wrapper around Docker Compose . This is very much intentional, and, we encourage developers to use Docker Compose for specifying application configuration. Many applications are already described with a Docker Compose file, so it seemed natural to leverage that for remote deployments. Since we're just wrapping Docker Compose, you can do a remote deployment with Zodiac by simply pointing the client at a remote Docker endpoint. Similarly, if you want to deploy to a cluster you can reference a Docker Swarm endpoint. Zodiac has the same limitations with Docker Swarm as Docker Compose does . Zodiac only requires that a Docker endpoint is exposed via TCP. Docker Machine can fulfill this pre-requisite, and we recommend using it, as Zodiac was designed to play nicely with Docker Machine managed endpoints. On the client machine Zodiac will rely on Docker Compose being installed, along with the Zodiac binary. Installation Instructions can be found in the Zodiac README . ] Let's walk through a simple example of deploying with Zodiac. With Docker Compose and Zodiac installed locally, and a Docker Machine-provisioned endpoint set up remotely, I can do my first deploy. So, in my local environment, I have a simple docker-compose.yml file: And the following Dockerfile for the demosite service: And in an index.html file we just have a string of text: The above compose and Dockerfile are intended to be brief and simple for example purposes. Real-world applications will often have multiple services, options, etc. Zodiac offers a verify command to ensure the endpoint is responding and using the correct version of Docker or Swarm. With that we can do our first deploy. Now let's list the deployments At this point we can curl the endpoint, and if we do a docker ps against the remote Docker host we'll see the running containers: We can see here the Image is not what we would've expected. This is because Zodiac snapshots the image's actual layer id at the time of deploy, enabling accurate rollbacks. This is probably the most important component of Zodiac. If an image tag is updated to point at a new layer ID after the deployment has happened, the rollback history would be inaccurate. By recording the actual layer ID we preserve the ability to rollback. I'll update some of the my source files, and deploy again, but this time with a message. Okay, now we've got two deployments in the history (the top being the current deployment). If we curl the endpoint or look at the containers we'll notice our updates. Notice the IMAGE for zodiac_demosite_1 has a different name now (77e9f456...). This is because we built a new image with the edits to the README.md baked in. Let's say the change to index.html was a bad idea, and we need to rollback. Notice the additional entry and message in the list output.\n    $ curl http://12.34.56.78 Initial Deployment. Now the IMAGE for zodiac_demosite_1 is back to the same as it was in the initial deploy. Rollback successful! From here, I recommend kicking the tires yourself. We had specific use cases in mind for Zodiac, but have tried to leave it open-ended. Again, community feedback is welcome via Github Issues . Stay tuned. Next in our deployment series, Brian Dehamer will talk about another take on deployment, one that we use for our Production Docker deployments.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "gracefully-stopping-docker-containers", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/gracefully-stopping-docker-containers/", "abstract": "Much of the focus of Docker is on the process of packaging and running your application in an isolated container. There are countless tutorials that explain how to run your application in a Docker container, but very few that discuss how properly stop your containerized app. That may seem like a silly topic -- who cares how you stop a container, right? Well, depending on your application, the process by which you stop your app could be very important. If your application is serving HTTP requests you may want to complete any outstanding requests before you shutdown your container. If your application writes to a file, you probably want to ensure that the data is properly flushed and the file is closed before your container exits. Things would be easy if you simply started a container and it ran forever, but there's a good chance that your application will need to be stopped and restarted at some point to facilitate an upgrade or a migration to another host. For those times when you need to stop a running container, it would be preferable if the process could shutdown smoothly instead of abruptly disconnecting users and corrupting files. So, let's look at some of the things you can do to gracefully stop your Docker containers. There are a number of different Docker commands you can use to stop a running container. When you issue a docker stop command Docker will first ask nicely for the process to stop and if it doesn't comply within 10 seconds it will forcibly kill it. If you've ever issued a docker stop and had to wait 10 seconds for the command to return you've seen this in action The docker stop command attempts to stop a running container first by sending a SIGTERM signal to the root process (PID 1) in the container. If the process hasn't exited within the timeout period a SIGKILL signal will be sent. Whereas a process can choose to ignore a SIGTERM, a SIGKILL goes straight to the kernel which will terminate the process. The process never even gets to see the signal. When using docker stop the only thing you can control is the number of seconds that the Docker daemon will wait before sending the SIGKILL: By default, the docker kill command doesn't give the container process an opportunity to exit gracefully -- it simply issues a SIGKILL to terminate the container. However, it does accept a --signal flag which will let you send something other than a SIGKILL to the container process. For example, if you wanted to send a SIGINT (the equivalent of a Ctrl-C on the terminal) to the container \"foo\" you could use the following: Unlike the docker stop command, kill doesn't have any sort of timeout period. It issues just a single signal (either the default SIGKILL or whatever you specify with the --signal flag). Note that the default behavior of the docker kill command is different than the standard Linux kill command it is modeled after. If no other arguments are specified, the Linux kill command will send a SIGTERM (much like docker stop ). On the other hand, using docker kill is more like doing a Linux kill -9 or kill -SIGKILL . The final option for stopping a running container is to use the --force or -f flag in conjunction with the docker rm command. Typically, docker rm is used to remove an already stopped container, but the use of the -f flag will cause it to first issue a SIGKILL. If your goal is to erase all traces of a running container, then docker rm -f is the quickest way to achieve that. However, if you want to allow the container to shutdown gracefully you should avoid this option. While the operating system defines a set list of signals, the way in which a process responds to a particular signal is application-specific. For example, if you want to initiate a graceful shutdown of an nginx server , you should send a SIGQUIT. None of the Docker commands issue a SIGQUIT by default so you'd need use the docker kill command as follows: The nginx log output upon receiving the SIGQUIT would look something like this: In contrast, Apache uses SIGWINCH to trigger a graceful shutdown : According to the Apache documentation a SIGTERM will cause the server to immediately exit and terminate any in-progress requests, so you may not want to use docker stop on an Apache container. If you're running a third-party application in a container you may want to review the app's documentation to understand how it responds to different signals. Simply running a docker stop may not give you the result you want. When running your own application in a container, you must decide how the different signals will be interpreted by your app. You will need to make sure you are trapping the relevant signals in your application code and taking the necessary actions to cleanly shutdown the process. If you know that you're going to package your application in a Docker image you might consider using SIGTERM as your graceful shutdown signal since this is what the docker stop command sends. No matter which language you're using, there is a good chance that it supports some form of signal handling. I've collected links to the relevant package/module/library for a handful of languages in the list below: If you're using Go for your application, take a look at the tylerb/graceful package which automatically enables the graceful shutdown of http.Handler servers in response to SIGINT or SIGTERM signals. Coding your application to gracefully shutdown in response to a particular signal is a good first step, but you also need to ensure that your application is packaged in such a way that it has a chance to receive the signals sent by the Docker commands. If you're not careful in how you launch your application it may never receive any of the signals sent by docker stop or docker kill . To demonstrate, let's create a simple application that we'll run inside a Docker container: This trivial bash script simply goes into an infinite loop, but will exit with a 0 status if it receives a SIGTERM. We'll package this into a Docker image with the following Dockerfile: This will simply copy our loop.sh bash script into an Ubuntu-based image and set it as the default command for the running container. Now, let's build this image, start a container, and then immediately stop it. If you're following along, you may have noticed that the docker stop command above took about 10 seconds to complete -- this is typically a sign that your container didn't respond to the SIGTERM and had to be forcibly terminated with a SIGKILL. We can validate this by looking at the container's exit status. Based on the handler we setup in our application, had our container received a SIGTERM we should have seen a 0 exit status, not 137. In fact, an exit status greater than 128 is typically a sign that the process was terminated as a result of an unhandled signal. 137 = 128 + 9 -- meaning that the process was terminated as a result of signal number 9 (SIGKILL). So, what happened here? Our application is coded to trap SIGTERM and exit gracefully. We know that docker stop sends a SIGTERM to the container process. Yet it appears that the signal never made it to our app. To understand what happened here, let's start another container and take a peek at the running processes. The important thing to note in the output above is that our loop.sh script is NOT running as PID 1 inside the container. The script is actually running as a child of the /bin/sh process running at PID 1. When you use docker stop or docker kill to signal a container, that signal is sent only to the container process running as PID 1. Since /bin/sh doesn't forward signals to any child processes, the SIGTERM we sent never reached our script. Clearly, if we want our app to be able to receive signals from the host we need to find a way to run it as PID 1. To do this we need to go back to our Dockerfile and look at the CMD instruction used to launch our script. There are actually a few different forms the CMD instruction can take. In our Dockerfile above we used the shell form which looks like this: When using the shell form, the specified command is executed with a /bin/sh -c shell. If you look back at the process list for our container you will see the process at PID 1 shows a command string of \"/bin/sh -c /loop.sh\". So the /bin/sh runs as PID 1 and then forks/execs our script. Luckily, Docker also supports an exec form of the CMD instruction which looks like this: Note that the content appearing after the CMD instruction in this case is formatted as a JSON array. When the exec form of the CMD instruction is used the command will be executed without a shell. Let's change our Dockerfile to see this in action: Rebuild the image and look at the processes running in the container: Now, our script is running as PID 1. Let's send a SIGTERM to the container and look at the exit status: This is exactly the result we were expecting! Our script received the SIGTERM sent by the docker stop command and exited cleanly with a 0 status. The bottom line is that you should audit the processes inside your container to make sure they're in a position to receive the signals you intend to send. Using the exec form of the CMD (or ENTRYPOINT) instruction in your Dockerfile is a good start. It's pretty easy to terminate a Docker container with a docker kill command, but if you actually want to wind-down your applications in an orderly fashion there is a little more work involved. You should now understand how to send signals to your containers, how to handle those signals in your custom applications and how to ensure that your apps can even receive those signals in the first place.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "developing-rails-apps-in-container-with-docker-compose", "author": ["\n              Mike Arnold\n            "], "link": "https://www.ctl.io/developers/blog/post/developing-rails-apps-in-container-with-docker-compose/", "abstract": "Many developers choose Rails because it allows them to be productive very quickly, but that can come with the trade off of dependency spaghetti, frustration with databases and version management, and file size bloat. Using Docker can alleviate some of these issues by isolating processes from one another, limiting the scope of dependencies, and allowing you to use optimized images. Of course, another layer of abstraction from your code can introduce problems of its own. Below, we'll guide you through the process of starting a new Rails project with containerization in mind, and explain some of the sticking points. A good base image is your best starting point. It should give you all the dependencies you need to build an application in development and run in production. In the case of Rails development, you want the Ruby language itself along with system libraries needed to build gems like nokogiri, pg, or mysql2. We've created a couple versions of popular operating system images for this very purpose. On the Docker Hub, along with the official Rails image, you can find our Rails base images that derive from Alpine and Ubuntu . To use the base image, you can certainly run a container and just SSH right into it and use it like a virtual machine, but that's not the best way to go about using Docker in development. During development, you'll want to use tools such as an editor or IDE, but these have no place in your container at runtime. Instead, you should volume mount your code into a container running the base image and rely on editing normally in your local OS and building and testing in-container. The benefit of this is that you can continue to rely on tools you're familiar with, like debuggers and code completion, without having to compromise the parity of your development and production environments. It's possible to run a container using the base image and pass the volume mount instructions on the command line, but it's easier to orchestrate and automate running your application if you make use of a Dockerfile. In some cases, it's absolutely necessary. For example, when using the official Rails image, in order to control the firing of the ONBUILD instructions, they make this recommendation: adjust your Dockerfile to inherit from a non-onbuild variant and copy the\ncommands from the onbuild variant Dockerfile (moving the ONBUILD lines to the\nend and removing the ONBUILD keywords) into your own file so that you have\ntighter control over them and more transparency for yourself and others\nlooking at your Dockerfile as to what it does. With the CenturyLink Labs images, the Dockerfile is as simple as a single line or two when using the alpine-rails image with PostgreSQL Once the Dockerfile is in place, the image can be built and tagged. Apart from the code you'll inject via volume mount, this image is identical to the image you'll run during testing, staging, and production. A slight modification to the Dockerfile to ADD your code during the image build and running it with RAILS_ENV=production is all that's needed to create the perfect image for your CI/CD process. This is best handled with a separate production version of your Dockerfile (e.g. Dockerfile.prod ) and then passing --file=\"Dockerfile.prod\" to the Docker CLI build command . You're using Docker, so you might as well embrace microservice architecture. You don't necessarily need to include your Rails application's database in the container which houses the Rails code. If you're using sqlite3, you can and should, but if you're using PostgreSQL or MySQL, there are already official images that are simple to configure and run containerized with Docker. There's a bit of work up front to use them with a Dockerized Rails app, though, but it's relatively simple. If you're working with MySQL or Postgres, you want to create a data volume container to house your data. There are plenty of good reasons to use a data volume container when using a containerized database, but suffice it to say that they are designed to persist data. When doing Rails development and you recreate the database container with a data volume you don't lose your data and you can follow the conventional mode of migrating your app's database. Here's an example of the Docker CLI command to run a data volume intended to house PostgreSQL data: This creates the data volume container and sets up the /var/lib/postgresql directory within the container as a volume to be mounted by other Docker containers. Although specific to PostgreSQL, this same concept works for other databases. You simply need to create the volume using the path expected by the database. Once you have a data volume container, mounting it from a database container is not difficult. You simply pass the name of the data container in as the value for the --volumes-from flag of the run command. Below is an example of a container mounting the volume created above. One important thing to note is that when you do decide to get rid of the containers and corresponding data volume containers, you want to remove the last container that references the volume with the -v flag or you could leave ophaned containers on the system just taking up valuable space. With an image, a running database container, and a data volume container all in place, we can fire up our Rails application container. Getting the code into the container is done by volume mounting the directory containing the code into a directory in the container. This is done with the -v flag of the Docker run command. Next, To bind the Rails server port to the container port, you'll need to do two things. First, you pass the -p flag to the run command. Second, you pass the -b flag to the rails server command to bind Rails to any IP address of the resulting container. Finally, we need to connect to the database container. One of the really nice features of Rails is that the database configuration in database/config.yml can be overriden by setting the connection information in the special DATABASE_URL environment variable . When we pair this with Docker container linking and the ease of injecting environment variables into Docker containers at runtime, we get a very simple way of connecting our Rails app to our containerized database. With the following command, we can run our Rails app in a container, mapped to the localhost on port 3000, and connected to the database service. If you're using boot2docker, you'll need to forward port 3000 from the VM to a local port to see the application in the browser, or simply access the GUI via $docker_host_ip:3000. The last thing we need is to be able to execute the usual bundle , rails , and rake commands. Thankfully, Docker has the exec command which is tailor-made for this very purpose. With it, we can execute arbitrary commands on our container. For example, if we make a change to the Gemfile locally, we can run bundler in the container simply by executing the following command which tells Docker to execute the bundle command on the container named 'webapp': Likewise, we can migrate the database with Rake: We can even execute our tests: In each case, Docker starts up the container, executes the command and then shuts the container down again. Unless we remove the container, it's there to do our bidding. Changes we make locally are reflected in the container because we've mounted the code with the -v flag. And with that, we've achieved the goal of a containerized development environment. Before we stick a fork in it, however, there's one last optimization available to us. Orchestrating the startup, shutdown, and linking between these containers can be a bit tedious. Docker Compose can alleviate much of the pain. With a single YAML file, we can reduce the coordinated control of this multi-container application to a simple start and stop. Here's the docker-compose.yml modeling the three containers. To start the multi-container application, we simply call docker-compose up . We can still execute our typical Rails and Rake commands, as well. Instead of using docker exec we just use docker-compose run webapp . If that's just too much typing for you, create a simple alias to call the Compose run command and pass in the name of the Rails app container followed by the command. Some developers advocate using containers while developing locally as if they were virtual machines. However, this has the potential to reduce parity between their Dev and Prod environments, while simultaneously limiting their ability to use familiar tools such as an IDE or debugger. The right way to use Docker in development is for running and testing the application and for standing up dependent services. In this post, we've shown how to set up a Rails development environment that allows you to code locally using the toolset for your environment while also making use of containerization for running and testing the application and a database alongside it.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-vs-vagrant-cloud", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-vs-vagrant-cloud/", "abstract": "Vagrant is a cross-platform tool that lets you specify Virtual Machines ( as a Vagrantfile ) to deploy to a hypervisor (like VirtualBox on your laptop or vSphere or Hyper-V at work). Docker is Linux-only tool that lets you specify Linux Containers ( as a Dockerfile ) to deploy to any host running Docker. So they are similar because they both take a config file. But different because Vagrant gives you Virtual Machines in minutes and Docker gives you Linux Containers in milliseconds. In fact, if you want to use Docker on your Mac, you usually use Vagrant to create a Linux VM first and then install Docker on that host (since it depends on the Linux kernel). Creating a Vagrant Linux box is killer easy: But the lines between VMs and Containers are blurring. With Vagrant's recent 1.5 release there was a lot added, including Vagrant Cloud. Vagrant Cloud is a hosted service for finding boxes, sharing boxes, managing Vagrant Shares (reverse proxy to any port on your vagrant boxes). One of the coolest parts of Docker is how collaborative the process of curating encapsulated Linux environments could become. From my laptop, I could now docker commit and docker push . Then from your laptop, someone else can docker pull and docker run your exact same Linux Container. This is revolutionary. This does to Linux what git has done to source control. Vagrant Cloud introduces the same social curation features to any regular Virtual Machine. With vagrant box update , you can pull changes to your virtual machine like docker pull . You can even host private virtual machine images in the Vagrant Cloud. This is very interesting for a number of reasons: So, although Virtual Machines are slower than Docker Containers, they make up for it in flexibility and isolation. No. Docker is interesting for many more reasons than just docker commit . But this is a clear indication that Virtual Machines are not going to give up without a fight. The worlds of Virtual Machines and Linux Containers are colliding. They are inspiring each other and leading to mutual technical advancements. For example, Serf is a project that comes from the makers of Vagrant but can easily be used with linux containers . The world is headed to a place where Linux Containers co-exist with AND on Virtual Machines. Some startups are already supporting native Linux Container hosting. OpenStack allows Docker Containers as a first-class citizen. We are going to continue to see Vagrant and Docker play really well together and push each other's boundaries, which will only lead to more goodness.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-is-docker-and-when-to-use-it", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/what-is-docker-and-when-to-use-it/", "abstract": "Heard of Docker, but still having trouble understanding it? Don't worry, you are not alone. Today, I want to try to explain Docker in a few different ways. Before we talk about what Docker is, I will talk about what it isn't. What is the negation of Docker? What are its limits? What can't Docker do? So then what is Docker good at? Docker is a basic tool, like git or java, that you should start incorporating into your daily development and ops practices. Java's promise: Write Once. Run Anywhere. Docker has the same promise. Except instead of code, you can configure your servers exactly the way you want them (pick the OS, tune the config files, install binaries, etc.) and you can be certain that your server template will run exactly the same on any host that runs a Docker server. For example, in Java, you write some code: Then run javac HelloWorld.java . The resulting HelloWorld.class can be run on any machine with a JVM. In Docker, you write a Dockerfile : Then run docker build -t my/ruby . and the resulting container, my/ruby can be run on any machine with a Docker server. The Docker server is like a JVM for systems. It lets you get around the leaky abstraction of Virtual Machines by giving you an abstraction that runs just above virtualization (or even bare metal). Git's promise: Tiny footprint with lightning fast performance. Docker has the same promise. Except instead of for tracking changes in code, you can track changes in systems. Git outclasses SCM tools like Subversion, CVS, Perforce, and ClearCase with features like cheap local branching, convenient staging areas, and multiple workflows. Docker outclasses other tools with features like ultra-fast container startup times (microseconds, not minutes), convenient image building tools , and collaboration workflows. For example, in Git you make some change and can see changes with git status : Changes to be committed:\n(use \"git rm –cached …\" to unstage) Add new file README.md to git... Push change to git repo... Branch master set up to track remote branch master from origin... Use git whatchanged commmand to see what changed... In Docker, you can track changes throughout your entire system: Github commit... Docker push to update repo... Docker pull to get image... Docker history to see recent changes... These collaboration features ( docker push and docker pull ) are one of the most disruptive parts of Docker. The fact that any Docker image can run on any machine running Docker is amazing. But The Docker pull/push are the first time developers and ops guys have ever been able to easily collaborate quickly on building infrastructure together. The developers can focus on building great applications and the ops guys can focus on building perfect service containers. The app guys can share app containers with ops guys and the ops guys can share MySQL and PosgreSQL and Redis servers with app guys. This is the game changer with Docker. That is why Docker is changing the face of development for our generation. The Docker community is already curating and cultivating generic service containers that anyone can use as starting points. The fact that you can use these Docker containers on any system that runs the Docker server is an incredible feat of engineering.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "top-10-open-source-docker-developer-tools", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/top-10-open-source-docker-developer-tools/", "abstract": "Navigating the ever-changing Docker ecosystem can be difficult, so we at CenturyLink would like to make it easier for you. A few weeks ago, I wrote about the top 10 startups using Docker . This week we are talking about the top 10 open-source projects using Docker . I will group them to highlight what each project focuses on. Although Docker was a technology originally built out of a PaaS (DotCloud), there have been multiple attempts to create micro-PaaS'es out of Docker. 1. Flynn - https://github.com/flynn 961 stars, 24 forks in Mar 2014 \"Flynn is like Sinatra where Cloud Foundry is like Rails\" Flynn is one of the most anticipated Docker PaaS'es right now. With nearly 1,000 stars and dozens of forks, this open-source Docker project has not even been released yet. With a git push deployment to Docker, it is easy to see why there is so much anticipation. Flynn simplifies deploying and maintaining applications. Instead of using complex configuration management systems, Flynn allows self-serve management of containerized deployments, making life easier for ops and developers. Flynn is also different than the other projects on this list because they are a sponsored open-source project. With 14+ sponsors having donated over $80,000, this project is definitely one to watch. 2. Deis - https://github.com/opdemand/deis 1,341 stars, 120 forks in Mar 2014 \"Your PaaS. Your Rules.\" With over 1,300 stars and over 120 forks, Deis is more established than Flynn and also has a git push deploy style. Deis leverages Chef, Docker, Django, Celery, Heroku Buildpacks, and Slugbuilder to do its magic. Deis comes with out-of-the-box support for Ruby, Python, Node.js, Java, Clojure, Scala, Play, PHP, Perl, Dart and Go. Also, Deis can deploy anything using Heroku Buildpacks, Docker images or Chef recipes. Deis can be deployed on any system including every public cloud, private cloud or bare metal. Deis currently supports automated provisioning on EC2, Rackspace and Digital Ocean. In an upcoming blog post, we will compare Deis and Flynn in more detail. 3. Dokku - https://github.com/progrium/dokku 4,806 stars, 384 forks in Mar 2014 \"Docker powered mini-Heroku in around 100 lines of Bash\" If you just want the smallest viable git push to Docker container functionality in the world, take a look at Dokku. From the maker of localtunnel (a super useful utility to reverse-proxy your localhost ports to a public url), this little power-house of an open-source project has the most eye-balls. It is not as feature rich as Deis or Flynn, but it is incredibly easy to install and use on Ubuntu 13 or 12.04 x64. Note: Users on 12.04 will need to run apt-get install -y python-software-properties before bootstrapping stable. 4. CoreOS - https://github.com/coreos 2,564 stars, 237 forks in Mar 2014 \"CoreOS enables warehouse-scale computing on top of a minimal, modern operating system.\" CoreOS is developing best practices for deploying containerized applications to production. It is not a single open-source repository, but rather a collection of many open-source tools that can be used together including etcd , docker , and systemd . We have written about generating CoreOS files from Fig files before because getting started with CoreOS can be a daunting experience. The etcd library is used as a universal key/value store to stitch services together and share service credentials across an entire application. Unlike many of the other projects in this list, CoreOS is both an open-source project and a venture backed startup (which is why CoreOS is also listed in our Top 10 Startups Built on Docker post). 5. Fig - https://github.com/orchardup/fig 1,526 stars, 51 forks in Mar 2014 \"Fast, isolated development environments using Docker\" I have written a few times about Fig because it is one of my favorite little utilities for Docker ( Auto-Loadbalancing Docker with Fig, HAProxy and Serf and Building Complex Apps for Docker on CoreOS and Fig ). Fig lets you write a simple fig.yml file that lists all the Docker containers your app needs and how they should link together. Once you write the fig.yml you just fig up -d and your app will be up and running. This blog is managed by fig right now. 6. Serf - https://github.com/hashicorp/serf 1,652 stars, 91 forks in Mar 2014 \"A decentralized solution for service discovery and orchestration that is lightweight, highly available, and fault tolerant\" Although Serf is not Docker specific, it is like jelly to Docker's peanut butter. Serf is one of my favorite new open-source projects and came from the guys who make Vagrant. I wrote about it a few weeks ago in Decentralizing Docker: How to Use Serf with Docker so you can get a great insight into how to use it with Docker there, but essentially it is a hammer you can use where CoreOS and etcd is a nail-gun. Serf is also really easy to use outside of Docker and can be used in a lot of different ways where etcd and CoreOS are pretty specific tools that aren't nearly as flexible (though definitely very powerful). 7. Drone - https://github.com/drone/drone 2,516 stars, 133 forks in Mar 2014 \"A Continuous Integration platform built on Docker\" Drone (another project that is both an open source project and a startup) gives you a simple go binary, distributed in a debian file, that gives you a full CI/CD pipeline hooked natively into Docker. Cool, right? Your code never needs to leave your laptop or your company's network to be tested, which is a huge deal for big company developers who have policies that prevent them from using public hosted services like GitHub and Travis. The other cool part of Drone is that you can deploy the fully tested containers into production and be assured that the exact same environment is used in both locations. Finally, Drone lets you build custom Docker containers with whatever custom binaries and configuration you need, which is way more flexible than most CI platforms today. 8. Shipyard - https://github.com/shipyard/shipyard 1,443 stars, 96 forks in Mar 2014 \"Open Source Docker Management\" Shipyard gives you the ability to manage Docker resources including containers, images, hosts, and more all from a single management interface including: Multi-Host Support, Container Metrics, and a RESTful API. I love this part, to deploy Shipyard, you just run: Then you should be able to login to http://localhost:8000 and get the pretty UI (more QuickStart docs available). Slick! Being able to visually see all your containers is killer and Shipyard is a great way to do that. 9. Kubernetes - https://github.com/GoogleCloudPlatform/kubernetes 3,598 stars, 501 forks in Sept 2014 \"Google's Docker Orchestrator\" Kubernetes is an open source implementation of container cluster management. In other words, it is a system for managing containerized applications across multiple hosts, providing basic mechanisms for deployment, maintenance, and scaling of applications. Its APIs are intended to serve as the foundation for an open ecosystem of tools, automation systems, and higher-level API layers. 10. Panamax - https://github.com/centurylinklabs/panamax-ui 510 stars, 43 forks in Sept 2014 \"Docker Management for Humans\" Panamax is a containerized app creator with an open-source app marketplace hosted in GitHub. Panamax provides a friendly interface for users of Docker, Fleet & CoreOS. With Panamax, you can easily create, share and deploy any containerized app no matter how complex it might be. If you measure the health of an open-source project by the open-source projects created around it, I think it is fair to say that Docker is the picture of health. This article was meant to be more of an overview than an in-depth comparison. In future weeks, we plan to talk more about the differences between some of these technologies like Flynn vs Deis.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "create-a-compose-yaml-with-panamax", "author": ["\n              Pat Cox\n            "], "link": "https://www.ctl.io/developers/blog/post/create-a-compose-yaml-with-panamax/", "abstract": "We are excited to announce a great new feature in Panamax: the ability to generate a Compose YAML from your Panamax application. Like our PMX template, a docker-compose.yml is used to define and run your multi-container applications with a single file. Docker Compose and the docker-compose.yml are Docker's standard for application deployment and Panamax now gives you the flexibility to leverage it directly. As a demonstration, run an image or PMX template in Panamax. For this example, we will use our trusty WordPress with MySql PMX template. Once you have deployed it locally, you are redirected to the application detail page. From here you can make any modifications to the application you like,i.e. open more ports, add environmental variables. When its setup like you want it, click the gear and select Save as Compose YAML. Panamax then auto-generates your docker-compose.yml from your application and displays it for you in a modal. From here you can copy it to clipboard, save as a local file or open it in Lorry , our docker-compose inspector and validator. That's it! With this feature, Panamax gives you even more flexibility on how you want to save your application for future deploying.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "deploying-multi-server-docker-apps-with-ambassadors", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/deploying-multi-server-docker-apps-with-ambassadors/", "abstract": "The  number 1 problem faced by people trying to use Docker in production today is how to run multi-server Docker apps. Docker's linking mechanism only works within a single-host environment, so if (for example) you want to link your database to your wordpress, they have to be on the same host. But there is a new ops pattern being established to fix this problem. It is called ambassadors. Both Docker and CoreOS have tutorials [ 1 , 2 ] for using the ambassador pattern. So what are ambassadors? Ambassadors are containers who's sole purpose is to help connect containers across multiple hosts. If you think of each container as it's own country, the ambassador of the country is in charge of communicating with foreign countries. How's that for explaining a self-explanatory metaphor? Here's a visual representation of this metaphor: (container) --> (local ambassador) --network--> (foreign ambassador) --> (container) The docker -link command works well within a single Docker host, but when your app needs to scale across multiple servers, linking gets you nowhere. This is where ambassadors come in. There is a clever little one-liner container based on busybox called ctlc/ambassador which sets up a simple reverse-proxy with socat . First, you need to create the basic service container (let's say MySQL): Notice that we are not exposing the MySQL port to the external world here. That's the ambassador's job, not the service's. Now, your service container needs an ambassador on the same host: Finally, run this command on a totally separate server to create a \"Foreign Ambassador\" for your MySQL service: The end result is a socat proxy running this little reverse proxy: Notice that the 192.168.1.52 IP address should be an address accessible to the foreign host, but ideally not to the entire world. In AWS, you would have to create a private network to do this. In CenturyLink Cloud , you get subnets for free by default, so this is less work. If you've been following along our progress on the blog, we've been using Fig, Serf and Haproxy to build increasingly complex applications . Now we will finally explain how to create multi-server AND multi-container apps. First, on your WordPress server, run fig up -d with this fig.yml: Notice that we can link to the db-ambassador container as if it were the MySQL database itself. On the machine that runs your MySQL server, you will need to run fig up -d with its own fig.yml . Now you have Docker containers on separate servers communicating with each other. Yay! What else can you do with ambassadors? The real power of ambassadors is when you use them to auto-discover services and to create audit and compliance barriers. Here are some of the cool things you can do: For example: Imagine being able to enforce a compliance barrier that prevents DELETE actions to a MySQL database. Your ambassador can be more than a passive reverse-proxy and can enforce what actions your code can take with your services. In future posts, we will dig into showing exactly how to go beyond this simple example. We will show you how to turn any container on the Docker Hub into a Serf-aware container and how to incorporate any container image into CoreOS's etcd . We will also talk about how to do audit and compliance with ambassadors. Join the mailing list to make sure you get updates when these posts are made.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "ansible-cant-make-breakfast", "author": ["\n              Adam DuVander\n            "], "link": "https://www.ctl.io/developers/blog/post/ansible-cant-make-breakfast", "abstract": "We're big fans of automation, especially Ansible. When I found Allan Denot's post on the configuration management and orchestration engine, I knew I wanted to learn more about his experience. Here's our conversation with Allan on the differences between Ansible and Puppet or Chef. He shared his excitement about Ansible 2 and, believe it or not, that wishes the tool could make breakfast for him too. With Ansible you can describe infrastructure and configuration in a very simple format: YAML. Instead of writing commands to run, you write the final state desired for the server. Ansible uses its modules to ensure it's achieved. If nothing has changed, Ansible doesn't touch the server configuration. It verifies item by item and reports back. Since the files can be version controlled, we now have a workflow that is similar to a software development workflow where any changes to infrastructure can be tracked and approved by other engineers. Ansible defines infrastructure as data, not as code. That alone removes a big chunk of complexity from configuration management. Code is only necessary when you need to extend core modules. Ansible is agentless and allows both push and pull modes. Push mode is more common and configuration is applied to hosts using SSH. There's also no need to install anything in the target host other than Python. For speed concerns, Ansible provides several ways to speed up connections. The company I worked pushed Ansible as a configuration management and deployment tool. We were tasked to upgrade all of our deployments from Bash scripts to Ansible. Because of the flexibility that Ansible provides, in order to achieve some outcomes with minimal code in a simple way, we needed to add a level of creativity when writing the playbooks, which I think is great. I lost count of the number of times I kept trying something different only to realize \"Oh, I didn't know Ansible could do that\". Ansible isn't used to write bash commands as much as idempotent tasks. The frame of mind is that you're defining the desired state of your servers; so ideally when you run a playbook twice, the second time it has to return all OK and nothing changed as the state is already reached. Blocks! They are language feature that allow you to group tasks and pass common parameters once they apply to all tasks in the group. This makes playbooks much cleaner and easier to read. I've thought about that quite a bit. It would be something like: But seriously, While Ansible can't make your breakfast yet, there is a lot to be excited about. Be sure to check out Runner , the CenturyLink Cloud solution automation service built on Ansible. Allan Denot is a DevOps engineer living in Australia, though he's originally from Brazil. In 2001, Allan released a Linux distribution. He continues to be involved in cloud software. He has created products, started companies, and currently blogs on DevOps thoughts at allandenot.com .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "secure-coding-fundamentals", "author": ["\n              Eric Raisters\n            "], "link": "https://www.ctl.io/developers/blog/post/secure-coding-fundamentals/", "abstract": "\"Security is a process, not a product.\" -Bruce Schneier, Cybersecurity expert According to statistical studies led by Dr. Gary McGraw at Cigital, half of all vulnerabilities are due to coding errors - bugs. The other half are design flaws which can only be discovered and fixed through threat modeling, or an architectural risk analysis process – which is a topic for another blog. Just as with any bug, the cost to fix a vulnerability goes up exponentially the further down in the development cycle it is found and addressed. Unlike a simple bug, the cost of a vulnerability being successfully exploited has the potential to be much greater, especially once it gets into a production environment.  And the cost is not just monetary, but could also be reputational and legal. A secure software development lifecycle (SSDLC) process is a development methodology, an independent set of tasks in all phases of the software development lifecycle that includes several secure coding components. \"Security needs to be done by people who don’t have “security” in their job title. Doing what you do the right way is 90% of getting things secure\". -Paco Hope, Secure coding author Not all developers need to be security experts, but all developers need to be aware of their responsibility in writing code that reduces the chance of exploitable vulnerabilities showing up in applications. Training in secure design principles, common attacks against specific coding languages and use of tools, like static code analysis, to help find and remediate vulnerabilities is needed within the field, as these topics are usually not covered in any computer science or coding classes in school. Half of the OWASP Top 10 web application vulnerabilities and more than half of the MITRE/SANS Top 25 programming errors are due to insufficient input validation of user controlled data. So particular attention needs to be paid to tools and libraries available to validate and sanitize this data before it is used. Developers (and testers) need to develop a mindset of “What can an attacker do with this?” anytime user controlled input is requested or used. Fuzzing (also called random mutation testing) of input files or protocols is good way to find many input validation issues, and it is eminently automatable. Static code analysis tools are now built into most integrated development environments (IDEs) for the major coding languages and should be enabled, configured, and used religiously by all developers – before they check any code into their code management systems (CMS). These are effective at preventing known vulnerability attack vectors from being included into submitted code, as well as resulting in higher quality code. Code quality is directly correlated to application security. So good, clean code will be inherently more secure. Code review from a security point-of-view is also an important aspect of secure coding, as the Heartbleed vulnerability so clearly demonstrated. No static analysis program was able to find this seemingly simple input validation error prior to the publication of the vulnerability. However, having an additional validation step of code review for security issues by multiple reviewers, versus a lone reviewer, could have made the difference. Even with a fully implemented SSDLC there will be times when a previously unknown (or unfixed) bug becomes tomorrow’s vulnerability du jour . Developers will still need to quickly react to quantify impact, determine a remediation or develop a patch, and perform a root cause analysis so similar weaknesses can be found and corrected quickly. Attackers will often look for additional related  vulnerabilities, in case the responsible developers only fix the specific vulnerability exploited. The goal is that with a well implemented SSDLC we can reduce the frequency of these resource-diverting events and reduce risk to both ourselves and our customers. Learn more about how CenturyLink Cloud can provide the enterprise level security , speed, performance, and availability to meets your business demands.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "agile-product-launch-thrill-ride", "author": ["\n              Trey Willis\n            "], "link": "https://www.ctl.io/developers/blog/post/agile-product-launch-thrill-ride", "abstract": "Roller coasters are designed to give you a thrill. Part of the excitement is that they make you feel like you're in danger. Many product teams will tell you that the emotions and experiences involved in the product development life-cycle are much like the exhilaration and fear of being on a roller coaster. Excitement is high as we start the ride. We can be flying high one day as the team gets close to a working product, and suddenly we could hit a low point when an unexpected curve ball gets thrown at us. Anyone can focus for a few weeks. The harder challenge is finding a way to keep that same sense of purpose and exhilaration as things continue to shift and when the outcome isn’t quite certain. Here is a look at what it feels like to be on the wild ride of the product development journey that we take on each day. I’ve Got a Great Idea! Who’s with Me? At the onset of a new product development project there’s a good bit of energy and excitement. The team is formed. Most members enter with good attitudes and are very interested in strong teamwork and heavy collaboration. They want to make the best possible product. We jump out of the gate by making great progress on the idea and by creating the technical foundation for delivery of the service. We’re Almost There! Man, This is Going to be Great! A few weeks or months into the real work and it feels like you’re almost there. You have a working product that’s still a little rough, but functional. It’s alive! The team is still riding high together and we believe that we can create something exceptional. We believe in the mission and in the vision more than ever. The plan is perfect. We seem to have it all figured out. The team can see the light at the end of the tunnel and they’re committed to work really hard to get there. Wait. Maybe that Wasn’t the End of the Tunnel After All. As we get close to wrapping up the last few features, we hit a small bump in the road. Something unexpected happens during development. All of a sudden, the mood starts to shift. The rest of the plans start to feel a little shaky, since we didn’t see the first bump coming. Doubt starts to sneak in to the heads of a few team members. Your team might even start to question, is this product really going to make money? Other questions start coming in from other teams or stakeholders. You aren’t so sure this is such a good idea. You may wonder, maybe we should cut it now. While these scenarios can be true for us at times within CenturyLink, we have adapted to a work style that helps us cope. Don’t expect to have a perfect product on Day 0. \" If you are not embarrassed by the first version of your product, you’ve launched too late .\" - Reid Hoffman In order to enable our teams to build products that make a difference to our customers, we want to get those services into the hands of users as soon as possible. We build and release a minimum viable product (MVP) as soon as the product is able add value for a user. We do this so that customers can recognize that value as soon as possible. Releasing an MVP also allows us to start the process of constant feedback and progressive refinement of the product. This ensures that what we build really matters for our customers in the end. Be ready to adjust to feedback. We love our customers and we love their candid feedback even more! Through the use of Agile development practices, we not only encourage taking in customer feedback, we thrive on those interactions. We use surveys, customer ticket data, and in-depth customer interviews to ensure that we’re building products that help solve real business needs. We listen carefully and work to clarify the feedback so that we understand what is most critical to a customer's success. It’s imperative that we understand the objective and the business scenario when listening to customer feedback. Understand what drives your metrics and steady state expectations. We operate within a DevOps model on our team. We all attend daily stand ups to discuss current work in process and we collectively participate in release activities in order to create and maintain a shared understanding of what we are working on and how the service is performing. As a result, all members of the team understand the key performance and availability metrics to measure and track them against larger business goals. We are constantly examining what we are doing today and how it can become better. Also, we don’t create unrealistic plans. Plans and reality can often be far apart. We are willing to chuck our original plan out the window, change assumptions, and shift things based on how our launch is unfolding. We use this feedback and data to shape our plan instead. Iterate quickly. At CenturyLink, our development velocity is a feature. We run through a full development iteration every 15 days. Most importantly, we prioritize the customer feedback and evaluate what work will provide the highest return on investment every 15 days. Then, we gracefully adjust accordingly to what we have learned during the previous iteration. We are able to take advantage of specific customer feedback very quickly. It requires so much effort and teamwork to launch a product that it’s easy to understand why such high emotions get involved. And, we should celebrate our efforts and reaching such a critical milestone as in launching new product to market. But perspective is everything. Make sure that the team stays informed and aligned with what’s next. As long as the team knows that the ride has really just started, it’s easier to keep folks engaged with making the product better. As a result, our customers get access to new features much faster than they can internally or by other methods. You wanna know the best part about this roller coaster? It never stops. Check out ctl.io for the latest news on what we’re working on now.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "design-thinking-devops", "author": ["\n              Adam DuVander\n            "], "link": "https://www.ctl.io/developers/blog/post/design-thinking-devops", "abstract": "Anyone who writes code wants the end result to be relevant and valuable. There are plenty of factors that get in the way of creating something great, but the pursuit of an exemplary output is important. We've discussed our DevOps journey before. We believe it's applicable to others, which is why we share it here. That's also why we've invited Jeff Sussna to our Bellevue office the evening of November 4 for a public event covering the Key Principles of Continuous Design. Join us Wednesday, November 4 from 6:30pm - 9:00pm Sussna is an internationally-recognized systems thinker and IT expert. His talk will discuss the benefits of applying a customer focus to coding, which incorporates feedback loops and quality throughput. The design-operations loop unifies marketing, design, development, operations and support. In other words, the code we write becomes only one piece, or building block, of a relevant end-product. In the same way that DevOps combines development and operations , your business can incorporate Design Thinking and ultimately create real customer value. This means having conversations with your customers where you both gain new knowledge and acting on those insights to strengthen the way your company operates. RSVP now to attend for free; walk away with knowledge that helps you incorporate Design Thinking and DevOps into your own business.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "day-in-the-life-of-rebecca-skinner", "author": ["\n              Jessica Weber\n            "], "link": "https://www.ctl.io/developers/blog/post/day-in-the-life-of-rebecca-skinner", "abstract": "The CenturyLink Cloud team is made up of more than just developers and engineers. Granted, we do have a lot of developers. And engineers. And all of those other super-smart people who have a tendency to send my mind reeling while I'm trying to keep up with their conversation. My inferior logic capabilities aside, we've decided to showcase another aspect of our work here at CenturyLink; our \"Day in the Life\" series will give you a glimpse into our everyday lives as Cloud team members. This series will focus on all different avenues and roles within our company, from developers, to technical writers, to the awesome administrative staff that, in reality, saves us from ourselves. First up is Rebecca Skinner, an engineer on our Advanced Storage team. Rebecca came to CenturyLink a short time ago, but has already made a big impact on not only her team, but on our Cloud team as a whole. During her second week here she started a weekly workshop she calls \"Functional Fridays\" that focuses on helping devs get up-to-speed on functional programming. What makes this even more impressive is the fact that she came to the Advanced Storage team not knowing the language or the tech stack they were using, nor did she know the product they were developing. She credits her ability to learn so much, so fast to her team's willingness to share knowledge -- something she values highly and appreciates about the Cloud team. Although she didn't know the specifics of the Storage team's project at first, Rebecca has a strong background in functional programming, systems development, and computer science fundamentals, all of which helped her quickly adapt to her team's approach. In a former position she put these same skills to good use developing a 3D video capture and compression system that ran on a GPU connected to an unmanned ground drone. Basically, she turned a drone into a 3D video feed that would zip around her office and transmit 3D video back over a low-frequency radio signal (for a more in-depth technical explanation, see Rebecca). She also does a noteworthy Gollum impression (\"it's scary good\"), which only adds to her impressive list of skills. Rebecca shines in the communication, collaboration, and knowledge-sharing aspect of the Cloud team, and she's already been a valuable resource in promoting language learning and infecting others with her passion for identifying underlying commonalities of problem sets. Looking at something in a different light and discovering new connections between problems is something she has a particular strength in; she really enjoys getting people excited about the problems that our teams face and their potential solutions -- \"What's really valuable is being able to get people excited about the problem -- to want to know more about why a solution has value.\" To get a feel for a typical day in Rebecca's world, she provided the timeline below. 5:15 am : I wake up in the morning and start getting ready; thanks to our casual office environment, the weather and my mood are the main considerations for what I decide to wear. I usually check Slack on my phone as I'm getting ready in case I missed any interesting conversations from the night before in some of the social channels. 6:00 am : Two eggs and coffee for my breakfast, some chopped veggies for our bird, and then out the door to drop my spouse off at the train station on my way into the office. 7:00 am : My third cup of coffee for the day is enjoyed with the other \"morning people\" in our office, usually in the lunch room where we hang out and talk about important things, like coffee...and sometimes Fallout [4]. 7:15 am : I may check our Trello board for stories that need testing or are 'ready-for-work'. Since there is another early bird on my team we can pair first thing, but today I'm busy preparing some workshop exercises for the 'Functional Friday' session I'll be leading after our stand-up. 8:30 am : After finishing up some exercises for the 'Functional Friday' session and reading a new paper on type systems, I grab my fifth cup of coffee of the day and find someone to pair with on a story. 9:00 am : A few more people from our team have wandered into the office by now, and we catch up on whatever was at the top of the Reddit front page that morning, and then maybe pair switch once before stand-up. 9:30 am : I sit down through our stand-up. First world anarchy. 9:35 am : After stand-up I announce that the topic of today's 'Functional Friday' will be an introduction to Haskell with a Randori Mob programming exercise. 10:00 am : I facilitate a group workshop and mob programming exercise to help the other team members start to learn Haskell. 11:00 am : After 'Functional Friday,' I see who wants to pair on a card. We move the card from 'ready-for-work' to 'working', pull the code down from Github, and start writing tests. 11:30 am : Lunch Time! I usually eat some soup and salad that I brought, but I'll always check out the catered option we get a few days a week to see if it's tempting enough to break my willpower for the day. 12:45 pm : I see what stories everyone else is working on and decide what I want to pair on, or pick someone and we grab a new story. We write tests and then make them pass in a ping-pong method until we've met the acceptance criteria for the story. 3:45 pm : After meeting all of the criteria on the card we mark it as 'ready for test'. I pair switch onto a card with someone else whose story is ready for test. We review the code for refactoring opportunities and validate that it's working in our QA environment. If all works well, we let Jenkins deploy it into production. 4:00 pm : I leave for the day to pick my spouse up from work and enjoy an evening at home. 5:00 pm : I get home, put on PJs, and spend some quality time with my spouse and our bird. I'm in awe of his ability to say so many things that aren't what I'm trying to get him to say right then. 5:30 pm : I start cooking dinner and post photos on the food blog that doubles as our team Slack channel. 7:00 pm : I post in Slack about side projects I'm working on or any interesting articles I find while hanging out at home watching Netflix. 8:00 pm : Start getting ready for bed. Surf Reddit on my phone from bed. 9:00 pm : Finally put my phone down and get to sleep. Rebecca lives in St. Louis with her spouse and their Pinous parrot, George, a rather curious and sometimes stubborn bird who uses phrases like \"Whatcha doin' mommy?\", \"You wanna come out\", and \"nummy nummy\" to make his desires for habenero peppers and blackberries known. Be sure to stay tuned for future installments of our \"Day in the Life Series,\" and let us know if there's someone in particular you'd like to hear from!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "day-in-the-life-of-dlm", "author": ["\n              Content Team\n            "], "link": "https://www.ctl.io/developers/blog/post/day-in-the-life-of-dlm", "abstract": "Daniel is a member of the Content Team . It's a relatively new team  at CenturyLink Cloud . They're the engine for technical content across the cloud platform, public and the Developer Center . Their main focus is improving the developer experience by facilitating great documentation, tutorials, sample apps, copy edit, and blog posts. Basically, they take the fear and anxiety out of content creation. The word on the street is that they do all the \"heavy lifting\" for you to make that happen. They help the other cloud product teams navigate their content through corporate marketing, legal, social, and imagery. They also provide editorial and topic guidance along the way, and encourage the different teams wherever they can to build documentation into their sprints. It's all about driving traffic to the web site and the results coming from that. The Content Team shares a team room with the Digital Team on the 4th floor. Their room is affectionately called \" Words and Numbers \". It's a good set up. The two teams often work closely together. The time clock below is a general rule of thumb for his day. But it's pretty typical --- when he drives to the office that is. The team also works from home. Ah, the perks of working here . He says the only difference on a WFH day is that he tends to dress a little more on the casual side and his commute is a lot shorter! 6:45 AM - Wake up, hop in the shower, and get ready for work. I like my colleagues. I love what I'm doing. I'm on a great team. I like the culture and the environment at the office. It's nice looking forward to going to work. 7:00 AM - Sit around a bit and have a cup of coffee with my wife. 7:30 AM - Hit the road for the drive to the office. A good traffic day is 15 minutes or less. A bad traffic day it's 30. (I grew up in southern California. So I'm familiar with commute traffic. A bad day on the road in STL just doesn't compare.) 7:45 AM - Get everything set up for the day: Chrome, Slack, Trello, Beegit, GitHub, Atom, and a Terminal window for running localhost. Oh yea, there's Outlook too. 8:00 AM - Catch up on Slack messages, email, and see what's new on our Trello boards. Add my name to a card or two, if any are ready for assignment. 8:15 AM - If it's Bagel Wednesday, make the long walk down the hallway past the game room to the cafe. Grab a blueberry bagel and maybe an expresso. 8:20 AM - Start working my cards. Typically, I work on four or five at the same time. Each is in a different stage of completion. Some have dependencies. I prioritize my workload by level of difficulty and due date. 11:00 AM - If it's Monday, I join the weekly Content Team sync. We discuss the status of cards in the current sprint, any blockers, topics on the upcoming sprint, holes to be filled, interviews, and strategies to engage other product teams. 12:00 PM - Lunch time! If it's Tuesday or Thursday, lunch is catered in Moe’s Tavern. The menu rotates each week (Chick-fil-A, Pei Wei, BBQ, Indian cuisine, etc. And there's always with a vegetarian option with any of them.) Free food! Nice perk! On the off days there's always the eMart Store. By the way, the St. Louis Developer Center contributes monthly to a pre-paid account! That's pretty sweet as well. 1:00 PM - Collaborate with the Digital Team. Work on refining intra-team processes, GitHub challenges, NPM issues, troubleshoot why a story or parts thereof aren't visible. Follow-up on a merge or deploy request. Over-the-shoulder access to one another makes it pretty easy. 1:30 PM - Continue working cards and stories. Sometimes there's research to do. I go back and forth with an author via Slack or in person, if the author is here in STL. Sometimes a card only needs minor editing, an image, markdown formatting, or a combination thereof. Other times there's more work needed. After that, I move cards into the Peer Review and/or Corporate Marking process. 2:00 PM - Picked a new card. I like topics that require original content. I do a lot of creative writing in my free time. It's something I enjoy. (But I digress.) Generally it takes about two weeks to move a story through our editorial process before posting. Turn-around time can be quicker. It depends on where the content is going to live: on the public site or the Dev Center. 2:30 PM - Hand off to the Content Team peer review. Address any comments. Switch to GitHub and create a branch in the appropriate repo. Get to work. We use Atom and Gulp to view content locally. It's a handy way to troubleshoot problems, like typos in the markdown header, correcting the path to broken links and images, or fixing html syntax for an embedded video. After everything looks good I commit the changes to GitHub and create the pull request. 3:00 PM - Work on migrating blog posts to the Dev Center. I've done lots of these. Hold up! Where did the branch conflicts come from? But not to worry. I use my trusty cheat sheet on Resolving Branch Conflicts to fix them. When everything's good the PR gets merged and deployed. By the way, if you're interested in the cheat sheet, you can find it by visiting the Content Team Xtranet site and clicking the Underground tab. 4:30 PM - Shut down the Mac and head home. 5:00 PM - Hop on the bike and hit the road. It's the first thing I do when I get home whether I feel like it or not, weather permitting. On average I ride 4 or 5 times a week, an 11 or 12 mile circuit. I shoot for 5. The route takes about an hour -- lots of hills. 6:00 PM - Recover. Grab a power nap (maybe). Catch up with my wife, the events of the day, have a beer, solve the problems of the world, or, on occasion, watch a movie. (We think old classics or PBS are the best. Other movies get the thumbs up too, but not many). 7:30 PM - I spend a fair amount of time on professional interests outside of my involvement in the cloud: writing, volunteering, working with people, preparing myself to do that, correspondence, telephone calls, etc. There's always work around the house too, and more time when the days are longer. 9:30 PM - I read quite a bit. I prefer ancient texts. I know ancient Greek (Attic and Koine). With that background the word \"classic\" has a slightly different connotation for me. A piece of literature isn't really a classic unless its been around a good 700 years. That said, there are lots of exceptions to the \"age-old\" rule. 10:30 PM - Lights out. With everything going on, not out of the ordinary if that doesn't happen until around midnight. Daniel lives in St. Louis with his wife, Laura. They don't own a television. He's also a canonically ordained Orthodox Christian priest; so he's usually busy on the weekends. They both love cycling: Forest Park, Creve Coeur Lake, all points in between, the wetlands, Katy Trail, or the Great River Road. Heat and humidity isn't an issue. The hotter the better. You sweat buckets. They're used to it; and they love it. You just have to stay hydrated. That's all. Stay tuned for future installments of our \"Day in the Life Series,\" and let us know if there's someone in particular you'd like to hear from!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "day-in-the-of-life-brianh", "author": ["\n              Daniel Morton\n            "], "link": "https://www.ctl.io/developers/blog/post/day-in-the-of-life-brianh", "abstract": "Our \"Day in the Life\" series gives you a glimpse into our everyday lives as Cloud team members and our culture here in the St. Louis Development Center. The series focuses on all different avenues and roles in our company, from developers, to technical writers, to the administrative staff. Brian is an accomplished member of the CenturyLink Cloud Object Storage team. He has a strong engineering background with lots of experience in distributed systems written in Erlang and C#. He’s also skilled as an agile team lead and retrospective facilitator, and participates in TDD and pair programming. A team functions best when everyone gets along and morale is high and Brian is a peacemaker at heart who is also quick with a joke, which is a great combination for increasing team morale. He’s a sharp guy who is able to learn his way around complex systems very quickly. He loves software and difficult challenges, but succeeding as a team is what really drives him. Here's an example of his love of difficult challenges: one of his favorite projects was building the Erlang front-end to the Object Storage product. This type of project was right up his alley. Most of the coding was brand new, which meant he got to spend nearly an entire month in “heads-down” coding. The challenge leveraged his strengths and proved to be \"just a lot of fun\". But more than that, the project was an integral step in making Object Storage part of the CenturyLink Platform. To get a feel for a typical day in Brian's world, he provided the timeline below. 7:40 am - Hit the snooze button on my alarm. 8:00 am - Hop in the shower and remember what I was working on yesterday and wait for any epiphanies about the upcoming cards on our Trello board. 8:12 am - Did I use shampoo? 8:13 am - Shower part 2, this time with more focus on actual hygiene. 8:30 am - Check my phone for texts from my buddy Josh to see if we are riding into work together and if so, who is driving? 8:50 am - Arrive at the office (I love short commutes!) 9:10 am - Team standup. Get a brief overview of cards in flight and any blocking issues that anyone knows about. 9:20 am - Fill up my coffee mug, water bottle and maybe grab an energy drink. It’s about time to rock out. 9:30 am - First pairing session of the day. I grab a fellow rockstar dev and the most important card on the Trello board and dive in, test first. That's the first challenge of the day. This time we're working on splitting large object backups into multi-part uploads. Sounds like a job for Erlang! I also rush to be the first person to start playing music, which also gives me control of the playlist. My oldest son just introduced me to Leo Moracchioli, so we’re starting the morning with some serious metal! 11:00 am - Hit the restroom, you only rent coffee. 11:10 am - Switch pairs, I get a new rockstar dev to pair with, and perhaps a new challenge to work on. I jump on a card to deploy a new Ceph cluster. Production work requires tunes that let the whole room know a critical operation is in progress. We set the desk to standing mode and queue up Kenny Loggins’ Danger Zone; game faces on. 12:40 pm - Start the lunch debate; cache invalidation has nothing on trying to get 4 amateur food critics to agree on a place to eat lunch. On the days that lunch is brought into the office the process is greatly simplified. 2:30 pm - Another pair switch, this time I stay with the current challenge and bring the new person up to speed. We continue provisioning VMs for the new Ceph cluster and make a few improvements to our Ansible playbooks along the way. A musical coup occurred after lunch. I no longer control the music. I think whatever is playing is a genre called “Space travel nap-time music.” It really isn’t that bad, but if you can’t troll someone about their musical choices then the fun police have finally won. 4:00 pm - Pinball machine in the game room is open, time to set a new high score. 4:07 pm - So, technically, I set a new high score....albeit by restarting the machine and racking up the only score. This pinball wizard needs a better spellbook. 4:10 pm - The early risers on our team are out, so I find a fellow night owl and see if we can knock out one of the in-flight cards. Gathering metrics on various caching strategies for virtual block storage sounds like a good end-of-the-day card. 5:40 pm - Time to head for home. A lot of our attempts at caching didn’t come up with the IOPS we were looking for, but tomorrow is another day. 6:00 pm - Arrive home, where I expect to assist with homework or dinner prep but find I am on handy-man duty because even though no one knows how it happened, Thomas the train is half-flushed down the toilet, again. Sir Topham Hatt is very cross. I tell my son he's been a very disobedient engine, to which he replies, Yes sir, I know sir. But I will try harder! 6:30 pm - Dinner around the table with the family. I try to explain to them the various programs we tried for caching, but am met with blank stares. I switch to letting them know I played Leo Moracchioli’s Metal cover of Adele’s Hello for the team today; lively discussion of how all music is better as a Heavy Metal cover ensues. 7:30 pm - Drop into my recliner with the laptop. I must delegate the tasks of dish duty and floor vacuuming fairly, pick out a movie to stream, and referee the karate match between the little boys. It is a multi-tasking challenge, but I am up to the test. Besides, someone has to do it. 7:40 pm - Open up Slack to see how Batman (the on-call person for our team) is doing, or if anyone is working a side project or posting pictures of their latest cooking project. The team has a lot of amateur chefs in addition to food critics. I use the word \"amateur\" because they don’t get paid to cook professionally. 7:50 pm - Log my cleric into Project 1999 because killing specters only requires minimal attention every 6.5 minutes and is solid XP. 7:58 pm - Log into the admin page of the router and shut off internet access to the kindles because the dishes still aren’t done. 8:03 pm - Restore internet access to the kingdom. 8:20 pm - Movie time! 8:25 pm - Popcorn break. 8:32 pm - Bathroom break. 8:33 pm - OK, really, we aren’t stopping the movie anymore guys. 11:30 pm - Finally finish that “2 hour” movie. 1:00 am - One last kill with the cleric... 2:12 am - Actually go to bed. Brian married his high school sweetheart, Casey. The story is told that his wife was taller than him when they met, but he hit a couple growth spurts. Now he’s half a foot taller than her. They have 8 children and homeschool all of them a few miles from the St. Louis CenturyLink Development Center. Some people might think that he and his wife are little crazy, but he can’t imagine life any other way.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "devops-travesty-or-triumph", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/devops-travesty-or-triumph", "abstract": "DevOps is a growing phenomenon amongst companies. They want to develop software faster, with the flexibility to adapt quickly to changes as they arise.  Strategically, companies vary in the processes used to implement the DevOps model. This post, based on Richard Seroter's OpenStack summit talk , will provide a little background on DevOps and describe what it took to bring the approach to CenturyLink. By definition DevOps is a term for a group of concepts that, while not all new, have catalyzed into a movement emerging from the collision of two major related trends. The first trend, “agile system administration” or “agile operations” , sprang from applying newer Agile and Lean approaches to software engineering and operations work. The second trend was to form a routine that fostered a greater appreciation for collaboration between development and operations teams throughout all stages of the software development life cycle. This stems, in part, from the experience of creating and operating a service at scale, and how important operations have become in our increasingly service-oriented world. The DevOps life cycle spawned from Agile, where there are four core iterations, or \"cycles,\" that create the complete life cycle. These are Backlog, Development, Collaboration, and Production. The key element of success is adapting the model so that these cycles are fluid, the process time is decreased, and the team dynamics (along with automation) allow for a continuous delivery cycle. At CenturyLink Cloud , we had core business strategies that we wanted to ensure were cultivated both continuously and efficiently, and that is where our journey into DevOps began. We wanted to implement a strategy that allowed for greater overall velocity.  Doing DevOps just for the sake of it is meaningless. Implementing a system that yields the desired outcome while maintaining the core fabric of your company, well that made sense. Why we felt our company and customers would benefit from the change : We wanted to stay with the up-tick in the competitive market. To do so we had to migrate to a method where we optimize our business goals on a daily basis: So, we had our Why . Then came the hard part, the How . Implementing a lean culture into a large enterprise was just one of the challenges. Incremental changes were made to ease the progression of the shift. How we approached change within the organization : Tools — As your work flow evolves, so should your tools. Various tool sets were implemented to make Communication, Collaboration, and the cycle of each team visible and measurable. Below are just a few: Workspace — For the continuous delivery cycle to thrive, the environment must support it. We did this with: If you’re in Bellevue come take a tour of our facility or check it out here . We are done here, right? We successfully implemented DevOps into our culture. Not quite. DevOps is something that you have to cultivate. Its inherent nature is based off learning from usage. That is the stage we are currently in at CenturyLink Cloud. We are continuously striving to optimize, streamline, and automate processes. There are bumps along the way. Every retro session unveils a lesson learned, a key to a simpler path, or the future iterations of how we can evolve with this process as our company grows and transitions. We will share our journey here, at the Developer Center through tutorials, blog posts, knowledge base articles, and developer tools like SDKs. The DevOps path for us has been laid. Now we have to navigate it, learn from it, and adjust as needed in a vested interest towards either Travesty or Triumph.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-to-secure-your-private-docker-registry", "author": ["\n              Alex Weich\n            "], "link": "https://www.ctl.io/developers/blog/post/how-to-secure-your-private-docker-registry/", "abstract": "The Docker team has made it easy for us to host our own private docker registry by providing us with an Open Source, Python, web application for doing so . The web application also exists on the Docker hub as a single Docker image that we can execute to have our registry up and running as a Docker container. The running container provides us with a registry we may push and pull from, but it leaves it to us to secure the registry on our server via SSL, and optionally, basic authentication. Securing the registry is important, as the folks at Docker have made clear. See the recent updates strongly encouraging the use of SSL when interacting with private registries. There are plenty of options for setting up the registry and protecting it. The post, How To Set Up a Private Docker Registry on Ubuntu 14.04 , does a good job of explaining how to do so without the use of containers. Here, I'll show how we can set up our own secure registry leveraging Docker containers. The basic principle is this: We run the registry container as we normally would, then fire up a separate nginx container to provide the SSL and basic authentication. The nginx container will proxy the HTTPS requests internally over HTTP to the linked registry container. Use the htpasswd command to create a new .htpasswd file with the desired credentials. I had to install the apache2-utils package on Debian. Save the created file, we'll need it later. Previously at CenturyLink Labs we had created an image to facilitate creating self-signed certificates for Panamax . We can leverage that image here. This will create the private key, certificate signing request, and certificate inside the ~/certs directory. It doesn't really matter where we stash these at the moment, just keep the generated files handy, as we'll need the certificate and private key later. Note: Generating the SSL certificate went pretty quick here, but I'd encourage the reader to investigate this a bit more on their own. It's also possible, and generally preferred, to use a CA signed certificate. I feel that's beyond the scope of this post, so I'll leave it to the reader to decide how to handle this. Run the registry container , being careful not to bind the port to the host. We only want this container accessible via the docker link. With the registry container running, spin up the proxy container, passing in our public certificate, private key, and .htpasswd file. Additionally, we need to map port 8080 to the host (feel free to map 8080 to any port you like on the host) and supply the PUBLIC_IP_ADDR environment variable. Note: There was more handwaving in this step. We buried a bit of the work inside the centurylinklabs/nginx-ssl-proxy image. The source is located on GitHub . If all went well you should be able to copy your public certificate to another machine and curl the endpoint. See the post How to use your own registry for information on working with the private registries. Securing the registry is important. The Docker team made it pretty easy to host your own private docker registry by providing Open Source, Python, and web applications for doing that. Don't have an account on CenturyLink Cloud? No problem. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . Sign up for our Developer-focused newsletter CODE. Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "devops-practical", "author": ["\n              Ray Wakiyama\n            "], "link": "https://www.ctl.io/developers/blog/post/devops-practical", "abstract": "I was thrilled to see DevOps and security intersect at RSA’s 2nd Annual Rugged DevOps Conference. It was great to see the growing intrigue and the varying “best-practices” for the ideal implementation of DevOps as more than 600 attendees from varying backgrounds packed the room. It was enlightening to hear about the various functions that should play a role in rapidly delivering applications and services to both internal and external users. Taking a step back, this “still-evolving” concept has even the most mature organizations putting pressure on themselves to get DevOps right the first time. Perhaps that’s why most companies are still experimenting with containers and the presenters placing significant emphasis on a distinct culture or role, termed, “DevSecOps”. Here are some of my takeaways. DevOps and security go hand-in-hand and must collaborate to deliver against business and customer objectives. Kim Zetter gave us a great roundup of the notable hacks from 2015, ranging from a teenager hacking the CIA Director’s emails, the OPM hack, and the Ashley Madison case. You’d think companies would have processes in place to ensure the least amount of security vulnerabilities, but as technology pushes innovation to its limits, it also creates room for that same innovation to be tested. There is no prescribed path for DevOps success. The list of panelists who moderated the “DevOps Engagement: Politics, People and Process” shared interesting insights as many of their customers raised similar concerns to what we hear from our own customer-base. How do they begin such a long journey? How long is the journey? Where does the buy-in come from? At Cloud Application Manager, we notice that the companies with the most successful DevOps initiatives are the ones that take a “rugged” approach. Having security folks inject their input early on in the process tackles a large amount of the fear and vulnerabilities that comes with the journey. An exec champion is a must, and the team buy-in is also critical. As you’d imagine, having a senior executive or leadership sponsor as a champion is an absolute must, but more than that, having a buy-in from all levels of the organization is just as critical. When it comes to DevOps, we notice lots of different objectives and goals across the board. Development teams may see the value around accelerating application deployments, while IT may see the true value around creating a self-service catalog that falls within their policies. CIOs may see the value in reducing failed builds while leveraging multiple clouds. Win the day. Find short, quick wins. With varying perspectives on the underlying value of DevOps, it’s easy to get lost in the numerous ways companies can get started – think, short, quick wins. We’ve seen that instead of trying to change large enterprises, finding a single application within a specific team yields the best result. Security must be considered as part of the a DevOps initiative. As business demand for DevOps and public cloud services increase, we constantly hear how this new agile approach creates conflict with security practices and processes that already in place. For example, you’ve created a vehicle that’s as fast as a sports car and resourceful as an electric car, however, it does not pass the safety tests. To successfully have this car in production, making sure the airbags are a listed as a requirement in the beginning will have a higher chance at avoiding going back to the drawing board to re-engineer the entire car. Simply put, taking a Rugged DevOps approach would mean making sure that the airbags we’ve tested and trusted for so many years are still a necessary component in the final ride. I think Shannon Lietz summed up the state of DevSecOps, “DevOps tend to push code several times a day, hackers tend to find issues several times a day, and security professionals tend not to make very many security decisions, and especially only when asked… it means that right now the security for your organization is being decided by somebody who doesn’t necessarily know what’s happening or what’s going on in your environment.” Cloud Application Manager is a powerful, scalable platform for deploying applications into production across any cloud infrastructure – private, public or hosted. It provides interactive visualization to automate application provisioning, including configuration, deployment, scaling, updating and migration of applications in real-time. Offering two approaches to cloud orchestration — Cloud Application Manager and ElasticKube — enterprise IT and developers alike can benefit from multi-cloud flexibility. Explore ElasticKube by visiting GitHub (curl -s https://elastickube.com | bash). Visit the Cloud Application Manager product page to learn more.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "cyclops-on-the-cloud", "author": ["\n              Nathan Young\n            "], "link": "https://www.ctl.io/developers/blog/post/cyclops-on-the-cloud", "abstract": "Over the past few years CenturyLink's growth to becoming one of the largest telecommunications providers in the United States has been through acquisition. A little over two years ago, CenturyLink acquired Tier 3 , a startup based out of Bellevue, Washington, in a move to deepen its cloud infrastructure hosting and management capabilities. Tier 3’s Cloud Management Platform, would become the CenturyLink Platform . To bolster its cloud offering , teams based in St Louis, Denver, and Edmonton are developing products and services that will integrate into the Bellevue-based CenturyLink Platform . Now, 20+ product teams contribute to the Platform, each with their own development skill, technology stack, and deployment schedule. With so many different people developing for our Cloud Platform, we reached a point where product complexity and organizational size made us realize that if we wanted to give our customers a consistent, simple, CenturyLink-branded user experience, we would need to design and develop what’s known as a UI/UX pattern library. We needed to be able to scale the impact of a relatively small UX/UI team so that an organization of 300+ employees are able to create new products with a user experience that is fluid for our customers, and allow us to evolve our design language without needing to go to each product team and tell them to update their code or submit a pull request via Github .\nOver the course of four months, we developed the pattern library we named Cyclops . If you’re unfamiliar with the concept of pattern libraries or style guides, check out the resources section at the end of the article. Cyclops is an open source collection of atomic pieces that make up the CenturyLink Platform Control Portal interface. By documenting and assembling a reference site of UI/UX patterns, we are able to speed up the development process and address internal communication problems inherent with working on a distributed team. Codifying our UI patterns, both in behavior and actual code, benefits us in the following ways. It was important for us as the CenturyLink Cloud UI/UX team to make it as easy as possible for product developers to use the patterns we defined, thus facilitate the usage of Cyclops. Our team is small, so it’s even more important that each pattern is well-documented. Otherwise we’d be spending all day on Slack or phone calls explaining to developers and product owners about the ins and outs of each pattern. Patterns are displayed with markup and code samples along with a usage description.\nFor more complex UI patterns (e.g. sliders, toggles, account context switcher), web components were developed and documented so that implementors don’t have to deal with large blocks of markup. We released Cyclops 1.0 internally mid-November 2015, and adoption by teams within the cloud organization has been a success. As we work with remote teams to build their product UIs, product managers, developers, and senior leadership have realized the benefits of faster development time and a consistently higher quality user experience. It is exciting that our organization has adopted a product development process where UX/UI design is central rather than an afterthought, and Cyclops has been a catalyst for this. That may seem like table stakes, but if you’ve ever been part of a enterprise-sized organization, you’ll know that the effort required to change anything is non-trivial. However, as with any product that has been in existence for a significant amount of time, there’s a large amount of existing markup and JavaScript that we’re working to re-factor to Cyclops patterns. We’re opportunistically working through this technical debt while working with product teams to design user interfaces for upcoming products. There’s the obvious, like adding more patterns and refinement. We will look to judiciously add new patterns as we continue to collaborate with product owners and developers to iterate and define the customer experience. We will also focus on including web accessibility best practices as part of our patterns. All of this leads to what we call Monocle, our system for working with product teams to produce high-quality prototypes based on Cyclops. Stay-tuned for our post on our Monocle system. New to the Cloud? If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . Resources We couldn’t have developed our pattern library without help. We borrowed heavily from some fantastic resources.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "horizon-meets-the-cloud", "author": ["\n              David Cormier - PhD\n            "], "link": "https://www.ctl.io/developers/blog/post/horizon-meets-the-cloud", "abstract": "Earlier this year, the Platform Enablement team at CenturyLink Cloud made their way to the Sylvan Dale Dude Ranch in Loveland, Colorado approximately 50 miles North of Denver for collaboration, team building, and an opportunity to plan for their upcoming year. The Sylvan Dale Ranch is a serene setting, nestled in the Mountains, next to a crystal clear creek, with quaint cabins surrounding the main club house. The staff is warm and accommodating, and the in-house chef kept the teams well fed.  These types of offsite retreats are part of the team-oriented culture at CenturyLink Cloud, and are commonly called \" HackHouses \" - where the team lives together offsite in order to get a large amount of work done in a short amount of time. A HackHouse, according to Bob, \"is basically a group of engineers coming together to solve problems and deliver value. In Denver, engineers from Dynatrace joined our team to work on their integration with the CenturyLink platform.\" What is the purpose of this HackHouse practice? David explained, “We get away from the office together to deliver value, build repeatable engines, and develop new features.  We went to the ranch to get together and do nothing but focus…it’s a cultural practice within our agile organization. Getting away together helps us flatten out the team, and work with each other to build velocity.” Bob added, \"This is also a reward and a celebration for the team's hard work.” The CenturyLink Cloud team used the open spaces and spectacular horizons of the West to foster openness and genuine collaboration among team members. The Platform Enablement Team mission is to empower users, developers and partners of the CenturyLink Platform.  It is comprised of four teams, the members of which are spread out from St. Louis to Seattle, Portland, Florida, and beyond. The Digital Team is responsible for the interactive web properties that serve as the \"front door\" and \"instruction manual\" of the platform, ranging from backend coding for customer registration , the interactive knowledge-base , and the overall documentation for each platform service. The Content Team creates content for blogs , social media campaigns, tutorials, and knowledge-base articles for both the Platform and the Developer Center sites. They work hands-on with product owners and MarketPlace Partners to help build stories for their products and share how their product or service on the CenturyLink Platform provides solutions to real-life problems our customers face in the Digital Economy. The User Enablement Team fosters engagement with the platform, helping to inspire and educate users of CenturyLink Cloud to be more productive. They edit a series of Digital Magazines to reach out to various audiences to share knowledge around how users are achieving value. They work with MarketPlace Partners to create learning sessions, called \"brown bags\", to share technical depth with developers and engineers both internal and external. The Ecosystem Team , much like the other teams, dabbles in many aspects of the Platform. From directly working on the technical specs with prospective MarketPlace Partners, to creating BluePrints on the Platform, working with product teams to cross-pollinate tech, and being out in the field with sales teams to get a pulse for what issues customers are facing and how to help share knowledge about the Platform to solve their problems. They also get the honor of sharing knowledge on both a micro to macro scale, by collaborating with Platform Partners and local Meetup groups to host hands-on tech events within \" Connect \" user communities. While these four teams hold specific separate goals and workloads that keep the Platform Engine running, they are all part of the same machine, driving towards the same common goal. Often these teams paths overlap, or have dependencies on one another. Getting the four teams in one off-site setting to collaborate, share ideas, create goals for the upcoming year and build roadmaps on how to achieve those goals, is priceless. Bob explained, “This was time to focus on work away from daily distractions…knuckle down, produce, share, and inspire.”  What really excited Bob was the collaboration and the team synergy that fuels the engine of innovation. While at the ranch, team members participated in innovation games to learn new ways of disrupting the norm. The trip also provided opportunities for engaging at a personal level, getting to know each other better and helping to understand what makes each team member a unique contributor.  Social activities such as hiking, horseback riding, and even a late-night round of tournament poker, all contribute to a greater sense of camaraderie and commitment to each other as teammates. What were the takeaways of the retreat?  David elaborated, “We prioritized our work for the next several quarters, created a common understanding of how each team would interact, and developed some new features and integrations with our partners.\" CenturyLink is pioneering Cloud-computing services in the communities where it develops, such as Bellevue, Washington and St. Louis , Missouri, running Connect events such as technology meet-ups, kid-oriented Tech DoJos, building Cloud and Managed Services , engaging with the community, and more. CenturyLink focuses on educating customers in practices such as DevOps or how to use CenturyLink Cloud tools more efficiently. We do that by hosting Meetups and hands-on CloudWalk events, which enable a fluid environment for sharing and gaining technology-based knowledge. The company also hosts other community events to foster a learning environment for children in the community. We host Girl Scout events and monthly Coder DoJo Meetups for kids.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "five-iot-barriers-developers-consider", "author": ["\n              Jonathan Townsend\n            "], "link": "https://www.ctl.io/developers/blog/post/five-iot-barriers-developers-consider", "abstract": "When it comes to connecting and integrating technology with day-to-day life, it's safe to say the future is now. The Internet of Things, as we've stated before, is \"the intelligent approach to connect devices, integrate systems, and improve networks by leveraging and mining data gathered from reporting tools and sensors in physical objects and machines.\" And, it is indeed taking over the world . With so much rapid IoT progress and accelerated integration, there are bound to be a multitude of barriers at play – especially for developers. These hurdles range from operational to cost-based, and include elements such as: cost, effective management of still-developing technologies, security, privacy, connectivity and interoperability, and power type and source limitations. At the present, the total cost of production for many of these IoT-based technologies remains high – and it's increasing. Many developers recognize the increase in spending will continue with an upward trajectory for the foreseeable future as industry-defining changes have an effect on the IoT, its associated technologies, and its affected industries. For developers to truly optimize production from start to finish while ensuring a positive user experience, they must take the cost of time, research, resources, and dedicated labor directly into account. Core principles like security, data privacy, and ease-of-integration will increase total cost as well. Additionally, the implications of these costs affect the way developers work. Creating a truly digital ecosystem, with continuous data sharing and development of apps, physical and virtual machines, and data centers that are compatible with digital and physical technologies across global networks, requires incredible funding efforts. In the world of the IoT, developers are thought leaders and must conduct cost-centric analysis to accurately develop methods of delivery and go-to-market strategies. In a rapidly-advancing, tech-driven world, obstacles such as research and development, production, and data cultivation methods that are governmentally and legally compliant pop up every day, and developers need to prepare for that. As more industries align with IoT-centric technologies, more functions like Quality Assurance (QA), Research and Development (R&D), and beta testing will be required – all of which require a lot of time, money, resources, and most importantly, management. Management in the IoT depends on two factors which developers consider regarding integration: connectivity and interoperability. To manage the billions of devices across networks and maintain connectivity, developers need to integrate capacity planning . However, the bigger challenge is interoperability – the task of enabling all the devices to operate and communicate with the each other without compromising performance, diminishing user experience, or breaching security. It's no easy task. With so much data cultivation and innovation across physical and digital platforms, security is a prime barrier in the world of the IoT. With so many devices, networks, and connections using and transmitting valuable data, the IoT has become a haven for hackers. Developers know that hackers have already opened up and exploited critical vulnerabilities in everyday devices like mobile phones, baby monitors, smart home applications, security systems, medical devices, and wireless methods of financial delivery. The sheer volume of devices and their dependency on shared networks is a challenge. Once devices are connected to the Internet, hackers can gain access if the proper security measures aren't implemented. According to an article on Wall Street Daily , \"despite high-profile and alarming hacks, device manufacturers remain undeterred, focusing on profitability over security. Case in point: A recent survey by online authentication provider Auth0 found that 85% of IoT developers admitted to being pressured to get a product to market before adequate security could be implemented.\" One lesson the age of the Internet has taught society is that hackers are usually ahead of the curve and based on the high volume of IoT devices (current and projected) in use, providing cybersecurity solutions for all devices is impossible. In short, compromised devices and systems dramatically affect lives and business. People want their personal information kept personal and private. Just like security, privacy is major barrier in IoT integration. Think about it – security is important so people can maintain privacy. It's no secret that mobile phones have 'default' settings that track movement . These devices, along with wearable technology, automatically share data and transmit information that the average user might not even know about. It's becoming increasingly more difficult to 'get off the grid', and as the world of the IoT unlocks new technologies, companies must maintain a level of trust between consumer and developer. Medical, financial, professional, legal, and personal information is valuable and consumers want to know their information and data that is collected is safe. Without trust in the systems, networks, and devices, complete adoption of the IoT will not take place. Everything requires energy to exist. As the IoT becomes more integrated into daily life, energy resources could prove to be a barrier. Although developers continue to create IoT devices, none of them are valuable without the energy to power them and their corresponding systems. Anyone with a smartphone knows battery life is fleeting. That's because users demand more of their devices to accomplish day-to-day functions. As noted in an article detailing the Rate of Adoption of the IoT , Steve Rizzone, CEO of Energous Corp. (WATT) stated, “IoT is becoming more and more of a dominant market consideration and to support IoT you need two functions, you need Internet connectivity and you need power.” This barrier, as developers are correct to point out, is more than improving battery life and density, but rather about finding new and better fuel sources and battery types – all of which are currently rooted in prototyping and nowhere close to development (see 'Cost of Production' above). Not only is the energy barrier prescient, it's also devoid of a uniform solution. Powering the physical devices is but only one barrier. Powering the data centers and physical machines is a whole other challenge. Finding and funneling renewable energy to fuel data centers is already taking place and, as cited in Data Center Frontier , \"100 percent of the power used at Apple’s data centers is now derived from renewable sources. Other sustainability leaders in technology – including Amazon Web Services, Facebook, Google, and Microsoft – are moving toward a 100 percent renewable energy supply.\" Just because there are barriers doesn't mean there won't be breakthroughs in the IoT world. Technology will open new doors and pathways for progress. Emerging industries are certain to create beneficial ways to power devices and maintain security. For those of you working with the IoT , CenturyLink Cloud® Platform is the perfect place to build your applications. Our global network provides superior performance for your IoT sensors and cloud applications. Our managed services handle operating system and application operations – allowing you to focus on more important objectives. Not a CenturyLink Cloud customer? No problem. Designed for your business needs today and tomorrow, the CenturyLink Cloud is reliable, secure , robust , and global . If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "day-in-the-life-hannah-stevens", "author": ["\n              Jonathan Townsend\n            "], "link": "https://www.ctl.io/developers/blog/post/day-in-the-life-hannah-stevens", "abstract": "Our \"Day in the Life\" series gives you a glimpse into the everyday lives of team members and our culture here at CenturyLink Cloud . The series focuses on all different avenues and roles in our company, from project managers to developers, product owners, technical writers, and administrative staff. In this post, we highlight the St. Louis Development Center 's Office Manager, Hannah Stevens. This place can get pretty chaotic, and with so many moving parts and all the teams working interdependently, it takes a massive organizational effort to keep the office running smoothly. Hannah manages just about everything that goes on behind the scenes at the STL Dev Center on a daily basis. Whether she's working to help onboard new hires, ensuring CenturyLink employees have all the access they need on their workstations, or setting up and administering corporate meal coordination tasks, Hannah makes it happen. The following interview gives us a glimpse into a 'normal' day for her here at CenturyLink. 6:36 a.m. - First alarm goes off. I actually get up if it’s a Wednesday because we have bagels delivered for breakfast and I need to set it up for everyone. 'Bagel Wednesday' is a big deal around here! 7:45 a.m. - If it’s not Wednesday, the second alarm rings. If I actually got out of bed at this time, I could enjoy a cup of coffee peacefully sitting at my bar. Instead, I continue lying in bed reading emails that might have come over night and early-morning Slack messages. 7:53 a.m. - Roll out of bed and run to my closet to throw some clothes on. 8:04-8:11 a.m. - Depending on the time, fix a cup of coffee for the road or just head to my car. 8:24 a.m. - Get on highway 40. Traffic is guaranteed. If there isn’t traffic, my first assumption is that it’s a holiday that I completely forgot about. 8:41 a.m. - Arrive at work. Get a cup of water and start reading emails again while listening to music. Depending on the day, I listen to Classic Rock, Christian, Backstreet Boys, Frank Sinatra...you get the picture. 9-10:45 a.m. – Depending on the day, these next few hours vary. I make a to-do list, prioritizing each item based on time-sensitivity and importance. If it’s a Monday, I often have 1-3 new hires starting, so I need to onboard them and ensure they’re good to go when I hand them off to their team. 11:00 a.m. – If it’s Tuesday or Thursday, I, along with the dedicated team for the week, set up the catered lunch that is scheduled for the day. Other days, I head to the eMart and usually buy a salad (I genuinely love salads). 1:30-4:45 p.m. – If it’s a Tuesday or Thursday, start cleaning up the catered lunch with the help of the day's dedicated team. After lunch, I get back to my to-do list and address anything that pops up unexpectedly. This includes doing expenses for myself and a few others, researching new potential vendors for lunch, setting up interviews, working with staffing vendors for our open requirements, etc. A main goal right now is to help smooth and automate the onboarding and offboarding process for employees. I work closely with our internal RobotOps and CorpIT teams on this topic. But wait, there’s more! Have visitors coming or a tour ? I help get visitor badges printed with our Envoy system or request temporary badges. Having access issues? I help guide employees to the best point-of-contact if I can’t fix it or request something myself. Need help planning a Meetup , CloudWalk or coordinating large-scale meetings/meals? I help ensure your event onsite goes smoothly from the facility access to the food served. Is something in the E-Mart (our onsite convenience shop) not scanning properly or unavailable? I help get the issue resolved and meet with the E-Mart vendor to ensure we are offering desired items. Ultimately, my daily goal is to support the Dev Center Employees to ensure they are able to focus on their work, building cloud and managed services . 4:45 p.m. – Start the trek back home to Soulard. Afternoon traffic is much more unpredictable than morning traffic. Some days it’s a clear shot down the highway; other days, traffic starts as early as 2 p.m.. 5:40 p.m. – Traffic is usually bad. If it’s not, I will stop by the grocery store to grab the staple items: milk, yogurt, lettuce (as I said earlier, I love salads), etc. 6:45 p.m. – My fiancé arrives home. We try to hit the gym together 3-4 times a week. Afterwards, we either have a date night at a local restaurant or start cooking. We’ve signed up with Blue Apron , where we get ingredients delivered to us for 3 meals a week. We really enjoy cooking together and catching up on the day. We like to ask each other what the peak and pit of the day was for each of us. 8:00 p.m. – Catch up on a show or preferably watch sports. 10:00 p.m. – Say our prayers and hit the sack. As you can see, Hannah Stevens is always busy and keeps this place in order. We're all thankful for her hard work and continued efforts to make the St. Louis Developer Center a great place to work! Hannah moved to St. Louis about a year ago from Raleigh, NC when she and her fiancé got engaged. She went to what she would argue is 'the best university of all time', The University of North Carolina at Chapel Hill , where she received a degree in Management and Society (basically studying the way people interact in professional settings and how to successfully work with them). She really likes St. Louis, although she misses her family, including her twin sister, and her friends. She also misses the ability to make a day trip to the beach. Hannah and her fiancé love to try new restaurants, explore the city, and meet new people. Any recommendations for places to check out are always welcome! Storage Engineer: Brian Haggard Cloud Technical Writer: Daniel Morton Cloud Engineer: Rebecca Skinner Migration Manager: Traci Yarbrough Sign up for our Developer-focused newsletter CODE . It's designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. Check out the CenturyLink Careers page for opportunities to join a fantastic team that truly cares about the craft of engineering. Connect with CenturyLink.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "monocle-pattern-library", "author": ["\n              Nathan Young\n            "], "link": "https://www.ctl.io/developers/blog/post/monocle-pattern-library", "abstract": "Previously I wrote about Cyclops , an open source UX/UI pattern library for CenturyLink Cloud . In a distributed organization of 350+ people and 30+ product teams, there are zero designers assigned to any product team. Each product team consists of a product owner, product analyst, and n number of developers and engineers.\nSo knowing what Cyclops patterns are appropriate for which context, isn’t something that product teams inherently have the capability for. We’ve done a great deal of internal evangelizing of Cyclops, so there is an awareness within the org that it’s a valuable resource. However, without a designer on each team, a knowledge and capability gap exists that we have to account for as teams develop their product user interface. Finding a method to scale our design team’s expertise and value — enable product teams to create simple, beautiful, and consistent customer experiences  became our mission. When we created Cyclops, we knew it was a major step in scaling our team’s impact on the business, but our work was far from done. We couldn’t simply direct each product team that needed a UI to Cyclops and let them figure it out on their own. The outcome of such a process wouldn’t satisfy anyone, especially our customers. To demonstrate this point internally, I’ve been explaining this gap using this cheesy metaphor: Patterns are like groceries in a shopping bag; the bag contains everything needed to create a meal, but transforming those ingredients into a fabulous dinner rests with the skill of the chefs in the kitchen. We are the chefs, and we invited you over for dinner. We came up with a “sibling” project to Cyclops named Monocle. It’s also the name for our collaboration process with product teams to design and develop high-fidelity prototypes. The process looks like this: Product owner, product analyst, and engineering manager have a defined product vision, mission, and identified a minimum viable product feature set. We schedule a “kickoff” meeting(s) to learn as much as we can about the product, including target market, competitors, and logistical stuff like rough timeline and team capabilities. The goal of the kickoff is knowledge cross-pollination and to come away with a list of key flows that will take the form of stubbed out HTML pages. The first iteration of stubbed out pages is essentially a fully navigable site architecture, with the source hosted on Github . A standard website shell (header, footer, body background color) is applied to each page through the use of handlebars templating and partials. In the body of each page, a list of features and content is displayed as a list, in priority order. Each commit to our master branch automatically deploys the static site via Travis . This is presented to the product team and iterated upon. Once there’s general agreement upon the site architecture and content items, we’ll start to apply Cyclops patterns, increasing the fidelity of the views. As we work through page designs, and find that there isn’t an existing pattern that works for certain types of content, we’ll mock them up as part of the product team’s “Monocle” prototype. If we believe other teams may benefit from the new pattern, we’ll add it to Cyclops as a new pattern. When we feel there are enough new patterns to warrant a new version, we’ll push out a new release of Cyclops and notify teams via Slack . The outcome is a high-fidelity prototype that typically demonstrates create, read, update, delete (crud) operations and four states: first-run, empty, error, and populated. We’ll then share the staged site with the product team via Screenhero for feedback and iteration. Once there’s agreement that the Monocle prototype has reached a point where the product team is comfortable moving forward to “wire up” the UI, we publish a final version to a github repo and a staging site. Then product teams use the actual HTML files of the prototype as reference for development in their individual development environment. All teams have access to other team’s Monocle prototypes to use as reference. This has the benefit of helping us maintain consistency across teams. Lastly, as product teams begin to develop their user interfaces, we have regular check-ins to offer guidance and answer questions that come up. So far this has been a really effective process for both product teams and the design team. We’ve had as many three or four monocle projects happening at the same time. Seeing “real” content in a high-fidelity prototype almost always forces teams to consider how to design their systems to effectively “power” the desired user experience. Any gaps in thinking or inaccurate assumptions are quickly revealed through our discussions of the user experience and interface. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "object-storage-libraries", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/object-storage-libraries", "abstract": "Cloud object storage is a powerful tool for storing and delivering media, software, and other digital assets. Evolving enterprise requires scalable, long-term storage for a wide variety of digital objects. Amazon S3 (Simple Storage Service) revolutionized cloud object storage when it launched in 2006. The S3 API has since become almost a de facto standard for object storage design, and now many cloud providers are offering compatible services and solutions. CenturyLink Cloud Object Storage is one such service – providing enterprise-grade object storage in a highly-scalable, fault-tolerant, distributed datastore. Although Object Storage is fully-compatible with S3, not every S3-compatible library will connect easily to our API. In this tutorial, we will look at the best libraries for utilizing Object Storage. We will examine libraries for four popular programming languages: Java, Node.js, Go, and PHP. CenturyLink Cloud Object Storage offers enterprise-grade object storage. Our cloud servers store and manage your files in a highly-scalable, fault-tolerant, distributed datastore. For large-scale cloud applications, Object Storage is far more efficient than hierarchical file systems. Follow the steps below to use the CenturyLink Cloud Console to add an Object Storage bucket and user to your account. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to access CenturyLink Blueprints and deploy virtual servers. To create a new Object Storage user, follow the steps below: After logging into the CenturyLink Cloud Console, navigate to Object Storage from the top drop-down menu. On the Object Storage page, click the Users tab. Click the create user button and enter the user information. Note: The email address for the user must be unique across the Object Storage platform and cannot be reused. Click save . Click the newly-created user record to view the access key id and secret access key values, which act as the username and password for this Object Storage user. Save these values for later use. To create a new Object Storage bucket for storing digital assets, follow the steps below. After logging into the CenturyLink Cloud Console, navigate to Object Storage from the top drop-down menu. On the Object Storage page, navigate to the Buckets tab. Click the +create bucket button. Fill out the \"Create Bucket\" form. The bucket name should start and end with lowercase letters or numbers, and can only contain lowercase letters, numbers, dashes, and dots. Note: This value must be unique globally across the Object Storage system. Click the save button to create the bucket. For more information on managing Object Storage buckets and users from the Console, check out this article in our knowledge base . If you haven't used Object Storage or other S3-compatible systems before, some of the terminology might be unfamiliar. An object is an individual digital asset. This can be any sort of data from text to video to images. A bucket is a resource for holding objects. Buckets have simple names consisting of letters, numbers, and some punctuation. Any characters that are valid in a DNS hostname should be valid in a bucket name. A region is a data center or cloud service area that hosts Object Storage buckets. One region contains any number of buckets. An endpoint is a hostname that serves the Object Storage API for a region. Each region has one or more endpoints, and each endpoint belongs to a region. When selecting a library to access the Object Storage endpoint, it is critical to find one that is not only S3-compatible, but allows the developer to specify a custom endpoint. Throughout this article, we will use the \"Canada\" region in the examples. However, if you choose to use US-East as the data center for your buckets, you will need to change all references \"canada.os.ctl.io\" to \"useast.os.ctl.io\". The AWS libraries for Java can be reconfigured to access Object Storage. To add S3 support to your existing Java project, follow the directions below: If you are using Maven, add the following dependencies to pom.xml . The following example will list all objects in a bucket. Replace the private members of the TestS3Compat class with the values you collected in \"Adding Object Storage to Your Account.\" There are several S3-compatible libraries for Node.js that allow you to specify custom endpoints. Two that fill our requirements are Knox and the simply-named s3 library from Andrew Kelley . Unfortunately, because of some fundamental problems with the underlying Amazon SDK, s3 is no longer supported . The Knox library from Automattic is a very solid choice. It is produced and developed by Automattic, who are well-known for releasing solid, stable software. One of the main drawbacks of Knox is that it doesn't natively support multi-part uploads for large files. However, the library documentation contains a list of supplementary libraries that implement many other popular S3 features that might be needed. We used the Knox library to build our Node.js tutorial application. In your Node.js project directory, run npm install knox --save to add Knox to your project's list of dependencies. Create a Knox client in your application using code similar to the following: In case you haven't heard of it yet, Go is a compiled, statically-typed language released by Google in 2009. It has a range of powerful features, such as garbage collection, concurrency, and limited type inference. One library that does a good job supporting the Amazon S3 interface is Mitchell Hashimoto's port of goamz . This library covers a good selection of the S3 API. The following is a complete example of a short program to list the buckets in Object Storage. In a new project directory, create a file called bucketlist.go and edit it to look like this: Run the following commands: You can now run ./bucketlist and get a list of Object Storage buckets for your account. There are several solutions for S3 available out there, but the simplest and easiest to set up is Donovan Schönknecht's S3 REST client class . For this example, you will want to have the PHP cURL library installed on your system, as well as the Composer dependency manager . In your project directory, make a composer.json file that looks like this: Run composer install or, on some systems, php compose.phar install . Create a file called listbuckets.php and edit it to look like this: Run the program from the command line like this: php listbuckets.php In this article, you learned how to use S3-compatible libraries for three different languages to bring the power of CenturyLink Cloud Object Storage to your application. Storing digital assets in the cloud adds new levels of versatility and speed to an application. These digital assets benefit from the enhanced redunancy and fast delivery that CenturyLink Cloud brings to all of its cloud product offerings. Sign-up for our Developer-focused newsletter CODE . It's designed hands-on by developers, for developers. Keep up-to-date on topics of interest, including: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "exploring-cloud-agnostic-computing-and-interoperable-clouds", "author": ["\n              Rao Mikkilineni, Co-Founder and Chief Scientist, C3DNA\n            "], "link": "https://www.ctl.io/developers/blog/post/exploring-cloud-agnostic-computing-and-interoperable-clouds", "abstract": "“In recent history, the basis of telephone company value has been the sharing of scarce resources — wires, switches, etc. – to create premium-priced services. Over the last few years, glass fibers have gotten clearer, lasers are faster and cheaper, and processors have become many orders of magnitude more capable and available. In other words, the scarcity assumption has disappeared, which poses a challenge to the telcos’ “Intelligent Network” model. A new type of open, flexible communications infrastructure, the “Stupid Network,” is poised to deliver increased user control, more innovation, and greater value.” - Isenberg, D. S., (1998). “The dawn of the stupid network”. ACM netWorker 2, 1, 24-31. Much has changed since the late 90’s that drove the Telco companies to essentially abandon their drive for supremacy in the intelligent services creation, delivery, and assurance business and take a back seat in the information services market to manage the ‘stupid network’ that merely carries the information services. You only have to look at the demise of major R&D companies such as AT&T Bell Labs, Lucent, Nortel, Alcatel and the rise of a new generation of services platforms from Apple, Amazon, Google, Facebook, Twitter, Oracle, and Microsoft to notice the sea change that has occurred in a short span of time. The data center has replaced the central office to become the hub from which myriad voice, video, and data services are created and delivered on a global scale. However, today it can equally be said that the basis for current generation data center value has been the sharing of scarce and expensive resources - CPU, memory, network bandwidth, latency tolerance, and storage IOPs, throughput and capacity - to create premium priced applications with high quality of service (QoS) dealing with availability, performance, and security with compliance constraints. Over the last decade, the availability of commodity computing infrastructure (multi-core servers, server, and network virtualization technology and cheaper virtual storage etc.)  in the form of clouds, the scarcity assumption has disappeared, which poses a challenge to the current \"data center\" model. If the cloud providers can deliver the same QoS using the shared commodity resources, the scales of economy will make application creation, delivery, and QoS assurance more efficient, scalable, and tolerant to fluctuations in both workloads and user experience constraints. The cloud providers recognize this and are on a race to out-compete with each other to bring the same services in their clouds. While Amazon had a first strike advantage and has created a competitive differentiation with a variety of services to address the non-functional requirements dealing with application QoS, and decouple application development (functional requirement fulfillment using computing functions, workflows and processes) others are catching up to duplicate similar services. This has led to cloud islands and the choice between dreaded vendor lock-in or complexity of using different clouds (private, public, or hybrid) with a plethora of tools, point solutions, and their perpetual integration costs. The story of service islands and their eventual integration with interoperability to improve exponentially the efficiency through scales of economy while fostering competition has played out before in the evolution of telephone networks, the Internet, and voice over IP (VOIP). As technology has progressed, the interoperability framework has moved from hardware solutions in telephony (SS7, STP, SCP, etc.) to pure software solutions based on virtualization. As infrastructure becomes a commodity, most cloud providers are forced to provide other services that facilitate the migration of existing applications to the cloud and attempt to provide the same QoS that they are accustomed to in current data centers through availability, security, mobility, and compliance zones within their sphere of influence. This is done through optimizing their infrastructure and hiding the complexity through a Platform-as-a-Service(PaaS). However, as competing cloud providers offer their own differentiating PaaS, the cloud consumers are left with complexity of choice, innovation chaos, and perpetual integration cost. Telephone companies who have gone through this exercise before know the value of interoperable islands without proprietary lock-in. They also know that they have the global network connectivity that is essential to provide that interoperability which any cloud provider has to either leverage or build their own (like Google has). CenturyLink with a pedigree from both telecommunications and data centers seems to have realized this advantage and has gone from offering a competing cloud with similar features as any other cloud provider to providing interoperability among multiple clouds using their expertise in computing ( data centers and managed services ) and communications ( network services ). They are extending the application availability, security, mobility, and compliance zones with a policy based application management framework that spans across multiple clouds only using different provisioning processes offered by the competing cloud providers. In addition, they also eliminate the need to orchestrate or move virtual machine images in order to provide cloud interoperability. This eliminates the need for a plethora of point solutions and tools. It also eliminates the need for cloud consumers to depend on non-functional requirement fulfillment using different PaaS offerings. The application management framework that CenturyLink offers was recently shared at a Cloudwalk event in San Francisco. This technology uses provisioning of virtual machines in different clouds (including CenturyLink Cloud) and provides availability zones across clouds where applications (using web servers, application managers, and databases) can be migrated to fulfill both recovery time objectives and recovery point objectives with zero down time. The novelty of this approach seems to put on an equal footing both stateful and stateless application components and eliminates the need for cloud native computing.  Applications are treated as cloud agnostic and require no changes to the application or the operating system on which they are executed. Furthermore, no changes to infrastructure provisioning processes are required. As long as the operating system is the same in the source and target execution venues (containers, VMs, or physical servers), applications can be deployed and moved across any cloud with their framework. The cloud providers are used as mere commodity infrastructure providers on a global scale with application interoperability and QoS assurance. The message to cloud consumers seems to be \"instead of managing clouds with myriad cloud management platforms, start managing your applications on any cloud using a policy-based cloud application management platform.\" It's worth finding out for yourselves. Blog : Migrate Your Apps to any Cloud With Zero Downtime Using C3DNA Solutions : Transform Your Business with Hosted Applications in the Cloud Getting Started Knowledge Base : Getting Started with C3DNA Appliance on CenturyLink Cloud", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "server-general-cloudwalk", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/server-general-cloudwalk", "abstract": "Have you ever wondered how leading businesses protect their sensitive data in the cloud? Does your business need to meet compliance requirements by using encryption solutions? Are you interested or curious about encryption and cloud technology in general? If you answered “yes” to any of those questions, join your fellow cloud and security enthusiasts at the the CenturyLink User Group technical meet up on Wednesday, August 31st for food, drinks, and hands on, technical exercises to practice encrypting your data in the cloud! CenturyLink and Server General have partnered together for this free, hands-on, technical CloudWalk event. For those of you unfamiliar with this concept, CloudWalks are technical events where people can get together to experience technology in a firsthand learning environment. At the Cloudwalk, technical experts guide everyone, technical and non-technical alike, in a group development session. You can read this recap of our Dynatrace CloudWalk to gain more insight into this unique concept that inspires education, collaboration, and partnerships throughout the technology community. Server General allows customers to quickly and easily encrypt a cloud database or file server using key management. Using this service, installing, configuring, and encrypting a database can take as little as 30 minutes. The service works with most database servers like MongoDB, CouchDB, MySQL, PostgreSQL, Apache or Samba, including ones that store sensitive and regulated information. Quick Turn Up Compliance and data is a big issue that can slow down a company's ability to easily provide quality encryption in a timely manner. Technically speaking, there are location issues, server requirement issues, etc, that can hinder fast encryption. Server General takes that out of the equation by providing an easy way to encrypt data at compliance levels so customers can focus on other parts of the business. Compliance Their service will help you to achieve HIPAA/HITECH compliance while ensuring that your sensitive medical information is safe. Whether you need to encrypt medical files within your own network or on a public cloud, Server General provides the tools to encrypt it all quickly and securely. Security 24/7 monitoring and data at-rest encryption ensures that Server General's high standards for encryption are not taken lightly. Databases and file servers are safe and secure at any time of the day. Secure Keys Data in-place encryption means you don't have to move your data in order to encrypt it. Secure encryption keys are managed by Server General to make the process as easy, fast, and painless as possible. No matter where your server is located, The Server General Data Encryption Service is here to ensure that a high level of security and safety keeps your data safe, no matter where the server is located. Moving sensitive or regulated data to the cloud opens questions from security and compliance teams about data access, audits, and control. Encrypting data in the cloud enables your IT team to: • Set data access policies • Audit all file access • Prove control through encryption and key management At this event we will demonstrate important practices crucial to security and compliance teams that aid in protecting intellectual property, personally identifiable information (PII), electronic protected health information (ePHI), card holder data (PCI DSS) and other sensitive data. Bring your laptop to learn and practice: • How to deploy workloads to the public cloud through automation. • How to protect data within 30 minutes • Data protection options, principles, and best practices for role-based access and secure databases. At the end of this event you will have a better understanding of how cloud security tools can be applied to multi-tenant models in the public cloud in order to mitigate security risks. • 4:30pm - 5:00pm - Arrive, Food, Drinks and Conversation • 5:00pm - 5:45pm - CenturyLink Platform overview with hands-on Exercises: Create an environment and deploy applications through automation inside CenturyLink Cloud. • 5:45pm - 6:30pm - Server General overview with hands-on Exercises: Encrypt sensitive data in the Cloud. • 6:30pm - 7:00pm - Q&A, Networking, & Raffle. Instructors : Bob Stolzberg, Principal Engineer @ CenturyLink - Bob is a local Principal Engineer on the Cloud Platform Enablement Team. In this role, Bob works with CenturyLink's Ecosystem Partners to integrate their technology with the CenturyLink Cloud Marketplace and evangelize the technology with the community. Bob has served as a subject matter expert in the areas of cloud technology and DRBC for the past several years and has more than 17 years of engineering experience in the IT industry. Check out our CenturyLink St. Louis Users Group Meetup page to register for this and other cool events. Feel free to bring friends and share this event through social media; just make sure to have them register and mention they should bring a laptop to participate. Please contact the CenturyLink Ecosystem team with any questions, or message us through our MeetUp page. We look forward to seeing you!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "c3dna-cloudwalk", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/c3dna-cloudwalk", "abstract": "Are you interested in learning how to migrate your applications to any cloud on demand with zero downtime? Do you want to learn how to scale IaaS and intelligently move your applications using portable, self-managed applications and policy-driven automation? Do you want to practice multi-cloud automation? If you answered “yes” to any of those questions, join your fellow cloud enthusiasts at the CenturyLink Bay Area User Group technical meet up on Wednesday, July 28. CenturyLink and C3DNA partnered together for this free hands on technical CloudWalk event. For those of you unfamiliar with this concept, CloudWalks are technical events where people can get together to experience technology in a firsthand learning environment. At the Cloudwalk, technical experts guide everyone, technical and non-technical alike, in a group development session. You can read this recap of our Dynatrace CloudWalk to gain more insight into this unique concept that inspires education, collaboration, and partnerships throughout the technology community. C3DNA brings end-to-end Application Lifecycle Management(ALM) to enterprise private or hybrid clouds. Both existing applications and new cloud-native applications can be on-boarded and deployed across clouds. They can then be managed via policies to provide developers and application owners with application-centric visibility and dynamic policy-based real-time management with instant cross-cloud mobility. Founded in 2013, C3DNA's vision is to enable a more connected and collaborative world by infusing cognition into computing to create and deliver highly scalable and resilient services using distributed, boundary-less, heterogeneous infrastructure. Their mission is to empower enterprises with business velocity and agility like never before to enable delivery of innovation, communication, collaboration, and commerce securely at the speed of light. Freedom for developers C3DNA's technology allows developers to focus on what they do best without having to worry about certain architectural constraints, multiple layers of orchestration or platform specific programming. Rapid time to value Migrate your legacy applications and endow them with cloud like features: application mobility, scaling up/down, availability, performance, and security standards across multiple clouds. Visibility and Control Use a single pane of glass to manage, monitor, and control operations of your applications, including performance data. Cost reduction Enable infrastructure-agnostic and cloud independent, distributed workloads on monthly billed platforms. Reduce the need for multiple orchestration solutions. Business Continuity Policy driven orchestration to move your applications on demand to reduce risk and maintain availability. Bring your laptop to learn and practice: • How to automatically deploy workloads to multiple public clouds. • How to use an application focused cloud operations platform. • Migrate your applications between multiple clouds on demand with zero downtime. • At the end of this event you will have a better understanding of how using portable, self-managing applications can provide flexibility, resiliency, scalability and allows applications to be operated anywhere. 4:00 - 4:30pm - Food, Drinks, and Conversation 4:30 - 5:30pm - CenturyLink Platform overview with hands-on Exercises: Create an environment and deploy applications through automation inside the CenturyLink Cloud. 5:30 - 6:30pm - C3DNA overview with hands-on Exercises: Migrate your applications to multiple clouds on demand with zero downtime. 6:30 - 7:00pm - Q&A, Networking, Raffle Instructors : Bob Stolzberg, Principal Engineer @ CenturyLink - Bob is a local Principal Engineer on the Cloud Platform Enablement Team. In this role, Bob works with CenturyLink's Ecosystem Partners to integrate their technology with the CenturyLink Cloud Marketplace and evangelize the technology with the community. Bob has served as a subject matter expert in the areas of cloud technology and DRBC for the past several years and has more than 17 years of engineering experience in the IT industry. Surendra Keshan, SVP @ C3DNA is a senior technology executive with 25+ years of global experience in design, implementation, and management of complex enterprise IT infrastructures.  Previously, he held many high-profile roles including - head of IaaS and cloud enablement at Visa. He has a great passion for technology and is a recognized thought leader and innovator holding 11 patents. Check out our CenturyLink St. Louis Users Group Meetup page to register for this and other cool events. Feel free to bring friends and share this event through social media; just make sure to mention they should bring a laptop to participate. Please contact the CenturyLink Ecosystem team with any questions, or message us through our MeetUp page. We look forward to seeing you!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "escaping-the-cube-the-evolution-of-the-office", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/escaping-the-cube-the-evolution-of-the-office", "abstract": "There has been a shift in how companies think about both physical workspaces and company culture. Creating the right mix of a productive and collaborative workplace with a culture that harmoniously fits the needs of the company goals and its employees can be a challenge. Generally, corporate offices are not labor-intensive mills, but rather collaboration centers where individuals and groups can develop their best ideas and then action on them at scale. When companies are looking to build-out an office, that effort requires a whole new approach extending beyond the four walls. Office-based amenities can help attract and retain talented candidates, but people are also attracted to the culture of the office, which should foster collaborative efforts and shift the paradigm of work being a laborious task to something that mixes and creates a workplace where fun and professionalism meet. The ideal office environment should combine technology with culture so people can work productively and proactively while escaping the outdated 'cube mentality'. In today's ever-changing corporate landscape, what defines a work place is less about the physical location of where work takes place and more about how the work gets done. The layout and floor plan of an office is just a piece of the puzzle. The expectations associated with the office are more important. That old school office mentality of clocking in at 9 and punching out at 5 has become a thing of the past. Gone are the days where people want to spend 8+ hour work days with their heads down at a cubicle going in-and-out of mundane meetings about meetings. Over the past decade, the office matrix has changed immensely. The modern office manager understands that allowing employees and teams a healthy mix of remote work capabilities is essential to getting the most out of in-office time, which remains extremely valuable for many companies. One lasting obstacle for companies to overcome is the expectations of a new generation of the workforce. The current generation, composed largely of Millenials are uniquely trained to be tech-savvy, which allows them to automatically multi-task, delegate, and collaborate. However, there's a catch -- whereas Millenials are conditioned to collaborate, there's a large segment of workers who opt for a more maverick approach to work and instead take an independent route to completing work. That being said, trends continue to reinforce the notion that Millennials enjoy and depend on collaboration, team-based work projects, and an unstructured flow of information at all levels.( Gartner Research Naturally, Baby Boomers and Gen Xers prioritize job security and consistency in their work structure, while Millennials strive for\nemployability and flexibility by expressing a desire to always add to their skill-sets and apply learned skills in meaningful and impactful ways. The modern employee understands there's more to work than the paycheck. Personal and professional growth along with the flexibility to work from anywhere are priorities. In fact, a recent Mercer study (in Nekuda, 2015) found that the top three career priorities for Millennials were compensation (most Millennials graduate from college with an average of $40,000 in debt), flexible work schedules, and the opportunity to collaborate with others to make a viable difference. One approach that companies are taking to increase productivity and foster collaboration in the workplace is through gaming. In new building layouts, it's quite common to see a game room or recreation space on the blueprint. According to How Playing Games Increases Workplace Productivity , which discusses the benefits of game-play in the office, TechnologyAdvice.com \"compiled research on how games impact the brain, and it looks like the easiest way to increase productivity is not providing more vacation, bigger paychecks, or improved benefits. Instead, you should encourage your employees to play more games!\" It has been found that games impact and affect the brain to boost motivation, help improve memory and time-management, and build collaborative skills between individuals and teams. When CenturyLink acquired Tier 3 , an eclectic clash of an old school TELCO mentality mashed with a hip start-up culture. The outcome? Three newly-modeled offices that foster collaboration, productivity, and gaming! The culture shift took the company towards the future in both innovation and culture.  All day meetings, PowerPoint presentations, and impersonal email threads dwindled as a more Agile-centric workflow emerged. Now, employees collaborate over lunch, a game of Pinball, or via Slack in their team room or on their mobile device to complete work. All of this allows for the teams to build and deliver products and services at scale. It's quite common to walk down the halls of the CenturyLink Development Center and have a team member whiz by on a scooter, or witness an intensive dart game between members of a software development team. What may seem, at first glance, to be an adult playground, is actually an infrastructure for productivity to escape the cube. If you listen to the conversations around the old-school arcade game, you'll often hear a team walking through a brain-storming activity or exchanging ideas about how to navigate a road block on a project. In essence, the game room is the new conference room. The best part -- the collaboration is not forced. No one is staring at the clock on the wall. Organic and authentic conversations between colleagues and teams carryover to the team rooms where employees are revitalized and inspired by the gaming/collaboration break and are ready get back to attacking the code block. Walking down the hallways, you'll see white boards that aren't so white -- they're filled with ideas and written content geared to educate and inform.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-is-backend-as-a-service", "author": ["\n              Jessica Weber\n            "], "link": "https://www.ctl.io/developers/blog/post/what-is-backend-as-a-service", "abstract": "Backend-as-a-Service (Baas) is a mobile development trend that will continue to increase in popularity in the coming years. According to Global Industry Analysts Inc., a business strategy and market intelligence source, \"the global market for Backend-as-a-Service (BaaS) is projected to reach US$31.6 billion by 2020, driven by the rise of app driven economies worldwide and the ensuing indispensability of mobile applications in the creation of economic value\" Global Industry Analysts Inc. . As BaaS develops into a market-driver for app-based economies, it's important for the developers who work on the backend, and for tech company investors and product managers alike, to understand what BaaS is and what value it brings to the market. The basic premise for BaaS is \"API-first\" development, meaning an API is constructed first and acts as base on which to build different mobile platforms (IoS, Android, etc). This contrasts with another popular approach, \"mobile-first\", sometimes known as Platform-as-as-Service (PaaS), where a mobile web page is constructed first, and then the backend product is built for each different platform Nordic APIs . Traditionally, the mobile-first approach focuses on top-down development, with developers first creating the landing page for the app, and then developing separate products for each platform, essentially creating self-contained and self-managed apps for each platform. The API approach operates on the premise that all platforms and apps, regardless of their separate idiosyncrasies and more complex requirements, have essential base elements and needs in common, such as push notifications, user administration, and storage capabilities. Over the coming years, the API-based BaaS approach will provide third-party services as a backend function, with users building platform-specific apps on top of a reusable base. This API base will house repeatable functions, such as push notifications, integration with social networks, file storage and sharing, location services, messaging and chat, user management, business logic, and usage analysis . These elements will be available to all apps built on top of the backend, regardless of what app platform the end-user will consume, such as IoS or Android. The reusable services provided by BaaS contribute several advantages over traditional front-end development. A reusable and repeatable backend brings many benefits, such as: Instead of many developers being forced to recreate a stack for each mobile app they develop, a BaaS service can provide for much of their underlying processing needs. Their main issue would then be connecting to an API, instead of spending hours developing customized stacks that then have to be re-created, changed, and reassembled to fit the needs of each different app platform. Developers can build just what they need on top of existing structures, instead of starting from scratch each time. If each app has the same underlying base, than BaaS has the potential to easily link apps across platforms. This has many benefits, from easier data sharing to better accessibility for cloud storage, a quicker spin-up time, and an overall better user experience. Think of BaaS like a \"starter home.\" Each user starts off with the same basic elements and continues to add to those elements to create their own customized \"home.\" However, because the base elements of the house are all the same, other users have the potential to more easily understand and even interact with or fix the \"house,\" creating a unified backend that has a better and stronger user base. There are a few issues that developers who create or use BaaS products should consider before implementing or using a BaaS system. For developers helping to create BaaS products, the main focus should be continuity in the consumable APIs and SDKs. This means each one should be able to be consumed repeatedly in the same way, across the backend. This creates less confusion to consumers, as they can learn the patterns and code once for consumption of any API you provide, instead of having to learn different rules for each product. Unlike mobile-first development (PaaS), BaaS is, understandably, focused more on the underlying systems of a backend. It requires that web apps be developed around the functionality of that system, instead of building a customized stack for each app. Mobile developers will have to take this into consideration in their design and use of the APIs and SDKs in a BaaS solution. In a BaaS model, data is first Nordic APIs . The API that provides this data should be the center of the app development. There are many decisions that will have to be made around the data, such as who can access it and how the app will handle the data. This can put a strain on the security of an app, so precautions should be taken to make sure that the data is not only accessible, but secure as well. Using a backend to take some of the development tasks off your plate provides many benefits, including less complicated app development, less development time, and more accessibility to different platforms. When considering a switch to the BaaS model, either through building your own or using a BaaS provider, make sure to research the issues and come up with a development plan -- a uniform design can make all the difference in a successful implementation. Sign up for our Developer-focused newsletter CODE . Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. CenturyLink Cloud – We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "five-popular-messaging-apps-encryption-behind-them", "author": ["\n              Jonathan Townsend\n            "], "link": "https://www.ctl.io/developers/blog/post/five-popular-messaging-apps-encryption-behind-them", "abstract": "The world is more connected thanks to mobile apps. After all, just about everyone needs a way to communicate with friends, family, and colleagues at work. When it comes to messaging apps, it's easy to be overwhelmed by the plethora of choices available to us based on network provider, platform, device versioning, and of course, the type of communication taking place. As such, we've decided to list five popular messaging apps and the important security features behind them, because secure messaging and privacy should always be a top priority. As the world of mobile devices continues to expand, messaging apps have to follow suit. Listed below are five popular messaging apps and their encryption features that users depend on for daily communication. What is end-to-end encryption? For the uninitiated, end-to-end encryption basically means that only a sender and the recipient of a particular message can see one another's messages. In essence, the messages cannot be decoded and unraveled in-transit or during transmission by outsiders or even the maker of the application. According to The Electronic Frontier Foundation (EFF) – a leading nonprofit organization that is committed to \"defending civil liberties in the digital world\" and \"champions user privacy, free expression, and innovation through impact litigation, policy analysis, grassroots activism, and technology development\" – very particular methodologies have been put in place to assess the security of certain messaging apps, starting with encryption. The primary question surrounding encryption for messaging apps is centered on determining whether communication is encrypted in-transit. According to the EFF, \"this criterion requires that all user communications are encrypted along all the links in the communication path.\" In some instances, data and metadata (user names and addresses) transmitted on a company network may or may not be encrypted. However, for external, and more importantly, more common usage, encryption is a driving security feature – especially on unsecured and public networks. This criteria requires messages to be encrypted during transmission. Those messages then pass through a firewall before hitting the provider's servers, which means they can only be decrypted by a private key on the intended receiver's device. This is important because end-to-end encryption ensures messages can only be read by the intended recipients, with no data logged on untended servers. Enabling end-to-end encryption for all messages allows the user to remove metadata from single messages (i.e. the time the message was sent, geo-location data, seeing if and when the message was read by recipient, etc.). WhatsApp is perhaps the most popular messaging app because it enables easy communication across global networks. Most people use WhatsApp on a wireless Internet connection or using their cellular data network to text, send picture and video messages, and even record voice messages to others. This is crucial for international travel, which is one reason why Facebook acquired WhatsApp. Regarding end-to-end encryption for WhatsApp, which was unrolled in May 2016, the encryption is based on the Signal protocol – the same tech also used by open source messenger Signal. Another popular messaging app, Facebook Messenger, isn't quite as secure, yet. However, Facebook is testing end-to-end encryption on its popular messaging application. Facebook Messenger currently uses secure communication channels to help block spam and malware. However, users need additional safeguards – especially when disclosing private health, financial, and location-based information with trusted recipients. SnapChat might be the trickiest messaging app, based on the functionality that makes it so popular – the immediacy and temporary status of messages sent and received. At the most basic and common method and level-of-use, users send and receive messages that are on set timers for one-time viewing before the messages are 'deleted' and inaccessible. SnapChat allows messages to be encrypted in-transit; however, there's a catch. According to the tech-based site, Recode , 'Snapchat messages are encrypted while at-rest on Snapchat’s servers (though the company has the encryption key if needed). Snaps are deleted from the servers as soon as they’re opened by the intended recipients, and Snapchat claims these delivered messages “typically cannot be retrieved from Snapchat’s servers by anyone, for any reason.” But unopened Snaps are kept on the servers for 30 days before being deleted. That means Snapchat might have to hand over unopened, private messages if required by law.' Although Skype is most-commonly used for video and voice conference calls, it has an instant messaging chat window feature. This is incredibly helpful for private conversations that take place with people on the call, in queue for the next call, or who just happen to be online and connected to Skype. Skype does allow messages to be encrypted during transmission, but Skype does not offer end-to-end encryption for any instant messages sent on the platform. Instead, messages are stored on Skype’s servers for a presumably set length of time. What does this mean for users? Well, in short, it means Skype has the capability to turn over private messages on its servers if summoned by a judge or required by law depending on the country and communication in question. Allo is a messaging app made by Google to be released this summer on Android and iOS platforms. What makes it popular is a feature-rich interface complete with graphics and doodles. From a functionality standpoint, the app works with Google Assistant to merge schedules and make suggestions for calendars and tasks. Allo does include an option for users to send their messages with end-to-end encryption, but they must opt to turn on and configure the encryption in the settings. This 'incognito' mode will be an option that includes capabilities like expiring chats, private notifications, and of course, end-to-end encryption. In an effort to provide a uniform security feature that users have come to expect with messaging apps, Allo differs in that end-to-end encryption is not a default setting like it is in WhatsApp and Viber. The complexity, however, really lies in the app's dependence on reading your messages to help coordinate events, tasks, and fixtures in the calendar – which it can’t do if your chats are end-to-end encrypted. Edward Snowden , whistleblower and former NSA contractor, was critical of Allo and expressed his views on Twitter stating, \"Google's decision to disable end-to-end encryption by default in its new #Allo chat app is dangerous, and makes it unsafe.\" For users, Allo presents two options: privacy and increased security or transparency through interactivity. Although users will often choose for the transparent and less-secure option, security is still an issue and civil liberties advocates are already voicing displeasure about it. What users need to know is that end-to-end encryption is becoming a standardized expectation built into the software of popular messaging apps. Ultimately, the level of security is up to the user's discretion and day-to-day needs. Additionally, the type of communication and the network on which that communication takes place on are main factors to consider. Overall, end-to-end encryption is a topic that continues to drive discussion and debate on multiple forums. From personal civil liberties to law enforcement agencies demanding access to files and data – there are many reasons that default encryption on mobile apps is a Catch-22 of sorts. By providing increased security, it could also cause law enforcement and auditors to lose access to important communications they need in order to secure criminal and civil court convictions. Sign up for our Developer-focused newsletter CODE . Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. CenturyLink Cloud – We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "five-books-every-developer-should-have", "author": ["\n              Jessica Weber\n            "], "link": "https://www.ctl.io/developers/blog/post/five-books-every-developer-should-have", "abstract": "Reading Time: about 3 minutes In the world of software development, books can sometimes seem like an outdated medium. Trends often change in the development world faster than books can be re-written and printed, causing a lot of \"continuing education\" material to be hosted online instead. However, there are some books that are just meant to be read more than once — the classics that a developer can reference again and again when encountering new challenges that put their skills to the test. Code Complete , Steve McConnell This book stresses that construction is the central activity in software development and should therefore be taken seriously, with a detailed process to follow that includes: design, construction planning, coding and debugging, unit testing, integration, and integration testing. McConnell emphasizes construction above requirements and documentation because construction delegates how the software actually works, unlike documentation and requirements, which can both be changed. The author also shares tips for becoming a good programmer. The Pragmatic Programmer , Andrew Hunt and David Thomas Andrew Hunt and David Thomas wrote The Pragmatic Programmer with one thing in mind -- the central process of making working and sustainable code out of abstract requirements. Each chapter is a self-contained category that covers topics such as: dynamic code, exceptions, effective testing, and authentic requirements. The chapters also contain many real-world examples that make the book relatable and engaging to coders everywhere. Growing Object-Oriented Software, Guided by Tests , Steve Freeman and Nat Price Test-Driven Development (TDD) is the central theme of this book, and Freeman and Price have delivered an exceptional publication on using test doubles to drive design. TDD is based on the concept of writing tests for code before developing the actual code. This book incorporates real-world examples for the processes and designs the authors present, along with extensive knowledge on how best to implement the process of TDD into actual software development projects. SmallTalk Best Practice Patterns , Kent Beck This book holds a special place among the esteem of the developers at CenturyLink, and rightfully so. One programmer describes it best as \"the best book about patterns 'in the small' — at the level of functions, methods, and classes — as opposed to the 'design patterns' that involve multiple classes.\" Patterns are the crux of this piece, as Beck dives deeply into the \"how's and why's\" about the code he writes. As with all good development books, extensive examples are used to explain the theory behind structuring and patterning code and working with methods, messages, state, collections, classes and formatting for a more effective outcome. Refactoring: Improving the Design of Existing Code , Martin Fowler, Kent Beck, John Brandt, William Opdyke, and Don Roberts The many authors of this book may look familiar to those in the developer community — this book is a true collaborative effort among many of the big authorities in the development game. As new and more efficient techniques and languages are developed in the coding community, there is an increasing number of code that will have to be re-factored in order to stay current with emerging technologies. An increasing and expanding part of a developer's job could easily be reworking code, so having a firm grasp on the easiest and most impactful ways to work with and change that code into an optimal application is a skill-set that is vital to any coder's portfolio. Not a CenturyLink Cloud customer? No problem. Designed for your business needs today and tomorrow, the CenturyLink Cloud is reliable, secure , robust , and global . If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "five-meetups-you-should-join-today", "author": ["\n              Jessica Weber\n            "], "link": "https://www.ctl.io/developers/blog/post/five-meetups-you-should-join-today", "abstract": "Reading Time: About 3 minutes Meetup.com has become an increasingly important online venue where people from all backgrounds can make plans to meet over common interests. From hiking and exercise, to book clubs, supper clubs, and social clubs, Meetup has a group for just about everything. This includes the IT community, which has taken to the website to organize formal and informal events where everyone is welcome and encouraged to participate. These Meetup events play an important role in the St. Louis IT community. As a growing hub and contributing player to the IT scene, St. Louis has developed a number of Meetups that many professionals use to share their knowledge and expand their education. The following are 5 well-established groups in the area to check out today. Note: Many other cities have similar groups in their areas as well (many of them have \"chapters\" in all major cities) – be sure to search your own city for similar Meetup opportunities. St. Louis Full Stack Web Development This group deals with everything full-stack, such as database design, server-side coding, mobile javascript, and web hosting. Users focus on sharing tips and helping each other create full-stack designs that are scalable and dynamic. Members meet once a month or more to share knowledge and code as a community. St. Louis Unix Users Group (SLUUG) The SLUGG is a well-established, not-for-profit that maintains a big presence within many communities. Members promote open information exchange on UNIX and UNIX-like systems. Meetings can be more formal or informal; sometimes there are formal presentations, but many times members show up to collaborate on projects and solve problems. Also check out slugg.org for more info. DevOps STL DevOps is unique in that it is not targeted towards a developer-specific user group, since within and organization it affects more than just a development team. Engineers, managers, team leads, and architects all participate in the DevOps world. This group brings everyone together to discuss the latest trends, processes, and tools that continue to help the definition of DevOps grow and evolve. St. Louis Machine Learning and Data Science The Machine Learning and Data Science group focuses on the science behind artificial intelligence. This includes statistics, data analytics, the internet of things, big data, etc. Members focus on these areas with various methods, including discussions, hands-on learning, and group coding. Code Until Dawn St. Louis Code Until Dawn is all about giving developers the push they need to see significant progress on their personal projects. Once a month, the group holds an all-night coding session where fellow coders each other on, focus on group projects, or participate in knowledge-swapping as a cohesive group. The set time and \"up-all-night atmosphere\" provide the incentive for many to finish what they've started or iterate on an ongoing project. DevOps Automation at CenturyLink is more than a series of tools or automation. It’s a culture. DevOps breaks down traditional silos and departments through the intersection of core methodologies that the industry is built upon – automation, collaboration, integration, and communication. Designed for your business needs today and tomorrow, the CenturyLink Cloud® Platform is reliable, secure , robust , and global . We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "angularjs-cloud-application-manager-automation", "author": ["\n              Manuel Martin\n            "], "link": "https://www.ctl.io/developers/blog/post/angularjs-cloud-application-manager-automation", "abstract": "AngularJS is a big part of life at Cloud Application Manager. We build our front-end with it. The principles of AngularJS and Cloud Application Manager are strikingly similar. One relates to applications while the other to automation. As a huge Angular fan, I enjoy building one of the most powerful automation interfaces using Angular. It’s easy to see why. Angular is a Javascript framework to build single-page web applications. A pretty popular one at that because it’s got a lot of blogs and comments devoted to it. We chose Angular at Cloud Application Manager because of what it allows us to do. Angular includes many tools. A template system, a router, basic components like controllers or services. It includes elements to communicate smoothly with a server, modules for the unit and end-to-end testing, HTTP backend REST call mocks, and an injection subsystem to handle dependencies. On top of these, we use RequireJS as a package dependency manager. It constructs dependencies and loads classes, modules, and files automatically. RequireJS helps us divide Cloud Application Manager into small chunks of code, which we join and minify later at build time like other compiled programming languages. One big reason we like AngularJS is the componentization. Componentized architecture gives isolation. It lets you encapsulate logic in a standalone code and then reuse it. Most web applications evolve in that direction today. Even browsers now support the web components working specification . In Angular, we componentize using directives. It lets us extend HTML syntax to create custom elements. You may ask, why components and not just Angular templates, controllers, and services? That’s because components give us the following advantages: Isolates component scope . Isolated scope helps noob Angular developers avoid common problems like scope inheritance. Encapsulates things inside . A component can have its template, directive, controller, and in some cases services. Thanks to RequireJS, we also encapsulate all the component assets as images or styles. Provides reuse . Once you create a component, you can use it everywhere. Components give much more. Each component can nest more components. These can interact with each other. Angular2, the next on the horizon for Angular, is built on this principle and so is Cloud Application Manager if you think about it. Cloud Application Manager applies the same principles of AngularJS programming toward application automation. I am proud to claim that with AngularJS, we’ve built one of the best user interfaces to simplify cloud application management and automation out there. Want to Learn More About Cloud Application Manager and ElasticKube? Cloud Application Manager is a powerful, scalable platform for deploying applications into production across any cloud infrastructure – private, public or hosted. It provides interactive visualization to automate application provisioning, including configuration, deployment, scaling, updating and migration of applications in real-time. Offering two approaches to cloud orchestration — Cloud Application Manager and ElasticKube — enterprise IT and developers alike can benefit from multi-cloud flexibility. Visit the Cloud Application Manager product page to learn more.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "decentralizing-docker-how-to-use-serf-with-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/decentralizing-docker-how-to-use-serf-with-docker", "abstract": "In last week's post, we talked about how to create a 2-container application with Docker . Now we want to use a brand new project called Serf (from the makers of Vagrant) to have the containers learn about each other automatically. Serf uses gossip-based membership to create a decentralized solution for service discovery and orchestration that is lightweight, highly available, and fault tolerant. Sounds awesome, but how do you use it with Docker? First, let's setup serf agent on a Docker container. Well that was easy! Now let's try to connect to our serf agent from our Docker host machine to make sure it is working. Now that we have serf available on your host, you can connect to your Docker container's serf agent to see it in action. Sweet. We now have a decentralized solution for Docker containers with a mini two-system serf cluster. The next step is to launch a MySQL database Docker container with serf built-in. Conveniently, you can find a Trusted Image on the Docker Index called ctlc/mysql-serf . The final step is to launch a WordPress Docker container that has serf built-in. Luckily, I have built one already called ctlc/wordpress-serf and link it to the serf container we already named \"serf\". Congratulations! We now have created a distributed Docker container system that is self-aware. Every container can run docker members and get a list of the other members in the system and what their roles are. We can even see WordPress in action by curling the website. How does this work? By a special line in wp-config.php that makes the magic happen. Notice that the serf members -tag role=mysql call will automatically find the correct IP address for MySQL. Before we finish this week's blog post, let's start scaling out our WordPress by adding more instances of the ctlc/wordpress-serf container. You can see how easy it is to create a distributed application with Docker and Serf together, but this does not yet show the power of Serf. In next week's column , we will add an Nginx Docker container that will automatically detect when a new WordPress container comes into existence and HUP nginx to take advantage of it using serf event hooks.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "day-in-the-life-steven-landow", "author": ["\n              Daniel Morton\n            "], "link": "https://www.ctl.io/developers/blog/post/day-in-the-life-steven-landow", "abstract": "Our \"Day in the Life\" series gives you a glimpse into the everyday lives of team members and our culture here at CenturyLink Cloud . The series focuses on the different teams and the roles within each team, from project managers to developers, product owners, technical writers, and administrative staff. In this post, we highlight Steven Landow, whom CenturyLink Cloud hired directly out of a local high school here in the St. Louis metro area. He's obviously a brilliant guy and a motivated self-starter. He taught himself most of what he knows about programming. At present, Steven is a member of the Managed Services Product Automation team. He's been with the company less than a year, but in that brief tenure he's already worked in various technologies like Java, C#, JavaScript, Spring, React, and SQL, not to mention loads of other work associated with the DNS migration project that he's part of. He looks forward to the role he's going to play in helping to build a Managed DNS portal using Spring and React. Steven shows day in and day out that he's an important contributor on a critically important team. He enjoys the sense of accomplishment he gets from seeing something that he created, or changed, or improved going into production, and knowing that he made it happen. The following interview provides insight into Steven's role at CenturyLink Cloud and, of course, some little-known facts about Steven's hobbies and interests. 6:45 AM - Wake up for the first time, then hit the snooze button. I think the morning comes way too early. 7:00 AM - I actually wake up and get out of bed. It's time to feed the cat, my girlfriend's fish, and our three dogs. 7:30 AM - Shower, get dressed, and head out the door. 7:45 AM - Sitting in traffic. Sometimes Highway 64 is more like a parking lot that a highway. You just never know. 8:00 AM - I get to the office. I check my email and see if there is anything to follow up on. After that I head down the hallway to grab my first cup of coffee for the day. There are so many choices to pick from. I usually go for the Colombian dark roast, but sometimes I like a mochaccino. 8:15 AM - Time to follow up on emails and finish up any work from the day before. 9:15 AM - Stand-up and Team discussions. Daily stand-up usually takes about 10 minutes. Directly after that the team always has a \"collaboration\" session where we have more in-depth discussions on projects. We discuss what we've done and what we plan to do. 10:00 AM - Sometimes it's a meeting after standup, sometimes I will start working cards and stories on our Trello board. 11:00 AM - Lunch time! If it's Tuesday or Thursday, lunch is catered down the hallway in Moe's Tavern. Otherwise, I usually bring leftovers or grab something from the e-mart or the cafeteria. 11:45 PM - On occasion, the team gets together to walk some of the trails around the buildings here in Maryville Center. 1:30 PM - Continue working cards and stories for the day. 4:30 PM - Wrap up, pack up and go home. 5:00 PM - My girlfriend gets back from soccer. She's at it six days a week, and sometimes seven, if there's a game. 6:00 PM - Dinner time. 7:30 PM - Free time. Sometimes I'll work on side projects, or just read interesting things online. 8:30 PM - Netflix with my girlfriend. 10:30 PM - Lights out. Steven started teaching himself code as a kid. He landed some freelance work along the way and then starting shooting out applications. At 17 he decided to throw his resume on on Monster.com and see what happened. The rest is history. He got scooped up and works at CenturyLink Cloud. He's 18 now and a very busy guy. Steven shares a passion for technology. He fits right in with our culture here and our passion for technology. He's convinced that people excel when they look forward to coming to work. The casual environment at CenturyLink Cloud allows him to focused on his work without feeling uncomfortable. While pursuing a career as a software developer, Steven is also working on an undergraduate degree. He's taking night classes in Calculus III (love that multi-variable stuff!) and Statistics at one of the local community colleges. As he says, \"I just enjoy math.\" He's also looking at a long-range goal of obtaining a degree, possibly in Computer Science with an emphasis in Machine Learning. Check out our other Day in the Life features: Interested in the world of cloud development? Sign up for our Developer-focused newsletter CODE . It's designed hands-on by developers, for developers. Keep up to date on topics of interest including tutorials, tips and tricks, and community building events. Check out the CenturyLink Careers page for opportunities to join a fantastic team that truly cares about the craft of engineering.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "top-10-startups-built-on-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/top-10-startups-built-on-docker", "abstract": "Docker is a new technology gold rush. Docker, Inc. just raised another $15M in venture capital, and it seems like new Docker-based projects and startups are popping up every week. In this weeks post, I'm going to highlight the top 10 that have caught my eye. I have grouped them to emphasize the areas in common between them, but many do multiple things. All three of the Docker hoster startups listed here have SSD containers. 1. StackDock (by Copper.io) \"Docker hosting on blazing fast dedicated infrastructure\" StackDock sells SSD Docker-containers for $5/month. You do this by creating what they call a \"Docker Deck\" which allows you to specify a template with any number of technologies you can put into a Docker container. Then \"Drops\" are \"distilled Decks\" that give you an instance of the container. StackDock is a fast way to get going quickly with Docker. There is no free plan with StackDock. 2. Orchard \"Host your Docker containers in the cloud\" Orchard has a novel take on Docker hosting. Instead of providing push-button web interfaces for launching Docker containers like StackDock or Tutum, Orchard provides a pass-through CLI that lets you deploy Docker containers on the command line remotely with the same types of commands you use locally (just prepending their orchard CLI in front of the docker command you would usually use). For example: You can get $5 of credit for free with Orchard (effectively two weeks free for one 512MB container, paid containers start at $10/month charged hourly). Orchard also provides free private Docker registries for your apps which is a nice plus and persistent storage. One of the cool things Orchard has contributed to the Docker community is an open-source project called Fig which allows you to hook together code and databases really easily. 3. Tutum \"IaaS-like control, PaaS-like speed\" Tutum aims to differentiate with value-added services like load balancing and persistent storage. Their web console is the most advanced out of the three mentioned here and has a logging console and a nice management dashboard. Tutum also has the smallest available Docker containers, starting at $2/month for 62MB RAM in what they call XS plan. They offer one month of free hosting for one XS container, so it is easy to try before you buy with them. One of the cool things Tutum has done is created an excellent set of Trusted Repositories on the Docker index that you can use to quickly setup databases and technologies using multiple stacks. 4. Quay.io \"Secure hosting for private Docker repositories\" This is the closest thing to GitHub that Docker currently has. Although Orchard has private Docker registries as well, Quay focuses on organizations so that you can collaborate with your team on building the perfect linux containers. Prices range from $12/month to over $200/month. 5. Flynn \"The product that ops provides to developers\" Flynn is not a traditional startup in the sense of bootstrapping or venture funded business, rather it is an open source project that is sponsored by developers and corporations (including yours truly, CenturyLink). The promise of Flynn is a lightweight Docker-based PaaS that runs anywhere. Flynn provides a set of “PaaS Lego” including an API that orchestrates the management of containerized services across a cluster. For easy out-of-the-box deployment Flynn includes Heroku-style “git push” deployment and management tools that utilize buildpacks. Flynn was inspired by PaaSes like Heroku but is designed to run more than traditional 12 factor apps by embracing the problem of state. 6. CoreOS \"Linux for Massive Server Deployments\" CoreOS, a Y-Combinator alum, is a startup backed by top-tier VCs Andreessen Horowitz and Sequoia Capital. This is one project to keep a close eye on. Unfortunately the documentation is not great yet, but you can essentially think of CoreOS as 3 projects right now. When you combine those three technologies and chant the right incantation (hint: look at this page where the systemd conf file says \"Description=My Advanced Service\") you will get a distributed CoreOS platform for deploying and managing your large-scale distributed application requirements. Awesome! If you need more hands-on guidance, you can always pay them to step you through it themselves. 7. Serf (by HashiCorp, makers of Vagrant ) \"A decentralized solution for service discovery and orchestration that is lightweight, highly available, and fault tolerant\" One of my favorite new open-source projects came out of the guys who make Vagrant: it's called Serf. I wrote about it last week in Decentralizing Docker: How to Use Serf with Docker so you can get a great insight into how to use it with Docker there, but essentially it is a hammer you can use where CoreOS and etcd is a nail-gun. Serf is really easy to use outside of Docker and can be used in a lot of different ways where etcd and CoreOS are pretty specific tools that aren't nearly as flexible (though definitely very powerful). 8. Shippable Shippable is a Techstarts startup that just raised $2M from angels and VCs . Their claim to fame is that they use Docker to build containers that run your unit tests faster, more distributed, and safer than in other CI/CD environments. Since they TEST your code using Docker containers, the awesome thing that they can do is DEPLOY the containers that they just tested wherever you want. This feature is not in general availability yet, but once it is, it could change the game for CI/CD across the board. 9. Memcached as a Service Though today this appears like more of a hobby project than a startup, it still provides an awesome utility and shows of the speed and potential of Docker by spinning up memcached services for you in less than a second. A really cool project and Julien Barbier has posted some awesome slides about how he built this SaaS using Docker. 10. The Docker Book Last but certainly not least is the Docker Book. A book has more alike with a startup than you might realize at first. As an author, unless you are satisfied with selling 10 copies of your book (all to your mother), you have to figure out your go-to-market strategy and your product-market fit. Nathan Barry has written a great book about treating technical books like startups called Authority . If you measure the health of an open source project by the ecosystem created around it, I think it is fair to say that Docker is the picture of health. Next week we will spend more time looking at some of the top open source projects built using Docker. Make sure to subscribe to our newsletter to get updates. You can subscribe at the top or bottom of this page.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "deploy-to-a-mesosphere-cluster-with-the-panamax-marathon-adapter", "author": ["\n              Ross Jimenez\n            "], "link": "https://www.ctl.io/developers/blog/post/deploy-to-a-mesosphere-cluster-with-the-panamax-marathon-adapter", "abstract": "Late last year we announced the Remote Deployments feature in Panamax which allows the deployment of Panamax templates to either Kubernetes or CoreOS based Clusters on Remote Infrastructures. Today, the Panamax team is happy to announce that a Marathon remote deployment adapter is now available, making it possible to deploy to Apache Mesos and Mesosphere DCOS clusters with Panamax, right from your browser. Before you run your application on your cluster, you’ll need to set up a Panamax remote agent and the Marathon adapter on the cluster. In order to install and utilize the Panamax Remote Agent, Panamax uses a dedicated installer node. The Panamax remote agent can run on Linux with docker installed. However, we recommend using CoreOS for simplicity. Following the normal methods of adding VM with your cloud provider. Ensure that the node resides on the same VPN as your deployment node or cluster and the node has a publicly accessible IP. Docker must be installed on the node where you agent will reside. After Docker is installed successfully, follow these steps to install the Panamax Remote Agent: Next, use this token to add a remote deployment target to your Panamax client. From the Manage nav button, go to the Remote Deployment Targets page. Click on Add a new Remote Deployment Target, enter a friendly name and your copied token from the Remote Agent. Click Create Remote Deployment Target to save. The deploy target is now available to deploy applications. Next, back on the search page, enter the term for the application you want to deploy. Click “Run Template” but now select “Deploy to Target.” On the modal window, select your Marathon target. From here, you’ll have the option to specify scaling information. After you’ve deployed, you’ll see the deployment jobs and applications in your Marathon UI. Note: In order to reach your deployed application, additional ports may need to be opened on your cloud provider. Using Docker and Marathon requires some unique considerations, depending on how you've chosen to architect your application. First, Marathon does not support Docker links. Panamax has translated these dependency declarations into pre- and post-deployment conditions that must be satisfied for a successful deployment. For example, if your web service depends on a database service, the Marathon adapter will wait for the database to finish deployment before firing off the web service. In this example, the database has a post-condition that it must have an available port. The web service has a pre-condition that expects to access environment variables indicating the database’s available host and port. If the database service were to fail deploying, or if any of these conditions were not met, the application deployment job would time out. Instead of Docker links, the dependency relationship is defined via these pre- and post-conditions, and through environment variables and port forwarding rules. Additionally, dependent services must either be fronted by a proxy that you have set up, or they are limited to one instance, regardless of scaling information given at the time of deployment. If an application asks for 3 web services and 3 databases, the database service will be limited to one instance.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "auto-loadbalancing-with-fig-haproxy-and-serf", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/auto-loadbalancing-with-fig-haproxy-and-serf", "abstract": "In this series of blog posts, we have progressively built increasingly complex Docker-based applications. We started with a simple 2-container Docker app . Then we built a 3-container decentralized Docker app with Serf . ( Edit: Fig was purchased by Docker in mid-2014, and renamed Docker Compose. As such, all Fig references now apply to Docker Compose.) Today we are building a 4-container Docker app that uses Fig , HAProxy and Serf . When you start building apps this complex with Docker, maintaining the state of your containers can become very difficult. There are multiple ways to manage this complexity, but this we will cover just one: Fig from the makers of Orchard . Fig is a great way to keep all the state of your complex application in one place. Let's look at a fig configuration that is equivalent to the 2-container system we built in our first blog post . And to run this fig.yml file, you put it in your current working directory and run: Adding Serf is a great way to get your containers to auto-discover each other. Let's look at a fig configuration that is equivalent to the 3-container system we built in our second blog post . And to run this fig.yml file, you put it in your current working directory and run: Fig can automatically build the links between Docker containers and save you the time and hassle of writing docker run -link commands yourself. However Fig can not talk to the containers themselves, so if you add a new web server via Fig's scale command (e.g., fig scale web=3 ), you would need to restart any load balancers to take advantage of the new environmental variables which could mean downtime for your users. This is where Serf combined with Fig shines. The most powerful part of Serf are the event hooks, and the previous examples did not show off this power. To see the power of Serf's event hooks, you can create an haproxy container. Luckily I have created a ctlc/haproxy-serf Trusted Build Image with Serf built-in for you to use. This image has a different serf agent call than the other images. For reference, let's first look at the serf agent call in the WordPress container. You can see it on GitHub if you want to. Ok, simple enough. So then what does the haproxy serf look like? You can check it out on GitHub but here it is if you don't want to click: Clearly a lot more going on. Let's break it down. First let's look at serf-member-join.sh : This bash script will be called every time a new Serf agent is added to the network. The script looks for only the web servers and adds them to the haproxy config file. Then at the end, it gracefully restarts haproxy with no downtime. Cool, right? I won't paste the code for serf-member-leave.sh , but it un-does the above work. Now let's add haproxy to the fig.yml . Notice something interesting here, I actually replaced the DB image from ctlc/mysql-serf to ctlc/mysql and the app still works. I did this to make a point. The ctlc/mysql container doesn't have a Serf agent in it like ctlc/mysql-serf does, so in this example we use a hybrid of Serf detection for the load balancing and Docker linking for the database connection string. The upside to using the ctlc/mysql container is that you can set the MySQL database and root password through environmental variables instead of having to build a custom local image that has a random password assigned. To show this 4-container system in action, you simply run fig up -d (which is the same command used to restart the running system). Sticking HAProxy, Apache and MySQL in a single Docker container might be the easiest way to get a site up and running quickly, but it doesn't scale. Getting a Docker-based system to scale is not rocket science, but takes some careful thought and attention. The projects and tools being built around Docker today can make your life a lot easier if you know how to use them correctly. In the coming weeks we will look at multi-host Docker solutions and alternatives to Fig and Serf like CoreOS .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "heroku-on-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/heroku-on-docker", "abstract": "I am sure you have heard of Docker , but have you ever actually deployed a real app on it? How would you even start to move Heroku's 4+ million apps into Docker Containers? Not many people have even tried. Building an app on Docker can be incredibly hard and frustrating. Not at all like using Heroku where everything is taken care of for you. With Docker, you have to learn about Dockerfiles and then try to get one that works just right with your code. If you are lazy (like me) and want to just try out Docker with no fuss, I will guide you through the whole process from start to finish. In the end, we will have generic containers we can use with any Docker Developer Tools like CoreOS or Docker Compose manifest files. We will create: All without learning anything about Linux Containers or Dockerfiles. If you are on a Mac, the install process has been greatly improved. You can now get Docker running in just seconds: You can use github.com/CenturyLinkLabs/building (like progrium/buildstep but without tarfiles and with extra goodies) to turn any Heroku-compatible app into a Docker Container: Hint #1: To run your app, try: docker run -d -p 8080 -e \"PORT=8080\" myapp Hint #2: To re-build your app, try: docker build -t myapp . (Optional: this will allow you to share your app container with anyone in the world via docker pull myapp. Make sure to make it a private repo if it has your private code in it) Yes, that's it. Everything else is handled for you. Code discovery: done. Dependencies: done. Startup script: Done. Heroku on Docker: Done. This works with any technology: Ruby, Node, Java, Play, Python, PHP, Clojure, Go, Dart and more. Don't believe me? Let's try a simple Node application. PRO TIP: Avoid using semver ranges starting with '>' in engines.node (See https://devcenter.heroku.com/articles/nodejs-support ) To run your app, try something like this: In my case: Ok, that was cool, but how about a Rails app? Can you do a Rails app? To run your app, try something like this: docker run -d -p 8080 -e \"PORT=8080\" myuser/rails-container-name:latest Ok, that was cool, but how about WordPress running HHVM? To run your app, try something like this: docker run -d -p 8080 -e \"PORT=8080\" myuser/wordpress-container-name:latest Wait, that's not HHVM. That's nginx. No fair, I cheated. Now to do it right: To run your app, try something like this: docker run -d -p 8080 -e \"PORT=8080\" myuser/wordpress-container-name:hhvm With building you can too. There is a -o flag that lets you set a fig.yml output file which you can pass to Fig. ( Edit: Fig is now Docker Compose ) Notice that when you combine building with fig , you never need to manually run a docker command again. Ahhh, abstraction. One of the most powerful parts of Docker is the fact that you can modify every byte of the underlying \"slug\" as Heroku calls it. You can put any binaries in any locations. You can build your own Docker images from scratch. But that power comes at a price: ease of use. Tools like building and fig set out to bridge the gap and let everyday developers take advantage of cool new technology without having to get a PhD in DevOps.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-vs-vagrant-cloud", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-vs-vagrant-cloud", "abstract": "Vagrant is a cross-platform tool that lets you specify Virtual Machines ( as a Vagrantfile ) to deploy to a hypervisor (like VirtualBox on your laptop or vSphere or Hyper-V at work). Docker is Linux-only tool that lets you specify Linux Containers ( as a Dockerfile ) to deploy to any host running Docker. So they are similar because they both take a config file. But different because Vagrant gives you Virtual Machines in minutes and Docker gives you Linux Containers in milliseconds. In fact, if you want to use Docker on your Mac, you usually use Vagrant to create a Linux VM first and then install Docker on that host (since it depends on the Linux kernel). Creating a Vagrant Linux box is killer easy: But the lines between VMs and Containers are blurring. With Vagrant's recent 1.5 release there was a lot added, including Vagrant Cloud. Vagrant Cloud is a hosted service for finding boxes, sharing boxes, managing Vagrant Shares (reverse proxy to any port on your vagrant boxes). One of the coolest parts of Docker is how collaborative the process of curating encapsulated Linux environments could become. From my laptop, I could now docker commit and docker push . Then from your laptop, someone else can docker pull and docker run your exact same Linux Container. This is revolutionary. This does to Linux what git has done to source control. Vagrant Cloud introduces the same social curation features to any regular Virtual Machine. With vagrant box update , you can pull changes to your virtual machine like docker pull . You can even host private virtual machine images in the Vagrant Cloud. This is very interesting for a number of reasons: So, although Virtual Machines are slower than Docker Containers, they make up for it in flexibility and isolation. No. Docker is interesting for many more reasons than just docker commit . But this is a clear indication that Virtual Machines are not going to give up without a fight. The worlds of Virtual Machines and Linux Containers are colliding. They are inspiring each other and leading to mutual technical advancements. For example, Serf is a project that comes from the makers of Vagrant but can easily be used with linux containers . The world is headed to a place where Linux Containers co-exist with AND on Virtual Machines. Some startups are already supporting native Linux Container hosting. OpenStack allows Docker Containers as a first-class citizen. We are going to continue to see Vagrant and Docker play really well together and push each other's boundaries, which will only lead to more goodness.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "top-10-open-source-docker-developer-tools", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/top-10-open-source-docker-developer-tools", "abstract": "Navigating the ever-changing Docker ecosystem can be difficult, so we at CenturyLink would like to make it easier for you. A few weeks ago, I wrote about the top 10 startups using Docker . This week we are talking about the top 10 open-source projects using Docker . I will group them to highlight what each project focuses on. Although Docker was a technology originally built out of a PaaS (DotCloud), there have been multiple attempts to create micro-PaaS'es out of Docker. 1. Flynn - https://github.com/flynn 961 stars, 24 forks in Mar 2014 \"Flynn is like Sinatra where Cloud Foundry is like Rails\" Flynn is one of the most anticipated Docker PaaS'es right now. With nearly 1,000 stars and dozens of forks, this open-source Docker project has not even been released yet. With a git push deployment to Docker, it is easy to see why there is so much anticipation. Flynn simplifies deploying and maintaining applications. Instead of using complex configuration management systems, Flynn allows self-serve management of containerized deployments, making life easier for ops and developers. Flynn is also different than the other projects on this list because they are a sponsored open-source project. With 14+ sponsors having donated over $80,000, this project is definitely one to watch. 2. Deis - https://github.com/opdemand/deis 1,341 stars, 120 forks in Mar 2014 \"Your PaaS. Your Rules.\" With over 1,300 stars and over 120 forks, Deis is more established than Flynn and also has a git push deploy style. Deis leverages Chef, Docker, Django, Celery, Heroku Buildpacks, and Slugbuilder to do its magic. Deis comes with out-of-the-box support for Ruby, Python, Node.js, Java, Clojure, Scala, Play, PHP, Perl, Dart and Go. Also, Deis can deploy anything using Heroku Buildpacks, Docker images or Chef recipes. Deis can be deployed on any system including every public cloud, private cloud or bare metal. Deis currently supports automated provisioning on EC2, Rackspace and Digital Ocean. In an upcoming blog post, we will compare Deis and Flynn in more detail. 3. Dokku - https://github.com/progrium/dokku 4,806 stars, 384 forks in Mar 2014 \"Docker powered mini-Heroku in around 100 lines of Bash\" If you just want the smallest viable git push to Docker container functionality in the world, take a look at Dokku. From the maker of localtunnel (a super useful utility to reverse-proxy your localhost ports to a public url), this little power-house of an open-source project has the most eye-balls. It is not as feature rich as Deis or Flynn, but it is incredibly easy to install and use on Ubuntu 13 or 12.04 x64. Note: Users on 12.04 will need to run apt-get install -y python-software-properties before bootstrapping stable. 4. CoreOS - https://github.com/coreos 2,564 stars, 237 forks in Mar 2014 \"CoreOS enables warehouse-scale computing on top of a minimal, modern operating system.\" CoreOS is developing best practices for deploying containerized applications to production. It is not a single open-source repository, but rather a collection of many open-source tools that can be used together including etcd , docker , and systemd . We have written about generating CoreOS files from Fig files before because getting started with CoreOS can be a daunting experience. The etcd library is used as a universal key/value store to stitch services together and share service credentials across an entire application. Unlike many of the other projects in this list, CoreOS is both an open-source project and a venture backed startup (which is why CoreOS is also listed in our Top 10 Startups Built on Docker post). 5. Fig - https://github.com/orchardup/fig 1,526 stars, 51 forks in Mar 2014 \"Fast, isolated development environments using Docker\" I have written a few times about Fig because it is one of my favorite little utilities for Docker ( Auto-Loadbalancing Docker with Fig, HAProxy and Serf and Building Complex Apps for Docker on CoreOS and Fig ). Fig lets you write a simple fig.yml file that lists all the Docker containers your app needs and how they should link together. Once you write the fig.yml you just fig up -d and your app will be up and running. This blog is managed by fig right now. 6. Serf - https://github.com/hashicorp/serf 1,652 stars, 91 forks in Mar 2014 \"A decentralized solution for service discovery and orchestration that is lightweight, highly available, and fault tolerant\" Although Serf is not Docker specific, it is like jelly to Docker's peanut butter. Serf is one of my favorite new open-source projects and came from the guys who make Vagrant. I wrote about it a few weeks ago in Decentralizing Docker: How to Use Serf with Docker so you can get a great insight into how to use it with Docker there, but essentially it is a hammer you can use where CoreOS and etcd is a nail-gun. Serf is also really easy to use outside of Docker and can be used in a lot of different ways where etcd and CoreOS are pretty specific tools that aren't nearly as flexible (though definitely very powerful). 7. Drone - https://github.com/drone/drone 2,516 stars, 133 forks in Mar 2014 \"A Continuous Integration platform built on Docker\" Drone (another project that is both an open source project and a startup) gives you a simple go binary, distributed in a debian file, that gives you a full CI/CD pipeline hooked natively into Docker. Cool, right? Your code never needs to leave your laptop or your company's network to be tested, which is a huge deal for big company developers who have policies that prevent them from using public hosted services like GitHub and Travis. The other cool part of Drone is that you can deploy the fully tested containers into production and be assured that the exact same environment is used in both locations. Finally, Drone lets you build custom Docker containers with whatever custom binaries and configuration you need, which is way more flexible than most CI platforms today. 8. Shipyard - https://github.com/shipyard/shipyard 1,443 stars, 96 forks in Mar 2014 \"Open Source Docker Management\" Shipyard gives you the ability to manage Docker resources including containers, images, hosts, and more all from a single management interface including: Multi-Host Support, Container Metrics, and a RESTful API. I love this part, to deploy Shipyard, you just run: Then you should be able to login to http://localhost:8000 and get the pretty UI (more QuickStart docs available). Slick! Being able to visually see all your containers is killer and Shipyard is a great way to do that. 9. Kubernetes - https://github.com/GoogleCloudPlatform/kubernetes 3,598 stars, 501 forks in Sept 2014 \"Google's Docker Orchestrator\" Kubernetes is an open source implementation of container cluster management. In other words, it is a system for managing containerized applications across multiple hosts, providing basic mechanisms for deployment, maintenance, and scaling of applications. Its APIs are intended to serve as the foundation for an open ecosystem of tools, automation systems, and higher-level API layers. 10. Panamax - https://github.com/centurylinklabs/panamax-ui 510 stars, 43 forks in Sept 2014 \"Docker Management for Humans\" Panamax is a containerized app creator with an open-source app marketplace hosted in GitHub. Panamax provides a friendly interface for users of Docker, Fleet & CoreOS. With Panamax, you can easily create, share and deploy any containerized app no matter how complex it might be. If you measure the health of an open-source project by the open-source projects created around it, I think it is fair to say that Docker is the picture of health. This article was meant to be more of an overview than an in-depth comparison. In future weeks, we plan to talk more about the differences between some of these technologies like Flynn vs Deis.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "deploying-multi-server-docker-apps-with-ambassadors", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/deploying-multi-server-docker-apps-with-ambassadors", "abstract": "The  number 1 problem faced by people trying to use Docker in production today is how to run multi-server Docker apps. Docker's linking mechanism only works within a single-host environment, so if (for example) you want to link your database to your wordpress, they have to be on the same host. But there is a new ops pattern being established to fix this problem. It is called ambassadors. Both Docker and CoreOS have tutorials [ 1 , 2 ] for using the ambassador pattern. So what are ambassadors? Ambassadors are containers who's sole purpose is to help connect containers across multiple hosts. If you think of each container as it's own country, the ambassador of the country is in charge of communicating with foreign countries. How's that for explaining a self-explanatory metaphor? Here's a visual representation of this metaphor: (container) --> (local ambassador) --network--> (foreign ambassador) --> (container) The docker -link command works well within a single Docker host, but when your app needs to scale across multiple servers, linking gets you nowhere. This is where ambassadors come in. There is a clever little one-liner container based on busybox called ctlc/ambassador which sets up a simple reverse-proxy with socat . First, you need to create the basic service container (let's say MySQL): Notice that we are not exposing the MySQL port to the external world here. That's the ambassador's job, not the service's. Now, your service container needs an ambassador on the same host: Finally, run this command on a totally separate server to create a \"Foreign Ambassador\" for your MySQL service: The end result is a socat proxy running this little reverse proxy: Notice that the 192.168.1.52 IP address should be an address accessible to the foreign host, but ideally not to the entire world. In AWS, you would have to create a private network to do this. In CenturyLink Cloud , you get subnets for free by default, so this is less work. If you've been following along our progress on the blog, we've been using Fig, Serf and Haproxy to build increasingly complex applications . Now we will finally explain how to create multi-server AND multi-container apps. First, on your WordPress server, run fig up -d with this fig.yml: Notice that we can link to the db-ambassador container as if it were the MySQL database itself. On the machine that runs your MySQL server, you will need to run fig up -d with its own fig.yml . Now you have Docker containers on separate servers communicating with each other. Yay! What else can you do with ambassadors? The real power of ambassadors is when you use them to auto-discover services and to create audit and compliance barriers. Here are some of the cool things you can do: For example: Imagine being able to enforce a compliance barrier that prevents DELETE actions to a MySQL database. Your ambassador can be more than a passive reverse-proxy and can enforce what actions your code can take with your services. In future posts, we will dig into showing exactly how to go beyond this simple example. We will show you how to turn any container on the Docker Hub into a Serf-aware container and how to incorporate any container image into CoreOS's etcd . We will also talk about how to do audit and compliance with ambassadors. Join the mailing list to make sure you get updates when these posts are made.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "building-complex-apps-for-docker-on-coreos-and-fig", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/building-complex-apps-for-docker-on-coreos-and-fig", "abstract": "For Docker deployments, CoreOS is powerful but can be complex to setup. Fig is simple, but hard to scale to multiple servers. This blog post will show you how to bridge the gap between building complex multi-container apps using Fig and deploying those applications into a production CoreOS system. In last week's blog post, we covered building a 4-container app in Fig. To cut to the chase, here is the fig.yml that gets you a 4-container app: To show this 4-container system in action, you simply run fig up -d (which is the same command used to restart the running system). As you can see, Fig is super easy to get started with and use, but the #1 question people had was how do you scale this to multiple servers? CoreOS is not the easiest or most approachable system in the world. Understanding how to write systemd config files and getting everything configured just right can be time consuming and frustrating. Which is why I wrote a fig2coreos gem (github.com/centurylinklabs/fig2coreos), which converts fig.yml to CoreOS formatted systemd configuration files automatically. For some reason (I am not sure why yet), the default version of CoreOS shipped when you vagrant up is not the latest version of coreos, which means it does not have Fleet installed and has an old version of Docker (0.7.2). Once the vagrant coreos box stays up and running long enough to download the latest version of coreos, you can apply the update by either running sudo reboot in the coreos image, or by running vagrant reload --provision in the generated directory. The fig2coreos command parses your fig.yml file and generates a bunch of systemd config files. You can see the files in the directory that is created. Which looks complicated, but is the CoreOS version of this part of your fig.yml file: In addition to the start script for MySQL, there is also an etcd auto-registry script\nvia systemd in the db-discovery.1.service file. This registers the service into the etcd key/value store. In this example Fig application, we used Serf to make the Docker containers self-aware, but etcd is the standard CoreOS alternative to Serf available in all CoreOS containers. Do you need both Serf and etcd? No. The biggest difference between the two is where the daemons are run. In etcd, the etcd daemons are run outside of the Docker containers and it is up to a system like systemd to keep the etcd key/value pairs up to date. In Serf, the serf daemons are run inside the Docker containers, so when a container dies or goes offline, the peers of that container figure it out automatically. These are two very different philosophies for configuration and system management, but in the end result is essentially the same. Both Serf and etcd have the same ability to let Docker apps become more aware of their containers and the roles their containers play and interconnect. You can (in theory) use Fig on each one of your servers, each running independent Docker daemons. However linking containers becomes more complicated and network issues make it non-trivial, even with Serf. I am getting closer to showing how to deploy multi-host Docker in this series of blog posts, but this post won't show you exactly how just yet. Subscribe to get weekly updates and we will get there in due time. In the meantime, so that you know that I am not lying to you, watch this 2-minute YouTube Video about CoreOS Fleet to see an example of a real multi-host CoreOS deployment. You can see the power of CoreOS and Fleet in this simple video, but it doesn't connect the dots of how to put all the components together to re-create the demo. We are getting very close to showing you how to deploy production multi-host Docker apps in a cloud environment. Each week, I am getting you closer and closer to the answer by introducing slightly more complexity into the example. CoreOS and Fig are just two Docker technologies being built to manage complex applications. In the future we will show you other ways you may want to accomplish this same task with tools like Flynn and Deis .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-docker-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-docker-on-the-centurylink-cloud", "abstract": "One of the hottest technologies today is Docker and the ability to manage Linux Containers. As with most new things it is important to be able to setup and play with it, just to see what all the excitement is about. Follow along as we install a fully functioning Docker system on a CenturyLink Cloud server. In order to create a CenturyLink Cloud server we need to use the web interface, or the \"control panel\". This can be found at control.tier3.com . You will need to have an account and associated user name in order to login. From the pull down menu select \"create server\" and follow the dialogs to select the datacenter, number of CPU instances, amount of memory, and other pertinent information. Make sure you remember the admin password you enter, this will be associated with the root account for this server. Also, it is very important that you select \"Ubuntu 12 64-bit\" as the operating system. Docker relies on pretty new capabilities in the Linux kernel (lxc and cgroups) and is currently only officially supported on the Ubuntu distribution. By the time you read this tutorial this may have changed since the project is moving at an incredible rate. Once you have the server built we need to add a public IP and open the appropriate ports in the firewall. Select your server instance in the UI and you will see a choice for \"Add Public IP\". You can take the default networking choices but we need to adjust the ports selection. Select \"ping\", \"http\", \"https\", and \"ssh\". In addition we need to open the specific port range that Docker will be using for container applications so select \"custom ports\" and open the range 49100-49199. When we start deploying applications we need to remember that our firewall is blocking all ports except the ones we have explicitly opened. You can look at the server properties to see that the Public IP has been added. It is generally not a good idea to run as root, so in order to update our Ubuntu system and install Docker we will first set up a new user. After giving the new user the appropriate permissions we will logout of root and login as our newly created user. Open your terminal/SSH program and login to your server. Since we are logging in as root the command will be 'ssh root@Public IP'. From the above screen shot you can see my public IP is 74.201.240.211 and so my command will be 'ssh root@74.201.240.211 '. The root password is the server admin password you set while creating the server. Create a new user by following these steps: newuser can be a user name of your choosing. At the prompt create a password for this user and then accept the default user information by pressing ENTER at the prompts. In order to give newuser the correct privileges we must add the user to the sudoers file. Add new user in the ‘User privilege specification’ section. Once the file has been successfully edited press CTRL-x to exit the file; and Y to save the changes. Logout as the root user and then login as newuser . We now have a normal user with sufficient privileges to update the Ubuntu operating system and then install Docker. The version of Ubuntu that we installed while creating the server is 12.04 (Precise LTS 64-bit). This particular version of Ubuntu ships with a version 3.2 Linux kernel. In order to get the correct version of LXC that Docker requires we need to update the kernel from 3.2 to version 3.8. The Ubuntu folks at Canonical have made this easy for us by providing backported versions of new kernels. We will be updating Ubuntu Precise to Ubuntu Raring, at the same time will be getting built-in kernel support for AUFS. Once you have the base operating system updated and ready to go it is a pretty straight-forward process to install Docker. And that is all that is necessary to install Docker! Lastly, let's ensure that Docker is, in fact, correctly installed. The following is a very simple example of how to start an interactive Docker instance, download and run an Ubuntu instance, and then interact with the running container. You will be at a bash prompt within a Docker container. Type ‘echo Hello World from a Docker Container’. This will run the command “echo” inside a container and echo back the string “Hello World from a Docker Container” to standard out. Congratulations! You are now running Docker on Ubuntu within a CenturyLink Cloud server.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "linking-docker-containers-with-a-serf-ambassador", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/linking-docker-containers-with-a-serf-ambassador", "abstract": "A few months ago, we wrote about using Serf with Docker to create a service registry so that your app could talk to your database indirectly. Then just a couple weeks ago, we wrote about using the Docker ambassadors model. But in the Docker ambassadors post, we just setup a proxy ambassador, it didn't do anything but forward TCP traffic. This week, we are going to use Docker ambassadors to do something more interesting: the ambassador will register the app container and the database container into the Serf network for us . This could just as easily be using ambassadors to register into etcd for us (in fact, I will show how at the end of this post). But for now, let's start with Serf. First, let's setup a serf agent on a Docker container. Well that was easy! Now let's connect to the Serf agent with a Serf client container that will print out any members added to the network automatically. So now we can see our progress as we continue. Now we need to setup a MySQL database. I will pick a completely random MySQL database container from the Docker index: This is not a trusted image, so we actually have no idea what code we are running. However, brice/mysql is probably pretty unlikely to be running Serf since I picked it randomly from the index. So how do we get this database registered into Serf? Using the ambassador model of course. Now look back at your other terminal window running the ctlc/serf-members container. Now you can see that Serf now knows the internal IP address of the host running MySQL. We are almost done! Now all we need is a WordPress container that can consume the information from Serf. We can do this two ways: In this case, we will pick option 1 and leave option 2 as an exercise for the reader. Now look back at your other terminal window running the ctlc/serf-members container. Success. I promised I would show you how to register your database into Etcd instead of Serf at the beginning of this article. If you've made it this far, you deserve to see more. Ready for it? Instead of this: Run this: The ctlc/amb-etcd container will register with etcd instead of serf automatically. Of course you will need to put this in systemd formatted file and deploy using fleetctl for this to actually work, but the idea is the same. There is a lot you can do with the Docker ambassador model. This is just scratching the surface. In upcoming posts, we will show you how to do things like centralized transparent network logging and other fun stuff you can do with ambassadors.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "hacking-hack-with-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/hacking-hack-with-docker", "abstract": "The Hack programming language is the new hotness. Now that Facebook uses it for basically everything, every new startup is wondering if they should start using it too. But how do you even get started? What if I told you that you could be running a Hack server on your laptop in 30 seconds with Docker? ( Edit: Docker Machine , part of the Docker Toolset, is recommended over boot2docker) If your app gets more complex than \"Hello World\", you will need to create an updated container. To do this, you can use the built-in docker build command because the building gem created a Dockerfile for you. This will take you a fraction of the time that the initial build took because Docker caches your images in stages and most of your image will not have changed at all. This is just a simple example of how much better the developer tooling around Docker has become over the last year. With Docker 1.0 just around the corner, there has never been a better time to start incorporating it into your daily workflow. If you are ready to get your feet wet with CoreOS, try our Building Your First App on CoreOS tutorial.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "one-hour-with-lucas-carlson", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/one-hour-with-lucas-carlson", "abstract": "This week, Lucas Carlson, head of the CenturyLink Labs, spent an hour with the team at Hangops . Hangops has a weekly interactive DevOps video podcast. Some of the topics discussed this week include: HangOps hosts weekly discussions with the DevOps community via Google Hangouts and IRC. (Edit: As of Jan 2016, Hangops is in hiatus, but their Slack channel focused on DevOps and related topics is free to join. Join at: http://signup.hangops.com .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "dropbox-in-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/dropbox-in-docker", "abstract": "**In a post-Heartbleed world, even if you trust a company to do all security processes correctly, that might not be good enough. Security has become the responsibility of the user, not just the providers. Luckily, Docker gives us an easy way to encapsulate and reduce the attack footprint for many of the tools we use everyday. If you are using Windows, you might look into Bromium for a solution similar to Linux Containers for Windows. Today, I will show you how to encapsulate and isolate Dropbox on Mac OS X into a Docker container. Why? Because do you really know everything dropboxd does** First, let's install Docker and SSHFS with Fuse on OS X: The next steps you need to run within the boot2docker virtual machine to setup your shared disk. By default, boot2docker does not share a filesystem with your native OS X, but it may in the future which will make this step unnecessary. Now, back on your Mac, you setup a local mountpoint for your SSHFS filesystem mount. Now that you can access the shared filesystem, you need to link your new container to Dropbox. Copy the link into your browser and authenticate your new Docker container to use Dropbox. Once you are authenticated, wait 2 minutes while the Dropbox start syncing. Then commit your local changes so that the container remembers that you logged in. NOTE: Execute the following commands in a NEW terminal window, do not exit or close the old window until after you run these commands. Finally, you can run this command and keep it running to have Dropbox in Docker. Docker is not limited to running web apps: you can do so much more with it. This post was inspired by a BitTorrent Sync in Docker post. If you have any fun ideas of things to put in Docker containers, let us know in the comments.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "5-unexpected-ways-to-use-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/5-unexpected-ways-to-use-docker", "abstract": "What's the next big thing in Docker? It just might be desktop apps. Did you know Docker can be used for everyday apps, not just web apps and database containers anymore? Here are just 5 of the most creative ways we have seen Docker used for things that don't relate to coding or DevOps at all. Skype in Docker (Image: tomparys/skype ) Yes, Skype. Did you know that the Skype Linux binary is disguised against decompiling? Nobody can reproduce what it really does and it produces encrypted traffic even when you are not actively using it. The notification sounds (like call ringing) do not work when you isolate it in a Docker container, but everything else works great. Dropbox in Docker (Image: ctlc/dropbox ) Dropbox has been around for 7 years. Who knows what has gotten into the dropboxd daemon in that time. Also, given the recent Snowden announcements, you can never be too careful with the systems that keep your sensitive information. That's why you should consider running Dropbox from within Docker. No matter what happens, Dropbox will never be able to see beyond a severely isolated filesystem and process list. BitTorrent Sync in Docker (Image: shykes/bittorrentsync ) Want a totally decentralized, totally free Dropbox? BitTorrent Sync is just the ticket. It uses BitTorrent P2P technology and encryption to sync as much data between your computers as you want. However unlike BitTorrent, BitTorrent Sync is NOT open-source. So if you want to encapsulate and isolate it, try it within Docker. Transmission in Docker (Image: angelrr7702/transmission ) My favorite BitTorrent app for the Mac by far is Transmission. Even though Transmission is open source, why not run it in Docker if you can? uTorrent in Docker (Image: linux/utorrent-server ) Not only can you run BitTorrent clients within Docker, but you can also run BitTorrent servers/seedbox using uTorrent. uTorrent is one of the most popular and well known BitTorrent servers out there, but again (common theme) it is closed source. Will Docker become a new distribution model for mainstream Desktop application developers? It is certainly possible. We can even see the seeds of such a model already emerging. However the tools for setting up and using Docker will need to become easier to use for everyday users if this future will ever arrive. In the mean time, the more applications you can run in isolated environments like Docker Containers, the better. Apple has started App Sandboxes in OS X in Mavericks, but it depends on the app authors to use this feature of OS X, and even though Apple is enforcing Sandboxing for the Mac App Store , many apps simply don't use the Mac App store and have a lot of privileges. Companies like Bromium are fixing this problem for Windows, but Docker is a working solution for Mac and Linux today.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-is-docker-and-when-to-use-it", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/what-is-docker-and-when-to-use-it", "abstract": "Heard of Docker, but still having trouble understanding it? Don't worry, you are not alone. Today, I want to try to explain Docker in a few different ways. Before we talk about what Docker is, I will talk about what it isn't. What is the negation of Docker? What are its limits? What can't Docker do? So then what is Docker good at? Docker is a basic tool, like git or java, that you should start incorporating into your daily development and ops practices. Java's promise: Write Once. Run Anywhere. Docker has the same promise. Except instead of code, you can configure your servers exactly the way you want them (pick the OS, tune the config files, install binaries, etc.) and you can be certain that your server template will run exactly the same on any host that runs a Docker server. For example, in Java, you write some code: Then run javac HelloWorld.java . The resulting HelloWorld.class can be run on any machine with a JVM. In Docker, you write a Dockerfile : Then run docker build -t my/ruby . and the resulting container, my/ruby can be run on any machine with a Docker server. The Docker server is like a JVM for systems. It lets you get around the leaky abstraction of Virtual Machines by giving you an abstraction that runs just above virtualization (or even bare metal). Git's promise: Tiny footprint with lightning fast performance. Docker has the same promise. Except instead of for tracking changes in code, you can track changes in systems. Git outclasses SCM tools like Subversion, CVS, Perforce, and ClearCase with features like cheap local branching, convenient staging areas, and multiple workflows. Docker outclasses other tools with features like ultra-fast container startup times (microseconds, not minutes), convenient image building tools , and collaboration workflows. For example, in Git you make some change and can see changes with git status : Changes to be committed:\n(use \"git rm –cached …\" to unstage) Add new file README.md to git... Push change to git repo... Branch master set up to track remote branch master from origin... Use git whatchanged commmand to see what changed... In Docker, you can track changes throughout your entire system: Github commit... Docker push to update repo... Docker pull to get image... Docker history to see recent changes... These collaboration features ( docker push and docker pull ) are one of the most disruptive parts of Docker. The fact that any Docker image can run on any machine running Docker is amazing. But The Docker pull/push are the first time developers and ops guys have ever been able to easily collaborate quickly on building infrastructure together. The developers can focus on building great applications and the ops guys can focus on building perfect service containers. The app guys can share app containers with ops guys and the ops guys can share MySQL and PosgreSQL and Redis servers with app guys. This is the game changer with Docker. That is why Docker is changing the face of development for our generation. The Docker community is already curating and cultivating generic service containers that anyone can use as starting points. The fact that you can use these Docker containers on any system that runs the Docker server is an incredible feat of engineering.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "the-easy-way-to-deploy-java-tomcat-with-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/the-easy-way-to-deploy-java-tomcat-with-docker", "abstract": "If you are trying to deploy a Java Tomcat app on a Docker system, you might find other tutorials which will take you hours of heartache to get just right. How about deploying a full custom Tomcat app in just 30 seconds? Don't believe me? Just try these 10 simple steps: The basic Heroku Java buildpack doesn't work with every Java app. If you run it from the jesperfj/webapp-with-jndi base directory it will fail. But if you use the jesperfj/buildpack-tomcat custom buildpack within the app's directory, everything works smoothly. This is just a simple example of how much better the developer tooling around Docker has become over the last year. With Docker 1.0 just around the corner, there has never been a better time to start incorporating it into your daily workflow. If you are ready to get your feet wet with CoreOS, try our Building Your First App on CoreOS tutorial.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "long-running-processes-in-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/long-running-processes-in-docker", "abstract": "Today, I am taking a request from one of our fans on Twitter. Apparently, I have talked too much about building WordPress in Docker , so today, I am going to do something totally different. Today we are building a simple Node app with a web and worker. First, let's start by setting up our Docker environment if we don't have one already. Now let's create a simple Node web app called web.js . So far so good. Let's create a simple package.json file. And let's install the dependencies. Our web app is ready to be Dockerized. We are just one command away: The Docker container is now built. We can test this by running: So now how do you create a background process? Easy. First let's create the background process file. Let's call it background.js . Since we are adding the sleep library, let's install the dependency. Now let's create a new file called Procfile . There is a lot you can learn about Procfiles , but for now we will keep it simple. One line for the frontend web app and one for the backend worker app. Since the building tool already created a Dockerfile for us, let's just use docker build to update our container. And now we can run our background worker by changing the word \"web\" to worker\" in the docker run command. If you want to run the container in the background, use docker run -d instead of docker run -i -t Background tasks are easy to build in Docker. In fact, Docker is perfectly suited for them. Whether you are building a Java Tomcat app that needs a background task or any other kind of app, you can combine the genius of Docker with the simplicity of Heroku Procfiles to accomplish wonderful results. The original request was to show how background tasks could talk to a database backend, but since a worker container and a web container are the same in the eyes of Docker, you can just use any tutorial about connecting containers to databases to accomplish the task. Do you have a request for a Docker tutorial? Tweet us at @CenturyLinkLabs and let us know what you would like to see.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-and-shipping-containers-a-useful-but-imperfect-analogy", "author": ["\n              Luc Perkins\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-and-shipping-containers-a-useful-but-imperfect-analogy", "abstract": "This last week I read a book that I would recommend to just about anyone: The Box by economic historian Marc Levinson . I picked up the book because I wanted to explore the analogy between the containerization movement in Cloud Computing led by Docker and shipping containers —- you know, those enormous, colorful boxes you see piled up at every shipyard on the planet nowadays. The analogy is not a new one —- after all, you even see it in Docker’s logo —- but I wanted to explore two things: (1) just how much the analogy can tell us about the nature and promise of LXC-type containers; and (2) where does the analogy break down and why we should have a look into the world of gardening, of all places, to complete our understanding\nof this nascent software packaging and deployment technology. The Box is a wonderful book because it vividly explains how something that seems so simple and, in retrospect, just plain obvious , came to act as a pillar of international trade and the global economy as we know it. To me, the most compelling part of Levinson's history comes in the second chapter, \"Gridlock on the Docks,\" where he describes just how frighteningly inefficient and haphazard the shipping industry was before the advent of shipping containers. He describes docks on which \"swarms of workers clambered up gangplanks with loads on their backs\" (p. 16), ship holds full of items of all kinds sorted and stacked based on the whims of the people sorting and stacking, ships spending days if not weeks in port while longshoremen—often disgruntled and prone to labor unrest—moved items by hand . Perishable goods often perished; fragile goods often broke. The end result: shipping goods was a haphazard, wasteful process, with costs high enough to deter many industries from even attempting to engage in global trade. While Levinson spends chapters 3 through 12 telling the story of Sea-Land , the company most responsible for making containers the norm in the industry, it's in chapter 13, \"The Shippers' Revenge,\" that he paints the picture of the new, containerized world most clearly: shipping costs dramatically reduced; ships spending hours or less at dock instead of days or weeks; massive cranes loading and unloading ships with dexterity and grace; and fragile goods safe inside sturdy walls. The end result: shipping costs are transformed into a rounding error on the total costs of goods, decaying industries are revitalized, and new industries are born that were previously impossible. As Levinson notes: \"Low transport costs helped make it economically sensible for a factory in China to produce Barbie dolls with Japanese hair, Taiwanese plastics, and American colorants, and ship them off to eager girls all over the world” (p. 265). To put it a different way, try to imagine the tech industry as we know it today without cheap shipping. Spoiler alert: say goodbye to your iPhones and your commodity servers. It should be pretty clear already why I like the shipping container analogy for Docker so much. Docker containers are like shipping containers because they are so self-sufficient. All that they ask is that they are run by a sufficiently powerful operating system. That's the extent of their \"contract\" with their environment. Similarly, shipping containers don't care which ship they're on, as long as the ship is sturdy enough to carry them. No need to stack goods by hand below deck and worry about temperature, moisture, etc. The container takes care of that, not the ship. I also like the analogy because of the depth of the changes wrought by shipping containers. As Levinson says, \"the cost of transporting goods was decisive in determining what products they would make, where they would manufacture and sell them, and whether importing or exporting was worthwhile\" (p. 246). Shipping containers didn't just cut costs and change decision making. They changed the whole economic landscape, altering global consumption patterns, revitalizing industries in decay, and even allowing new industries to take shape. In short, shipping containers remade global trade in their own image. I truly think that the same can be said for Docker containers in IT. Why? Because they don't just trim the fat in operating costs or streamline this or that process. They are a potential game-changer just like shipping containers were for global trade. The shipping container analogy is apt, but for me it completely breaks down on one crucial point: when shipping containers go down, their cargo is lost . If a ship sinks or if the basic integrity of a shipping container is compromised, everything in it is (or can be) lost forever. Docker containers don't work this way. When they go down, you simply rebuild the container. In this sense, Docker containers are more like plants than shipping containers. If a plant dies, you simply plant a seed and replace it. If the environment surrounding the plant is the same, you'll end up with an essentially identical plant. If a Docker container goes down—the machine it's running on goes haywire, the OS runs out of memory, etc.—the consequences are far from dire. You can simply use the old container's Dockerfile (think of it as a container seed) to sprout a new container identical to the old one. Even better, Dockerfiles, like seeds, tend to be quite small (usually less than a kilobyte), but they can grow into mighty oaks. The beauty of Linux containers is that they are ephemeral in a way that shipping containers could never even hope to be. Let's say that you \"lose\" a container because someone pours coffee on the server it's running on. The consequences? Minimal. In the right orchestration environment, a few keystrokes will tell another, functioning server what to do: look at the Dockerfile , see what needs to be built and what commands need to be run, and go. In an even better scenario, your orchestration environment would know that the container had gone down and would automatically create a new one. Or perhaps a fallback container exists that can be switched to immediately. The possibilities are vast. Docker containers are a beautiful thing because they combine the modularity of shipping containers with the ephemerality of plants. The promise of Docker for transforming IT from the ground up are clear—and if they're not clear yet, then stay tuned here, because there's lots more on the way. But the problem is that the promise of Docker remains just that: a promise. To see why, we need to look to shipping containers and gardening once again. Shipping containers are immensely useful, but they don't ship, stack, or unload themselves. They require the infrastructure surrounding them to realize their potential. Similarly, just because seeds can turn into plants doesn't mean that cultivation is always easy. A skilled gardener has to have a strong organizational acumen to orchestrate a proper garden. As it stands, the infrastructure necessary to realize the promise of Docker containers is on the way, but it hasn't arrived yet. Container orchestration at scale remains a challenge, but tens thousands of really smart developers are working to resolve turn this promise into a reality, and we hope that this analogy helps illustrate how just how important this work is.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "persistent-distributed-filesystems-in-docker-without-nfs-or-gluster", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/persistent-distributed-filesystems-in-docker-without-nfs-or-gluster", "abstract": "Persistent filesystem is one of the hardest parts of running a CMS or blog in the cloud. If you have ever tried running WordPress or Drupal on Docker or a PaaS environment before, you know what I am talking about. Each container or dyno is ephemeral (goes away when the container goes away) and when the dyno goes away, all your uploaded content goes away too. You can't easily Docker-ize NFS or Gluster. There are tutorials out there, and even a few containers in the index that say they work. But I could not get any of them working in the latest version of Docker. Even if you could in theory run NFS or Gluster in Docker, would you really want to? Who hasn't had trouble with those technologies here? Whether it is kernel patches or TCP ports, it is one thing or another. It is never easy. Wouldn't it be great if you could use a syncing technology that didn't require you to open ports or patch your kernel? Like Dropbox, but without the central server so you don't have to worry about your code being compromised? Although BitTorrent Sync isn't open source, it does work really well to sync your servers for no cost and without a centralized server. A new open-source project called Syncthing is a promising alternative to BitTorrent Sync, but since it is not stable yet, we will show you how to use it after it becomes more stable. Here is how you can add syncing to your Docker app: And on any other computer, you can run the ctlc/btsync container with the secret from your first instance: You only need to run one ctlc/btsync container per virtual server. You can run as many containers that bind to that container's volumes through --volume-from as you want. That way all your containers can share their ephemeral data locally AND across systems. If you combine BitTorrent Sync with Docker, you can create a potent and powerful solution to the persistent filesystem problem without debugging NFS or kludging together an rsync solution. Data-only volumes are a great solution for shared filesystem within a single host, but when you want to build multi-host apps, using a solution like this can be an elegant way to scale.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "interview-with-deis-cto-gabriel-monroy", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/interview-with-deis-cto-gabriel-monroy", "abstract": "This week, we are starting a new interview series as part of the CenturyLink Labs. We are going to try to keep these interviews short (30-40 minutes) and packed with great content every week. Last week, we talked about Deis and Flynn. This week, we are talking with the CTO of Deis, Gabriel Monroy. Gabriel Monroy , CTO at Deis Gabriel Monroy is a systems architect with 15 years experience designing, developing and operating large-scale IT infrastructure. Gabriel is the creator of Deis, the leading Docker PaaS. As an early contributor to Docker and CoreOS, Gabriel has deep experience pushing the boundaries of early-stage technology and speaks frequently at industry events on PaaS and distributed systems. Gabriel co-founded OpDemand which was acquired by Engine Yard in 2015. He lives in Boulder, CO, but spends a lot of time in airplanes traveling to tech conferences around the country. You can subscribe to this podcast on: Over 100 known installations Single-tenant Deis is built to be multi-server, but without any database or backend services managed by Deis Deis 0.9 just released with Dockerfiles, Domain names and HA Routing Different from Dokku because it is built for multi-server Deis does not do any database service management, only managed application code Deis uses ambassadors for auto-registering and load balancing application instances transparently By adding more Deis images, registering them into CoreOS via etcd OpDemand is the parent company that develops the Deis open-source project If you have any questions for Gabriel, feel free to leave comments in this post. Lucas Carlson: I am the Chief Innovation Officer for CenturyLink and I'm interviewing Gabriel Monroy from Deis. He's the CTO and I'm very excited to have him on, hear about him and the Deis project. Could you tell us more about yourself, Gabriel? Gabriel Monroy: Sure, yeah. First of all, thanks for having me. Appreciate the opportunity. I've been working in IT operations for a long time, about 15 years now. I started off my career working for Intuit. I left there as a senior systems architect. From there, I went to work on a number of start-ups in the New York area and later as a Freidman's consultant. Most of the work I did was focused on deployment automation and developer tooling. It was a natural flow into the land of Docker. I've been an early and frequent contributor to the Docker project, and created Deis, and open sourced it by August of last year. Lucas: It's a cool project and it's fun to watch. I myself built app [inaudible 01:20] the platform as a service built on Cloud Foundry. Platform as a service is very close and near and dear to my heart. It's really cool to see this new next generation Micro-PaaS stuff coming out. Could you tell us a little bit about Deis, what it is, why people want to use it? Just give us the pitch. Gabriel: It's interesting that you bring up the Cloud Foundry perspective here. After Docker had come out, it was clear to me that the way that you would build application platforms for developers, certain changes after the introduction of a [inaudible 02:04] Docker, you're going to do things differently than you might have before. We were interested in tackling that problem head on and figuring out, \"What does a Docker PaaS look like?\" For us, from the get-go, we were focused on the developer workflow portion of things and probably less so on the underlying implementation. We thought it was more interesting to figure out, \"Now that Docker exists, how can developers operationalize this and use it?\" We took a lot of inspiration, especially in the beginning, from Heroku. Heroku provided a fantastic experience for developers and what we wanted to model. Over time, we see ourselves as more of a Docker PaaS than as a private Heroku, although we are that as well. [laughs] Lucas: Which is interesting, because the whole Docker project came out of the dotCloud platform as a service. It's interesting how Docker came out, and yet, it's birthing these new PaaSes. How do you see it different than Heroku, or Docker, or dotCloud? Gabriel: We buy into the whole 12-factor methodology. That's the best way to build applications to scale horizontally, to make them stateless and follow a number of rules for best practices for distributed systems and service-oriented architectures. That was a natural shed for us. In terms of different than dotCloud, I truthfully wasn't that familiar with dotCloud's platform. I know they were doing a ton of interesting things with services and databases. From our standpoint, we like the idea that you can have these horizontally scalable, stateless applications connected to a load balancer. Then attach those to backing services that are managed separately from the platform. In lot of cases, loosely [inaudible 04:15] and often managed using traditional means. That's one of the things that we purposely don't do in Deis today, because we're really so focused on trying to get the stateless application part of it right. Lucas: Deis is really focused on the application code, and running the application code. It doesn't handle services. Can you or can you not spin up MySQL in Postgres within Deis? Gabriel: No, you cannot. As someone with many years of Ops experience, running databases inside containers makes me a little nervous, at least today. That's going to change over time, but if I'm a DBA and I'm telling someone that your data is under var/lib/docker/vfs/GUID, whatever, and you don't have a choice of underlying lock device that you can put it on that's distinct from the rest of your containers, there are a lot of problems there. Not to mention, how do you properly do backup restore, disaster recovery policy? All those things need to be thought out properly. One of the things that we implemented early on, in the early days of Docker was the dash b file, for by mounting files from the host of the containers. It's something we needed. We know firsthand that there is a ton of problems with things like UIDGI, e-mappings and stuff like that if you're actually running production databases. We want that stuff to sort out a little bit over time. Even after it becomes possible or recommended to run databases, I still think that Deis will have the idea of a service gateway that is used to do service attachments in a Heroku style, loosely type of fashion. Lucas: For me, I think that helps me understand more what you guys are doing. Helps the audience understand better. The two big things that I would take away from your description is that the big part of the difference between Deis and Heroku, or Deis and dotCloud is one, that you can run it on your stuff, that's a huge differentiator. Two, is that it doesn't deal with services. So, you have to do your services on your own. That makes sense for me. Some of our audience will know things like Flynn and Dokku as other PaaS alternatives for Docker. How do you see it different? Where are its strong points? How do you see it differentiated from some of the other Docker PaaS there? Gabriel: Yes. First of all, Dokku has been around forever. Justin Deis. Talented developer and I considered a friend of mine. We took a lot of inspiration from the early days of Dokku. But as people may or may not know, Dokku is a single host, and only designed to be a single host PaaS. There is obviously a key difference right there. But a lot of the concepts are that the way that Bill packs are integrated with Docker. We took a lot of inspiration there. We contributed to the Bill Shep project and the [inaudible 07:38] for while. With regard to Flynn, that's pretty interesting. There are a number of differences between the two projects. There are philosophical differences, technical differences, expert differences, and general approach to the [inaudible 07:56] system. Maybe we could spend some time talk into those. On the philosophical side. Before you guys did a great job coming up with a specification. Like the expect doc that describe how everything would work. That was smart guys, and respects, well done. We had a different approach. Our approach was premised on the idea that, we don't really know how things are going to play out, with technologies as newest docker. Who could have predicted the stuff that the Hashi Corp guys are doing. Back when we were doing the stuff. Or that CoreOS and Flynn, was going to advance in the way that it did. Rather than saying, \"This is like the Bible and we are going to go after that.\" We started first with, what incentive of workflow that can get developer moving right away today? Even if it means Steel and coal may not be pretty, or some immigration that may or may not be ideal. How do we get people testing our workflow so that we can learn from that workflow, and improve the underlying implementation over time? Philosophically, that's a big difference. Technically, the Flynn PaaS is incredibly ambitious especially given how few developers are. They're writing everything from scratch and go. There's no real external dependency. We aren't doing that. We're more focused on working with other players in the community who are focused full time on things like CoreOS leader. There are some other companies that we're talking with right now that have different scheduling implementations, that we hope to integrate with Deis as well. We're happy to use third-party off the shelf components, where they make sense. Always with an eye towards, what is the best and workflow for the developer. Lucas: Deis is built to be single-tenant or multi-tenant? Gabriel: We haven't done a terminal work right now, to make it multi-tenant, and do proper isolation, and c groups. If you're really doing multi-tenant for some stuff, you should be doing for kernel-hardening and lot deeper resource constraints. Right now, most of the deployments and most of the customers that we work with are single-tenant multiple applications. Lucas: For multi-server Deis, when you have to grow beyond a single server like the Dokku is stuck on a single server, how does that work? Gabriel: It's based on the way that you scale SED clusters. As you add nodes, typically the way this works is you plug in and you're discovering your LN points so the SED nodes can find each other. Really you just add nodes to the cluster, they discover each other and they automatically get included in the scheduling logic. Lucas: It seems like there's a lot of CoreOS integration to Deis, and the 0.8 release really made that solid. Can you tell us how does Deis and CoreOS interact? How much is CoreOS, how much is Deis and in what way do they leverage each other? Gabriel: CoreOS, much like Docker, provides a great set of parameters for building something like an application platform. Docker provides core containerization technology. CoreOS provides SED which is a terrific way to do services discovery as well as Flynn, which is a terrific way to do job scheduling. I have a lot of faith in those guys and they're doing a great work and the Flynn project is increasing rapidly. We're really interested in evolving as they evolve. Where did Deis begin? That's an interesting question. For CoreOS, the entry point to CoreOS is system [inaudible 12:20] files. Writing system [inaudible 12:22] files that may or may not have some special Flynn meta data inside of them. That's great if you're into that level of system and administration, but if you're looking for just an office shelf platform that works, you're going to need to do things like routing containers, ambassador, [inaudible 12:41] containers for thing like logging, announcing. You're also going to need authentication, authorization. You're going to need databases, going to track builds and config changes and releases. There's a whole set of platforming stuff that CoreOS is not designed to solve. But if you want a PaaS, you need that stuff. That's really what we are laser-focused on. We are happy to work with CoreOS, Flynn, and also others. We can make a great way to schedule Docker containers across a distributed system. Frankly, we just submit the jobs and make sure we still have an exit to the router and again, try and stay focused on that developer work form. We have some interesting things planned in that space, too. Lucas: You bring up two interesting things. One is I want to hear what's next for Deis, but the other thing you bring up is Ambassadors which I've written about a little bit. How does Deis and Ambassadors...is it something that you're building into Deis? Or is it something that you're just supporting people, introducing them to their systems on application level? How do you thing about Ambassadors? Gabriel: Part of the idea with Deis is that we do all that stuff forward. Best practices for those type of things evolve. They evolve over time, they evolve as Docker gets better, they evolve as best practices evolve. Right now, when you scale a container inside of Deis, we actually spin up three containers. We spin up the container you ask for, we spin up a log container which does upper-container log application through our separate log channel, then an announce container which is also known as the presence container, which does a health-check on the container to make sure it's listening or healthy and publishes that to the SED so that we can publish it on the router. That's how get things like zero-downtime to [inaudible 14:44] and all that. Could you write all that stuff yourself? Yeah, sure. It's a pain not only to get working the first time, but also to make sure that it stays up-to-date as best practices evolve. Lucas: Does Deis have any integration to the Docker index or do you build containers for your application using the build-step stuff? Gabriel: This is where I wanted to share a little bit of what we have planned. We start off with git push Heroku-style workflow. That works great for a type of people and we're happy with it and it's done its job. Going forward though, we really see the idea of Docker PaaSes as moving beyond the git push process. One of the big advantages of Docker is that you can actually take a built image and promote it step-by-step through a good CICD pipeline and not have it change. Every time you get push into Deis or Heroku or whatever, you're actually going and fetching a runtime from a third-party. Although your source-code didn't technically change, there are lots of other things that might have changed that means that your image is bit-for-bit compatible. Something that we hope to show off at DockerCon is a new workflow that bypasses the Get-Push process. It's still deeply integrated with the 12-factor model for promoting existing Docker images whether they are the public index or on the private registry, and promoting those through Deis sustain where you would get push build. Lucas: Would that enable more services like MySQL and Postgres, or are you still staying away from that? Gabriel: We're still staying away from that. Could you run them? Sure, but it's not something that we recommend. At the end of the day, unless you've made the containers replicate amongst each other and ephemeral and design that in to your database, if the host goes down you're losing your data and we don't want people to be doing that on the platform. Lucas: What do you use for your routing layer? Gabriel: Right now we use EngineX. We've had some discussions with some of the folks that are working on VulcanDeek. We think that that's a really promising project that's going forward. There's some other projects that could potentially allow Deis to be a little more dynamic. Between using ComfD to template out EngineX and do soft reloads on it has proven to be surprisingly scaled, so we've been very happy with that to-date. Lucas: Is there a head-node that you need to have a public address for the router mesh or is the router mesh something distributed as well? Gabriel: Yeah. We actually have, and something that we're going to be announcing in our 0.9 release is HA routers. You can run as many routers as you want. We recommend three for most deployments using Flynn. Those are going to conflict with each other, so they'll be spread out across our minimum three-node cluster. We typically recommend you use something like Wildcard DNS to balance traffic across. Honestly most of the real-world deployments at Deis have some front-end load balancer in front of our router mesh that's doing a cell termination caching and is actually exposed. Another way to think about the router is one of the things that Deis is doing is making it possible to horizontally scale different sets of allocations represented by containers. In order to access those containers you need a router that is aware of which ones are healthy. We don't actually see a lot of people connecting our router mesh directly to the public Internet. We recommend some sort of front end load bouncer [inaudible 18:59] is a great solution. Lucas: Is anybody hosting Deis or managed-hosted Deis? Is OpDemand? I know Deis is the open-source project, and OpDemand is the parent company. That's the company you work for. You guys work on premise Deis, but are you thinking or are you working on anything that's hosted-managed so that people that want Deis but don't necessarily want to install it themselves might be able to come to you or somebody else? Gabriel: It's a good question. First of all, OpDemand as you mentioned is the commercial entity behind Deis, the open source project. We sell professional services, support contracts, subscriptions, for Deis deployments. That's how we fund the open source effort. We've got six guys now, and we're growing. We're hiring. If you're interested in writing go and distribute systems, come [laughs] find me. Was the question about a hosted version of this? Lucas: Hosted, so that somebody who wants to get started with Deis but doesn't want to install it themselves can come to some service? Or is there a provider or is OpDemand thinking about something like that? Gabriel: Ultimately, we probably will get there. For now we're really just focused on getting the platform part of Deis right, and we've got a lot of work to do there. We are working with a ton of customers right now. We do go out and do installations of this on premise typically bare-metal-type environments. We're focused on letting people install this in their own environments, helping them with that to the extent they need help. Maybe down the road we'll look at some hosted solutions. Lucas: What do you think is the biggest problem in real-life Docker adoption today for organizations, for people just getting started with Docker trying to figure out how to implement it into their workflows? What do you think is the biggest barrier today for Docker? Gabriel: That's a fantastic question. We work with a ton of customers who face these same challenges, and it really varies. Some companies have a really high tolerance for experimentation and that sort of thing, but I found probably the biggest barrier is that some applications as written currently just don't fit inside of a Docker container. Part of the idea behind Docker is that you should really be splitting up monolithic services inside their natural components and moving towards a SOA architecture. The reality is we go and talk to customers about using Deis, and sometimes we have to spend most of the time getting their apps into a place where they can leverage Docker. It's different than virtualization. A lot of people compare the advent of Docker to the advent of virtualization, but one of the big differences there is that virtualization allows you to take existing machine images and run them unmodified inside of a VM. That's not really true at Docker. There is some work that you have to do in most cases to get your application code to run inside a Docker container. I see that as probably the biggest hurdle to adoption. Lucas: Not to go into too much detail, but what do you think are the most common things that you have to do? Gabriel: There's a lot of moving from templated configuration files to environment variables. If you don't want to do that, which a lot of people don't, implementing a shared configuration through service discovery mechanisms like SED instead because that's typically a better option than statically injected environment variables. The [inaudible 23:12] project, which we contribute to, is a great way to help bridge that gap for non-service discovery-aware applications, so you can write out template files and help processes inside a container if that doesn't do that in memory. Lucas: What about persistent file systems? I know this is something that matters more for traditional legacy applications, CRM applications. I'm so glad that you added the Dash B to the command. When you go into something like CoreOS and you have multiple systems, Dash B doesn't do much help for that persistent file system. Gabriel: Actually, you guys wrote a great post on this about doing true replicated file systems. This just gets back to part of the reason why we're not tacking state inside of containers yet because it's rapidly evolving, and we're not clear where the chips are going to fall. In my mind, if there was some glusterFS/share file system-style service underpinning a series of Docker hosts, that can be an incredibly powerful way to get state working. I have some experience with GFS or one of those, and running those in production is very tricky. Not only that, the IO performance for those type of files systems is terrible, and that's not exactly what you want for your database. You don't want crappy random IO because you have a share file system underpinning everything. Again, it just goes back to, if you've got a production database especially one that's going to be multi-tenant, we recommend you use something like Chef to just manage it the old way and connect it up to a high SCSI ray, how you would traditionally do it, and not take risks there. [laughs] Lucas: For the post that you mentioned, I actually brought up the idea of containerizing BitTorrent Sync and using volumes to mount volumes through BitTorrent Sync. It's not meant for databases either. That's really meant for things like uploaded content when you upload an image. If you have a CRM and you have to upload a PDF or a picture or something, and if you have a distributed system, you've got 100 nodes that you don't want a 1-in-100 chance that you're actually going to see the content you've uploaded. I hate the idea of having uploaded content and going into a database. Obviously going into something like an object storage is ideal, but that can sometimes get tricky especially if you're dealing with something like WordPress. There's this middle ground of applications that do want some persistence but don't require transactional IO at the level of the database. Gabriel: That's an interesting point. To me there's a difference between stateful and ephemeral, and it's an important distinction to point out. The containers that are deployed on Deis, for example, it's not so much that they can't hold state. It's that they have to be able to be ephemeral and disappear at any time. If you could build around that, it's possible to use containers to store state, but that's often not easy to do with things like file systems. The approach that you had mentioned could be a good fit for things like a WordPress upload store. Lucas: I was looking a lot into gluster and NFS for containerized gluster and running the gluster server within a container. You mentioned something that I hadn't thought of or maybe I misheard -- a gluster server to run the underlying Docker file system itself in the var Docker folder. Did I misunderstand that or is that why you...? Gabriel: No, no, no. That was what I was proposing, and I honestly haven't thought enough about it to... Lucas: It's an interesting thought. Gabriel: ...pretend [laughs] I know what I'm talking about. In theory, if you were able to cluster var [inaudible 27:37] Docker, there may be some core issues with the Docker engine sharing some stuff inside that folder that it may not be designed to be clustered. If there was a way to maybe tackle that at the Docker engine side, it's possible to get fast host-level performance for something like var Docker, and that could be an interesting approach. Lucas: What do you think about the future of Docker and CoreOS? Do you think that these technologies are going to become staples and standard technologies that developers and OPs guys adopt as commonly as we now use GIT within the next five years, or do you think that it's going to be something different? Gabriel: That's a great question. The first way I'd answer that is that those of us who work with Docker and these technologies are in our own bubble. We hear everyone using Docker and working on Docker, and \"It's taking over the world\" and so on and so forth. If you try to get out of that bubble and talk to some of the guys in the trenches, they're worried about uptime of services that are miles and miles away from being inside a Docker container. It's very possible and probably likely that Docker is going to become the de facto way of containerizing new green field-type applications. I certainly hope so, and everything seems to be pointing in that direction. There are still so many verticals out there that are just not going to be rewritten anytime soon that aren't for whatever reason a good fit for a Docker container. They're going to have to be managed with traditional tools for a while. I try to temper my enthusiasm about Docker with the realities of some of these enterprises we talked about using. Lucas: Do you have any statistics about Deis adoption? Do you have any idea of how many people are using it? We can obviously just see the stars and the forks on GitHub, but do you have any usage information that you can share? Gabriel: Yeah, there are over 100 Deis clusters out there that we know of. Unfortunately, the way open source codes [laughs] work, there's really no way to know any more than that. We do know that. I'd love to have a better answer for that, and so would our CEO. [laughs] Lucas: My last question is you have a great slogan, \"Your PaaS. Your rules.\" Can you explain what that means? What does that mean to you? What is the meaning behind that slogan? Gabriel: You brought up before that one of the important things about Deis is that it's run on your hardware. You get to choose your network topology and your hosting infrastructure and your hardware providers and all that stuff. More than anything, the slogan is about, that it's about this idea of a private platform that you run and has your architecture behind it. One of the things we don't mean by that is, and one of the things that the Flynn guys do really great is this idea of building modular components that you can sort or swap out. We're more of the mind that we want to package together a solution that you can run wherever you want, but it doesn't require a ton of tinkering. It just works out of the box. That's, \"Your PaaS, your rules.\" Lucas: It's been a lot of fun talking to you. I'm sure the audience is going to be really excited to hear about you and understand this. This is a great introduction. I appreciate this. I'd love to keep in touch. As Deis progresses, we should continue talking about the future of Docker and where things are going. Gabriel: Thanks so much, Lucas.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "flynn-vs-deis-the-tale-of-two-docker-micro-paas-technologies", "author": ["\n              Lucas Perkins\n            "], "link": "https://www.ctl.io/developers/blog/post/flynn-vs-deis-the-tale-of-two-docker-micro-paas-technologies", "abstract": "Flynn and Deis are two Docker-related open source projects that I find incredibly promising, so much so that I included both of them in my list of the Top 10 open-source Docker projects at numbers 1 and 2, respectively, and I stand by that decision. What I love about both Flynn and Deis is that they're highly ambitious. Both openly describe themselves as PaaS platforms, but it's clear that they're not just taking the old PaaS paradigm and throwing Docker containerization into the mix. Instead, they're seeking to redefine the meaning of PaaS. Just as Sinatra redefined the idea of web frameworks by creating a micro-web framework, Flynn and Deis have created a micro-PaaS concept where anyone can run their own PaaS on their own hardware with very little complicated overhead. While this list is likely far from comprehensive, I take the following to be fundamental similarities between Flynn and Deis: There are surely other similarities -- such as the fact that both are written in Go -- but the ones listed above strike me as the most fundamental. What it comes down to is that both Flynn and Deis are attempting to provide a distributed scaffolding that enables IT departments to build potentially massive, containerized, service-oriented architectures. The robust systems layer is meant to make life easier for ops, and the Heroku-inflected deployment tools are intended to make life easier for devs. Today, the two projects have similar ambitions, architectures, and messaging. It would be highly beneficial to the Docker and PaaS communities to hear more from the project leaders themselves about what makes their respective visions fundamentally different. In the next few weeks, we will be interviewing the leaders of the Deis and Flynn projects to get this perspective right from the source. In the mean time, I'd like to invite anyone involved with any PaaS projects---not to mention any of our readers---to join the discussion in the comments or elsewhere. In particular, I'd love to get your thoughts on the following questions:", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-does-docker-1-0-mean-for-the-community", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/what-does-docker-1-0-mean-for-the-community", "abstract": "This morning was an exciting day for Docker aficionados, Docker 1.0 was just released ! At the surface, the reason this is so important is that Docker is officially and for the first time being suggested for production workloads now. Before 1.0, Docker would tell you not to run Docker in production. They said that things might break between upgrades and that APIs would change. But now they are set in stone for backwards compatibility and upgrading (until Docker 2.0 of course). So what does this mean for the community? One of the biggest improvements wasn't on the codebase itself but in the Docker Index (now the Docker Hub). But before I go into those changes, let's look at the numbers... But what has changed under the hood? Let's review. In a not-so-subtle ode to GitHub, today the Docker Hub has been released– hub.docker.com I have to say that I LOVE the new design. It is equal parts minimalistic and informative. And the container search feels like it returns results that are much easier to judge. One of the other neat things is how many new \"official\" Docker containers there are. MySQL, MongoDB, Node, MySQL are just a few. A docker run wordpress gets you the official Docker WordPress container. One of the biggest questions I have is how these officials containers are being built. Unlike Trusted Images, there are no links to the Dockerfiles on the Hub that produced these images. I am sure that will be coming soon as the Hub is brand new. One of the most exciting parts for the Docker ecosystem is the addition of an OAuth API. This will allow third party apps to tie into the Docker Hub to pull public and private container information directly into an app. For PaaSes built on Docker (like Flynn and Deis , this is going to allow greater flexibility. For CI/CD built on Docker (like Shippable and Drone ) this is going to enable a MUCH more powerful workflow. Docker's power has always been on the social/collaborative side of linux containers , and with OAuth support, this is going to supercharge the community and ecosystem growth. For those itching to get started, you might have to wait a little longer. Here is a quote from the OAuth page... \"You will need to register your application with docker.io before users will be able to grant your application access to their account information. We are currently only allowing applications selectively.\" In the code itself, nothing major has changed between Docker 0.11 and 1.0, but there were a few minor changes, like the addition of the COPY instruction. The new COPY instruction copies files and folders as-is from the build context. On the surface, COPY is a lot like ADD but without the ability to grab things from the Internet. Also, ADD will unzip local tar files when moving them and COPY doesn't try to be so smart. The Dockerfile ADD and COPY commands now do a better job with permissions. Instead of stopping containers, you can now pause for better resource scheduling. I will have to do more research on the difference between pause and stop and followup with a new blog post soon. Increasing the number of filesystems supported will help Docker become ubiquitous. XFS is an exciting next step for new storage and filesystem options because of its snapshotting capabilities. Ever wonder who is in charge of the /etc/services file? If you don't know the IANA (Internet Assigned Numbers Authority) is responsible for the global coordination of the DNS Root, IP addressing, and other Internet protocol resources. They are the guys who dole out official port numbers for IP protocols. And now Docker's API ports are official: 2375 for HTTP and 2376 for HTTPS. For those who have struggled with port conflicts between CrashPlan and Docker on their Mac, this is a godsend and the best news of the day. Though boot2docker has gotten around this port conflict in recent releases, it has always been a sore point for those of us using Docker on a Mac. It is incredible to think that the Docker project is just 15 months old. So much progress and so much adoption over this amount of time. It is clearly resonating with the DevOps community and making tidal waves in terms of both innovation and adoption. It leaves you wondering what the landscape will look like in the next 15 months. If I were to make a prediction, I think that the tooling for building apps on top of Docker is the next big area to see explosive growth. It is still not easy, and that leaves an opportunity for people to keep expanding the ecosystem with new ideas. I want to extend a big congratulations to Solomon Hykes and his team for creating a technology that is changing all our lives for the better. Thank you guys!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "analyzing-dockers-new-oss-libchan-and-libswarm", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/analyzing-dockers-new-oss-libchan-and-libswarm", "abstract": "This morning's keynote from Solomon Hykes was inspiring. Just after the keynote, Solomon told me he was really nervous. But if he was, it didn't show. The way they are building out their ecosystem is as equally inspiring as Solomon's speech this morning. Today, there were three new projects announced from Docker: And two \"secret\" Docker projects were hinted at too: This is really exciting news for people building tools around Docker, however it may be less interesting to people who are just using Docker for building apps. Let's review what this means for the Docker ecosystem: In Docker's words: Libswarm is a toolkit for composing network services . In my words: Libswarm is like an ORM or a type system for distributed systems . The biggest problem with Docker is multi-container and multi-server inter-container communication. What does that mean without buzz words? Imagine being able to run docker ps and it shows all your containers running on 5 different infrastructures (some public, some private, some on your laptop). That's what libswarm enables… a swarm of Docker containers available through a unified API. Here is a demo video shown at the DockerCon Keynote made by Ben Firshman of Orchard of what you can do with libswarm: The \"Distributed Docker Containers\" problem has lead to lot of open-source projects on top of Docker: The problem with so many choices is that they all have their own protocols and work in different ways. Once you adopt one of these tools, you effectively are locked into their solution because they don't share an API or an underlying communication method. libswarm exposes a standard interface to systems like Fleet, Mesos, Consul etc Libswarm is built to help provide a shared API between all of these orchestration tools. This is a brilliant move from the Docker guys. It simultaneously creates a common ground that allows people to collaborate on orchestration, but it also encourages healthy competition. When the orchestration tools are interoperable, it breaks vendor lock-in which allows people to move more freely back and forth and lets the best tool win . It levels the playing ground. This is a revolutionary building block for Docker ecosystem development. It keeps incumbents honest and gives up-and-comers a real chance to create something new and better. Solomon notes on Hacker News that this project is very rough and early, and not yet ready for production use yet. Like Go channels over the network In Docker's words: Libchan is an ultra-lightweight networking library which lets network services communicate in the same way that goroutines communicate using channels . It is for: In my words: Libchan is the underlying library that enables libswarm . It is like ZeroMQ or RabbitMQ for Docker. The difference is that the existing messaging tools are heavyweight and general purpose… libchan is built to support Docker and containerized apps in Docker. For network engineers and people who get excited about Unix sockets, this tool is very exciting. For an average developer, you will more likely interact with things built on libswarm than libchan. This was not the biggest news of the day, since libcontainer has been a part of Docker for a while now. It is simply spinning it out into its own repository. However it is a smart move and it will enable faster velocity in expanding support for more types of containers and more abstract support for things like managing VMs. The Docker community continues to evolve quickly and tackle big challenges. The new projects announced this morning will go a long way to addressing some of the biggest problems Docker has to get to mainstream adoption.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "the-future-of-linux-container-hosting", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/the-future-of-linux-container-hosting", "abstract": "This week we discuss the future of Docker hosting with one of the pioneers in the Docker hosting space: Borja Burgos. Borja started Tutum , which is one of the world's first pure Linux Container hosting providers. For as little as $4/month you can get a Linux Container managed by Tutum. Borja Burgos , Tutum Founder and CEO Borja is an Entrepreneur, Engineer, DIYer, Tinkerer and Hacker who enjoys thinking about, designing, and building solutions to new and existing problems. He is currently 100% focused on revolutionizing Public Cloud infrastructure and Platform Services with Tutum. Tutum is a single endpoint to an infinite Docker host. Anybody who is into DevOps should use Tutum. Running Docker at scale with multiple hosts is very hard to do on your own. Tutum creates value added services on top of Docker to make this easier like automatic load balancers. Tutum lives in between the Iaas and PaaS layers. It provides more orchestration than IaaS and gives you more control than PaaS. With PaaS like Heroku, you just git push your code. With Tutum, it is about stitching Linux Containers together. Multi-tenant machines on Amazon. Yes, you can have Shippable or Drone deploy container to Tutum after passing all its tests. The persistent filesystem problem, especially for databases, hasn't been solved yet, but we are working on an open source solution that we will be releasing to the community soon. Linux containers won't take over virtual machines, but there will be a balance based on picking the right tool for the right purpose. For one thing, big cloud hosters will start doing native Linux Container hosting. We will continue to build value added services on top of just Docker hosting including multi-cloud services soon.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-case-study-1-interview-with-matt-butcher-from-revolv", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-case-study-1-interview-with-matt-butcher-from-revolv", "abstract": "The #1 request from CenturyLink Labs readers is to hear about real-life case studies about Docker in production. This week, we are honored to have a fantastic use-case example of Docker in the real world. Matt Butcher , Lead Cloud Engineer at Revolv Matt Butcher is currently head of Cloud Services at Revolv ... a crazy-cool home automation hub (internet of things) startup. Think Nest for everything else in the house. Works with your exiting devices (Belkin, Hue, Honeywell, Sonos, etc). He has authored of seven books and numerous articles on technology, and teaches at Loyola University Chicago. We are still running many core services on Virtual Machines. We have played with a half-dozen Docker technologies and haven't yet committed to any one just yet. But we have replaced our entire CI/CD solution with Drone (a Docker based on-prem open-source CI/CD solution). It took about a week and a half. We had been using Jenkins and it was a nightmare. We are actively looking for more ways to incorporate Docker into production. We are seriously looking into using Amazon's Elastic Beanstalk with Docker, but haven't made commitments on it yet. At the time of writing that article, Docker was just 3 months old and not well understood and Virtual Machines were gang-busters. Containers looked like a faddy kind of toy. But I did not foresee the cool things that came out of the Docker community like CoreOS and Deis and the other micro-PaaSes. Containers are becoming a very elegant and compelling model for building applications. From a DevOps perspective, it is starting to turn out to look like Docker Containers are the right way of doing things. A week ago, it would have been the perpetual putting off of the 1.0. But now that is out. My biggest concern right now is that the tools around Docker are immature, but this problem is being solved by the community right now. The biggest thing is that right now if I want to deploy Docker, I still have to use Virtual Machines and then put Docker on them. It would be great to have pure Docker hosting from one of the larger hosting providers out there. I am most excited about libcontainer . Seeing libchan which gives go channels at the network level is very exciting too. I am still not sure what to make of libswarm . It appears to be something more for the ecosystem than for end-users. First started playing with Dokku PaaS a year ago and I like the idea of minimalist build-your-own PaaS. I think it is very promising, but still takes hour to setup all the dependencies. We check into these projects every 2-3 months to see how it looks. So far it is not robust and mature enough, but we think it will be within 2-3 months from now. However we have backed off from PaaS and are going a little lower on the stack, closer to CoreOS. Drone works by pulling stuff out of your git repository, build a custom Docker image with whatever dependencies you need (binaries and other), and then execute any arbitrary command you want. In Jenkins, even if you could wire up the code just the way you needed it, you were still running on the slave's OS which may or may not match up with production. From the moment the Drone container finishes building, we know that the production environment will match exactly the same state as dev/test. With Drone you can also spin up database containers that match production database containers. This creates a much more robust workflow for testing things than what has been available before. I started out doing Java for 10 years. Then I did PHP/Drupal for a while. When I joined Revolv, I joined as Java. However recently it felt like Java was nesting library upon library. With go I was impressed that I was able to build a remarkably robust application in go with just the core libraries. On the other hand, the fact that go compiles to a small size with low memory meant that I could use dramatically fewer resources. In go, not everything may be easy, but everything in the language should be in the language. That is the suite spot that I wanted in a language. PHP had too much built-in and Java had too little, requiring you to use too many nested libraries. Go was a great middle ground. Lucas Carlson: Hello, and welcome. This podcast is brought to you by CenturyLink and CenturyLink Cloud. Today, I am very excited to be interviewing Matt Butcher about how he uses Docker. This is a real world use case of somebody using Docker to get stuff done. I am very, very excited about it, and Matt is such a great guy. He is a senior software developer and an author. He has written a lot of books. I've written a couple books so I know how hard it is. It's amazing, this guy is a machine. He has written \"Drupal 7 Module Development,\" \"Mastering OpenLDAP,\" \"Learning Drupal 6,\" \"Managing and Customizing OpenCMS,\" \"Building Website with OpenCMS,\" and \"Drupal 6 JavaScript and jQuery.\" I could go on about this guy, but I just want to let him tell you a little bit about himself, and then we'll start talking Docker. Matt Butcher: All right, actually working on another book on \"Go In The Cloud\" right now. Actually, this is the first time I've talked about it in public. I'm really looking forward to that. I'll be writing that with Matt Farina. I'm the Lead Cloud Engineer at Revolv. Revolv is a company doing what I'm most excited right now about which is Internet of Things stuff. Internet of Things. We're building a hub that can connect a whole bunch of devices together. There is an app component. There are lots of connected devices. The part I'm responsible for is the cloud part. I've got this great team here. One of the coolest things about working for a startup like us is that we're kind of inventing stuff as we go. That means that we have a certain amount of flexibility in the technologies that we adopt. We can do so very quickly. I feel like I work at the toy store. We just have new stuff going on all the time. We're excited about the Docker stuff that's been going on. We're kind of getting our feet wet in some parts of it. We really dove in in other parts of it so I'm excited to talk about that. My background, I came to Revolv directly from HP Cloud, where I was steeped pretty deeply in OpenStack. I did a lot of the aPaaS and PaaS work while was there. My first big run-in with containers was as the lead on the aPaaS project that we were building a container-based solution that ran on top of virtual machines. I've got all this cloud background that has given me certain perspectives on some of the stuff that I find changing as I watch this industry moving at an incredibly nimble pace. I'm one happy person. [laughs] I'm in my happy place. [laughs] Lucas: That's great. It's really interesting, because you've got the enterprise-y, HP background, yet you're at a startup, so you have both sides of the vision of how both enterprises and startups could use Docker. Could you tell us a little bit about how you're using Docker today at Revolv? Matt: A lot of our core services were still running on full on virtual machines. What we've been doing is looking at what the emerging trends are cloud-wide. Docker is, of course, at the top of the roadmap right now. Everyone on my team has played around with half a dozen to a dozen different container-based projects, many of them centered on Docker. CoreOS is a big one. Dokku, or Dokku, however you say it, and of course, Deis. The Deis guys are down the street from us here. The thing is, we're really excited about all these things, but we haven't committed to any one yet, because they've all been fending off people, saying, \"We're still unstable, we're still unstable.\" But, we're playing around with all of those actively. Where we've really jumped in, is that we are using Drone now. We've replaced all of the clouds, automated CI and continuous deployment stuff with Drone. That total migration took us probably about a week and a half. We're thrilled with the way Drone is working out for us. To me, it's exactly what we wanted. We were working with Jenkins. Jenkins was a nightmare to get configured, to keep running, and carried an element of danger with it. You corrupt a slave, and you spend the afternoon rebuilding the slave from scratch. Drone has turned out to be the solution to a lot of our problems. That's where we first got a chance at building our own Docker images that are doing exactly what we want. Hosting Docker servers and virtual machines in Amazon. We've had a lot of fun. One of the things we're very seriously looking at right now, we're doing a lot of exploration into the new Elastic Beanstalk services that Amazon is offering. They're now allowing you to deploy Docker containers. That, for us, looks like the right kind of middle move between having Docker and containerized applications on the back end, and pushing those out into production on the front end. We are still exploring, we haven't made commitments on that yet and we're measuring our steps, because we'd hate to have front line services compromised because we chose poorly. Lucas: Makes total sense. One of the things that I want to talk about is about a year ago now, you wrote a very interesting article titled \"Why containers won't beat Virtual Machines.\" I'm very, very curious at can our containers going to beat Virtual Machines now? Are you a leader or you have same view or has it changed at all? Matt: Yes. It was fun to read back over that article because at the time Docker was three months old maybe. Most of the containers solutions were very new. Security people were still running wild, this may not be terribly secure. The container trajectory was still kind of down here system, where the Virtual Machines trajectory was up here, right there. It had gained a lot of momentum. OpenStack was stable. Rackspace and HP and IBM and all these big companies had entered the market for long. Amazon of course was running with everything. Google had just announced their computing services. To me what the Virtual Machines were representing at that time was kind of the clear path to a stable and scalable technology and containers were looking like a faddy kind of toy. I fully expected when I wrote that articles that a year later we would see people's interest in containerization kind of winning. I'm looking at as something to do as maybe an alternative for running a heavier local development environment, but I really did not see it coming down the pipe. What's come out in colorless and with the PaaS layers on top like Deis, Libchanon some of these awesome things that are coming out of this community. Basically, the virtual machine is still going to have its place for sure, but containers are really starting to look like a compelling model for grouping together, clusters of services, application running with each component and a separate container. It's becoming a very elegant and compelling model. A lot of the new configuration stuff that I'm seeing come out CoreOs...I keep saying that CoreOs because that's the one I've been evaluating most often but I'm seeing it with all kinds of projects here. From the Dev Ops perspective, these kinds of containerized models are turning out to look like the right way of going. From the security stand point, it's actually starting to look really compelling that I can control and shape traffic between containers very careful. From the reliability and resilience stand point, the ability to deploy a container and have an auto-configure into its environment in a far more elegant way in what we're seeing in the Virtual Machine level is really looking great. This is Dev Ops Can Asia, Dev Ops kingdom of glory or something like that. We could actually run these things and not have to fret every time a NUC goes down, that something somewhere in there is going to break. A year ago Netflix was talking about Chaos Monkey and kind the trend I was hoping to see really catch on in a Virtual Machine layer was this idea that we would test continually for the resilience of the Virtual Machines. I think to some extent that sort of happened but in the container world, that's when they become sort of a core value. This idea that a clustered container should be able to self-manage, and that's what I see a lot of these technologies driving towards. That I find very, very exciting. My complaints originally with the container model was it, as I saw then people were saying, \"Look, containers are better because we can boot them faster.\" When I wrote that article, I was thinking, \"Yes, sure, you can boot something faster but does booting faster mean better over Dev Ops experienced. Does it mean developers are going to be more productive? Does it mean customers aren't going to experience downtime?\" I did not have the foresight at the time to see what was going to happen in the container war. I'm kind of happy to announce [laughs] that I was wrong... Lucas: [laughs]. Matt: ...that I think containers really are going to be kind of the trend and rightfully so because I feel like the people who are focused on containers and focused on building and Eco-system of containers are focusing on the right things. Lucas: Yes. That makes a ton of sense. I think that a lot of your points that you came up with in the blog that this space in [inaudible 10:34] expensive upgrading our lab security. These points are either being handled already or the other thing that for me has always stood out, I think that a lot of the discussion on container and containerization has focused on those kinds of aspects. The security aspects and I think there is some value there. For me the break out features of containers has always been the social collaborative aspect of containers. The break out feature of Docker is the Docker Hub. It's GitHub for the Dev Ops community, for the Ops community to be able to collaborate together and almost start sharing knowledge. Being able to share the ideas of how to put systems together, distributed large scale systems at a collaborative level. What Git did amazingly well is take the source control stuff that had been done for years with CSV and some version and all that and make it something globally collaborative and distributed and hook into that broader knowledge. The GitHub has really brought out some great thinking and great collaboration. That's what I see in many ways being a corner stone of what makes Docker so amazing, is it creates a source control for Dev Ops. It creates a way for us to collaborate together and share container images collaboratively and aside from the security aspects, aside from the boot up speeds, that's something that I haven't seen anything touch before. That's what excites me about Docker and containers. Matt: Yes, and you got kind of the funny unit stereo type. That on the one hand you the developers who are learning from each other and sharing code and stuff like that and then you've got the operators in the C segment who are hiding in the corner with some kind esoteric knowledge the rest of us don't possess. That's the stereo type I think it's gone back a couple of decades. What's interesting about Docker's whole concept with the Hub style workflow is that it may turn out that the only reason that C segments were ever characterized that is because it wasn't ever really a good venue for sharing knowledge in this kind of world. When Juju started get popular started see that happen there, I think that this is really opening a new world for people to be able to collaborate on, exactly as you put it, collaborate in a space where collaboration just didn't ever seem to work before. Lucas: We've talked a little bit about what you like about Docker. What do you not like about Docker? Matt: What do I not like about Docker? A week ago I would've said the perpetual putting off of a one point, really this is my biggest concern. That got solved, that's great. My biggest concern now really is that a lot of the tools around it are still really missing and immature. That's a problem that would be solved provided people continue to be interested in it, and I think they will. That has been so far high on the radar that I haven't even really looked too much deeper to see what the other big concerns are going to be. The way my team works, our particular workflow has been centered around Docker files. It feels like every time I find something that we can't quite do and another version comes out and we seem to be a little bit further down the road and I feel the kinds of needs people like my team have are the kinds of needs that many teams have. I'm very optimistic that those small kind of things are going to get solved. Lucas: On a big level, the stability and the kind of, we're not going to break things of 1.0 was something you guys needed. But from a technical level what do think was the biggest problem for real life adoption of Docker? For companies like yourself, for bigger enterprises, is there something technical what you think is kind of a sore thumb? Matt: The biggest thing, the one thing that I'm really looking forward to someone solving for us is right now if I want to deploy Docker into a production environment, I am back to Virtual Machines if want one of the bigger clouds. Even with Amazon Elastic Beanstalk, Amazon in my mind is kind of the biggest player that's really...Although Google now too has just announced they are going to support Docker tenures. The way they're supporting leaves me wondering, are we really getting all we can get out of containers? I feel there's big space for someone to really, really solve well the public cloud version of Docker Containers. We've got OpenStack, we've got Amazon, we've got Google, we've got all these solutions for Virtual Machines based funds, but it feels like we're still stuck at the PaaS layer if we want a fairly robust Docker environment. Once we're at the PaaS layer, we're really one layer too high to do a lot of the Dev Ops stuff that a larger [inaudible 16:20] even a medium-sized installation would want to be able to accomplish. We backed off our usage of Paas style frameworks, because we wanted to step down a little bit and manage more effectively what we were doing with our resources, how we were compiling things, what the environment was like. Docker provides us that but we can't deploy it the way we really want to deploy it into a public cloud. One of my biggest reservations, the first time someone on my team said, \"Let's switch to Docker\" was, I don't want to do that because that means that I have to manage Virtual Machines, and then I have to manage Docker Containers on top of Virtual Machines. If there was a way to make it so I don't have to manage the Virtual Machines, ideally they wouldn't even be there or would be somewhere where I don't have to know about them. I think that's a big opportunity. It would be a company like Revolv, we're kind of looking. We are at the high level PaaS, we don't want to manage multiple tiers of infrastructure. We want what feels like the natural bottom layer of infrastructure and we don't really care what's hidden beneath that [laughs]. Lucas: Yes. This is exactly the sort of thing that CenturyLink and CenturyLink Cloud are thinking about, that's why I spend a lot of time in this space. As a Docker user, were you aware of the stuff going on in this week's announcement for libswarm and libchan, and some of the new open source projects. Are you appraised of what's going on? Do you understand what it is? Are you excited about it? Matt: Like many servers from a distance, we've been looking at things to see how they pan out. Revolv has made a pretty big commitment to Go at this point, to the Go programming language. A lot of these new libraries, a lot of these new technologies, have libraries for Go. Libcontainer, there's an example of something that where we might be able to build some really cool stuff, where before, the barrier of entry in building a container wrapper would have prevented someone like me from investing the time, because I have other things to accomplish. I was pretty excited about libcontainer. I could be the only one, but I'm really excited about that one. As far as any kind of real time messaging and message queuing systems, Revolv, in doing any kind of Internet of Things stuff, real time interaction is a very important aspect of this. For example, you've got light switches. You don't want to have outlets and light switches that take three to five seconds to turn on from when you press the button on your mobile app or whatever. Seeing libchan, which I feel like is really going to start propagating the Go notion of channels at a network layer, it's an exciting technology. I'm looking forward to it, and looking forward to seeing how it matures. Those two in particular, have bubbled up to the top of our radar. Libswarm, I'm still not sure what to make of it. I feel like a need to sit down, read about it for a while, see if it's something that's going to be usable for us, or if it's something that we'll let other people make usable for us. Lucas: You've mentioned that you're evaluating right now, things like CoreOS and Deis and Dokku. Can you tell us how you're thinking about these projects? How far along have you evaluated them? I know our audience is thinking about them as well. They probably haven't had as much experience as you have. How do you go about choosing which one's right for you? Can you talk more about that thinking process? Matt: With Dokku, that was the first one I started playing around with. That was almost a year ago now. It was fairly early on in Dokku's...I think whenever Hacker News covered it. That one was fun to play with. At that time, I was coming off the aPaaS project, which is an application platform as a service that HD Cloud offers. We've been working with a fairly robust PaaS solution. To play around with something that was, as the catchphrase goes, \"A hundred lines of bash script,\" it was fun. It was fun to look at. I've played around with it two or three times since. I like the idea of the minimalist build-your-own PaaS. With D-Zone a little while ago I wrote an article for their cloud report about how PaaS are evolving over the last two years. They'd gone from the mindset that the PaaS was going to be the monolith. Developers would set their code gently on the top, and everything in between would be taken care of. That was the old view of it. There are some decent stacks that were built that way. It's turning out that developers don't find that to be terribly satisfying once they make it past the prototype stage. We took a good, hard look at Deis and Dokku, and some of the other mini-PaaS, build-your-own PaaS sorts of solutions. I really think they're promising. I'm excited to watch Deis mature, because it's gone from...The first time I tried to install it, I think I spent hours just trying to get all the Ruby dependencies right. It's coming along really well. I think their decision to leverage a lot of what's going on in CoreOS has been fantastic. Our plan here, is we check on it every two to three months and see how it looks. My guess is that in two to three months from now, when we download it and start looking at it, the question we're going to be asking, is it going to be, \"How does this thing work? What exactly are its features?\" But, is this ready to meet our needs now? That said, since we started evaluating that, we backed off a little from the PaaS layer and started to roll our own in a little bit lower level on the stack. That's where I think CoreOS is looking really good. I feel like if CoreOS matures to the point where we can reliably deliver a cluster of CoreOS servers out into the cloud, and then use Docker and deploy into Docker, we're going to hit the kind of technology that Revolv is shooting for right now, which is the ability to keep services segmented where they ought to be segmented. But, still have a single common infrastructure that all the engineering team and all the DevOps people can feel comfortable using. CoreOS, to me, represents, again, that step down. I'm looking for that layer right there. Somewhere between having to manage a bunch of virtual machines and then another layer on top, but short of running on top of the monolith where all I have to do is set the code on top and hope that everything down the stack auto configures for me. Probably, the next time we go through our evaluation cycle, in two to three months, we will, again, take a look at the PaaS layer, but we will probably focus our attention on whatever is maturing out of that middle tier there, as a runtime for containers, a robust scalable runtime for containers. Lucas: That makes a lot of sense. We're actually working on a project at CenturyLink Labs right around that area that you might find interesting. We're going to be talking more about that soon. You've been blogging about using Drone for CI/CD. Drone is a Docker-based CI/CD. It's written in Go as well, and it's like a Daemon. It's all packaged into one little thing. It's really easy to install. It's really cool, and it lets you do Docker-based CI/CD stuff. I'd love to hear from somebody using it in a real life scenario. Tell us about it. Tell us about how it's different than using something like Travis or Circle, or some other CI/CD solution, or Jenkins. I'd love to hear real life use case. Matt: In a nutshell, Drone works on the principle that when you push something into your Git repository, it will pull out a copy of that, start up a new Docker image, then execute whatever you tell it you want to execute on that codebase in that container. Contrast this with what I view now as the old school, which was the Jenkins school of doing CI, where even if you could wire up the first part, so that all your initial checkouts and putting the code onto the slave board, you are still running on the OS, on the slave's OS. They've worked, of course, to try and integrate some of the Docker stuff. What's nice about this, is from the moment the Drone container spins up, we know what the state of that container is. It was exactly the state that we told it to spin up in when the Docker file finished. We drop our code into this known stable container. We can create and manipulate and compile and do whatever we want. Then have it respond by pushing out our end result. Then it destroys the whole environment. The next time it spins up, we've got a brand new, pristine, fresh one in exactly the same state we expected it to be in. The cycle goes on. Initially, I had looked at this as a way to handle some of our automated testing. We'd commit, it would spin up a copy, it would run the unit tests, it would shut itself down. Piece of cake. Had that working in no time. I kept discovering more and more things that I could do. I can tell it, \"I'm also dependent on having some other containers. I want my database in there.\" Now I can spin up the new container, load up the schema from my database into a separate Postgres container, then run all my integration tests and make sure that all that stuff is working well, too. We kept experimenting and trying new things. In the end, we ended up with a very robust workflow, where on a commit to our master branch, the Docker will spin up. It spins up with our own custom Docker image. It runs a series of pre-flight checks. It checks all the dependencies and makes sure everything's at the right version. It sets up the database. It installs the schema. It starts running through the tests. If it finishes the test, then it starts the process of building a Debian package. It builds our Debian package and pushes it into our APT repo. After that, it uses Packer, which will spin up a new AMI for us, build us a custom AMI based on the output of that. Bundle it all up and push that out to Amazon. Then start up another script that we wrote that will spin up virtual machines somewhere off in Amazon. We can then immediately start doing our development testing. We got an end-to-end CI/CD in no time. This was, like I said, project from beginning to end took us about a week and a half. Granted, that's because my colleagues are brilliant and could jump in here and do a lot of the esoteric stuff, like building Debian packages very quickly. That was fun. It was interesting. It works. We're getting a lot of mileage out of it. That's exactly where I wanted to be. Whereas, how we felt with the Jenkins system was that every time we had to do anything it was, \"All right, I've got to log into Jenkins. I've got to figure out what plugin to use.\" It was a burden. Whereas, with Drone we've been more impressed by how much we could accomplish, and how close we could get to our vision in so quick of a turnaround time. As I've been talking, I've been talking about how what we hoped to do is move off of virtual machines and on to Docker images. Drone, of course, is going to set up us perfectly for that. In our end run, we hope that after we build those Debian packages and push them out to our APT repo, we'll be able to start up some new containers, and have the containers run and build themselves off of a Docker file that says, \"Hey. Grab these APT packages out of this repo and get them going.\" What I think Drone has offered, that a lot of these other past platforms have not, has to do with the fact that you can instantly have a very stable, workable environment. You can tweak it to your desires. We install our own custom packages, so we don't have to keep installing at the beginning of every run. At the same time, when I really screw something up, and destroy the Postgres database, or I really screw something up and wipe out Etsy password or something in a container, which I've never done, but I could. [laughs] I did one time overwrite/bin. When you do stupid things like that, the test fails, the container goes away, and the next time you start back fresh. Nobody has to spend four hours rebuilding anything. That was the freeing thing that helped us dive in and experiment with a lot of this other stuff and get things done so quickly. Lucas: Interesting. That's fascinating. Have you tried Shippable, who is also doing some Docker related, but their more hosted version of Docker related CI/CD? Matt: Yeah. I looked at them quickly. Hosted wasn't something that we could do at the time, and may still be something we can't do at the time. I do think that some of these other Docker-based CI/CD tools that are hosted are going to be very successful, because they can provide this kind of tooling for people without even the hassle it took us to spin up a VM and get this running in there. It was out of scope and out of our ability to do it there right now. Lucas: Go seems to be one of the cool languages to build stuff in lately. You guys are working on stuff. Docker's built in Go. Drone's built in Go. Can you tell us about how you got into Go? You had been doing some triple stuff before, now it's Go. What you like about it, and what you not like about it? Matt: My kind of longer term background is as a Java developer for 10 years, then I switched and did some PhD and some [inaudible 31:03] for a while. Then, when I joined Revolv I joined again as a Java developer, and we had an existing Java API server. I have had good and bad experiences with Java, but more recently I felt like Java developing was piling on library upon library upon library, so that I could do something. Our application really was starting to feel like that. I have been playing around with Go even way back when I was at HP and over in my three time building level tools and things like that. I was impressed with the dual facts that on one hand I can build up remarkable powerful application just using the core libraries. That was because they focus so much effort on getting the networking right and getting the structure of programs right. On the other hand, the fact that it compiles well to fairly generic binaries that are small on size and use a very small amount of memory, meant that I can run them on lower powered machines, lower powered VMs than what was writing in Java, which took many, many times the resources. Those two things were compiling enough, but then once I really got going and realized...Rich Hickey gave a talk a couple of years ago, strangely that was called \"Simple Made Easy.\" He focuses a lot on what consensual simplicity means. While I know he is thinking mainly about functional programing, to me Go really represented the allegiance of simplicity. Not everything may be easy, but the stuff that's in the language feels like the stuffs that are to be in a language and the stuff that's not in the language largely feels like stuff I can comfortably live without. Now this is the sweet spot I really warranted in programing in general. I think that's really a sweet spot all of us, right. We don't want a language where we are continually trying to learn what the core features are nor that we want a language where we have to download 60 or 70 external libraries before we can build something like a simple functional HTTP server. That got us going. The funny thing is originally I was just building a demo that was going to power a little thing in a booth at a trade show. We invented our Cloud service in a matter of weeks, and went, \"Hey, I want to keep doing this.\" It's stuck and our team pivoted very, very well, very gracefully from Java to Go ,and we are all just kind of 100 percent Go as far as what we are planning for the future Cloud projects right now. Shouldn't say that, not a 100 percent. We have a healthy dose of side scripting languages like Python and Ruby and stuff that for our core services we really pretty much committed to Go over this point. Not because we had a do it by fiat, but because the engineering team feels like it is the rare language for building these kinds of tools. Lucas: I really appreciate your time and I am super curious to figure out what you guys land on in terms of that middle layer, what orchestration you pick. I would love to check in with you in two or three months and see where things stand, if that's OK. Matt: Yeah, that would be fantastic, that would be fine. Lucas: Great, thank you so much for you time. Matt: Yeah, thank you. It's been great.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "meet-docker-ceo-ben-golub", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/meet-docker-ceo-ben-golub", "abstract": "On this blog, we talk quite a bit about Docker. We have our opinions about what Docker is and when to use it or what the top 10 Docker tools are. But today, we had the opportunity to learn about Docker from the Chief Executive Officer of Docker, Ben Golub. Since Ben is the CEO, I tried to keep the questions around the business of Docker. I plan to interview Solomon Hykes (the CTO of Docker) soon and we will get into the more technical aspects of Docker. Ben Golub , Docker CEO Ben Golub has been CEO of Docker since April of 2013. Prior to Docker, Golub was CEO of Gluster, the open source, scale-out storage company, which was successfully acquired by Red Hat in 2011. Before joining Gluster, Golub was CEO of social media pioneer Plaxo, which was acquired by Comcast in 2008. Golub also spent eight years at VeriSign, Inc., serving as Chief Marketing Officer, as Senior Vice President of the security and payments business, and as General Manager of VeriSign's managed security services business. This is my 5th startup and 3rd as CEO. I grew up in Cupertino growing apricots near Apple Headquarters. I worked for Sun and Verisign. I was CEO of Plaxo and then CEO of Gluster (open-source storage acquired by RedHat). I started talking to Solomon Hykes about Docker when it was changing from DotCloud and started to see how Docker could break developers free from the pain they had to deal with on a daily basis. Simultaneously, it freed the sysadmins from all the chaos to help them build scalable stable solutions. Developers should be building containers and ops guys should be deploying containers and managing Docker. Actually, Gluster just announced official Gluster containers for Docker! The job of Docker is to create the right hooks for the community to build on. Before Docker, Linux Containers were: Docker adds on top of Linux Container: The Docker project is very grateful to the work of Linux Containers for paving the way for Docker. We worked really hard to make sure Docker runs great on every Linux operating system with a modern kernel. We also have worked hard to make it easy to run a Docker compatible Linux distribution on Mac OS X and Windows. Boot2Docker has a minimal OS and CoreOS also has a minimal OS. The right question is not which host OS works best for Docker, it is a question of which works best in your environment. There is some people who want to use Docker in a really radical way on a minimal OS like CoreOS and Atomic. There are other people who want the full stack so they use something like RHEL or Ubuntu. There are a lot of use-cases that people use for VMs where VMs really aren't well suited. Also a lot of places where VMs make sense and Linux Containers don't make sense. If you are trying to mix different OS families (Linux, Windows, etc), you are going to need a VM. If what you really need to do is build flexible infrastructure, then a VM makes sense. If you are tying to create and burst applications quickly or if you want to build applications from many loosely coupled components, that's where Docker really shines. In the VM model, the application becomes indistinguishable from the server. The application code and the system code become tightly coupled and intertwined. In a Docker world, the application and system code are decoupled. You don't have to worry in advanced what the underlying system is going to be. For example, the underlying host might be RedHat Enterprise Linux, but the developer could use Ubuntu in his containers. You can already put Linux Containers inside of VMs and VMs inside of Linux containers. Docker right now supports a whole bunch of kinds of Linux Containers. We have also built primitives in place to extend it to Solaris Zones or BSD Jails. The natural next step is to use something like Docker to manage things that look like containers inside of a Windows environment, which we are exploring but won't be available this year. Yes, absolutely. Many providers are working to various degrees on this like Rackspace, Amazon, Google, Azure and CenturyLink Cloud . The first thing to do is try the [15 minute tutorial]. If I can do it and my CFO can do it, anyone can do it. Most people don't realize that if you use an Android phone, you are already using Linux Containers. Applications like Angry Birds aren't just a bit of code or a VM, they are actually distributed as a Linux Container and you know that it is isolated so that it doesn't mess up your address book. Use Case #1: Developers Most organizations start by incorporating Docker by encouraging developers to create containers for stateless modern apps (example: API-driven Node apps). Use Case #2: Ops After they containerize their apps, they then try to figure out how to deploy them in a multi-server complex environment. Best Case Scenario Using CI/CD (through Drone , Shippable , or even Jenkins) with Docker to make the transition from use case #1 to use case #2 as smooth and automated as possible. Gilt and Ebay deployments went from weeks to minutes using this method. There are over 50 example uses cases listed on the Docker website. We just released 1.0 and Docker, Inc. is only 42 people and the project is only 15 months old, so it is still very young. Docker is not a VM and won't replace VMs. If you have an app that you really need VM functionality, don't adopt Docker. You need to use Docker to re-think your assumptions when building applications. The old way of thinking is that application = servers and servers = hard to build and spin up . With Docker, you can spin up containers in milliseconds, destroy them in milliseconds. You can migrate them easily, so a lot of the pain and complication to manage apps as if they were servers goes away. We want to encourage people to build above and below Docker. Some of those things will compete with what Docker, Inc. does and some won't. That's ok because we want the best tools to win. A few things we won't do: Here is a few things we have planned to do to make money: We are committed to building the company around open and transparent ideals. Our mission is to build tools of mass innovation. Docker is the gateway technology to making 12-factor application building a reality. We shouldn't have to care what kind of server our apps run on and bring consistency in what you deploy to production. We want to enable new kinds of collaboration. Docker Hub is the GitHub for DevOps people. It builds a bridge between the developer and the ops communities, which have traditionally been often at odds with each other. Docker is the soap that let's oil and water mix. Multiple thousands of new developers and ops people sign up every day. Over 16,000 containers have been published on Docker Hub already. Docker meet-ups in 100 cities across 35 countries on every continent except Antarctica. libcontainer, libchan, and libswarm are really important enhancements to Docker for linking containers together. On Docker Hub we are adding more collaboration tools and making it easier to hook in tools like Jenkins, Bamboo, Chef, Puppet or Salt. You will start seeing more and more use cases from large enterprises and banks using Docker more openly. Lucas Carlson: Hello. This is Lucas Carlson from CenturyLink and CenturyLink Labs. I'm super excited, because today we get to talk with Ben Golub, CEO of Docker, about how he came to Docker, what Docker is doing, how it's changing the world. This is an awesome treat to hear about somebody really leading the way with the technology that, I think, is going to be one of the basic technologies we all use in our command line experience every day within the next few years. Ben, thank you so much for coming. Thank you. Tell us about your background, about yourself, and how did you get to Docker. Ben Golub: First of all, thank you, Lucas, for inviting me. As I always tell anybody who wants to give props for what's happening with Docker, the real props go to the community. My background, this is my fifth start-up, the third as CEO. So, I've been around the block a bit. I grew up in Cupertino. My first Silicon Valley job was cutting apricots. They paved over the orchard. They built Apple headquarters there. In between, worked for Sun, tried to start a business school in Tashkent, my first failed startup. And worked at Verisign for eight years trying to build up the foundations for Internet commerce, was CEO of Plaxo. Immediately prior to this, was CEO of Gluster which was open source storage, eventually, became part of Red Hat. I've seen a lot of the good and I've seen a lot of the bad. As you may know, the company that is now Docker was originally known as dotCloud. It was a platform as a service company. They were started in 2011. I went to speak to Solomon in February of last year, when he had in his mind the notion of taking the fundamental container technology that was used at dotCloud, open sourcing it, and trying to create a company around it. The more I spoke to him, the more I understood his vision. I really saw what this could be in terms of breaking developers free from a lot of the really horrible things that they have to deal with on a daily basis and allowing them to just build great applications, and similarly, freeing the sysadmins from all of the chaos allowing them to build scalable, stable solutions. It just seemed like a natural. As you said, it has the potential to change the world, in development at least. So far, that seems to be happening. So I'm really pleased. Lucas: That's awesome you came from Gluster. Gluster is also a great technology. It's fascinating, because I think one of the concerns that I've heard most from the Docker community is, \"How is the file system stuff going to work with Docker?\" I'm not saying that Gluster is going to be the solutions, but it's great that the CEO of Docker used to be the CEO of a file system company. Is there anything there? Ben: Well, yeah. First, the guys at Gluster just released a really interesting integration with Gluster and Docker. That was Fred Kautz and Harshavardhana who did that. I think, more generally, what we're recognizing is that having built Docker, we now need to put in place the primitives that enable people to do the things they want to do with storage, the things they want to do with networking, the things they want to do with security. Our general philosophy at Docker is, our job is to build the right primitives into Docker, or to use the analogy of a shipping container, put the hooks and holes in the right places, and then let people build what it is that they want to build. We don't want to be in the business of saying, you should run Docker on any particular operating system, or run it on a VM versus not on a VM. We don't want to be in the business of telling you how to orchestrate Docker containers. We want to make sure that we build the fundamental technologies in there. So that people can create an ecosystem and business solutions around Docker. Lucas: That's really cool. Let's back up to people that might not be as familiar with Docker. Who should use Docker? What is it for? Should developers use it, ops people, IT? Who do you think is the right audience for Docker? Ben: Docker is designed to change the way that people build, ship, and run applications. There's a great deal of benefit for developers using Docker and there's a great deal of benefit for ops type in using Docker. A lot of the benefit, actually, comes from the fact that we draw nice, clean separation between dev and ops. Devs, if you will, can be really, flexible and creative inside the container. And yet, the outside of the container always stays the same, which enables ops guys to build stable, secure, automatable stems. A very fundamental level of Docker lets you take any application and its dependencies, package them in a lightweight container that will run virtually anywhere. Today, that means, you can take any Linux application, or components of a Linux application, containerize them, and then run them in that same container on pretty much any Linux server. It doesn't matter whether it's Red Hat or Ubuntu, whether it's physical or virtual, whether it's at Amazon or Rackspace or SoftLayer or Google and, in fact, move back and forth between them. Lucas: That's really cool. It's for the developers to work in the container, and it's for the ops guys to manage the Docker system itself. Is that right? Ben: That's right. Now, of course, some developers also act like ops folks and vice versa. The set of issues that people worry about in terms of application creation in management get really neatly separated on the inside of the container, if you will. The outside of the container hooks into whatever infrastructure management people decide is best for them. What we like to say is that, as opposed to the VM model which says an application is really indistinguishable from the application server. They are tightly bound, in the Docker world, they're separated. Applications can be whatever they want to be and not have to worry, in advance, about what the infrastructure is going to be. People who are building infrastructure, can build the infrastructure that they want. If that's a set of commodity servers that are all running Red Hat, that's great. If it's an open stack, that's great. If it's VMware, VMs, that's great. If it's outsourcing the whole thing to Amazon, that's great too. Lucas: How is Docker different than Linux containers? Linux containers have been around for quite a while. What does Docker bring to the world that Linux containers didn't have? Ben: There is core technology that we built on top of. We have to give a lot of credit to the people, who built the underlying technology. Before Docker, Linux containers were the providence of people like Google, who had specialized tools and training. They weren't portable between different environments. There wasn't an ecosystem around them. For the most part, they were a tool that was used by sysadmins, in very specialized organizations. They weren't useful for developers. We like to draw the analogy between Docker and shipping containers a lot. What I like to say is that, the basic technology was like steel boxes. What we have done is, make them all the same size and put hooks and holes in all the same places. Lucas: Fascinating. I know, you said you wouldn't do this. We got a question from one of the people in Twitter. Matt Apperson asked, \"What Docker based host OS will go the distance.\" He was mentioning, is it going to be CoreOS? Is it going to be Red Hat? What do you guys think is the best way to run Docker? Ben: I think, the best way to run Docker is on a Linux server with a sufficiently modern kernel. We worked really hard to work well with Red Hat, so we're shipping with RHEL. We've worked really hard to work with Canonical, so we're shipping in Ubuntu. We work on VMs. We have something called Boot-To-Docker, which is a great solution if you want a very minimal OS. CoreOS is another really minimal OS. I think, the question is not which of these will work best for Docker, but which works best in your environment. There are some people, who want to use Docker in a really radical way on minimal OS. They're looking at things like, CoreOS or Atomic. Then, there are people that want a full stack, in which case they're looking at RHEL or Ubuntu or something like that. Lucas: That makes a lot of sense. Are Linux containers going to replace virtual machines? We've been hearing in the news a lot about this. I'm curious what you have to say about that. Ben: What I believe is that, there are a lot of use cases that people put VMs to where VMs really weren't suited. I think you are going to see a lot of use cases where people are using Linux containers instead of VMs. There are a lot of use cases where VMs make sense and Linux containers don't make sense. There are a lot of use cases where the right answer is to run a bunch of containers inside of VM. Instead of having a thousand VMs, because you have a thousand applications, you have 10 VMs, which are each running 100 containers. Lucas: Can you give us a general sense of why somebody would pick a VM over a Linux container? What value does the VM bring? Ben: Yeah. Certainly, if you are trying to mix different OS families, you want to run a Windows app on a Linux box or vice versa, you're going to need a VM. Similarly, if what people are really trying to do is build flexible infrastructure and they really want to move and allocate servers or virtual servers to people, then VM makes sense. But if you are trying to create applications, modify them quickly, burst them across the platform sub servers or burst them to the cloud or if you are trying to build an application from lots of lucid cup of components and you want to containerize each of them. That's where Docker really shines. Lucas: So, the general idea here would be that a more consumer-facing thing might be, like a virtual desktop could be for virtual machines, but more server side stuff might be better suited to containers. Is that a general guideline? Ben: Well, again, if you want to combine, you want to run things in one operating system family on a machine that has a different host operating system, then, you need a VM. But if you are trying to build a system for creating, packaging and moving applications around and linking the components of an application together, you don't need a full VM. You don't need to treat and you shouldn't treat that application or that component as it were part of a whole server. And that's really where you want to use Linux Containers. It's interesting, because I think, once you probably realize that your Android phone, you've got a bunch of Linux containers. When you download Angry Bird, you are not downloading a VM. You are downloading something in a Linux Container and you know, it is isolated, so it doesn't mess up your address book. We do the same thing. We do it far more complicated, far more in-depth issue of running in the background on servers. Lucas: That's fascinating. I think, that really helps people think about it. So, do you think that the world is going to go back to bare metal hosting and do Linux Containers on bare metal rather than virtual machines -- we're cloud 1.0. Do you see us going back to dedicated hardware and adding Linux containers to them? Is that the next generation of cloud in your opinion? Ben: What I'll say is, Linux containers are far more flexible, they are much easier to spin up, they are much easier to destroy, and you can get significantly better performance density out of them. So, I certainly think, you are going to see a lot of usage of Docker on bare metal in the hosting environment. On the other hand, Docker and Linux containers in general, don't yet have a lot of the features that people have come to expect from VMs just because VMs have been around a lot longer. So I don't want to say that Cloud 1.0 is VM, Cloud 2 is Docker or Containers, but I will say that Cloud 2 will have a very healthy mix of Docker and VMs. Lucas: Radical. So that raises another question, because especially, in large enterprise a lot of development is still done on Windows machines, and right now, Docker is highly tied to Linux. Is Docker going to always be tied to Linux or could it potentially manage even virtual machines, regardless of Windows or whatever virtual machine you have. Ben: Yeah, also I was saying, today you can put VMs inside of Docker and you can put Docker inside of VMs. So, let's say what you are talking about you can already do. You can also run Docker host on a Windows machine using Boot2Docker. We have got some other way there. I think, what you all see is that Docker, right now, supports a whole bunch of different flavors of Linux Containers. We have also built the primitives in place so that we can extend that to things like Solaris Jones or BST JX which are more associated with Unix. And, of course, the natural place to go from there is to use something like Docker, to manage things that look like Containers inside of a Windows environment. That's certainly something we are exploring. We are talking to a lot of folks about doing it, but it won't be this year. Lucas: Got it. So, eventually, we will be able to run Windows apps within Docker, but it's not going to happen in the short term. Ben: That's right, and I think, more importantly, you will be able to create those apps using Docker to containerize all the components. And then, run them anywhere, at least, on any Windows machine. Lucas: Right. So it will still be running Linux applications, but it's all on Windows machine. Ben: Well, Docker relies on the Linux kernel. So, we need a Linux kernel somewhere. So, right now, you can create Linux apps and run them on a really light-weight Docker VM on a Windows box. Where we want to get in the near future is, where you can be really flexible in terms of creating Windows apps and run them on the Windows box. Lucas: Yeah, so, we've seen some of these new hosting providers, like Tutum, come out with Pure Docker host. We have only seen that in start-ups, but big cloud offerings like Amazon leaving central in cloud could go into the space as well and do you see that as part of the future too? Ben: Absolutely. We are really excited. At Docker Con, which was two weeks ago, we were joined by large numbers of people, who are creating Dockers as service offerings that includes people, who are Docker from the ground up like Tutum or Orchard as well as larger providers like, Rackspace, Google and Amazon, and Azure who have announced Docker support. And CenturyLink, we are really excited to be working with as well. Lucas: Great. So, what use cases make more sense for Docker? People that haven't yet used it, how are enterprises and large companies that you have been interacting with, adopting Docker? Where is the lowest hanging fruit? Where would it make sense to if you hadn't done something? Is there a kind of project? Did you try Docker with that? Ben: Yeah. I always recommend anybody who wants to get started with Docker should go to our site and do a 15 minute tutorial that we have online. It takes 15 minutes to get through and when I made it through successfully and our CFO made it through successfully, we know that we had something that was easy to use. But, I think, what you start to see is that most of us tend to start on one end of the software development life cycle. So starting with the developer, I am saying, how can I make the developer's life easy and make this path that goes from developer through test, QA stage into production work well. Once they have gotten that use case one down, then they start looking at, \"Hey, how can I use Docker to scale in production, whether that's across clusters or between clouds, et cetera.\" I think most people start with use case number one and they generally, start with stateless apps just because that's the easiest. What I think they often find is that using Docker is much faster to develop apps. I think, they also find that you can create a system, where you can go from dev to test to staging to production in minutes rather than in weeks, and have something that, goes smoothly rather than having things break at every step along the way, and have people pointing fingers at every step along the way. That's what we hear from people like Guilt and eBay and others who use Docker say. It went from weeks to minutes, nobody points fingers and basically developer pushes a button, the container gets built automatically, gets picked up by the automated testing system and 90 percent of the time it passes and it goes directly into production. The 10 percent of the time when it doesn't work, it's really clear whether the issue is inside the container and development needs to fix something or it is outside container and ops needs to fix something. Lucas: Yeah. I think that one of the really cool projects out there is around the CICD pipeline stuff. I know Drone is doing a pure Docker stuff. I also know that some of the big guys, Jenkins, are working on incorporating more Docker into some of the more traditional tooling for CICD. That really makes sense. For people that need a first application to try to Dockerize, would APIs or something around a modern API app, maybe a node app, make sense for Docker? Ben: Yeah. I think that'd be a good, easy place to go. Again, what I will say is, if you go to the Docker website, there are 50 different use cases that are published. Then to your point about things like Jenkins, one of the great things about Docker being so radically open and embracing the whole open source community ethos is that, we've got first-class integrations with Chef, Puppet, Salt, Ansible, Jenkins, Travis. And if you go on GitHub and you search for Docker projects, there are over 7,000 that have been done. There's a huge ecosystem of tooling and projects around Docker. If you go to Docker's hub, you'll also see that there are some 16,000 applications that have already been Dockerized that the community has published. There are lots of good use cases to look at, lots of good base components to get started with, and lots of great tools that allow you to build what you want with Docker as a LEGO kit. Lucas: That's really cool. If that's the best place for adopting Docker, what's the worst place? What are some of the biggest challenges and problems in adopting Docker into real life use cases right now? Where is it weakest? Ben: Sure. We just released 1.0. I always want to make sure people understand that, this is a 15-month-old project, and Docker Inc is 42 people and a pet turtle. Keeping that in mind, that can't even afford good headsets. Lucas: [laughs] Ben: I think that, certainly, Docker is not a VM. If what you want is a VM and you really understand what you want is a VM versus Docker, then, you shouldn't use Docker. Certainly, if you're trying to mix operating systems, if you've got an app where you really need to be able to freeze state and live migrate, we just don't have that functionality built in yet, but things are coming. On the other hand, what I encourage people to do is use Docker and containers in general as an opportunity to rethink a lot of their long-held assumptions. Because, I think, a lot of the tooling and the methodology we've built around ourselves assumes that application equals application server and that VMs are hard to build, take a long time to spin up, et cetera, and are expensive. In the case of a container right there, you can create them in milliseconds, you can destroy them in milliseconds, and you can migrate them easily, and so results a lot of the pain and complications that we've put into things to manage all of our apps, as if they were servers. You can rethink it and drill out. Lucas: Got it. That makes a lot of sense. In a few weeks I'll be interviewing Solomon on this podcast. For him, I'm going to be going a lot more into the technical weeds and asking him a lot of those kind of questions, but for you, I'm particularly curious. As CEO, how will Docker make money? Because I think a lot of the ecosystem around Docker, right now, does have a concern. Are we just building something that the Docker guys are going to eat our lunch? [indecipherable 0:23:00] , originally, had some private hosted Docker repositories, and now with the Docker hub, now Docker has that as well. How is Docker planning to make money? Ben: Good question. We've tried to be pretty transparent in terms of how we're planning to make money. We're also trying very hard to be good stewards. We know that we've got an open platform. There are going to be people who build on top and below us in that platform. Some of those things will compete with what Docker Inc does and some won't, and that's OK, because we want the best tools to win. A few things that I'll say that we won't do. We will never disadvantage any competitor by messing around with open source and messing with the API. We drive nice clean APIs that we use and everybody else uses. We don't have any hidden roadmaps. All the work that's done on Docker engine is done out in the open. The governance is out in the open. We've committed to not doing open core, so we're not going to create some commercial version of Docker that's better than the open source, or commercial version of Docker engine that's better than the open source engine. With that having said, what do we plan to do? We've launched Docker Hub. We've launched commercial support, so we're offering commercial support for Docker. Docker Hub, today, has a lot of tools around content and collaboration and workflow for that CICD use case. Right now, it's delivered purely as a service. We've also got lots of people who have expressed an interest in being able to bring Docker Hub on-premise or to extend its functionality to more production type use cases, management, marketing and orchestration of containers. Those are the directions that, we're going in to make money. Again, what we think is that, we've drawn a nice clean line between Docker and what's open source and are committed to maintaining that as a platform that will allow a lot of different tools to develop and some will compete with us, some won't. Again we think that the best tools will win. Lucas: This is really fascinating. Seeing companies grow up in an open source way, rather than you look at older companies like VMware that have built up great companies building great software that have been adopted by every Fortune 1,000 out there. It's fascinating to see different approaches and this open source stuff is a really interesting way to embrace a community, but also build a business. You launched Docker Hub, and that's an amazing thing. It's really cool to see this, because, for me, I've had a foot in system administration for a large part of my career. I've been doing the DevOps thing for a very long time. I've run large clustered systems, I've written the code for large clustered systems. It's fascinating to see things like GitHub come out and make coding collaborative. We've had version control for many decades and those version controlling tools were things, that we could use maybe internally and [indecipherable 0:26:49] within our own team, but now GitHub blasts that out of the sky. Anyone in the world can collaborate with you, and it makes it terrifically easy to work together, collaborate on open source software, on systems, on code, with people you might never have met. I think the things like the success of Bitcoin have come out of this. Whether Bitcoin could have survived as an open source project before the open collaborative stuff like GitHub existed is, I think a big question. One of the things that I think is, so cool about the Docker Hub is that it does for the DevOps community, for the system administrator community, a similar thing to what GitHub did for developers. Is that what you intended? Is this what you meant to do? Ben: Yes. What we wanted to enable is a lot of different kinds of collaboration. We've gone from a world where apps were long-lived, proprietary, monolithic, and deployed to a single server, to a world where development is iterative and constant. It's built from lots of loosely coupled components, tends to be open and tends to be deployed across huge numbers of servers. We want to create the collaboration tools for that environment, which means making it really easy for developers to collaborate with each other, really easy for them, to collaborate with sysadmins and DevOps types, and really provide the tools that makes DevOps not a fantasy, but a reality. You were thinking about the cultural and business implications of open source, and I think that, Docker brings along some other really important cultural and business implications. One of them is recognizing that instead of concerns that developers have...developers tend to like to experiment, try new things, break things and ops tends to like to have things be automated, consistent, secure, performant, all of those good things. They're both right, but getting them to agree on a set of tools and a set of methodologies is really difficult, because what one person does endangers the job of the other person. With Docker and with Docker Hub in particular, we get the best of both worlds, in that you can be really creative and share and reuse components as a developer, and yet as a sysadmin there's a nice clean boundary between it. You can automate the whole testing process and you can have a lot more consistency in how you deploy to production. For me, it's all really exciting, as exciting on a community and a business standpoint as it is on a technical standpoint. Lucas: Yeah. That makes a ton of sense. It really has the potential to transform an industry. How do you actually see Docker transforming the industry? What do you think it's going to do to the way people think about creating applications? Is this the gateway technology to making 12-factor application building a reality? Ben: Yeah. I hope so. I hope that it's the gateway towards a new era of development, where people really can develop anywhere and can be unleashed to develop amazing things without being shackled by concerns of knowing in advance, how things are going to run, where they're going to run. Our mission, as a company is to build tools of mass innovation, which sounds a little corny, but we really do think, it can be that significant. Stupid historical analogy, but why was the printing press so cool? It's because, it meant that the active creating content and creating new ideas, wasn't bound to your ability to produce books. Why was the Internet so cool? Because to get to the nth level, and then, you didn't have to worry about the cost of producing books, and you can collaborate in creating things. Why else was the Internet cool? Because you could send a message from one place to other. You don't have to worry about how it was getting from point A to point B. We think we can achieve all those things with the application world as well, right? Make it easy to create application without worrying about how they're going to run, make it easy to collaborate in the creation of applications. Also as Solomon says, \"Make running an application on the Internet, as easy as sending a message across the Internet.\" You send a message across the Internet, we don't care which routers are between point A and point B. [indecipherable 0:32:01] we shouldn't care, which servers are an application are going to run on. I just accept in term of highly secure types of scenarios. Lucas: It could really build the bridge between developers and ops teams. It could combine those oil and water that they've been always [indecipherable 0:32:25] . Ben: We're the soap, that less oil and water. Lucas: [laughs] That's really cool. What's next for Docker? What's on the horizon? 1.0 just came out. DockerCon was a huge success, lots of interest. Do you guys share how many people are on Docker Hub? Is that one of your public statistics? Ben: It isn't yet. We need to figure out what the good measurement is of it. We've had multiple thousands of people signing up per day, which is really exciting. Lucas: That's great. Ben: Docker, itself, we talked about, it has been downloaded, 3 million times, 16,000 applications published to the Docker index, 7,000 projects on GitHub. My favorite step though is that, we now have Docker Meetups in 100 cities across 35 countries in every continent except for Antarctica. If you know somebody in Antarctica... Lucas: You better take in Antarctica. One of our viewers is up there. Let's do this. Ben: We want to do it. We want to get all seven. Lucas: What's next? What's on the horizon? What's past, what we can see right now? Ben: Docker 1.0 was a huge milestone, but the Docker engine side, there's clearly a lot more that we need to do. Solomon spoke about some of the things that we're trying to do around libcontainer, libgen, and libswarm, which are all really important enhancements to Docker. In general, we'll be able to get much better in orchestrating containers in a consistent way, looking at containers together, and dealing with things like storage and networking. On the hub side, it also just released the DockerCon. We've got a huge list of improvements that we want to make, adding more collaboration tools, making it even easier to hook in tools like Jenkins or Bamboo or Chef or Puppet or Salt. What you're also going to see from Docker over the next several months now, that we've announced commercial support is more and more use cases that people in places like financial institutions or Fortune 500 toward using Docker. Talking about it publicly, we've had huge numbers of web companies like Guilt and Groupon and eBay and Spotify talking about their use of Docker. I think that the more conservative organizations are waiting to hear from their own, before they jump in. A lot of them are now starting to jump in. Lucas: That's very cool. It has been a real pleasure to have you on. It's really great to pick your brain and understand where things are going and how the industry is changing. I really look forward to continuing the dialogue. I can't wait to see more. Solomon mentioned some stuff around security. Is that also in the future? Ben: Absolutely. I think, we've also talked a little bit about this. If the future is lots of applications that are changing constantly built out of components themselves, containerized, and moved everywhere, obviously, security becomes a concern. We think that the right thing to do is just to start by building that in, at the container level, which means things like container signing, understanding where containers came from, who created them, when they were created, and what are some of the fundamental capabilities that containers are asking for. It's really easy to build security around containers if you know where they came from, who created them, setting policies around whether you're going to allow container rude access or not, et cetera. That's it. I've got other thing on our long list, but DockerCon just finished. We're giving ourselves the weekend off, and then we're going to get going. Lucas: Great. I am very, very happy. Thank you for your time. It's been great talking with you. Ben. Thank you, Lucas.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-makes-a-good-rest-api-framework-part-1", "author": ["\n              Luc Perkins\n            "], "link": "https://www.ctl.io/developers/blog/post/what-makes-a-good-rest-api-framework-part-1", "abstract": "What are some of the best practices used to create a Good REST API Framework? Read on to find out. A few years ago, the world of web development was crazy about new web frameworks. \"Hey everybody, check out my new framework! It does cookies, headers, status codes, the whole deal!\" While this excitement made sense at a time when there weren't a lot of options beyond ASP.NET, crufty Java frameworks, PHP, and a still-nascent Ruby on Rails, web frameworks don't generate the same kind of excitement, and rightfully so. Instead, the focus has shifted to abstraction layers on top of HTTP frameworks . REST API libraries are one of the most common abstractions that we now see built on top of traditional HTTP frameworks. What puzzles me, however, is that I find the discussion around REST API libraries to be somewhat lacking. Whereas you can find all kinds of HTTP framework comparisons, best practice-related discussions, and so on, I don't see REST API frameworks getting the same treatment. So I thought I'd get the ball rolling by delving into what makes a good REST API framework and providing a shortlist of what I take to be exemplary frameworks. While REST API frameworks vary a great deal in terms of what they offer, it'd be hard to consider a framework to be any good if it didn't Versioning This is one is pretty basic but nonetheless important. When exposing a REST API to the outside world, it's important that you don't change the core API incrementally and without fanfare, as that's a good way to break applications (if not businesses). Instead, any API changes should be grouped together and introduced via version changes. We've all seen REST APIs with routes that are prefixed with /v1/ or /v1.0/ , for example. A good REST API framework allows you to accomplish this in a programmatic fashion without having to go back and modify all of your routes one by one, as many HTTP frameworks would do. Resources rather than routes If you're building something simple and want to produce your API on a route-by-route basis---e.g. GET /posts , GET /posts/:id , POST /posts ---and define the logic associated with each route, an HTTP framework will usually be enough. But this can be unnecessarily burdensome. In many cases, it's easier to construct a resource---e.g. Post , User , Meetup , Tweet ---define what it means to perform CRUD operations on that resource, register that resource, and have the routes created for you. A good REST API framework should abstract away route definitions while leaving things like database interactions (ORM-based or otherwise) in your hands. Easy authentication While there are some REST APIs that provide public information only---the Marvel Comics API is my favorite example---most need to enable users to perform API interactions securely, whether via basic or digest auth, OAuth, or something different. A good REST API framework should make it easy to implement your own authentication logic and to apply that logic to your API's resources. Instead of being forced to sprinkle authenticate_user( ... ) functions all over your routing logic, as an HTTP framework would do, you should be able to define your authentication layer at the resource level . Form body parsing Most REST APIs nowadays accept multipart form data as a means of inputing new information, as in this API call: Parsing multipart form data on the server side can be a hassle, often demanding a complex if/then logic for parsing through key/value pairs in a way that maps onto resources that you've defined. A good REST API framework should make this very simple, allowing you to define which form fields must be present to create a resource and which data types the values of those form fields should be. Documentation generation Let's face it: an undocumented REST API is no good. If you're REST API is public facing, it needs to be documented so that people can build meaningful things on top of your services. And even if your REST API is used only internally as part of a service-oriented architecture, people within your organization need to feel at home in your API. One way to document an API is to do it the old-fashioned way, i.e. to REST API documentation can be a beautiful, inspiring thing. Personal favorites include Stripe and Parse . A good REST API framework should enable you to go from zero to decent documentation with as little effort as possible. In this post, I've provided a list of things that I take to be essential for any REST API framework worth its salt. I'd love to get your thoughts in the comments. Anything I'm missing? Anything that I've listed here that should be left to HTTP frameworks instead? Any examples you'd like to share? Thanks for reading, and please, don't hesitate to give us your feedback.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-is-flynn-an-open-source-docker-paas", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/what-is-flynn-an-open-source-docker-paas", "abstract": "Flynn.io is one of the most popular and powerful open source Docker PaaS solutions around. Flynn believes that ops should be a product team, not consultants. Flynn is the single platform that ops can provide to developers to power production, testing, and development, freeing developers to focus. This week we talk to one of the creators of Flynn, Jonathan Rudenberg. Jonathan is a super smart guy who used to work at Shopify and has brings his experience with large distributed systems into Flynn. Jonathan Rudenberg , Flynn.io Creator Besides being one of the cofounders of Flynn, Jonathan is a developer, security consultant and researcher. He is also one of the Tent Protocol architects. Besides working on the future of Docker-oriented PaaS', Jonathan occasionally posts on his blog, Titanous.com . Flynn is an open source PaaS built from pluggable components that you can mix and match however you want. Out of the box, it looks a lot like Heroku (with git push functionality), but you can replace pieces and put whatever you need into Flynn. flynn-host The Flynn host service discoverd The Flynn service discovery system flynn-controller The Flynn Controller for the management of applications running on Flynn via an HTTP API flynn-bootstrap Bootstraps Flynn Layer 1 gitreceived An SSH server made specifically for accepting git pushes that will trigger an auth script and then a receiver script to handle the push. (This is a more advanced, standalone version of gitreceive .) flynn-cli Command-line Flynn HTTP API client flynn-receive Flynn’s git deployer slugbuilder A tool using Docker and Buildpacks to produce a Heroku-like slug given some application source. slugrunner A Docker container that runs Heroku-like slugs produced by slugbuilder . flynn-dev Flynn development environment in a VM strowger Flynn TCP/HTTP router shelf A simple, fast HTTP file service sdutil Service discovery utility for systems based on go-discover flynn-postgres Flynn PostgreSQL database appliance taffy Taffy pulls repos and deploys them to Flynn go-flynn go-discoverd rpcplus Docker is great for running containers on single servers, but not great at running distributed apps on many servers. Flynn helps you do that better. Many PaaS technologies mainly focus on scaling a stateless app tier. They may run one or two persistent services for you, but for the most part you are on your own to figure that part out. Flynn is really trying to solve the state problems, which is pretty unique. We use etcd from CoreOS in Flynn. Flynn can run on CoreOS, but CoreOS is essentially just a Linux distribution whereas Flynn is a PaaS You can deploy a Flynn cluster with a simple tool we built here: https://flynn.cupcake.io/ . There is also a Vagrant VM and you can bootstrap it yourself on your own environment. Flynn managed database services for you. Right now we only have Postgres support, but our goal is to have Flynn help you run the database services so you don't have to. Application filesystems are currently ephemeral still, but we are looking into solutions in the future that could provide persistent filesystems for app instances. Although there is currently not a direct tie in, you could build a CI tool on top of Flynn. It is very much not production ready. We are focusing on improving the production readiness of Flynn, but right now there are a lot of rough edges and a lot of bugs. The general timeline on 1.0 is around early Fall 2014. Pinkerton is a tool for manipulating Docker images outside of Docker. Containers have been around a long time, but still aren't perfect yet. They don't provide as much security isolation as a VM, which means containers are not ready for multi-tenant environments yet. The goal of Flynn is to change operations from a consulting team that goes off and writes Chef scripts for people to an actual product team that runs the platform that enables engineers. To enable something like IT-as-a-Service.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "the-future-of-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/the-future-of-docker", "abstract": "Jeff Lindsay (creator of Dokku ) is one of the most prolific people in the Docker ecosystem. He not only wrote Dokku and contributed to Flynn , two of the most popular Docker PaaS technologies out there, but he is busy creating technologies right now that we are all likely to adopt within the next 6-12 months. During this interview, we talked about everything from the very beginning of Docker when Jeff and Solomon started hacking it together, to the latest unreleased open source projects in the Docker world. Jeff Lindsay , Creator of Dokku Jeff has been writing and designing software professionally for over 15 years, casually for almost 25 years. I've worked with Twilio, DotCloud/Docker, Transcriptic, Piston Cloud, CommerceNet, NASA Ames, and others to build prototypes, products, and platforms. He has a large open source portfolio, lots of speaking experience, and a history of starting communities and cultural institutions, including SuperHappyDevHouse and Hacker Dojo. I've been interested in PaaS since Heroku and AppEngine, so I love the idea, but I wanted something hackable. So with Flynn, I imagined it could be like a super-Dokku that is distributed across many servers. I found Daniel Siders and Jonathan Rudenberg and we all had such a close vision that we started collaborating on Flynn together. The ambassador model is like a gatekeeper for Docker containers to talk to each other across multiple servers. The ambassadord project is like the ultimate ambassador container so that you only need to run one of these per host server and every other container on that host can use it to route to any other server connected with its own ambassadord container. Consul is a service abstraction API on Serf that implements RAFT that creates a consistent key/value store like etcd . Consulate is distributed discovery and routing mesh for Docker. Registrator is a service registry bridge for Docker (formerly docksul). When you combine registrator, consulate and ambassadord, any Docker container across any host can talk to each other. This is the holy grail for Docker! Duplex is a simple, efficient, extensible application communications protocol and library. It's heavily inspired by ZeroMQ and BERT-RPC, but draws influence from Twitter Finagle, HTTP/2, zerorpc, and Unix pipes. Although heavily inpsired by ZeroMQ and nanomsg, those messaging abstractions work against the goals of Duplex. There are many great features that Duplex emulates, mostly to employ the overall distributed/edge messaging philosophy, and building in connection pools, reconnect logic, edge queuing, and optimizing for high-throughput async usage. I got the name Manifold from the Dyno Manifold from Heroku. Manifold is built on Consulate. It is service discovery with a distributed scheduler. The distributed scheduler part is like Apache Mesos . It is like a container compute grid. The idea with Configurator is to create a REST API frontend that will write configuration files to Nginx or Haproxy or Apache for you. This lets developers create robust systems quickly and easily without having to figure out how to write complex configuration file formats. Here is a list of the projects that are discussed in this podcast.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "optimizing-docker-images", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/optimizing-docker-images", "abstract": "Docker images can get really big. Many are over 1G in size. How do they get so big? Do they really need to be this big? Can we make them smaller without sacrificing functionality? Here at CenturyLink we've spent a lot of time recently building different docker images . As we began experimenting with image creation one of the things we discovered was that our custom images were ballooning in size pretty quickly (it wasn't uncommon to end up with images that weighed-in at 1GB or more). Now, it's not too big a deal to have a couple gigs worth of images sitting on your local system, but it becomes a bit of pain as soon as you start pushing/pulling these images across the network on a regular basis. I decided it was worthwhile to really dig into the docker image creation process so that I could understand how it works and whether there was anything we could do differently to minimize the size of our images. As a quick aside, Adriaan de Jonge recently published an article titled Create The Smallest Possible Docker Container in which he describes how to create an image that literally contains nothing but a statically linked Go binary that is run when the container is started. His entire image is an amazingly small 3.6 MB. I'm not going to be discussing anything quite so drastic here. As someone who's more comfortable working with languages like Python and Ruby I need a little bit more OS-level support and will happily sacrifice a hundred megabytes of space so that I can continue to run Debian and apt-get install my dependencies. So, while I'm jealous of Adriaan's diminutive image, I need to support a wider range of applications than is practical with his approach. Before we can talk about how to trim down the size of your images, we need to discuss layers. The concept of image layers involves all sorts of low-level technical details about things like root filesystems, copy-on-write and union mounts -- luckily those topics have been covered pretty well elsewhere so I won't rehash those details here. For our purposes, the important thing to understand is that each instruction in your Dockerfile results in a new image layer being created. Let's look at an example Dockerfile to see this in action: This is a pretty useless image, but will help illustrate the point about image layers. We're using the debian:wheezy image as our base, creating the /tmp/foo directory and then allocating a 1 GB file named bar in that directory. Let's build this image: $ docker build -t sample , sending build context to Docker daemon 2.56 kB: Step 0 : FROM debian:wheezy ---> e8d37d9e3476 Step 1 : RUN mkdir /tmp/foo ---> Running in 3d5d8b288cc2 ---> 9876aa270471 Removing intermediate container 3d5d8b288cc2 Step 2 : RUN fallocate -l 1G /tmp/foo/bar ---> Running in 6c797329ee43 ---> 3ebe08b36733 Removing intermediate container 6c797329ee43 Successfully built 3ebe08b36733 If you read the output of the docker build command you can see exactly what Docker is doing to construct our sample image: We can see the end result by looking at the output of the docker images --tree command (unfortunately, the --tree flag is deprecated and will likely be removed in a future release): In the output you can see the image that's tagged as debian:wheezy followed by the two layers we described above (one for each instruction in our Dockerfile ). We often talk about \"images\" and \"layers\" as if they are different things -- in reality each layer is itself an image. An image's layers are nothing more than a collection of other images. In the same way that we could say: docker run -it sample:latest /bin/bash , we could just as easily execute one of the untagged layers: docker run -it 9876aa270471 /bin/bash . Both are images that can be turned into running containers -- the only difference is that the former has a tag associated with it (\"sample:latest\") while the later does not. In fact, the ability to create a container from any image layer can be really helpful when trying to debug problems with your Dockerfile . Knowing that an image is really a collection of other images, it should come as no surprise that the size of an image is the sum of the sizes of its constituent images. Let's look at the output of the docker history command: This shows all of the sample image's layers along with the instruction that was used to generate that layer and the resulting size (note that the ordering of layers in the docker history output is reversed from that of the docker images --tree output). There are only two instructions that contribute anything of substance to our image: the ADD instruction (which comes from the debian:wheezy image) and our fallocate command. Together, these two layers total 1.16 GB which is roughly the size of our image. Let's actually save our image to a tar file and see what the resulting size is: When an image is saved to a tar file in this way it also includes a bunch of metadata about each of the layers so the total size will be slightly bigger than the sum of the various layers. Let's add one more instruction to our Dockerfile : Our new instruction will immediately delete the big file that was generated with the fallocate command. If we docker build our updated Dockerfile and look at the history again we'll see the following: Note that our addition of the rm command has added a new (0 byte) layer to the image but everything else remains exactly the same. If we save our new image we should see that it's almost exactly the same size as the previous one (it'll differ only in the little bit of space that is needed to store the metadata about the new layer we added): If we were to docker run this image and look in the container's /tmp/foo directory we would find it empty (after all, the file was removed). However, since our Dockerfile generated an image layer that contains a 1 GB file it becomes a permanent part of this image. Note: Each additional instruction in your Dockerfile will only ever increase the overall size of your image. Clearly, this example is a little silly, but the idea that images are the sum of their layers becomes important when looking for ways to reduce the size of your images. In the following sections, I'll discuss some strategies for doing just that. This is a pretty obvious tip, but the base image you choose can have a significant impact on your overall image size. Here are the sizes for some of the more common base images: On our team, we had been using the ubuntu image as the base for most of our work -- only because most of the team was already familiar with Ubuntu. However, after playing with the debian image we realized that it actually did everything we needed and saved us 100+ MB in image size. The list of viable base images are going to vary depending on the needs of the image you're building, but it's certainly worth examining -- if you're using Ubuntu when BusyBox would actually meet your needs you're consuming a lot of extra space unnecessarily. One thing I would love to see is the image size displayed on the Docker registry. Right now, the only way to see the size of an image is to first pull it to your local system. One of the nice things about the layered approach to creating images is that layers can be re-used across different images. In the example below you can see that we have three different images that all use debian:wheezy as their base: Each of the three images adds something on top of the debian:wheezy image, but there aren't three copies of Debian. Instead each image simply maintains a reference to the single instance of the Debian layer (one of the reasons I like the docker images --tree view is that it makes the relationship between the different layers very clear). This means that once you've pulled the debian:wheezy image you shouldn't have to pull those layers again and the bits for the image only exist once on your local file system. So you can save a significant amount space and network traffic by re-using a common base across your different images. In our sample image above we created a file and then immediately deleted it. That was a contrived example but similar scenarios often do occur when building images. Let's look at a more realistic example: Here we're pulling down a tar file, extracting it, moving some files around and then cleaning up. As we saw before, each one of these instructions results in a separate layer. Even though we're removing the tar file and the extracted directory those things are still part of the image. The wget command resulted in a layer that was 55 MB and then extracting the tar created a 99 MB layer. We don't actually need either of these things in the final image so we've got 150+ MB of wasted space here. We can work around this issue by refactoring the Dockerfile a little bit: Instead of executing each command as a separate RUN instruction we've chained them all together in a single line using the && operator. While this makes the Dockerfile a little harder to read it allows us to clean-up the tar file and extracted directory before the image layer is committed. Here's what the resulting image looks like: Note that we end-up with exactly the same result (at least as far as the running container is concerned) but we've trimmed some unnecessary layers and 150 MB out of the final image. I wouldn't recommend that you go and string together every command in your Dockerfile but if you find a pattern like the one above where you're creating and later removing files then chaining a few instructions together can help keep your image size down. The strategies we've discussed so far all assume that you're building your own image or at least have access to the Dockerfile . However, there may be situations where you have an image that was created by someone else that you'd like to slim down. To do this we can take advantage of the fact that creating a container from an image results in all of the image's layers being merged. Let's return to our sample image (the one with the fallocate and rm commands) and docker run it: Since our image doesn't really do anything it exits immediately and we're left with a stopped container that is union of all our image layers (I used the -d flag here only so that the ID of container would be displayed). If we export that container and pipe the contents into the docker import command we can turn the container back into an image: Note how the history for our new sample:flat image has only one layer and the total size is 85 MB -- the layer containing our short-lived, 1 GB file is completely gone. While this is a neat trick it should be noted that there are some significant downsides to flattening your images in this way: So I certainly would NOT recommend that you go and flatten all of your images, but it can be a useful in a pinch if you're trying to optimize someone else's image. Also, it can be a helpful tool if you simply want to see just how much space you could squeeze out of your own images. Managing the size of Docker images is a challenge. It's easy to make them smaller without sacrificing functionality. Don't have an account on CenturyLink Cloud? No problem. Just head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "the-docker-book-an-in-depth-interview", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/the-docker-book-an-in-depth-interview", "abstract": "This week we spoke to a leading member of the Docker community, James Turnbull. James is the VP of Services & Support at Docker and also the author of The Docker Book . He's the authoritative brain-to-pick when it comes to all things Docker. James describes himself as being “in IT far too long, about 20 years\". He has a broad experience across platforms; having started on the mainframes, he's seen the whole evolution. He's been a systems programmer, system administrator, security guy, and an operator way back when ops meant getting in to the office in the morning and distributing the overnight printouts from the mainframe to each department. James Turnbull VP of Engineering at Kickstarter James Turnbull is an Australian free software and open source author, security specialist, and software developer. He lives in Brooklyn, New York where he is VP of Engineering at Kickstarter and an advisor at Docker Inc. In addition to haven just written The Docker Book , James also has written many more books including Pro Puppet, Hardening Linux, Pro Linux System Administration, Pro Nagios and The Logstash Book Containers and like technology have been around in one form or another for ages; from old-school IBM VM technology -- perhaps better known now as z/VM -- to Solaris Zones . Linux has had a surprisingly long history with containers, largely through LXC's interface to kernel functionality. What Docker has done here is make containers easy to use and share, providing tools to create images that can be used to quickly realize new, running containers. This will probably depend on specific considerations about real-world workloads and the fundamental differences between machines (virtual or not) and containers impact their desirability? What about security and other considerations? There's a lot to consider before we can declare virtual machines dead. Docker has \"accelerated the concept of DevOps, not changed it\" by making Linux containerization technology easily accessible to the masses as well as providing the tools and infrastructure to shuttle images around. We see some prior art here (term used very loosely), with tools like Vagrant leveraging virtualization technology like VirtualBox to create custom VM instances \"on demand\". Docker and containers, by their nature, have helped lead to a reexamination of existing architecture. As we break our applications down into smaller and smaller chunks to take advantage of containerization -- \" microservices \" -- we see a lessening of the importance of the operating system itself. In this way, Docker has been a huge unanticipated benefit for Service Oriented Architecture . With static compiling, how about it? What's the thinking and pros vs cons behind single-file containers -- that is, containers who only contain a single file, with all its dependencies statically linked in. Look at your workloads and workflows. You may find looking at the creation of \"dev-centric\" workflows will take you to a happy place of \"dev and ops harmony\"! Tools like Puppet tend to encourage \"dev vs ops\"; while Docker containers and container images tend towards \"dev + ops\". Everything is very dynamic at the moment; orchestration tools, trust, multi-host deployment and service discovery all have a great deal of focus and a ways to go before we catch up with the tools available for traditional VM's. That's not to say Docker won't catch up, just that other companies like VMware have been doing this for years longer than we have :) Very, very few. About 1% \"have a clue\" -- those that do also tend to have a clear definition of the role they're recruiting for. There's a big opportunity to \"get it right\" in this area right now! Docker is an Open-Source project that has faced more attention and focus in the last 18 months than most do in 18 years. We're busy building out tools and infrastructure (like the Docker Hub ), and design in the open by default. Docker the company is quite the \"friendly, happy place to work.\" If anything, one of our largest challenges is \"too much work; not enough people to do it\" -- one of the better challenges to have, I think.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "announcing-panamax-docker-management-for-humans", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/announcing-panamax-docker-management-for-humans", "abstract": "\"Panamax allows developers to go from zero to pre-packaged, multi-container Docker environments in minutes. The web interface and overall user experience is impressive.” –Gabriel Monroy, creator of Deis Docker is an amazing tool, but it remains notoriously hard to use to build multi-container micro-service architected applications. That is until today . Today, I am proud to announce the release of the Apache 2 open-source platform we call: Panamax . Over 9 months in the making, CenturyLink has invested in a team of 11 killer seasoned engineers and designers to build a tool that we believe will dramatically lower the barrier of entry for developers to use Docker, CoreOS and other innovative technologies. To build complex clustered containerized applications before Panamax, one needed to chose a variety of new technologies, each with it’s own learning curve: fig, etcd, systemd, fleet, mesos, ambassadors, discoverd, consul, consulate, serf, registrator, skydns, libswarm and the list keeps growing. We have tried to keep up with all the changes on this blog, but it is easy to fall behind. What if there were just one project that applied Docker best practices and took care of stitching Docker containers together for you? That’s Panamax. That’s what I imagined 9 months ago. “Panamax is an exciting improvement on the Docker user experience.” –Jonathan Rudenberg, creator of Flynn Panamax has a beautiful and elegant UI component for deploying Dockerized applications with, but it is not just a Docker UI. First of all, CoreOS (an industry leading cutting edge operating system for running clustered Docker systems) is built into Panamax. Along with CoreOS, Panamax leverages Fleet. Built-in container orchestration and job scheduling from Fleet goes way beyond regular Docker UIs. But Panamax’s real killer feature is its Open-Source Application Template Library. Deeply inspired by Orchard’s fig format, Panamax took the idea one step further by creating a repository on GitHub that houses the configuration and architecture of these Dockerized apps. Anyone in the world running Panamax can use the Open-Source Application Template Library to reproduce complex multi-container Docker apps with a single click. \"We're pleased to see Panamax launched! It's an exciting project that shows the strength and diversity of the Docker ecosystem.\" –James Turnbull, VP at Docker Panamax is not a PaaS. It is a containerized cloud builder. In fact, you can (in theory) install any Docker PaaS on top of Panamax: Deis, Dokku, Flynn. We hope the community will create app templates for these PaaS that can be used to install any PaaS with one click. What makes Panamax different than PaaS? First of all, PaaS is inherently opinionated. You give a PaaS your code and it does all the rest. Panamax is a tool for building containerized clouds. If you don’t use an app template, you have total control over the entire architecture of your app as you build it. For example, in PaaS you have to use the router and load balancer that the PaaS choose for you. In Panamax, there is no opinionated router or load balancer. You search Docker Hub for nginx or haproxy, or build a container yourself and stitch it into your application. Panamax doesn’t force opinions. It gives you total control. “Docker containers are great, but you need more than just one to run your application. The Panamax solution brings application templates and a public repository to share and collaborate that is elegant and simple to use.” –Borja Burgos, founder of Tutum Edit: The Panamax Application Template Contest has concluded and the contest page listing the rules, prizes and winners is no longer available To celebrate the launch, we are giving away over $100,000 in cool gadgets. You can win one of 30 New Mac Pros, 30 New iPad Airs, a copy of the Docker Book and even be featured in this blog. All you have to do to win one is create the most elegant, clever and popular application templates for Panamax that you can. All application templates must be submitted within 2 weeks from today. To learn more about the rules and how to compete, go to Panamax.io. The launch is just the beginning. What comes next is up to you and the community that grows around Panamax. CenturyLink incubated this project with the intention of giving back to the open-source community. We did not just go and find some project nobody cared about and put the source code on GitHub and call it open-source. Our team of 11 has put their best foot forward trying to make clean and legible code so that it would be easy for people to join in and collaborate. Check out the source code on GitHub: https://github.com/centurylinklabs/ . We are very proud of it. Help us build features and functionality we only dream of. Join us in making complex Docker apps easier to build today. Introduction to Panamax from CenturyLink from cardmagic", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "caching-docker-images", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/caching-docker-images", "abstract": "Want to significantly speed up the Dockerfile code/build/test cycle? In this article I'll discuss how the Docker image cache works and then give you some tips for using it effectively. Each instruction in your Dockerfile results in a new image layer being created and added to your local image cache. That image then becomes the parent for the image created by the next instruction (see my previous article for a detailed explanation of the image creation process). Let's look at an example: If we docker build this Dockerfile and inspect the local image cache we'll see something like this: The FROM instruction in our Dockerfile corresponds to the image layer tagged with debian:wheezy . The three child layers shown underneath that correspond to the other three instructions from our Dockerfile . Another way to look at this is with the docker history command: With this view, the order is reversed (the child image appears before the parent) but you do get to see the Dockerfile instruction that was responsible for generating each layer. After you've successfully built an image from your Dockerfile you should notice that subsequent builds of the same Dockerfile finish significantly faster. Once docker caches an image layer for an instruction it doesn't need to be rebuilt. Let's look at an example for the Dockerfile above. We'll run the docker build command with the time utility so that we can see how long the initial build takes to complete. You can see that it took 21 seconds to complete the build. Our example here is fairly trivial but it's not uncommon for builds to take many minutes once you start adding more instructions to your Dockerfile . If we immediately execute the same instruction again we should see something like this: Note how each instruction was followed by the \"Using cache\" message and the total build time dropped from 21 seconds to less than a second. Since we didn't change anything between the two builds there was really nothing for docker to do — everything was already in the cache. As Docker is processing your Dockerfile to determine whether a particular image layer is already cached it looks at two things: the instruction being executed and the parent image. Docker will scan all of the children of the parent image and looks for one whose command matches the current instruction. If a match is found, docker skips to the next instruction and repeats the process. If a matching image is not found in the cache, a new image is created. Since the cache relies on both the instruction being executed and the image generated from the previous instruction it should come as no surprise that changing any instruction in the Dockerfile will invalidate the cache for all of the instructions that follow it. Invalidating an image also invalidates all the children of that image. Let's make a change to our Dockerfile and see how it impacts the local image cache. We'll update the apt-get install instruction to install Emacs in addition to Vim: Let's build our new image and see what happens: Since we didn't alter the MAINTAINER instruction it was found in the cache and used as-is. However, we did edit the apt-get line so it resulted in a completely new image layer being created. Furthermore, even though we didn't change the ENTRYPOINT instruction at all, its layer also had to be rebuilt since its parent image changed. If we look at the image tree again we can see the two new layers that we created alongside the layers that were generated from the previous version of our Dockerfile : Note how the layer for the MAINTAINER instruction (c58b36b8f285) remained the same, but it now has two children. The layers generated from the previous version of our Dockerfile are still in the cache it's just that they are no longer part of the tree tagged as vim:latest . Now that you are familiar with how the Docker image cache works, next week we will discuss some strategies for making the most of it when working on your own Dockerfile .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "exposing-a-local-panamax-application-to-the-internet", "author": ["\n              Ross Jimenez\n            "], "link": "https://www.ctl.io/developers/blog/post/exposing-a-local-panamax-application-to-the-internet", "abstract": "Once you have an application running via Panamax, you can always create a template for that application so it can easily be recreated via another Panamax client. This is powerful concept for a development workflow, but can fall short when you just want to share an application running on your development machine with a colleague, or demo an application with a client. Normally, making your application publicly accessible would require deploying to a server, but there is another option. Using a great open-source project called ngrok, you can quickly and securely expose a local web server to the Internet. Even better, you can do so by simply adding a temporary ngrok container to your existing application in Panamax. ngrok is a great open source project by Alan Shreve and is also available as a service at ngrok.com . At its base, ngrok is a reverse proxy that creates a secure tunnel between a public Internet-accessible endpoint to a locally running web service. Here at CenturyLink Labs we’ve recently built many different Docker images , including a ngrok image centurylink/ngrok which is highly leveraged from an image that Fletcher Nichol originally created: fnichol/ngrok . This image is great as it is very lightweight and will download quickly at slightly over 20mb. Lets walk through how we would add this image and configure it within an existing Panamax application. Let's use a simple Wordpress template as our example. Now from within the Application Detail page, click +Add a Service and search centurylink/ngrok: I added the ngrok service to the 'Web' category, but you can add it anywhere -- including creating a new category for it. Next we need to configure the environment variables for the ngrok service. Click the service name to go to the ngrok service details page. In the Environmental Variables section, add a new variable key HTTP_PORT with you localhost IP:port as the value. NOTE: The port value is the port number you setup in VirtualBox to be forwarded to reach the Wordpress GUI. Hit the Save All Changes button. This will trigger a rebuild of the app. As the App is rebuilding, watch the Activity log and make note of the public internet url is assigned: Now you can share that url, http://1af9484.ngrok.com in our example, to colleagues or anyone else you want to access your application via the Internet. To remove access, simply remove the ngrok service form your application. The application will rebuild and only be available from your localhost.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "deploying-docker-containers-in-centurylink-cloud", "author": ["\n              Bryan Friedman\n            "], "link": "https://www.ctl.io/developers/blog/post/deploying-docker-containers-in-centurylink-cloud", "abstract": "If you’ve been reading cloud-related news lately or you follow any developers or system admins on Twitter, then you’ve undoubtedly seen the words “container”, “ Docker ”, and “ CoreOS ” written a few thousand times over the past year or so. Chatter has particularly picked up in the last few months with Docker 1.0 being released in June and CoreOS announcing their first stable release within the past few weeks. CoreOS also received an 8 million dollar investment just a couple of months ago, and Docker just got another $40 million in funding a few days ago. And just yesterday, CenturyLink joined the container party and announced the release of the open-source Docker management platform, Panamax . Panamax was described by RedMonk principal analyst James Governor as “Docker management for humans. It dramatically simplifies multi-container app deployment.” This is bleeding edge technology we’re talking about here, so if you haven’t heard about any of it yet, there’s no time like the present. Docker is one of the fastest-growing open-source projects ever, with more than 550 contributors and 7 million downloads in just over a year since its release. The power of Docker lies in its ability to build and deploy applications in containers, which are extremely efficient and more portable than traditional virtual machines. This is because they abstract only the operating system kernel rather than an entire device. But if you’re like me, you’ll never be satisfied just reading about anything – you want to try it already! If so, I’ve got good news for you. Whether you’re looking to just get your feet wet and experiment with containers or you’re feeling ready to jump right into the deep-end and start deploying applications with them, CenturyLink Cloud has got you covered . There are at least three ways you can get Docker up and running on CenturyLink Cloud right now : install Docker on a CentOS server, provision a CoreOS server running Docker, or take advantage of Panamax and make it even easier to use Docker. Whichever route you choose, just activate a CenturyLink Cloud account . You might not be too familiar with CoreOS, so if you want to get started using Docker on a more familiar Linux distribution, you can easily use our Docker blueprint to install it on any CentOS server running on CenturyLink Cloud. You’ll even get the option to deploy a Hello World container so you can see a simple example of how Docker containers work and get started building your own. Interested in CoreOS? This lightweight Linux distribution is optimized for massive server deployments and it comes with Docker preinstalled because it’s designed specifically to run applications as containers. You can follow our step-by-step instructions for using blueprints to build a CoreOS server cluster on CenturyLink Cloud and start deploying your applications on Docker in minutes. Maybe you like the idea of Docker and CoreOS, but you’re not a Linux expert and you’re a little afraid of getting too into the weeds on the command line. If so, CenturyLink Labs has developed just the answer for you: Panamax . Panamax is a single management platform for creating, sharing, and deploying Docker-containerized applications. By following similar steps to our CoreOS deployment above and selecting the “with Panamax” version of the blueprint, you can have a CoreOS server up and running with Panamax installed in no time, and there’s no easier way to get started with Docker. Not only can you use Panamax to deploy images from Docker’s repository, you can also deploy complex multi-container Dockerized apps from Panamax’s Open-Source Application Template Library. Think of these templates as collections of Docker images that work together to form the complete architecture of an application, with separate containers for the database vs. web tiers, for example. If you’re looking to deploy one of the available template options like WordPress or Drupal, you’ll have it working with a single click in seconds flat. However, you can also choose to define your own custom templates to use and even add custom repositories to search as the Panamax community grows. There’s no easier or faster way to start using Docker containers than with Panamax, and it’s built to leverage the power and scale of CoreOS. Have a server already? Install Docker! Curious about CoreOS? Provision it! Feeling overwhelmed? Try Panamax. With CenturyLink Cloud, you’ve got lots of ways to get started using Docker right now, so no more excuses! If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account and add containers to your repertoire of application deployment options today and start enjoying their power, performance, and portability.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "get-rid-of-virtual-machines-once-and-for-all", "author": ["\n              Mike Arnold\n            "], "link": "https://www.ctl.io/developers/blog/post/get-rid-of-virtual-machines-once-and-for-all", "abstract": "Docker is great for developing and deploying distributed applications, but many developers still don't understand how to effectively use containers yet. This confusion exists in part because Docker's usefulness during development isn't as a replacement for a virtual machine. During development, having containers running production-ready images within your development environment is invaluable. However, if you're not building your piece of the distributed application in-container, how can you be confident the entire system will work in production just as it does in development? When we set out to build Panamax , we knew we wanted it to be a distributed application with all the services running as micro-services inside containers. We also knew that we didn't want to rebuild images every time we changed code. This meant that we needed to be developing the various components of the system in containers running images that would later have as few additional image layers as possible. Ideally, we wanted to just add the code for the component to a base image and then rely on runtime configuration. Developing in-containers isn't all that straightforward. You want your images to be lightweight , so you can't load them up with packages that are needed during development. Additionally, for many people, Linux isn't their primary development environment, so dealing with containers is best done in a VM. In the case of Panamax, we were reliant upon CoreOS as the Docker host VM and it's not really a development system. We had the additional requirements of needing to access the Docker Remote API without exposing the Remote API via TCP and to send jobs to Fleet, both from within a container. So how did we do it? Most of us in CenturyLink Labs use Macs as our primary development environment, so the first thing we needed to do was get code from the Mac into the VM. We had already chosen Vagrant and VirtualBox for running CoreOS, so it simply required setting up a synced folder in the Vagrantfile already used to run the VM. In this way, we can point the tools we are familiar with at the local source directory while relying on NFS to seamlessly keep the code we change locally up-to-date inside CoreOS. Vagrant also provides other options for synced folders if you don't want to use NFS. The next step was to get the code on the CoreOS VM file system into a Docker container. You might think the simplest way to do this is to add a Dockerfile to the source code root (i.e. the directory being synced) with the image build instructions, but this just lets you build an image with the source code 'frozen' to the version in place when the image is built. Every time you change the code, you have to build the image again. This is probably where most developers get discouraged and decide to ignore containers until it's time to deploy. We realized we could create a base image that had everything needed to run our application except the source code. Then we could run the image in the host VM in interactive mode and bind mount the source code directory as a volume, thereby injecting the code into the container and keeping it up to date as the code changed on the file system. Given the synced folder setup in the previous code snippet, this looks something like: The -v flag sets up the bind mount, and the -it flags together run the container interactively. Passing the /bin/bash command brings up the bash shell in the container in a terminal, and we can navigate to /var/app/panamax-api and run the Rails application that is the Panamax API . These services aren't useful if we can't interact with them, so in the same way we use a combination of synced folders and bind mounts to get code into the container, we use a series of port forwards to get messages into and out of an application. When running the CoreOS Vagrant box, we can forward a port from the VM to the local development environment by adding another line to the Vagrantfile: This makes anything exposed on port 8888 of the guest VM accessible through port 8888 on the host development environment. Subsequently, we can forward any port on our container to port 8888 on the guest VM (now acting as the host to the container) with the Docker -p flag when running the container. So the docker run code snippet above would change to: In the case of Panamax, the Rails app will run on port 3000, so it is this port we map to port 8888 on the VM. Through this Inception-like configuration of local dev to VM to container with ports forwarded from the container to the VM to the local dev env, one can achieve a fairly nice development flow. You write and unit test code as you normally would using tools and the OS with which you are familiar. You might switch to a shell to start/stop/build the application from source, except now it's in the container's shell. And you validate the service interface locally via the ports that pass down into and up from the application running in-container. When everything's working as you expect, you make use of a Dockerfile to build a new image FROM the base image you've been using all along, ADD your code and execute a CMD to start/stop/build when the image is run. Lastly, you use Panamax to compose the entire distributed application and watch the magic happen.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "using-wetty-to-create-docker-containers-for-clis", "author": ["\n              Pat Cox\n            "], "link": "https://www.ctl.io/developers/blog/post/using-wetty-to-create-docker-containers-for-clis", "abstract": "What if your multi-container Docker application needs a CLI? You might think you are stuck with installing it directly on your CoreOS host. But, you can actually create a tunnel to allow your CLI to access your application from your laptop. By shipping your application with Wetty , a terminal emulator, not only can you have a simple way to interact with your application but you'll also be able to control versioning of the CLIs and make sure they match the services you and your template consumers have running in your application. Cue Wetty from Krishna Srinivas , a terminal in a web browser accessed over http/https. Wetty has significant improvements over other terminal emulators by using web sockets instead of ajax and takes advantage of ChromeOS' terminal emulator, hterm , giving you, well, a full featured terminal emulator. Krishna has also release a dockerized version of Wetty - krishnasrinivas/wetty . Now that Wetty is containerized, you can use it in Panamax and include it on a template! We at CenturyLink have created a Wetty image just for that purpose -- to install CLIs on a docker image that can be included in your template. Read on! For our example, we will use centurylink/wetty-cli . This particular image is based on Krishna's original image, but designed to be used as the base image for adding your CLI or headless app. This image is not intended for use on its own, but via the Docker FROM command within your own Dockerfile. Before we start stitching together our application, let's better understand all the factors at play. Here is the Wetty image Dockerfile: Note that the username and password for Wetty are set as term and the exposed port is 3000 . As a simple working example, let's use OpenStack and its bevy of CLI tools. OpenStack has several CLIs, one for every service, and in this example you'll create a container with each CLI accessible and have the ability to run that container from Panamax. By referencing the centurylink/wetty-cli image with the Docker FROM command, all you need to do is add the prerequisites and packages for your CLI installation; everything else is taken care by the parent Wetty image. Here is the OpenStack Dockerfile: And that's it!!! Your Dockerfile is done. Now use the docker build command to create the image locally: Be sure to run this command within the same directory where your Dockerfile resides. You now have an image with all the OpenStack CLIs installed. Next, create your container from the image, subbing in your own values: Browse to http://localhost:8000 ( use VirtualBox port forwarding if on a Mac ), login with term/term and behold the terminal in all its glory: But wait, wait... there must be an easier way than typing all those environmental variables each time -- something repeatable, shareable, beautiful. Read on! Let's assume you have an image created and made available on the Docker Registry . We can now create a template in Panamax and use its GUI to add values we would typically manipulate during a docker run . To get the most out of your OpenStack CLI image, you need to provide environmental variables during the container creation. Without this, you would have to add them manually after the container has run in the Wetty terminal itself -- no fun! Panamax allows you to collect those values and save them to a template, ensuring your application will run with the same environmental variables each and every time. After searching for and launching your Docker OpenStack CLI image within Panamax, browse to the details of the service. Here you will be able to enter the values needed to run the container that you had to manually specify in your docker run command. This would be a good time to save your template to a GitHub repository so you can use it later and so that other Panamax users can kick the tires of your application. However, before saving remember that you wouldn't want to actually add the environmental variable values at this time (i.e. your OpenStack creds) as this would be saved to your template and could be exposed to the public. For authentication, it's better to run the template and then add those values in the GUI directly. Now that you have saved this as a template, you can reuse the template over and over again, providing your creds to access your OpenStack resources directly in the GUI. After those values are entered and you Save Changes , wait for the application to restart and then browse to http://localhost:8000 ( use VirtualBox port forwarding if on a Mac ). Login with term/term and BAM! This example focused on a standalone, single image CLI. The real power comes from adding a Wetty-based CLI image to compliment any application where a CLI is utilized. For example, you can add a Drush Wetty image to a Drupal application allowing you to include the CLI with the application that leverages it. Use the power of Panamax to setup linking between your containers and leverage the environmental variables available to you from Docker itself. You can then comfortably use the tools from your local machine's browser -- no fussing around with extra installs or configurations -- accessing the CLI directly in the environment where your application resides! Perhaps you have monitoring tools or utilities you would like to leverage directly in the container environment; Wetty gives you that ability. Furthermore, by creating a Wetty-based image and including it in your template, you can control the versioning around the CLI and the application, meaning no more issues where your CLI is out of sync with the rest of the app! You can find these Wetty-based images and templates available to you in Panamax right now.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-does-redhats-openshift-work-with-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/how-does-redhats-openshift-work-with-docker", "abstract": "RedHat and specifically the OpenShift team was an early adopter of Docker technology. OpenShift Online 3 (the hosted service) and OpenShift Origin (the open-source part of OpenShift) of was recently released. This week we talked to Krishnan Subramanian , Director of Strategy, OpenShift ( @krishnan ) on twitter and Clayton Coleman , Lead Engineer, OpenShift ( @smarterclayton ) on github) about why they believed in Docker so early and what makes RedHat a company philosophically aligned with the Docker project. The OpenShift team at Redhat was an early adopter of Docker technology. OpenShift Online 3 (the hosted service) and OpenShift Origin (the open-source part of OpenShift) was recently released. This week we talked to Krishnan Subramanian and Clayton Coleman about why they believed in Docker so early and why RedHat is philosophically aligned with the Docker project. We know of 30-40 people deploying OpenShift Origin right now but there are a lot of others that we never get to see or hear about directly. There are more than 1.8 million applications running in OpenShift Online right now. OpenShift Online is more like Heroku. It is meant for production use as well as dev/test. Cisco , FICO , and Boeing have all spoken publicly about using OpenShift in production environments. If it is not mature, why are these customers using it in production environments? Those claims are rhetorical. The goal of OpenShift is to bring people from the old world to the new world. People who may not know about Docker or PaaS yet. We don’t try to force people to change everything about what they do up front. Infrastructure scaling at the platform to meet the needs of applications. OpenShift is one of the very few platforms where real application scaling is available. Many other platforms are about infrastructure scaling whereas in OpenShift you can scale applications by also managing infrastructure scaling using the tools you already use. We used containers at based on SE Linux for a while at RedHat. Docker took the great step to separate the user space that matters to an application from the stuff that an operating system needs to run. It was obvious to me that this was a new way to solve the old problem with packages and install scripts and configuration management. Docker captures an easy-to-use way to distribute software. Open-source is in the DNA of RedHat. We are comfortable with working in any open-source project whether it be OpenStack or Docker. We can contribute and also make money. A command-line client and agent for integrating and linking Docker  containers into systemd across multiple hosts. Kubernetes is a Google project to run containerized applications at scale. Everything at Google runs inside of containers. Even their VMs run inside of containers. When we were thinking about how to address problems like scheduling in OpenShift, Kubernetes fit the philosophy we had with stateless and self-healing concepts. GearD is about operating orchestration end-points. The focus on PaaS is now on containers and the Docker ecosystem. People want to avoid lock-in not just for vendor-proofing but also to future-proof your applications. Easy portability is necessary to achieve that. Micro-services! Docker and PaaS enable micro-service architecture. OpenShift does a lot of legwork that manages a lot of the new concepts behind Docker for you. Integrate Kubernetes into OpenShift natively and help people adopt Docker easier. Quick Links", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "interview-with-sandstorms-founder-about-the-personal-cloud-platform", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/interview-with-sandstorms-founder-about-the-personal-cloud-platform", "abstract": "When I read Why Doesn't Sandstorm Just Run Docker Apps? , I instantly knew I wanted to pick Kenton Varda's brain. This week I had the honor to spend some time with Kenton. In this week's podcast interview, we are talking with Kenton about the recent launch of Sandstorm . We go into: I worked for Google for seven and a half years, ending at the end of last year. I worked on a few things there, I worked on search infrastructure for a while, helping to launch universal search. For a while I worked on sharing and access control for things like Google Docs, when you click on the share button on Google Docs the stuff you see there I wrote the infrastructure to enforce it. I'm probably best known for my work on Protocol Buffers, which is the data interchange format used by all of Google's servers to talk to each other, and for a lot of Google's storage. I didn't invent Protocol Buffers but I rewrote it and I open sourced it. Now there are probably thousands of companies using it, including companies like Twitter, and even Blizzard, gaming companies. For the past year and a half since I left Google I have been working on a couple of things, one is Cap'n Proto, a replacement for Protocol Buffers and RPC system. Sandstorm is built on top of that. Sandstorm is a radically easier way to run personal instances of web apps. It let's you have your own personal server and install apps to it entirely through a web interface like you are installing apps on a mobile phone. So there's no editing of config files or running commands at a command line. An end user, a non-technical user, can actually use Sandstorm and run apps on it. You run apps like email or document editors or blogging platforms, anything that involves personal data owned by a particular user makes sense on Sandstorm. Sandstorm is open source, so you can run it anywhere you want. We think most users probably want to use it in the cloud to get the benefits of being able to reach their server wherever they are regardless of the current state of their internet connection. You really wouldn't run it on a laptop, certainly, because a laptop is not running most of the time. You could, and many users do, run it on a machine on their home network for increased privacy. What we hope to see is a combination of some users doing that and a lot of users using cloud hosting services. We plan to offer a managed hosting service ourselves for something like $5 a month to help fun development. So its up to you, Either way you can install the software you want, there are no limits. I started working on Sandstorm at the beginning of this year [2014] so it hasn't been around very long. It sort of became usable a couple of months ago and we've been porting a lot of apps to it. Most of the apps that exist right now are things that we ported ourselves from open source apps. Since its based on containers we can take any Linux based tech stack and port it to Sandstorm. We have about 10-15 apps right now. There's things like Wordpress and Ghost, blogging apps, there's Etherpad and EtherCalc, document and spreadsheet editors. We ported Apache Wave, formerly known as Google Wave, you can now run that on Sandstorm. You can go try out the whole list if you go to demo.sandstorm.io, you can try installing any of these apps and see for yourself. Yes. It is much easier than setting up yourself because you just point and click. You click install and you have the app running. It takes like 5 seconds to spin up a container. Sandstorm uses the same Linux kernel features that Docker uses, but it turns out that there is not much reason for us to actually use Docker. The Linux kernel features involved are actually pretty simple to use. Its only a couple of dozen lines of C++ code that we use to set up our container. What Docker does is provide a configuration layer on top of that and enough features to create completely transparent containers in which you can host arbitrary existing Linux distros. That's not what Sandstorm is trying to do. Sandstorm is willing to say, OK, apps in our containers have to be tweaked a bit to understand our container environment. Like they have to store their data under /var because that's the only directory that is going to be writable in our containers. Or they have to understand that procfs isn't going to be available for them to use because we want to turn that off to reduce the attack surface of the Linux kernel. In any case, apps written for Sandstorm, since they do need to have a user interface, you can't just ssh into a Sandstorm app. The only way for a user to configure it would be through a user interface. So apps have to be specifically packaged for Sandstorm to make any sense on Sandstorm, and if that is the case we might as well say we are going to set up our containers in this one particular way and all apps have to conform to that. Right, Sandstorm is meant to run web apps for users. Unlike Docker, it has a user interface that is actually a web interface instead of a command line interface and is meant specifically for the kinds of apps that have private user data in them. We do not see Sandstorm as a competitor to Docker at all because it is a completely different use case that it is aiming at. Can you define what you mean by multi-tier? Can you have micro-services in a bunch of containers within Sandstorm that talk to each other, are the Wordpress and MySQL in the same container, can you tell us a bit more about the backend implementation? So, if you are using something like MySQL you actually want to run it in the same container as the application because you don't expect users to understand what MySQL is and why they need to start that up. Though we do encourage apps to use SQLite instead because when you have a small database and there is only one app talking to it there's not a lot of reason to have a whole MySQL instance for that. Apps will be able to talk to each other, this is a part we are still working on, but they talk to each other through Cap'n Proto RPC. The idea there is that we are introducing a user interface called \"powerbox\", the powerbox is a way to introduce apps to each other. What it does is one app says \"hey, I implement this Cap'n Proto interface here's an object reference implementing it\" and passes it off to the system. Another apps says \"hey I need an object implementing this particular Cap'n Proto interface\" and asks the system for that. The system then displays a picker to the user saying \"here are all the apps you can hook this up to, which one do you want to use?\" The system is privileged with the knowledge of all your apps without having to pass that on to the app making the request. When you choose one you are implicitly saying that you want that app to have permission to talk to the other app, so there's no need for a separate permission question. The requesting app just gets that object reference and can start making requests to it. So that's how you can hook up multiple containers to talk to each other. So you can imagine an environment where you do a more traditional distributed application that is going to do some big data crunching by creating a bunch of instances and hooking them up, but that's really not what Sandstorm is going for, its more about user-facing apps. Right now the code we have runs on a single machine and each container gets a private slice of the filesystem. If you want to make backups there is a button in the user interface that just downloads a zip of the app storage and you can re-upload that later. We will develop a more sophisticated backup system for when you want to run Sandstorm on a cluster of machines. It will not only written to the local filesystem, but will also have written to other machines before the HTTP request returns so you know you are not going to lose data if one particular machine dies. I'm going to use the money to feed myself! So, software development is expensive, we do have to pay developers, so that's a big part of what we will use the money for. We will probably have to raise money from other places as well, that's the idea. If you pay into our crowd-funding campaign now you are basically pre-paying for service. If you choose one of these service perks you are pre-paying for managed Sandstorm hosting which will become available early next year. We also have perks for people who want to self-host, things like t-shirts and stickers and such, as well. We have four people, there's me and two other developers who have been working on porting a lot of the apps and adding some new features, and there is my co-founder Jade who is developer relations. It depends on what you are trying to do with them. One misconception I think a lot of people have is about security of Linux containers. There's kind of a conflict between security and transparency. You want to have a fully transparent system that appears to the guest app as if it a complete Linux system running on bare metal or a VM. You have to enable a lot of kernel features which gives the container a large attack surface. There are Linux kernel vulnerabilities found on a fairly regular basis that would allow an app inside that container to break out. Now for a lot of people that doesn't really matter, if you are running apps that you, more or less, trust are not malicious then Docker is pretty awesome. You can run what feels like VMs a lot more efficiently than VMs. You do have to keep in mind that Linux containers with all the kernel features enabled are not a good sandbox right now. That is why in Sandstorm we actually disable a lot of the Linux kernel features. We don't mount procfs, we don't mount sysfs, we use setconf to disable a lot of system calls that apps really have no business using. In that way we get closer to something that is not going to be broken quite as often. Yes, because it has a smaller attack surface it is a lot less likely that an app is going to be able to find an exploit and break out. Sandstorm is a sandbox in the cloud, is one way to look at it. Another way to look at it is that the app instances on Sandstorm are meant to be very \"fine-grained\", you can imagine a bunch of grains of sand flying around, we actually call our app instances \"grains\". It's interesting, actually, our containerization is designed to allow super fine-grained instances so not only does every user have their own instance of an app but every document can be a separate instance for document oriented apps. There's a couple of reasons why that is a good idea; one is that Sandstorm itself can implement sharing on those so you don't have to have every app have its own sharing model, another is that if you have one document that is public and one document that is super secret that you don't want anyone to know about you don't have to worry that someone that has access to the public document might be able to hack through using a bug in the app to get access to the private document. Sandstorm is enforcing that security for you. To make the containers that fine-grained we have to do things like make sure that all of the app's code and libraries are shared read-only among all instances and aggressively shut down instances that aren't in use. If you have a per-user instance of an app they are not going to be using it 99% of the time so it shouldn't be running and using up RAM at that time. We think that corporations will have a lot of interest in this because there are a lot of corporations that can't really use SaaS offerings today because either some of them feel Google is a competitor and so they don't want to use Google Apps, some of them can't due to regulations. In the United States we have things like HIPAA, in other countries things are even stricter, in Germany its really hard to put data on another companies servers because of the regulations there. There are a lot of companies that really want to keep their data in-house on their own machines and have a lot of trouble maintaining that, its a lot of work. So we think that Sandstorm will make a lot of sense for them and we plan to eventually build an enterprise offering optimized for those environments. Well, I think Sandstorm is exciting! To be honest, I obviously know all about Docker and Docker is pretty cool for the use case it targets. I haven't kept up enough on the other container projects out there. It looks like we're going to be funded so next up we have to build our hosting service which we've been selling in the perks. So that means making Sandstorm able to use multiple machines and treat that as a single instance so we can actually scale up to however many users we get. Implementing things like quotas, so that one user can't consume all our resources, but also adding features like the ability for apps to export API's is a request we are getting a lot because that will not only enable apps to talk to apps outside of Sandstorm but also client-side apps. For instance, if you want a mobile client that talks to your server there needs to be an API for it to talk to. Currently, the only way you can access an app is through a browser and its not really possible to export an API through that path. That's the thing about Sandstorm apps since they are intended to be sandboxed and intended to prevent leakage of data we do not allow an app to arbitrarily talk to the rest of the world. It does not get arbitrary network access, it has to go through Sandstorm's features for communicating, for exposing APIs, or sending emails for instance. So we have to expand those features and make it easier to add new protocols to Sandstorm. Which we will be doing through a notion of \"cloud device drivers\" which are really more protocols that we support. I think containers make much more sense than virtual machines for most use cases. Obviously, they are much cheaper, why should you be running a whole separate kernel for every application, that's a lot of waste and all the emulation layer and everything you need there. There are the security issues but over time that will improve, we will find and fix the security holes and we will find ways to lock down containers better. Long term, I would say that VMs won't make a lot of sense in a few years just because of the huge waste of resources to do what you could do with containers. In addition to the hosting perks there are a number of things like stickers, t-shirts, you can get App Store credit. Our App Store is going to support a \"pay what you want\" model for open source apps which we hope will encourage more open source web app development. So you'll get credit that you can actually give to these open source apps in the future. There were seats on the \"app committee\" which is going to direct our porting efforts but they've been sold out, unless you want to splurge for a Key Individual Sponsorship level at which point we will actually put your picture on our team page. And you'll get a app committee seat and a bunch of other goodies. There are also a few LAN Party invites left. You may have heard of my epic LAN Party house, its been on the internet a few times. Its all on the campaign page, just look at the perk list. Sandstorm.io. We are @sandstormio on Twitter, we are also on Facebook and Google+. If you go to Sandstorm.io there are all the buttons for all those.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "killing-your-outlook-with-docker-and-davmail", "author": ["\n              Chris Weyl\n            "], "link": "https://www.ctl.io/developers/blog/post/killing-your-outlook-with-docker-and-davmail", "abstract": "Love it or hate it, sometimes we have to use an Exchange server to communicate. This may pose a problem for you, if you prefer a non-Microsoft mail client: if the compatibility features are enabled, you'll be able to access your mail via IMAP and send over SMTP, as the Internet intended. If not... well... that's where tools like davmail come in. davmail is a Java application that knows how to translate between standards-compliant clients (like, say, Thunderbird ) and an Exchange server that speaks only Exchange. It's a great tool -- and one of the only solutions. It's also standalone, can be used statelessly, and -- with apologies -- is Java, making it a fantastic candidate for running inside a Docker container. Install Docker on a CenturyLink Cloud instance, provision a stateless container image running a davmail instance, and have the system manage the container via upstart . Provisioning a CenturyLink Cloud instance is left as an exercise for the reader. For simplicity, we're using the docker.io package from Ubuntu and configuring by hand. One may find a more satisfying approach in using tools like puppet-git-receiver and Gareth Rushgrove's most excellent docker Puppet Forge module to manage the use of the upstream Docker packages as well as our container's upstart configuration -- both of which will be covered in a future tutorial. Security is of utmost concern, particularly in a corporate environment, so please note that securing the connections between your local workstation and the cloud instance. Common -- and well-established -- solutions include the use of ssh port forwarding or (for a less ad-hoc approach) stunnel . Even if your mail is boring beyond belief, please, do not access it unencrypted. It just makes the rest of us look bad. For the purposes of this article, we're going to assume that you're an enligtened sort, and are running a Ubuntu 14.04-based cloud instance (or workstation, or...). If not, keep on reading. You can fire up basic Ubuntu machine fairly easily. Install and configure on your system: Our previous tutorial on containing Chef with Vagrant may provide some guidelines, but, as always, this is left as an exercise for the reader. Ok, ready? Go! We'll use Docker as packaged by Ubuntu: [gist id=c31941bbb5a4a5f449b3 file=\"docker.io-install.log\" /] On line 33 we see a docker group being created. It's worth mentioning that adding your userid to this group will allow you to interface with the docker service without needing to \"sudo\" it all the time; this is left as an exercise for the reader. There are a couple davmail images available on the Docker Hub . We're going to use the rsrchboy/davmail-savvis-docker image -- for obvious reasons -- as it gives us a configured davmail container that: You may need to edit the davmail configuration to reflect the specific needs of your Exchange environment. If so, you can use this image as a starting point, ADD the changed configuration to the image, and rebuild. Here's the Dockerfile for this image: [gist id=c31941bbb5a4a5f449b3 file=\"Dockerfile\" /] ...and for kicks, let's try it: [gist id=c31941bbb5a4a5f449b3 file=\"docker-run-trial.log\" /] Tada! :) upstart is an approach to unix daemon management. It is available by default on recent Ubuntu systems, and is very convienient to our purposes. [gist id=c31941bbb5a4a5f449b3 file=\"docker-corporate-davmail.conf\" /] Note how our upstart config does a couple different things here: Not surprisingly, the heart is centered around the docker run command that we can see at the core of the upstart config. [gist id=c31941bbb5a4a5f449b3 file=\"docker-run-command.sh\" /] Our Docker port-forwards are also established; from the \"trial run\" log above we can see that the ports davmail will listen on are: Remember nix systems disallow non-privileged process from binding to ports < 1024, and as such ours have been remapped. We additionally tell docker to bind ours such that our cloud instance: That is, when you go looking for IMAP, you'll find it accessible at 127.0.0.1:11143 from the cloud instance only; this prevents attackers from being able to connect to your davmail instance remotely. Now that we have an upstart config file, all that remains is to install the file appropriately (that is, copy it into /etc/init/ and start the service: [gist id=c31941bbb5a4a5f449b3 file=\"upstart-config.log\" /] This container is stateless -- that is, it only handles translating between standards-compliant and Exchange-compliant tools. You can start, stop, kill, abuse, disabuse, etc, the container davmail is running inside without fear of anything more than disrupting communications between your clients and the server. Complexity avoided is hours of time we get to keep. And with that, you have davmail running inside a container, with the appropriate port-mappings configured. You can now use this to interface the mail client(s) of your choice with an Exchange server. This author recommends pine ... But that might just be him :)", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-filesystem-persistence-with-flocker", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-filesystem-persistence-with-flocker", "abstract": "How do you do databases with Docker? Or for that matter, how do you do anything that stores persistent data in Docker? Sure you can attach a volume container to your application container, but that doesn’t scale and it makes your container dependent on your host, which is not ideal if you have a multi-server Docker cluster. Flocker is one of the few projects that is trying to tackle this problem (along with Flynn ). They are using cool technology like ZFS to snapshot your underlying container filesystem and make it transferable to any Docker host in your control. This week, we talk to Flocker’s founder, Luke Marsden, about his company ClusterHQ who is positioning themselves to solve this big problem. Luke Marsden CTO and Founder of ClusterHQ, Flocker creator Luke has 12 years of experience building web apps & running a web hosting company. Inspired by the practical operational problems faced running web apps at scale, he started ClusterHQ. Luke holds a Computer Science degree from Oxford University. Solving the data problem for Docker at https://clusterhq.com ClusterHQ is “The Data People for Docker”. They believe that support for portable and resilient data volumes is a major missing piece of the puzzle for Docker. Without support for data migration, cloning and failover, Docker is unable to easily capture entire applications (including databases, queues, and key-value stores) in production in a way that ops can manage. ClusterHQ as a company is 100% focussed on building Flocker, which is an open source project. The mission for Flocker is to solve the data problem for Docker. The big problem at the moment is that if you have data services within containers is that as soon as you put a data service (anything that has state) inside a Docker container, best practices dictate that you attach a volume from outside the container to inside the container. The database will be continuously writing to that volume, which means that container get stuck on that physical host. Building on technology from previous projects, and leveraging ZFS on Linux, each Docker container that needs to have state is backed by its own ZFS filesystem. ZFS is a beautiful storage analog for containers because ZFS filesystems arte light-weight, portable filesystems which you need for your light-weight, portable containers. Think of Flocker as a \"container sandwich\" that is made of containers in the middle, a network proxy on top, and a storage layer underneath. You can have multiple sandwiches for multiple hosts and Flocker can control any request for any container and also control the Flocker volumes that are attached. This allows container movement by allowing the network and storage layers to be synchronized in conjunction in real time, and switch the network proxy to the new location. This architectural choice removes the need for a SAN in the system. With today's modern cloud systems it is even more important since you don't get a SAN, which would be a single point of failure in any case. Cloud application design is predicated on that fact that \"things will fail\" so having a SPOF is a terrible thing. You need to remember that Flocker 0.1, at the moment, is a command line tool that reaches in and interacts with some servers to do static configuration. Currently the movement of containers is done in response to a manual request. Utilizing \"application manifests\" and \"deployment manifests\" Flocker 0.1 allows the deployment manifest for an application to be changed, Flocker re-run, and the container and data moved to the new target location based on the information in the updated deployment manifest. While this is manual (with some latent downtime) the project is moving toward a seamless, atomic migration. With HybridCluster (the former name for ClusterHQ) there was a large feature set; including built-in HA (with local or remote DC fail-over) and auto-scaling (or \"auto-juggling\" as they called it) that implements the notion that stateful things scale vertically while stateless things scale horizontally. See Data-focused Docker Clustering on the CusterHQ blog for more details. HybridCluster was based on FreeBSD and so used FreeBSD jails. Absolutely. They noticed a huge opportunity to solve some of the big problems that the Docker community haven't quite figured out yet. ClusterHQ is becoming more engaged with the Docker community and try to help out to solve some of those problems. To that end, ClusterHQ is not providing yet another orchestration layer but rather is targeting integration with the other orchestration projects in existence. They want to integrate with CoreOS, Kubernetes, Mesosphere, etc. Docker did a great job in getting to 1.0 and it has clearly won the hearts and minds of developers in a big way, opening up a huge set of opportunities around containers. However, customers want/need to be able to deploy production applications to more than one machine, for performance reasons, for scaling, and for resilience. This drives two big themes when moving from a single host deployment to a multiple host deployment; storage and networking. This is where the Docker community has a lot to figure out still. We are just at the beginning of that road. It's great to see the Flynn guys tackling this head-on. The difference in approach is that ClusterHQ is taking a generic approach to being able to replicate arbitrary filesystem state (utilizing ZFS) while the Flynn guys are building on top of the built-in replication methods that exist in some, but not all, of the data services. There is room for both approaches. Yes, it is. The reason is that handling state for containers you are exposing a POSIX filesystem rather than a block device as is typical for VM's. You could allocate and attach a block device for each container, but think of a large VM with thousands of containers, each requiring it's own block device. Current systems (such as EC2) are not designed to work this way. Weave just launched, and is a really interesting project by the founders of RabbitMQ. They are building, effectively, a layer-2 Ethernet switch into a cluster of Docker containers. This allows on-prem and cloud containers to join together and feel like they are on the same network segment. Combined with Flocker, this could allow live migration of stateful applications between on-prem and cloud services. Also, excited to see the start of \"agreement\" around the concept of orchestration. People are starting to think about Kubernetes serious, which will be a big part of our future. It's interesting to see how containers are being used for the installation of things (opposed to Chef, Puppet, etc.) and also how they are encroaching on the traditional VM functionality. That provides a very powerful combination; providing a consistent way to deploy stuff is a big win and containers will start to encroach into both of those markets. This is an important question for any open source company. You need to walk a fine line between building an open source technology that is really powerful, really production-ready while at the same time being able to draw a hard line of differentiation. This differentiation is what allows companies to pay for the software as opposed to just running the free version. We are still trying to figure out what to open source and what makes sense as a proprietary add-on in the future. Over the next 12 months we are solely focused on building the Flocker open source community. Go to www.clusterhq.com and check out the docs section. Also, the source project is on GitHub under an Apache 2 license, the Google Groups is flocker-users , and on IRC go to freenode and check out #clusterhq.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-docker-changes-the-way-people-learn-technology-with-oreilly-cto-andrew-odewahn", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/how-docker-changes-the-way-people-learn-technology-with-oreilly-cto-andrew-odewahn", "abstract": "You probably own at least a few O’Reilly books. You know, the ones with the animals on the cover? O’Reilly Media is well known to put out the highest quality technical books on the market. Does Docker not only change DevOps, but could it change the way we fundamentally learn technology? What does the future of technical books look like? With the rise of eBooks, videos, interactive online learning, and guided courses, online education is changing rapidly and dramatically. Andrew Odewahn , CTO at O'Reilly Media As CTO of O'Reilly Media, Andrew helps define and create new products, services, and business models that will help O'Reilly continue to make the transition to an increasingly digital future. He's written two books on database development, has experience as a software developer and consultant in a number of industries, including manufacturing, pharmaceuticals, and publishing. The thing that has not changed in the need for people to learn. This need is, and continues to be huge. The big change is the increased competition for people's attention, for learning. Khan Academy is just an amazing resource for learning all sorts of stuff. Code School, Codecademy… the list goes on and on. Blogs have information on anything you want to learn, obviously Stack Overflow is an amazing resource where pretty much any kind of question you have is right at the tips of your fingers. There still is a need for book publishers to act as a guide to a big area. For example, if you look at something like Docker there is a lot of great resources, but having a way to walk-in and have guided experience is where publishers and books add a lot of value. I think it is up. Just look at the mass quantities of words you consume on a daily basis... It's just not on a printed page, necessarily, any more. There is really a good role for text, it is a really good medium for transmitting information directly. Video is awesome for some things and text is great for other things. The distinction between \"is this a book?\" or \"is this a video?\"... those lines are going to really blur. This is where the next exciting innovation phase is for publishing and for information on the web, in general. They definitely have plusses and minuses. When the iPad and Kindle first came out there was huge interest, a huge surge in simply porting things over straight from the printed book into the new formats. The led to great portability, to great search, there's a great experience you can have from being able to carry a thousand books in your pocket. In areas where people want richer experiences the eBook doesn't really carry over that well. There is a lot further we can go if you think of the \"book\" in a digital-first manner. That’s what we are trying to figure out. Restricting the answer to the types of books from O'Reilly (technical, etc.) in providing learning of complex concepts and systems; being more interactive, more responsive to what you do as a learner, such as testing to see if past context has been \"learned\" or internalized. You have to start with the fact that the package management system is really broken. As a newcomer to a system you are having to learn its particular build system, its package manager, and all these other unfamiliar things. How do you get started, installed, setup and running? There's such a hurdle before you've even decided whether you are interested in the new technology or not, to get to kick the tires. This is one of the first real places, if you are looking at Docker, where the experience is seamless and great. The consistency of Docker is one of its big strengths. As an author, I hated the chapter one where you explained how to install the system. It was the worst, and then you become lifelong tech support for people that are still on Windows 95, or whatever. this is another big strength that Docker can bring to the learning environment. First, you could think of Docker in augmenting the content, in a companion to the book. Being able to create and image that people could \"Docker pull\" and have a consistent view of all the technology, dependencies, and versions would be a great experience. This would allow the author to know what the reader's experience will be from the start. We experimented a lot with virtual machines and the experience as a reader of creating a virtual machine and pulling it down is a little bit alien. Having a Dockerfile and just getting up and going is so much better than trying to create Chef recipes or Ansible or other sorts of things. Virtual machines are great for specific stacks, but for a publisher like O'Reilly where we have a wide range of authors, with a wide range of experience it is difficult to deal with so many different approaches. Docker gives us the ability to be very consistent about what the file format is, that it is going to work in multiple things, and it is a pretty simple process for people to grasp. One of the most interesting things I am most excited about is thinking about the social nature of learning. Not necessarily \"getting badges\" for completing tasks, but rather being able to connect to other people, to form groups of like-minded people, and build into the content mechanisms to facilitate that. Sure, the ability to chat on each page of the \"book\", so you could chat with people that are on the same page as you are. Another aspect is the ability for teachers to connect with students in a variety of ways is an important aspect of learning and education. There are a number of people that are doing some really interesting work. Self-publishing is interesting. There are a lot of topics that, because of the economics, O'Reilly would like to cover but can't. Self-publishing lets these sorts of topics be covered in an in-depth manner, it creates a marketplace, but its not something that a publisher at the scale of O'Reilly would necessarily do. I think its awesome that there are people out there creating content and publishing it and reaching an audience. Its not a threat at all and is a great way to expand the market, expand the pie for knowledge and for people to learn a lot of different things. Atlas is an authoring environment that reflects the philosophy that O'Reilly has had about publishing for over 25 years. This philosophy has 3 parts: For a long time, the system was a little bit of a pain. The idea behind Atlas was to see if they could create a beautiful experience, an easy to use experience, that embodies the 3 part philosophy. You start with a git repository and use the web interface (looks like working with MS Word). Many authors just clone the repo and write locally, using whatever editor they are comfortable with. When they are ready they will commit and push the repo back to O'Reilly. There is a build process that takes the uploaded content and puts it through the same process that O'Reilly uses to send content to the printer. You can control this through the web interface or there is an API and control through the command line. There is also built-in support for some of the difficult areas of a publishing tool-chain, such as rendering math or styling things easily. There is also an option for building a website, this is one of the things I am excited about. You can create your book, create a PDF, but also port it out to the web where you can add videos, have a branch that has interactive and live things. This is where O'Reilly is creating the platform to explore new ways to interact with new kinds of content. It is shockingly easy and great. You just check a Dockerfile into your project and instead of creating creating an ePub, PDF, or mobi the worker could take the Dockerfile, install all your dependencies and create the environment. Instead of thinking of your book as something other things go into, think of the place your book goes out to. As an author, you just hand in your Dockerfile with your requirements and Atlas will handle the rest. I used Atlas to generate the PDF, but Did you use Atlas to generate the Docker Jumpstart webpage is built using Harp, a static website generator. With Harp it is easy to stand up a site and style it... I was able to push that repository into Atlas and then drag my Markdown content files in and hit \"build\" and turn it into the PDF. We tried to build a tool that not only plays nicely with PDF and print, but with other existing systems. We want a tool that plays nicely with the rest of the web, rather than fighting it. More software oriented, we need to think digital first. We have to think \"what does the reader need?\" and \"what does the learner need?\". Then we have to think \"how can we package the knowledge that person has in a really compelling way so that it can teach most effectively?\". We have a vibrant conferences business and are seeing demand for smaller, more local events, and how can O'Reilly support this trend and how can content be created that is really good in that environment. We will be pushing the boundaries of how do we bring more people in, how do we scale learning, how do we scale the ability for people to self-organize into groups around these learning contexts that we create and that we make it easy for authors to build.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-does-apache-mesos-work", "author": ["\n              Chris Weyl\n            "], "link": "https://www.ctl.io/developers/blog/post/how-does-apache-mesos-work", "abstract": "Managing multiple Dockers hosts is hard. Docker orchestration systems are built to address that problem. There are multiple options for Docker orchestration. Last week we talked about Google's Kubernetes , and this week we are going to discuss a cluster resource management solution that has recently become an Apache \"Top Level Project\" : Mesos . A project originally born from scaling Twitter, Apache Mesos is a datacenter-level project that aims to provide high-level, finely-grained, and collaborative locality-aware resource management to various computing frameworks. Mesos operates as a cluster manager, with a master controller that communicates with agents residing on each node (e.g. host machine) and \"framework schedulers\". Nodes run with a Mesos agent on them, which in turn is responsible for managing the local resources, any \"framework executors\" running on the nodes and coordinating with the master. \"Framework schedulers\" are responsible for telling the Mesos master what jobs they have ready to run and what resources those jobs require; \"framework executors\" in turn are handed jobs that the cluster is ready to have run on the right nodes with the right access to resources. Data locality is -- simplistically -- the concept of how \"expensive\" the data or resource you need is for you to access. Generally speaking, the cost here is latency or time needed to access a resource, but could be, say, machine or network use charges. A fundamental example of locality and cost can me found inside every IPv4/IPv6 attached computer: the routing table details how to send packets from your machine to a target ip, and when there's more than one acceptable route for a packet to take the kernel chooses the route (generally network interface) to send those packets over based on which route has the lowest \"metric\", the key indicator of cost. (You'll appreciate this on a well configured-laptop that has a wireless and wired interface both off the same network!) An even more tl;dr version of this can be found on -- where else -- Wikipedia, at Locality of Reference . To impress on the importance of leveraging data locality, let's imagine a Mesos cluster that runs on 4 nodes, two of which have direct access on node-attached storage of, say, a 500TB dataset (dataset A), and all of which have direct access to a 100GB dataset (B); three frameworks run inside the cluster, all of which run jobs that use either dataset A or B, but never both at the same time. When the framework schedulers communicate with the Mesos master as to what jobs they have and what resources those jobs need, all other things being equal Mesos will offer slots on the nodes with direct access to dataset A to the jobs that need it. This locality of access can result in drastic speed increases, as the cost of accessing data locally -- e.g. internal disk, SAN/SAS/SCSI-attached storage, etc -- is often significantly less than accessing data remotely, often over a network and through another host. Lower cost to access data/resources translates into faster processing and more efficient utilization of cluster resources. We are interviewing Florian Leibert, Founder and CEO of Mesosphere (the commercial company behind the Mesos project) on the CenturyLink Labs Podcast . Check back soon to get his background and history on the project in more detail. We also soon publish a follow-up post on how to get started using Apache Mesos. Until then, check out these great links...", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "running-drupal-in-linux-containers-but-not-docker-on-pantheon", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/running-drupal-in-linux-containers-but-not-docker-on-pantheon", "abstract": "Pantheon is the professional website platform top developers, marketers, and IT pros use to build, launch, and run all their Drupal and WordPress sites. Pantheon powers 70,000+ sites, including Intel, Cisco, Arizona State University, and the United Nations. Recently Zack Rosen, Pantheon CEO, published a blog post entitled Why we built Pantheon with Containers instead of Virtual Machines . We were intrigued and wanted to learn more about Pantheon's Docker-based platform. David Strauss , Pantheon's CTO, met with me (virtually) so we could learn more about Pantheon. David Strauss , CTO at Pantheon After co-founding Four Kitchens, a successful web development shop, David found himself gravitating away from custom client work and toward infrastructure solutions. Large clients like Creative Commons, Internet Archive, The Economist, and Wikimedia had already benefited from his scalability and database optimization work. In addition to his role as Pantheon CTO, David also co-maintains the systemd/udev layer that runs on most of the world’s Linux systems, serves on the Advisory Board for the Drupal Association, contributes to the infrastructure and security teams at Drupal.org, and leads the development of Pressflow. This goes right to the primary drivers we had for designing the product. This included having a consistent experience and it is hard to implement this using VM’s. This is due to the fact that production may be a fleet of VM’s while a developer instance may be a single VM or a set of shared VM’s. Then you are going from local database access to network DB access, you’re going from local file system access to something like GlusterFS . These introduce major consistency issues. Even the tools used for local development might be different from production; locally you may not have access to memcached or SOLR. The way to overcome these limitations and provide a consistent experience at a reasonable price is to use containers. You can’t deploy a fleet of VM’s for every developer in every single environment along the way to deployment at a reasonable cost. There is huge overhead, and costs, associated with replicating databases servers, etc. With containers we can still spread the application over the network but only take a small slice of each system. You can have the application in a small container, have the database server in another small container, have SOLR in another small container and the overall footprint is small. In addition, the containers can be started and stopped on demand when they are actually accessed the footprint of memory and CPU is not even persistent. This allows the dev and test environments to be representative of the later stages of deployment in an economical way. This architecture also pays off in production where the on-demand nature allows for scaling without the costs of over provisioning. Since the overhead costs in resources is small the Pantheon grid system can contain the blips in usage from their customers with little effect on the overall edge traffic that they process. For one, our container approach pre-dates Docker by a number of years. We’ve been using containers since 2011 on Pantheon’s platform. The Linux kernel doesn’t actually have the concept of containers. There is a set of API’s that, used in unison, gives you something that looks like a container. It’s not like Zones on Solaris or Jails in BSD, where there is a monolithic thing that you configure that creates that isolation. On Linux there are mandatory access controls, there are capability filters, there are cgroups, there are namespaces, and probably one or two other things other than standard users and groups. If you configure enough to isolate a process you get something that looks like a container. It’s called a container on Linux because, for all practical purposes, they are the same as containers on other platforms that have the concept at the kernel level. What Docker does is configure all these resources for you. We use systemd to do the same thing. Docker also provides a packaging and deployment model and infrastructure for publishing containers for other people to be able to install them. Unfortunately, some of the design goals for Docker have not aligned with some of our needs. We already have technology for configuring what is in the container, we use Chef to set the context of the container itself rather than the base system. Also, our density needs are considerably more advanced than what Docker provides right now. Our technology that allows containers to be activated on usage is done by a technology called “socket activation”. systemd sits on the base system, it opens a socket (the “listener”), it puts an “epoll listener on there that allows it to identify that a connection is coming in but not actually process it. Knowing that a request is coming in systemd can activate a service or container on the system. This also allows containers to be hibernated, by identifying processes that have not been accessed or processed a request in a while and then being “reaped”. This allows these processes to not consume any resources, when a new request comes in it takes a few seconds to restart the container. We deeply thousands of containers to any particular box, but only about 5-10% are running any any particular point. All of the stuff we do for socket activation is entirely open source and built into the systemd project. This is shipping widely now, back when we started on systemd it was only on Fedora, now it is on SuSE, it’ll be on the next Ubuntu release, it’s on the latest Red Hat Enterprise 7. So access to this architecture is available to everyone except Ubuntu LTS. It’s in systemd. You basically separately configure a listener as a socket unit and then, in the service itself, you can have it start a container with something called systemd-nspawn . It also integrates with RedHat’s libvirt infrastructure to do activation of LXC containers. At that point it is a matter of configuration, I have published articles on the systemd wiki about this. You can actually trick a lot of daemons into thinking they are reloading their configuration and they will accept an inherited socket. For example, the way that nginx or php-fpm handle their safe reload without interrupting their request availability is by forking off a new child and then handing off the current listeners to the new child. Once the child appears to be fully operational the parent just dies off. They are kind of socket activating their child, and that chain can continue indefinitely. This can go on indefinitely with no performance impact because there is no proxy involved, nothing is actually copying the bytes from one to another. When you hand off a socket this way it is completely, natively in the kernel. There is no performance degradation versus the application having created the listener itself. The systemd-nspawn project has full support for mandatory access controls with SELinux, the reason this is important for security on containers is there are root escalation vulnerabilities all the time. People figure out if you call this kernel call with this data you can escalate your current user to root. With SELinux access controls are paired with the concept of operating with a “Label”, which contains the process from only consuming the resources allocated to that label. This means to attack within SELinux systems you not only have to find a way to become root, you also have to manipulate the SELinux state machine or find a way to alter your label. Labels are great for containers, you just label everything in the container to a value and you launch with permissions to only be able to access through that label. Even if the stuff in the container got root it wouldn’t have permissions to read or write to anything on the base system, except those thing you have granted it access to. We are running a couple of services we are running more centrally in Docker containers because it is a convenient way to package applications that are always running. We haven’t looked at using it directly for our user’s containers yet, because of that lack of on-demand launching. So far, the Docker upstream project has not been receptive to accepting the type of changes necessary to support on-demand launching. However, it looks like RedHat is starting to maintain a patch set for Docker for the version it is going to be shipping on Fedora and RHEL, then of course CentOS and derivative ones too. This patch set will support on-demand launching. Red Hat has some use cases where this really fits, such as the OpenShift project. OpenShift is moving to Docker as its foundation, but they don’t run Docker directly. They run Docker with Geard , which is more like CoreOS. The difference is instead of launching a container that dockerd launches and manages, it creates a systemd service and then it runs Docker within that systemd service. If this is in place you can patch Docker to hand off the socket into the container and support socket activation. OpenShift has a need to be able to launch containers on-demand. Currently, they do this at the application level by utilizing an HTTP proxy. This requires the proxy and there isn’t a clean way to hold the request until the container launches. At Pantheon we are hoping they see socket activation as the future model for how you say a container needs to listen on something and then have it start when a request comes in, so you can have an on-demand model that doesn’t require a lot of complexity. We looked at Cloud Foundry a couple of years ago, but haven’t taken a fresh look at it. So I can’t really comment on Cloud Foundry, although they have had some uptake. I have more connection with Red Hat and the Fedora community and so am more familiar with OpenShift. We are probably not going to use OpenShift directly because it is pretty heavy-weight and requires running a lot of components to get the infrastructure set up. We are much more interested in something lighter-weight for container orchestration like CoreOS. We want to provision, hibernate, migrate, etc. containers without having the overhead of a GUI, authentication layer and manager, and the other infrastructure that goes hand-in-hand with something like OpenShift. That being said, OpenShift will be much more interesting once they have the Docker and on-demand launching abilities. We haven’t and the main issue for us to run our product is that these systems are based on a billing model where you are paying for the container, and you are paying for the container to be persistent and run indefinitely until you stop paying. Our model is based on several kinds of compounded efficiency. One is that we are moving more to running containers on bare-metal which is massively cheaper than occupying an entire VM in the cloud. This is due to the fact that we have very predictable provisioning needs in terms of our customer growth path. We can know 30 days in advance what our container growth is going to be. Systems like Rackspace OnMetal are providing API provisioning access to bare-metal where there is no downside as long as you have a way to slice up the servers. We get our compute cheap, because we eat all of it by not using VM’s as our isolation layer. We get tons of density by using the on-demand container stuff. We are twenty-fold more dense than running all the containers all the time. We also get massive underlying efficiencies by how we do our packaging and distribution of binaries. We have the binaries and libraries on the base OS, have the containers use those, and take advantage of the fact that Linux will map the running binaries to a single image. This provides gigabytes of RAM saving by not having every container instance run its own set of binaries and libraries. Primarily on virtual machines. We had a project called “Pod One” that was a rack of bare-metal hardware that was provisioned in a traditional way, and it was a pain to manage. We really like API-based provisioning even though we have pretty predictable needs. Rackspace OnMetal is changing that, we are looking for OnMetal to come to some of the data centers we use. We were launch partners with Rackspace and participated in the design of the system, where Rackspace came to us and asked us what we needed, and what did we think about future needs for the infrastructure. I wouldn’t be surprised if we move almost entirely to this system with Rackspace data centers because it is 2-3 times cheaper than cloud infrastructure. It turns out that due to the size of the VM’s on the cloud we were essentially the only tenant on the box. You have to remember that with cloud infrastructure you are paying on-demand for your compute and other services, but there is an overhead that you are charged to allow the cloud provider to have the idle machines standing by. Certainly products like OnMetal where you provision a machine like you provision a VM, that will change a lot. It depends on what people are running. Many people wouldn’t know what to do with an entire machine, even if they could slice them up into containers. If you have an application that takes 4 his, and another application that takes 4 gigs, what do you do with a machine that has 128 gigs of RAM? Providers want to get the best match of machine to their datacenter profile; racks, power consumption, cooling requirements. This means they will have a “sweet spot” for the profile of their machines. Bare-metal machines that only have 4 Gig of RAM are not in the sweet spot or economical. VM’s have utility when the size of the machine you require is less than the sweet spot of the bare-metal server. VM’s will persist and be offered to satisfy this demand. Complementary to this issue are the super light-weight VM’s. These are VM’s with a minimal kernel and just the application. Over the past 5 years the emergence of high level virtual device drivers (instead of existing device emulation) has led to “thin VM’s” that only support these types of virtual devices. This allows for less complexity, they look the same no matter where you deploy them, and allows for clean aggregation. Since there is no user land required, the thin VM only has an OS plus the application and can be deployed in less than one hundred megabytes for the OS. The OS can be very much stripped down due to the fact that it is specialized for running a single application. It doesn’t give all the efficiency we get with containers, but it gives better isolation for security and performance due to the fact that it gets things like it’s page table protected by hardware, the hypervisor is protected by the processor as a separate system, and you’re only paying a couple of percentages for the virtualization overhead. In either case, containers or light-weight VM’s you will want to run these on bare-metal as opposed to trying to run virtual systems on virtual systems. Either for cost or capability, this is the future. The great thing about Hack is that it is not an alternative to PHP. Hack is an additional layer within their HHVM that allow you to port existing code and write new code that supports the new semantics of Hack. Typing is a key concept, it is just better for complex projects, but it is just running in the JIT compiled virtual environment. You can run existing code, you can add new code with new semantics and features and there isn’t an overhead associated with running this mixed program. It retains complete compatibility with existing code. You can iterate gradually by porting code over, that is neat, the biggest problem with moving code around with PHP the only option with the Zend runtime is to write a C extension. Which has a really nasty API, its hard, its like halfway C and halfway macros. Its some of the ugliest stuff I’ve ever seen. With Hack and HHVM you don’t have to change the way your developers work. The biggest concern people have with Hack is that it is not supported at all from the Zend runtime. So the only way you can include Hack stuff in your code is to run it with HHVM. Facebook is trying to mitigate this with a new initiative where they are trying to jointly standardize the PHP language with the Zend engine. This would allow Hack or HHVM to be used without having to “never look back”. There is a pretty big step right now. We demoed a version of some experimental support of Pantheon using HHVM at the Austin DrupalCon. We showed benchmarks, it is a pretty massive improvement in performance. It is pretty exciting, while it is not ready as a customer thing we have built the system to be able to support it long term.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "apache-mesos-docker-orchestration-frameworks", "author": ["\n              Chris Weyl\n            "], "link": "https://www.ctl.io/developers/blog/post/apache-mesos-docker-orchestration-frameworks", "abstract": "Apache Mesos is a Docker orchestration datacenter-level project that aims to provide high-level, finely-grained, and collaborative locality-aware resource management to various computing frameworks. For Mesos, a framework is really any system that supports running jobs, and knows how to talk to Mesos. It will have a scheduler component, and one or more executors . Mesos operates as a cluster manager, with a master controller that communicates with agents residing on each node (e.g., host machine) and \"framework schedulers\". Nodes run with a Mesos agent on them, which in turn is responsible for managing the local resources, any \"framework executors\" running on the nodes and coordinating with the master. \"Framework schedulers\" are responsible for telling the Mesos master what jobs they have ready to run and what resources those jobs require; \"framework executors\" in turn are handed jobs that the cluster is ready to have run on the right nodes with the right access to resources. The framework components are quite key. The schedulers are responsible for talking to the Mesos master, telling Mesos what resource requirements they have, and for accepting or rejecting the resources that are in turn offered to them. Because Mesos offers resources to schedulers and allows those resource offers to be accepted or rejected by the frameworks themselves , Mesos has a kind of pass-through awareness: the schedulers know what resources their tasks are going to require, so can accept/reject resources based on that awareness. For instance, let's say I'm running a Mesos cluster in my basement with some number of frameworks and nodes configured. I decide that it's time to generate acoustid information against all the media files I've transcoded from my CD collection, I might — in some framework specific way — submit a job to go do that. The framework would know that the job requires local access to my music collection, because I told it so, and would wait for resource offers from the Mesos master. Node A is offered, but because only B has access to my music the scheduler declines the offer. Meanwhile, whatever jobs were consuming all the resources on B finish, and B's executor informs the master, which then makes an offer of processing space and access to the music library on B to the scheduler. The scheduler finding node B agreeable accepts it, and the master dispatches the job details to B's executor. The Mesos project itself has a great description of the process here, with a great image I'm going to take partially out of context: It may seem odd to say that \"data locality is a key part of Mesos\" when only the framework scheduler knows what resources its queued jobs require. But that's ok — in a neat way, this is also an example of data locality: the schedulers know what's needed, so Mesos defers to them on what resources they need — or don't need — for a given job. Good question!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "survive-a-zombie-apocalypse-using-docker", "author": ["\n              Rupak Ganguly\n            "], "link": "https://www.ctl.io/developers/blog/post/survive-a-zombie-apocalypse-using-docker", "abstract": "Always Be Prepared If you want to survive the incoming zombie horde then its best to follow the Boy Scout motto and \"Be Prepared\". Knowing when the apocalypse starts will be crucial and let's face it you won't hear about the zombie invasion on the news. Way before any news media, ok maybe not TMZ, the twitter 'verse will be clamoring. Let's get dialed in by installing the Birdwatch template . After installation update the terms to zombie, zombie attacks, apocalypse, or any other term you believe will give you plenty of warning. Don't be selfish. Better get the word out about the rising invasion. Fire up Panamax and search for WordPress. Selecting the WordPress template will provide a very popular CMS system to communicate information to the survivors. Get creative. Install a google map plugin to highlight safezones. Install a photo gallery so survivors can upload images of their loved ones before and after being zombified. You could find some reliable hosting provider but the brainless zombies will be sure to destroy those. So, let's follow these [/developers/blog/post/exposing-a-local-panamax-application-to-the-internet/) to nGrok that WordPress application so the world will have access after the ISPs fall. The post-apocalyptic world will require some survival skills so start training with Minecraft. You will learn some foraging skills and the importance of having a shelter by nightfall. Fighting creepers will provide plenty of practice before the real flesh eaters arrive. So install the Minecraft template and start training. In the aftermath, lead society out of the debris by setting up a chatserver. Install the tornado chat server by searching for chat within Panamax. Once its installed start organizing the survivors to push back the zombie horde. When the zombie menace has been subdued and civilization begins again, you will be one of the survivors thanks to Docker.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "making-clustered-infra-look-like-one-big-server-with-mesosphere", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/making-clustered-infra-look-like-one-big-server-with-mesosphere", "abstract": "What if your laptop asked you which core you would like to run Word in today? Sounds archaic, but that's the current state of the art with server infrastructure. You still have to pick with virtual machine you want to put your Rails app in. Mesosphere offers a layer of software that organizes your machines, VMs, and cloud instances and lets applications draw from a single pool of intelligently- and dynamically-allocated resources, increasing efficiency and reducing operational complexity. It provides an HA and fault-tolerant platform, and powers production environments at some of the most agile companies in the world. Mesosphere is based on Apache Mesos , an open source technology invented at UC Berkeley's AMPLab and run at large scale at Twitter. Enjoy my interview with Florian as I learn about Mesosphere and find out why it making big waves in the Docker world. Florian Leibert , CEO and Founder of Mesosphere Florian Leibert's mission at Mesosphere is to create a smart infrastructure mesh that lets developers to stop thinking about which piece of code should run where. I came to Silicon Valley in 2009 and started with a company called Ning. Later I moved from Ning to Twitter, at the time that Twitter's user base was scaling up. The demand for the site was going crazy, but at the same time the company was resource constrained in terms of hardware and engineering resources. My host family when I came to the US in 2000 included Benjamin Hindman, one of the co-creators of Mesos [ http://mesos.apache.org/] was working over at UC Berkeley. He was invited to talk to Twitter, and some of the ex-Google employees recognized something in the technology that they used at Google to really help them scale. This was Google Borg which became Google Omega. We convinced Ben to join Twitter, eventually full-time, and Twitter invested heavily into this technology. This solved a lot of issues for them. One of the main issues was that it really increased the level of automation and simplified the infrastructure. We were pulling micro-services out of the monolithic Rails application (called Monorail), and creating micro-services written in Scala or Java and needed to be able to deploy them. It took forever to coordinate with Ops to get the Puppet config files correct for deployment. The whole cycle took two or three months at the time to get a piece of code deployed to production. Mesos is very different from many of the technologies out there. It gathers all of your resources together and presents them as one large pool. With software like Marathon or Aurora you can actually deploy new applications in minutes to this shared pool of resources. After two years of adopting Mesos within Twitter I joined Airbnb. At Airbnb we also started using Mesos and we built a system called Chronos. Chronos targeted the analytic stack it really was an ETL tool that allowed you to construct dependency chains of individual jobs. After we successfully created Chronos and got traction within Airbnb, and outside Airbnb, we decided to start a company around Mesos, Chronos, Marathon and a number of other technologies. It was Tobi (Knaup) and myself, and Ben has now joined us full-time and we are working on Mesosphere. Yes certainly. Our laptops today have multiple cores, if you think back in time when we first started working in Assembler and low-level programming languages wed didn't really specify the resources we were using. As we moved up the stack to higher level programming languages we were able to use abstractions that the kernel provided, that allowed us to fire off threads that ran on an arbitrary CPU. In our data centers today we write against individual machines, often times in virtualized environments these are virtual machines. For a distributed application we have to manually wire these machines together. If one of these machines fail we, as developers, have to manually take action to recover. What Mesos does is provide a higher level abstraction that allows us to think about only the business logic when writing a distributed application. That's well and good when you are writing a brand new application, think Apache Spark [ https://spark.apache.org/] . Spark is often touted to replace Map-Reduce, but Spark was a sample application for Mesos when it was invented over at UC Berkeley as well. They argued that they could only write Spark and get so much adoption so quickly because they were building on top of Mesos. They were able to use the abstractions that Mesos provides and forget there was a network underneath. We are trying to use Marathon to be able to run existing programs that run on a particular box, like a Ruby on Rails application that runs on a single machine, and schedule those in the cluster scale those in the cluster and also deal with failures of individual nodes. Marathon is a native Mesos application and uses the Mesos abstractions to carry out these tasks. Yes, as you are building out the higher levels something has to be scheduling the lower levels. If you think of the hybrid environment where you may have some nodes at Amazon, some nodes in your private data center, if something were to abstract all of these resources away and you as a developer only had to worry about telling the system \"Hey I want to launch this application five time, and I don't really care\". Under the hood is a scheduler that takes the application and makes sure its running five times across all this hardware, and if it fails somewhere it will restart it. On a laptop with multiple cores, that is exactly what the kernel does. What Mesos is trying to bring is this metaphorical kernel for your data center or cloud. Mesos is an Apache project that has a large community behind it and has a lot of adoption. A lot of companies, like Twitter, Airbnb that have adopted it have a lot of engineering resources they can go in, they can fix bugs, they can install it everywhere. When you look at a lot of other companies that might not be focused around engineering as much might want to just have a drop that is efficient. They may want to install their own PaaS layer they can use our products and services to get up and running with Mesosphere. Which is more than Mesos, because it encompasses a series of products all of which are currently open source. If you think of Mesos itself, it is like the metaphorical kernel for your data center. Most users don't just download the kernel, they want the full system. That's what we want to deliver. Right now all of the public products are free. A number of them are heavily used, for example Mesosphere for GCE. You go to google.mesosphere.io and you can launch a Mesosphere cluster in less than 10 minutes. It comes with things like a file system installed, Marathon installed, it comes with Mesos of course installed, and we are going to add more and more components to that. It really allows you to get up and running with a cluster that we built up at Airbnb over the course of a year, in ten minutes or so. Mesos has multiple entry points. If all you care about is launching and keeping a Rails application running it less interesting. However, if you are building the next distributed system, let's say you want to build the next distributed database, ot the next distributed message bus, you can look at Mesos and the abstraction and write a fully distributed system much easier and with very little work. Chronos is a good example. It is a fully distributed system, its elastic, highly available and fault tolerant and the core of it has about 2500 lines of code. Even though it is a distributed system it has almost no lines of networking code. Mesos is the base for all of that and it has hooks in for most modern programming languages Go, JVM, Java, Scala and so forth, Python, you can write new Mesos frameworks in no time at all in most any language. We want to make it as easy as possible to use Mesos and Mesosphere products so we argue that if you can build for scale with zero overhead, why not do that from day 1? You can actually run Marathon in local mode or you can start a small cluster on GCE or on Amazon. You can start the application via Marathon on then maintain it and I argue that even if you are a shop with two to three developers it is important to run something that is fully automated because you probably don't want to be paged in the middle of the night when one of your machines goes down. Maybe its more cost effective to have three machines running and then is one goes down you can deal with it during lunch break. One aspect of Mesos, and particularly if you look at Marathon on top of Mesos, then we are talking about this very programmable PaaS layer. But we don't want to limit it to that, we think the spectrum is much wider. When you think of Mesos as this base of launching tasks into the cluster it becomes much more than just a PaaS, it becomes Infrastructure as a Service. If you are thinking of running a truly multi-tenant cluster you can think of running Spark, maybe your Hadoop distribution of choice, and Marathon; then you run long running services alongside your analytics batch services. That's when it becomes very compelling, because you have shared hardware, everything runs on the same cluster, you are really simplifying your infrastructure quite dramatically. That's a great question. Kubernetes is more like Marathon and Mesos is a layer below. As a matter of fact, we have ported Kubernetes onto Mesos. Because Kubernetes really allows you to run existing applications, such as your Rails app, and if you want to run it at scale you still need a good scheduler underneath and that is where Mesos comes in. Fleet is similar to Kubernetes, it is not as feature rich as either Marathon or Kubernetes, I don't know of anyone who uses it at scale. We have a customer running Marathon at scale on tens of thousands of physical servers, Twitter runs Mesos on 30,000+ servers as far as I know. Its really a case of scaling, and Marathon is this PaaS-like component. Kubernetes you can look at as a PaaS-like component, Fleet is this PaaS-like component, but you cannot run Spark on top of Kubernetes or Fleet, you can run it on Mesos. Today if you want to deploy MySQL on top of the Mesos architecture, using Marathon or Kubernetes, you would use static associations between hosts and the database. A database is not going to be a system that is very dynamic, for example if there is a network blip you cannot just move the database to somewhere else. You would want to wait until an event occurs and then declare the database host to be dead. Then, right now, you would have to do manual recovery. Nobody has a universal solution that really deals with databases and transactional systems. That being said we are building abstractions into Mesos itself that make it much easier to build new frameworks to build transactional systems on top of Mesos directly. We call these \"persistent offers\" where a certain application will always be offered a host with certain data. This is on our very short term roadmap and later this year we will have a very good solution for this as well. What we actually did was to make the containerizer pluggable. Mesos has been using cgroups for three or four years, since it inception, we've alway been able to use the isolation. We've seen a lot of momentum with Docker and the Docker ecosystem so it was natural for us to allow someone to plug in the Docker container as one of the implementations on how to do isolation packaging. We are seeing a lot of companies that are really jumping onto Docker for that. But we also see companies that are really happy with having a tar archive and they still use cgroups for isolation. If you want to write and run a Ruby on Rails application you don't have to change anything. All you need to do, once you are ready to deploy it onto a server, you upload it to the cluster into HDFS or S3, and then point Marathon at the binary that represents your application. It takes zero additional effort to start your application on a Mesos/Marathon cluster. That being said, if you are writing a new application, let's say you are trying to write the next distributed message bus, you could write directly against the Mesos abstractions and write a fully distributed system that is fully elastic, that is self-healing, and then launch it against a GCE cluster just like you could on your laptop. The great thing about Mesos is that it really turns your compute resources into this pluggable system. You can just add resources, you don't have to restart any applications, resources are added to the pool and then applications can be scheduled against the resource pool. Absolutely, you can just create a Docker image and put it into your Docker registry of choice and then specify the URI to Marathon to launch it. You can scale it up and down, and if one of the container instances fails the system will self heal and restart the application instance. We really want to enable developers to write stateful applications, specifically transactional applications directly against the Mesos API. We are really aiming to have a rich ecosystem of native Mesos applications. We want developers to be able to leverage the underlying system to reduce their complexity and make distributed systems programmatic and as easy as programming against a single machine. With the Internet of Things and the need for more frameworks and models for machine learning applications, we will see more need for writing distributed applications. We aim to be the platform for all of these distributed systems. I think it is similar to the discussion of whether one programming language is better. A lot of these things are very opinionated, for Marathon I can say it is running in production at a very large scale at a number of companies. It really is around your opinion on how you want to deploy your application. You can just try it out, go to the GCE cluster and try Marathon, you can install Kubernetes on the same cluster and try it also. We don't actually charge anything, your Google charges will carry forward. It's roughly 60 cents a minute for a small cluster and you can start clusters of arbitrary size. We have instructions on how you can add nodes if one dies. I think it is four nodes. In my opinion, we need to start architecting for fault tolerance and scale out. We need to stop thinking about individual machines that all have their snowflake configuration. We should treat compute as a resource, as a big aggregate resource pool. Particularly in larger enterprises it is really inefficient, every department does their own purchasing of hardware, and you end up with these clusters that are terribly underused. I think they will co-exist for a while. Changes in IT happen slowly, there are early adopters but there are many laggards. It will be a while until the whole landscape will have shifted. In some industries, such as financials, we are seeing a shift now. Linux for the enterprise is driving a lot of this. Data centers are fundamentally changing as well. In the past, the way we thought about applications was exactly that, we thought about individual machines and connecting them together. Next generation hardware will be these disaggregated hardware, these racks, Open Compute is going in this direction as well. Amazon has given developers the ability to start a cluster, whether it is a Hadoop cluster or a cluster of Rails instances, with a couple of mouse clicks. What we are doing is very similar to that. You can run your analytics, you can run your long running services, all on a shared infrastructure. The advantage of Mesos and Marathon is that these are open source projects, they are not tied to a cloud provider. They are not tied to specific hardware, they are not tied to virtual machines. Supporting hybrid configurations, some running in the cloud some running on-prem, and being able to merge them together logically and make them look like one big pool.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "should-cios-care-about-docker", "author": ["\n              Ross Jimenez\n            "], "link": "https://www.ctl.io/developers/blog/post/should-cios-care-about-docker", "abstract": "The cloud has had a direct impact on all businesses. From creating new efficiencies in IT spend, to enabling new business models, most companies have seen their business change as a result of offloading some workloads on the cloud. Living though and participating in cloud technologies over the past 10 years -- along with watching the \"as a Service\" evolution from SaaS, PaaS to IaaS develop and mature -- has informed my perspective. So how does Docker fit into the cloud evolution? I see Docker providing a more efficient cloud paradigm, and in some ways democratize the cloud so it is easier to adopt and do on your own without relying on service providers and \"as a Service\" models. The key difference with Docker is that it is nothing like IaaS, PaaS, or SaaS. It solves a different problem lower in the stack and serves to enrich the cloud models that use it; potentially, Docker could even eliminate some technologies. As with life, nothing in technology is black and white there are trade-offs and unique cases where the benefits of a technology might be useful to one use case and less useful to others. However, the thing with Docker is that it is extremely flexible. It is not subject to the same constraints as a PaaS or even an IaaS (although some have described it as a mini-IaaS ). Docker enables you to build more flexible versions of a PaaS, and many IaaS solutions like OpenStack (and others soon) will allow you to launch light-weight containers instead of the heavy VMs that compromise a good chunk of our workflow. The beauty of Docker is that it lets you move down the stack and do what you want. Yes, you can have freedom! Is that more work? Yes, it can be. Is that more to manage? Yes, it can be. As with everything, doing things yourself vs. handing work off to a service provide such as a PaaS is a trade-off that enterprise IT leaders and developers need to understand. If you are happy with your PaaS provider today you probably will not care too much... although your PaaS provider probably already does. On the other other hand, if you put a lot of work in moving to IaaS and using configuration management tools to create an architecture that is more flexible than a PaaS, you might be more interested in what Docker can do for you. For some folks it is a matter of scale. It is great to start off on a PaaS that does everything for you, but eventually the overhead costs outweigh the benefits and that is when companies decide to roll their own infrastructure using a IaaS provider rather than just accepting the black box of a PaaS. Considering all Docker can do, is multi-tenant SaaS really needed when I can deploy isolated containerized versions of an application for every tenant? Is it not better if I can scale per each tenant's need or service level? Do I want a noisy neighbor impacting other tenants? The fundamental ability for Docker to allow for dependencies to be containerized and thus makes entire applications and the application stack they rely on portable is truly disruptive. However, we are far from having the tools or process ecosystem around it to make Docker and containerization mainstream now. There is no doubt that Docker will get to the \"slope of the enlightened\" but there is still much work to be done in creating the management tools and processes within enterprises to truly leverage the technology. Also the container alone is only part of the disruption, the other piece of it is the architectures it enables. Micro-service architectures are not that uncommon for large distributed systems, but for many enterprise developers it is and will be a new way to think about and architect solutions. Once the tools and platforms exist many enterprises will also now have a way to ensure their developers are building on \"approved\" or \"blessed\" software stacks. This reduces the need for manual governance -- things like infrastructure governance reviews, enterprise architecture reviews etc. will not be needed once the CIO has a control point beyond the PMO, enterprise architecture and manual review meetings. Ensuring \"approved\" containers are used does not eliminate \"bad code\" by any means, but it does reduce the risk of change later in the lifecycle where it is most costly. First and foremost it is paramount to recognize the high disruption potential on several fronts from IT governance processes and tools, to service provider spend, to licensing costs. There are many areas of IT spend that eventually will be touched by Docker and the evolving Docker ecosystem. There are no shortcuts to knowledge, as with most new technologies. Keep an eye on Docker and Linux containers, have a small team do pilot projects, continue to gain and capture knowledge and figure out what trade-offs have to be met for you to make the jump and heavily invest in it.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "virtkick-an-open-source-vpn-and-your-own-digital-ocean", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/virtkick-an-open-source-vpn-and-your-own-digital-ocean", "abstract": "Check out this week's interview with Damian Nowak , one of the founders of Virtkick. VirtKick wraps awkward virtualization technology in a convenient, usable panel. VirtKick does to open source what DigitalOcean did to SaaS. Simplicity and privacy are their core goals. The project is currently alpha and the code is on GitHub . Damian Nowak , Co-Founder and CEO of Virtkick Damian Nowak is a CEO of VirtKick. He's a Ruby coder, an Arch Linux hacker, and drinks good beer. VirtKick is, essentially, a self-hosted Digital Ocean. We enable almost anyone to manage their clouds without having to buy the VPS from any company we just let people order a dedicated server and then manage their clouds themselves. They can get a dedicated server from anywhere, it can be for example SoftLayer or OVH in Europe, you can get them from anywhere. Since dedicated servers are a bit different from VPS the security and the privacy you get is much better than with VPS. Thanks to this, if they would like to snoop on you they would need to physically get to the server and read the disks. Whereas with a VPS providers can read it without you noticing it, at any time. They are able to just perform a live snapshot of the system and pass it to the authorities. Privacy is one of our concerns. The other concern that we are targeting is building competition in the VPS provider business. We will not only have the virtualization features that are essential for running the cloud but VirtKick will also ship with features usable by VPS providers. That means that payments, customer management, and a small ticket system will be shipped with VirtKick this means that many new VPS providers will be able to start their business, or existing providers will be able to switch to VirtKick and improve the user experience. As you know, Digital Ocean revolutionized the business by doing this panel super reliable, super easy, and great. We are building the same thing for open source. At the moment we are at alpha stage but already the instructions are very very simple. There are only five commands to execute, one of which is 'git pull' and another is 'virtkick start'. So its already very simple however we are going to simplify it even more and package it so you just have a ddm package, an rpm package so it very simple to use. Unlike many usual Linux packages VirtKick will not need any manual install procedures, like editing some configuration files. We will get everything configured from the network, the tool storage pools, and things like that. Currently, VirtKick downloads all the ISOs so when you start VirtKick you can already pick Ubuntu and the image will just load up and you will have to install it in the VNC console. The next step is providing virtual appliances so that you don't even have to manually install Ubuntu and other distros on the hard disk. You will just get your SSH keys injected. We only support one hypervisor, however we have plans on our short term to add support for many hypervisors and it will be very very easy to be the VirtKick administrator who goes to the panel and hit a button to add a hypervisor. You will provide SSH connection details and VirtKick will do everything on the remote hypervisor to make it available and after a while you will be able to create a new VM right on this server. Another problem we have on our long term plan is to order a new dedicated server from SoftLayer or OVH or any other provider. So you don't even need to go to the VPS provider to order it we'll just do it for you without even leaving the VirtKick interface. A few years ago I had a few servers that were unused whereas my friends wanted some VPS servers, this was when Digital Ocean did not exist. I was trying to set up something for my friends and concluded there was really nothing interesting that was simple, easy and provides a good experience to these users. And there was, obviously, nothing available open source. OpenStack, for example, is a great project however I believe it is great for big infrastructures, big corporate infrastructures where people expect more that to just have a VM. They expect to have some complicated networking infrastructure, failover redundancy, and things like that. These are things the neither Digital Ocean or Linode provide and they are fine with this. So there are many more people that just want the VM rather than people who want sophisticated configuration. So four years ago there wasn't anything like that, just a conclusion that it was not possible. A year ago I starting think a bit on how I could do something to improve it. Three years had passed and nothing had happened in the open source community so I decided to do something about it. Then presented the idea to Rush (Damian Kaczmarek) who concluded this is a good idea and off we go. The very first thing that you see is the UI. The UI is built with Rails and AngularJS, some parts are generated on the backend and some parts with more interaction are AngularJS. Our temporary backend that we are using is WebVirtMgr. WebVirtMgr is another open source application that lets users control their virtualization with libvirt. We wrapped it into VirtKick, added JSON API on top of it, and use it as a JSON API to libvirt, where libvirt is another abstraction to KVM that we are using for virtualization. However, KVM is our first step, the next step will be OpenVZ, and the next step will be Docker. We started our crowd-funding some twenty days ago and the crowd-funding isn't going exactly well. In my understanding that is because people think it is too technical for them, whereas we aim for VirtKick to be super usable so anyone can kickstart their virtualization with one click. The problem we have in this virtualization business is that people are already used to this being very very hard. Hard to start, hard to use, people think this is too technical for it to benefit them. However, we have been accepted to a startup accelerator program and we will receive some money that will boost our development. I believe we will be able to show people in three or four months from now that virtualization doesn't need to be that hard as it used to be with technologies that were already there. Federated VPS for developers is something you can think like, people building open source projects need to run some sophisticated infrastructure tests. They currently do it through Vagrant which connects to Digital Ocean and they test all the stuff in Digital Ocean. What we are thinking of doing is after VirtKick is super popular amongst home users and small businesses that they would be able to donate their resources on servers, or even desktop computers, to the VirtKick federation. Open source developers will be able to run these infrastructure tests in virtual machines that will be created somewhere in the federation. For example, on your computer. Thanks to that, open source developers won't need to pay money for these VMs spinning and running tests. For example, testing Docker files. You need a VM for that because it is currently not possible to do it in TravisCI and that is what Digital Ocean is using. We are planning on federating all these home users and providing something free and cool to software developers. Yes, that is also the idea. Of course, our first step will be to be able to donate these resources that are Linux boxes, however we have been thinking of supporting VirtualBox so that Mac users can join us. In the long term we will support VirtualBox and Mac users because this is essential for this to succeed. Let's begin with what \"copyleft\" is. Copyleft is a way to ensure that the software and the source code belongs to people, belongs to the users, instead of to the corporation. That is the case for VirtKick. We license it against the AGPL license, which means that not only do they have to keep it still open but also share the source code with their users and visitors. For example, if a company implements some new extra feature in VirtKick, like an integration with BitCoin, they have to share it with their users so that we developers will be able to merge it back into the project. This is how AGPL works and that is good for the community to benefit from companies that do something extra on VirtKick. However, some companies may need to develop something and they don't want to share this with others. So we let them do this as long as they pay something to us to boost our development. Even Richard Stallman (rms) said this is acceptable and OK so, if he says so, I am happy to do the same. I myself like the GPL approach whereas the other co-founder (Rush) is rather an MIT guy. The most important thing in startups and earning money is that first you have to focus on users, and find those users, and the ways to monetize what you do are always to be found. We are currently not sure how to make money on it, maybe it will be paid support, maybe it will be licensing, or maybe it will be something else. For now we are just doing something that we know it cool, that we are doing something that comes out of our passion, something for the open source community. If it kick some money it will automatically go to us and our investors. We are hoping for good. Yes, that is an assumption that has grown from this being open source. With this being open source anyone can run it and we have no means to get some cut of this, that's not possible. Anyone who sells these VPS servers to their customers has got to register their account with Stripe or Paypal so we won't get anything from that. However, if we add some links in VirtKick and they click on these links at least we will get some money by referring people to this. Indirectly there is a way to get some little money on the referrals, however there is no way to get the percentage of sales of other companies. Frankly we don't even need something like this, we don't want... We want them to sell these VPS servers and make VirtKick even more popular. Both of us co-founders were previously working in some daily job and at the same time were doing VirtKick. I was working for Zerigo, a cloud hosting provider, for one year and I was working on the source code of the DNS offer and the VPS offer. At the same time it has been almost four years I have been working on my small business that provided hosting of Atlassian applications, Jira and Confluence, and at the same time we are offering VPS servers to our friends. Virtualization is something I have been working with for four years now. Rush is a Linux guy. He knows how C bindings work, how NodeJS calls interact with Linux, so he is essential in the project because he will be the guy who will contribute to the libvirt project, who does things like wrapping VNC connections with NodeJS, HTTP servers. He used to work on some imbedded code on some ARM processors. For the last two months we have been doing it full time because we saw the potential and we decided that this is something that we have to focus because we have to keep up with this and make it finally. So that it is not something that is announced and never developed. We plan to get the 1.0 version somewhere in January. The wildest dream would be that when people when they want to have a new VPS server they will have to think about it for a minute because there will be so many great VPS providers they won't be able to choose the best one because all of them will be very good. We haven't tested it, but someone who tested it added it so we just accepted the pull request. I know that Digital Ocean currently supports nested virtualization so its possible to create VirtKick in Digital Ocean and start a virtual machine. The install on Digital Ocean button is a very cool initiative. Someone thought they could use these new features, metadata API feature from Digital Ocean and implement something like this.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "deis-1-0-released-popular-docker-paas-becoming-more-robust", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/deis-1-0-released-popular-docker-paas-becoming-more-robust", "abstract": "Check out this week's interview with Gabriel Monroy, one of the founders of Deis, a Docker-oriented PaaS. Deis 1.0 was released on Nov 11, 2014. Here is the blog post announcing the release: Deis 1.0 . Gabriel Monroy , Founder of Deis Gabriel is an entrepreneur, hacker, Linux container fanatic and the creator of Deis, a popular open source application platform (PaaS), purpose-built for Docker. Deis is a Platform as a Service that uses Docker and so it makes Docker application development very easy to use and very familiar if you know something like Heroku. It is very similar to a Heroku-like system. Well, there's really a few things. I was speaking with someone recently who made a point about 1.0 that I liked which was \"You wave the 1.0 flag when you run out of reasons why you shouldn't\". For us we have had the project out there being used in production by a number of companies for a long while, we have APIs that rae stable, the stability of the project has proven itself out. So we were running out of reasons why we shouldn't call it 1.0. We really spent some time to put in the last changes to the public-facing APIs that we wanted to get in there to make sure we were future-proofing the 1.0 API and once that was out there we went ahead and cut that and it has been exciting ever since. One of the best ways to think of Deis, besides being a private Heroku, is that there is a lot of churn in the container eco-system. The main thing that Deis provides is a workflow for software teams that are looking to adopt this stuff that is stable, that isn't going to change. Yes there is a 'git push' component to this typically that mimics what you are going to get from Heroku, there's also the ability to suck in native Docker images and deploy those Docker images using our Deis \"pull\" functionality. That workflow along with the ability to scale containers, the ability to view aggregated logs, manage runtime configuration, collaborate with a team and all those sort of things we've really just hardened and solidified the core feature set that's proving to work at the software teams that have deployed Deis today. First of all, taking a step back, persistence (statefulness), as most of us in this space know, is one of the hardest problems in the distributed systems game. Though we still recommend that the types of applications you deploy on Deis are stateless, following the Heroku and 12 Factor model, Deis itself has to store state. We needed to solve that in a way that was resilient to host failures for our production 1.0 release. The way that we did that was we actually containerized Ceph , the distributed database. Its a very interesting project, one of the things that was attractive to us about Ceph was that it provide three different access modes. It provides a blob store API along the lines of S3 or Swift which was useful to us for integrating the Docker registry that we ship as part of the platform. There's also a block store API which allows you to mount block devices that are actually backed by the distributed Ceph database. And there's also a true filesystem called CephFS , support for CephFS landed in the 3.1.7 kernel and CoreOS included it very recently. That's important for us, our log aggregation system has concurrent readers and writers and that was a pretty important point. We have our Postgres database, the log aggregation system, the Docker registry all using this highly available Ceph containerized deployment of Ceph as part of the platform control plane. Another thing that is sort of interesting along those lines, we actually have some other users who are using our containerized version of Ceph outside of Deis. So, if you are potentially interested in rolling your own Ceph cluster and want to do it inside Docker containers you should take a look at the Deis store containers and there is a blog post by a fellow named Ian Blenke who's an active member of the Deis community and he has shown how to use the Ceph that we created outside of Deis . An etcd key-value store is really based around the Raft protocol and its to achieve consensus around a distributed system. etcd is not really designed to store large amounts of data the likes of which you would use Ceph for. Similar to Zookeeper you are going to write important configuration bits, important bits that are going to drive service discovery on the system, but you're not going to be writing massive amounts of data. The quorum is too expensive and replication costs are too expensive. Yeah, I think so. The way I like to judge that is by taking a look at the past release notes and if you look at some of the stuff that Docker has been working on there hasn't been a whole ton of really disruptive churn in the project, maybe with the exception of a few security related changes that have gone in recently. I think that Docker as a whole has proven, at least at the container engine portion of Docker, Docker is expanding beyond that, the container engine portion seems to be definitely suitable for production. It's interesting times in this eco-system. We were kind of caught in the middle of this, we use CoreOS as you mentioned, we use Docker heavily as you mentioned, we have relationships with the engineering teams and we also have business relationships with both companies. I definitely see the idea of competing technologies on the container engine front as a being good thing overall, I think competition is good. Yet at the same time I think a lot of this stuff is still relatively immature so its kind of difficult to predict where things are going to shake out. We are definitely interested in the ACI spec and are interested in contributing to that, as are other in the space. But for the foreseeable future we are going to be using Docker and especially now that we are 1.0 with a Docker container engine. I definitely think its interesting and I think Docker has earned the right to do what they want as a company. I personally am very interested in what I like to refer to as layered architecture. I tweeted something out recently that got a decent amount of play which was a diagram that kind of outlines the stata of the container eco-system. Really if you look at the container engine it but one small portion of the strata. I see no reason why companies can't play at different points of the eco-system as long as the lines between what is scheduling versus what is orchestration versus what is the container engine, as long as those lines are well defined architecturally that's what I'm most interested in. Absolutely, and I think this is one of the key differentiators of Deis. We have a lot of folks ask us \"What are the Deis and Cloud Foundry?\" for example. The main thing I try to express is, besides the fact that we were architected after Docker which is sort of a soft answer to that, Deis is an order of magnitude easier to install and easier to operate. All you really need to get Deis running is an existing cluster of CoreOS machines, we've just moved to the CoreOS stable channel, so anything on stable or any later release should suffice just fine. And there is really just three commands that you need to run. There is a command line utility called 'deisctl' that is used for the operational side of this, so 'deisctl install platform', 'desictl start platform', and you're up and running. It takes a few minutes to download some Docker images but that's about it. No actually the desictl command is, you can run it on nodes in the cluster, but really it is designed as a tool to run on your workstation that operates across a cluster of CoreOS machines. Another way of thinking about it at a lower level is that deisctl is wrapping the Fleet and etcd APIs. So Deis is scheduling the platform control plane and routing mesh via the Fleet APIs, its performing configuration functions via the etcd APIs and you can do that from your workstation over an SSH tunnel to one of the nodes in the CoreOS cluster and those changes will replicate across the other nodes. Yes, if you go to deis.io you can find our documentation. One of the things that I am very proud of with our project is we've done, I think, a really great job fleshing out the documentation, making sure it stays updated as the project evolves, we don't accept pull requests that don't have documentation and the like. You can find an Installing Deis section of our website, find a Quick Start guide, find guides for specific platforms; EC2, Digital Ocean, Google Compute, bare-metal and the like to get you up and get it running. If you look on Twitter and inside GitHub you'll see lots of companies have been able to stand-up Deis quite easily and they are quite happy with the process. A little bit of background. Deis has evolved from being a platform that could support a single host configuration, to a platform that is highly available by default. Deis has a minimum of three nodes. That was kind of new and that was before we announced 1.0 and as part of the lead-up to that I know we had a number of folks that were using Deis on a single node. To try and provide a better answer for those folks who really wanted these small setups, Jeff and I are friends and we talk regularly and it turned out that Dokku was looking for sponsorships. So we thought it was a good opportunity to give back to the open source community in the form of Dokku, but also to try to tie the two projects together a little more closely so that folks that are using Dokku when they outgrow it they can keep the same workflow, or similar workflow, and know their applications will work on Dokku as well as on Deis when they start to need to achieve scale. I'm really excited about it and I'm always excited to work with Jeff, Jeff is always churning out great things, I am definitely excited about some of the stuff we can do around standardizing around buildpacks, Docker integration, example applications and test suites that can prove out if problems are in buildpacks or in some of the build infrastructure. Over time, the goal is to share the build stage of Dokku and Deis and maybe other projects like Flynn down the road. I love it. I think there is a lot of benefits, the biggest one is that you can really shrink the image sizes pretty significantly. The Heroku Cedar stack, which is kind of the requirement for using buildpack based deploys, is one of the main reasons it takes about 30 minutes to provision a Deis cluster because we have to download that and sort of prep it. If that could be eliminated from the process it could be great. Moreover, the general idea of it is pretty exciting to me. However, we are interested in shipping stuff that works for teams today and buildpacks are proven and reliable and very comfortable for folks migrating off Heroku and we do a lot of companies that have been successful on Heroku but need to move to metal or to their own EC2 instances for example. We want to offer them a great migration story. That's an interesting question, we should probably fail fast and let you know right up front, I'm not sure we do today. I believe what might happen is, I think maybe the Ceph store won't actually come up, but that's a good point. Right now we warn you as much as we can throughout the documentation the idea is that you should follow those warnings (laughs). Yes, absolutely. And that is really a function of the scheduler and the scheduler recognizing the new host is part of the cluster. Really going forward for some of these larger deploys that we are working on if you look at the Deis architecture diagrams there's kind of the control plane of Deis and the data plane of Deis. The control plane is designed to be three to five hosts where you're really maintaing the Raft quorum, and that sort of thing, the platform control components like the Ceph cluster reside there. The data plane is designed to be kept really light-weight so a minimal set of components, minimal memory requirements and that's where you run the containers that power the applications for your team. We see those two as kind of distinct, the idea is that you would setup a more or less static control plane and you really achieve scale and add and remove hosts on the data plane side of the equation. That's a great question, its difficult for us to know, we're working with customers well above the 20 host point at this point. We're also working very closely with the folks at Mesosphere on achieving scale that is on the order of thousands of nodes. For us, we are a scheduling consumer so there's not really much besides the scheduler itself that is going to affect how large the platform can scale. So we are really excited to work with the folks at Mesosphere, we actually have an alpha of this that we're testing that uses the Marathon API from Mesos. Its pretty exciting to us, pretty soon you are going to start seeing posts from us about Deis clusters that are over a thousand nodes. Correct, the default scheduler is Fleet. I just gave a talk at QCon recently on cluster schedulers and specifically on cluster schedulers as they relate to Docker. One of the interesting things about cluster schedulers is that most of them are designed to do their own process isolation. One of the tricks with Docker is that Docker is also designed to do process isolation. What you have is cluster schedulers and Docker sort of fighting over who gets control over the cgroups for the platform. That's a tricky place, there's only a few schedulers out there that support Docker controlling the cgroups, Mesos is one of them. Because we are based on CoreOS Fleet was kind of free, right? We ship with Fleet, we use Fleet to schedule the control plane so it is an easy choice. Sort of an zero config default that allows you to schedule with Fleet. Over time as we achieve scale we think other scheduling solutions like Mesos are going to prove better. Atomic is not on the roadmap as we are really focused on making CoreOS work well. We work very very closely with the team over there and I think they are doing great things, we're really interested in getting users that are not super concerned about what the underlying operating system is. If you are deeply concerned about that and you want to be tinkering at that level then Deis may not be right for you. And that is perfectly OK and not every project is a fit for every team. Does that mean that we are going to be CoreOS forever, no absolutely not. We certainly reserve our right to choose some other operating systems down the line. Brings up an interesting question, one of the things that I would really love to see shake out in the eco-system is, I have this diagram I mentioned before about the strata of the container eco-system and really where we are sitting with Deis, and I would argue Panamax also, is kind of at the top at this workflow layer. This is kind of \"what are you exposing to the end software teams?\" Below that I see an orchestration layer along the lines of something like Kubernetes, and below that a scheduling layer which is doing placement across the distributed system, and below that you have the scheduling layer talking to a container engine, which could be Rocket or Docker and so forth. And below that you have an operating system. Of course, these lines can be blurred and its not 100% clear... But what I would love as someone who is building tools in this space is to have some well defined boundaries between these layers along the lines of the OSI model. One of the reasons the OSI model is so great, for those of you who aren't familiar the OSI model defines networking layers for the networking stack, and the real beauty is that if you are building a solution on layer 3 of the networking stack for IP connectivity you only have to worry about the layer beneath you. As a platform builder I would love nothing more than to build my workflow layer, worry about an integration to Kubernetes or something that exposes similar APIs and then not have to worry all the way down the rest of the stack about the container engine and what the scheduling system looks like and all that. With a relatively immature eco-system like this the truth is that we are not there yet and these strata are still going to take some time to solidify. Yeah, that's really exciting. So there's a couple of things that I can share and there's other stuff that I can't share but all of it very exciting. The things I can share, if you follow the project these won't be big surprises, we are definitely interested in shipping a production ready Mesos integration to achieve scale. We're also very interested in providing a service gateway implementation that will allow explicit service attachments. One of the things that Deis is used for in the field is microservice architectures. Having explicit attachment between different services that are backed by authorization and access controls and can be audited and that sort of thing is incredibly important. As opposed to some of the implicit, service discovery attachment systems that you'll find a lot of work on in the Docker eco-system today. Beyond that there is a lot of interesting proposals, one that I'll mention is that we had a community member write up a whole pull request to implement SSL support at the per application level. So each deployed application can actually ship its ow certificate and private key which will integrate with the custom domains that are added and allow users to granularly control SSL at a per app level. Which is really exciting to me because its a feature we have wanted for a long time but one that we haven't had the resources to build and someone from the community came along and shipped it for us. Check out our blog at deis.io/blog . You can also follow myself on Twitter at @gabertv or you can follow @opendeis on Twitter.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-to-secure-your-private-docker-registry", "author": ["\n              Alex Weich\n            "], "link": "https://www.ctl.io/developers/blog/post/how-to-secure-your-private-docker-registry", "abstract": "The Docker team has made it easy for us to host our own private docker registry by providing us with an Open Source, Python, web application for doing so . The web application also exists on the Docker hub as a single Docker image that we can execute to have our registry up and running as a Docker container. The running container provides us with a registry we may push and pull from, but it leaves it to us to secure the registry on our server via SSL, and optionally, basic authentication. Securing the registry is important, as the folks at Docker have made clear. See the recent updates strongly encouraging the use of SSL when interacting with private registries. There are plenty of options for setting up the registry and protecting it. The post, How To Set Up a Private Docker Registry on Ubuntu 14.04 , does a good job of explaining how to do so without the use of containers. Here, I'll show how we can set up our own secure registry leveraging Docker containers. The basic principle is this: We run the registry container as we normally would, then fire up a separate nginx container to provide the SSL and basic authentication. The nginx container will proxy the HTTPS requests internally over HTTP to the linked registry container. Use the htpasswd command to create a new .htpasswd file with the desired credentials. I had to install the apache2-utils package on Debian. Save the created file, we'll need it later. Previously at CenturyLink Labs we had created an image to facilitate creating self-signed certificates for Panamax . We can leverage that image here. This will create the private key, certificate signing request, and certificate inside the ~/certs directory. It doesn't really matter where we stash these at the moment, just keep the generated files handy, as we'll need the certificate and private key later. Note: Generating the SSL certificate went pretty quick here, but I'd encourage the reader to investigate this a bit more on their own. It's also possible, and generally preferred, to use a CA signed certificate. I feel that's beyond the scope of this post, so I'll leave it to the reader to decide how to handle this. Run the registry container , being careful not to bind the port to the host. We only want this container accessible via the docker link. With the registry container running, spin up the proxy container, passing in our public certificate, private key, and .htpasswd file. Additionally, we need to map port 8080 to the host (feel free to map 8080 to any port you like on the host) and supply the PUBLIC_IP_ADDR environment variable. Note: There was more handwaving in this step. We buried a bit of the work inside the centurylinklabs/nginx-ssl-proxy image. The source is located on GitHub . If all went well you should be able to copy your public certificate to another machine and curl the endpoint. See the post How to use your own registry for information on working with the private registries. Securing the registry is important. The Docker team made it pretty easy to host your own private docker registry by providing Open Source, Python, and web applications for doing that. Don't have an account on CenturyLink Cloud? No problem. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . Sign up for our Developer-focused newsletter CODE. Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-is-rocket-and-how-its-different-than-docker", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/what-is-rocket-and-how-its-different-than-docker", "abstract": "Docker is not the only Linux Container manager available. Recently, CoreOS launched their own project named Rocket which has some architecturally different approaches to managing containers. This week, we interview the brain behind Rocket to ask what is Rocket and why they built it. Brandon Philips is the Co-founder and CTO of CoreOS Prior to starting CoreOS, Brandon spent some time at Rackspace where he worked on monitoring products. He worked on a platform called Lubbock where they did LUA sort of a Node.js style LUA thing. Before that he did kernel infrastructure and kernel hacking at SuSE and the SuSE Labs Group. Rocket is a container runtime, which means a few things. The first is that it downloads images of the application over the Internet, and in future versions it will verify those, and it runs those inside of a container. A lot of people have a good sense of what a container is, but in simple terms a container is a process that is running on your host and it is isolated from other processes. A few things that we have done a little bit differently; first is that Rocket does not have a daemon so when you run \"rocket run coreos/etcd\" that is actually executing directly under the process that you started it from. So, you know your bash is PID 400, then \"rocket run\" is going to be 401. This is a pretty substantial design difference, we wanted to have Rocket be essentially a view on a static binary. Much in the same way when you compile a Go program and run that it executes directly underneath your initial process. In a way, in that Docker runs these things in separate process. You can think of it in another way... So you already have an init system like upstart or systemd or daemon tools or runit, when you do \"rocket run\" it actually gets monitored and run under that init system. This can be a little bit of a tricky point if you're using Docker for these sorts of use cases. Because the (Docker) daemon will, when you do \"docker run\" will be underneath the PID namespace of the Docker daemon. Yes, there is a community project that someone announced yesterday that converts a Docker image into the App Container image. Another thing is that we have worked with a number of people and groups within this \"container eco-system\" and got some feedback and created a separate spec outside of Rocket which defines what we are calling an App Container, an App Container image, and an App Container runtime. Those projects now live at github.com/appc and so there is a tool that converts from a Docker container to an App Container image. Through the image itself. Yeah, you can imagine doing that, there's no direct tool right now. Part of what Docker is, is that Dockerfile and that Docker build process. I think that process is fine and it is a pretty reasonable way of putting together a container. With the App Container runtime another difference from Docker is that Rocket runs multiple processes inside the container. So you have an active monitoring init system that ensures you have the ability to restart processes, or say, have two or three processes sharing resources inside of a single container. So, you can think of it as there's an outside container, the root container, and each app runs in its own individual container an you can put contraints on those things. This kind of fixes the problem that a lot of people have where they will install daemon tools or runit inside their Docker container. Instead you can do that as a first class process and compose different images together in that single root container. Yeah, it's identical to how you have a root process (in Docker) and that can come from wherever you want; you can build it yourself or use a Debian or use a Fedora or whatever. So, right now there there are build tools that are merging, like build tools that create a Go ACI (Application Container Image) directly from Go source code, or convert from Docker images to ACI's. That sort of whole eco-system is developing. Any modern Linux kernel should work, so if you have a modern Ubuntu or Fedora or CoreOS or whatever, its a single static binary. There's a few technical reasons why we wanted to build this. One of the major ones was this daemon-less mode and this enables us to do things like integrate well with socket activation, so the idea that I pass file descriptors from one process to another and those file descriptors may be open, listening sockets. This has a lot of nice security things that come out of it and also dependency management things. So you can imagine that I have, on a systemd system I can say listen on port 443 and then hand the file descriptor, if someone comes in on that socket, hand that file descriptor to my Rocket container that is running my web server. The cool thing about that is that if you architect your application correctly, the web server doesn't have to have any networking at all. It just has this one file descriptor and so it doesn't have to have any outbound networking, it doesn't have to know anything about routable Internet addresses. So this is a nice little feature, but it means that it can play well with existing systemd or upstart or whatever init systems. We have had a few bootstrapping problems that we have hit (with Docker) and talk about on our mailing list, for example we have an overlay networking system called Flannel and we'd like to run that in a container. We ended up having a bootstrapping problem how do we run that in a container that then needs to configure the outside Docker daemon and then start that Docker daemon with the correct network configuration? So these sorts of advantages were what we had in mind for daemon-less mode. Also the (App Container) image format allows for a few interesting things. The first is that it can be signed by regular GPG keys and it has a discovery mechanism. So your registry for these images can be a simple http server or you can do more interesting things like if you run coreos.com/etcd:version050 on Rocket that actually uses a mechanism that looks at the http pages and finds out where the image that the binary for etcd lives and downloads it and launches it. You don't have to know where that image is; its actually hosted on GitHub, but we own that domain (coreos.com/etcd) so people can kind of spread out their namespaces where containers live over the internet. This is how a lot of protocols on the Internet work. Yes, so Docker isn't going anywhere; we have a ton of users using Docker and we think the Docker platform is just fine. What Rocket will be used for is for some of these problems I've talked about, for example being able to do this Flannel overlay stuff. There's a number of places where we just continue to add more stuff to the CoreOS system but it would be really nice if we could run those things in containers. Helping to solve those problems is where we will start to see Rocket more. I can certainly see a use case for both things depending on what you are trying to do with your host. I think that is reasonable. From that time period when we had git coming out what we kind of found was that there is plumbing, git has this concept of plumbing and porcelain. The plumbing is the stuff that hopefully everyone can agree on but there is going to be different workflows on top of that plumbing. What I would like to see with the App Container spec that kind of becomes the plumbing of what we think of as these \"application containers\", so we agree on file formats, how these images are found, how they are downloaded, and how they are assembled on disk. Then it is perfectly great, and I actually want to encourage this idea, that lots of different runtimes might exist. Because POSIX and the way people architect their systems, they're complicated and a lot of people have very strict or interesting needs, so I think it is OK if we end up in a place where there are lots of different runtimes. For example, Mesos may have a different runtime than Cloud Foundry than, etc. etc. And I think that is a reasonable place to end up. But we need to make sure we can agree on that plumbing piece that's kind of the thing we are trying to do with the App Container thing. That's a great question. Early on we tried to get the stand-alone mode into Docker and Red Hat contributed a bunch of things that kind of go down that path and it kind of kept getting delayed. That was one of the reasons. The heritage of the code was we started it about two or three weeks was the first commit before we put it up on our blog post. It uses a bunch of libraries that we had already written for Fleet and for some of our other tools. We kind of put together the prototype and the specification over that time period, its definitely a raw project its a brand new raw open source project and it is progressing pretty well. Right now, we are definitely on the alpha side of the development process. What we are doing with Rocket is fairly straight-forward and well scoped. We're not trying to create a lot of software here and we want to leverage as much stuff as we can. So hopefully early next year (2015) we can get to the point where it is beta and start testing it out with some of the use cases that we have in mind. We'll see how it goes, you always underestimate how hard software is. The first place we can definitely work together is on the App Container spec and the image format. Over time, the goals of the projects will look less similar, which is perfectly reasonable. We want this tiny little package manager plus container runtime and the Docker project is moving more towards a platform, and that's a reasonable thing too. With CoreOS we want try to ship as little stuff as possible so its a slight difference of end goal. The big exciting one is that etcd has been maturing quite nicely, we have projects like Flynn and Deis that rely on etcd in a pretty big way inside those projects. We have made a big investment in the stability of etcd and, not just the stability but the operationability (if that's a word). We have had all this experience with users interacting with the system and for the most part it has worked as designed, but there are definitely some sharp corners that people have been stubbing their toes and cutting themselves on. Etcd now has a lot of nice tooling in these alpha releases for 0.5 that we have been doing for adding and removing hosts and for making sure that configuration errors don't actually end up in cluster failures. We added all these unique UUID's all over the place that we can detect \"hey you're adding a member from an old cluster or from this other cluster and you can't bridge two clusters\" and reject those sorts of membership changes. And detect changes and faults for hard disk and that sort of thing where we found people would have a log file that had a random bit flip, and its very hard to debug so we added checksums and that sort of stuff. So, we are really focusing so people can detect and recover from mistakes.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "ephemeral-containers-in-a-microservices-workflow", "author": ["\n              Ross Jimenez\n            "], "link": "https://www.ctl.io/developers/blog/post/ephemeral-containers-in-a-microservices-workflow", "abstract": "One of the exciting patterns we have been utilizing and working on within the Panamax project is the ability to use Docker containers to perform a functional workflow or a series of asynchronous tasks in the background. Recently, I ran across this interesting post on the Iron.io blog: The Ephemeral Life of Dockerized Microservices , which inspired me to write this post. Although I was already familiar with the Iron.io Worker service it was great to see how they were using containers, specifically short lived or ephemeral containers, to accomplish their service. We have been using ephemeral containers as well, albeit somewhat differently, as a series of functional containers in a workflow. These functional workflows can be triggered on an event or scheduled and allow work to be done on-demand in the background. In our case, the workflow required initial steps to be completed and the output provided to later steps in order for the complete workflow to be successful. In the Panamax project, we want to help our users setup a remote cluster on a cloud provider so that they can easily deploy Panamax templates to them. Although we have the capability today to deploy templates to remote infrastructures, the setup of the infrastructure and the installation of our Remote Agent/Adapter are all separate processes. Lets run through an example of what it would take to automate this use case end-to-end. This is a simplification but assume there are three basic steps to accomplish this capability: Now let's say the logic for each one of these steps is encapsulated in a Docker container so that each container is responsible only for a specific step. We could fire up these containers in parallel, but are limited by the fact that each step needs to get some data from the previous step in order for it to execute successfully (e.g. we can’t install the orchestrator in step 2 until we know the IP address of the server created in step 1). In trying to address this use case, we set out to build a general purpose framework that we could use to orchestrate container-based workflows like the one described above. Among other things our framework would need to: One of the more interesting problems to solve was how to move data between containers. The solution that we settled on was to use a Unix piping model where we would attach to the standard output stream of the running container, capture any output, and then write that data to the standard input stream of the next container in the chain. As each subsequent step in the workflow is executed, the output of the previous step becomes the input to the next step. Initially we're going to bundle this framework into Panamax, but we'll soon be releasing it as a separate stand-alone project to help others implement this similar pattern. Let’s talk about some of the advantages of using Containers in a functional workflow: Given each step in our process is decoupled it has been easy for us to reuse each logical unit. This is one of the exciting aspects of Microservices architecture: the ability to have composable units of work that can be re-ordered, easily modified and extended. Although, we are writing all our worker containers in GoLang in reality each container is a black box and therefore developers can choose any implementation language they wish. This is the same concept as Panamax's Remote Agent/Adapter model , which is also encapsulated in containers. Hopefully, now you have some idea as to how such a pattern can be applied. Although the example that was discussed solves a very specific use case, there are much more exciting possibilities for the pattern. The ability to truly build on-demand sensor driven or event based architectures excites me tremendously and can have a very broad impact on how computing resources are consumed. If Docker containers allow you to start a container sub-second, then how solutions are architected can be totally rethought in many cases to be more event driven and on-demand. This means less resource intensive solutions, as compute resources would only be needed for the time the ephemeral container exists. There is also the potential to provide functional libraries of containers that could easily be visually composed into all sorts of interesting workflows by less-technical users. Sensor containers could query APIs where Actuator containers could do units of work. Finally, although we are only doing basic sequential workflows once you add in complex workflows and some simple logic/business rule definition along with the workflow schema you can truly create some unique solutions. Combine all these possibilities with the 'run anywhere' ability of a container itself and the possibilities are truly exciting. How are you using microservices? Have you been able to use containers as more than just holders of application stacks ? We would love to hear from you or please jump in and get involved in one of our many open source projects .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "small-docker-images-for-go-apps", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/small-docker-images-for-go-apps", "abstract": "Over the past year we've written a few articles about optimizing your Docker images -- usually with an emphasis on creating the smallest possible image. Unfortunately, when using an interpreted language like Ruby (which has been used for many of the CenturyLink projects) there is only so much fat you can trim from your images. To run a Ruby application in a container you still need the Ruby interpreter and a whole host of OS packages and Ruby Gems installed. Even with our best effort to optimize the images, our Docker-packaged Ruby applications often weigh-in at 400+ MBs. We've now started to use Go for some of our latest projects which, among other things, gives us the ability to package our apps into really compact images. With a Go application, you can compile your code into a self-contained, statically-linked binary that has absolutely no external dependencies. Your application code along with the Go runtime and any imported packages are all compiled together into one binary. The primary benefit of a statically-linked binary is that it allows you to deploy your application by simply copying the binary. When deploying with Docker this means you can build an image for your application that contains nothing but the app itself. You can go from using ubuntu (130 MB) or debian (85 MB) as your base image to something like busybox (2 MB) or even scratch (0 bytes). If you take your 4 MB, statically-linked Go binary and package it in a Docker container using scratch as your base you'll end-up with a 4 MB image -- that's 1/100th the size of our packaged Ruby app! After playing with a few Go applications and reading Adriaan de Jonge's blog post \" Create the Smallest Possible Docker Container \", we thought it would be interesting to see if we could create a general purpose tool that could take any Go project and turn it into a slim image. The result of that work is golang-builder . golang-builder is itself a containerized app that can compile Go code into a static binary and package it in a Docker image. Let's say we have a basic \"Hello World\" Go application: and a corresponding Dockerfile : To package this application in a Docker image, we would invoke the golang-builder as follows: Assuming that we're currently in the directory containing our hello.go source file and Dockerfile , the first -v flag will mount our project directory into the golang-builder container as /src . The script inside golang-builder is hard-coded to look for your Go code at /src . The second -v flag mounts the Docker API socket into the container. Since the golang-builder code needs to interact with the Docker API in order to build the final image, it needs access to /var/run/docker.sock . The golang-builder will set-up the appropriate GOPATH for your project, resolve your dependencies, compile your code and then issue a docker build against your project's Dockerfile . The end result should be a new image in your Docker image list: If you're only interested in the statically-linked binary, you can omit the volume mount for the Docker socket and golang-builder will stop after compiling your application: If you're interested in learning more take a look at the golang-builder page on the Docker Hub or check out the source on GitHub .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "automated-deployment-endpoint-creation-with-panamax", "author": ["\n              Pat Cox\n            "], "link": "https://www.ctl.io/developers/blog/post/automated-deployment-endpoint-creation-with-panamax", "abstract": "Panamax is happy to introduce a powerful new feature, automated remote agent cluster creation in version 0.2.12 of the UI and API projects. Using Dray (Look for a future blog post on Dray soon!), you can now create a remote cluster on a cloud provider directly from Panamax, while having the Panamax Remote Agent and appropriate service adapter pre-installed. By only collecting a few specifics around your cloud provider account, Panamax can create the cluster with the orchestrator pre-installed and ready for deployment – all in one click! Note: Based on the cloud provider, this process can take several minutes. Feel free to continue your work in Panamax and come back to this screen to check its progress . Note: If you see an error during the remote deployment process, be sure to check the logs to diagnose the error. Panamax does not delete any VMs created during the process, so please visit your cloud provider to delete any VMs or artifacts to avoid unwanted charges. Note: In order to reach your deployed application, additional ports may need to be opened on your cloud provider . In our initial release, Panamax supports CenturyLink Cloud with Kubernetes and AWS for cluster creation. In the coming weeks we plan on adding more cloud providers and support for additional orchestrators. Be sure to check out our upcoming builds for these additions.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "creating-a-panamax-adapter-in-go", "author": ["\n              Gary Paige\n            "], "link": "https://www.ctl.io/developers/blog/post/creating-a-panamax-adapter-in-go", "abstract": "Learn how to Create a Remote Adapter Our remote adapter model was designed so developers could use their preferred language, but so far, all of the remote adapters were written in Ruby. We recently released a Marathon Adapter using Go. After creating the Marathon Adapter, we split the codebase to provide developers an easy integration to their favorite orchestrator using Go. This article will explain the process of using the Panamax Adapter project to build a remote adapter. Everything you need to create an adapter is outlined in the Adapter Developer Guide . An adapter must implement the REST interface methods described in the developer guide and be deployed within a container so the remote agent can communicate with it. The Panamax Adapter library provides a server which already implements the adapter REST contract and an interface for developers code. Therefore, a development team using the Panamax Adapter has only two steps to create a working adapter. Within the Panamax Adapter library an interface is provided which has the CRUD operations defined in the developer guide. A structure which implements these methods enables the integration between the Panamax Service definition and the orchestration system. Once the interface has been implemented the final step is to hook it into the server. This simply is providing the adapter as a parameter to the server. Once the previous tasks are complete and the adapter build successfully its time to create an image for deployment. The standard process will create an image of at least 500MB size using a Go base image. However, you can create the smallest image possible (~5mb) using the Golang Builder process outlined in the article Small Docker Images for Go Apps . Building an adapter to any orchestration framework does not require learning a new programming language, because the architecture relies on containers. The labs team created the Marathon Adapter using Go to demonstrate this powerful architectural advantage. Using the Panamax Adapter library and the Golang Builder process it is very easy for teams to create a small efficient Golang container for the adapter they require. A sample using the points in this article can be found here: https://github.com/CenturyLinkLabs/sample-go-adapter . Don't have a CenturyLink account? No problem. Just head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "setting-up-a-simple-docker-dev-environment", "author": ["\n              Laura Frank\n            "], "link": "https://www.ctl.io/developers/blog/post/setting-up-a-simple-docker-dev-environment", "abstract": "Using Docker, your applications sit atop an excellent platform for packing, shipping and running low-overhead, isolated execution environments -- so it's no wonder that many teams are choosing Docker for deployment. But your deployment choices directly affect your development workflow, because your Dev, Staging, and Prod environments should match each other as closely as possible. No matter where you are on the stack, if your apps are deployed inside a container, you should get to know how to use containers in your development environment. The great news is that Docker makes it easy. In this tutorial, we'll be working on developing a simple Sinatra application. If you'd like to try it yourself, you can pull down the code at https://github.com/rheinwein/hello-world-container-demo . Before getting started, make sure that you're able to run Docker in your environment. On Linux, simply install Docker via the official packages . On a Mac, I recommend using Boot2Docker , a tool that will fire up a Tiny Linux VM and make working with Docker extremely easy. There are two different ways of developing with Docker. First is what I refer to as the static container way: running code inside a container that does not need to be modified during the course of development. Most likely, you'll use this style for dependencies, and also during QA and testing. In this type of container, the code is packaged up and you can't modify it after the image is built and the container is running. The dynamic container way is what you'll use when working on an application under active development. Instead of running an image with the code packaged up, you'll link the folders from your dev environment to the container by mounting them as a volume. You can work on the file on your local system and see the changes propagate all the way through the container. For the sake of simplicity, we'll look at the simplest way first: the static way. These types of images, when run in a container, provide an immutable service. You can interact with it, but not change the code. You'll want this type of service for dependencies, and you'll likely build your application in this way when you're ready to test and ship. Here is a Dockerfile for a static Sinatra application that prints out \"Hello World\" in the browser. We'll break down the important parts of this Dockerfile: RUN mkdir -p /usr/src/app -- make a new directory ADD . /usr/src/app -- copy all of your current files into this directory WORKDIR /usr/src/app -- set this new directory as the working directory RUN bundle install -- execute your bundle install within the app directory The ADD instruction copies everything in the current directory into the /usr/src/app directory. Once the copying is done, you won't be able to modify this code. Running docker build -t hello-world . will build this image on your host. You will see each layer of the image being downloaded. You can check to see that your image was downloaded successfully by running docker images . Next, run this image inside a container. The -p flag sets up a port binding rule of -p host_port:container_port . If you're working with a VM (like Vagrant, etc.) remember to also set up a port forwarding rule on your VM in order to access the application from your host. The -p flag only creates a rule from the container to the container's host system, which, if you're not running Linux on your local machine, is most likely a small VM. If you're using Boot2Docker, you can either set up a port forwarding rule to access the application on localhost:XXXX, or you can use the Docker host address given to you during setup. After the port rules are set, you can access your application in your browser at localhost:4567. Alternatively, and probably the most common use case for the static way, you will pull down dependencies from the Docker Hub and run them in your application. Most common dependencies, like databases, can easily be download from the Docker Hub and run in your development environment. But what about the parts of your application that need to change? It's not practical to rebuild and run the image every time you make a change. Instead of packaging the code up inside the container, the dynamic way of creating a container will allow you to access and modify your code, and see those updates propogated to your browser. If you've already built the previous hello-world image, great news! You have a ruby-base image, which is all you need to run a container in this way. If you haven't built a Docker image yet, modify the Dockerfile so that it only has one line. Alternatively, you could just pull down centurylink/ruby-base:2.1.2 by saying docker pull centurylink/ruby-base:2.1.2 That's it! The only thing this image does is creates an Ruby environment for your application to run in, using version 2.1.2. To get the code into the container, there are a few config options that need to be set on the docker run string. The -it flag allocates a tty for interactive sessions, which we'll need in order to bundle install and fire up the application. Unlike the previous static way, which had the container start command (CMD) specified in the Dockerfile, we have specified /bin/bash as the container entrypoint command. When the container is up, you'll be dropped into a bash session automatically. Think of this dynamic container as just a normal dev environment inside a container -- you still need to execute all of the commands needed to get your application running. An important note about the volume mount: just like with the ports, there could be an extra hop in here if you're working on a VM. Make sure that /PATH/TO/CODE is the correct directory on the VM, not your local machine. In some cases, like Boot2Docker, the filepath will be identical. You'll see pretty quickly if you made a mistake as your container directory will be empty. Once inside the container (you'll see root@container_id as the prompt), cd into the code directory, in this case /var/app/hello-world. From here, bundle install. Then start the application with ruby hello_world.rb . If you're working with a VM (like Boot2Docker) remember to also set up a port forwarding rule on your VM in order to access the application from your local host. You have to make two jumps: one from the container to the VM (the -p flag) and then one from the VM to your host. Make sure to avoid any port conflicts from previous containers or projects (you may want to use a different port just to make it easier on yourself). Check out the application in your browser and you should see \"Hello World!\". On your local system, go modify public/index.html. You'll see the changes after a page refresh. That right there is you modifying code that's running in a container! This dynamic container concept can be applied to much larger projects as well, and even multiple projects running inside containers. For example, you could run both a UI and API project in a container -- using container linking, port forwarding, and environment variables to get them to communicate -- and actively develop against both projects.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "dray-docker-workflow-engine", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/dray-docker-workflow-engine", "abstract": "Most of the Docker use cases involve the use of containers for hosting long-running services. These are things like a web application, database or message queue -- services that are running continuously, waiting to service requests. Another interesting use case that we've been playing with here at CenturyLink Labs is leveraging Docker to execute short-lived, single-purpose tasks (in fact our own Ross Jimenez published an article titled Using Ephermeral Containers in a Micro-Services Workflow a few weeks back that discusses this exact thing). We've published a few of these Docker-wrapped tasks in the past: All of the benefits that apply to long-running services (portability, isolation, etc...) also apply when wrapping one-off tasks in Docker images. Both of the examples above happen to have been implemented in Ruby but the fact that they're wrapped in Docker images means that no one has to worry about running the correct version of the Ruby interpreter or installing our various dependencies. If you've got Docker, you can execute either of these utilities with a simple docker run . Once you've started to wrap tasks like these in containers you may find it useful to string them together and have the output of one container feed the input of the next container. Something like the capability provided by Unix pipes: The project we're announcing today, Dray , does exactly that -- it's like Unix pipes for Docker containers. Dray is a Go application that exposes a RESTful API for managing the execution of task containers. Tasks are grouped together in a job and posted to Dray as a JSON document: The example above is pretty simplistic but it describes a job which consists of three steps. Each step references the name of a Docker image to be executed. When receiving this job description, Dray will start a container from the \"centurylink/randowrd\" image. As the container is executing Dray will capture any data written to the container's stdout stream so that it can be passed along to the next step in the list. Once the \"randword\" container exits, Dray will start the \"centurylink/upper\" container and pass any data captured in the previous step to that container's stdin stream. Dray will continue executing each of the steps in this manner, marshalling the stdout of one step to the stdin of the next step, until all of the steps have been completed. The log output from Dray for the job shown above would look something like this: In between all the container management messages can see the the output that is generated by the containers themselves. The \"randword\" container outputs a string to stdout which is picked-up by Dray and passed to the \"upper\" container. The output from \"upper\" is an upper-cased version of the string that it received as input -- this is captured by Dray and fed into the \"reverse\" container which simply reverses the string. Again, this is a fairly useless example, but it serves to illustrate the data marshalling that Dray provides. We've been using the Dray engine for a little while now to power some of the features in our Panamax project. One of the newer features allows users to provision server clusters on a number of different cloud providers and automatically install the Panamax agent software. This provisioning process requires a number of different steps including the provisioning of servers, the configuration of networks and installation of software. Some of these steps are provider-specific while others are provider-agnostic. The approach we took was to implement each of the discrete steps as an image-wrapped task and then create a set of Dray jobs that composed different combinations of these tasks for the different cloud-providers. The Dray engine coordinates the execution of the jobs and handles the marshalling of data from one step to the next (e.g. the IP addresses of the servers created in one step need to be passed to the step that does the software installation). This is just one application of Dray, but we suspect that there may be other interesting ways to use it. If you want to give it a spin, you can pull the centurylink/dray image from the Docker hub. This is just our first release and aren't entirely sure where we're going to take it next, but we'd love to hear from you if you come up with an interesting use case or want to contribute to the code (it's all available on GitHub ).", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "dockerfile-add-vs-copy", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/dockerfile-add-vs-copy", "abstract": "This blog post will help you understand the differences between two similar Dockerfile instructions – ADD and COPY – how they became what they are today, and our recommendation on which instruction you should use.  (Hint: It's not ADD) When building Docker images from a Dockerfile you have two instructions you can choose from to add directories/files to your image: ADD and COPY. Both instructions follow the same basic form and accomplish pretty much the same thing: In both cases, directories or files (the <src> ) are copied and added to the filesystem of the container at the specified <dest> path. So if both instructions are equivalent, why do they both exist and which one should you use? Read on to find out. If you're not interested in the nuances of ADD and COPY and just want an answer to \"which one should I use?\", all you need to know is: use COPY. Unlike the COPY instruction, ADD was part of Docker from the beginning and supports a few additional tricks beyond simply copying files from the build context. The ADD instruction allows you to use a URL as the <src> parameter. When a URL is provided, a file is downloaded from the URL and copied to the <dest> . The file above will be downloaded from the specified URL and added to the container's filesystem at /tmp/main.go . Another form allows you to simply specify the destination directory for the downloaded file: Because the <dest> argument ends with a trailing slash, Docker will infer the filename from the URL and add it to the specified directory. In this case, a file named /tmp/bar.go will be added to the container's filesystem. Another feature of ADD is the ability to automatically unpack compressed files. If the <src> argument is a local file in a recognized compression format (tar, gzip, bzip2, etc) then it is unpacked at the specified <dest> in the container's filesystem. The command above would result in the contents of the foo.tar.gz archive being unpacked into the container's /tmp directory. Interestingly, the URL download and archive unpacking features cannot be used together. Any archives copied via URL will NOT be automatically unpacked. Clearly, there is a lot of functionality behind the simple ADD instruction. While this makes ADD quite versatile it does NOT make it particularly predictable. Here's a quote from an issue that was logged against the ADD command back in December of 2013: Currently the ADD command is IMO far too magical. It can add local and remote files. It will sometimes untar a file and it will sometimes not untar a file. If a file is a tarball that you want to copy, you accidentally untar it. If the file is a tarball in some unrecognized compressed format that you want to untar, you accidentally copy it. - amluto The consensus seemed to be that ADD tried to do too much and was confusing to the user. Obviously, no one wanted to break backward compatibility with existing usage of ADD, so it was decided that a new instruction would be added which behaved more predictably. When version 1.0 of Docker was released the new COPY instruction was included . Unlike ADD, COPY does a straight-forward, as-is copy of files and folders from the build context into the container. COPY doesn't support URLs as a <src> argument so it can't be used to download files from remote locations. Anything that you want to COPY into the container must be present in the local build context. Also, COPY doesn't give any special treatment to archives. If you COPY an archive file it will land in the container exactly as it appears in the build context without any attempt to unpack it. COPY is really just a stripped-down version of ADD that aims to meet the majority of the \"copy-files-to-container\" use cases without any surprises. In case it isn't obvious by now, the recommendation from the Docker team is to use COPY in almost all cases. Really, the only reason to use ADD is when you have an archive file that you definitely want to have auto-extracted into the image. Ideally, ADD would be renamed to something like EXTRACT to really drive this point home (again, for backward-compatibility reasons, this is unlikely to happen). OK, but what about fetching packages from remote URLs, isn't ADD still useful for that? Technically, yes, but in most cases you're probably better off RUNning a curl or wget . Consider the following example: Here we have an ADD instruction which retrieves a package from a URL followed by a RUN instruction which unpacks it, builds it and then attempts to clean-up the downloaded archive. Unfortunately, since the package retrieval and the rm command are in separate image layers we don't actually save any space in our final image (for a more detailed explanation of this phenomenon, see my Optimizing Docker Images article). In this case you're better off doing something like this: Here we curl the package and pipe it right into the tar command for extraction. This way we aren't left with an archive file on the filesystem that we need to clean-up. There may still be valid reasons to ADD a remote file to your image, but that should be an explicit decision and not your default choice. Ultimately, the rule is this: use COPY (unless you're absolutely sure you need ADD). Don't have a CenturyLink account? No problem. Just head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "dallas-lxc-meetup-sneak-peak", "author": ["\n              Mike Arnold\n            "], "link": "https://www.ctl.io/developers/blog/post/dallas-lxc-meetup-sneak-peak", "abstract": "In a recent presentation to a Dallas Meetup, Mike Arnold and Gary Paige shared some basic Docker information along with demonstrations of existing and upcoming projects we're working on in CenturyLink Labs. On Thursday, March 19, 2015, we travelled up from Houston to Dallas to present to the Linux Containers & Virtualization Meetup . We were invited by Sunday Ogwu , the Meetup organizer, and spent a delightful couple of hours with him and several other folks talking about Docker, Panamax, and other CenturyLink Labs projects. Our presentation started out with a discussion about the basics of Docker as it relates to architecting microservice applications. We demonstrated how to create and run Docker images and containers, but one of the things people found interesting was how Docker builds images by layering the file system. We thought it helped people understand this conceptually by seeing the layers with the Docker CLI history command. With this command you can see how any image was built simply by executing docker history <image name> Optionally passing the -q flag will display only the image ids, while passing the --no-trunc flag will display every bit of information available including the full instruction executed to create each layer. Here’s what is shown when we execute that command on the image built with the Dockerfile we used in our demo. Note that the first 7 lines are the instructions from the Dockerfile in reverse. So what then are the next 9 lines? Those are the instructions used to build the image from which our image was built and the image from which it was built and so on all the way down to the scratch image. This image layer information drives one of the next projects we’ll be releasing aptly named ImageLayers . We gave a brief demo of the application to the Meetup attendees and it seemed to really drive home the understanding of how Docker builds up images with a layered filesystem and how you can take advantage of that by using base images. Next in our presentation we demonstrated the use of Docker Compose as a way to automate the management of micro-service applications. Here at CenturyLink Labs, we are big fans of Compose and look forward to seeing what the Docker team delivers, especially in regards to Swarm integration. We took this opportunity to show off another project we are currently working on called Lorry which aims to simplify using Compose. The last portion of our presentation focused on Panamax. Within Panamax, we use Dray to execute a workflow that creates a server cluster setup and installs the Panamax Remote Agent in environments like CenturyLink Cloud, AWS, or DigitalOcean. The last step in the workflow is the addition of the newly established cluster as a remote deployment target in Panamax. Subsequently, users can deploy applications working in their local Panamax installation straight to the cloud. It’s a thing of beauty, and if you haven’t experienced it yet, you should install Panamax and give it a try. A heartfelt “Thank you” goes out to Sunday and all those who could attend the Dallas Linux Containers and Virtualization Meetup. We appreciate having been invited to present and look forward to seeing you all again soon. Microservice Composition with Docker and Panamax from Michael Arnold", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "pmxcli-a-cli-for-panamax-remote-deployments", "author": ["\n              Don Petersen\n            "], "link": "https://www.ctl.io/developers/blog/post/pmxcli-a-cli-for-panamax-remote-deployments", "abstract": "Late last year we released Panamax Remote Agent deployments, which allow you to deploy Panamax templates to your orchestrator of choice. This was followed by automated endpoint creation , which provisions the Agent on many cloud providers from within Panamax itself. An Agent generates a unique token that can be used for deployments by anyone you trust, but until now that required your collaborators to install Panamax and its dependencies. Today we are announcing pmxcli , a stand-alone command-line utility for interacting with a remote agent. It offers the basic remote deployment features of Panamax in a smaller download free from all dependencies. If you already have Panamax, updating your installation will automatically give you pmxcli . If you haven't installed Panamax, you can find the instructions here . If you'd like pmxcli stand-alone, 64-bit binaries for OSX and Linux are available for direct download. Remember that you'll need to give yourself execute permissions after the file is downloaded, and you'll need to place it somewhere in your PATH (we'd suggest /usr/local/bin ) if you'd like to run it from anywhere. Before you can do anything with pmxcli , you'll need a Remote Agent. The simplest way is still to install Panamax and let it do the work for you. Please note that if you have a Remote Agent from before Agent Installer release 0.1.3, you can still use the pmxcli , but you should read the \"SSL Warnings\" section of the README . Once that is finished, there are two resources that can be managed in pmxcli : remotes and deployments. Remotes are the Panamax Agents you have installed, and deployments are the applications that are currently deployed on any one of those agents. You can get an exhaustive list of commands and help by running pmxcli with no arguments, but here are the basics to get you started. First, you should add a remote. Remotes must be created in the Panamax web interface, and their tokens can now be downloaded as files in the latest release of Panamax: You'll reference that file in your add command: Your first remote is automatically made active. The active remote will be the one whose deployments you'll be interacting with when you run any pmxcli deployment commands. You can deploy any Panamax template, both existing ones you've downloaded from the public templates repository , or those you create yourself: Run pmxcli deployment help for a list of commands to interact with deployments. If you're developing your own Panamax Adapter or troubleshooting unexpected behavior, pmxcli includes a --debug global flag that will log all requests and responses from the Remote Agent. If you already have Panamax, update your installation and give pmxcli a try! Or, if you were wary of installing Panamax locally, pmxcli just might be your gateway to take advantage of Panamax's application template and remote agent features.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "gracefully-stopping-docker-containers", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/gracefully-stopping-docker-containers", "abstract": "Much of the focus of Docker is on the process of packaging and running your application in an isolated container. There are countless tutorials that explain how to run your application in a Docker container, but very few that discuss how properly stop your containerized app. That may seem like a silly topic -- who cares how you stop a container, right? Well, depending on your application, the process by which you stop your app could be very important. If your application is serving HTTP requests you may want to complete any outstanding requests before you shutdown your container. If your application writes to a file, you probably want to ensure that the data is properly flushed and the file is closed before your container exits. Things would be easy if you simply started a container and it ran forever, but there's a good chance that your application will need to be stopped and restarted at some point to facilitate an upgrade or a migration to another host. For those times when you need to stop a running container, it would be preferable if the process could shutdown smoothly instead of abruptly disconnecting users and corrupting files. So, let's look at some of the things you can do to gracefully stop your Docker containers. There are a number of different Docker commands you can use to stop a running container. When you issue a docker stop command Docker will first ask nicely for the process to stop and if it doesn't comply within 10 seconds it will forcibly kill it. If you've ever issued a docker stop and had to wait 10 seconds for the command to return you've seen this in action The docker stop command attempts to stop a running container first by sending a SIGTERM signal to the root process (PID 1) in the container. If the process hasn't exited within the timeout period a SIGKILL signal will be sent. Whereas a process can choose to ignore a SIGTERM, a SIGKILL goes straight to the kernel which will terminate the process. The process never even gets to see the signal. When using docker stop the only thing you can control is the number of seconds that the Docker daemon will wait before sending the SIGKILL: By default, the docker kill command doesn't give the container process an opportunity to exit gracefully -- it simply issues a SIGKILL to terminate the container. However, it does accept a --signal flag which will let you send something other than a SIGKILL to the container process. For example, if you wanted to send a SIGINT (the equivalent of a Ctrl-C on the terminal) to the container \"foo\" you could use the following: Unlike the docker stop command, kill doesn't have any sort of timeout period. It issues just a single signal (either the default SIGKILL or whatever you specify with the --signal flag). Note that the default behavior of the docker kill command is different than the standard Linux kill command it is modeled after. If no other arguments are specified, the Linux kill command will send a SIGTERM (much like docker stop ). On the other hand, using docker kill is more like doing a Linux kill -9 or kill -SIGKILL . The final option for stopping a running container is to use the --force or -f flag in conjunction with the docker rm command. Typically, docker rm is used to remove an already stopped container, but the use of the -f flag will cause it to first issue a SIGKILL. If your goal is to erase all traces of a running container, then docker rm -f is the quickest way to achieve that. However, if you want to allow the container to shutdown gracefully you should avoid this option. While the operating system defines a set list of signals, the way in which a process responds to a particular signal is application-specific. For example, if you want to initiate a graceful shutdown of an nginx server , you should send a SIGQUIT. None of the Docker commands issue a SIGQUIT by default so you'd need use the docker kill command as follows: The nginx log output upon receiving the SIGQUIT would look something like this: In contrast, Apache uses SIGWINCH to trigger a graceful shutdown : According to the Apache documentation a SIGTERM will cause the server to immediately exit and terminate any in-progress requests, so you may not want to use docker stop on an Apache container. If you're running a third-party application in a container you may want to review the app's documentation to understand how it responds to different signals. Simply running a docker stop may not give you the result you want. When running your own application in a container, you must decide how the different signals will be interpreted by your app. You will need to make sure you are trapping the relevant signals in your application code and taking the necessary actions to cleanly shutdown the process. If you know that you're going to package your application in a Docker image you might consider using SIGTERM as your graceful shutdown signal since this is what the docker stop command sends. No matter which language you're using, there is a good chance that it supports some form of signal handling. I've collected links to the relevant package/module/library for a handful of languages in the list below: If you're using Go for your application, take a look at the tylerb/graceful package which automatically enables the graceful shutdown of http.Handler servers in response to SIGINT or SIGTERM signals. Coding your application to gracefully shutdown in response to a particular signal is a good first step, but you also need to ensure that your application is packaged in such a way that it has a chance to receive the signals sent by the Docker commands. If you're not careful in how you launch your application it may never receive any of the signals sent by docker stop or docker kill . To demonstrate, let's create a simple application that we'll run inside a Docker container: This trivial bash script simply goes into an infinite loop, but will exit with a 0 status if it receives a SIGTERM. We'll package this into a Docker image with the following Dockerfile: This will simply copy our loop.sh bash script into an Ubuntu-based image and set it as the default command for the running container. Now, let's build this image, start a container, and then immediately stop it. If you're following along, you may have noticed that the docker stop command above took about 10 seconds to complete -- this is typically a sign that your container didn't respond to the SIGTERM and had to be forcibly terminated with a SIGKILL. We can validate this by looking at the container's exit status. Based on the handler we setup in our application, had our container received a SIGTERM we should have seen a 0 exit status, not 137. In fact, an exit status greater than 128 is typically a sign that the process was terminated as a result of an unhandled signal. 137 = 128 + 9 -- meaning that the process was terminated as a result of signal number 9 (SIGKILL). So, what happened here? Our application is coded to trap SIGTERM and exit gracefully. We know that docker stop sends a SIGTERM to the container process. Yet it appears that the signal never made it to our app. To understand what happened here, let's start another container and take a peek at the running processes. The important thing to note in the output above is that our loop.sh script is NOT running as PID 1 inside the container. The script is actually running as a child of the /bin/sh process running at PID 1. When you use docker stop or docker kill to signal a container, that signal is sent only to the container process running as PID 1. Since /bin/sh doesn't forward signals to any child processes, the SIGTERM we sent never reached our script. Clearly, if we want our app to be able to receive signals from the host we need to find a way to run it as PID 1. To do this we need to go back to our Dockerfile and look at the CMD instruction used to launch our script. There are actually a few different forms the CMD instruction can take. In our Dockerfile above we used the shell form which looks like this: When using the shell form, the specified command is executed with a /bin/sh -c shell. If you look back at the process list for our container you will see the process at PID 1 shows a command string of \"/bin/sh -c /loop.sh\". So the /bin/sh runs as PID 1 and then forks/execs our script. Luckily, Docker also supports an exec form of the CMD instruction which looks like this: Note that the content appearing after the CMD instruction in this case is formatted as a JSON array. When the exec form of the CMD instruction is used the command will be executed without a shell. Let's change our Dockerfile to see this in action: Rebuild the image and look at the processes running in the container: Now, our script is running as PID 1. Let's send a SIGTERM to the container and look at the exit status: This is exactly the result we were expecting! Our script received the SIGTERM sent by the docker stop command and exited cleanly with a 0 status. The bottom line is that you should audit the processes inside your container to make sure they're in a position to receive the signals you intend to send. Using the exec form of the CMD (or ENTRYPOINT) instruction in your Dockerfile is a good start. It's pretty easy to terminate a Docker container with a docker kill command, but if you actually want to wind-down your applications in an orderly fashion there is a little more work involved. You should now understand how to send signals to your containers, how to handle those signals in your custom applications and how to ensure that your apps can even receive those signals in the first place.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "imagelayers-io-docker-visualization-and-badges", "author": ["\n              Gary Paige\n            "], "link": "https://www.ctl.io/developers/blog/post/imagelayers-io-docker-visualization-and-badges", "abstract": "ImageLayers.io is a new project which provides a browser-based visualization of user-specified Docker Images and their layers. This visualization provides key information on the composition of a Docker Image and any commonalities between them . ImageLayers.io allows Docker users to easily discover best practices for image construction, and aid in determining which images are most appropriate for their specific use cases. Docker uses a union filesystem for managing images. Each layer of an image is the result of some action taken during the build. This could be installing a package, copying files from a local or remote source, or setting an environment variable. Each action creates a new layer in the filesystem; layers reference their parents and are cached, so they may be used as a part of an entirely different image. ImageLayers will show you these relationships as a tree. There is a direct correlation between your Dockerfile and the number of layers in your image. Docker offers some advice on Dockerfile best practices , and we have also published an article on how to best optimize Docker images . The layered filesystem is similar to a tree, and Docker is smart enough to branch from any layer if it sees the same work being done. For this reason, it's possible to consolidate work in order to optimize your images. There are two basic strategies for optimizing your Docker images: chaining commands and using smart base images. Here's a sample Dockerfile that pulls down a base image and installs a package. Because there is a 1-to-1 correlation between the Dockerfile and the layers in the image, this image would have many layers. A new layer is created for each of the RUN commands. Since each of the RUN commands above is related to the same package, we can chain them together and execute all of the commands on the same layers. That looks like this: The resulting image has fewer layers and therefore takes up less space than the first example. ImageLayers lets you see each layer and its instruction more readily, making it easier for you to identify resource-sucking parts of your Dockerfile. For example, the analysis of the wordpress:latest image shows several run commands which could be concatenated using the technique above. Using images with shared layers within your application will reduce download times and the overall filesystem size. A layer is only downloaded once regardless of how many images require it. Therefore, choosing a good base image for your application can be a performance boon. Using ImageLayers to investigate images on the DockerHub before they are downloaded is a quick way to find which layers may be shared. Within ImageLayers, a shared layer spans the image columns so it is easy to identify which images a layer may have in common. The more layers in common, the fewer requests are required to download the images, leading to better performance for your application. Don't be the last image on the Docker Hub to have an ImageLayers badge! Use the 'Get an Embed Badge' feature to create an html link or a markdown fragment which can be embedded into a document. ImageLayers was created to provide visual insight into the way an individual image is created and how many images can share layers. This information can be used to understand how the Docker unified file system behaves or fine tune an application composed of many images. Create an analysis of some of your favorite images and share the url with your friends.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-swarm-and-compose-with-ben-firshman-product-manager-at-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-swarm-and-compose-with-ben-firshman-product-manager-at-docker", "abstract": "Ben Firshman , Product Manager at Docker, Inc. Ben Firshman and his partner, Aamand Prasad, were the founders of Orchard. In the summer of 2014, Docker, Inc. acquired their 2-man startup. Ben and Aamand became Docker employees with the mission to create great developer experiences Docker users. Read more about it on the Docker blog . I'm doing good, it's a pleasure to be here, thanks for inviting me as well. It's quite flattering, I guess to sort of know you are doing the right thing, it's been very exciting. One of the main reasons we joined is we just started talking to each other and realized we actually had the same goals in mind, we wanted to build really great open source tools for orchestrating containers. They seemed to line up pretty well with what we had in mind, so we thought \"hey, we should team up\" and do this together. It's a little known fact that we started Orchard about six months before we started using Docker, I think. Sort of trying to solve this problem of how do you run development environments and get them through your app lifecycle and get them into production. We tried loads of technologies, sort of experimenting with containerization with really small VM's and things like that. And then Docker came along and we sort of realized, \"oh, this is the thing\". This was obviously getting traction and we should jump on this bandwagon because it is the right technology. It turned out to be the right decision. Since we picked Docker, that was about September 2013, then we got acquired in August so it was just under a year. Yep, that is correct. Orchard turned into what is now the Docker London office. We have a little office there where we are building little bits of orchestration tooling. It's only three of us for now, we are growing and building the team. Docker is huge. And it sort of grows every week. Last count I heard was 130 I think. Most of that is based in San Francisco, we've got a bunch of people on the east coast, a bunch of people in Europe and a little team in London, as well. I want to make really great tools for developers and admins to deploy their stuff. That is from doing that job myself in companies where I had both developer and sysadmin hats. And from watching people do this and it just kind of being a pain at the moment. Lots of tools which are doing really good jobs but there are sort of some missing bits in there. What we are trying to do is to fill some of those holes and trying to create a really great user experience, particularly for developers running development environments and stuff like that. There are a lot of similarities, obviously. Mesos was kind of the first thing that appeared, 2009/2010 I think. It was being built as an internal tool at Twitter to run Twitter and kind of inspired by some of the papers Google had published on their blog system. So didn't use any containerization at all, it was pre-containerization hype. It was intended to run processes on machines that were set up in the correct environment by things like Puppet and Chef. So it went very well if you had an application where you could predict its environment. Twitter ran a lot of Java apps as well, which was really easy because you just need Java installed on the machine. As long as you live in the JVM world it's a bit like having Docker containers. One of the things we are also building internally is a thing called \"Swarm\". What we are trying to do with Swarm is sort of say \"there are a lot of tools appearing that let you schedule containers and other things onto clusters\" and what we are saying is \"you don't have to choose between technologies\". What we are doing is building a thing that on one end speaks what is called the Docker Remote API, which is the thing on your computer that Docker uses to actually run containers. And exposing that API to a thing that can run containers on lots and lots of hosts but still talk the same API. That can be backed by a simple scheduler that we've included in Swarm if you want to get started really quickly. And it can also be backed by Mesos. So, the idea is that you can use all this Docker tooling you already have so things like your Docker client, tools like Shipyard, things like Compose (which is another of our tools that I can talk about in a bit) and you don't have to modify these but you can use them on top of Mesos or your laptop as well, depending on what environment you have. You don't have to write it specifically for these environments. [Ben demos Docker Swarm by sharing his screen]. Here is a Swarm that I prepared earlier. We have a tool called \"Machine\", you gave a nice description of Orchard at the start, which was that is was a service for getting a Docker host started very quickly so you could run Docker containers without worrying about computers and things like that. Orchard has sort of turned into a thing calle \"Machine\" now, which is just a way of really easily getting started with Docker so you can create Docker hosts easily for running containers. You can create them on VirtualBox or you can create them on Digital Ocean. Machine has an option called \"--swarm\", which lets you create Swarms. If you are interested in how to set up Swarms there is some really good documentation on Machines documentation. [Ben shows 'docker ls'] You can see some of the machines I created earlier. There is a node called \"swarm-master\" which exposes the Docker Remote API and sort of controls your cluster. And also another one called \"swarm-01\", which is a node, and you can imagine this could be any number of Swarm nodes. So we have two nodes, we could create \"swarm-02\" and \"swarm-03\" and grow our cluster that way. [Ben sets up the environment by setting environment variables in the shell pointing to the Swarm] Now what I can do is start running Docker commands against this. [Ben runs 'docker info'] This is a simple command that gives information about your Docker engine. We can see that this is kind of a special Docker engine where it says \"I'm a Docker engine that has 2 nodes\", which is \"swarm-master\" and \"swarm-01\". If I run containers against this Swarm cluster, let's say running nginx, which is an official Docker image... [Ben runs 'docker run -d -p80:80 nginx'] If I run nginx in the background, and I can expose a port here, what this is doing is that Swarm has received this API request from the Docker client to start up nginx. It then picked one of our nodes which has spare capacity and it has started nginx on it. Yes, there are a bunch of ways of configuring that logic that you can use. You can configure various strategies for Swarm. So Swarm can either try to binpack containers where there's enough capacity so it tries to fill up your nodes one by one. We can also just pick stuff randomly, or it can do a smart balance of that where it tries to balance it evenly but also picks nodes where there is some spare space. If you're feeling really adventurous you can also write your own strategies in Go and compile your own version of Swarm with those strategies. Yes, correct. As long as they are accessible by each other then they don't need to be on the same network. Through TCP on the standard Docker port and it is secured by TLS as well, in the same way the Docker engine can be secured by TLS. [Ben runs 'docker ps'] And we can all see it running. It's giving the IP address and we can also see its name. Normally this would just be a container name but it is also prefixed with the Swarm node it's been allocated to. This is the one bit where Swarm doesn't help you yet. If you link two containers together, Docker has this feature where you can link two containers together so they can talk to each other essentially, Swarm will automatically schedule them onto the same node. Because links don't work across hosts yet. So, if start up a container, then start up another container and link it to the other container it will schedule it to the same node. It's kind of a hack because we know that's kind of a limitation, that's something that we haven't sorted out yet. If you want you can roll this yourself so you can either make sure they are accessible on the same network like if you put it into the same Amazon EC2 network, for example, and use host networking and know what ports things are on, that will work absolutely fine. There's a bunch of other options with things like networking systems like Weave, which is a company building container networking. We are also working on this internally at Docker. A few months ago we acquired a company called SocketPlane and they were building networking for Docker and they are building some quite cool things there. We will have more to talk about soon but we are working hard on making networking work really well on Swarms. The thing called \"Compose\" which we built at Orchard, originally called \"Fig\", this was for running development environments locally. [Ben shows the contents of a 'docker-compose.yml' file] It defines web server and a redis server and how they link together. If I run this against my local computer... [Ben runs 'docker-compose up'] This is really handy for development environments where you are working on an application and you just want to run a bunch of services together. The neat thing about what Compose is that Compose is just a Docker client. So, Compose speaks to anything that opens up the Docker Remote API, including Swarm. This demo doesn't work with this Compose file because as we said before it requires a link. If this application didn't require links and it was a single service then you could run it on top of Swarm. And those things do work together and there is some good documentation on Compose that explains how this works. That's a good question. There are kind of two things I am excited about. I am excited about all these bits of technology that are essentially making people's lives easier. Like Compose has made loads of developer's lives so much easier by being able to run development environments and I think we can do the same with Swarm as well for people who just want to get an application up and running really quickly on their cluster. And when Compose and Swarm are working together that all works so well. Then I'm also excited about with Docker, as well as the little bits of technology that we're building, making building and deploying applications just work really well. Like be a really great user experience for developers and something that is not a pain to use and deploy applications, that is something I am excited about. Mesos specifically, the reason you might want to use Mesos is, we are quite open about the fact that we have only been building Swarm for a year and Mesos has been around for several years and runs Twitter. So if you are building on an enormous scale and still want to deploy Docker containers onto it then you might want to run your infrastructure on Mesos. Also, for the case where you've already set up a Mesos cluster, we've talked to lots of people who already have big Mesos infrastructure set up and Mesos has built-in Docker support as well. They kind of want to keep their huge Mesos infrastructure, obviously, since there is a lot of investment to set up but also be able to run Docker native applications on top of it. That's where Swarm comes in where Swarm essentially sits between Docker world and Mesos world and can let you use all of your Mesos infrastructure with Docker tools. Cool open source projects, that's a good question. I'm sure there is... but nothing that is on the top of my mind. It's been a pleasure to talk to you as well. What's ImageLayers? Oh yeah, that is really cool, actually. Have you seen RancherOS? One of the things I have seen lately is this company called Rancher Labs building an operating system called RancherOS. Which is a, it's a bit mad really in an exciting kind of engineering mad scientist way. This operating system which runs entirely on Docker. So PID 1 is the Docker engine and it essentially boots up your init system and all of your system services inside Docker containers. And, that's all it is. It's kind of similar to CoreOS but CoreOS uses a system called systemd and has a bunch of other processes running on the host system. Which is, honestly, probably a more sensible way to do it than RancherOS for a lot of things. But RancherOS is sort of experimenting with the idea of \"what if absolutely everything is inside of Docker containers?\" Everything you need on your operating system is inside Docker containers. It's really interesting, it's sort of an experiment at this stage but could be a glimpse of the future as well.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "lorry-io-pathway-to-docker-composable-apps", "author": ["\n              Rupak Ganguly\n            "], "link": "https://www.ctl.io/developers/blog/post/lorry-io-pathway-to-docker-composable-apps", "abstract": "Recently, Docker released the Compose tool for defining and running complex applications with Docker. The basic component of that tool is the file, docker-compose.yml. We at CenturyLink were big fans of Fig , the basis for Compose and the docker-compose.yml, but always envisioned a utility that could facilitate creating the 'docker-compose.yml' files intuitively. From that idea, we are happy to announce our latest project, Lorry.io , a docker-compose.yml validator, editor and composer. We were primarily writing the docker-compose.yml file by hand and had to keep going back to the docker-compose YAML reference to get the syntax right. We ran into issues with bad indentation, kept forgetting which keys had string vs sequence values, and found it hard to remember all the keys that were available.  We quickly realized that to create multi-container based applications with Docker Compose, a person not only needed an understanding of the YAML format itself but an understanding of the components and options available to create a valid docker-compose.yml. Additionally, once a docker-compose file was created manually there was not a simple way to validate what was created was actually valid besides running it. Recognizing that others could benefit from such a tool, we developed Lorry.io . Lorry allows you to import an existing docker-compose.yml file and validate it against a schema based on the rules provided by Docker. You can also create a docker-compose.yml file from scratch or see some examples of good and bad docker-compose.yml files. Once editing is complete the file can easily be copied, shared or downloaded. If you are working on a \"Dockerized\" application, you probably already have a docker-compose.yml file that you use to run your app. In that case, you can use Lorry to import your YAML file either by pasting the contents of the file or uploading the file from your local machine. Once you import your YAML file, Lorry performs schema validations on it. The YAML file is then displayed with line numbers and visually broken up into service blocks corresponding to each service in your app. If Lorry finds any issues, it will highlight the appropriate lines with errors. If you get an error, check the info (i) icon to understand what's wrong. In cases where the file has warnings related to the format of the values entered by the user, a squiggly underline with a popup explaining the warning message is displayed. After you have imported your YAML file and seen what Lorry has highlighted for you based on the validations, you can choose to edit the YAML file and fix the issues. You can edit one service block at a time by clicking the pencil icon from the action menu on the far right. You can also choose to delete the whole service block by clicking the delete (X) icon. Once you are in edit mode for a service block, you can see the errors with invalid keys highlighted in red. To fix the error, you can delete the key and its value by clicking the delete icon next to the item. You can add new key/value pairs by using the 'Add a key' dropdown. You can also add a new sequence value to an existing key, by clicking the appropriate '+ Add xxxxxx' link underneath the sequence. After you have made all your edits, you can save your changes or cancel your changes if you change your mind. You can also look for help by hovering over the info icon for each key. In case of a service made from an image (as opposed to being built from source with the 'build' key), Lorry provides a convenient feature to search for an image on the Docker Hub, and add it along with the appropriate tag. To use the YAML file that was created, you can either export and download it locally, copy it to the clipboard or save it as a Github Gist. As part of saving the YAML as a Github Gist, Lorry also provides a link to the YAML file that you can share with your friends. If you are new to Docker or new to running apps with Docker Compose, it is likely that you don't have a docker-compose.yml file yet. No worries. Lorry lets you create a docker-compose.yml file from scratch. Just select that option form the homepage. You can add as many service blocks as you want and then go through the edit/validate workflow as usual. If you are still not comfortable to start on your own, we have included some samples on the home page, that will showcase some of the basic uses cases. Feel free to peruse those samples, modify them to your needs and get started. If you have been using our flagship project Panamax , you will be delighted to hear that Lorry supports importing the Panamax templates that you have already created or the ones that are publicly available. Just use the 'Import PMX Template' tab in Lorry to import your PMX template file. Lorry will import the PMX template and convert it to a docker-compose.yml file, ready for use. Finally, as with all CenturyLink projects Lorry.io is open-source and licensed under the Apache 2 license. Please check out our Github project and submit a pull request; we welcome all contributions.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "is-from-scratch-the-root-of-all-docker-images", "author": ["\n              Gary Paige\n            "], "link": "https://www.ctl.io/developers/blog/post/is-from-scratch-the-root-of-all-docker-images", "abstract": "ImageLayers, a new project from the team at CenturyLink Labs, gives you the ability to inspect all the layers of an image, including ancestors. And of course, all images must begin with some primordial layer at the base. Generally this image is the Docker scratch image but it can also be a custom image. Using the ImageLayers tool we can demonstrate the various types of root images found in the Docker ecosystem. You can load the example images in your browser using this example link . The most common root image is the Docker scratch image, which is pulled into a Dockerfile using the FROM scratch instruction. In the photo above, all of the images except one (mysql) are derived from this scratch image. But that fact may be hard to observe because Docker has recently changed the way it reports the FROM instruction. Prior to Docker 1.5 an image's history would look like the following: The final line shows an image with size 0 and no command. This is the Docker scratch image which is also reflected in ImageLayers as a 0 size layer with the instruction FROM scratch . A change was made in Docker 1.5.0 which changes the reporting of the scratch image, but it's still there. The golang:latest was created with Docker 1.5.0 or later because the FROM scratch image has been omitted. This is shown in the docker history golang:latest report: The root image remains the 0 byte scratch image but Docker 1.5.0 made the following change: \"Dockerfile FROM scratch instruction is now interpreted as a no-base specifier.\" Therefore images will not show the 0 byte no command scratch layer if they have been created using Docker 1.5.0 or greater. In the history and in ImageLayers.io the last reported instruction will be the one after FROM scratch. In the golang case above the last reported command is the ADD file:96977352301efe982e It is possible to create a custom base image. The appcontainers/mysql:latest image shows a 271mb unknown instruction in ImageLayers. This instruction is the result of Creating a Base Image . Having a custom base image may allow a single layer as a base to improve application deployment by using the shared base. However creating a custom base is akin to squashing an image. It could reduce the size and number of layers but it prevents incremental changes. The merging of the layers possibly creates a single optimized image, but the entire merged image must be updated on changes rather than a single layer. The most common base for Docker images will be the FROM scratch image even if it is reported as a no-base specifier. The history of images which constitute a single image can be found using the docker history command or shown visually using the ImageLayers.io tool. Knowing the ancestry of images may not be an everyday need but when optimizing your images it can be useful to see what additional baggage is brought along.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-networking-rules", "author": ["\n              Laura Frank\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-networking-rules", "abstract": "If you've built any multi-container applications, chances are you've had to define some networking rules in order to allow traffic between your containers. There are several ways to do this: you can expose a port via the --expose flag at runtime, or include an EXPOSE instruction in the Dockerfile. You can also publish ports by using the -p or -P flags in the Docker run string. There's also container linking via --link . While these may achieve mostly the same results, they are different. So which one should you use? For reliability, use -p or -P to create specific port binding rules; treat EXPOSE as a documentation mechanism, and approach --link with caution. Before comparing the different approaches, let's learn about each individually. You can expose a port in two ways: either in the Dockerfile with the EXPOSE instruction, or in the docker run string with --expose=1234 . These are equivalent commands, though --expose will accept a range of ports as an argument, such as --expose=2000-3000 . However, neither EXPOSE nor --expose depend on the host in any way; these rules don't make ports accessible from the host by default. Given the limitation of the EXPOSE instruction, a Dockerfile author will often include an EXPOSE rule only as a hint to which ports will provide services. It is up to the operator of the container to specify further networking rules. Used in conjunction with the -P flag, which I'll get to a bit later in this article, this strategy for documenting ports via the EXPOSE command can be very useful. Essentially, EXPOSE or --expose is just metadata that provides information to be used by another command, or to inform choices made by the container operator. In practice, there is no difference between exposing a port at runtime or exposing it via an instruction in the Dockerfile. Running a container configured either way will yield the same results when looking at the networking configurations via docker inspect $container_id | $container_name output: We can see that the port is noted as exposed, but there are no mappings defined. Keep this in mind as we look at publishing ports. ProTip: Using the runtime flag --expose is additive, so it will expose additional ports alongside whatever EXPOSE instructions were specified in the Dockerfile. Instead of merely offering up a port, you can explicitly bind a port or group of ports from container to host using the -p flag. Note this is the lowercase p, not uppercase. Because this configuration does depend on the host, there is no equivalent instruction allowed in the Dockerfile; it is a runtime configuration only. The -p flag can take a few different formats: Essentially, you can omit either ip or hostPort, but you must always specify a containerPort to expose. Docker will automatically provide an ip and hostPort if they are omitted. Additionally, all of these publishing rules will default to tcp. If you need udp, simply tack it on to the end such as -p 1234:1234/udp . If I run a very simple application with the string docker run -p 8080:3000 my-image , whatever service running on my container on 3000 will be available on my host on 8080. The ports need not match, but you must be careful to avoid port conflicts in the case of exposing ports on multiple containers. The best way to avoid conflict is to let Docker assign the hostPort itself. In the same example as above, I could choose to run the container with docker run -p 3000 my_image instead of passing in a host port. In this case, Docker will select a port on my behalf. I can see what port was selected by running the command docker port $container_id | $container_name . Aside from docker port -- which will only display ports bound to the host while the container is running -- we can also see networking information by running docker inspect on the container and browsing around in the config. This is usually only interesting if there are port mappings defined. They will be in Config, HostConfig, and NetworkSettings. We'll use the information here in a bit to compare and contrast a few different styles of setting up networking between containers. ProTip: You can specify as many port mappings with -p as you fancy. To better understand the differences, let's run containers from images with different port settings. Let's run a very bare-bones application that simply echos 'hello world' when you curl it. We'll call the image no-exposed-ports : If you're playing along at home, make sure you're on the Docker host, and not using an intermediary like boot2docker. If you are using boot2docker, run boot2docker ssh before running any example commands. Note: that we'll run this container with the -d flag so that it stays running in the background. (It is worth noting again that port mapping rules only apply to running containers): There's really not much going on, other than the echo of the container's ID to let us know the service successfully started. As expected, there is no information to see in docker port no-exposed-ports or via docker inspect no-exposed-ports because we made neither a port mapping rule nor publish any ports. So, what happens if we do publish a port, and what's different between the -p flag and EXPOSE ? Taking the no-exposed-ports image above, we will add a -p flag at runtime, but NOT add any expose rule. Recall that the result of either an --expose flag or EXPOSE instruction is data in Config.ExposedPorts . Awesome! We see the available port. Also note that it defaulted to tcp. Let's snoop in the network settings to see what else is up. There are a couple interesting things in here: Notice that the entry in \"Config\" for an exposed port. It's exactly the same as when we just exposed a port via EXPOSE or --expose . Docker implicitly exposes a port that is published. The difference between an exposed port and a published port is that the published port is available on the host, and we can see that in both \"HostConfig\" and \"NetworkSettings\". All published ( -p or -P ) ports are exposed, but not all exposed ( EXPOSE or --expose ) ports are published. Ready for a little Docker sugar? Because EXPOSE is often used as a documentation mechanism -- that is, just to signal to the user what port will be providing services -- Docker makes it super easy to translate the EXPOSE instructions from the Dockerfile into specific port binding rules. Just add -P at runtime, and Docker will automatically create port mapping rules for you, and you are guaranteed to avoid port mapping conflicts. I've added the following lines to the same web application Dockerfile we used in the above examples: We'll build this image and tag it as exposed-ports. Then let's just run it with the -P flag, but not pass in any specific -p rules. We can see that Docker will map each of the ports associated with an EXPOSE instruction to a port on the host: Handy, right? You may have come across using the runtime flag --link name:alias for specifying a relationship in a multi-container application. While --link is a convenient flag, you can approximate nearly all of the functionality with port mapping rules and environment variables, if needed. Think of --link as a mechanism for service discovery instead of a gatekeeper for network traffic. The only additional thing provided by a --link flag is that it updates the /etc/hosts file of the consumer container (i.e. the one that has the --link flag) with the source container's host and container ID. Docker has a standard set of environment variables that are set with the --link flag, and you can find them in the docs if you're curious. While --link can be handy for smaller projects with an isolated scope, it functions mostly as a service discovery tool. If you are using any orchestration service in your project like Fleet, it's probably true that there will be some other service discovery tool managing relationships. That orchestration service may throw out your Docker links altogether in favor of whatever service is included with the tool. In fact, many of the remote deployment adapters used in the Panamax project do just that! Depending on who (or what other containers) consume the services you run with Docker, one networking option may be highly preferable. Remember that you can never know how someone might use your image if you publish it on the Docker Hub, so try to keep your images as flexible as possible. If you generally consume images from the Docker Hub, running containers with the -P flag is an easy and fast way to create port mapping rules based on the authors' suggestions. Remember that every published port is an exposed port, but the inverse is not true.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "create-a-compose-yaml-with-panamax", "author": ["\n              Pat Cox\n            "], "link": "https://www.ctl.io/developers/blog/post/create-a-compose-yaml-with-panamax", "abstract": "We are excited to announce a great new feature in Panamax: the ability to generate a Compose YAML from your Panamax application. Like our PMX template, a docker-compose.yml is used to define and run your multi-container applications with a single file. Docker Compose and the docker-compose.yml are Docker's standard for application deployment and Panamax now gives you the flexibility to leverage it directly. As a demonstration, run an image or PMX template in Panamax. For this example, we will use our trusty WordPress with MySql PMX template. Once you have deployed it locally, you are redirected to the application detail page. From here you can make any modifications to the application you like,i.e. open more ports, add environmental variables. When its setup like you want it, click the gear and select Save as Compose YAML. Panamax then auto-generates your docker-compose.yml from your application and displays it for you in a modal. From here you can copy it to clipboard, save as a local file or open it in Lorry , our docker-compose inspector and validator. That's it! With this feature, Panamax gives you even more flexibility on how you want to save your application for future deploying.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "running-a-paas-in-docker", "author": ["\n              Tommy Hughes\n            "], "link": "https://www.ctl.io/developers/blog/post/running-a-paas-in-docker", "abstract": "One of Docker’s greatest benefits is its ability to reduce complex software and application installations into standard, reusable images. It doesn’t get much more complex than deploying the open source Platform-as-a-Service, Cloud Foundry v2 . The CF-Mini image allows developers to deploy a functional Cloud Foundry v2 PaaS in 2-steps: Pull & Run with Docker. Note: Most of this blog entry focuses on the technical reasons for, and challenges around, bringing the CF-Mini Docker image to fruition. If this doesn't interest you and you just want to try it out, then click on the CF-Mini link above, read the documentation, and start playing around. Due to the size of the Cloud Foundry installation, a minimum of 6GB free memory is required. 16GB of total memory is recommended. There were many issues to work through on this project, but the following were probably the most significant & interesting to overcome. They also provided a lot in the way of learning Docker's capabilities and limitations. Long Docker builds, and failures 45 minutes into builds, made early development difficult. It didn't take long to realize that multiple Dockerfiles would solve many of my problems. As such, the following three images came to be: Here are some of the realized benefits of segmenting this way: A Docker image of 13GB or larger was what I was facing after the initial completed builds. This was problematic for a number of reasons, the biggest of which was it broke Docker Registry's Automated Build every time. I ultimately figured out how to fool the CF install into thinking some unnecessary installation packages still existed by truncating them. The stack wouldn't run if these packages were removed entirely. ImageLayers.io proved very helpful to me as a new Docker developer trying to shrink my image. The \"cf-mini.example\" domain doesn't exist, and it was chosen for this very reason. So how to make the CF installation believe it is real and resolvable without losing our Docker host's working nameservers? Dnsmasq and some scripting against resolv.conf during runtime did the trick within the container. Step-by-step client-side dnsmasq instructions for Ubuntu and Mac OS can be found on the cf-mini information page . One can't connect to CF from outside the CF-Mini Docker container without wildcard DNS resolution of this fake subdomain. After testing this image with aufs, btrfs, and overlayfs I settled on devicemapper for my storage solution. I'd still like to move to overlayfs one day when Docker fully supports it. Out of the box devicemapper worked well initially until the project's large images began to cause corruption over time. The solution was to enable \"Udev Sync\". Not only that, but devicemapper assigns a default Docker container filesystem size of 10GB. My image consumes most of that after startup leaving little left for application deployments to CF. This needed to be increased as well. The final solution is documented here and, once completed, results in the following output on one's Docker server: Every time a container is either created or started, a new internal container-side IP address is assigned. Cloud Foundry, especially in a Docker container, is heavily reliant on that IP for communication between core stack components (e.g. cloud controller, dea, health manager, etcd, router). In order to overcome this, I had to force cf-mini to reconfigure its CF install with the current IP address at container runtime. BOSH is core to the installation of Cloud Foundry v2. BOSH was designed, however, to leverage IaaS offerings directly in order to build, scale, and manage the CF stack. BOSH also, typically, requires its own Director server. Luckily, Nise BOSH (a lightweight bosh emulator) & CFv2 Installer offered me an alternate install option without having to abandon BOSH entirely. This proved to be the most difficult issue to overcome. For those not familiar with Cloud Foundry, warden is essentially CF's version of Docker... a container technology for isolating application & service deployments. So the challenge was to nest a different type of container within a Docker container. The following were some of the toughest of warden's requirements to solve: The ability to execute mount commands -> Warden requires extended Docker container privileges. Adding \"--privileged\" to the Docker run command accomplishes this. CGroups -> Now that the root user can mount, cgroups have to be reapplied in an alternate location (/tmp/warden/cgroup/) when starting the container. This is accomplished with the following scripted runtime commands: Modprobe & Warden storage -> When warden creates its containers, it first runs modprobe against Docker's host OS. In order to provide both the Docker and nested warden containers with the necessary libraries for this function, we have to mount a shared read-only volume at container runtime. This is accomplished by passing \"-v /lib/modules:/lib/modules:ro\" to the Docker run command. The project is very young, but I think it's a solid start to getting Cloud Foundry up and running without spending hours configuring BOSH and CF. If nothing else, this is a great demonstration of Docker's promise and has opened my eyes to the incredible potential of this emerging technology. Don't have a CenturyLink account? No problem. Just head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "ctllabs-goes-to-dockercon-2015", "author": ["\n              Rupak Ganguly\n            "], "link": "https://www.ctl.io/developers/blog/post/ctllabs-goes-to-dockercon-2015", "abstract": "It was an exciting time for us again to be attending DockerCon 2015 this year. It was the 2nd DockerCon, and was way bigger than the first one. Our whole CenturyLink Labs team was there, excited to hear the new announcements from Docker and the rest of the community. The opening keynote by Ben Golub, CEO of Docker, was followed by Docker's Founder & CTO, Solomon Hykes. The keynote session was followed by a variety of breakout tracks taking place throughout the day. We attended numerous sessions, met with a multitude of vendors, and talked to a ton of people. Whew, what a day. Our weary brains needed all the rest we could get for Day 2. I awoke on Day 2 before my alarm went off, already energized without even a cup of coffee. What a day! It started out with a bang and added even more highlights to our already amazing conference experience. We got the chance to catch-up with Solomon Hykes, Evan Hazlett and the Kitematic team. It was great to finally meet some of the Docker team members and chat face-to-face. CenturyLink Labs & Docker teams at DockerCon 2015, San Fransisco We were excited to hear more details about the various announcements mentioned during the Day 1 Keynote , and were especially stoked to see our very own Lorry.io project mentioned in the \"Thank You to the Ecosystem\" slide. Ben Golub shared more insight into Docker's incredible growth stats and ever increasing enterprise adoption trends for Docker. Courtesy: Ben Golub's DockerCon 2015 slide deck Solomon Hykes opened with big emphasis on \"Mass Innovation\", with a shoutout to the developer community to help \"build a software layer to make the Internet programmable\". He evangelized on \"Incremental Revolution\", and listed out Docker Container Runtime, Docker Distribution Tools, Docker Compose, Docker Machine, and Docker Swarm as key components in the Docker tool chain. Courtesy: Solomon Hykes' DockerCon 2015 slide deck The key announcements from my perspective included: Docker Experimental releases and the acquisition of SocketPlane, leading up to the new, much awaited, Docker Network , finally providing multi-host networking out of the box. Docker Plugins , was next to be introduced with extension points for network, volume, scheduler and service discovery plugins. Then Docker Plumbing Projects was introduced, including announcements for Notary , Open Container Format (OCF, an universal intermediary format for OS containers) along with the Open Container Project , and runC (the universal container runtime), being an OCF reference implementation. A lot of updates to Engine, Compose, Swarm and Machine were announced as part of the next Docker 1.7 release . Deepak Singh from AWS, announced that the EC2 Container Service will be fully integrated providing native Docker Compose experience, later this year. With Matt Butcher of Deis/Engine Yard Docker Hub 2.0 (beta) was announced, featuring a new UI and improved performance, along with the next iterations of Registry 2.0 and Automated Builds v2. The Docker Trusted Registry (beta launched in February as Docker Hub Enterprise), with an eye towards the enterprise customers, was also announced. Last but not the least, Evan Hazlett, showcased \"Project Orca\", a top-to-bottom integrated stack for Operations teams, providing an integration of Swarm, Machine and Compose to provide a seamless Run experience. CenturyLink Labs team at the photo booth Other really intriguing vendor stories included: VMWare previewing Project Bonneville, a Docker runtime that works with vSphere, Google launching Container Engine beta, IBM becoming the first reseller of the Docker Trusted Registry, Microsoft announcing integration of Docker into Azure and in the next Windows Server release along with Visual Studio tooling support, and many more… Cluster of Raspberry Pi's running Docker There were so many sessions that it was hard to attend all of them, but Docker has made quite a few resources from the conference publicly available. Slides and videos of Day 1 and Day 2 from all the sessions at DockerCon 2015 were shared. Docker also posted a roundup of DockerCon 2015 Day 1 and Day 2 . Overall, it was an amazing conference with great announcements from Docker and the various vendors. I'd love to hear what your favorite session, speaker or announcement was — please share in the comments below.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "dockerfile-entrypoint-vs-cmd", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/dockerfile-entrypoint-vs-cmd", "abstract": "When looking at the instructions that are available for use in a Dockerfile there are a few that may initially appear to be redundant (or, at least, have significant overlap). We've already covered ADD and COPY and now we're going to look at ENTRYPOINT and CMD. Both ENTRYPOINT and CMD allow you to specify the startup command for an image, but there are subtle differences between them. There are many times where you'll want to choose one or the other, but they can also be used together. We'll explore all these scenarios in the sections below. Ultimately, both ENTRYPOINT and CMD give you a way to identify which executable should be run when a container is started from your image. In fact, if you want your image to be runnable (without additional docker run command line arguments) you must specify an ENTRYPOINT or CMD. Trying to run an image which doesn't have an ENTRYPOINT or CMD declared will result in an error Many of the Linux distro base images that you find on the Docker Hub will use a shell like /bin/sh or /bin/bash as the the CMD executable. This means that anyone who runs those images will get dropped into an interactive shell by default (assuming, of course, that they used the -i and -t flags with the docker run command). This makes sense for a general-purpose base image, but you will probably want to pick a more specific CMD or ENTRYPOINT for your own images. The ENTRYPOINT or CMD that you specify in your Dockerfile identify the default executable for your image. However, the user has the option to override either of these values at run time. For example, let's say that we have the following Dockerfile If we build this image (with tag \"demo\") and run it we would see the following output: You can see that the ping executable was run automatically when the container was started. However, we can override the default CMD by specifying an argument after the image name when starting the container: In this case, hostname was run in place of ping The default ENTRYPOINT can be similarly overridden but it requires the use of the --entrypoint flag: Given how much easier it is to override the CMD, the recommendation is use CMD in your Dockerfile when you want the user of your image to have the flexibility to run whichever executable they choose when starting the container. For example, maybe you have a general Ruby image that will start-up an interactive irb session by default ( CMD irb ) but you also want to give the user the option to run an arbitrary Ruby script ( docker run ruby ruby -e 'puts \"Hello\"' ) In contrast, ENTRYPOINT should be used in scenarios where you want the container to behave exclusively as if it were the executable it's wrapping. That is, when you don't want or expect the user to override the executable you've specified. There are many situations where it may be convenient to use Docker as portable packaging for a specific executable. Imagine you have a utility implemented as a Python script you need to distribute but don't want to burden the end-user with installation of the correct interpreter version and dependencies. You could package everything in a Docker image with an ENTRYPOINT referencing your script. Now the user can simply docker run your image and it will behave as if they are running your script directly. Of course you can achieve this same thing with CMD, but the use of ENTRYPOINT sends a strong message that this container is only intended to run this one command. The utility of ENTRYPOINT will become clearer when we show how you can combine ENTRYPOINT and CMD together, but we'll get to that later. Both the ENTRYPOINT and CMD instructions support two different forms: the shell form and the exec form . In the example above, we used the shell form which looks like this: CMD executable param1 param2 When using the shell form, the specified binary is executed with an invocation of the shell using /bin/sh -c . You can see this clearly if you run a container and then look at the docker ps output: Here we've run the \"demo\" image again and you can see that the command which was executed was /bin/sh -c 'ping localhost' . This appears to work just fine, but there are some subtle issues that can occur when using the shell form of either the ENTRYPOINT or CMD instruction. If we peek inside our running container and look at the running processes we will see something like this: Note how the process running as PID 1 is not our ping command, but is the /bin/sh executable. This can be problematic if we need to send any sort of POSIX signals to the container since /bin/sh won't forward signals to child processes (for a detailed write-up, see Gracefully Stopping Docker Containers ). Beyond the PID 1 issue, you may also run into problems with the shell form if you're building a minimal image which doesn't even include a shell binary. When Docker is constructing the command to be run it doesn't check to see if the shell is available inside the container -- if you don't have /bin/sh in your image, the container will simply fail to start. A better option is to use the exec form of the ENTRYPOINT/CMD instructions which looks like this: CMD [\"executable\",\"param1\",\"param2\"] Note that the content appearing after the CMD instruction in this case is formatted as a JSON array. When the exec form of the CMD instruction is used the command will be executed without a shell. Let's change our Dockerfile from the example above to see this in action: Rebuild the image and look at the command that is generated for the running container: Now /bin/ping is being run directly without the intervening shell process (and, as a result, will end up as PID 1 inside the container). Whether you're using ENTRYPOINT or CMD (or both) the recommendation is to always use the exec form so that's it's obvious which command is running as PID 1 inside your container. Up to this point, we've discussed how to use ENTRYPOINT or CMD to specify your image's default executable. However, there are some cases where it makes sense to use ENTRYPOINT and CMD together. Combining ENTRYPOINT and CMD allows you to specify the default executable for your image while also providing default arguments to that executable which may be overridden by the user. Let's look at an example: Let's build and run this image without any additional docker run arguments: Running the image starts to feel like running any other executable -- you specify the name of the command you want to run followed by the arguments you want to pass to that command. Note how the -c 3 argument that was included as part of the ENTRYPOINT essentially becomes a \"hard-coded\" argument for the ping command (the -c flag is used to limit the ping count to the specified number). It's included in each invocation of the image and can't be overridden in the same way as the CMD parameter. When using ENTRYPOINT and CMD together it's important that you always use the exec form of both instructions. Trying to use the shell form , or mixing-and-matching the shell and exec forms will almost never give you the result you want. The table below shows the command string that results from combining the various forms of the ENTRYPOINT and CMD instructions. If you want your image to actually do anything when it is run, you should definitely configure some sort of ENTRYPOINT or CMD in you Dockerfile. However, remember that they aren't mutually exclusive. In many cases you can improve the user experience of your image by using them in combination. No matter how you use these instructions you should always default to using the exec form .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "zodiac-easy-container-deployment-rollback", "author": ["\n              Alex Welch\n            "], "link": "https://www.ctl.io/developers/blog/post/zodiac-easy-container-deployment-rollback", "abstract": "With the recent launches of ImageLayers and Lorry, the CTL Labs team has been thinking a lot about the deployment of containerized applications. Over the past month we've prototyped a few ideas that tackle container deployment in different ways. Today we're happy to announce our first project, Zodiac. Built with small teams and single developers in mind, it is designed to be lightweight and plug into the existing Docker tooling. Zodiac makes it easy to deploy and rollback containerized applications. Our focus is on simplicity and ease of adoption, not features. We welcome your feedback in helping us to chart the future path for Zodiac. At this point Zodiac is more or less a wrapper around Docker Compose . This is very much intentional, and, we encourage developers to use Docker Compose for specifying application configuration. Many applications are already described with a Docker Compose file, so it seemed natural to leverage that for remote deployments. Since we're just wrapping Docker Compose, you can do a remote deployment with Zodiac by simply pointing the client at a remote Docker endpoint. Similarly, if you want to deploy to a cluster you can reference a Docker Swarm endpoint. Zodiac has the same limitations with Docker Swarm as Docker Compose does . Zodiac only requires that a Docker endpoint is exposed via TCP. Docker Machine can fulfill this pre-requisite, and we recommend using it, as Zodiac was designed to play nicely with Docker Machine managed endpoints. On the client machine Zodiac will rely on Docker Compose being installed, along with the Zodiac binary. Installation Instructions can be found in the Zodiac README . ] Let's walk through a simple example of deploying with Zodiac. With Docker Compose and Zodiac installed locally, and a Docker Machine-provisioned endpoint set up remotely, I can do my first deploy. So, in my local environment, I have a simple docker-compose.yml file: And the following Dockerfile for the demosite service: And in an index.html file we just have a string of text: The above compose and Dockerfile are intended to be brief and simple for example purposes. Real-world applications will often have multiple services, options, etc. Zodiac offers a verify command to ensure the endpoint is responding and using the correct version of Docker or Swarm. With that we can do our first deploy. Now let's list the deployments At this point we can curl the endpoint, and if we do a docker ps against the remote Docker host we'll see the running containers: We can see here the Image is not what we would've expected. This is because Zodiac snapshots the image's actual layer id at the time of deploy, enabling accurate rollbacks. This is probably the most important component of Zodiac. If an image tag is updated to point at a new layer ID after the deployment has happened, the rollback history would be inaccurate. By recording the actual layer ID we preserve the ability to rollback. I'll update some of the my source files, and deploy again, but this time with a message. Okay, now we've got two deployments in the history (the top being the current deployment). If we curl the endpoint or look at the containers we'll notice our updates. Notice the IMAGE for zodiac_demosite_1 has a different name now (77e9f456...). This is because we built a new image with the edits to the README.md baked in. Let's say the change to index.html was a bad idea, and we need to rollback. Notice the additional entry and message in the list output.\n    $ curl http://12.34.56.78 Initial Deployment. Now the IMAGE for zodiac_demosite_1 is back to the same as it was in the initial deploy. Rollback successful! From here, I recommend kicking the tires yourself. We had specific use cases in mind for Zodiac, but have tried to leave it open-ended. Again, community feedback is welcome via Github Issues . Stay tuned. Next in our deployment series, Brian Dehamer will talk about another take on deployment, one that we use for our Production Docker deployments.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-hub-top-10", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-hub-top-10", "abstract": "Have you ever wondered how many images are hosted on the Docker Hub? Or what the most popular base images are? We've got answers to those questions and lots more in this overview of data we've pulled from the Docker Registry API. The CenturyLink Labs team has been working with Docker for about a year and a half now. One of the things that has been fascinating to see is the growth of the community around Docker in that time. One place you can see this growth pretty clearly is in the number of public images hosted on the Docker Hub. When the Docker Hub was announced at DockerCon 2014, Ben Golub (CEO, Docker Inc.) made a point of mentioning that there were already \"over 14,000 Dockerized applications\" in their public registry. Compare that to DockerCon 2015 where Marianna Tessel (SVP Engineering, Docker Inc.) showed that the Docker Hub had grown to over 150,000 repos . That's a tenfold increase in one year! Seeing the crazy growth in the number of images pushed to the Docker Hub got me wondering what other stats we could tease out of all that data. Many of the projects we've worked on ( Panamax , Image Layers , etc...) have direct linkages to the Docker Hub so we knew that we could get a lot of information by simply querying the Docker Registry API . We decided to see if we could mine the data from the Hub to learn anything interesting about the 150,000 repositories being hosted there. Note that repositories and tags are constantly being added to and deleted from the Hub so the data below is just a snapshot at a particular point in time. The specific numbers shown below will likely have changed by the time that you read this, but the general scale of these numbers is still interesting. Our last count showed 125,289 public repositories. This is slightly lower than the 150,000 repos mentioned at DockerCon in June (and that number has certainly grown since then), but the difference can likely be attributed to the number of unlisted and private repositories. Of those public repos, 39,441 (31%) are using automated builds which means that the images are being built by the Hub itself using a published Dockerfile. The remaining 85,848 repos house images that have been uploaded manually via a docker push command. There are 84 official repositories representing common services like MySql, MongoDB and Redis. This a pretty big increase over the 13 repos that were part of the initial launch of the Official Repositories program when it was announced at DockerCon 14. The CenturyLink Labs team has published a number of images but we can't even begin to compete with some of the more prolific organizations publishing to the Docker Hub. Below are the ten organizations with the most public repos: I'm not sure what datdocker is doing with those 8261 repositories but it looks like they may be generating new repositories as part of some CI process (there are lots of similarly named repos with numerically increasing suffixes). The imiell organization is owned by Ian Miell who appears to be writing a Docker book . Many of these repositories look like they are being used as examples for the book. In case you were wondering, the CenturyLink Labs team with our 65 repositories ranks 30th on this list. It would be interesting to see which images have been pulled the most but that information isn't available via the Docker Registry API. The closest metric available for image popularity is the star count. Not surprisingly, the repos with the highest star counts are all official repositories. The top ten official repos are: The top ten non-official repos are: The most surprising entry on this list for me was the aspnet image. There are clearly a lot of people in the Microsoft dev community who are excited about Docker. Across the 125,289 public repos, there are 243,966 tagged images. That's an average of 1.9 tags per repo. There are over 100,000 repos (82%) that have only a single tag and then there are a handful of repos with thousands of tags. The overall winner is rstiller/jetty with 2104 tags (they appear to be creating tagged images for every possible combination of OS, JDK and Jetty version). The top five most common tag names across all the repositories are: Given that latest is the tag assigned by default if you don't specify one explicitly, it's not surprising to see it at the top of the list by a huge margin. It didn't make the top-ten, but the most interesting tag we found was \"kitten\" which has been applied to 73 different images. My best guess is that it has something to do with this Kubernetes demo app . Interestingly, there are 247 repositories with no tags at all -- they are just empty repos with no images. One of the neat things about Docker's layered image format is that you don't have to build all of your images from scratch -- you can use any of the public images as a foundation for creating your own image. With that in mind we wanted to see which images were being referenced most often as the base for other images. Here are the top ten most referenced base images: Rank Layer ID Tag References The scratch image is a pretty obvious #1. Prior to Docker 1.5, scratch was the empty layer at the root of every image . Almost every image built with Docker 1.5 or earlier will have scratch at the base of its layer hierarchy . Clearly, Ubuntu is extremely popular with different flavors occupying 6 of the top 10 spots on the list. It's a natural choice given that a lot of developers are familiar with it. However, I encourage you to look at alpine , especially if you are interested in minimizing the size of your images. The alpine image is 5MB (compared to ubuntu 's 188MB ) and has a nice package manager. So you can start small and then add just the packages you absolutely need for your application. We've slowly been migrating a lot of our images from Ubuntu to Alpine and have seen some significant size reductions as a result. Note that four of the layers shown in the \"top 10 most referenced base images\" list (marked with a &ast;) don't currently have a tag associated with them. They were likely tagged at one point with the values shown in the table above, but those tags have since been reassigned. For example, at some point in the past the layer bf84c1d84a8f was tagged as debian:latest (as well as debian:jessie ). While those tags were in place a number of images were built that used FROM debian or FROM debian:jessie in their Dockerfile. Some time after those images were built, the Debian images were rebuilt and the tags were moved to point at different layers. This probably isn't a big deal for most of these images, but it is worth noting that image tags are not static -- they may change to point to different images over time. If you want your image to always be built upon the latest and greatest Debian Jessie or Ubuntu Trusty you need to make sure that you track those tags and rebuild your image whenever it changes. The repository links feature on the Docker Hub can help you rebuild your images automatically whenever your base image changes. As I said above, this is just a snapshot of the data (as of August 6th, 2015). It will be really interesting to see how these trends evolve over time.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-to-inspect-when-youre-inspecting", "author": ["\n              Gary Paige\n            "], "link": "https://www.ctl.io/developers/blog/post/what-to-inspect-when-youre-inspecting", "abstract": "It is common to think of Docker images and containers as mystical black boxes where all the application goodness gets locked away. Fortunately, there is a way to unlock both structures using the docker inspect command. Inspect is a core Docker instruction with detailed documentation . However, there is little information about interpreting the output. This article will explain running docker inspect and detailing the results. Every Docker command has built in documentation. Typing docker inspect --help will generate the following help documentation. Based on the help output, the inspect command can be run on one or more images or containers. Either the name or id can be used to identify an image or container, and by using docker images a list of local images can be found. For images the Repository name or the Image ID can be used to identify the images. Using the results above a valid inspect command could be the following: The ps command by default shows only running containers but passing the -a flag to the command will show all containers. Containers have both a name and identifier either can be used to identify the container to inspect. Using the results above, valid inspect commands could be: All of the previous examples only inspected a single item but it is possible to inspect multiple combinations of images and/or containers in a single command. The last example above mixes images and containers in the same result which is valid. One of the options for inspect is the format option. This allows the results to be filtered on a specific stanza of the inspection result. This format string follows go templating which may require some additional learning to use in detail but in general it's pretty easy to get the information you need from the results. For example, passing '{{.NetworkSettings.IPAddress}}' will return just the ip address which is 172.17.0.3 for the watchtower container. Results can be filtered using the standard grep tool as well. While not as precise the IPAddress is present. However, for some data structures in the result, a simple grep is not sufficient. Let's try to get all of the NetworkSettings using grep. Since grep is a text based tool it returns the line where \"NetworkSettings\" occurs but in this case its not very helpful. Using the -f option, however, will return much better results as shown in the example below. The results of inspecting an image differ from the results of inspecting a container. Most of the elements are pretty easy to understand but a few of them may not be as obvious. ID This is the unique identifier of the image. Parent A link to the identifier of the parent image. It is very common for an image to have a defined parent. Container A container identifier is interesting because this is meta-data for an image not a container. This container identifier is a temporary container created when the image was built. Docker will create a container during the image construction process, and this identifier is stored in the image data. ContainerConfig This data again is referring to the temporary container created when the Docker build command was executed. DockerVersion The version of Docker used to create the image is stored in this value. It could be very useful as the Docker ecosystem continues to advance. Virtual Size The size of the image reported in bytes. The results from inspecting a container have very different options. The container is the runtime environment for the image, so the meta-data for containers must provide all of the runtime configurations needed to start and restart the image with all of its settings intact. ID The unique identifier for the container State This stanza has various status flags and the process id for the container. Using the ExitCode from this State element a graceful shutdown or recovery process could be initiated. The following format will return just the ExitCode of the most recently run container: Image The image this container is running. NetworkSettings The network environment for the container and therefore for the application(s) within the image. LogPath The system path to this container's log file. RestartCount Keeps track of the number of times the container has been restarted. This value is the key value used when defining a container's restart policy .. Name The user defined name for the container. Volumes Defines the volume mapping between the host system and the container. HostConfig Key configurations for how the container will interact with the host system. These could take CPU and memory limits, networking values, or device driver paths. Config The runtime configuration options set when the docker run command was executed. Part of this configuration is another \"Image\" value. This image {{.Config.Image}} is the tagged image which may be different than the image listed in {{.Image}} Docker images and containers are not the lock boxes many believe. Using the basic inspect command a wealth of information about images and/or containers can be gathered. restart policy docker inspect gracefully stopping docker containers", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "watchtower-automatic-updates-for-docker-containers", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/watchtower-automatic-updates-for-docker-containers", "abstract": "Last month, we launched a new tool named Zodiac which is used to deploy (and rollback) Docker apps. In his introductory post , Alex mentioned that the CTL Labs team has been exploring a few different ideas for deploying containerized applications. This is something that's become more important to us as we've moved from just creating images for other people to actually running containerized applications in production. Today, we're going to present another approach to deployment that we call Watchtower . Whereas Zodiac uses a push-based deployment model (a user pushes an application to a remote system), Watchtower enables pull-based deployments. Watchtower is an application that will monitor your running Docker containers and watch for changes to the images that those containers were originally started from. If Watchtower detects that an image has changed, it will automatically restart the container using the new image. With Watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container, and restart it. Watchtower will take care to ensure that any flags/arguments that were used when a container was deployed initially are maintained across restarts. If you specified a -p 80:8080 port mapping when you started your container, Watchtower will use that same configuration any time it restarts it. Additionally, Watchtower knows how to properly restart a set of linked containers. If an update is detected for one of the dependencies in a group of linked containers, Watchtower will stop and start all of the containers in the correct order so that the application comes back up correctly. This style of deployment is definitely not appropriate for all applications, but it may be useful to hear how we're using it within the CenturyLink Labs team. Back in May of this year, we launched ImageLayers which was the first hosted application released by our team. We had published a number of Docker images prior to that, but this was the first one that we were going to run on a public-facing server. As part of our previous image creation efforts we had already developed a continuous integration process that was working well for us. Whenever a commit is pushed to the master branch of our GitHub repo a webhook triggers a CircleCI job which is responsible for running tests, compiling code (where appropriate), and building a Docker image. Once the image was built, our CircleCI job would then push the image to the Docker Hub. For most of our images, the process stops there. However, with a hosted application, we needed the extra step of starting the container on one or more of our servers. Our initial pass at this had our CircleCI job SSH'ing into our servers and executing docker commands to pull images and stop/start containers. This worked but we realized that we were opening a port on our server just for our CI/CD process. Most of our servers sit on a private network behind a load balancer and don't require any sort of direct, incoming connection from the public internet. Opening a public-facing port isn't the end of the world but, due to the security implications, is something we'd rather avoid if possible. With that limitation in mind we started to think about ways we could do a pull-based deployment that was initiated directly from the hosting server. Our first implementation was a collection of shell scripts that looked at the Hub for new images, pulled them down and restarted the running containers. This approach worked well enough for us that we decided to turn our shell scripts into a general-purpose service that could be used by anyone. Watchtower was born! Now we have Watchtower running on each of our QA and production servers and our apps are automatically updated anytime a new image is pushed to the Docker Hub. We use different image tags for the different environments so the images destined for our QA server are tagged with \"qa\" while anything tagged with \"latest\" goes to production. There are a few important Watchtower limitations that should be noted. First, Watchtower doesn't make any attempt to address the first-run issue. Watchtower will monitor running containers and ensure that they stay up-to-date, but getting those containers running in the first place is outside of its scope. The initial start-up of your application will need to be handled by something like Docker Compose, Zodiac or plain ol' docker run . In the future, we may add a feature to Watchtower which would allow it to boot-strap your application, but even then you're still gonna need a way to start Watchtower itself. The second Watchtower limitation relates to application downtime. While Watchtower is pretty quick at restarting containers, it makes no guarantees about application downtime. Since the old container needs to be stopped before the new one can be started there will be some (hopefully small) period of time where the container isn't running at all. For many of our use cases one second of downtime during a deployment is no big deal, however, we recognize that there are plenty of applications which may have stricter requirements. If you have an application that is sensitive to downtime, you'll want to be careful how you use Watchtower. Luckily, applications with 100% uptime requirements are probably already running with multiple instances. In that case, you may still be able to use Watchtower -- just so long as not all of your containers get updated at precisely the same time. Please take Watchtower for a spin and let us know if you find it useful. If you have comments or ideas for improvement you can leave feedback on our GitHub Issues page.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "more-docker-image-cache-tips", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/more-docker-image-cache-tips", "abstract": "In my previous article I described the Docker image cache and how Docker decides that a particular layer needs to be rebuilt when you do a docker build . Now, let's build on that knowledge and discuss some strategies for making the Dockerfile code/build/test cycle as fast and reliable as possible. This one should be pretty obvious by now, but as you're iterating on your Dockerfile you should try and keep the stable parts toward the top and make your additions at the bottom. If you know you need to install a bunch of OS packages in your image (which is typically one of the slower parts of building an image) put your package installation instructions toward the top of the Dockerfile . That way you only need to sit through the installation process for those packages once as you go through the code/build/test/repeat cycle for your image. Similarly, if you have a core set of instructions that you use across all of your images (like a MAINTAINER value you always use), it's best to keep those at the top of your Dockerfile and always in the same order. That way those cached layers can be shared between different images. When executing docker build the first line of output typically reads \"Sending build context to Docker deamon . . .\" The build context constitutes everything in your build directory (the directory that you pass to the docker build command) and is used by Docker so that you can inject local files into your image using the ADD and COPY instructions. This is the one place where the caching rules change slightly -- in addition to looking at the instruction and the parent image, Docker will also check to see if the file(s) being copied have changed. Let's create a simple Dockerfile that uses ADD to copy a file into our image: Now let's docker build the image: If we were to execute the docker build again we'd see that no new images are created since we haven't changed anything. However, let's update the README.md file and then build again: Note that a new image was generated for the ADD instruction this time (compare the image ID here to the one from the previous run). We didn't change anything inside the Dockerfile , but we did update the timestamp on the README.md file itself. For the most part, this is exactly the behavior we want when building images. If the file changes in some way, you would expect that the next build of the image would incorporate the changes to that file. However, things get a bit trickier when you start adding lots of files at once. A common pattern is to inject an application's entire codebase into an image using an instruction like: In this case we're injecting the entire build context into the image. If any single file changes in the entire build context, it will invalidate the cache and a new image layer will be generated on the next build. If your build directory happens to include things like log files or test reports that are updated frequently you may find that you're getting new image layers generated with every single docker build . You could work-around this by specifically ADD ing ONLY those files which are necessary for your application but if you have many files spread across a number of directories this can be pretty tedious. Luckily, Docker has a better solution in the form of the .dockerignore file. In much the same way that the .gitignore file works, the .dockerignore file allows you to specify a list of exclusion patterns. Any files/directories matching those patterns will be excluded from the build context. If you have files in your build directory that change often and are not required by your image, you should consider adding them to .dockerignore file. A good rule of thumb is that anything in your .gitignore is a good candidate for inclusion in your .dockerignore . One Catch-22 related to the use of ADD . is that the Dockerfile itself is also part of the build context -- so any changes you make to the Dockerfile result in a change to the build context, and you can't add the Dockerfile to the .dockerignore file because it needs to be part of the build context in order for Docker to read the build instructions. If you're using ADD . and making changes to your Dockerfile don't be surprised to see new image layers generated every time you do a build. For the most part, the image cache is incredibly helpful and can save you a lot of time while building your images. However, there are times when the caching can bite you if you aren't paying attention, so it's good to know how to selectively bust the cache. Let's say we have a Dockerfile which contains the following: When I build this the first time, I'm going to get exactly what I expect -- it'll clone my Git repo and checkout the v1.0.0 tag. Now imagine I push some changes to my repo and tag it as v1.1.0 . I'm going to update the Dockerfile to reference the new tag: When I go to build the image from the updated Dockerfile I get the following error: I definitely pushed a v1.1.0 tag to my repo, yet Git is telling me that no such tag is found. This is one of those times where the Docker image cache is being a little too helpful. In the output above note how the git clone step had already been cached from our previous build and was re-used in this run. When we get to the git checkout instruction we're still using a copy of the repo that doesn't have a v1.1.0 tag. This is quite different from the example with the build context above. In this case the contents of the git repo are not part of the build context -- as far as Docker is concerned, our git clone is just another instruction that happens to match one that already exists in the cache. The brute-force solution here is to simply run docker build with the --no-cache flag and force it to re-create all the layers. While that will work, it doesn't allow us to take advantage of any earlier instructions in the Dockerfile that were just fine to be pulled from the cache. A better approach is to refactor our Dockerfile a bit to ensure that any future changes to the tag will force a fresh git clone as well: Now we've combined the git clone and git checkout into a single instruction in the Dockerfile . If we later edit the file to change the tag reference it will invalidate the cache for that layer and we'll get a fresh clone when the new layer is generated. Note also that I moved the WORKDIR instruction so that the directory would be created before the cloning the repo. Then, by cloning into the current directory (note that . after the repo's URL), I was able to execute my clone and checkout without needing to switch directories in-between When building images based-off of Debian/Ubuntu you'll often see this same pattern applied to installing OS packages: Here the apt-get update is like the git clone in the previous example -- we want to ensure that we've got access to all the latest packages anytime we add another package or update the version of the vim.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "developing-rails-apps-in-container-with-docker-compose", "author": ["\n              Mike Arnold\n            "], "link": "https://www.ctl.io/developers/blog/post/developing-rails-apps-in-container-with-docker-compose", "abstract": "Many developers choose Rails because it allows them to be productive very quickly, but that can come with the trade off of dependency spaghetti, frustration with databases and version management, and file size bloat. Using Docker can alleviate some of these issues by isolating processes from one another, limiting the scope of dependencies, and allowing you to use optimized images. Of course, another layer of abstraction from your code can introduce problems of its own. Below, we'll guide you through the process of starting a new Rails project with containerization in mind, and explain some of the sticking points. A good base image is your best starting point. It should give you all the dependencies you need to build an application in development and run in production. In the case of Rails development, you want the Ruby language itself along with system libraries needed to build gems like nokogiri, pg, or mysql2. We've created a couple versions of popular operating system images for this very purpose. On the Docker Hub, along with the official Rails image, you can find our Rails base images that derive from Alpine and Ubuntu . To use the base image, you can certainly run a container and just SSH right into it and use it like a virtual machine, but that's not the best way to go about using Docker in development. During development, you'll want to use tools such as an editor or IDE, but these have no place in your container at runtime. Instead, you should volume mount your code into a container running the base image and rely on editing normally in your local OS and building and testing in-container. The benefit of this is that you can continue to rely on tools you're familiar with, like debuggers and code completion, without having to compromise the parity of your development and production environments. It's possible to run a container using the base image and pass the volume mount instructions on the command line, but it's easier to orchestrate and automate running your application if you make use of a Dockerfile. In some cases, it's absolutely necessary. For example, when using the official Rails image, in order to control the firing of the ONBUILD instructions, they make this recommendation: adjust your Dockerfile to inherit from a non-onbuild variant and copy the\ncommands from the onbuild variant Dockerfile (moving the ONBUILD lines to the\nend and removing the ONBUILD keywords) into your own file so that you have\ntighter control over them and more transparency for yourself and others\nlooking at your Dockerfile as to what it does. With the CenturyLink Labs images, the Dockerfile is as simple as a single line or two when using the alpine-rails image with PostgreSQL Once the Dockerfile is in place, the image can be built and tagged. Apart from the code you'll inject via volume mount, this image is identical to the image you'll run during testing, staging, and production. A slight modification to the Dockerfile to ADD your code during the image build and running it with RAILS_ENV=production is all that's needed to create the perfect image for your CI/CD process. This is best handled with a separate production version of your Dockerfile (e.g. Dockerfile.prod ) and then passing --file=\"Dockerfile.prod\" to the Docker CLI build command . You're using Docker, so you might as well embrace microservice architecture. You don't necessarily need to include your Rails application's database in the container which houses the Rails code. If you're using sqlite3, you can and should, but if you're using PostgreSQL or MySQL, there are already official images that are simple to configure and run containerized with Docker. There's a bit of work up front to use them with a Dockerized Rails app, though, but it's relatively simple. If you're working with MySQL or Postgres, you want to create a data volume container to house your data. There are plenty of good reasons to use a data volume container when using a containerized database, but suffice it to say that they are designed to persist data. When doing Rails development and you recreate the database container with a data volume you don't lose your data and you can follow the conventional mode of migrating your app's database. Here's an example of the Docker CLI command to run a data volume intended to house PostgreSQL data: This creates the data volume container and sets up the /var/lib/postgresql directory within the container as a volume to be mounted by other Docker containers. Although specific to PostgreSQL, this same concept works for other databases. You simply need to create the volume using the path expected by the database. Once you have a data volume container, mounting it from a database container is not difficult. You simply pass the name of the data container in as the value for the --volumes-from flag of the run command. Below is an example of a container mounting the volume created above. One important thing to note is that when you do decide to get rid of the containers and corresponding data volume containers, you want to remove the last container that references the volume with the -v flag or you could leave ophaned containers on the system just taking up valuable space. With an image, a running database container, and a data volume container all in place, we can fire up our Rails application container. Getting the code into the container is done by volume mounting the directory containing the code into a directory in the container. This is done with the -v flag of the Docker run command. Next, To bind the Rails server port to the container port, you'll need to do two things. First, you pass the -p flag to the run command. Second, you pass the -b flag to the rails server command to bind Rails to any IP address of the resulting container. Finally, we need to connect to the database container. One of the really nice features of Rails is that the database configuration in database/config.yml can be overriden by setting the connection information in the special DATABASE_URL environment variable . When we pair this with Docker container linking and the ease of injecting environment variables into Docker containers at runtime, we get a very simple way of connecting our Rails app to our containerized database. With the following command, we can run our Rails app in a container, mapped to the localhost on port 3000, and connected to the database service. If you're using boot2docker, you'll need to forward port 3000 from the VM to a local port to see the application in the browser, or simply access the GUI via $docker_host_ip:3000. The last thing we need is to be able to execute the usual bundle , rails , and rake commands. Thankfully, Docker has the exec command which is tailor-made for this very purpose. With it, we can execute arbitrary commands on our container. For example, if we make a change to the Gemfile locally, we can run bundler in the container simply by executing the following command which tells Docker to execute the bundle command on the container named 'webapp': Likewise, we can migrate the database with Rake: We can even execute our tests: In each case, Docker starts up the container, executes the command and then shuts the container down again. Unless we remove the container, it's there to do our bidding. Changes we make locally are reflected in the container because we've mounted the code with the -v flag. And with that, we've achieved the goal of a containerized development environment. Before we stick a fork in it, however, there's one last optimization available to us. Orchestrating the startup, shutdown, and linking between these containers can be a bit tedious. Docker Compose can alleviate much of the pain. With a single YAML file, we can reduce the coordinated control of this multi-container application to a simple start and stop. Here's the docker-compose.yml modeling the three containers. To start the multi-container application, we simply call docker-compose up . We can still execute our typical Rails and Rake commands, as well. Instead of using docker exec we just use docker-compose run webapp . If that's just too much typing for you, create a simple alias to call the Compose run command and pass in the name of the Rails app container followed by the command. Some developers advocate using containers while developing locally as if they were virtual machines. However, this has the potential to reduce parity between their Dev and Prod environments, while simultaneously limiting their ability to use familiar tools such as an IDE or debugger. The right way to use Docker in development is for running and testing the application and for standing up dependent services. In this post, we've shown how to set up a Rails development environment that allows you to code locally using the toolset for your environment while also making use of containerization for running and testing the application and a database alongside it.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "introducing-flatcar-tool-for-creating-docker-ready-rails-projects", "author": ["\n              David Gardner\n            "], "link": "https://www.ctl.io/developers/blog/post/introducing-flatcar-tool-for-creating-docker-ready-rails-projects", "abstract": "One of the most compelling reasons to use Ruby on Rails is the ease in which you can get a web project up and running. And one of Docker's key benefits is freedom from \"dependency hell”. In his prior post , Mike Arnold ( @dharmamike ) provided a 5-Step guide on how to setup In-container Rails development on your local machine using Docker Compose. After over a year of developing with Docker and Rails, the Labs team wanted an easier method for creating Docker-ready Rails projects. That was the genesis behind the gem now available on Github , Flatcar. Flatcar provides a single CLI command that sets up a Rails development environment with data volume and persistent database support, enabling you to use your preferred IDEs or other dev tools, while including just the application code in the final container build. First, You must have Ruby installed on your machine to install Flatcar. The Ruby installation steps can vary between Linux, Windows or Mac, but this is a great place to get get started. One recommendation is to use a version manager like rbenv or RVM . Personally, I prefer rbenv, but both have large fan bases. Provided you have Ruby installed, you also have Docker and Docker Compose on your machine in order for Flatcar to work. If you are on Linux, install Docker with official packages. On a Mac or Windows , you will need to run a lightweight VM in order to use Docker. You can do this via Docker Toolbox or by running a VM of your choice. If you haven't yet made the switch from boot2docker to docker-machine, now would be a good time. Although Flatcar will work with any VM, Docker Machine is easy to use and with a shallow learning curve. Once you've got your system setup, run gem install flatcar and you're ready to get rolling. From the desired parent directory, run flatcar init $new_project_name . This will initialize a new Rails application in a directory of the same name. For existing Rails projects, run flatcar init $path_to_project to begin a new project. You can also initialize a flatcar project from the current directory by simply running flatcar init . (Keep in mind that Flatcar only works with the simplest of existing Rails projects, and will likely not work \"as is\" for projects more complex that a basic web front-end and database app). flatcar init ($new_project_name|$path_to_project) will perform the following operations:\n1. If no project path is specified, it runs rails new -B $app_path .\n2. Flatcar generates a Dockerfile for the Rails service\n3. A docker-compose.yml file is generated for the entire project. Flatcar applications are run via docker-compose in the project's directory. If you're using a VM, make sure to create appropriate port forwarding rules in order to view the application in your browser. Flatcar will bind 3000:3000 from host:container via Docker's -p flag at runtime. To run the application, cd into the project directory and run docker-compose up . To run the services in the background, use the -d flag. Flatcar uses the following syntax: flatcar [global options] init [command options] [APP_NAME] The init command initializes a Rails application with an optional base image and database specified. If executed within an existing Rails project directory without the APP_NAME argument, the Flatcar-generated files will be placed in the current directory. If the command is executed with the optional APP_NAME argument, a new Rails project will be generated along with the Flatcar files in a subdirectory of the current directory. You can specify which OS to use for your Rails service by passing in the -b flag. Valid options are alpine , ubuntu , or rails (debian-based). If no option is given, the default Rails image will be used. You can also specify which database to use with your Rails application using the -d flag. Valid options are mysql , postgresql , or sqlite3 . If mysql or postgresql is chosen, a new database service will be defined in the generated docker-compose.yml file. If no option is given, the default sqlite3 will be used. Besides specifying the base image or database, Flatcar also supports a variety of other configuration options, such as not including Sprockets files, using a specified Javascript library, or ignoring source control .keep files. See the Flatcar README.md for the full set of command options. We hope you find this tool helpful for quickly getting started with Docker-ready, Ruby on Rails projects. Of course we welcome community contribution. If you have ideas on how to improve Flatcar, community feedback is welcome via Github or simply submit your PR.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-tomcat-graylog", "author": ["\n              Matthew Close\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-tomcat-graylog", "abstract": "Managing logs for cloud infrastructure can be a daunting task. Fortunately, we have tools like Graylog to help us out. Graylog is useful for storing, searching, and analyzing logs from a multitude of different sources. One common log source is a Tomcat app running in a Docker container. However, getting logs from containers isn't always simple. Oftentimes, you have to spend extra time configuring other services like syslog or logstash to deal with log aggregation. Thankfully, there is a faster way. The most recent release of Docker, v1.8.0, makes collecting and sending logs to Graylog much easier. Additionally, the logs are significantly enhanced with extra data such as container ID, container command, and configurable tags. These extra fields can be invaluable when troubleshooting an application problem. In this tutorial I will cover how to set up Docker to natively send Tomcat logs to Graylog. Using the ideas explained in this tutorial should help simplify your existing Tomcat container deployments and logging requirements. As an added bonus, the techniques discussed here can also be generalized to other services running in Docker. Docker version: >=1.8.0 Install Docker. Docker's native support for Graylog Extended Log Format (GELF) is new, and you will need to install the latest version of Docker for your host OS. For detailed instructions, please consult Docker Supported Installations . I'm using Ubuntu vivid and in order to install the latest version I simply run the following command: Create a Java8 Dockerfile. We'll be using Dockerfiles to create two Docker images. The first is strictly Java8. In a later step, we'll use this image to build the Tomcat8 server. I like to create separate directories for each Dockerfile in case any helper scripts are needed during the image creation. First mkdir jdk8-oracle and then cd jdk8-oracle . Place the contents below into a file called Dockerfile . The Dockerfiles in this tutorial are available from GitHub . Build the image. This step will take a few minutes to complete if you haven't already pulled down the Ubuntu vivid container image. Create a Tomcat8 image. Similar to above, first mdkir tomcat8 and cd tomcat8 . Then place the contents below into a Dockerfile . In order to use the latest version of Tomcat8, I'm installing tomcat manually as opposed to using apt-get which will install an older version. Basically, this Dockerfile creates a Tomcat user, downloads and installs the latest version of Tomcat8, and finally runs Tomcat. In order to keep the container running we tail -F the output of the logs. More importantly, by using the tail command, Tomcat logs are sent to stdout. The GELF log driver then picks up both stdout and stderr and sends them to Graylog. By tailing the catalina.log files we are able to direct Tomcat logs to Graylog. To generalize this to other applications besides Tomcat is simply a matter of tailing your service's log file. Build the image. This build should be much quicker than the previous one. Start a Graylog container. For the purpose of this tutorial, I'm going to use a Docker container to run Graylog. Since Docker is already installed this is the fastest way to test. However, if you already have a Graylog server configured, please feel free to use it. First pull then run the container. In this example, I'm exposing two ports. Port 9000 is used for Graylog administration. Port 12201/udp is used to receive GELF log entries. For more information, see Graylog and Docker . Login into Graylog. http://localhost:9000 . For the Docker container, the user is admin and the password is admin . Create the GELF UDP input in Graylog. From the main screen select System -> Inputs . In the dropdown on the next screen, select GELF UDP and click Launch New Input . Configure the GELF UDP input in Graylog. Simply give the input a Title and click the Launch button at the bottom of the window. There is no need to change any of the defaults for this tutorial. Verify the Graylog input is running. Once launched, the input should now show as running in Graylog. Start a Tomcat8 container. There are a couple of important things at work in the docker command below: First -p 8080:8080 exposes the default Tomcat port so we can validate it is running. Next --log-driver=gelf selects GELF for logs, --log-opt gelf-address=udp://localhost:12201 sets the destination of the logs, and --log-opt gelf-tag=\"tomcat8 example\" will set a useful tag in the Graylog entries. If you are using your own server, change localhost to the correct IP. For a complete list of Docker GELF options, please see Docker's Configuring Logging Drivers . Verify Tomcat is running. Tomcat should have successfully started during the previous step. To verify go to http://localhost:8080 . View Tomcat logs in Graylog. You should now see log entries for Tomcat within Graylog. View log detail. Click on one of the entires and you can see the amount of extra detail you get from the GELF driver. Not only can you see the log entry, but now it includes a wealth of extra data about the Docker container that sent the log. I hope this tutorial has convinced you that using the new Docker GELF drivers is well worth it. The extra data about Docker containers included in every log will help reduce troubleshooting time. Additionally, your Tomcat deployments should now be easier.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "kubernetes-bare-metal-servers", "author": ["\n              Chris Kleban, Product Manager - CenturyLink Cloud\n            "], "link": "https://www.ctl.io/developers/blog/post/kubernetes-bare-metal-servers", "abstract": "Are you considering using containers? Do you care about network latency? If so, read on. Kubernetes is an open-source container platform. It creates a cluster made out of a set of infrastructures. To accomplish things like load balancing and container-to-container communication, Kubernetes relies on both a network overlay inside the cluster and a proxy process called the kube-proxy that runs on each node in the cluster. This post will describe two ways you can have ultra-low latency for containers: physical servers and Kubernetes's new IPTables-based proxy mode. Physical servers don't have the hypervisor overhead layer that is common to virtual machines (VMs). As such, running containers directly on Bare Metal (physical servers) should offer you faster performance. But how much of a difference? As it turns out, there is quite a bit of difference. Here at CenturyLink Cloud, we offer Kubernetes clusters on both VMs and Bare Metal servers. To test network latency, we used the open-source netperf testing utlity that the Kubernetes community wrapped up in order to make it easily deployable to a Kubernetes cluster. The test has a client process running inside a Docker container on one host in the cluster, and a server process running inside a Docker container on a different host in the cluster. When comparing VMs to our smallest physical servers (the 4-core option) located in our VA1 data center we saw a 3x improvement in network latency . Now, we would like to share how we did these tests and the detailed test results. Using the tool found here , we created two Kubernetes clusters on top of the CenturyLink Cloud infrastructure. One cluster was made up of four Bare Metal servers and the other cluster was made up of four VMs. Commands to Create the Two Clusters Next, we needed to perform the network tests. To build the netperf Golang tool, we ran: To run the network testing tool, we ran the following test against each cluster. Note: This tool uses the kubectl CLI tool to communicate to the Kubernetes cluster in order to run the tests. We ran this command twice, once against each Kubernetes cluster. A snippet from the test results looks like this: Bare Metal Servers Virtual Machines We then took the full results (1000 measurements on each platform) and created a histogram chart similar to the one above. The results are pretty impressive, around a 3x improvement. Also, you will notice that the jitter (standard deviation) seen in the physical server results is much smaller. As you can see, running Kubernetes and containers on the physical machines provides much lower latency than running Kubernetes on virtual machines. If you are running containers, and network latency is important to you, you may want to consider Bare Metal Servers as an option. Regardless of the hardware you choose to run your cluster on, the Kubernetes community has made substantial progress on building out networking services that are both scalable and highly performance-oriented. One recent improvement from the open source community is the re-basing of the kube-proxy process to use IPTables on the host to perform various load-balancing, proxy, and NAT functions. As seen in the diagram below, this change decreased latency as measured by the open-source network testing tool netperf. Latency (measured in microseconds) seen by the older method (called userspace) can be seen in red while the latency of the newer method (called IPTables) can be seen in blue. For the source information, click here . Nice work Kubernetes community! If you are a Kubernetes user, nothing is required of you as new Kubernetes clusters now come with IPTables proxy mode enabled by default. If you are running an older version of Kubernetes, consider upgrading. If you care about ultra low latency networking for containers, consider using physical machines. If you don't care about this, you might want to consider using virtual machines to run your containers as you can scale more incrementally. Thanks for reading,\nChris Kleban Are you looking for some of this low latency network love? If so, you are only 15 minutes away from having it. We give you the deployment tools you need to manage your applications quickly and easily. Check out our Kubernetes Knowledge Base article. It will get you started using Ansible to create a Kubernetes cluster on CenturyLink Cloud - all by running a single script. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-vs-vagrant", "author": ["\n              Adam DuVander\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-vs-vagrant", "abstract": "The number of tools available for developing, deploying, and managing applications continues to expand. One comparison we keep seeing is between Docker vs Vagrant. While not exactly competing tools, they are frequently used in the same sorts of conversations. A little over two years ago we wrote that the lines between Vagrant and Docker were already blurring. Each community has grown quite a bit since then. We thought it would be useful to revisit the topic again from a high level. Let's start by defining each tool. Definitions can be touchy; so we'll use the product description from the home pages for Docker and Vagrant right now. While the descriptions are divergent, there is similarity in the use cases. Where the two overlap the most is in the ability of each to solve the problems related to sharing environment setups so that multiple instances of the same application act in a predictable, repeatable manner. Docker is often described as \"lightweight,\" because it simply provides containers within the host machine's Linux operating system. Docker containers hold an application's components, including binaries, libraries, and configurations, as well as the application's dependencies. As you'll see, when we compare Docker to other tools (including Vagrant), it can be confusing to figure out the differences. Here is an excellent primer/tutorial on that subject if you want to learn more about how Docker fits into the larger development ecosystem. Topics in the article include: Vagrant is a workflow for development projects. Vagrant provisions any machine, which is typically a virtual machine (but does not need to be). Here's an excellent overview/tutorial that discusses using Vagrant with CenturyLink Cloud. Topics in this article include: Where Docker relies on the host operating system, Vagrant includes the operating system within itself as part of the package. One big difference between Docker and Vagrant is that Docker containers run on Linux, but Vagrant files can contain any operating system. That said, Docker does work with non-Linux operating systems. It just needs to run within a Linux virtual machine. Docker provides VMs for Mac and 64 bit Windows, or you could use another. It's in this type of scenario where you are most likely to find Docker and Vagrant used simultaneously. Though Docker and Vagrant are most frequently used separately from one another, often for very different purposes, they can be used together. You can either provision Docker using Vagrant, or go the more advanced route and use Docker as a provider within Vagrant (i.e., in place of a virtual machine). We give you the deployment tools you need to manage your applications quickly and easily. Check out our Knowledge Base of articles on CenturyLink Cloud. We also have several container tools listed in our Developer Center. Not a CenturyLink Cloud customer? No problem. You can just head over to our website and activate an account . Sign-up for our Developer-focused newsletter CODE. Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-docker-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-docker-on-the-centurylink-cloud/", "abstract": "One of the hottest technologies today is Docker and the ability to manage Linux Containers. As with most new things it is important to be able to setup and play with it, just to see what all the excitement is about. Follow along as we install a fully functioning Docker system on a CenturyLink Cloud server. In order to create a CenturyLink Cloud server we need to use the web interface, or the \"control panel\". This can be found at control.tier3.com . You will need to have an account and associated user name in order to login. From the pull down menu select \"create server\" and follow the dialogs to select the datacenter, number of CPU instances, amount of memory, and other pertinent information. Make sure you remember the admin password you enter, this will be associated with the root account for this server. Also, it is very important that you select \"Ubuntu 12 64-bit\" as the operating system. Docker relies on pretty new capabilities in the Linux kernel (lxc and cgroups) and is currently only officially supported on the Ubuntu distribution. By the time you read this tutorial this may have changed since the project is moving at an incredible rate. Once you have the server built we need to add a public IP and open the appropriate ports in the firewall. Select your server instance in the UI and you will see a choice for \"Add Public IP\". You can take the default networking choices but we need to adjust the ports selection. Select \"ping\", \"http\", \"https\", and \"ssh\". In addition we need to open the specific port range that Docker will be using for container applications so select \"custom ports\" and open the range 49100-49199. When we start deploying applications we need to remember that our firewall is blocking all ports except the ones we have explicitly opened. You can look at the server properties to see that the Public IP has been added. It is generally not a good idea to run as root, so in order to update our Ubuntu system and install Docker we will first set up a new user. After giving the new user the appropriate permissions we will logout of root and login as our newly created user. Open your terminal/SSH program and login to your server. Since we are logging in as root the command will be 'ssh root@Public IP'. From the above screen shot you can see my public IP is 74.201.240.211 and so my command will be 'ssh root@74.201.240.211 '. The root password is the server admin password you set while creating the server. Create a new user by following these steps: newuser can be a user name of your choosing. At the prompt create a password for this user and then accept the default user information by pressing ENTER at the prompts. In order to give newuser the correct privileges we must add the user to the sudoers file. Add new user in the ‘User privilege specification’ section. Once the file has been successfully edited press CTRL-x to exit the file; and Y to save the changes. Logout as the root user and then login as newuser . We now have a normal user with sufficient privileges to update the Ubuntu operating system and then install Docker. The version of Ubuntu that we installed while creating the server is 12.04 (Precise LTS 64-bit). This particular version of Ubuntu ships with a version 3.2 Linux kernel. In order to get the correct version of LXC that Docker requires we need to update the kernel from 3.2 to version 3.8. The Ubuntu folks at Canonical have made this easy for us by providing backported versions of new kernels. We will be updating Ubuntu Precise to Ubuntu Raring, at the same time will be getting built-in kernel support for AUFS. Once you have the base operating system updated and ready to go it is a pretty straight-forward process to install Docker. And that is all that is necessary to install Docker! Lastly, let's ensure that Docker is, in fact, correctly installed. The following is a very simple example of how to start an interactive Docker instance, download and run an Ubuntu instance, and then interact with the running container. You will be at a bash prompt within a Docker container. Type ‘echo Hello World from a Docker Container’. This will run the command “echo” inside a container and echo back the string “Hello World from a Docker Container” to standard out. Congratulations! You are now running Docker on Ubuntu within a CenturyLink Cloud server.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "install-manageiq-on-ubuntu-14-04-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/install-manageiq-on-ubuntu-14-04-on-the-centurylink-cloud/", "abstract": "One of the more interesting announcements to come out of the 2014 Atlanta OpenStack Design Summit was Red Hat's statement that they were going to open source ManageIQ, a cloud management platform. You can read the press announcement here . ManageIQ is actually a complete and mature project, having over 8 years of development. ManageIQ the company was founded in 2006 and acquired by Red Hat in December 2012. Subsequent to the acquisition, Red Hat has both integrated ManageIQ into its CloudForms product and spent significant effort to get the source code ready to be an open source project. About a month after the ODS announcement, Red Hat followed up and published the source code and created an open community to shepherd the project. Look here for the ManageIQ community site and here for the sources (hosted on GitHub). From the README: ManageIQ is a Cloud Management Platform that delivers the insight, control, and automation enterprises need to address the challenges of managing hybrid cloud environments. It has the following feature sets: We are going to install the current ManageIQ community code on the CenturyLink Cloud . If you don't already have an account, you can sign up for an account in minutes. When we are done you will have the complete system running on a CenturyLink Cloud server, be able to log into the management console, and be able to access the ReST API. While we go through this process, we'll overcome some \"challenges\" that as we deploy ManageIQ. Sounds like fun? Let's get started... We are going to install on an Ubuntu 14.04 server, so we first need to create the server on the CenturyLink Cloud and install the OS. Instructions for doing this are here . Note that in addition to the standard ports (when adding the public IP address) you should specify port 3000 as well. This is the Rails default port. Reboot your server and ensure that you have a terminal open and are logged into your server as root. While you can do initial development with the built-in SQLite database you will not want to go to production, or even serious testing, without a more industrial strength database. The standard choices are either MySQL or PostgreSQL, today we are going to install and use PostgreSQL as this is the recommended package for ManageIQ. First, let's update our apt information and then get the packages we need: When PostgreSQL was installed it set up a standard default user postgres . For ease of use we are going to remove the database password from this account. What the above commands do is: In order to ensure we can connect locally to the database we need to alter the pg _hba.conf file. Currently this is located in the /etc/postgresql/9.3/main/ directory. Find the following section and update \"peer\" and \"md5\" to \"trust\". Before: After: Lastly, we need to ensure that PostgreSQL is listening correctly for localhost connections. To do this we need to edit the file postgresql.conf , located in the same directory as the pg_hba.conf file. Find the line _listen_addresses_ and ensure it looks like: _listen_addresses = 'localhost' # what IP address(es) to listen on;_ (you should be able to just uncomment this line). Restart the database: The PostgreSQL database should now be running and you should be able to connect and interact with it locally (just as our ManageIQ applications will). To test enter the following: The first command will show you all the tables currently defined. The second command shows the currently defined users on the system. Remember to enter \"q\" to exit. Next we need to create the specific user and databases that ManageIQ will use. We just set up 3 databases, created a database user evm , and set the user evm as the owner of the development database. Exit from the postgres user back to the root user. The last thing we need to do before diving into getting and running the ManageIQ is to install and start the memcache daemon and install git. The following will fetch memcache and start the daemon, then install git. The ManageIQ project currently has a dependency on Ruby version 1.9.3. Also, bundler must be version 1.3.5, so we will need to remove the up-level version and replace it with the expected version. First, let's create a user to host and create a runtime context for the Ruby runtime and development. For ease of use set the password to centurylinkdev as well. In order to give centurylinkdev the correct privileges we must add the user to the sudoers file. Add new user in the ‘User privilege specification’ section. Once the file has been successfully edited press CTRL-x to exit the file; and _Y_ to save the changes. Logout as the root user and then login as centurylinkdev . We now have a normal user with sufficient privileges to install ManageIQ. To start with, we are going to make sure the apt repos are up to date and then install some dependencies. There are a few ways to install Ruby, we are going to use rvm as it one of the most popular install systems. Of course, first we have to install rvm and then we will install Ruby . Finally, we are ready to actually install ManageIQ! And everything is installed. Edit your PostgreSQL config file (at config/database.pg.yml) and add the user evm with the password smartvm to the base definition. When you are done your config file should look like: Lastly, we need to set up the database and start the service: Relax for a few minutes and then point your browser at [public IP]:3000 . You should see the ManageIQ welcome screen: Log in using the username admin and the password smartvm . You will see the global ManageIQ dashboard: Congratulations! You have the open source (community) version of ManageIQ up and running on the CenturyLink Cloud. And have it running on the latest and greatest Ubuntu Server and PostgreSQL database to boot. Future tutorials will build off this base as we explore how to manage your private OpenStack cloud with ManageIQ and also explore the ManageIQ ReST API.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "optimizing-docker-images", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/optimizing-docker-images/", "abstract": "Docker images can get really big. Many are over 1G in size. How do they get so big? Do they really need to be this big? Can we make them smaller without sacrificing functionality? Here at CenturyLink we've spent a lot of time recently building different docker images . As we began experimenting with image creation one of the things we discovered was that our custom images were ballooning in size pretty quickly (it wasn't uncommon to end up with images that weighed-in at 1GB or more). Now, it's not too big a deal to have a couple gigs worth of images sitting on your local system, but it becomes a bit of pain as soon as you start pushing/pulling these images across the network on a regular basis. I decided it was worthwhile to really dig into the docker image creation process so that I could understand how it works and whether there was anything we could do differently to minimize the size of our images. As a quick aside, Adriaan de Jonge recently published an article titled Create The Smallest Possible Docker Container in which he describes how to create an image that literally contains nothing but a statically linked Go binary that is run when the container is started. His entire image is an amazingly small 3.6 MB. I'm not going to be discussing anything quite so drastic here. As someone who's more comfortable working with languages like Python and Ruby I need a little bit more OS-level support and will happily sacrifice a hundred megabytes of space so that I can continue to run Debian and apt-get install my dependencies. So, while I'm jealous of Adriaan's diminutive image, I need to support a wider range of applications than is practical with his approach. Before we can talk about how to trim down the size of your images, we need to discuss layers. The concept of image layers involves all sorts of low-level technical details about things like root filesystems, copy-on-write and union mounts -- luckily those topics have been covered pretty well elsewhere so I won't rehash those details here. For our purposes, the important thing to understand is that each instruction in your Dockerfile results in a new image layer being created. Let's look at an example Dockerfile to see this in action: This is a pretty useless image, but will help illustrate the point about image layers. We're using the debian:wheezy image as our base, creating the /tmp/foo directory and then allocating a 1 GB file named bar in that directory. Let's build this image: $ docker build -t sample , sending build context to Docker daemon 2.56 kB: Step 0 : FROM debian:wheezy ---> e8d37d9e3476 Step 1 : RUN mkdir /tmp/foo ---> Running in 3d5d8b288cc2 ---> 9876aa270471 Removing intermediate container 3d5d8b288cc2 Step 2 : RUN fallocate -l 1G /tmp/foo/bar ---> Running in 6c797329ee43 ---> 3ebe08b36733 Removing intermediate container 6c797329ee43 Successfully built 3ebe08b36733 If you read the output of the docker build command you can see exactly what Docker is doing to construct our sample image: We can see the end result by looking at the output of the docker images --tree command (unfortunately, the --tree flag is deprecated and will likely be removed in a future release): In the output you can see the image that's tagged as debian:wheezy followed by the two layers we described above (one for each instruction in our Dockerfile ). We often talk about \"images\" and \"layers\" as if they are different things -- in reality each layer is itself an image. An image's layers are nothing more than a collection of other images. In the same way that we could say: docker run -it sample:latest /bin/bash , we could just as easily execute one of the untagged layers: docker run -it 9876aa270471 /bin/bash . Both are images that can be turned into running containers -- the only difference is that the former has a tag associated with it (\"sample:latest\") while the later does not. In fact, the ability to create a container from any image layer can be really helpful when trying to debug problems with your Dockerfile . Knowing that an image is really a collection of other images, it should come as no surprise that the size of an image is the sum of the sizes of its constituent images. Let's look at the output of the docker history command: This shows all of the sample image's layers along with the instruction that was used to generate that layer and the resulting size (note that the ordering of layers in the docker history output is reversed from that of the docker images --tree output). There are only two instructions that contribute anything of substance to our image: the ADD instruction (which comes from the debian:wheezy image) and our fallocate command. Together, these two layers total 1.16 GB which is roughly the size of our image. Let's actually save our image to a tar file and see what the resulting size is: When an image is saved to a tar file in this way it also includes a bunch of metadata about each of the layers so the total size will be slightly bigger than the sum of the various layers. Let's add one more instruction to our Dockerfile : Our new instruction will immediately delete the big file that was generated with the fallocate command. If we docker build our updated Dockerfile and look at the history again we'll see the following: Note that our addition of the rm command has added a new (0 byte) layer to the image but everything else remains exactly the same. If we save our new image we should see that it's almost exactly the same size as the previous one (it'll differ only in the little bit of space that is needed to store the metadata about the new layer we added): If we were to docker run this image and look in the container's /tmp/foo directory we would find it empty (after all, the file was removed). However, since our Dockerfile generated an image layer that contains a 1 GB file it becomes a permanent part of this image. Note: Each additional instruction in your Dockerfile will only ever increase the overall size of your image. Clearly, this example is a little silly, but the idea that images are the sum of their layers becomes important when looking for ways to reduce the size of your images. In the following sections, I'll discuss some strategies for doing just that. This is a pretty obvious tip, but the base image you choose can have a significant impact on your overall image size. Here are the sizes for some of the more common base images: On our team, we had been using the ubuntu image as the base for most of our work -- only because most of the team was already familiar with Ubuntu. However, after playing with the debian image we realized that it actually did everything we needed and saved us 100+ MB in image size. The list of viable base images are going to vary depending on the needs of the image you're building, but it's certainly worth examining -- if you're using Ubuntu when BusyBox would actually meet your needs you're consuming a lot of extra space unnecessarily. One thing I would love to see is the image size displayed on the Docker registry. Right now, the only way to see the size of an image is to first pull it to your local system. One of the nice things about the layered approach to creating images is that layers can be re-used across different images. In the example below you can see that we have three different images that all use debian:wheezy as their base: Each of the three images adds something on top of the debian:wheezy image, but there aren't three copies of Debian. Instead each image simply maintains a reference to the single instance of the Debian layer (one of the reasons I like the docker images --tree view is that it makes the relationship between the different layers very clear). This means that once you've pulled the debian:wheezy image you shouldn't have to pull those layers again and the bits for the image only exist once on your local file system. So you can save a significant amount space and network traffic by re-using a common base across your different images. In our sample image above we created a file and then immediately deleted it. That was a contrived example but similar scenarios often do occur when building images. Let's look at a more realistic example: Here we're pulling down a tar file, extracting it, moving some files around and then cleaning up. As we saw before, each one of these instructions results in a separate layer. Even though we're removing the tar file and the extracted directory those things are still part of the image. The wget command resulted in a layer that was 55 MB and then extracting the tar created a 99 MB layer. We don't actually need either of these things in the final image so we've got 150+ MB of wasted space here. We can work around this issue by refactoring the Dockerfile a little bit: Instead of executing each command as a separate RUN instruction we've chained them all together in a single line using the && operator. While this makes the Dockerfile a little harder to read it allows us to clean-up the tar file and extracted directory before the image layer is committed. Here's what the resulting image looks like: Note that we end-up with exactly the same result (at least as far as the running container is concerned) but we've trimmed some unnecessary layers and 150 MB out of the final image. I wouldn't recommend that you go and string together every command in your Dockerfile but if you find a pattern like the one above where you're creating and later removing files then chaining a few instructions together can help keep your image size down. The strategies we've discussed so far all assume that you're building your own image or at least have access to the Dockerfile . However, there may be situations where you have an image that was created by someone else that you'd like to slim down. To do this we can take advantage of the fact that creating a container from an image results in all of the image's layers being merged. Let's return to our sample image (the one with the fallocate and rm commands) and docker run it: Since our image doesn't really do anything it exits immediately and we're left with a stopped container that is union of all our image layers (I used the -d flag here only so that the ID of container would be displayed). If we export that container and pipe the contents into the docker import command we can turn the container back into an image: Note how the history for our new sample:flat image has only one layer and the total size is 85 MB -- the layer containing our short-lived, 1 GB file is completely gone. While this is a neat trick it should be noted that there are some significant downsides to flattening your images in this way: So I certainly would NOT recommend that you go and flatten all of your images, but it can be a useful in a pinch if you're trying to optimize someone else's image. Also, it can be a helpful tool if you simply want to see just how much space you could squeeze out of your own images. Managing the size of Docker images is a challenge. It's easy to make them smaller without sacrificing functionality. Don't have an account on CenturyLink Cloud? No problem. Just head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "upgrading-to-ubuntu-14-04-trusty-tahr", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/upgrading-to-ubuntu-14-04-trusty-tahr/", "abstract": "One of the primary Linux server distributions in use today in the cloud is the Ubuntu distro from Canonical . Ubuntu is a Debian-based distro and has a regular cadence of releases every 6 months. If you haven't cracked the numbering code yet, each release is indicated by the year (e.g. \"12\" for 2012) and then the month (e.g. \"04\" for April). In addition, each release has a whimsical name associated with it (the 12.04 release is \"Precise Pangolin\") with each subsequent release incremented alphabetically. In conversation you may hear someone refer to the \"Precise\" release, now you know that it was released in April, 2012. Every 2 years Canonical signifies a release as a \"Long Term Supported\" release, or \"LTS\". A normal release has a 9 month support window while an LTS release is supported for 5 years. This allows folks who desire a stable platform that is not moving every 6 months to stick on an LTS release, and then have time to migrate to the next LTS release. Two years ago the 12.04 Precise release was designated an LTS release. Recently Canonical released 14.04 (Trusty Tahr, or simply \"Trusty\") as the most recent LTS. We are going to demonstrate how to upgrade the latest Ubuntu image on the CenturyLink Cloud , which is the 12.04 release, to the latest 14.04 release. By the way, the interim Ubuntu releases have been: And the upcoming release will be 14.10 - Utopic Unicorn. We start with creating a CenturyLink Cloud server. Select the \"Ubuntu 12 64-bit\" image, 2 CPU's, 4GB of memory and build the server. Once this is complete add a public IP, enabling ports for PING, HTTP, HTTPS, and SSH. Your completed server should look like: From your terminal shell (ssh) into your new server, logging in as root. First thing is to check the version of Linux and Ubuntu that we are currently running: This will return a Linux version string that looks like: Linux 3.2.0-39-generic x86_64 And you should see a return that indicates that this is, indeed, the Precise release. No LSB modules are available.\nDistributor ID: Ubuntu\nDescription: Ubuntu 12.04.4 LTS\nRelease: 12.04\nCodename: precise First let's ensure that the apt package list is up to date and that we have the latest version of the update manager. Now comes the fun part. Since the Trusty release is so new it doesn't show up in the update manager list yet. Canonical waits until the first point release (14.04.1 in this case) to include it as an update target. While this is expected later this summer we can force the system to give us a Trusty release by indicating that we want the \"devel\" or latest \"unstable\" version. You shouldn't be scared of the \"unstable\" terminology, this is used to indicate code that is incremental to the last stable build but not yet certified as a release. In order to start the upgrade process (note that the \"-d\" parameter indicates the \"devel\" code): Answer 'y' to the intermediate questions until the following screen appears: There will be a series of screens with selections to follow: The system restart will terminate your ssh session, so wait a few minutes and then log back in as root. Now let's check the version of Linux and Ubuntu that we are currently running: This should now return a Linux version string that looks like: Linux 3.13.0-30-generic x86_64 And you should see a return that indicates that this is, indeed, the Trusty release. No LSB modules are available.\nDistributor ID: Ubuntu\nDescription: Ubuntu 14.04 LTS\nRelease: 14.04\nCodename: trusty Congratulations, you are now running the latest and greatest version of the Ubuntu server on your CenturyLink Cloud server!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "caching-docker-images", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/caching-docker-images/", "abstract": "Want to significantly speed up the Dockerfile code/build/test cycle? In this article I'll discuss how the Docker image cache works and then give you some tips for using it effectively. Each instruction in your Dockerfile results in a new image layer being created and added to your local image cache. That image then becomes the parent for the image created by the next instruction (see my previous article for a detailed explanation of the image creation process). Let's look at an example: If we docker build this Dockerfile and inspect the local image cache we'll see something like this: The FROM instruction in our Dockerfile corresponds to the image layer tagged with debian:wheezy . The three child layers shown underneath that correspond to the other three instructions from our Dockerfile . Another way to look at this is with the docker history command: With this view, the order is reversed (the child image appears before the parent) but you do get to see the Dockerfile instruction that was responsible for generating each layer. After you've successfully built an image from your Dockerfile you should notice that subsequent builds of the same Dockerfile finish significantly faster. Once docker caches an image layer for an instruction it doesn't need to be rebuilt. Let's look at an example for the Dockerfile above. We'll run the docker build command with the time utility so that we can see how long the initial build takes to complete. You can see that it took 21 seconds to complete the build. Our example here is fairly trivial but it's not uncommon for builds to take many minutes once you start adding more instructions to your Dockerfile . If we immediately execute the same instruction again we should see something like this: Note how each instruction was followed by the \"Using cache\" message and the total build time dropped from 21 seconds to less than a second. Since we didn't change anything between the two builds there was really nothing for docker to do — everything was already in the cache. As Docker is processing your Dockerfile to determine whether a particular image layer is already cached it looks at two things: the instruction being executed and the parent image. Docker will scan all of the children of the parent image and looks for one whose command matches the current instruction. If a match is found, docker skips to the next instruction and repeats the process. If a matching image is not found in the cache, a new image is created. Since the cache relies on both the instruction being executed and the image generated from the previous instruction it should come as no surprise that changing any instruction in the Dockerfile will invalidate the cache for all of the instructions that follow it. Invalidating an image also invalidates all the children of that image. Let's make a change to our Dockerfile and see how it impacts the local image cache. We'll update the apt-get install instruction to install Emacs in addition to Vim: Let's build our new image and see what happens: Since we didn't alter the MAINTAINER instruction it was found in the cache and used as-is. However, we did edit the apt-get line so it resulted in a completely new image layer being created. Furthermore, even though we didn't change the ENTRYPOINT instruction at all, its layer also had to be rebuilt since its parent image changed. If we look at the image tree again we can see the two new layers that we created alongside the layers that were generated from the previous version of our Dockerfile : Note how the layer for the MAINTAINER instruction (c58b36b8f285) remained the same, but it now has two children. The layers generated from the previous version of our Dockerfile are still in the cache it's just that they are no longer part of the tree tagged as vim:latest . Now that you are familiar with how the Docker image cache works, next week we will discuss some strategies for making the most of it when working on your own Dockerfile .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-chef-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-chef-on-the-centurylink-cloud/", "abstract": "A key element in any Continuous Integration and Continuous Deployment strategy is the automation of infrastructure and application deployment. Over the past few years a discipline called \"DevOps\" has arisen around this philosophy, driven by the availability of automation tooling, such as Puppet from Puppet Labs and Chef from the company of the same name. Being able to codify how infrastructure is setup and how applications are deployed in a repeatable and immutable manner allows for a much higher velocity process without introducing the fallible human element. In other words, let the computers do what they do best. We are going to install the Chef Server and Chef Client/Workstation ; together these comprise the management, control, and authoring systems for Chef. Once you have these servers up and running we will create a Chef Node and initialize it to the server. Note that we are going to be running these systems (Server, Client/Workstation, and Node) on separate servers (so you will end up provisioning 3 CenturyLink Cloud servers). We want to be running on a modern OS, so lets create these CenturyLink Cloud servers and bring the Ubuntu OS up to the latest version. Instructions for doing this are here . Select the server you have chosen to be the Chef Server and login as root. We are going to set up a Chef version 11.1.4 server, you can check to see if there are newer deb packages and substitute the package name where appropriate. Also, since we are not setting up DNS, we will use the public IP as the FQDN to identify the Chef Server and Client/Workstation. Pull up your browser and point it to https://[public ip] . If you get a certificate error don't worry, just indicate you want to continue.You will see the Chef Server logon screen (as below). At this point we have successfully installed the Chef Server and next will install the Chef Client/Workstation. Next, select the server you want to install the Client/Workstation on, and login as root. We need to first grab git : Then, install Chef: And that's all there is to installing the Client/Workstation. Check that everything is OK by running a validation: You will get the version string of the installed version of Chef back. The Chef Client/Workstation is the primary interface to the Chef system, and will contain a local copy of the repository. In order to create the local repo: In order to start using the Client/Workstation we need to set the local system variables and then configure the client to talk to the Chef Server. First, let's configure the local system vars: In order to securely contact and interact with the server we need the private keys for two accounts: admin and chef-validator . These are located on the server we created earlier, to fetch them we will use scp and connect to the server using its private IP . The primary client program is called knife , we now need to create a knife configuration file. Start by: Work through the questions, using the private IP of the server when asked for the server URL. Your screen should look like this when you are done: Finally, make sure your Client/Workstation is installed and can talk to the server: Should return:\nchef-validator\nchef-ui Should return:\nadmin\nknife The final piece of the Chef configuration is the Chef Node. This is the server that will be deploying and running the code as specified by the appropriate run lists, cookbooks, and recipes. This process is very similar to setting up the Workstation. First, install Chef: Copy the chef-validator.pem key from the Chef Server: Register your Node with the Server: Create a client.rb file in /etc/chef (using vi or your favorite editor) and add the following: log_level :info\nlog_location STDOUT\nchef_server_url 'https:// [Server private IP] :443′ Finally, validate your Chef Node installation: You should see a response that looks like: You now have a working 3 server Chef installation running on the CenturyLink Cloud. The next steps will be to create cookbooks for your Chef Node and deploy them using the chef-client. An excellent resource to getting started is the Chef Supermarket , a large collection of cookbooks and recipes that have already been vetted by the Chef community.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "openstack-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/openstack-on-the-centurylink-cloud/", "abstract": "OpenStack is one of the fastest growing open source projects being worked on today. As a cloud IaaS solution it has many aspects and moving parts, which can make even getting a demo version installed a difficult task. Today we are going to stand up a complete OpenStack cloud on top of the CenturyLink Cloud and allow you to play with the Web UI, the exposed services API's, and whatever depth you would like to go into looking at the inner workings of the system. Lucky for us there is a shortcut to standing the demo version up... We will be using the single VM install that the actual contributors to the project use to develop new code and patches. This project is called \" DevStack \" and it allows us to deploy and run the base OpenStack components within a single CenturyLink Cloud virtual machine. It is important to realize that in order to run OpenStack Compute (Nova) effectively we need to give it resources... Nova is a cloud controller that will allow us to create and run multiple VM's so our CenturyLink Cloud Server needs to be pretty large. I have created a server with 4 CPU's, 48 GB of RAM, and the default amount of storage, as seen here: The base OS is Ubuntu 12.04 64 bit. A key advantage of installing this base OpenStack image is that we can snapshot it and re-use it as we move to add more services, multiple servers, etc. After creating the server and adding a public IP you should create the non-root user that will be the primary demo user. You might think that just running as root would be simpler, but DevStack is smart enough to detect that you are trying to run in this \"dangerous\" manner and refuse to start! It is generally not a good idea to run as root, so in order to update our Ubuntu system and install DevStack we will first set up a new user. After giving the new user the appropriate permissions we will logout of root and login as our newly created user. Open your terminal/SSH program and login to your server. Since we are logging in as root the command will be 'ssh root@Public IP'. From the above screen shot you can see my public IP is 64.15.184.33 and so my command will be 'ssh root@64.18.184.33 '. The root password is the server admin password you set while creating the server. Create a new user by following these steps: At the prompt create a password (use 'stack' for convenience) for this user and then accept the default user information by pressing ENTER at the prompts. In order to give stack the correct privileges we must add the user to the sudoers file. Add new user in the ‘User privilege specification’ section. Once the file has been successfully edited press CTRL-x to exit the file; and _Y_ to save the changes. Logout as the root user and then login as stack . We now have a normal user with sufficient privileges to install OpenStack. We now need to install 'git': DevStack is a shell script that is intended to offer a single system, running OpenStack cloud. Primarily developed and maintained so that folks can develop new code and patches for the open source project it is also a great way to install a OpenStack demonstration system. The DevStack script will install everything needed to run a full OpenStack Cloud, no need to piece-meal anything else. Since the script points to the latest builds there is another advantage that you will get the latest bits by default. Look here for more details on DevStack. First clone the DevStack code repo: And, run the DevStack install script: This is an extensive install script (as you can imagine) and you will see a bunch of stuff scrolling by on the screen. Eventually you will get prompted to enter passwords for the default services that are being installed. It is OK to let the system generate these as you will not be interacting with the services directly at this point. However, when it asks for the Horizon/Keystone password enter 'stack', as this will be your login to the OpenStack Dashboard. The Horizon Dashboard is located at the public IP for your CenturyLink Cloud Server. Point your browser to this IP (in my case this is http://64.15.184.33 ) and you will be prompted to login to your newly installed and running OpenStack Cloud. Use 'admin' as the user and 'stack' as the password and you are now able to fully administer and play with your OpenStack Cloud! Be sure to take a snapshot of this server as we will use this in future tutorials to expand the services mix and show you various things you can do with your new OpenStack Cloud. For more information on OpenStack look here .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "heroku-on-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/heroku-on-docker/", "abstract": "I am sure you have heard of Docker , but have you ever actually deployed a real app on it? How would you even start to move Heroku's 4+ million apps into Docker Containers? Not many people have even tried. Building an app on Docker can be incredibly hard and frustrating. Not at all like using Heroku where everything is taken care of for you. With Docker, you have to learn about Dockerfiles and then try to get one that works just right with your code. If you are lazy (like me) and want to just try out Docker with no fuss, I will guide you through the whole process from start to finish. In the end, we will have generic containers we can use with any Docker Developer Tools like CoreOS or Docker Compose manifest files. We will create: All without learning anything about Linux Containers or Dockerfiles. If you are on a Mac, the install process has been greatly improved. You can now get Docker running in just seconds: You can use github.com/CenturyLinkLabs/building (like progrium/buildstep but without tarfiles and with extra goodies) to turn any Heroku-compatible app into a Docker Container: Hint #1: To run your app, try: docker run -d -p 8080 -e \"PORT=8080\" myapp Hint #2: To re-build your app, try: docker build -t myapp . (Optional: this will allow you to share your app container with anyone in the world via docker pull myapp. Make sure to make it a private repo if it has your private code in it) Yes, that's it. Everything else is handled for you. Code discovery: done. Dependencies: done. Startup script: Done. Heroku on Docker: Done. This works with any technology: Ruby, Node, Java, Play, Python, PHP, Clojure, Go, Dart and more. Don't believe me? Let's try a simple Node application. PRO TIP: Avoid using semver ranges starting with '>' in engines.node (See https://devcenter.heroku.com/articles/nodejs-support ) To run your app, try something like this: In my case: Ok, that was cool, but how about a Rails app? Can you do a Rails app? To run your app, try something like this: docker run -d -p 8080 -e \"PORT=8080\" myuser/rails-container-name:latest Ok, that was cool, but how about WordPress running HHVM? To run your app, try something like this: docker run -d -p 8080 -e \"PORT=8080\" myuser/wordpress-container-name:latest Wait, that's not HHVM. That's nginx. No fair, I cheated. Now to do it right: To run your app, try something like this: docker run -d -p 8080 -e \"PORT=8080\" myuser/wordpress-container-name:hhvm With building you can too. There is a -o flag that lets you set a fig.yml output file which you can pass to Fig. ( Edit: Fig is now Docker Compose ) Notice that when you combine building with fig , you never need to manually run a docker command again. Ahhh, abstraction. One of the most powerful parts of Docker is the fact that you can modify every byte of the underlying \"slug\" as Heroku calls it. You can put any binaries in any locations. You can build your own Docker images from scratch. But that power comes at a price: ease of use. Tools like building and fig set out to bridge the gap and let everyday developers take advantage of cool new technology without having to get a PhD in DevOps.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-is-rocket-and-how-its-different-than-docker", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/what-is-rocket-and-how-its-different-than-docker/", "abstract": "Docker is not the only Linux Container manager available. Recently, CoreOS launched their own project named Rocket which has some architecturally different approaches to managing containers. This week, we interview the brain behind Rocket to ask what is Rocket and why they built it. Brandon Philips is the Co-founder and CTO of CoreOS Prior to starting CoreOS, Brandon spent some time at Rackspace where he worked on monitoring products. He worked on a platform called Lubbock where they did LUA sort of a Node.js style LUA thing. Before that he did kernel infrastructure and kernel hacking at SuSE and the SuSE Labs Group. Rocket is a container runtime, which means a few things. The first is that it downloads images of the application over the Internet, and in future versions it will verify those, and it runs those inside of a container. A lot of people have a good sense of what a container is, but in simple terms a container is a process that is running on your host and it is isolated from other processes. A few things that we have done a little bit differently; first is that Rocket does not have a daemon so when you run \"rocket run coreos/etcd\" that is actually executing directly under the process that you started it from. So, you know your bash is PID 400, then \"rocket run\" is going to be 401. This is a pretty substantial design difference, we wanted to have Rocket be essentially a view on a static binary. Much in the same way when you compile a Go program and run that it executes directly underneath your initial process. In a way, in that Docker runs these things in separate process. You can think of it in another way... So you already have an init system like upstart or systemd or daemon tools or runit, when you do \"rocket run\" it actually gets monitored and run under that init system. This can be a little bit of a tricky point if you're using Docker for these sorts of use cases. Because the (Docker) daemon will, when you do \"docker run\" will be underneath the PID namespace of the Docker daemon. Yes, there is a community project that someone announced yesterday that converts a Docker image into the App Container image. Another thing is that we have worked with a number of people and groups within this \"container eco-system\" and got some feedback and created a separate spec outside of Rocket which defines what we are calling an App Container, an App Container image, and an App Container runtime. Those projects now live at github.com/appc and so there is a tool that converts from a Docker container to an App Container image. Through the image itself. Yeah, you can imagine doing that, there's no direct tool right now. Part of what Docker is, is that Dockerfile and that Docker build process. I think that process is fine and it is a pretty reasonable way of putting together a container. With the App Container runtime another difference from Docker is that Rocket runs multiple processes inside the container. So you have an active monitoring init system that ensures you have the ability to restart processes, or say, have two or three processes sharing resources inside of a single container. So, you can think of it as there's an outside container, the root container, and each app runs in its own individual container an you can put contraints on those things. This kind of fixes the problem that a lot of people have where they will install daemon tools or runit inside their Docker container. Instead you can do that as a first class process and compose different images together in that single root container. Yeah, it's identical to how you have a root process (in Docker) and that can come from wherever you want; you can build it yourself or use a Debian or use a Fedora or whatever. So, right now there there are build tools that are merging, like build tools that create a Go ACI (Application Container Image) directly from Go source code, or convert from Docker images to ACI's. That sort of whole eco-system is developing. Any modern Linux kernel should work, so if you have a modern Ubuntu or Fedora or CoreOS or whatever, its a single static binary. There's a few technical reasons why we wanted to build this. One of the major ones was this daemon-less mode and this enables us to do things like integrate well with socket activation, so the idea that I pass file descriptors from one process to another and those file descriptors may be open, listening sockets. This has a lot of nice security things that come out of it and also dependency management things. So you can imagine that I have, on a systemd system I can say listen on port 443 and then hand the file descriptor, if someone comes in on that socket, hand that file descriptor to my Rocket container that is running my web server. The cool thing about that is that if you architect your application correctly, the web server doesn't have to have any networking at all. It just has this one file descriptor and so it doesn't have to have any outbound networking, it doesn't have to know anything about routable Internet addresses. So this is a nice little feature, but it means that it can play well with existing systemd or upstart or whatever init systems. We have had a few bootstrapping problems that we have hit (with Docker) and talk about on our mailing list, for example we have an overlay networking system called Flannel and we'd like to run that in a container. We ended up having a bootstrapping problem how do we run that in a container that then needs to configure the outside Docker daemon and then start that Docker daemon with the correct network configuration? So these sorts of advantages were what we had in mind for daemon-less mode. Also the (App Container) image format allows for a few interesting things. The first is that it can be signed by regular GPG keys and it has a discovery mechanism. So your registry for these images can be a simple http server or you can do more interesting things like if you run coreos.com/etcd:version050 on Rocket that actually uses a mechanism that looks at the http pages and finds out where the image that the binary for etcd lives and downloads it and launches it. You don't have to know where that image is; its actually hosted on GitHub, but we own that domain (coreos.com/etcd) so people can kind of spread out their namespaces where containers live over the internet. This is how a lot of protocols on the Internet work. Yes, so Docker isn't going anywhere; we have a ton of users using Docker and we think the Docker platform is just fine. What Rocket will be used for is for some of these problems I've talked about, for example being able to do this Flannel overlay stuff. There's a number of places where we just continue to add more stuff to the CoreOS system but it would be really nice if we could run those things in containers. Helping to solve those problems is where we will start to see Rocket more. I can certainly see a use case for both things depending on what you are trying to do with your host. I think that is reasonable. From that time period when we had git coming out what we kind of found was that there is plumbing, git has this concept of plumbing and porcelain. The plumbing is the stuff that hopefully everyone can agree on but there is going to be different workflows on top of that plumbing. What I would like to see with the App Container spec that kind of becomes the plumbing of what we think of as these \"application containers\", so we agree on file formats, how these images are found, how they are downloaded, and how they are assembled on disk. Then it is perfectly great, and I actually want to encourage this idea, that lots of different runtimes might exist. Because POSIX and the way people architect their systems, they're complicated and a lot of people have very strict or interesting needs, so I think it is OK if we end up in a place where there are lots of different runtimes. For example, Mesos may have a different runtime than Cloud Foundry than, etc. etc. And I think that is a reasonable place to end up. But we need to make sure we can agree on that plumbing piece that's kind of the thing we are trying to do with the App Container thing. That's a great question. Early on we tried to get the stand-alone mode into Docker and Red Hat contributed a bunch of things that kind of go down that path and it kind of kept getting delayed. That was one of the reasons. The heritage of the code was we started it about two or three weeks was the first commit before we put it up on our blog post. It uses a bunch of libraries that we had already written for Fleet and for some of our other tools. We kind of put together the prototype and the specification over that time period, its definitely a raw project its a brand new raw open source project and it is progressing pretty well. Right now, we are definitely on the alpha side of the development process. What we are doing with Rocket is fairly straight-forward and well scoped. We're not trying to create a lot of software here and we want to leverage as much stuff as we can. So hopefully early next year (2015) we can get to the point where it is beta and start testing it out with some of the use cases that we have in mind. We'll see how it goes, you always underestimate how hard software is. The first place we can definitely work together is on the App Container spec and the image format. Over time, the goals of the projects will look less similar, which is perfectly reasonable. We want this tiny little package manager plus container runtime and the Docker project is moving more towards a platform, and that's a reasonable thing too. With CoreOS we want try to ship as little stuff as possible so its a slight difference of end goal. The big exciting one is that etcd has been maturing quite nicely, we have projects like Flynn and Deis that rely on etcd in a pretty big way inside those projects. We have made a big investment in the stability of etcd and, not just the stability but the operationability (if that's a word). We have had all this experience with users interacting with the system and for the most part it has worked as designed, but there are definitely some sharp corners that people have been stubbing their toes and cutting themselves on. Etcd now has a lot of nice tooling in these alpha releases for 0.5 that we have been doing for adding and removing hosts and for making sure that configuration errors don't actually end up in cluster failures. We added all these unique UUID's all over the place that we can detect \"hey you're adding a member from an old cluster or from this other cluster and you can't bridge two clusters\" and reject those sorts of membership changes. And detect changes and faults for hard disk and that sort of thing where we found people would have a log file that had a random bit flip, and its very hard to debug so we added checksums and that sort of stuff. So, we are really focusing so people can detect and recover from mistakes.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "getting-started-with-application-templates", "author": ["\n              Laura Frank\n            "], "link": "https://www.ctl.io/developers/blog/post/getting-started-with-application-templates", "abstract": "Panamax application templates allow you to deploy your applications with one click, to whatever cloud target you want. Keep reading to learn how to create your own application templates and simplify your deployment workflow with Panamax's intuitive UI. While your first experience with Docker was probably tinkering with it via the CLI, you may have quickly become tired of manually configuring and managing each container independently. Wouldn't it be great if you could just customize everything in one place, and tell Docker to fire up all of your containers with one command? Application templating allows you to do just that. There are a few different choices for authoring templates, including Docker Compose (formerly Fig) and Panamax , but the basic principle is universal across all tools: provide everything necessary to get your application running -- services, variables, dependencies, you name it -- and build it with one command. Specifically, Panamax templates are powerful because Panamax allows you to deploy your applications with one click, either on your local machine or on a variety of remote hosts like AWS, Google Cloud or CenturyLink Cloud. This is a useful workflow if you'd like to test your code in multiple environments, easily package your code, share a prototype you've been working on, or allow other Panamax users to build your application. There are two basic strategies for working with application templates in Panamax. The first is to author the template from scratch and save it out as a template.pmx file. The .pmx files are very similar to .yml, which was done on purpose. For this reason, it's easy to go back and forth between Docker Compose and Panamax. Here is a sample Panamax template: The second way to work with templates is by building your application with Panamax and saving it as a template -- no typing or formatting required. Below, I'll explain some Panamax features and best practices to aid you as you templatize your application. After installing and firing up Panamax , you can start building your application by searching for images to include. Panamax search will query the Docker Registry as well as any private registries you've set up. You can even specify an absolute path to an image . While working directly with images -- not existing templates -- your only option is to deploy the image locally. For any image, you can use Panamax to customize the container to include volume mount points, environment variables, and port mappings. A feature specific to Panamax is the concept of service categories. By grouping your services into logical categories, it's easier for other users of your template to see your application's architecture. You can create categories, rename those categories, assign services to categories, and even reorder services within categories all on the application details page. Within your application, you can move a service from one category to another by dragging and dropping it. Additionally, you have the capability to create a new category by clicking the button in the \"Add Category\" panel. Note that these categories exist in the presentation layer, but have no impact on the way your application performs. However, they are a useful way to outline service architecture to future users of your template. After you've configured your application, saving it as a template is easy. From either your application list page, or the application details page itself, click \"Save as Template.\" If it is your first template, you'll be asked for a GitHub access token so that Panamax can save your template to a repository. You can create a new authorization token directly on GitHub . When saving your template, you have the opportunity to provide additional details about the application and its intended use. The template's icon will accompany its name in search results, and keywords will help other Panamax users find your template. Most importantly, it's essential that you include documentation. Even a simple application requires quite a few customizations to get it up and running. In order for other developers to easily use your template, make sure to document any necessary configurations needed to run it. This includes environment variables, port bindings, and virtual machine requirements (like memory and CPU). Check out the suggestions for creating good template documentation . Templates within your source repositories will come back in search results for any matching term. You will be able to see a short description of the template as well as how many services (containers) it includes. Clicking Run Template will give you two options: either running locally (great for testing!) or deploying remotely. For an introduction to remote deployments, check out the documentation or read the blog post. And that's it! Your application is now running without having to fire up containers individually. Panamax also provides you with the capability to access templates from any public GitHub repository. You can find a list of your template source repositories on the Dashboard. When you drill down into your sources, you can see how many templates are in each repo and when it was last refreshed. You also have the option to refresh the source in order to load any new templates you may have created. To add a repository, just click \"Add a New Source\" and provide the username and repository name. The templates will be loaded, and you'll be able to see those templates in the search results. Use official, versioned images, or images with a Dockerfile Make sure the images included in your application have a Dockerfile, which is valuable when trying to troubleshoot. It's a good idea to have a record of exactly how your images are being constructed. If using a remote image, you can check out the image by viewing it on the Docker Registry. Almost all images have a tag 'latest' that is run by default. This is a running tag which encapsulates whatever the most recent code on master is. This can be problematic because code changes rapidly -- the newest version of the image could work slightly differently than the version you ran, creating a problem in your template. If possible, try to use a version tag to avoid potential conflicts. Again, be sure to include as much documentation as possible with your template. If you're not sure what to include, check out the suggestions for creating good template documentation . Selecting a specific host port for your port mappings and bind mounting directories from your host OS can be useful tools but might limit the re-usability and portability of your template. If you can simply provide instructions on mappings or bound directories, it may make your template more usable for others. Future users of your template may have a use case that is very different from yours, so if it makes sense within the context of your application, avoid hard and fast definitions when you could instead provide instructions.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "from-panamax-to-docker-compose-and-back-again", "author": ["\n              Mike Arnold\n            "], "link": "https://www.ctl.io/developers/blog/post/from-panamax-to-docker-compose-and-back-again", "abstract": "When we created Panamax, there was no standard way to define and run multi-container applications with Docker, so we created Panamax templates knowing full well they would only run within Panamax. Later we added remote deployments in Panamax , allowing users to easily deploy running applications into environments where Panamax was not running. This remote deployment capability supports a \"pluggable\" adapter model, and adapters have been written for Fleet and Marathon . However, there are still cases where users will want to deploy to environments using standard Docker tools and processes like Compose and Swarm after having built up a multi-container application within Panamax. Last week's UI update, Panamax v0.2.21 , enables users to export Panamax templates as Docker Compose YAMLs, and then run and deploy those YAML files using Panamax. Panamax is not just for deployments, it is also a great tool for rapidly composing multi-container applications. The ability to search for images and link containers together makes it simple to create and run applications with Docker. And if there’s a Panamax template already defining the application to be run, it’s even simpler. Once an application is running, there are several actions that can be executed from within the Manage Applications view. Expanding the action menu reveals the latest of these, the ability to “Save as Compose YAML.” There are a few different options for creating a Compose YAML discussed in a previous blog post . If modification to the YAML file is desired, clicking the link to “Inspect & Validate in Lorry.io” will export the file contents to a secret, anonymous Gist and subsequently open and import the file into Lorry. Once within Lorry, the document can easily be edited to change or add values of existing services or add more services to the application, and when the changes are done, the document can be saved as a secret, anonymous Gist or saved to a local file. With a Compose YAML tucked away in the file system, it’s a simple matter to run the application defined therein with Docker Compose. However, with this latest release, an application defined in a docker-compose.yml can also be run within Panamax. Better still, along with the ability to upload a docker-compose.yml to Panamax, one can also specify the URL for a remote Compose YAML, for example one saved out of Lorry as a secret, anonymous Gist. To make this possible, we’ve added a new “From a Docker Compose YAML” option to the “Create a New Application” button on the Manage Applications view. Choosing this new option opens a dialog box with two tabs, one for uploading a docker-compose.yml and one for entering a URL to a remote YAML file. In either case, once the file or URI has been specified, clicking the “Run Compose YAML” button will create and run the application defined in the Compose YAML in Panamax. Whether you have an application already defined in a Docker Compose YAML that you want to visualize and modify with Panamax, or you have an existing Panamax application that you want to run with Compose, we’ve got you covered.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "data-visualization-reflect-dvaas", "author": ["\n              Brad Heller\n            "], "link": "https://www.ctl.io/developers/blog/post/data-visualization-reflect-dvaas", "abstract": "Check out this interview with Brad Heller, the co-founder and CTO of Reflect.io , a data visualizations and analytics platform for developers. He is also a long-time Orchestrate user, which is why we reached out to learn how he's using our NoSQL database-as-a-service (DBaaS) and what's going on with his new startup. My passion is building companies and cultures that foster innovation. I'm always looking at things through a new or different lens--the status quo doesn't have to exist. Big data is ripe for a disruption, and we're going to lead that at Reflect. Reflect is located in Portland, Oregon. It’s actually getting less and less, even though we’re still young. I’m handling a lot of customer success stuff and a lot of product stuff, so I would say I spend about 50% of my time coding. Reflect is data-visualization-as-a-service. We’re doing for data visualization what Twilio did for communication, or what Stripe did for payments. We’re making it possible for developers to add really great dashboards and reports for their products in just a couple of minutes with our hosted tools and our JavaScript libraries. Go – all the programming communities are starting to move over to Go. I was a sort of an early adopter, I’ve been writing for a year and a half. We use a lot of JavaScript too. While we were busy with dynamic programming languages and scripting languages, there was all this research work being done in the compiler world (LLBM and GCC), and it’s like we almost forgot about it for some time. Then someone mentioned it, and it was like ‘oh man’ we could actually statically link and compile programs (we had forgotten you can do that) as a super convenient way of distributing software. Go is this super cool mixed-language in that it has the performance of a low-level system like C, but it has the expressiveness of Python. My co-founder and I had been working on analytics and data visualization for a very long time. I’ve done it from the back-end, and he’s done it from the front-end. We found we were basically building the same infrastructure over and over again. Companies are spending way too much money building and maintaining the teams that enable this stuff. There aren’t great tools on the market that provide a higher level of functionality besides the standard charting libraries. It’s a very unproductive experience creating data visualizations right now. The third component is that doing data visualization right is really hard -- it’s a well-established science in terms of how humans digest information and it gets screwed up a lot. We can help with the design; we can help choose the right way to visualize your data. Our hope is that since so much of the industry is struggling to get the basics of data visualization up and running, if we can commoditize the process and functionality, we can move the industry forward. Our tools replace an entire function within an engineering organization. Often times when you buy a developer tool it replaces an entire horizontal slice in the company, e.g., Orchestrate replaces the database portion. However, our tool replaces more of a vertical slice – we have our front end tools, our middleware for generating report data, and then we have our back end. We have a lot of technology that goes into making Reflect work, so there are a lot of features and functionality that we need to keep an eye on. Also scaling other people’s databases is challenging. The way that our tool works, we have an agent that connects to your database and then we use that to issue queries to get data out to service data visualization. Often we'll run into random performance problems with other people’s databases that hinder our performance. For our transactional stuff -- we’re inserting ourselves in the middle of a user’s request cycle/their interaction with a company. Our services always have to be on and performing. So doing simple things like checking an authentication key is still valid -- we’re going to do that a lot with our tool, but we also need to do it as quickly as we possibly can, as any performance degradation on our side is going to effect how our customers and their customers see us and them, respectively. Availability is really important and is a reason we delegate that to external services like Orchestrate. It lets people do something that they couldn’t very easily do. You would have to build a team to do it, and now you can get it off-the-shelf by using Reflect. Companies can now get data to their customers in a much better way than they could previously. Some companies may have customers with custom data that wouldn’t be easy to represent in their current system, but with Reflect it’s easy to create custom, on-demand reports. The line between genius and crazy is super thin. Sometimes it’s really hard to figure out which side you fall on. There’s been a ton of features/ideas that we think are amazing, but don’t pan out when you put them out in the market. Gut checking yourself early, often, and knowing when to stop is a challenge, but it’s important to discover the checks and balances. We don't worry about scaling our transactional stuff at all. We deploy in a new data center and Orchestrate is able to handle it, no problem. Multi-region deployment is simple. The different semantics that Orchestrate puts on top of data -- having the graph, search, and events functionality allows us to solve problems without having to think about how the data is structured too deeply. We’ve always been able to get at the data we need without having any significant data performance. We think about visualizations much more holistically. We don’t think about just the graph itself, but the visualization in the context of the graph. A graph is just a part of the visualization. The data is important and different ways of exploring the data that drives the visualization are important. Yes. Check out Reflect.io – we’re in private beta. We use Github for all our orchestration and Trello for managing tasks. 90% of our projects are in JavaScript or Golang (we use ESlint and Golint to make sure everyone is consistent). We also use Gitflow for workflow management, and Docker and Mesos in production. We’ll deploy software maybe 10x a day. We move fast; so our workflow is geared around that. I worked with Alex in Cloudability in 2011. I first met Alex in PIE (Portland Incubator Experiment) when I was working on my last startup. At Cloudability, Alex’s side project was working on the front-end, and since I had back-end experience in the same space, we decided to team up. Techstars is a global ecosystem that helps entrepreneurs build their businesses. We picked Techstars because we had been working really closely with Chris Devore who is the PM up here. Chris is great and his network is awesome. It was a no-brainer to jump in when the invitation was offered. I’ve been in and around the TechStars ecosystem for a long time -- with Cloudability -- so I knew what to expect. The advice I would give is that you shouldn’t stress out about your performance relative to the class. It doesn’t really matter. You should focus on what you’re going to do and do the best that you can, and everything else will fall into place. Sign up for our Developer-focused newsletter CODE . Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. CenturyLink Cloud – We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "manage-and-scale-three-tier-applications-with-bare-metal", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/manage-and-scale-three-tier-applications-with-bare-metal", "abstract": "The cloud revolution has ushered in some amazing advancements in the IT world, but with advancements often come obstacles for companies and those who support the technology. With the emergent new possibilities of what Big Data , IOT (Internet of Things) , and Docker can bring to the marketplace, industry leaders are now, more than ever, in need of a better way to scale their applications without losing the features they love about the current multi-tenant cloud environment. Developers and Enterprise execs alike have voiced the demand for powerful, dedicated servers that allow provisioning and management from a single platform. They want features like self-service, on-demand provisioning, and vast automation. The solution: single-tenant, high-performance physical machines with cloud-like attributes integrated into a single, easy-to-use platform . CenturyLink Cloud Bare Metal Server technology brings an integrated user experience with single-tenant environment to the cloud. These self-service dedicated servers can be an ideal home for enterprise applications that are either difficult to virtualize or that carry compute-intensive workloads, such as Oracle & SAP. Physical servers can often offer a cost saving opportunity vs VMs when dealing with software licensing constraints. This solution provides our customers more predictable performance for their applications to thrive combined with the ability to spin them up incredibly fast and provision them using the same management interface as their CenturyLink Cloud virtual servers . Previously, the only way to gain complete physical isolation and single tenancy would have been to provision a dedicated server, either on-premises within an enterprise’s data center, or off-premises with a web hosting provider. While the latter would generally have been most expedient, it could easily take two-to-five days to deploy a physical server. And you’d be locked into a monthly contract, usually on at least a 2-year term. With Bare Metal cloud servers, you can be up and running in less than an hour, and pay on a true utility-basis, as you go, for only as long as you need that server. Customize your hybrid cloud solution through a unified management of resources, by combining CenturyLink’s Bare Metal Servers with our elastic Cloud Servers on the stack to manage and scale three-tier applications. Multi-tier applications can have diverse requirements that the CenturyLink Cloud solution can easily tackle by combining virtual machines with our Bare Metal Servers. Our customers can benefit from this hybrid cloud model by deploying the appropriate application tiers to the best suited form of infrastructure. The web tiers get the agility and elasticity of the virtual environment that they demand, while the application and database tiers requirements for performance and isolation are met on CenturyLink Cloud Bare Metal Servers. Another way of benefiting from this hybrid approach is to start with a development environment that’s entirely virtual. Then as you move to test and production environments, you can migrate different workloads to physical servers. This puts the flexibility and cost-effectiveness of the cloud to good use. So in the end you may keep your web tier on Cloud Servers, but deploy your applications and database on Bare Metal Servers. Implementing CenturyLink Bare Metal Severs give your engineers the raw power and consistency needed to successfully scale and remain agile. In an effort to better meet our customers' needs and the ever-changing market, we beefed up the feature set and availability for our Bare Metal servers, providing our customers with more options regarding how and where they can build their Bare Metal instances. Any database administrator can tell you how challenging it is to adapt a database from dev into test and eventually into production. How many resources are needed? How much storage? How much RAM? How many vCPUs? CenturyLink's Relational DB Service provides a safe and simple way to help transition a database through the application development process, and Bare Metal provides the perfect hosting environment. During the development phase, Relational DB Service can provision databases in low-cost configurations geared toward development activities. As the application evolves, the Relational DB Service allows you to seamlessly scale these databases for production, reconfiguring them with higher levels of availability and performance. Or you can just turn off any instances that you don’t need once you’re out of dev/test. Relational DB Service takes the pain out of managing the infrastructure so you can put your energies into more important things, and improve your ability to transition seamlessly from development to production. From Application Services to Bare Metal: A Complete Platform for a Complex World Tutorial: Installing Docker on Bare Metal Servers Spin Up a Cloud Relational Database in Minutes", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "building-your-first-app-on-coreos", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/building-your-first-app-on-coreos", "abstract": "I am sure you have heard of CoreOS , but have you ever actually deployed a real app on it? Not many people have. Building an app on CoreOS can be incredibly hard and frustrating. The documentation is scattered and you have to learn all about different technologies before you even get started (etcd, systemd, and docker). If you are lazy (like me) and want to just try out CoreOS with no fuss, I will guide you through the whole process from start to finish. We will create a simple WordPress application together running with a MySQL service in CoreOS. If you are on a Mac, you can install fleetctl and etcdctl natively to control your CoreOS clusters. Here is how: Getting a local clustered CoreOS up and running with Vagrant is pretty easy. Now you have a mini 3-system local CoreOS cluster running on your laptop. Easy enough, now let's check our local fleetctl . Awesome. It worked! Great, now you have a clustered CoreOS. What do you do with it? The fleetctl command lets you deploy your app into a cluster of CoreOS nodes, but writing service files for fleet sucks. Luckily, you don't have to write them yourself. You can use a simple yaml format to generate service files. The fleetctl client tool uses etcd's key/value store to understand the servers it has access to and needs access to the etcd servers running on each machine in the cluster. Here is how to deploy your app into the cluster. Now your app is running, but the services are not registered into etcd yet. Luckily, fig2coreos generated the discovery service files as well for us. That's it! You're done! In fact, if you are using Vagrant 1.5 with a Vagrant Cloud account, you can actually get to your WordPress app and see it in action. There is much more you can do with CoreOS, but at least you have now done the basics. If you are planning on leveraging multi-server CoreOS clusters in production, you will need to add ambassador containers to your system. In fact, you can even combine the etcd service discovery with ambassador containers, but we will leave that for another blog post next week.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-swarm-integrated-into-docker-engine", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-swarm-integrated-into-docker-engine", "abstract": "The release of Docker Engine v1.12 included the integration of Docker Swarm, which was previously a separate application, thus allowing the creation and management of a cluster of Docker Engines referred to as a swarm. This built in orchestration increases both reliability and scalability. Docker Captain Adrian Mouat believes the new swarm mode makes Docker “a serious competitor in the orchestration space” . A Swarm is a collection (cluster) of nodes running Docker Engine following a decentralized design where services can be deployed. Swarm mode includes scaling, service discovery, desired state, and other features such as: Cluster Management Integrated with Docker Engine: Use the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm. Decentralized Design: Instead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image. Declarative Service Model: Docker Engine uses a declarative approach that allows you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web frontend service with message queuing services and a database backend. Scaling: For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state. Desired State Reconciliation: The swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state and your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available. Multi-Host Networking: You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application. Discovery: Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm. Load Balancing: You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes. Secure: Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA. Rolling Updates: At roll-out time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service. With swarm mode you create a swarm with the 'init' command, and add workers to the cluster with the 'join' command. The commands to create and join a swarm literally take a second or two to complete. Mouat said “Comparing getting a Kubernetes or Mesos cluster running, Docker Swarm is a snap” . If your ready to create your own Docker swarm, visit the tutorial: How to Create a Docker Swarm on CenturyLink Cloud . Other Docker tutorials to review: Swarm mode key concepts . How to deploy an application image when Docker Engine is in swarm mode . Tutorial: Installing Docker on Bare Metal Servers . Gracefully Stopping Docker Containers .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-is-kubernetes-and-how-to-use-it", "author": ["\n              Laura Frank\n            "], "link": "https://www.ctl.io/developers/blog/post/what-is-kubernetes-and-how-to-use-it", "abstract": "What do you do when you want Docker containers managed across vast fleets of servers and infrastructure? You use Docker orchestration tools like Kubernetes. Developed by Google, Kubernetes is essentially a cluster manager for Docker. With it, you can schedule and deploy any number of container replicas onto a node cluster and Kubernetes will take care of making decisions like which containers go on which servers for you. The name Kubernetes originates from Greek, meaning \"helmsman\" or \"pilot,\" and that's the role it will fill in your Docker workflow. Kubernetes is a solution for overseeing and managing multiple containers at scale, rather than just working with Docker on a manually-configured host. Docker and its ecosystem are great for managing images, and running containers in a specific host. But these interactions are local; Docker alone can't manage your containers across multiple nodes, or schedule and manage tasks to be completed across your cluster. Docker alone works best when you're able to manually manipulate and configure the host. But we all know that only takes us so far. In order to manage a Docker workload on a distributed cluster, you need to bring some other players into the mix. Several other services are available to make working with Docker more efficient and streamlined. Fleet, Geard, and Marathon are designed to schedule and orchestrate jobs on the host; Fleet also functions as a cluster manager, along with Apache Mesos. Theoretically speaking, you could mix and match these different components and come up with a roll-your-own solution. While Kubernetes was designed to make working with containers on Google Compute Engine easier, the bits are available for anyone to use, and you need not be running GCE. Kubernetes offers a few distinct advantages, first and foremost being that it packages all necessary tools -- orchestration, service discovery, load balancing -- together in one nice package for you. Kubernetes also boasts heavy involvement from the developer community. Written in Go, the Kubernetes project has close to 2500 commits from over 100 different contributors. Despite heavy development, Kubernetes is still in beta, so you may discover some bugs. While Kubernetes does work as a cohesive package, there are several components at play, each with a specific role. Kubernetes also has a specific collection of terms, some of which are overloaded in the container/cloud space. To manage resources within Kubernetes, you will interact with the Kubernetes API. Pulling down the Kubernetes binaries will give you all the services necessary to get your Kubernetes configuration up and running. Like most other cluster management solutions, Kubernetes works by creating a master, which exposes the Kubernetes API, allowing you to request certain tasks to be completed. The master then spawns containers to handle the workload you've asked for.  Aside from running Docker, each node runs the Kubelet service -- which is an agent that works with the container manifest -- and a proxy service. The Kubernetes control plane is comprised of many components, but they all run on the single Kubernetes master node. Let's assume you've pulled down and started the Kubernetes services, and you're ready to build your first pod. Interacting with Kubernetes is quite simple, even outside of the context of Google Compute Engine. Kubernetes provides a RESTful API in order to manipulate the three main resources: pods, services, and replicationControllers. A pod file is a json representation of the task you want to run. A simple pod configuration for a Redis master container might look like this: [gist id=ad8fe83c3cbf0fdc3c19] In this example -- from the Kubernetes example project -- you can see information pertaining to the service's source image, but also some Kuberentes-specific information, like the labels used for service discovery. Let's take this pod and actually run it on a single-node cluster using Kubernetes. You can use the kubecfg tool to send a request to your running Kubernetes API server: Then, the kubelet starts the service. Verify the pod is running by listing the pods: After you're finished, you can delete your pod: This is a very rudimentary example of working with Kubernetes, but it does illustrate the way in which you interact with the Kubernetes API in order to manipulate resources. For this example, only pods were handled, though the API also handles replicationControllers and services . Along with the service , the replicationController uses the pod's labels in order to do its job. In short, the role of the replicationController is to ensure that a certain number of replicas of each pod are running. If there are too many, it will kill them, and if there are too few, it spins them up. The pods that the replicationController monitors is determined via labels. A simple example of a redisSlaveController looks something like this, again coming from the Kubernetes example application: [gist id=2e91ab943a172a7906f9] In short, this file tells Kubernetes that you want two replicas of the Redis template you've defined, and from there, Kubernetes works to make sure you have two running at all times. Kubernetes perhaps chose a poor word for this resource, since service is already so overloaded. But to Kubernetes, a service is a config unit for the proxies running on the minion nodes. It too can have a name and labels, and it points to one or more pods. It provides an endpoint for load balancing across a replicated group of pods. Below, you can see a simple schematic of what an example application could look like with Kubernetes. Now that you have a basic understanding of Kubernetes, go forth and built something! Here are a few resources to help you get started.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "what-is-kubernetes-and-how-to-use-it", "author": ["\n              Laura Frank\n            "], "link": "https://www.ctl.io/developers/blog/post/what-is-kubernetes-and-how-to-use-it/", "abstract": "What do you do when you want Docker containers managed across vast fleets of servers and infrastructure? You use Docker orchestration tools like Kubernetes. Developed by Google, Kubernetes is essentially a cluster manager for Docker. With it, you can schedule and deploy any number of container replicas onto a node cluster and Kubernetes will take care of making decisions like which containers go on which servers for you. The name Kubernetes originates from Greek, meaning \"helmsman\" or \"pilot,\" and that's the role it will fill in your Docker workflow. Kubernetes is a solution for overseeing and managing multiple containers at scale, rather than just working with Docker on a manually-configured host. Docker and its ecosystem are great for managing images, and running containers in a specific host. But these interactions are local; Docker alone can't manage your containers across multiple nodes, or schedule and manage tasks to be completed across your cluster. Docker alone works best when you're able to manually manipulate and configure the host. But we all know that only takes us so far. In order to manage a Docker workload on a distributed cluster, you need to bring some other players into the mix. Several other services are available to make working with Docker more efficient and streamlined. Fleet, Geard, and Marathon are designed to schedule and orchestrate jobs on the host; Fleet also functions as a cluster manager, along with Apache Mesos. Theoretically speaking, you could mix and match these different components and come up with a roll-your-own solution. While Kubernetes was designed to make working with containers on Google Compute Engine easier, the bits are available for anyone to use, and you need not be running GCE. Kubernetes offers a few distinct advantages, first and foremost being that it packages all necessary tools -- orchestration, service discovery, load balancing -- together in one nice package for you. Kubernetes also boasts heavy involvement from the developer community. Written in Go, the Kubernetes project has close to 2500 commits from over 100 different contributors. Despite heavy development, Kubernetes is still in beta, so you may discover some bugs. While Kubernetes does work as a cohesive package, there are several components at play, each with a specific role. Kubernetes also has a specific collection of terms, some of which are overloaded in the container/cloud space. To manage resources within Kubernetes, you will interact with the Kubernetes API. Pulling down the Kubernetes binaries will give you all the services necessary to get your Kubernetes configuration up and running. Like most other cluster management solutions, Kubernetes works by creating a master, which exposes the Kubernetes API, allowing you to request certain tasks to be completed. The master then spawns containers to handle the workload you've asked for.  Aside from running Docker, each node runs the Kubelet service -- which is an agent that works with the container manifest -- and a proxy service. The Kubernetes control plane is comprised of many components, but they all run on the single Kubernetes master node. Let's assume you've pulled down and started the Kubernetes services, and you're ready to build your first pod. Interacting with Kubernetes is quite simple, even outside of the context of Google Compute Engine. Kubernetes provides a RESTful API in order to manipulate the three main resources: pods, services, and replicationControllers. A pod file is a json representation of the task you want to run. A simple pod configuration for a Redis master container might look like this: [gist id=ad8fe83c3cbf0fdc3c19] In this example -- from the Kubernetes example project -- you can see information pertaining to the service's source image, but also some Kuberentes-specific information, like the labels used for service discovery. Let's take this pod and actually run it on a single-node cluster using Kubernetes. You can use the kubecfg tool to send a request to your running Kubernetes API server: Then, the kubelet starts the service. Verify the pod is running by listing the pods: After you're finished, you can delete your pod: This is a very rudimentary example of working with Kubernetes, but it does illustrate the way in which you interact with the Kubernetes API in order to manipulate resources. For this example, only pods were handled, though the API also handles replicationControllers and services . Along with the service , the replicationController uses the pod's labels in order to do its job. In short, the role of the replicationController is to ensure that a certain number of replicas of each pod are running. If there are too many, it will kill them, and if there are too few, it spins them up. The pods that the replicationController monitors is determined via labels. A simple example of a redisSlaveController looks something like this, again coming from the Kubernetes example application: [gist id=2e91ab943a172a7906f9] In short, this file tells Kubernetes that you want two replicas of the Redis template you've defined, and from there, Kubernetes works to make sure you have two running at all times. Kubernetes perhaps chose a poor word for this resource, since service is already so overloaded. But to Kubernetes, a service is a config unit for the proxies running on the minion nodes. It too can have a name and labels, and it points to one or more pods. It provides an endpoint for load balancing across a replicated group of pods. Below, you can see a simple schematic of what an example application could look like with Kubernetes. Now that you have a basic understanding of Kubernetes, go forth and built something! Here are a few resources to help you get started.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "5-tech-meetups-to-join", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/5-tech-meetups-to-join", "abstract": "Today's technology places a vast amount of data at your fingertips across various mediums. A quick Internet search of a topic will yield an immense amount of resources within seconds. While online searches can be highly-beneficial to finding resources for learning, there's really no substitute for face-to-face collaboration with people who can challenge and enhance your learning experience. However, finding a group discussing specific topics that interest you, in your area can be tedious. Meetup.com makes that task simple. It removes many of the modern-day barriers to meeting people who share the same interests by organizing a network of local groups. These groups are easy to locate and join -- the central website hosts thousands of groups whose members are already meeting face-to-face. In fact, more than 9,000 groups get together each day to work towards personal or community improvement. Meetup's mission is \"to revitalize local communities and help people around the world self-organize. Meetup believes that people can change their personal world, or the whole world, by organizing themselves into groups that are powerful enough to make a difference.\" Where does this leave someone in the tech world looking to expand their knowledge or even share what they know with others? We live in a world of connectedness and information sharing and it's easy to feel overwhelmed or isolated at times. This is why something like Meetup is so helpful. So whether you're looking to collaborate on specific topics of interest, learn a new skill, or venture to change the world -- joining a Meetup is an easy step towards achieving your goals. While new Meetups are sprouting up daily, new members join constantly, and interests vary widely from person to person, it is hard to quantify the \"top\" Meetups to join. Below are five Meetups that cover trending topics of the tech world and have represented groups in most major cities. Docker containers wrap a piece of software with everything needed in a filesystem: code, runtime, system tools, system libraries, etc. The main idea behind containers is that each one is a self-contained block that can be easily combined with other blocks to create a working system. Docker helps ensure consistency in operations by storing objects in a specific way, regulating access to the objects, and controlling how the objects are used. It allows users to take copies of their live environments and run them on any new endpoint running a Docker engine. Docker containers are quickly becoming a key part of many system standardizations, making it a great skill to possess for future development. Touting use cases from application deployment to security and integrations like cloud and containers, Ansible is an IT automation engine built to make other tasks simple. Ansible focuses on its simplicity by providing automation, specifically the automation of cloud provisioning, configuration management, intra-service orchestration, and other IT essentials. One of the best things about Ansible is its ability to take an entire, multi-tier IT infrastructure into account by understanding how all of your systems relate to one another. This provides a holistic view of your system and helps determine the best way to automate each piece. Ansible also retains its simplicity by using the YAML language (in the form of Ansible Playbooks) that allows users to describe automation jobs in plain terms that most people can understand, making it more accessible to everyone. It also keeps its system customization requirements to a minimum, making it easy to deploy and use the self-service applications. One of the hottest trends in tech are automation and self-service applications, Ansible allows for both, making it an appealing skill set to master. \"Building on the best of relational with the innovations of NoSQL\" is MongoDB's slogan. With their recent release of MongoDB 3.2 , the company is focusing on standardization, most importantly on creating one current database to run all of their most important applications. The 3.2 release has enabled encryption and access controls for data-at-rest, which increases their ability to comply with industry safety and compliance standards. They've also released a Hosted MongoDB-as-a-Service, MongoDB Atlas, that allows customers to easily scale and deploy their applications in the cloud while providing high-availability, security, and disaster recovery. With Hadoop and Big Data trending within the industry, having the skills to maintain NoSQL databases, or paying someone to manage them for you, is becoming a necessity in the market. Apache Spark ™ provides large-scale data processing with their industry-leading, advanced, DAG execution engine. This engine allows for lightning-fast processing by using in-memory computing and cyclic data flow. These qualities make it easy not only to process data, but process it simultaneously by building parallel apps within their high-level operators. Spark includes several libraries, including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming, which you can combine in the same application -- making it a great choice for systems that require customization. Spark is quickly becoming an industry standard and the engine-of-choice for big data processing, making it a key skill set to learn. The Internet of Things (IoT) is a broad area of growth that just keeps getting bigger. There are already billions of smart devices sharing and compiling data, and experts expect the smart device trend to continue into the trillions and beyond. Cloud-based applications are key to leveraging data collected by devices and translating it into useful information that can be shared with day-to-day users. IoT has the potential to impact the way everyone in the world lives by fundamentally changing the fields of transportation, healthcare, security, education, etc. From developing IoT software, to enabling sensors and devices to talk to each other, to architecting intricate smart cities, the IoT developer path is abundant. The IoT has been stated by numerous technology forecasters to be one of the biggest technology trends in the years to come, thus producing the most opportunities within the tech sector and beyond. Immersing yourself in the Meetup community can leverage your self-growth, both personally and professionally. If you are looking to network, take on a new hobby, learn new skills, or make a difference in your community, join a Meetup! Don't see a Meetup group on your topic of interest in your local community? Start one! That is the underlying power of the Meetup community -- it is easy to start and/or join. Want to learn more about the above topics?\nCheck out these resources: Docker Tutorial: Understanding the Security Risks of Running Docker Containers: Don't Lose Your Sock(et)s Tutorial: Gracefully Stopping Docker Containers Ansible Tutorial: Up And Running with Ansible & CenturyLink's Runner Ansible Q&A: It Can't Make Your Breakfast Yet MongoDB Tutorial: Replicating MongoDB into Orchestrate Getting Started with MongoDB: Blueprint Spark Blog: 5 Ways Apache Spark Powers Your Big Data Blog: Apache Spark: Mining More Value from Data IoT Solutions: Internet of Things Blog Series: IoT", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "secure-code-matters-2", "author": ["\n              Eric Raisters\n            "], "link": "https://www.ctl.io/developers/blog/post/secure-code-matters-2", "abstract": "\" Security is a process, not a product \"\n  -Bruce Schneier, Cybersecurity expert As mentioned in a previous blog , according to statistical studies led by Dr. Gary McGraw at Cigital, half of all security vulnerabilities in systems are due to coding errors – bugs, but the other half are design flaws which can only be proactively discovered and prevented through the threat modeling (or architecture risk analysis) process. The alternative is embodied by several high-profile vulnerabilities announced recently – POODLE and Shellshock . Both of these vulnerabilities were due to features added to base functionality to either simplify use for backwards compatibility (POODLE) or to extend capability for administrators (Shellshock) – without considering the security consequences of these features, especially as the feature aged and attack vectors changed. A secure software development lifecycle (SSDLC) process includes threat modeling or architecture risk analysis as part of the design review stage of the development lifecycle – preferably before any code is written. There are two primary threat modeling approaches – adversarial and defensive.  The adversarial approach involves after-the-fact analysis, such as security code review or penetration testing.  While this approach may provide impetus to jump-start a secure software development lifecycle program, by exposing vulnerabilities which exist in the application, it is a purely reactive (and hence expensive) approach.  It often results in the rewriting of code and in “bolted on security” rather than security by design. “ You cannot build secure systems until you understand your threats .”\n  -Michael Howard, Microsoft SDLC co-developer More mature approaches call for defining the attack surface of the application or system, then looking for possible threats against that surface. Alternately, one can list and classify the assets that the application or system will have access to, and thus needs to protect, then look for threats against those assets. For each threat identified, one then needs to come up with a mitigation to the threat from the commonly used toolkit of security functions – authentication, authorization, encryption, integrity validation, auditing, and reliability.  Here is where the all-important threat question “ What can an attacker do with this ?” is answered by “ Doing this is how we prevent the attacker from doing that .” A good threat modeling approach must not just look at threats and their mitigation, but also must quantify the risk that the threat could be realized.  This risk-based approach looks at both the value of the asset and the probability of a successful attack to prioritize mitigating the threats which have the highest risk first. Experience has shown that often a mitigation applied for one threat will also mitigate a threat in a different area, or for a different asset, if the mitigation is applied centrally through the application or system. For instance, requiring all network communications to utilize a secure protocol such as TLS or SSH, will mitigate any occurrence of information leakage when data is transmitted, while requiring all data residing on disk to be encrypted will mitigate any occurrence of information leakage while data is stored.  Similarly, requiring the use of OWASP , OWASP Java or Microsoft Anti-Samy libraries for sanitizing all web traffic will mitigate all occurrences of XSS (Cross-Site Scripting) vulnerabilities. Once mitigations have been implemented, it is important to verify that they are working properly; so either the above-mentioned penetration testing or other automated functionality tests (including misuse cases) should be run. Any negative findings either need to be fixed and retested, or accepted as residual risk by the product owner or product manager, and then added into the workload for the next release. Threat models should be revisited any time there are major changes introduced, these could include changes in the feature sets, assumptions made in the threat model, or new threats or risks are discovered. Ensuring that the implementation of a feature or functionality is secure by design will go a long way in securing our systems and hence our customers.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "secure-coding-fundamentals", "author": ["\n              Eric Raisters\n            "], "link": "https://www.ctl.io/developers/blog/post/secure-coding-fundamentals", "abstract": "\"Security is a process, not a product.\" -Bruce Schneier, Cybersecurity expert According to statistical studies led by Dr. Gary McGraw at Cigital, half of all vulnerabilities are due to coding errors - bugs. The other half are design flaws which can only be discovered and fixed through threat modeling, or an architectural risk analysis process – which is a topic for another blog. Just as with any bug, the cost to fix a vulnerability goes up exponentially the further down in the development cycle it is found and addressed. Unlike a simple bug, the cost of a vulnerability being successfully exploited has the potential to be much greater, especially once it gets into a production environment.  And the cost is not just monetary, but could also be reputational and legal. A secure software development lifecycle (SSDLC) process is a development methodology, an independent set of tasks in all phases of the software development lifecycle that includes several secure coding components. \"Security needs to be done by people who don’t have “security” in their job title. Doing what you do the right way is 90% of getting things secure\". -Paco Hope, Secure coding author Not all developers need to be security experts, but all developers need to be aware of their responsibility in writing code that reduces the chance of exploitable vulnerabilities showing up in applications. Training in secure design principles, common attacks against specific coding languages and use of tools, like static code analysis, to help find and remediate vulnerabilities is needed within the field, as these topics are usually not covered in any computer science or coding classes in school. Half of the OWASP Top 10 web application vulnerabilities and more than half of the MITRE/SANS Top 25 programming errors are due to insufficient input validation of user controlled data. So particular attention needs to be paid to tools and libraries available to validate and sanitize this data before it is used. Developers (and testers) need to develop a mindset of “What can an attacker do with this?” anytime user controlled input is requested or used. Fuzzing (also called random mutation testing) of input files or protocols is good way to find many input validation issues, and it is eminently automatable. Static code analysis tools are now built into most integrated development environments (IDEs) for the major coding languages and should be enabled, configured, and used religiously by all developers – before they check any code into their code management systems (CMS). These are effective at preventing known vulnerability attack vectors from being included into submitted code, as well as resulting in higher quality code. Code quality is directly correlated to application security. So good, clean code will be inherently more secure. Code review from a security point-of-view is also an important aspect of secure coding, as the Heartbleed vulnerability so clearly demonstrated. No static analysis program was able to find this seemingly simple input validation error prior to the publication of the vulnerability. However, having an additional validation step of code review for security issues by multiple reviewers, versus a lone reviewer, could have made the difference. Even with a fully implemented SSDLC there will be times when a previously unknown (or unfixed) bug becomes tomorrow’s vulnerability du jour . Developers will still need to quickly react to quantify impact, determine a remediation or develop a patch, and perform a root cause analysis so similar weaknesses can be found and corrected quickly. Attackers will often look for additional related  vulnerabilities, in case the responsible developers only fix the specific vulnerability exploited. The goal is that with a well implemented SSDLC we can reduce the frequency of these resource-diverting events and reduce risk to both ourselves and our customers. Learn more about how CenturyLink Cloud can provide the enterprise level security , speed, performance, and availability to meets your business demands.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "making-security-agile", "author": ["\n              Aaron Young\n            "], "link": "https://www.ctl.io/developers/blog/post/making-security-agile", "abstract": "Security is important. But not every team has the processes in place to make it happen. Building secure software is a holistic process, requiring a solid top down understanding of the product architecture as well as a deep knowledge of the building blocks. Likewise, responsibility is shared among the team members. The product owner has a responsibility to allocate resources and prioritize security, while the individual developers need to be proactive in being aware of security risks and pitfalls. Moreover, communication from both sides is required for this dynamic to function. If developers are not bringing security concerns to the attention of management, management cannot allocate resources needed to fix the problems. If managers are not prioritizing security, it's likely that developers are going to be distracted by the task at hand, and then security holes can end up deeply embedded and forgotten. Like most things, the first step is education. Just as developers need to keep up to date with the latest technology, being informed of the latest security best practices needs to be a continual effort. As the threat environment changes, and as code evolves, the developer needs to be able to perceive threats before there is the probability of addressing them. Ensuring that a development team is trained to deal with possible threats is key. Ways to achieving this are: Knowledge alone is not enough. Processes are critical in ensuring that the team as a whole meets its security standards. In an Agile software project, an easy way to ensure that security is consistent is by building security checks into the teams sprints: Sprint Planning : Sprint Tasks : Sprint Retrospective : Releases and Milestones : In software development, Agile project management has grown in importance and evolved to the point where it is widely respected as a viable alternative approach to projects. Agile has been codified in Scrum ), XP (Extreme Programming) , and other specific \"styles\". Below are some additional resources that cover Agile, DevOps, secure coding, and Security tips for Docker.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "day-in-the-life-traci-yarbrough", "author": ["\n              Jonathan Townsend\n            "], "link": "https://www.ctl.io/developers/blog/post/day-in-the-life-traci-yarbrough", "abstract": "Our \"Day in the Life\" series gives you a glimpse into the everyday lives of team members and our culture here at CenturyLink. The series focuses on all different avenues and roles in our company, from project managers to developers, product owners, technical writers, and administrative staff. In this post, we highlight Traci Yarbrough, who manages the Lifecycle Management Team located in St. Louis. The Lifecycle Management Team works to transition existing and new customers from their current systems to strategic platforms/products. The following interview provides insight into Traci's role at CenturyLink, what makes her team such a valuable asset within the organization, and of course, some little-known facts about Traci's hobbies and interests. What skills do you bring to your team? TY: I am happy to be part of a team that has mad skills as a whole; what I bring to the group is a background in software sales, product marketing, and product management. That 16+ years has allowed me to develop specific skills in market sizing, customer communication, data analysis, and sales enablement. What are your top three unique skills? TY: Completing a 3x3 or 2x2 rubrics cube in five minutes or less, sharp turns on a scooter, and moonwalking. What’s the biggest strength you bring to the job? TY: A passion for collaboration and teamwork. Both drive innovation and improve productivity within the business. What has your favorite project been? Why? TY: The hardest project has also been my favorite project. Migrating customers from Virtual Private Data Center (VPDC) is my favorite because it provided the greatest opportunity to learn. We started the work with no precedent and structure. The business went through several changes and we had to become well-versed in the roles and responsibilities of each department that influenced the outcome of the program. What have you done within your team that you’re most proud of? TY: Creating a framework for product migrations and decommissioning. We have historically been very good at launching products but poor at customer transitions. The framework we have now is enhanced every day, but we are beginning to look across the business and we believe that soon we will be able to proactively transition customers instead of being reactive with their transitions. What ideas have you brought to the company that you have implemented? TY: Within our team and department, impact analysis is something that we have focused on. Specifically, we assess and quantify the potential impact to the customer experience, Operations team, and revenue if we make a decision to decommission or upgrade a product. How does your personality help you perform at CenturyLink? TY: I am a huge smart ass; my sense of humor, persistence, and honesty come along with that. I enjoy laughing and believe people appreciate sincerity and honesty. Our company is filled with individuals that are very busy and I make a point to follow through with commitments and hopefully make them laugh during the process. What is the biggest challenge you’ve had to overcome and how did you do so? TY: The biggest challenge(s) I have faced so far are the organization changes that have occurred in the last 2 years. Every time the Sales, Professional Services, Product Management or Engineering teams are restructured we must re-educate the business and make a case for them to understand the value of what we are doing. This is time-intensive and often grinds projects to a halt. To overcome this, we are working very closely with executive management. In my case, James Feger and Aamir Hussain are strong advocates for what we are doing and work proactively with their peers to ensure that we are able to move forward. Describe what makes your specific skill-set an asset to your team and CenturyLink TY: My focus on strategy and outcomes is a benefit to my team. They know what is expected of them, how it fits in the organization, and how they will be measured. The benefit to CenturyLink Cloud as a whole is that I understand how to build a team that can meet the strategic outcomes defined by the business. My team has experience with CenturyLink Cloud, storage, networking, order processing, and operations. They know what needs to get done, how to do it, and they are motivated to execute. What motivates you each day? TY: Knowing that we have a positive impact on the bottom line. The programs we develop and manage improve the experience of the customer and increase productivity within the organization. To accomplish this we get to work with several departments and individuals in the business. Every week I learn something new and have an interaction that makes me proud to be at CenturyLink. What’s a little-known-fact about yourself that you’d be OK sharing? TY: I am a huge Harry Potter fan. I have read the books 4-5 times, listened to the audiobooks 2-3 times, and watch the movies while doing housework on a loop. However, I have never been to the The Wizarding World of Harry Potter at Universal Studios -- it's one of the biggest items on my bucket list. How does your role translate to your influence and collaboration with others? TY: When people ask me how we fit into the business, the answer is: “We collaborate with the Operations, Product Management, Professional Services, Onboarding, Sales, and Support teams to design and manage transition programs for our customers.” My team is very focused on setting targets and measuring our progress and overall impact. When you can show data and make a case for why your work is impactful, people want to be a part of it. I provide updates with supporting data to a wide audience each week. I'd say 99% of the time, a recipient of the information responds with a question, suggestion, or an offer to assist. This demonstrates the value of what we are providing and the willingness of others to support the goals. What is your degree in? How do (or don't) you use it every day? TY: I have a double major in Psychology and Sociology and a Masters Degree in Social Work with emphasis on Mental Health and Finance. Both majors help me every day. I always try to have a “make sure you understand the position of the other person first” approach when interacting with my team and co-workers. Making assumptions about what someone is thinking or not respecting the unique perspective they have often leads to misunderstandings and people talking over one another. Understanding the other person allows us to collaborate more openly, work on processes, and agree on outcomes that are mutually beneficial. Sign up for our Developer-focused newsletter CODE . It's designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-data-exhaust-affects-user-behavior", "author": ["\n              Jonathan Townsend\n            "], "link": "https://www.ctl.io/developers/blog/post/how-data-exhaust-affects-user-behavior", "abstract": "On any given network in the Internet of Things (IoT) , thousands of devices produce data that is logged, cultivated, analyzed, and used to refine system functionality. There's no arguing that the way people interact with their devices greatly affects their behavior. For example, individuals utilizing wearable fitness technology such as a GPS watch, a step counter, or a heart rate monitor generally alter their activity, and thus their behavior, to fully-utilize the device's functionality. Another example is the use of GPS monitors for traffic flow patterns in an urban environment -- the information gathered and reported to the user can affect the route they decide to take on their daily commute. It should come as no surprise that technology integrated into everyday life tells a robust story about user behavior that's worth exploring. As noted above, the wearable fitness technology industry is booming with innovations that match the real-life applications and have become expectations in the consumer market. Whether it's planning a running route, helping to create smart city networks as detailed by Strava , or even identifying health risks -- data analysis on this level directly affects user behavior . Here at CenturyLink Cloud, we've been exploring how data analysis in the form of data exhaust affects the freight industry (pardon the pun). With many devices generating data streams and schemas that are collected and logged in the cloud, a massive amount of computational output, known as \"data exhaust\", is produced. In essence, data exhaust is data created as an output or byproduct that is based on users' actions and activities with devices, computers, and pieces of software. One example of data exhaust can be classified as the user-generated files on web-based systems and networks such as cookies, logs, temporary browsing history, and indicators to help websites, companies, and developers improve the way they serve their target audiences over a broad user base. According to TechTarget , \"Studying data exhaust can also help improve user interface and layout design. As these files reveal the specific choices an individual has made, they are very revealing and are a highly-sought source of information for marketing purposes. Websites store data about people’s actions to maintain user preferences, among other purposes. Data exhaust is also used for the lucrative but privacy-compromising purposes of user tracking for research and marketing.\" Understanding data exhaust is incredibly important in this era of emerging smart cities and networks. Through the careful and streamlined collection of an individual's or a user group's digital footprint, digital communities, communication methods, and online systems can be refined to help people. Naturally, the capabilities of a cloud-based platform with cloud servers and virtual machines , APIs , simple backup services , and containers make capturing a company's or individual's digital footprint more akin to creating an accurate digital portfolio or profile composed of the relevant data required to meet their personal and business needs, which can be accurately traced back to the source. According to Computerworld's article on data exhaust, big data, and data analytics across business markets, data exhaust is, \"essentially all the big data that isn't core to your business.\" At a basic consumer behavior level, data exhaust manifests as data channels generated by things like wearable fitness devices, computers, and smartphones. As stated by the article, \"If big data is \"primary\" data that relates to the core function of your business, data exhaust is secondary data, or everything else that's created along the way.\" The article lists numerous ways data exhaust affects markets and user behavior and is well worth the read. Another insightful article on Storagecraft's Recovery Zone site frames applicable ways data exhaust affects user behavior in relation to big data and the world of IT integration to accomplish tweaks and augmentations across platforms and systems with ultimate aim to educate and address business needs in realtime, assess KPI success and failure rates, and improve testing processes. For those of you working with the IoT, CenturyLink Cloud® Platform is the perfect place to build your applications. Our global network provides superior performance for your IoT sensors and cloud applications. Our managed services handle operating system and application operations – allowing you to focus on more important objectives. Not a CenturyLink Cloud customer? No problem. Designed for your business needs today and tomorrow, the CenturyLink Cloud is reliable, secure , robust , and global . If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "devops-leadership", "author": ["\n              Trey Willis\n            "], "link": "https://www.ctl.io/developers/blog/post/devops-leadership", "abstract": "As a developer, it's common for you to balance sometimes opposing priorities. Stakeholders want you to create high quality software quickly. We're trying to do the same thing. We aim to build product capabilities that are automated and programmable, self-service , and very scalable. We want to be developer friendly , while staying enterprise relevant. Those are all big goals that are tough to balance against one another. To accommodate all of those requirements simultaneously, we needed to make a change in how we utilized our resources and in how we prioritized our work in order to become more effective. To get where we needed to be we had to undo some of the old-school, more traditional boundaries that we had created over the last several years. We needed less functional management – what we call overhead – and more focus on getting the work done. In order to catch up to the market demand, we had to move fast and produce high quality products. Several months ago, we made the move to organize in a DevOps fashion in order to emphasize some of the key values on which our teams are built. We acknowledge that organizational culture matters. We operate with an expectation of high trust within cross-functional collaboration and through open communication to create a shared consciousness regarding our vision. This approach creates a strong feeling of empowerment and enablement throughout the team, as we constantly strive to improve what we do by learning from failures and embracing new ideas. Our typical team structure consists of the following roles: Product Owner, Product Analyst, Engineering Manager, Engineers/Developers. Each role has its own well-defined responsibilities and expectations, and all are expected to be accountable for the success of the team. The Product Owner determines the “what” and the prioritization as to when those things should be taken on. The Engineering Manager leads the team in discovery and in decisions on how to technically build and implement what the Product Owner has determined will create value to the customer. There is significant value to having a strong, well-functioning relationship between the Product Owner and the Engineering Manager. A strong and supportive relationship between the Product Owner and the Engineering Manager keeps the team tightly aligned and working in the right direction effectively. A sturdy relationship between the Product Owner and the Engineering Manager increases team velocity and can also reduce the time-to-market for the product. The following philosophies are key tenets to how we have created resilient Product Owner and Engineering manager connections on my teams, which has enhanced team performance and resulted in a better customer experience. Every team needs leadership. At CenturyLink, we expect that our team leaders provide guidance, direction and instruction for the rest of the team. While we anticipate leadership qualities will be demonstrated by all members of the team, the two primary leadership roles within each team are those of the Product Owner and of the Engineering Manager. This DevOps approach, in which the Product Owner and Engineering Manager model the way by their actions, enables each team member to contribute their assets at a very high level. As we venture on this DevOps journey we will be sharing our milestones as a company, but more importantly, as product teams. The stories will be told from different perspectives within the team from Product Owners to Developers, each outlining the tools and processes that we adapt along the way. If this approach to leadership and team-building is appealing, perhaps you want to work with us ?", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "bare-metal-for-devs", "author": ["\n              Bryan Friedman\n            "], "link": "https://www.ctl.io/developers/blog/post/bare-metal-for-devs", "abstract": "The cloud revolution has ushered in some amazing advancements in the IT world, but with advancements often come obstacles for companies and those who support the technology. With the emergent new possibilities of what Big Data , IOT (Internet of Things) and Docker can bring to the marketplace, industry leaders are now, more than ever, in need of a better way to scale their applications without losing the features they love about the current multi-tenant cloud environment. Developers and Enterprise execs alike have voiced the demand for powerful, dedicated servers that allow provisioning and management from a single platform. They want features like self-service, on-demand provisioning, and vast automation. The solution Single-tenant, high performance physical machines with cloud-like attributes integrated into a single, easy to use platform. Simply stated, it is the best of two worlds combined. Bare Metal has the power and reliability of the long-lived, dedicated server environment coupled with the flexibility, scalability and efficiency of the cloud model. Bare Metal cloud servers do not run a hypervisor, meaning that they are not virtualized, but they can be consumed in a cloud-like service model. Providers are offering Bare Metal in an array of configuration choices, each touting high-performance, fully-networked, Bare Metal-dedicated servers that enable users to build powerful cloud solutions to meet their  current and future needs. Clients can choose their server specs -- capacity, memory, hard disk space and software -- and off they go! Create, push and manage your individual Bare Metal cloud servers and fully-networked solutions in a single, integrated user experience through the same control portal as your cloud servers. This allows for fast access to your platform and speed of progression for your application or project without the delays. Resource controls are at your fingertips, via the platforms' control portal, to fine-tune and adapt to the changing requirements and sync with the current needs of the project. The core benefit of the Bare Metal cloud environment for developers is easier coding; a developer does not have to account for a plethora of uncertainties. This is unlike multi-tenancy environments, where developers often need to account for an unknown variable of \"noisy neighbor\" hosts in the virtual server environment. Install Docker on Bare Metal Servers Create a Bare Metal Server using the same interface as VMs. Often many companies who are experiencing exponential growth are also faced with the conundrum of financial and velocity-centric loss caused by scaling their applications on the public cloud -- causing them to revert back to co-location hosting. The heartache starts when the need to scale within a multi-tenant cloud environment arises. Amidst inconsistencies in the performance and reliability of compute, storage and network, engineers are tasked with making it work. They have to combat slow or inconsistent node times, often causing over-provisioning and increased complexity of the application. From a business standpoint, this process causes substantial financial and efficiency losses, both in the time spent engineering amidst \"noisy neighbors\" and managing the scaling process. Implementing Bare Metal Severs will alleviate this bottleneck and give your engineers the raw power and consistency needed to successful scale and remain agile. Learn more about Bare Metal on CenturyLink Cloud. Sign up for our Developer-focused newsletter CODE . Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider — let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "open-source-kb", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/open-source-kb", "abstract": "Open Source has changed the software model. Over a decade ago companies dabbled in open source to save money. Today, companies need open source to keep a competitive edge in regards to agility and time to market. At CenturyLink, we organize around open source ideals for our software, and that continues through how we've approached documentation and tools. We encourage contributions from partners, customers, and employees alike by making public as much of our workflow as we are able. Market gains are still a part of why companies are taking part in the revolution to shift away from legacy hardware and software to the OSS technology suite. A growing and potentially larger advantage of open source is that fostering a culture and a community that not only utilizes but contributes to open source is a win all in itself. As Kim Polese says in Open Sources 2.0 , \"open source has grown far beyond the mainstream. It has become the bedrock over which the mainstream flows.\" At CenturyLink we agree that open source will continue to drastically alter both how we share knowledge and what that will spawn in the future. Our knowledge base enables developers, customers, and other users to better use our platform. It's where our products are described and integrations explained. And anyone can contribute . The result is 50+ contributors, from within CenturyLink as well as outside. We're able to receive more and better contributions from partners, which enabled our shared customers to have better experiences. If you notice an inaccuracy in a document, the fix is a pull request away,decreasing the chance that anyone else will have the same issue. That's the open source ethos. Just as partners benefit from an open knowledge base, they similarly can use our open sourced cost estimator or the total cost of ownership tool which we also released under the Apache 2 license. These tools started as internal projects to help market our solutions. Our partners find them valuable for extending to include their own scenarios. Of course, we welcome your feedback. Like the knowledge base, if you see something, say something, in the lingua franca of open source--by contributing to the repos. \"The embrace of open-source technologies within the enterprise continues to rise, and we are proud to be huge open-source advocates and contributors at CenturyLink. We believe it's critical to be active in the open-source community, building flexible and feature-rich tools that enable new possibilities for developers.\"\nJared Wray, SVP of Platform at CenturyLink The main reason for us to open source our knowledge base and certain tools, however, was for our CenturyLink Platform users. Turning over the control, through open source, to many of our tools and documentation allows our users the freedom to modify and reuse them as their own. Open Sourcing our KBs and tools also created an internal advantage. Integrating them with tools such as Slack, lead to improved collaboration internally between teams, and allows those who control the CenturyLink Cloud Public KB Github repository to get notified of when and who updates articles. We support open source, in many aspects, because we believe it is the best model for creating software. Reusable open source components allow for easier creation of modular and scalable systems. Our teams are composed of engineers, designers and technologists who are passionate about innovation and open-source software. Our teams have consumed and created amazing projects, inspired by the open source community. A few of them are Panamax , Lorry , Image Layers , Chef integration with VMware , Dray , Iron Foundry , ElasticLINQ . Open source is a big change for how the industry does business. Millions of people working on a project is an amazing thing! The growth of the industry has been huge, and as we become a multi-tier platform, we can open source our tools and technology and share it. Which is what the open source model is really all about. The great thing about open source is that the original contributor cannot possibly conceive how the tool will be modified and used by others. Many attribute stability, bug fixes and improved UX to businesses’ participation in these projects. With newly implimented directives here at CenturyLink Cloud to “reduce barriers to employee participation” in these initiatives, there will be a lot more to come.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "dev-plus-ops", "author": ["\n              Trey Willis\n            "], "link": "https://www.ctl.io/developers/blog/post/dev-plus-ops", "abstract": "There’s been a recent buzz about DevOps that has continued to grow louder over the past few years. We’ve seen content that certainly makes a good argument that it’s explicitly a cultural change. Likewise, we’ve seen articles insisting  that it’s not a culture, but a concrete process. The truth is that in order to take advantage of the benefits and values offered within DevOps, you have to consider both process and culture. The goal of DevOps is continuous delivery; and to do that well you’ll need these two concepts to come together successfully. Our teams consist of a careful balance of skills and experience generated from years of creating products, building applications, and maintaining infrastructures and networks. When a new team is assembled, we compile the players necessary to scope, implement, deliver, manage and even enhance a service. Those skills include a spectrum of knowledge that includes  traditional application developers, IT operations veterans, security engineers and network architects. Their experience levels bring together senior staff members with strong opinions, deep knowledge and perspective who are pairing side by side with the likes of conventional IT operations members. We focus on assembling talents from various backgrounds to create a team of very proficient generalists, rather than bringing in a cadre of specialists for various project needs. As a result of forming a team with such a diverse background, we have a constant opportunity to grow through daily cross training and exposure to new experiences. We turn those assorted backgrounds into four roles on our teams: Engineers, Engineering Managers, Product Analysts, and Product Owners. Everyone contributes. All engineers, including the engineering manager, check in code daily. Each feels a responsibility for the customer experience. And, we all have an obligation to ensure consistent uptime of the platform. As a result of our DevOps culture, we all hold each other accountable for the quality of the service that we create and support. One of the key practices that we use to create this shared accountability is through pair programming. Bringing together team members from a developer and an IT operations background, and then asking them to work closely together in pairs creates long lasting results for the team and for the product. Team members with a system administration background will now better understand the application’s architecture and technology stack, while developers will learn more about the environments in which the application will run. Sharing that foundational knowledge about the full infrastructure and application stack creates a very resilient team and an even better customer experience. The team interaction and involvement with one another doesn’t stop with pair programming alone, however. Those engineers in our teams, who were once called developers and IT operators, attend planning meetings with product owners and analysts together as a team. We all attend daily stand ups to discuss current work in process. We also collectively participate in release activities in order to create and to maintain a shared understanding of what each other is working on and how the service is performing. As a result, all members of the team understand the key performance and availability metrics to measure, and track them against larger business goals. All engineers are also on call. The same set of resources are responsible for support requests, minor and major enhancements, and urgent operational issues. This responsibility helps to solidify the “you build it; you run it” culture. It also creates a different perspective for the developer knowing that they, or their team, might have to deal with subpar quality code. This mindset helps all of us to hold each other accountable for the value of our work. Personally dealing with any of the challenges in the service also exposes each of us to the opportunities to make the service better. We automate as much as possible, and we look for ways to reduce any failure opportunities and manual intervention. Though we’ve come to find a nice balance of development and operational practices within our teams, it’s not all peaches and rainbows. Different backgrounds and experience generally lead to each member of the team having different value structures. So, where an operational background might value internal tools for performance visibility, a development background might value more creative development within the user interface. To balance those influences, the product owner is constantly working to find areas for automation and to demand consistency within the customer experience. It’s critical to ensure that all members of the team hold a shared consciousness of the vision and goals for the product. In the end, our job is to solve problems. Our DevOps approach is about bringing together people who have good experience, with varying perspectives to help  solve problems in the most effective way in a short period of time. We deliver on our promises to the business, and to our customers. We continually work on developing collaboration, clear communication and trust between our team members. The results of a well-balanced and motivated team are obvious. Our speed to deploy new features improves, given that we have good visibility of the challenges in front of us due to the mix of experience. We have less issues with quality concerns when we release a new feature based on the array of skill sets and knowledge coming from previous roles. Further, if we do encounter unexpected challenges, our mean time to repair has shortened due to the deep understand that each of our team members have related to the full solution stack. This creates an environment that encourages a consistent, reliable customer experience. We are continually learning and improving as a team. We spend less time fighting fires and more time focusing on great work. When you don’t have to spend time triaging as a result of bad code or poorly designed infrastructure, there is more time to focus on improving the service and creating an even better customer experience. In DevOps, the culture enables the practices, and vice versa. You can’t do one without the other.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "jenkins-slackraspberrypi", "author": ["\n              Thomas Broadwell\n            "], "link": "https://www.ctl.io/developers/blog/post/jenkins-slackraspberrypi", "abstract": "When I walked into the office this morning I couldn't help but notice the red glow coming from our team room. First thought: \"Who jacked up the code?\" I looked at our Slack channel and saw that integration tests had failed. Using the link in Slack, I was taken directly to the entry in the Jenkins logs, which showed that it was none other than Yours Truly who jacked up the code! I had failed to update the shared data access layer when I committed my code last night. With egg on my face, I corrected the issue. By the time the team arrived in the office, they were greeted with that calm blue glow we've all come to enjoy. While Continuous Integration (CI) can be fast and effective, if environmental health is not confirmed prior to integration you may still end up in “Integration Hell” . With multiple developers checking in code throughout the day, a line of bad code from one could cause issues for another. Continuous Integration, is in part, the practice of integrating new code in to the mainline early and often to avoid “Integration Hell” . Automation is a key feature of a successful CI practice. With multiple contributors checking in code several times a day, it is impractical to perform builds and tests manually. Instead, automated integration and testing of code is performed using a suite of tools: version control (Git), CI (Jenkins), build automation (Maven) and artifact repository (Nexus). Once code is locally-tested and committed, these tools build and test the environment with no further intervention. There is a catch here...what if the environment you are checking your code into is currently unhealthy? Before you check-in your code, you need to check Jenkins and sift through the logs to ensure that the last user's code did not screw anything up. With Jenkins + Slack, you can just look at the Slack channel! Yep, Integration is Awesome! “Slack is most useful when you can see everything your team is doing — that includes all the tools you use outside of Slack! Integrations let you automatically pull information and activity from outside tools into Slack in a way that's timely, relevant, and searchable. “ – Slack Integrations Slack with Jenkins allows for messages to be broadcast on your Slack channel, giving you confirmation of the health of your environment with a simple glance - no digging through log events. It is simple and effective. Slack Integrations takes you through six simple steps to get your designated Slack channel up and integrated with Jenkins alerts. For the slightly more ambitious, the same result – simple confirmation of the health of the environment – can be achieved without having to log into any systems. For less than the cost of a round of beers for your development team, we setup a Raspberry Pi with a daemon that calls the Jenkins API and parses the returned XML for the status of jobs we are interested in. The Pi calls on an open-source Python library, and based on the status returned from the Jenkins API call, the appropriate color lights are lit on an LED strip driven by the Pi. Successful completion of Unit and Integration tests means the environment is healthy – blue lights (Jenkins default) If Unit or Integration tests should fail, the environment is unhealthy – red lights Now, prior to checking in new code, we glance up, see the light and know the state of the environment and react appropriately to the current state. When I walk in the office tomorrow, I will do so with the anticipation of seeing that calm blue glow coming from our team room. Blue lights go great with a fresh cup of coffee and a warm bagel smeared with a dollop of cream cheese. If instead, I find myself immersed in red light, I know I can quickly identify and correct the problem before the rest of the team shows up and scarfs down the last of the bagels.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "60-best-open-source-tools-to-do-devops", "author": ["\n              Mrina Natarajan\n            "], "link": "https://www.ctl.io/developers/blog/post/60-best-open-source-tools-to-do-devops", "abstract": "Don’t you love free stuff? When it comes to automation, open source tools are coveted because they attract support from the developer community. So we compiled a list of the best. Take a look. When I began to research open source tools, I was overwhelmed by the noise. Mostly the marketing kind. Truth as we know is hard to find. After a lot of sifting and interviewing experts at Cloud Application Manager, we came up with this list, which is by no means final. Want to Learn More About Cloud Application Manager and ElasticKube? Cloud Application Manager is a powerful, scalable platform for deploying applications into production across any cloud infrastructure – private, public or hosted. It provides interactive visualization to automate application provisioning, including configuration, deployment, scaling, updating and migration of applications in real-time. Offering two approaches to cloud orchestration — Cloud Application Manager and ElasticKube — enterprise IT and developers alike can benefit from multi-cloud flexibility. Visit the Cloud Application Manager product page to learn more.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installation-troubleshooting-elastickube", "author": ["\n              David Navarro\n            "], "link": "https://www.ctl.io/developers/blog/post/installation-troubleshooting-elastickube", "abstract": "We introduced ElasticKube to ease the management of containers on Kubernetes , and the feedback from the community has been phenomenal.  ElasticKube Diagnostics adds a simple and intuitive module for troubleshooting deployments. It displays the various components and requirements of your containerized applications and their status, giving you the ability to check the health of the service and easily identify what needs to be fixed. This latest update is automatically available within the admin console and accessed at /diagnostics. Diagnostics are extremely valuable during the installation of ElasticKube. If, for instance, ElasticKube is not properly deployed, you will find information on what might be missing or what is configured incorrectly. For common errors, users will be redirected there automatically to begin troubleshooting in real time. Kubernetes Connection: Check if ElasticKube has access to the Kubernetes API. It’s a common problem that prevents ElasticKube from working. Mongo Connection: Check if a MongoDB Replica Controller is running. Websocket Service and Chart Service: Check if the ElasticKube Server Replica Controller is running. If any of these services are not identified as “Ok”, ElasticKube will fail to start or may not be deployed. Internet Connection: Internet connection is needed and most importantly, the ability to fetch the Charts from your target repository. Heapster Connection: This is a key component for pulling data for the various metrics. Kubernetes DNS: Checks if the required DNS is up and running. When deploying ElasticKube to manage your containers on top of a Kubernetes cluster, check out the diagnostics panel and make sure you’re aware of how your services are performing. This will help reduce downtime and provide guidance on which components need immediate attention or will be at risk in the near future. Cloud Application Manager is a powerful, scalable platform for deploying applications into production across any cloud infrastructure – private, public or hosted. It provides interactive visualization to automate application provisioning, including configuration, deployment, scaling, updating and migration of applications in real-time. Offering two approaches to cloud orchestration — Cloud Application Manager and ElasticKube — enterprise IT and developers alike can benefit from multi-cloud flexibility. Explore ElasticKube by visiting GitHub (curl -s https://elastickube.com | bash). Visit the Cloud Application Manager product page to learn more.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "ghost-docker-container-six-easy-steps", "author": ["\n              Manuel Martin\n            "], "link": "https://www.ctl.io/developers/blog/post/ghost-docker-container-six-easy-steps", "abstract": "In this post we are showing you how to deploy the Ghost blogging platform in a Docker Container on any cloud using Cloud Application Manager, digging deeper into Docker as a Service, one of the latest Cloud Application Manager features released in Cloud Application Manager. The following step-by-step instructions will teach you how to build a Box that will allow you to deploy an instance right away. If you haven’t already signed up for Cloud Application Manager, create your account to get started . In this tutorial, we will be showing you how to launch and instance of Ghost, a simple but powerful publishing platform, in a Docker Container on the Cloud Provider of your choice. By the end, you will have built a Ghost / Docker Box in Cloud Application Manager that can be deployed, versioned and shared. 1) Setting Up Dockerfile As a first step, we need to get the Dockerfile required to deploy the Ghost service in a Docker Container. You can find the required commands and information on the Ghost Repository on Docker Hub. Download the trusted Build Bundle from the Docker Ghost Repository, you’ll be needing that file in just a few minutes. 2) Create a Ghost Box The next step is to create a new Box in Cloud Application Manager and make sure to use Docker as the selected ‘Service.’ 3) Upload the Ghost Dockerfile Once your Box has been created, upload the Ghost Dockerfile you just downloaded from Docker Hub. Every time you save a Dockerfile, Cloud Application Manager will parse the template looking for EXPOSE commands. For every match, Cloud Application Manager creates a port variable and modifies the Dockerfile according to Jinja syntax . In this case, Cloud Application Manager will create a port variable called expose_2368. The result will look like: EXPOSE  {{ expose_2368 }} After all these steps, your Ghost Box should look like the screenshot below! Hint: You won’t be able to deploy the Box just yet because it doesn’t have access to the files included with the ADD commands in the Dockerfile. 4) Add Additional Scripts Add all the additional files as file variables. In the current case add the file ‘start.bash’ and call it START_BASH (the file is located within the Build Bundle). We also marked the file variable as required because it has to be included during the deployment phase. Click the ‘New Variable’ button in the Box view and select the ‘File’ option from the dialog that pops up. Name the file ‘START_BASH’. Then, upload the start.bash file from the Docker Build Bundle you just downloaded. 5) Add a New Path in the Dockerfile Head back to box and edit the Docker file to  add a substitute local path using the Jinja syntax: {{ variable_name}} It should look like: ADD {{ START_BASH }} /ghost-start 6) Deploy the Box on any Provider! Congrats! You’ve built a Ghost Docker Box which you can now deploy on any provider that’ve added in Cloud Application Manager. For more information on deploying instances in Cloud Application Manager, please visit our documentation on that subject. Cloud Application Manager is a powerful, scalable platform for deploying applications into production across any cloud infrastructure – private, public or hosted. It provides interactive visualization to automate application provisioning, including configuration, deployment, scaling, updating and migration of applications in real-time. Offering two approaches to cloud orchestration — Cloud Application Manager and ElasticKube — enterprise IT and developers alike can benefit from multi-cloud flexibility. Explore ElasticKube by visiting GitHub (curl -s https://elastickube.com | bash). Visit the Cloud Application Manager product page to learn more.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "five-developer-blogs-to-follow", "author": ["\n              Jessica Weber\n            "], "link": "https://www.ctl.io/developers/blog/post/five-developer-blogs-to-follow", "abstract": "When it comes to keeping up on current trends and information within your industry, there is a ton of information out on the web. How do you know what to follow and who gives the best information? It's hard to keep up on individual blogs, even when they have mailing lists and RSS feeds (which, thankfully, do exist). But we've come up with a list of 5 varied blogs that can keep you up-to-date on a wide berth of information in the field and maybe even entertain you in the process. The Netflix Tech Blog The tech behind Netflix can be a fascinating subject, especially when it comes to how they make all those shows magically appear across the screen. Learn the science behind how they develop their billing systems, code for a global audience, and manage the processes a large network of people. David Walsh As Mozilla’s senior web developer, David Walsh speaks to skills in HTML/5, JS, and CSS. He's also the core developer of the MooTools Javascript Framework . David’s blog focuses on these front-end technologies, offering insight into open source contribution and trial-and-error development. Programmable Web Programmable Web is all about APIs and the evolving tech around them. It's most impressive feature is probably the giant amount of quality, hands-on content. Programmable Web boasts up to 10 posts a day by respected pracitioners of the field, and also provides a directory of APIs for web and mobile development, along with whitepapers and research to back up its posts. Code Simplicity As a software engineer at Google, Maz Kanat-Alexander's is a complimentary blog to his book Code Simplicity: The Fundamentals of Software . Max is also the chief architect of the Bugzilla Project ; his blog expands upon the same focus of his book – simplification. In the code world, you can never go wrong with his mantra of \"Complexity is stupid. Simplicity is smart.\" Coding Horror Web application developer and co-founder of StackExchange , Jeff Atwood writes this blog based on the human component of development. Coding Horror focuses on how human behavior can make or break good software development and security. Its the focus on this piece of the puzzle that makes this blog stand out; Jeff reminds us all of the humanity behind the code. If you are ready to get started but are not yet a CenturyLink platform customer, no problem. Get started by activating an account . today. Designed for your business needs today and tomorrow, CenturyLink is reliable, secure , robust , and global . We’re a different kind of Hybrid-IT provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "improve-accuracy-of-predictive-monitoring-with-metric-forecasting", "author": ["\n              Jim Greene\n            "], "link": "https://www.ctl.io/developers/blog/post/improve-accuracy-of-predictive-monitoring-with-metric-forecasting", "abstract": "Anomaly detection has a range of business applications, from discovering fraud in credit card transactions to intrusions in a computer network. With deeper context for fluctuating metrics through anomaly detection, forecasting and predictive monitoring, identifying unusual network traffic patterns can help an IT Security team ward off attempted attacks. Algorithms modeled to forecast expected behavior can give your team the ability to both visualize expected trends and specify when they want to receive alerts about potential issues. Forecasting evaluates a metric’s evolution and predicts future values, but there’s more to defining critical levels for the checks and warnings that measure these values. Setting levels too low creates false alarms, while setting them too high blinds monitoring to potential problems. Metric Forecasting is based on Holt Winters, a mathematical model that relies on historic behavior to establish value for an input signal. Through a proprietary service that collects and track metrics, monitors log files and sets alarms, CenturyLink Cloud Application Manager weeds out a greater number of false positives than standard systems. Other monitoring options can leave much to be desired, especially when alerts occur and there’s really nothing to worry about. Holt Winters, or Triple Exponential Smoothing, models and predicts the behavior of a sequence of values over time and is the IT industry’s best-practice approach for time series. More than 60 years old, this popular methodology is still used in many applications, including monitoring, for anomaly detection, as well as for purposes such as capacity planning. When applied to monitoring, Holt Winters can observe metric trends as they extend into the future, alert on problems based on changes in normal behavior, or not alert on normal behavior that may approach or exhaust system capacity. Holt-Winters forecasting takes into account three components of a dataset: Three coefficients are critical to the accuracy of the model: Any metric available can be graphed in the Watcher UI — including AWS CloudWatch metrics. Anomaly Check Forecast Check Forecasting with Holt-Winters can handle many complicated seasonal patterns by simply finding the central value, then adding in the effects of slope and seasonality. By combining log data from the risk profiles of each customer asset with near real-time threat intelligence data from our global corporate network and partner threat intelligence feeds, anomaly detection with CenturyLink Cloud Application Manager gives you the power to set thresholds that are likely to be more meaningful — so your monitoring doesn’t “cry wolf,” or overlook a real-time threat. More resources: Anomaly Detection & Forecasting Cloud Application Management Monitoring – Suppressions Cloud Application Management Monitoring – Events MSSP - Security Log Monitoring with Trending and Threat Analysis", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "high-school-students-hosting-robotics-peers", "author": ["\n              Jim Greene\n            "], "link": "https://www.ctl.io/developers/blog/post/high-school-students-hosting-robotics-peers", "abstract": "CenturyLink is sponsoring an exciting initiative created by ECG Robotics this year, providing cloud computing resources to host websites for teams participating in FIRST Robotics , a family of international competitive robotics programs. ECG Robotics has nearly 100 members who are high school students from Greensboro and Guilford County, North Carolina. Its five teams compete in two levels of FIRST ’s “sport of the mind,” where teams build robots to score points on a challenge field. With one program entering the last few weeks of the build season, we checked in with Aidan Hunt, who told us that his team’s robot should soon be ready for competitions. Meanwhile, one of ECG’s teams in the other program brought home a Connect Award at their recent qualifier. Today, let’s take a closer look at the team’s project to provide shared hosting services for their fellow teams, and the technology and applications they’re using to host and support these sites. The team exclusively uses CenturyLink's Cloud products. Websites are hosted on a production Bare Metal server, speced out with a quad-core Intel Xeon processor and 8 GB of RAM. An additional virtual server is used to host mail services and other internal sites such as a status page. With the flexible CenturyLink Cloud Control Portal, the team can quickly spin up additional lightweight servers for testing without disrupting production. Unlike many competing platforms, CenturyLink Cloud allows users to customize CPU, RAM, and storage independently, rather than forcing users into predefined packages. The team uses Ubuntu 18.04.1 LTS, providing a nice balance of recent tool versions and the stable package quality needed for production. The team follows the configuration-as-code approach by keeping all config files in a Git repo privately stored on GitHub . This approach removes many of the manual tasks involved in infrastructure provisioning, maintenance and software configuration, and also allows the team to roll back potentially disruptive changes. Technology for the operating system, content management, and traffic management are mostly open source, including the WordPress content management system that the project's users work with to build their sites. Though the users won't ever encounter them, two open-source projects are especially critical to the platform's success: These programs work together to implement the microservices architecture, so that subsystems, such as the MySQL databases required by WordPress, are stored in their own containers, rather than being shared between teams. This arrangement allows Docker to be used to manage almost all activity on the service, and to control file storage and access for effective backup and security. The team has previous experience with Docker from developing their StrangeScout competition scouting program, which they will also be hosting for other teams on the server. From the start, the platform was designed with high security in mind. The Traefik container handles all traffic on ports 80 and 443, and enforces the use of HTTPS for all connections to the outside. Traefik automatically requests the certificates it needs from Let's Encrypt , the nonprofit certificate authority, so full encryption of all traffic comes at no cost to the team. Additionally, Traefik makes it easy to set the HSTS header , which blocks protocol downgrade attacks. On the application side, the microservices design pattern provides further benefits. Giving users the ability to host their own dynamic websites can be a challenge for security, but the team provides their platform with several lines of defense. First, the PHP-FPM and web server processes run as non-root users within the container. Secondly, the container is only given access to data on a filesystem mounted with the noexec option on the virtual server, further protecting against compromise. Lastly, Docker networks are configured to limit unneeded access; for example, the MySQL containers can only be accessed by their respective WordPress install. Handling client data also requires a carefully defined backup plan. The team uses CenturyLink Object Storage with restic for easy, reliable backups of config files and webroots. It’s capable of backing up local files to a number of different backend repositories: a local directory, an SFTP server or an S3-compatible object storage service. The team had previous experience using restic with S3, so it was easy to upgrade to CenturyLink’s lower-cost Object Storage backend. Designing a production system requires consideration of many factors and assembling a technical stack involving many different programs working together. Hunt says the team appreciates CenturyLink's investment not only in their project, but in providing them the ability to get real-world skills in system development and administration, communications, and technical support by working on a real project while in high school. Here at CenturyLink, we're thrilled to give back to the FIRST community by providing the infrastructure for this exciting program. FIRST®, FIRST® Robotics Competition, and FIRST® Tech Challenge are registered trademarks of FIRST® www.firstinspires.org which is not overseeing, involved with, or responsible for this activity, product, or service.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "why-to-not-trust-the-root-user-with-your-data", "author": ["\n              Server General Team\n            "], "link": "https://www.ctl.io/developers/blog/post/why-to-not-trust-the-root-user-with-your-data", "abstract": "Attacks on businesses computing infrastructure are becoming more sophisticated and potentially more damaging. Threats have escalated as IT organizations are increasingly being asked to provide more services. Security officers are fighting these threats on a daily basis, but their efforts are limited to the effectiveness of various ‘gates’ that are in place. These gates are of little use when the attacker has successfully gained access to the machine as the “root” user. This is because there is no easy way to enable the “root” user to manage the computing environment without also giving them unfettered access to the business data that is stored within that environment – like critical data stored in a database or a file server. Information theft is defined as any access of sensitive information that is not performed in accordance with the methods provided by the application service and by organizational policies. This form of information theft is typically accomplished by technically capable attackers such as a malicious or compromised “root” user who knows how to get around access controls or exploit existing holes in operating system and application software. An outside attacker can gain “root” user privileges via advanced persistent threat (APT) or by exploiting holes in the application software, e.g. buffer overflow or via spear phishing. Irrespective of how the attack is launched the motivation remains the same – gain access to the system as the “root” user. Once the attacker assumes “root” privileges then they can launch all types of attacks to gain access to the sensitive information stored in a database server or a file server as discussed below. A malicious “root” user is an insider who has access, privilege, skill, motivation and knowledge of the process. They can access information directly at the operating system layer. This advanced knowledge about the application and system provides the ability to cover their tracks easily. From a security perspective there is no difference between a compromised or a malicious “root” user. Below are the main ways that the “root” user privilege can be abused to gain access to the sensitive information. Privilege Escalation Attack: A malicious/compromised “root” user can escalate their privileges or assume identity of another user who has been granted access to the sensitive information. This is a very simple type of data theft. Application Server Attack: A malicious or compromised “root” user can modify the application server executables or operating system shared libraries thereby compromising the integrity of the server and gain access to data that the server generates and attempts to control. This method can be used to bypass data-at-rest encryption but requires advanced skill set. Data Tampering Attack: Data tampering is generally launched by sophisticated attackers who have an endgame in mind e.g. to either disrupt the normal operation of the organization, or tamper specific data sets to compromise other computing systems which may be used for payroll, accounts payable, or storing trade secrets or intellectual property. This type of an attack is very hard to trace. Attackers can use data tampering to manipulate encrypted data sets. Do not trust the “root” user with your data. This obviously is hard to implement. However, a good data security solution must find a way to protect data against attacks launched by a malicious or compromised “root” user. Server General provides a solution by implementing advanced access control mechanisms that go above and beyond the POSIX based access controls. Both  solutions, Server General TDE and Server General KMS, are designed not to trust the “root” user and therefore deny access to the protected data sets. HIPAA/HITECH When you encrypt your ePHI using embedded transparent data encryption (TDE) functionality of your MySQL server then you have to ensure the safety of your encryption key that was used to encrypt patient information. Moreover, you will have to rotate your data encryption keys periodically to comply with good security practices. Server General KMS for MySQL can help you manage your MySQL master key in a manner that will make it easy for you to comply with the HIPAA/HITECH Act. Payment Card Industry Data Security Standard (PCI/DSS) Businesses rely on Server General KMS for MySQL to meet the PCI DSS mandates when their in-scope data is stored in a MySQL database server and is encrypted using MySQL’s transparent data encryption (TDE) functionality. We have years of experience helping tier-1 customers go through their PCI audits and have designed our solution in a manner that makes it easy to comply with the PCI DSS mandates. General Data Protection Regulation (GDPR) The European Union’s General Data Protection Regulation (GDPR) will become effective as of May 25, 2018. Just like California’s SB 1386 data breach notification legislation, GDPR stipulates that any entity that handles EU citizen’s data must provide notification of a successful breach. The law requires the entity to prove that it had put all the right measures in place to protect personal information. Many businesses use the embedded encryption functionality of their MySQL database to protect the sensitive information. Server General KMS for MySQL can help such businesses to manage their MySQL master encryption keys in a secure and compliant manner. If you are a current CenturyLink Cloud customer and you are ready to get started with Server General Data Security Solutions, visit the Server General page on the CenturyLink Marketplace .\nIf you are ready to get started but are not yet a CenturyLink Cloud customer, no problem. Just head over to our website and activate an account .\nDesigned for your business needs today and tomorrow, the CenturyLink Cloud is reliable, secure , robust , and global . We’re a different kind of cloud provider – let us show you why .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "lamp-stack", "author": ["\n              Mrina Natarajan\n            "], "link": "https://www.ctl.io/developers/blog/post/lamp-stack", "abstract": "What if a little genie told you that Aladdin’s magic LAMP is the stuff of app dreams. LAMP what? Well, the acronym stands for Linux the base OS, Apache the web server, MySQL the database, and PHP the scripting language respectively. It’s a popular mix of technology to build high performance websites and web applications. Now you’re thinking what Cloud Application Manager has to do with it. It’s simple. We help you put together a LAMP Stack environment in no time so you can be on your way to build out your application. Besides, here are solid reasons why Cloud Application Manager can help you build apps better: How would you like to build a powerful web application that auto scales and automatically load balances as you attract more site traffic?\nWant to rest assured that your app configuration can work on any cloud, AWS today, Google cloud or a combination thereof tomorrow?\nWant to take advantage of the offerings from different cloud providers to maximize the best that the breed have to offer?\nIf you answered yes to any one of these, then help is at hand. Better than explaining how it works, why not give it a shot? Go ahead and build a LAMP Stack using Cloud Application Manager in under 30 minutes. See how this simple environment can jump start the app of your dreams. Cloud Application Manager is a powerful, scalable platform for deploying applications into production across any cloud infrastructure – private, public or hosted. It provides interactive visualization to automate application provisioning, including configuration, deployment, scaling, updating and migration of applications in real-time. Offering two approaches to cloud orchestration — Cloud Application Manager and ElasticKube — enterprise IT and developers alike can benefit from multi-cloud flexibility. Explore ElasticKube by visiting GitHub (curl -s https://elastickube.com | bash). Visit the Cloud Application Manager product page to learn more.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "the-name-of-the-online-development-game", "author": ["\n              Jonathan Townsend\n            "], "link": "https://www.ctl.io/developers/blog/post/the-name-of-the-online-development-game", "abstract": "About Chris Molozian: Chris is a founder and CEO of Heroic Labs , a game development company that has an API that allows game developers to develop, customize, and add innovative and interactive capabilities directly into games quickly and easily without forcing developers to deal with and navigate a server's backend. Heroic Labs builds social infrastructure for developers and is focused on social and real-time experiences for the games industry. The following interview is a tech-driven chat with Chris Molozian about online game development. Erlang and Rust are your favorite languages. Those are obscure choices, especially to a game developer. What's the story behind that? For business development on the platform for Heroic Labs, the main reason we chose Erlang is that as a small team, we want to be able to build scalable and resilient systems without requiring a larger Operations and DevOps team. We wanted to be able to build a system that was able to have the level of availability guarantees we were looking for without requiring us to have the kind of Operations teams that, in my experience, are more necessary with other programming languages. Erlang’s features – supervision trees, restart strategies, and the ability to monitor process ID's – tie together in the way the language is designed, to enable you to build systems that I describe as self-healing. This self-healing is evident in the ability for a system to transparently recover from failures, network timeouts, disk failures, all different kinds of transient failures that you tend to get when you’re running inside cloud infrastructure on top of infrastructure-as-a-service (IaaS) providers like AWS. Erlang allows us to more efficiently manage those kinds of restart strategies and transient errors that can turn into cascading system failures. Rust was more of a personal choice for us to be able to experiment, with a degree of safety, at a systems level when it comes to getting the kind of performance requirements necessary from things like dynamic language runtimes or integration with C-based libraries for things like v8 for javascript runtimes or lure. We wanted to wrap them in a safer language that still enables us to achieve the kind of memory safety guarantees you typically get with managed languages, where they have some kind of garbage collector or managed object lifetime strategy. We don’t use any Rust code in production yet, but it’s an area where I’d like us to get closer to BareMetal, without constantly having that deep concern that if we don’t have a huge engineering team with an advanced knowledge of C++/C we may be leaving behind memory that can be double free or introducing memory errors, memory leaks, and security vulnerabilities into our code because memory management strategies can get very complex as code bases get larger and larger over time. That’s what I think Rust is very good at – giving you that BareMetal guarantee of performance while introducing lifetime concepts around managing objects and states between threads and system-level communications. Rust has simplified C++ to the right extent. The lifetime controls you get over data and the way you share data between threads does introduce a more cognitive load for the engineer (you need to understand how this data is being owned by different threads in the system), but overall, and as a result of it, you get more explicit control of how the code communicates together and reduces the likelihood of errors creeping into the code. How do you use Orchestrate Events? Events is one of our smaller use-cases – intended for us to enable an audit trail. We use it to build up time-based audit trails on some developer actions performed inside the configuration of our dashboard. It was easy for us to build. It was a no-brainer to use Orchestrate for that part of our system. How do you use key value listings and filters (filter queries)? We use that to namespace data that belongs to different games within our system. We put all data into a particular collection and then we filter that data out so that specific games that own certain elements of data only have access to those records within their game. It’s a way for us to build a multi-tenant storage on top of a collection system like Orchestrate. How do you use search and how do you decide when to use it instead of key value listings? Search is our most widely-used feature. It’s embedded inside of our game API in what we call shared storage. Shared storage is a way for players to share objects (data) between each other in the game. Take a game like BoomBeach from SuperCell; within the game players build up an army and those armies defend their islands and then other players that are in the vicinity of those islands are able to compete in some kind of combat scenario – attack the islands, attack the armies, etc.. So to power that, we enable game studios to say that the particular data being stored into our shared storage API is read-only accessible to all other players in the game. What happens is the game client can retrieve a collection of criteria based on properties that they want to match with opponents in the vicinity of a game through a search query. This allows for optimal opponent matching, i.e. someone can be matched with another opponent based on who has close to the same experience or similar weaponry. It can also allow gamers to play against old records they set for themselves or, for example, other players set in a racing game. Anything that can be stored can be retrieved again for the most relevant and ideal playing experience – this also helps us to establish a connection between players without requiring a real-time/live connection. A search engine is built to give you relevancy, and yet not a lot of the industry invests in incorporating it into their systems; probably because this kind of search infrastructure is so complicated to run, which is why it’s easier to do it on top of Orchestrate. You're the CEO – how often do you write code? Not as much as I’d like. I do a lot of the tinkering and experimenting – with new game engines, new APIs – seeing if we can drive some new innovation in API design from those areas. I probably spend 30-40% of my time writing code, just because I end up needing to do everything else – business development, marketing, sales...everything. What's your development setup look like? SD for service discovery (a tool from Google) allows services in our infrastructure to publish themselves for other parts of the system to find and communicate with. Our system has four major types that have a collection of services: The Account System: handles player accounts, social login (integration with all the social networks), and upcoming work on things like social graph. The Game API: the core set of APIs that are available for game developers to use from their game clients – leaderboards, cloud storage, achievements, and everything that also communicates with Orchestrate. The Management System: everything that is configured through our developer dashboard is managed in the management code and servers and then gets mirrored into the rest of our services so they can have a cache copy of it, to reduce latencies and reduce dependencies between services. The Cloud Code: JavaScript execution service that we’ve set up. It has a collection of services that handle untrusted code execution so developers can upload scripts and manage scripts through Github, which they can access and execute through the game client on our API. We have that as an isolated part of the system – it's effectively an execution environment that uses a combination of Java and Erlang to execute these scripts within a runtime and enable them to make things like HTTP requests, do data validation, do business rules (if you need rules around which player wins after a match is complete) and execute any other custom behavior that a game developer needs. We have developers who’ve built custom game tournaments or gifting and rewards through the code base and everything in between. All of these systems interact with each other as they need to. OK, let's switch gears...how do you make an Old Fashioned? Sugar syrup/sugar water, orange rind, some kind of whiskey (rye of some kind), some ice, and some bitters. Still my favorite drink. Sign up for our Developer-focused newsletter CODE . Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. CenturyLink Cloud – We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "build-a-2-container-app-with-docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/build-a-2-container-app-with-docker", "abstract": "Docker is fundamentally changing the way people do infrastructure in the cloud. However many developers I know are still confused about how to use it to do anything beyond a simple \"Hello World\" or a WordPress-all-in-one container with everything bundled into a single container, Apache, MySQL, nginx, etc. The power of Docker is in being able to ship and scale individual components or \"containers\". However it is still a bit of a black art about how that works. There are a few awesome projects being started to make it easier to build multi-container apps like CoreOS , Serf , Flynn , Maestro , Deis , and Dokku . But what if you want to do it without any fancy tools? How do you do it from scratch? I am not going to say this is the best way, but I would like to walk you through how I recently built a 2-Container app (WordPress in one container and MySQL in the other). In future articles, we will go into more complex setups and walk you through how to use some of the cool Open Source tools out there. This tutorial will guide you through 5 somewhat easy steps ( Panamax is a new project we are working on to make this kind of tutorial unnecessary, you will be able to stitch together Docker containers in a simple web UI). First, setup docker on your machine. If you are on a Mac, this is a simple 3-step process: boot2docker which will install Docker on your Mac. Now that you have docker running on your development machine, let's do something interesting. Let's create your first single-container application. First, you go to the public Docker Index to find a container you want to use. Let's search for WordPress. You will find about 30 results, but you only want to use a Trusted Repository. NOTE: Most of the docker containers in the Docker Index are unsafe to use as-is. You have no idea how they were built, so you can't be sure there isn't malware on the images. If you want to use any public Docker images as a starting point, make sure to use a \"Trusted Repository\" and make sure to audit it's Dockerfile before you use it. One of the good trusted WordPress containers out there is managed by Tutum , a Docker-based hosting company that lets you run Docker containers and provides private docker registries and private docker images. The WordPress container you want to use is called tutum/wordpress and you can use it quickly and easily by doing the following: Congratulations! You are now running WordPress in a single container with MySQL on the same container. Easy right? Want to see WordPress running for yourself? Run docker-osx ssh , and then run curl 0.0.0.0:49163/readme.html Now how do you get MySQL to run in its own container? Now that you can use existing \"Trusted Images\", let me show you how to modify them. Instead of running in daemon mode with the -d flag, use the interactive and TTY flags ( -i -t ) and specify bash to get into the container as root. Ever wonder what /run.sh did in step 2's run of the docker command? Let's find out. Now that we don't want MySQL running on the same container as WordPress, let's modify this container to not run MySQL. In a new Terminal window, run docker ps again to find the container id of your interactive container. Now that we have the container id (dfa24049c264) we can commit our changes to the filesystem. Now we have our own version of tutum's WordPress with MySQL disabled. Let's try it out. First we will stop our other containers and then we will start a new one with our newly committed image, myuser/wordpress . Notice that the 3306 port does not show as open in the new container when it did in the previous one, but that you can still curl the url curl 0.0.0.0:49164/readme.html . Just like WordPress, you can get yourself a MySQL container from the Docker Index. Again, let's go in and check out what run.sh does. Look familiar? It should, the same tutum folks made this image. We don't need to remove any components out of this container, but we do want to make sure that the root username and password do not change. If you go back to your OS X terminal, we can find out what username and password were generated and commit the new container locally. You can now connect to this MySQL Server using: mysql -uadmin -pqa1N76pWAri9 -h -P Please remember to change the above password as soon as possible! OK now we know what the username and password are and have created our own container with a static root username and password. Let's stop the generic container and start our new custom MySQL server container. Congratulations, you have reached the last step! All you have to do now is bind the two containers together. In order for the two containers to know about each other, we will need to link them using the docker -link flag. Before we do that, we will need to make another modification to our WordPress container so that it can pull credentials from environment variables. Now you go back to another OS X terminal (keeping the bash container running) and commit these changes to the container. Finally, it is time to start the myuser/wordpress container with a special -link flag and the environmental variable for the MySQL DB password. We now have a 2-container setup that are bound together. To see it in action, go to: We can now create a 2-container application in Docker without even having to touch a Dockerfile. We can tie the containers together in a loosely coupled way. You can create multiple instances of the WordPress container that all talk to the same MySQL database. Note that this is not a production environment yet. The MySQL container's data is ephemeral and we did not cover how to persist it yet (hint: look at the docker run -v flag). Also note that in production we would want to docker push our myuser/wordpress and myuser/mysql images to a private Docker registry. We will show you how to run your own registry in an upcoming tutorial. In the next installment next week, we will talk about adding an nginx container to load balance and using Serf to automatically make all of the containers aware of each other. To automatically get an email when the next installment comes out, subscribe to our weekly newsletter that has Docker tips and news.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "15-quick-docker-tips", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/15-quick-docker-tips", "abstract": "Brian Morearty recently gave a lightning talk at Twitter HQ about Docker. You should really check out his slides, they are fun and have a lot of character. He also runs a hands-on training course for Docker. But as a reference, here are the 15 tips he suggests: Thanks to Brian Morearty who has a hands-on training course for Docker. ( Edit: Brian removed this online course because it was too out-of-date, created in Jan 2014 and using Docker 0.7.6)", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-to-test-http-apis", "author": ["\n              Originally posted on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/how-to-test-http-apis", "abstract": "When you decide you’re going to integrate a web-based API like Orchestrate into your application, writing tests can seem daunting, annoying or even impossible. But fear not! I’m going to share a really easy way to write API tests. By the end of this post, you should have a clear picture of how to test your application when using APIs like Orchestrate. Hat tip to Myron Marston for a great addition to the Ruby (and programming) community, called VCR . Rather than trying to describe what VCR does in my own words, I’ve lifted what Myron says from the VCR’s README: Record your test suite’s HTTP interactions and replay them during future test runs for fast, deterministic, accurate tests. The moniker “VCR” is a fantastic metaphor for what the library actually does. It “records” by serializing (generally in YAML) the request and response of the HTTP interactions as the test suite exercises the application code. These recordings are known as “cassettes” and are usually called fixtures in your test suite. The fixtures are then used to mock the request and response of the HTTP or network libraries without actually sending anything over the wire. The quick example below is in Python, but you can do this in many other languages: Now for the feature presentation—Below, I will demonstrate the use of the Python port of VCR, VCR.py . For this example, I’ve chosen to write tests for a community-contributed Orchestrate client library written by Jeremy Brown called orchestrate-py . The client library is fairly new, and as Jeremy pointed out : \"This is early in it’s development and subject to change. Collaborators, suggestions and testers welcome.” I’m also going to use a testing suite called py.test . We’re using Python’s pip to install and freeze the dependencies from the root directory of our repository. Keeping the API key in the code is OK in this instance because, with Orchestrate and many other services, it’s easy to destroy an API key and regenerate a new one. We’ll want to do that at the end once we publish our commits; then only the old, inactive key will be in the fixtures and tests. Here we’re just simply testing the ability to put a key and value into a collection in Orchestrate. At this point, we’ll want to run our test suite and make sure the tests pass. Now that we have a working test, we simply decorate it with VCR’s fixture (aka “cassette”) location Note the use of: When I decorated the function, I didn't create the directory structure or the file name. VCR.py will auto-magically create this for us the next time we run the test suite. Based on our configuration of VCR.py in step 1, we are going to use the record mode “once”. This means we’ll let the HTTP library make a call once and write the output to our cassette file. There are other modes, discussed in VCR.py’s documentation . This is straightforward enough.  In our case, since we’re using py.test, all we do is: Once we’ve done this, we can see that the fixtures (aka “cassettes”) have been recorded and placed in: ./fixtures/vcr_cassettes/put_key_value.yaml . Feel free to explore it, since it is written in YAML it is pretty straightforward. Note, the fixture has recorded the Basic Authentication headers. Since this is hard-coded into our fixture and we will check this into a repository, we will want to destroy our API key that we used for testing. Finally, if you run your test again, you’ll notice a marked performance gain in testing. This is because there isn’t any network traffic—VCR.py is handling those requests for us and using the fixtures we have defined. Have fun building great things with Orchestrate . By the way, we're live now in the CenturyLink Cloud . Sign up today for Orchestrate at orchestrate.io .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "build-user-authentication-with-node-js-express-passport-and-mongodb", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/build-user-authentication-with-node-js-express-passport-and-mongodb", "abstract": "One of the trickiest aspects of building my first application was implementing User Authentication. The hours I spent with my head against the keyboard trying to will it to work, instead of gleefully logging in and out, will never be regained. So, let me help you navigate these tricky waters! In this series we are going to build a simple application in order to explore implementing user authentication off of MongoDB with a Node application running an Express server combined with Passport. We will be building local authentication, as well as authenticating through Twitter, Google, and Facebook. Each installment of this series I will show how to implement a new User Authentication Strategy, as Passport calls them, addressing each of our four methods mentioned. This first post covers setting up our application and the joys of locally authenticating users! To start, let's go over the technologies we will be using: In this post we will: IIf you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to access CenturyLink Cloud products. Our first step is to deploy a new CenturyLink Cloud virtual server. Follow the steps below. On the left side menu, click Infrastructure and then click Servers . On the left-hand side of the server panel, click on the region for the server we will provision. Your server provisioning request will enter the queue. You can watch the progress of your request on the screen. Your server is provisioned when the status of all tasks in the queue is complete. After your new server is provisioned, in the CenturyLink control portal, click Infrastructure on the left side menu, and then click Servers . Type \"27017\" in the blank box to open up the MongoDB server port. Click add public ip address . From a shell on your local machine, connect to your new server with the following command. Replace \"YOUR.VPS.IP\" with your server's public IP address. Install the MongoDB server software by running the following commands. With your favorite text editor, open /etc/mongod.conf . Look for the line that begins \"bind_ip\" and comment it out. The top of your file should now look like this: Start the MongoDB service by running the following command. First up, we need our basic application. If you already have Node.js and npm set up on your computer, then by all means carry on! If you do not, drop by the Node.js website to set yourself up and we will be waiting for you here. Now then, let's go ahead and establish our dependencies and install them. You will need to either build your package.json file or you may clone the code from the GitHub repository linked at the end of this post. In your package.json make sure you have at least the following: After making sure you list the above dependencies in your package.json go ahead and run: It is time to build our server now that we have all of our dependencies. (Note: not all of these dependencies will be used in this post. That is alright since we will need them in the future) Let's move over to our index.js file, which will hold our server, and bring in our necessary dependencies: Now that we have built our server we can test it: If our application is functioning properly we should see a message, part of which reads, \"listening on port 5000!\" or whichever port you indicated your application to run on. Go ahead and kill the server by pressing Control-C , it is time to add our templates so that we will have something pretty to look at! If you have not cloned the repo, create a \"views\" folder in your project's directory. Within that folder we will have two files (home.handlebars and signin.handlebars) and one folder (layouts). The layouts folder will contain one file (main.handlebars) which is the default layout we chose when we configured our Express application. Our main.handlebars file will look like the following: Our home.handlebars file should look like the following for now: Finally, our signin.handlebars file should look like the following: Now that we have our layouts in place, let's write some routes so that we can see them! In the routes section of our index.js file add the following code to get us up and running: Even though we have not set up our Passport Strategies yet, we can now run our application and view the homepage and sign in page from our browsers. We are well on our way! Speaking of those Passport Strategies, let's take a moment to discuss how Passport works. According to the Passport documentation , different authentication mechanisms are known as Strategies which can be installed and used modularly as desired. In this post we are using the Local Strategy. Toon Ketels does a good job of explaining the Passport authentication flow on his blog . The important information to note is that when a user submits their credentials, the request will be passed to Passport's authenticate() middleware we have established, and invoke which ever Strategy we have configured for that request route. These credentials (req.body.username and req.body.password) are passed into our Strategy and subjected to verification. If verified, the user will be serialized into the session and logged in to the application, if not verified the Strategy will return an error and the authenticate()middleware redirects to our designated failure location. This process looks like the following: Now that we have an understanding of how Passport functions, let's build our Local Strategy! For our application, we will need to use two Local Strategies, one for registering new users (local-signup) and one for logging in already registered users (local-signin). We already have routes using each of these Strategies, and will just need to make sure to name our two Strategies to match their routes. Before we build our Strategies, we will need to build some helper functions for storing and retrieving users from MongoDB, as well as a creating a file to hold our various token information. We will start by uncommenting these lines in our index.js file: Next we'll create our config.js file which will hold our MongoDB configuration. Replace \"YOUR.MONGODB.HOST\" with the IP address of your virtual server. With this information established we can create functions.js to hold our helper functions: The localReg function will be used in our Local Strategy to register new users and save their information into our database. It works by constructing a user object, including encrypting the password for protection. The function then checks if the user already exists in the database. If the user already exists it will reject the request by returning false to avoid overwriting the existing user. If the user does not already exist, the user object we created will be stored using its username as the key. This user will then be returned in order to be passed back to the Strategy so that it may proceed with the Verification process. The localAuth function will be used to check if the user attempting to log in matches with the one stored in our database. This function checks the database for a user matching the given username. If a match is found, the retrieved password is compared to the one provided. If the passwords do not match the request is rejected by returning false. If the passwords do match, the user will then be returned in order to be passed back to the Strategy so that it may proceed with the Verification process. Now that we have our helper functions, let's build our two Local Strategies! In the Passport section of index.js let's add: The final pieces of our Passport section we need to add are the serialization and deserialization of users into and out of the session. Since our user objects are very simple we will be serializing and deserializing the entire user object, but as they become larger and more complex using only one aspect of the user object can be more efficient. For our needs inserting the following at the start of our Passport section will suffice: As a side note, if you have any sections of your app you need to protect so that only authorized users can access them, Passport makes the process pretty easy. Simply use the following function on your route for the protection section: Now we have a working application with Local User Authentication! Let's test it out. If we start up our server and visit our specified port, we should be able to proceed to \"Sign in\" by clicking the big blue button or selecting the menu item. From here our \"Sign in Locally\" button should yield two further options to log in or create an account. Since we do not have any users in our database we will need to create an account. You can test your security by trying to log into a non-existent account. You should be denied. Once you create your account you should be logged in! If you head over to your MongoDB database you should also now have one user in \"local-users.\" Congratulations! You have Local Authorization! Happy authorizing! View the Code on GitHub", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "wordpress-in-a-docker-container-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/wordpress-in-a-docker-container-on-the-centurylink-cloud", "abstract": "Installing and playing with Docker on an Ubuntu based server is interesting, but what we really want is to install and run containerized applications. A good example application is WordPress, as it consists of a web front-end backed by a MySQL database. First make sure you have your CenturyLink Cloud Server setup correctly with Docker installed as per this tutorial . Next go to the public Docker Index to find a container you want to use. Search for WordPress and you will find about 30 results. For the most part you only want to use a Trusted Repository. NOTE: Most of the docker containers in the Docker Index are unsafe to use as-is. You have no idea how they were built, so you can't be sure there isn't malware on the images. If you want to use any public Docker images as a starting point, make sure to use a \"Trusted Repository\" ( read more about trusted builds and make sure to audit it's Dockerfile before you use it. One of the good trusted WordPress containers out there is managed by Tutum , a Docker-based hosting company that lets you run Docker containers and provides private docker registries and private docker images. The WordPress container you want to use is called tutum/wordpress and you can use it quickly and easily by doing the following: And that is everything you need to know to set up Wordpress in a single Docker container. Just to ensure that everything is as it should be let's actually bring up WordPress. In order to do that we need to know the port that Docker has assigned to the application. Make a note of the returned port (e.g., 0.0.0.0:port number ). In the example above the returned port is 49154 and that is where WordPress is listening. Validate by pointing your browser at http://public ip:port number. This will bring up the Wordpress installation dialog and everything is good to go. This is a framework for downloading and installing containerized applications that have been designed to run everything in one container. Check out our Kubernetes Knowledge Base article. It will get you started using Ansible to create a Kubernetes cluster on CenturyLink Cloud - all by running a single script. Don't have a CenturyLink account? No problem. Just head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "build-a-hacker-news-style-voting-application-with-orchestrate", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/build-a-hacker-news-style-voting-application-with-orchestrate", "abstract": "We’re really interested in seeing the cool projects the community is building with Orchestrate. During a brainstorming session, we talked about a Hacker News style voting app, where people can submit projects and then other people can up vote them. We authenticate people through GitHub, so we can ensure they only vote once for each project. Since we eat our own dog food, we wanted to build this off Orchestrate. We can take advantage of Orchestrate’s events API to handle voting and ensure each user can only vote once per project, graph search so we can see what projects our friends voted on, and full text search so we can find projects. After talking about it, we decided we could throw it together pretty fast, and it’d be a great tutorial on building a real world app with Orchestrate. You can see the finished app here or view the source on GitHub . The tech we’ll be using First, we need to set up our basic app. We need to install Orchestrate, Express, Passport and the GitHub strategy for Passport. $ npm install orchestrate express passport passport-github --save Now we can create the basic Express app in a new index.js file. This gives us the basic app that responses with “hello world”. We have Express and Passport set up and next we’ll handle the GitHub authentication. To use GitHub for authentication we need to go to GitHub and create a new application . Once created, we can plug the keys into our code. You can put the keys directly in your code but if you plan to use source control, it’s best not to commit those. So let’s add them to a new .env file that Heroku will use. Create a new .env file and add your keys into these spots. We’ll also set the callback url. This is the url that GitHub redirects to after the login. I won’t go into detail here about how the GitHub auth works, but you can read more about it on the Passport site . This configures Passport to use the GitHub strategy and sets up our callback paths. Now we can start our server with foreman start visit http://127.0.0.1:5000/auth/github and we’ll be redirected to GitHub to authenticate. Upon success, GitHub will redirect back to our app. (If you’re not using the Heroku CLI, then you can put the keys into your code and run it simply with node index.js.) Now we get to start storing our data in Orchestrate. GitHub will return user data for the authenticated user and we want to save it. Luckily this is really easy! In your Orchestrate Dashboard click the “New Application” button. Once you’ve created the application, click the “New Collection” button. This is similar to a table in a relational DB. Name it “Users”. Now we’re ready to start adding Orchestrate into our app. We first need to add our API key into our .env file (or into the code, if that’s your thing). In the interest of organization, we’ll separate our Express routes from our app functions. We can store all our functions in a new module called functions.js Include the new module in our index.js file: Then add our authUser call into our authentication callback. When the authentication is successful, we’ll call our authUser function to save the user data. Now let’s save the user data into our User collection. We’ll create a new key for the record based on the provider (GitHub) and the username. Each piece of data needs to have a unique key. We know that the username will be unique across GitHub, but we’ll add the provider (GitHub) to it incase we want to support signing in from other services too. In the Orchestrate Dashboard, we can do a GET call on our Users collection and see that the data is saved! Now each time the user signs in, the data will be saved. But we only want to save the data the first time they sign in so we don’t write the user data for existing users each time they sign in. Luckily, that’s really easy to do. We can use Orchestrate’s conditional PUT functionality. Conditional PUT works two ways, if-match will only write data if they key and ref matches, or if-none-match will only save the data if the key doesn’t exist. That’s the one we want to use. Our conditional PUT looks at the key and if there is a record at that key already, it will skip the operation. This way, the first time they sign in, we save the data, and subsequent times we don’t. All we need to do is pass false as the 4th parameter for the PUT operation. Now we have our basic app set up. Users can sign in through GitHub and we will save the user’s data into our database. In the next post, we’ll create projects and use Graph to associate them to users. We’ll also start collecting votes using Events. You can see the finished app here or view the source on GitHub . Photo Credit: KCIvey", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tip-seamless-restarts-with-unicorn-and-upstart", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/tip-seamless-restarts-with-unicorn-and-upstart", "abstract": "While discussing some operational topics with a friend recently, one subject struck up particular interest – how do you manage to perform seamless restarts of Unicorn under Upstart ? He thought the answer was novel and recommended that I throw it up somewhere on the internet for others to see. For some background, Orchestrate uses Ubuntu on our servers, and therefore naturally fell into using Upstart for process management. Upstart is, until the next release at least, the core process manager and initd replacement for Ubuntu. One of our applications is a Ruby on Rails app that runs under Unicorn. Unicorn is a http server for rails applications that manages a pool of backend rails processes. Upstart and Unicorn work with different models for process management. Upstart starts a process, and then watches that pid to see when a given service has terminated. If the process dies, then it will automatically restart the process to ensure that it is running. Unicorn can do seamless restarts by starting a new master and then restarting each worker one by one, eventually terminating the old master. When Unicorn kills the old master, Upstart thinks the whole application is dead, so it attempts to restart.  Since it can’t listen to the port it crashes, which then prompts Upstart to try again, etc. The easiest solution to this is to use a shell process as the Upstart job, which in turn starts the dashboard job. This is not complete though, as the restart returns control back to the shell process after the first seamless reload. Typically, the solution at this point is to watch the pid file and ensure that the expected process still exists in a loop with a sleep. This is problematic as it introduces latency to Upstarts ability to detect a failure and correct it. It also introduces the need to spend CPU cycles executing a process and checking the results. A better solution is to use file locking in order to be notified when all the Unicorn processes have terminated. This is cheap, requires no polling, and adds no latency via a sleep. In our unicorn.conf.rb file we added the following block to the before_fork block: In our upstart script we attempt to obtain a file lock on the lock file. The running unicorn processes maintain a lock on the file, so until all of the unicorns have exited that call will block. This is a very cheap solution which eliminates polling in the upstart script all together.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-drone-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-drone-on-the-centurylink-cloud", "abstract": "Install a fully functioning CI platform on the CenturyLink Cloud , hook it up to GitHub, and demonstrate full CI automation through the use of Docker and Drone. Prerequisites: It is the fairly modern practice of merging developer changes into a single trunk version several times a day. Additionally, most teams using a CI methodology hook in their QA systems to allow the automation of unit and integration tests. By designing the CI pipeline to invoke QA processes prior to merging the changes to the mainline trunk, an organization can effectively have a “gated trunk” whereby all of the changes have passed tests before being merged, thereby guaranteeing that the trunk version is always runnable. Drone is a CI platform that has tight integration with GitHub (public and Enterprise) and BitBucket, with more code repos to come, I am sure. It is based on Docker and container technologies and is available through a SaaS model (at http://drone.io ) or via open source that you install yourself ( https://github.com/drone/drone ). Today we are going to install the open source Drone platform on the CenturyLink Cloud and demonstrate how easy it is to get going. To start, ensure the 3 pre-requisites (above) are in place. We will want to have 2 windows open at this time: From your terminal: And that is how easy it is to install Drone on a Docker-based CenturyLink Cloud server. From your browser navigate to http://public IP/install . Drone, by default, is listening on port 80 (of of the default ports we opened when installing the Docker server). Follow the steps to create the admin user. The second page will ask for a GitHub Client ID and secret. This has to be done from your GitHub account and is done by registering a new application. Open a second browser window, the link is here and it should look like: Enter a name for the application (eg. “Drone”). Enter http://public IP for your server under “Homepage URL”. An optional Application description can be entered. Set the “Callback URL” to http://public IP/auth/login/github . Push the button to register the application. In the upper right of the subsequent screen you will find the Client ID and Client Secret. Enter these into the original Drone install browser window. Scroll to the bottom of the page and you will find settings for email notifications. This is super helpful in keeping track of the CI process flow, so take a few minutes to enter these and then press the “Save” button. Look at the top of the page, see the “New Repository” button? Click this, and then the GitHub “Link Now” button on the next page. Press the “Authorize Application” button and the enter your GitHub name and repo. Congratulations! You have successfully installed and configured Drone and GitHub on the CenturyLink Cloud . You are ready to start adding code and feeling the power of build automation! Go Hello World Since our version of Ubuntu does not have the user ‘ubuntu’ we need to add it. This is (currently) required for Drone to execute correctly. Create a new user named 'ubuntu' by following these steps: At the prompt create a password for this user and then accept the default user information by pressing ENTER at the prompts. In order to give ubuntu the correct privileges we must add the user to the sudoers file. Add the new ubuntu user in the \"User privilege specification\" section. Once the file has been successfully edited press CTRL-x to exit the file; and Y to save the changes. The key thing to make Drone work with your GitHub repo is to add a .drone.yml file to your repo. This is the instruction manual to tell Drone what to do. We are going to build a simple Go application and our .drone.yml file looks like: Make sure when you commit this file it has the leading period. If you commit this file with no Go language files (as I just did) you will notice in your Drone browser window that a new file has been detected. Click on the commit number (eg. 1aaf85) and you can see that the commit has started. Why does it take so long (approx. 3 minutes for me) to complete, particularly since there is actually no application code to build? Since this is the first commit Drone is downloading and installing the base images (including Ubuntu) that the applications will run on. Go back to the README file, make a trivial change, and see how long the second commit takes. On my system it took less than a minute. Let’s add the Hello World application to the repo. Over at GitHub create a file called GoHello.go and add the following: Once you are done, commit the file. Quick... look at your Drone browser to see that the CI server has detected the new file and is processing it. Click on the commit number and you should see something like this: This shows a successful clone, build, test, and install CI pass! Now all you have to do is add some tests, rinse, and repeat!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tip-hubot-orchestrate-brain", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/tip-hubot-orchestrate-brain", "abstract": "Hubot is a fun (and useful) chat robot. It signs into your Slack, Hipchat, IRC channel, or Jabber chat room, and then awaits your command. Teaching Hubot new things is fairly easy by adding custom scripts. A robot tailored to your needs. What more could you want? Hubot has a brain. The brain stores various state that is useful to keep around when Hubot is restarted (Hubot’s memories basically). Custom scripts can also store data in the brain so Hubot doesn’t forget about it. This is nice because you don’t have to worry about how to persist data for your custom Hubot command, just tell Hubot to remember it. By default, Hubot will store its memories in Redis for safe-keeping (kind of like “The Ghost in the Shell”). Adding a different store is also fairly easy as it is just another custom script that responds to lifecycle events from Hubot to restore and periodically save its brain data. Orchestrate-brain is a script that allows (well, teaches) Hubot to store the brain data in Orchestrate. With the memories safely in Orchestrate, you (and your Hubot) have one less thing to worry about. If you’re building custom Hubot scripts, it can also be educational to search the brain via the Orchestrate Dashboard . Hubot is pretty fond of its memories. They’re very important, so Hubot tries to store its memories every 5 seconds or so. Most of the time, nothing has really changed, Hubot is just being overly paranoid (not unlike the ‘ctrl-s’ twitch many programmers seem to develop). The Orchestrate-brain will only send data to Orchestrate if something has changed, so it should not put a dent in your Orchestrate usage.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "the-primary-key", "author": ["\n              Originally posted on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/the-primary-key", "abstract": "The concept of a primary key in a relational database is an interesting subject because in almost all examples online, in every blog post and tutorial, you’ll see the primary key be defined as an auto-incrementing ID. This field is created and managed by the database. Its purpose is to uniquely identify the record, but the value itself has no semantic relationship associated to the rest of the data in that row. In SQL storage engines, auto-incremented IDs are a nice feature. They're created and managed for you. They’re guaranteed to be monotonically increasing. This ordering is important as it ensures the items in a table are insertion-ordered and, therefore, clustered on disk based on time of insertion. This is a useful property when data generated at the same time is generally accessed at the same time. Unfortunately, distributed storage engines (e.g., NoSQL technologies, like those that power Orchestrate) generally don’t provide auto-incrementing primary keys. This is because auto-incrementing IDs are hard in distributed systems. They require a coordination system to maintain the property of monotonicity. Further, insertion order is difficult to define in a distributed environment largely due to clock skew. (Aside: Google’s Spanner has a novel solution to this problem, something called “TrueTime”. “TrueTime” is a time API that is built with the understanding that the exact time is uncertain. When calling the now() method, a range is returned with the lower bound being the earliest time it could be and the upper being the latest it could be. This interval is the window of uncertainty. Google minimizes this range by using atomic clocks and GPS for clock synchronization.) While auto-incrementing IDs are a nifty feature, they require a level of coordination in a database that doesn't play well with the kind of distributed storage solutions being built today. It’s also worth noting that over their years of use in SQL databases, they’ve become somewhat of a crutch when designing the data model for a system. In Orchestrate, we do plan to add server-generated IDs to the service soon. You can find the UserVoice for the feature here (please upvote it if you need it), but in the meantime it’s worth noting that, in many cases, clever naming strategies for your keys can serve you much better than auto-increment IDs. It’s worth asking yourself, is the insert-ordering for this type of data necessary? A natural key is a term we sometimes use to describe a field that is the obvious and most common identifier for a piece of data. For example, if we imagine a user document of some kind it is likely to have a common set of fields like so: In the document above we have three natural keys, any of which could be used to uniquely identify that document. The “email” field, “uid” field and the “username” field. An email address is a piece of information that is guaranteed to be unique to each user because everybody has their own email address. We know, based on the purpose of the data in those fields, they are not going to be the same between user documents. They're unique to a particular user’s record. In fact, even in RDBMS’s, developers sometimes should be using a natural key as their primary key instead of an auto-incremented ID. This will lead to better performance when the natural key is the most commonly used identifier, but is often not considered out of habit. An important factor for deciding on a key is that it should be something that will never change. Your data model will likely involve a graph of relationships between objects. These relations will include the keys for the items involved (foreign keys in RDBMS). If you choose a field whose value may change for an item, then migrating those related items when the field changes may be complex and expensive. In the example above, the email or username may not be a good choice if users are allowed to change either value. A natural key is often a better fit for data than what is typically chosen as the primary key, but they are not a panacea in this problem space. What if you have a use case that benefits from particular sets of keys being grouped together? For this case you should consider a “composite key”. Like a natural key, a composite key can be built from a group of fields in a document or from other categorization information about the dataset as a whole. Composite keys are very useful when you have a use case that benefits from clustered keys and you have a database that provides ordered access. Orchestrate is an example of such a system. If we go back to our user document, we could build a composite key by taking the “created_at” field, a separator character like “_”, and prefixing them to the “username” field. From the example above this would give us the key “1394019038077_orcy”. We’re now able to take advantage of the semantics of this key format to ensure that user records are stored in the order that they were created at. Keys with similar timestamps are clustered together. We can also use composite keys to great effect as a namespacing strategy. For example, let’s take a database that supports key filtering. With it we could create a composite key format for an IoT sensor device something like this: “{deviceID} {sensorID} {timestamp}” and query over it in useful ways. We could filter to get all documents for the keys with a specific “deviceID” or filter further and request all documents with a particular “deviceID” and “sensorID”. We could even make use of the timestamp field and filter down to documents that only match a particular point in time going forwards. Orchestrate supports key filtering in our API via the KV list query . We could perform one of those queries like this: Composite keys are a powerful key format that builds on top of the concept of a natural key by encoding more information about a document into a key, making it possible to filter and query the data efficiently in interesting ways. Despite the options for avoiding auto-incremented IDs, there are use cases that will require this kind of ID generation strategy in a distributed system. As mentioned above this is a hard problem to solve, but there are a few solutions out there. Back in 2010, Twitter reached a scaling problem with their use of MySQL. They were in the process of switching databases and realized that, in the new database, they would not be able to rely on auto-incremented IDs. They'd relied on it up until that point to ensure that tweets posted around the same time had IDs that indicated as much. In their words, “We needed something that could generate tens of thousands of ids per second in a highly available manner. This naturally led us to choose an uncoordinated approach.” In pursuit of a suitable replacement for the monotonicity guarantee provided by auto-incrementing IDs, they developed a service called Snowflake . You can read more about the rationale for this service in their blog post . It's possible to generate snowflake-like keys without much effort. Snowflake keys are 64 bits. The first 41 bits represent the time in milliseconds. The next 10 bits are a machine identifier. The 12 bits a sequence ID. You can easily build a similar solution using epoch timestamps, a unique machine identifier, and a counter that rolls-over. These keys will not be as compact as snowflakes’, but will maintain the same “roughly” time ordered property. Data modeling can be difficult. Choosing the right keys can be a challenging process. If you’re experimenting with Orchestrate or would like help with the design of your data model, you can get in touch with us very easily through the Dashboard . We’re happy to help in any way we can. We also have a Community channel where you can find us throughout the day or by email at hello@orchestrate.io . We’re happy to answer questions, look at code, and point you to resources. Most importantly, have fun building great things with Orchestrate . By the way, we're live now in the CenturyLink Cloud . Sign up today for Orchestrate at orchestrate.io .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "openstack-solum-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/openstack-solum-on-the-centurylink-cloud", "abstract": "Solum is an OpenStack related project that is designed to provide an application lifecycle framework over the top of the IaaS cloud. In this context Solum is \"related\" to OpenStack in that it is not a \"core\" or \"incubated\" project. There are many ecosystem projects that leverage and extend OpenStack, at this point Solum is one of these. From the description at GitHub , Solum is \"natively designed for OpenStack clouds and leverages numerous OpenStack projects, including Heat, Keystone, Nova, Trove, and more. We value vendor neutrality, open design and collaboration, and leveraging existing solutions where possible. One example is our use of Docker for deployment of containers. Multiple language run-time environments will be supported with a modular \"language pack\" solution so you can easily run applications written in any language of your choice.\" When the project was introduced in the Havana development cycle there was a fair bit of controversy; everything from \"this is not within the OpenStack IaaS charter\" to \"why not just embrace Cloud Foundry (another open source project) for application orchestration and deployment?\". There was lots of discussion at the IceHouse Design Summit, and the key argument for the project was that it utilizes and makes best use of the native OpenStack technologies. It's sponsors are clear that Solum is just another way (not the only way) to deploy and manage your applications on OpenStack. So let's stand up Solum on the CenturyLink Cloud! If you looked at the \"OpenStack on the CenturyLink Cloud\" tutorial you will find this is very similar. Actually, the Solum setup is a modified and customized DevStack, with most of the heavy lifting being done under the covers and automatically. OpenStack is one of the fastest growing open source projects being worked on today. As a cloud IaaS solution it has many aspects and moving parts, which can make even getting a demo version installed a difficult task. It is important to realize that in order to run OpenStack Compute (Nova) effectively we need to give it resources... Nova is a cloud controller that will allow us to create and run multiple VM's so our CenturyLink Cloud Server needs to be pretty large. Here is the CenturyLink Cloud Server that I set up to run OpenStack + Docker + Solum: In addition to Nova we will be installing the following OpenStack services (click for larger view): A key element of both the build and runtime of Solum is Docker and associated container technologies. In order to install Solum we first need to install Docker on our server. The version of Ubuntu that we installed while creating the server is 12.04 (Precise LTS 64-bit). This particular version of Ubuntu ships with a version 3.2 Linux kernel. In order to get the correct version of LXC that Docker requires we need to update the kernel from 3.2 to version 3.8. The Ubuntu folks at Canonical have made this easy for us by providing backported versions of new kernels. We will be updating Ubuntu Precise to Ubuntu Raring, at the same time will be getting built-in kernel support for AUFS. Once you have the base operating system updated and ready to go it is a pretty straight-forward process to install Docker. [preformatted-text]# Add the Docker repository key to the local keychain $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9 # Add the Docker repository to the list of apt sources $ sudo sh -c \"echo deb http://get.docker.io/ubuntu docker main > /etc/apt/sources.list.d/docker.list\" # Update the apt files $ sudo apt-get update # Install Docker $ sudo apt-get install lxc-docker [/preformatted-text] And that is all that is necessary to install Docker and we move on to installing Solum. First clone the Solum code repo: And, run the Solum install script: This is an extensive install script (as you can imagine) and you will see a bunch of stuff scrolling by on the screen. It is OK to let the system generate all the default passwords. The Horizon Dashboard is located at the public IP for your CenturyLink Cloud Server. Point your browser to this IP (in my case this is http://64.15.184.220 ) and you will be prompted to login to your newly installed and running OpenStack Cloud with Solum. Use 'admin' as the user and 'password' as the password and you are now able to fully administer and play with your OpenStack Solum project! This is just the start of developing and demoing Solum on OpenStack on the CenturyLink Cloud. In future tutorials we will demonstrate how to describe and run Solum as a full featured, native PaaS for OpenStack.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "openstack-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/openstack-on-the-centurylink-cloud", "abstract": "OpenStack is one of the fastest growing open source projects being worked on today. As a cloud IaaS solution it has many aspects and moving parts, which can make even getting a demo version installed a difficult task. Today we are going to stand up a complete OpenStack cloud on top of the CenturyLink Cloud and allow you to play with the Web UI, the exposed services API's, and whatever depth you would like to go into looking at the inner workings of the system. Lucky for us there is a shortcut to standing the demo version up... We will be using the single VM install that the actual contributors to the project use to develop new code and patches. This project is called \" DevStack \" and it allows us to deploy and run the base OpenStack components within a single CenturyLink Cloud virtual machine. It is important to realize that in order to run OpenStack Compute (Nova) effectively we need to give it resources... Nova is a cloud controller that will allow us to create and run multiple VM's so our CenturyLink Cloud Server needs to be pretty large. I have created a server with 4 CPU's, 48 GB of RAM, and the default amount of storage, as seen here: The base OS is Ubuntu 12.04 64 bit. A key advantage of installing this base OpenStack image is that we can snapshot it and re-use it as we move to add more services, multiple servers, etc. After creating the server and adding a public IP you should create the non-root user that will be the primary demo user. You might think that just running as root would be simpler, but DevStack is smart enough to detect that you are trying to run in this \"dangerous\" manner and refuse to start! It is generally not a good idea to run as root, so in order to update our Ubuntu system and install DevStack we will first set up a new user. After giving the new user the appropriate permissions we will logout of root and login as our newly created user. Open your terminal/SSH program and login to your server. Since we are logging in as root the command will be 'ssh root@Public IP'. From the above screen shot you can see my public IP is 64.15.184.33 and so my command will be 'ssh root@64.18.184.33 '. The root password is the server admin password you set while creating the server. Create a new user by following these steps: At the prompt create a password (use 'stack' for convenience) for this user and then accept the default user information by pressing ENTER at the prompts. In order to give stack the correct privileges we must add the user to the sudoers file. Add new user in the ‘User privilege specification’ section. Once the file has been successfully edited press CTRL-x to exit the file; and _Y_ to save the changes. Logout as the root user and then login as stack . We now have a normal user with sufficient privileges to install OpenStack. We now need to install 'git': DevStack is a shell script that is intended to offer a single system, running OpenStack cloud. Primarily developed and maintained so that folks can develop new code and patches for the open source project it is also a great way to install a OpenStack demonstration system. The DevStack script will install everything needed to run a full OpenStack Cloud, no need to piece-meal anything else. Since the script points to the latest builds there is another advantage that you will get the latest bits by default. Look here for more details on DevStack. First clone the DevStack code repo: And, run the DevStack install script: This is an extensive install script (as you can imagine) and you will see a bunch of stuff scrolling by on the screen. Eventually you will get prompted to enter passwords for the default services that are being installed. It is OK to let the system generate these as you will not be interacting with the services directly at this point. However, when it asks for the Horizon/Keystone password enter 'stack', as this will be your login to the OpenStack Dashboard. The Horizon Dashboard is located at the public IP for your CenturyLink Cloud Server. Point your browser to this IP (in my case this is http://64.15.184.33 ) and you will be prompted to login to your newly installed and running OpenStack Cloud. Use 'admin' as the user and 'stack' as the password and you are now able to fully administer and play with your OpenStack Cloud! Be sure to take a snapshot of this server as we will use this in future tutorials to expand the services mix and show you various things you can do with your new OpenStack Cloud. For more information on OpenStack look here .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "jdbc-importer", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/jdbc-importer", "abstract": "We know getting existing data into Orchestrate is one of the barriers to trying us out. As such, we are pleased to announce the first in a suite of tools to help users migrate existing data, the Orchestrate JDBC Importer . The Importer was initially forked from Algolia ’s JDBC Java Connector , but has since seen major changes and improvements. The JDBC Importer allows users to transfer data from existing JDBC compatible databases. Out of the box it supports MySQL and PostgreSQL. Other databases will be supported in the future, but given that Oracle DB’s and Microsoft SQL Server’s JDBC drivers are under restrictive licensing they will be supported under a bring your own driver model. To get started with the JDBC Importer, download the latest version from the releases page on Github . You will need Java 7 or later and uuencode/uudecode installed on your system. To run the importer, execute orchestrate-jdbc-importer passing in a JSON configuration file like so: Once started, the JDBC Importer will iterate over the result set produced by the “selectQuery” and convert each row into JSON. It will then PUT the data into Orchestrate with the provided “collection” and the value of the row’s “primaryField”. The importer also has the ability to run periodically and either re-execute the “selectQuery” or use an “updateQuery” and only import the data that was been added or changed since the last run. With the importer, users of RDBMS can easily migrate to and synchronize data with Orchestrate. Before, migrating data would require writing a one-off ETL tool. Now, developers can simply point us at their existing database, tell us how to traverse it, and we do the rest. The importer is MIT Licensed . Contributions and issue reporters are welcomed on Github . P.S. Be on the lookout for a similar solution for the NoSQL world.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "resolving-conflicts-in-orchestrate-with-hextor", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/resolving-conflicts-in-orchestrate-with-hextor", "abstract": "Databases don’t always know what you want. As engineers, we do our best to build resilient, scalable systems that address your needs with intuitive interfaces, but sometimes, we don’t know how to handle what we’re given. The system does its best, but leaves behind a conflict so you can clarify how you want to handle this situation. Many databases, including Orchestrate and CouchDB, generate conflictss when they don’t know how to handle something. Conflicts contain all the information you as a developer would need to resolve them how you see fit, such as what happened, and what documents were affected. Then you can fix them up to your heart’s content. So, in Orchestrate, how do conflicts happen? Say you’re writing an application that saves names, like this: But one of your developers hasn’t dismantled the falsehoods programmers believe about names , so they insert a document like this: But when they try to insert it, they get a 409 -- indexing_conflict . The document is saved, but it doesn’t show up in searches. What gives? N.B. : This post gets into the nitty-gritty of Orchestrate as a piece of technology. We recognize this implementation falls short of our goal of effortlessness , and we’re working on it. In the meantime, we want to make clear how that implementation affects you, and how you can keep it painless for you. In Orchestrate, conflicts occur when the database doesn’t know how to handle something. Specifically, it doesn’t know how you want a new document to be searched. Orchestrate does its best, but leaves behind a conflict that you can resolve at your leisure. Whenever you insert a document, we analyze its contents to determine how to index it for searches. Based on the type of each field in the document, we create a mapping. When you insert more documents, we refer to this mapping for how to index fields, or update it when we encounter fields we haven’t seen yet in your data. But when a document has data in a known field of an unexpected type, Orchestrate doesn’t know how to index it. Because it’s of a different type, we can’t index it along previous values, but the document is technically valid, so we don’t want to just reject it. So, we create a conflict detailing the difference between what we expected and what we saw, so you can resolve it as you please.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-coreos", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-coreos", "abstract": "CoreOS is an interesting new Linux distribution that is taking a new approach to building out Internet scale infrastructure. From their website: CoreOS is a new Linux distribution that has been re-architected to provide features needed to run modern infrastructure stacks. The strategies and architectures that influence CoreOS allow companies like Google, Facebook and Twitter to run their services at scale with high resilience. We've implemented them correctly so you don't have to endure the slow, learn-as-you-go infrastructure building process. CoreOS can run on your existing hardware or on most cloud providers. Clustering works across platforms, making it easy to migrate parts of your gear over to CoreOS, or to switch cloud providers while running CoreOS. Specifically, CoreOS features 3 characteristics that allow for manageable, yet large, scale-out systems. This tutorial is going to walk through the steps to install a CoreOS cluster and provide the facilities to play with the distribution. Currently (as of this writing) the CenturyLink Cloud does not allow custom images to be uploaded and deployed. We are going to make use of the facts that a) OpenStack can run on the CenturyLink Cloud, and b) OpenStack provides the facilities to upload and run custom images. While obviously not a system you would put into production it allows CoreOS to be bootstrapped on the CenturyLink Cloud for exploration purposes. To start with we need a running copy of OpenStack. Directions on how to set this up are here . We are going to create a new project within OpenStack to encapsulate and identify the CoreOS demo. After logging into the OpenStack Horizon dashboard, choose _Identity_ and then _Projects_ . In the upper right of the screen select _Create Project_ and give it the name CoreOS Demo . After hitting the confirm you should see a screen that looks like: Next, we are going to create a unique user within OpenStack that is associated with this project. From the Horizon dashboard select _Identity_ and then _Users_ . Again, in the upper right of the screen you will find the _Create User_ button. Add the new user CoreOSuser , give it an email address, and then enter for the password CoreOSuser (as an example). On the _Primary Project_ selector choose the CoreOS Demo project that we just created. The _Role_ should be assigned to admin . After hitting confirm you should have a screen that looks like: OK, we have now set up OpenStack in preparation to install and run CoreOS. Take a small coffee break and we will get onto the real fun! Move back to the SSH shell you have open. We will be interacting with the OpenStack CLI and in order to do this we need to set our credentials in the environment. Enter the following from the Linux shell: Lookup the _Project ID_ for the _CoreOS Demo_ project and enter it as the OS_TENANT_ID. You will find this on the Project page (as shown above). You need to make sure you enter the specific details for your setup (every installation will have unique Project identifiers). Lastly, we need to tell OpenStack where to find the Keystone authorization service. From the Horizon dashboard go to _Projects_ , then _Access & Security_ , and select the _API Access_ . Your screen should look like: Lookup the _Service Endpoint_ for the _Identity_ service and enter it as the OS_AUTH_URL. You need to make sure you enter the specific details for your setup (every installation will have unique service endpoints). OpenStack is now set up to host CoreOS as a supported image. The general process will be to download the CoreOS image, import it into the OpenStack Glance image registry, and then launch instances based on this image... Pretty straigh-forward, right? To download the CoreOS image enter the following from the Linux shell: And install the image into Glance: On a successful completion of the import you will see a confirmation screen that looks similar to: Note the _id_ that was assigned to the image, this is what you will pass into Nova to launch instances of CoreOS. Finally, let's go ahead and launch 3 instances of CoreOS on our OpenStack over CenturyLink Cloud system. Again from the Linux command line enter: Nova will give you back a confirmation that looks like: And finally, do a Nova list command to see our 3 CoreOS instances up and running, attached to a private network. Now that you have a running CoreOS cluster you can start to experiment with all of the cool aspects of the system.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "curl-vs-httpie-http-apis", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/curl-vs-httpie-http-apis", "abstract": "When I want to play around with an API, I usually toss a couple example calls from the command line as an initial test. The go-to application for command line HTTP calls has long been cURL. However, it's usage can be complex and arcane. I often find myself forgetting command line options, retreating to the man page and defeating the purpose of making a quick call. As handy as cURL is, I was pleased to discover HTTPie , a cURL replacement that is particularly well-suited to JSON-based REST APIs. Below I’ll use a few example calls to CenturyLink Runner to show off some of HTTPie’s features. To use any Runner commands, you need to first get a bearer token. Here's how you would do that via the command line in cURL and HTTPie. Note: In all of these commands, replace \"YOUR.USERNAME\" with your CenturyLink Cloud username, and \"YOUR.PASSWORD\" with your password. First, in cURL: Then, in HTTPie: The first thing you’ll notice is that the HTTPie version is shorter. JSON APIs are common, so HTTPie assumes that’s what’s coming. In fact, HTTPie makes a lot of assumptions that make it a good tool for a specific job, whereas cURL is much more general. In addition to automatically setting the Content-type, HTTPie automatically converts key=value items into JSON, which means the end of escaping quotes. You just need to make sure you use = for string values and := for non-strings. Note: Record the accountAlias and bearerToken results from one of your shell commands. We will be using it in the next section. Note: In the URLs below, replace XXX with the three-letter account alias for your account. Replace the \"BEARER.TOKEN\" with the bearer token you recorded in the last section. Also, these commands are addressing the \"WA1\" datacenter. If you want to use a different datacenter, use its code in the URL. cURL: HTTPie: Retrieving datacenter information looks essentially the same. HTTPie doesn’t require a flag before the request method. In this example, neither need a request method at all because cURL and HTTPie both assume GET. The biggest difference you’ll notice is in the response, which is automatically color-coded and JSON is formatted. These defaults make HTTPie very friendly to my tired developer eyes. cURL: HTTPie This command creates a new virtual server group. You will need to look up parentgroupid by navigating to a server group in the CenturyLink Cloud Control Portal . For example, if the group control portal page URL is \" https://control.ctl.io/manage#/wa1/group/f0f49dc6d0024bf68a0807e2b0fe32ea\" , the group ID is \"f0f49dc6d0024bf68a0807e2b0fe32ea\". Making API calls from the command line often requires a bunch of copy and pasting, most notably of the bearer token. Since the CenturyLink Cloud API uses basic headers to pass authorization, we can eliminate the need for the key after the first call with the session flag: If you include a session ID along with your API key authentication, then subsequent calls can just include the same session ID. HTTPie also has a feature that allows you to set your own defaults. Just edit your config.json to always pass the session ID (or any other flags you want set with every call): You can edit the default options in ~/.httpie/config.json (or under your Application Data folder in Windows). Try out HTTPie on your favorite new API. HTTPie is written in Python, so if you have the pip package manager, it’s as easy as: If you haven’t already, create a CenturyLink Cloud account and try some calls with HTTPie.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "continuous-integration-with-node-js-pt-1-travis", "author": ["\n              \n            "], "link": "https://www.ctl.io/developers/blog/post/continuous-integration-with-node-js-pt-1-travis", "abstract": "How do you prove your tech works? You can write a test suite, but how do you demonstrate that it works to others? Enter Travis , a continuous integration service. Travis uses Github webhooks to run your project’s test suite whenever you push new changes, thus demonstrating the health of a project’s code and answering the fundamental question, “Does this thing even work?” Let’s see how! This is the first post in a series in which I’ll demonstrate how to use continuous integration services like Travis to communicate code correctness to your users and automate tasks like deployment and status notifications. In this first post, we’ll cover how to get started with Travis on a Node.js project. Travis watches your project’s repository for new commits. Whenever it sees one, it runs the test suite, along with any other instructions you might give it, on a machine provided by the community (thus eliminating any “well, it works on my computer” problems early). If the test suite passes, hooray! If it doesn’t, you’ll get an email letting you know something went wrong. The easiest way to get started with Travis is with its command-line interface. To get it, you’ll need Ruby installed. After, use gem to install the command-line interface, like this: Great, now we need a project to integrate. If you have a project hosted on GitHub , just navigate to that project’s folder in terminal, and run travis init, like this: This does two things: The .travis.yml file instructs Travis how to run your test suite, what to do before and after, what versions of your language to use, and other options. By default, a Node.js project will have a .travis.yml file like this: This informs Travis that this is a Node.js project and that Travis should test it with Node.js versions 0.11 and 0.10. But wait, where do we tell Travis how to run our test suite? If you run your test suite with npm test, you already have. Travis uses sane defaults to minimize the effort of integrating, if you’re already following community best practices. For more details, check out the docs . Now, whenever you push new commits to your project, Travis will execute NPM test using Node.js version 0.11 and 0.10. If either version fails the test suite, the build is considered failing. If both pass, hooray! Your test suite needs access to an API key to work with the database. How do you give Travis your API key, securely? Use travis encrypt! Check it out: This will encrypt your API key and add it to your .travis.yml, so to anyone except Travis, it looks like gibberish: Now, whenever Travis runs your test suite, the environment variable DATABASE OF CHOICE_API_KEY will contain your API key. Use this pattern to pass your test suite the credentials it needs without putting them straight in your code. For visibility’s sake, Travis exposes a status image that publicly states how the latest build performed. To get your status badge, go to your project’s Travis page, and click on the badge there. It will present you with a dialogue to generate the code for your badge. For example, this badge comes from Porc : Put that badge in your project’s readme, and now viewers can know at a glance that your project works. Or can they? Next time we’ll discuss using Coveralls , a code coverage service, to demonstrate just how robust your project is.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "server-generated-keys-unique-ids-for-distributed-databases", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/server-generated-keys-unique-ids-for-distributed-databases", "abstract": "Long before I had ever met a “distributed database,” I already had a great relationship with my relational database. I loved it for its BTree indexes, and I admired it for its UNIONs and JOINs. But the feature I used most often was probably the humble auto-incrementing primary key: a quick, simple way to guarantee unique keys for any table. These days, with most of my work now using distributed databases, I miss having those automatically generated, guaranteed-unique, ascending keys. If you’ve worked with distributed databases, you’ve probably scratched your head more than once trying to create system-wide unique primary keys for all your objects. Are natural keys better than synthetic keys? And how do you guarantee uniqueness without locking up the whole cluster? UUIDs are tempting, but they can be so bulky . Can we get the same benefit of uniqueness in half the size? In situations like this, some developers may end up using “probably-unique” randomized keys. For example, a sixteen-character randomized string using the characters A – Z and 0 – 9 would have 3616 (7.95 x 1024) possible unique values. The chances of a collision are pretty slim for any individual object, but what if the system needs to start generating billions of objects with these kinds of keys? How long before the inevitable collision? And how would you even know that a collision had occurred? It’s easy to make collisions rare. It’s very hard to make them impossible. That’s why we’re introducing Server Generated Keys as a new feature of the Orchestrate API. Now you can submit new objects into your collections, and Orchestrate will generate a 64-bit key that’s guaranteed to be unique across our entire cluster. Not probably unique . Guaranteed. To use this new functionality, execute an HTTP POST request directly against your collection URL: The server will respond with a Location header, containing both the newly generated ID and the canonical ref for the newly inserted object: In this example, the value 036ea872f9011a7c is the newly generated ID, and fab82eac8414ded3 is the canonical ref for this particular version of the object. Of course, If we update the object later, the updated object will keep the same ID but get a new ref value. By contrast, when you don’t need the server to generate a key, you can continue to use the existing REST endpoint. Just execute an HTTP PUT request, with your key already included in the URL path: Using the new API should be easy and convenient to use on any of your Orchestrate collections. For those of you interested in what’s going on behind the scenes, here’s how it works: Each of our API servers has its own unique ID internally. When you request a new Server Generated Key, the server combines its own ID with a millisecond-granularity timestamp and a sequence number. Since no two machines have the same ID, it’s impossible to generate duplicate IDs anywhere in the cluster, even during the same millisecond. The best part about these IDs is that they have a natural sort-order according to their underlying timestamp. For example, here are a few IDs generated this morning (during the same millisecond), from two different servers: In this 64-bit structure, the first 40 bits (the first 10 hex characters) are used for the timestamp, the next 12 bits (3 chars) are used for the machine ID, and the final 12 bits (3 chars) are used for the sequence number. (By the way, if this sounds a lot like Twitter Snowflake , that’s because we took a lot of inspiration from their work when we designed and built our solution.) We use a different epoch than the standard library (our timestamps begin in 2014 instead of 1970 because we need the extra bits for machine IDs and sequence numbers), so you probably shouldn’t try to convert these values back into standard timestamps. But you can rely on the timestamps to create approximate lexicographical ordering, according to their insertion chronology. Any individual API server can guarantee correct ordering down to a single millisecond (with approximate ordering at the sub-millisecond level). But since each API call might be load-balanced to a different server in our cluster, and since the hardware clock on those individual servers can drift, relative to one another, by a few milliseconds in either direction, it’s important not to rely on strict ordering of these ID values. So don’t use the ordering of these IDs as a critical component of your new high-frequency-trading app. But their approximate chronological ordering makes them great for blog posts or chat messages or friend requests or game events, or a zillion other useful things. We hope you love the new functionality and that it helps grease the wheels as you develop your next breakout application! And, as always, let us know what you think by dropping us a line at UserVoice . This feature grew directly from user feedback, and your feedback will continue to drive our development priorities.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "upgrading-to-ubuntu-14-04-trusty-tahr", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/upgrading-to-ubuntu-14-04-trusty-tahr", "abstract": "One of the primary Linux server distributions in use today in the cloud is the Ubuntu distro from Canonical . Ubuntu is a Debian-based distro and has a regular cadence of releases every 6 months. If you haven't cracked the numbering code yet, each release is indicated by the year (e.g. \"12\" for 2012) and then the month (e.g. \"04\" for April). In addition, each release has a whimsical name associated with it (the 12.04 release is \"Precise Pangolin\") with each subsequent release incremented alphabetically. In conversation you may hear someone refer to the \"Precise\" release, now you know that it was released in April, 2012. Every 2 years Canonical signifies a release as a \"Long Term Supported\" release, or \"LTS\". A normal release has a 9 month support window while an LTS release is supported for 5 years. This allows folks who desire a stable platform that is not moving every 6 months to stick on an LTS release, and then have time to migrate to the next LTS release. Two years ago the 12.04 Precise release was designated an LTS release. Recently Canonical released 14.04 (Trusty Tahr, or simply \"Trusty\") as the most recent LTS. We are going to demonstrate how to upgrade the latest Ubuntu image on the CenturyLink Cloud , which is the 12.04 release, to the latest 14.04 release. By the way, the interim Ubuntu releases have been: And the upcoming release will be 14.10 - Utopic Unicorn. We start with creating a CenturyLink Cloud server. Select the \"Ubuntu 12 64-bit\" image, 2 CPU's, 4GB of memory and build the server. Once this is complete add a public IP, enabling ports for PING, HTTP, HTTPS, and SSH. Your completed server should look like: From your terminal shell (ssh) into your new server, logging in as root. First thing is to check the version of Linux and Ubuntu that we are currently running: This will return a Linux version string that looks like: Linux 3.2.0-39-generic x86_64 And you should see a return that indicates that this is, indeed, the Precise release. No LSB modules are available.\nDistributor ID: Ubuntu\nDescription: Ubuntu 12.04.4 LTS\nRelease: 12.04\nCodename: precise First let's ensure that the apt package list is up to date and that we have the latest version of the update manager. Now comes the fun part. Since the Trusty release is so new it doesn't show up in the update manager list yet. Canonical waits until the first point release (14.04.1 in this case) to include it as an update target. While this is expected later this summer we can force the system to give us a Trusty release by indicating that we want the \"devel\" or latest \"unstable\" version. You shouldn't be scared of the \"unstable\" terminology, this is used to indicate code that is incremental to the last stable build but not yet certified as a release. In order to start the upgrade process (note that the \"-d\" parameter indicates the \"devel\" code): Answer 'y' to the intermediate questions until the following screen appears: There will be a series of screens with selections to follow: The system restart will terminate your ssh session, so wait a few minutes and then log back in as root. Now let's check the version of Linux and Ubuntu that we are currently running: This should now return a Linux version string that looks like: Linux 3.13.0-30-generic x86_64 And you should see a return that indicates that this is, indeed, the Trusty release. No LSB modules are available.\nDistributor ID: Ubuntu\nDescription: Ubuntu 14.04 LTS\nRelease: 14.04\nCodename: trusty Congratulations, you are now running the latest and greatest version of the Ubuntu server on your CenturyLink Cloud server!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-ruby-2-1-2-on-rails-4-1-2-on-ubuntu-trusty-14-04", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-ruby-2-1-2-on-rails-4-1-2-on-ubuntu-trusty-14-04", "abstract": "One of the most popular development languages and application runtimes is Ruby on Rails . From Wikipedia: Ruby on Rails, often simply referred to as Rails, is an open source web application framework which runs via the Ruby programming language. It is a full-stack framework: it allows creating pages and applications that gather information from the web server, talk to or query the database, and render templates out of the box. As a result, Rails features a routing system that is independent of the web server. Ruby on Rails emphasizes the use of well-known software engineering patterns and principles, such as active record pattern, convention over configuration (CoC), don't repeat yourself (DRY), and model–view–controller (MVC). It is estimated that over 600,000 web sites are running Ruby on Rails. We are going to install the latest version of the Ruby language (version 2.1.2) on the latest version of Rails (version 4.1.2) on top of Ubuntu Trusty (version 14.04). With this setup you will be one of the cool kids on the cutting edge of development technology! We need to get the CenturyLink Cloud server and OS created and set up. Follow the directions here to do so. Note: In addition to the standard ports (when adding the public IP address) you should specify port 3000 as well. This is the Rails default port. Reboot your server and ensure that you have a terminal open and are logged into your server as root. While you can do initial development with the built-in SQLite database you will not want to go to production, or even serious testing, without a more industrial strength database. The standard choices are either MySQL or PostgreSQL, today we are going to install and use PostgreSQL . First, let's update our apt information and then get the packages we need: When PostgreSQL was installed it set up a standard default user postgres . For ease of use we are going to remove the database password from this account. What the above commands do is: In order to ensure we can connect locally to the database we need to alter the _pg_hba.conf_ file. Currently this is located in the /etc/postgresql/9.3/main/ directory. Find the following section and update \"peer\" and \"md5\" to \"trust\". Before: After: Lastly, we need to ensure that PostgreSQL is listening correctly for localhost connections. To do this we need to edit the file postgresql.conf , located in the same directory as the pg_hba.conf file. Find the line _listen_addresses_ and ensure it looks like: _listen_addresses = 'localhost' # what IP address(es) to listen on;_ (you should be able to just uncomment this line). Restart the database: The PostgreSQL database should now be running and you should be able to connect and interact with it locally (just as our Rails applications will). To test enter the following: The first command will show you all the tables currently defined. The second command shows the currently defined users on the system. Remember to enter \"q\" to exit. Now that we have the operating system and database set up, let's complete the exercise by installing the Ruby language and Rails runtime. We will not be doing this as root so first let's create a login (user and password). When prompted add centurylinkdev as the password on the account, then take the rest of the defaults. In order to give centurylinkdev the correct privileges we must add the user to the sudoers file. Add new user in the ‘User privilege specification’ section. Once the file has been successfully edited press CTRL-x to exit the file; and _Y_ to save the changes. Logout as the root user and then login as centurylinkdev . We now have a normal user with sufficient privileges to install and run Ruby on Rails. To start with, we are going to make sure the apt repos are up to date and then install some dependencies. There are a few ways to install Ruby, we are going to use rbenv as it one of the most popular install systems. Of course, first we have to install rbenv and then we will install Ruby . Note the version of Ruby installed on the system. Also, let's not have all the Rubygems packages documentation copied locally: Before installing the Rails package we are going to install the NodeJS runtime. This lets us use Coffeescript and the Asset Pipeline in Rails, this combines and minifies the javascript to provide a faster production environment. The initial command is to fetch the software-properties-common package that contains the add-apt-repository command. And, finally, we get to install Rails: Run the following command to ensure that Rails is properly installed and at the correct version: We are very close to being done...We have to set up a PostgreSQL user, hook up the database, and then create our initial Ruby on Rails application. Create a PostgreSQL user that has permissions to create databases, or expediency sakes name it the same as the current login user (centurylinkdev). If you decide to create a different PostgreSQL user you will need to update config/database.yml to add the new user to the credentials for the databases you create. And the grand finale... Let's create the initial Rails application: At this point you should logout and re-log into the centurylinkdev account. Run the following: And, viola! You have a Rails application server running at PUBLIC_IP:3000. Point a browser here and you should see: And if you hit the \"About...\" link you will see that Ruby is running version 2.1.2 and Rails is running version 4.1.2. Now that you have the Rails framework up and running, with all components at the newest levels, you are ready to start writing the next big application in Ruby on the CenturyLink Cloud !", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "continuous-integration-with-node-js-pt-2-coveralls", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/continuous-integration-with-node-js-pt-2-coveralls", "abstract": "In our first post of this series, we walked through using Travis to automate project testing . This time, I’ll show you how to use code coverage to assert code quality, and how to use Coveralls to do so as part of continuous integration. Let’s say your project has a test suite. Great! …how do you know it’s running your code? Code coverage ! Code coverage tools show which lines of your project’s code are being run by your test suite. This identifies logic branches that aren’t being tested, and places where your code follows unexpected branches, even if it returns the values your tests expect. And if you’re using coveralls.io , you’ll get a sweet badge to show off your code’s quality :D So, how do we make it happen? Most languages have tools for code coverage, but because we’re using Node.js, we’ll use jscoverage , which will modify our code so we can see which lines ran. To get jscoverage, use NPM: jscoverage will modify our code in pretty wild ways. For example, this: … turns into this: Gross! That mess of code instruments our project so that every line has an “I ran!” statement right next to it. This lets us know which lines ran during tests and which lines didn’t. Luckily, the interface for the instrumented code is exactly the same as the uninstrumented version, but because jscoverage alters our code, we’ll want to place its output in a different folder before our tests try to require it: That instruments all the code in the lib folder, and puts the instrumented versions into lib-cov. The cov is short for coverage. Now our tests can require the instrumented code just the same as before: Now we can get a coverage report — that is, a report on how well our tests cover our code — but how we do so will depend on what we’re using to run our tests. Many test runners, like Mocha or Istanbul , have built-in support for reporting coverage stats, so see if yours has documentation on working with jscoverage. For this series, we’ll be using Mocha, which generates reports like this: Which generates an HTML page displaying your coverage results. Hooray! Coverage! But, how do we tell The Internet? Coveralls.io is a service that saves your coverage reports to show coverage changes over time and to give you a sweet badge like this one: To start using Coveralls, sign up ! Then, in your project, install node-coveralls like this: If you’re using Mocha like I am, you’ll also need the mocha-lcov-reporter package to properly format your coverage report: Then, in your .travis.yml file from [Part One], add an after_success step like this: Wahoo! You’re all set. Now, the next time you commit code to your project, Travis will run your tests, and Coveralls will pick up the coverage report. To get the badge, sign in to Coveralls, go to your project’s page, and click “Get Badge URLs”. That will give you badges in Markdown, RST, HTML, and other formats. Choose as you please, and you’re done. Next up, we’ll talk about deploying our project to production based on test success and coverage reports. Happy coding!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "developer-tools-of-interest-runscopes-traffic-inspector-part-1", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/developer-tools-of-interest-runscopes-traffic-inspector-part-1", "abstract": "Runscope was founded in January of 2013 by John Sheehan and Frank Stratton. Since then, the company has raised $7.1 million dollars in financing. The mission of the company is to build tools to support application development lifecycle. One goal of Runscope is to make building with web-based APIs as easy as building with your favorite framework. At the time of this writing, Runscope offers three tools – Traffic Inspector, Radar, and Passageway. Radar is a continuous integration and functional testing service for APIs. Passageway is a proxy hosted by Runscope that makes it possible to share a development web server running local, accessible to people that are not on the local network. In this article, we will take a look at Runscope’s Traffic Inspector. Traffic Inspector is a proxy hosted by Runscope that records the lifecycle of requests. The service allows for a convenient way to inspect the headers and body of a request and a response. In order to proxy a request through the Traffic Inspector, Runscope provides a deterministic URL based on the API URL of your choice. In this case, we are using the Orchestrate API as the endpoint, but we will make requests directly to the Runscope URL. The first request we will inspect is a simple GET requests via a cURL-like tool called HTTPie. Through HTTPie we can see that the exact same response, but with a few extra headers. Namely, the CORs headers and the Runscope-Message-Id header. All requests are “bucketed” in Runscope, which is a logical partitioning of requests by project, company, bug, etc – ultimately whatever you decide. Buckets can be further partitioned into Collections, which enable developers to logically group requests however they see fit. There are two other main views – the Summary view and the Dashboard Stream view. The Dashboard Stream view shows all requests, while the Summary view groups requests by the method and URI and provides request counts. Additionally, at the top level view of a bucket, you also have the URL Helper (shown earlier) which helps construct the Runscope proxy URL, the Bucket settings which have administrative and team sharing functions, and finally the Request Editor which is shown below. In a given request, we now have the ability to “View Request” and “View Response.” Runscope also gives us the ability to notate the request as we debug and inspect. Furthermore, for a given recorded request, the Traffic Inspector tools provides a URL which replays the exact response from the upstream server. One of the features of the Runscope Traffic Inspector that I found particularly interesting, was the ability to compare requests. To illustrate this feature in Runscope, I will use the Conditional PUT ) feature in Orchestrate. In the case of Orchestrate, a Conditional PUT helps if two actors read an object from Orchestrate and mutate the data object in different ways. When these two actors attempt to write the object back, one of the write requests will fail, ensuring you don’t lose data. For example, imagine two users editing a wiki page at the exact same time. You do not want the first write to be overwritten by the second write. The example below illustrates two concurrent requests that happen in parallel. To do that, we will use, the concurrent.futures module from the python standard lib ~>3.2. Additionally the below code requires the requests module . The first request we want to succeed, but the second we want to fail. To do that, an HTTP Conditional PUT will be used, with the If-Match header, set to the value of the Orchestrate object reference. The Orchestrate object reference is a content hash of the object that represents the canonical location of the object. The If-Match header tells Orchestrate “If the content I’m about to overwrite matches this content hash (aka “the ref”), then proceed, otherwise please fail with an error.” The error we are want is an HTTP 412 PRECONDITION FAILED . After running the example a few times in the shell, you will notice a random return of results and a random order of failures. And now the result in Runscope of comparing the requests, gives us a nice Github-esk diff style, first the requests: …and then the responses: In summary, the Runscope Traffic Inspector tool is deceptively simple, but a powerful debugging tool for HTTP APIs. The layout is clean and minimal which makes it a pleasure to look at and use; however, the tools like the Response Playback, Comparison, and Search make it far superior to any locally running proxy you may use for testing. It’s worth noting that in order to proxy requests via the Traffic Inspector, you do have to provide your security credentials. With Orchestrate, I have created a temporary API key, which I have since revoked. In the end, if you’re willing to make the security tradeoff, you get a powerful tool with a great user interface and experience. UPDATE: John Sheehan, CEO of Runscope, has graciously offered our readers access to a Runscope Standard Account for free, just follow http://runscope.com/orchestrate . Thanks John!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-we-replicated-npm-into-orchestrate", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/how-we-replicated-npm-into-orchestrate", "abstract": "NPM or Node Packaged Modules, makes possible the Node.js community’s package ecosystem. Most Node.js developers know NPM for the command-line utility used to manage dependencies, publish packages, and run scripts, but behind the scenes NPM Inc. is running a beastly infrastructure to serve more than 1500 requests per second . Every time you run npm install orchestrate, you’re making a series of requests of NPM’s infrastructure to deliver the orchestrate.js package, and all its dependencies. And it’s a whole ton of requests, too. To get orchestrate.js, it’s 31 requests. Many of those are for dependencies stored as large blob files, and yet installing the orchestrate.js package takes just over two seconds. As an experiment, I wanted to see if I could replicate NPM’s package metadata (that is, not the packages themselves) into Orchestrate, to see what scaling concerns emerged from handling such an enormous and volatile dataset. For example: What did it take, and how did it turn out? When you first insert an item into an Orchestrate collection, we construct a schema based on the item’s properties, and index subsequent objects based on this schema. Orchestrate will generate a conflict if a known field ever contains an unexpected type. New fields are absorbed into the schema, but adding many fields (as in thousands) will cause ElasticSearch to stutter and even crash. This is because ElasticSearch loads these schemas into memory as part of its indexing process. If a schema is too big to store in memory, bad things will happen. Luckily, there’s npm-normalize to normalize metadata so it’s easier to index. npmsearch uses this to format changes from NPM before it indexes them in their own ElasticSearch cluster. But I had to write two more normalizers for the time, times, and scripts fields, to turn fields like this… … into this: The difference is that an object with an indeterminate number of fields (the first example) is hard to index, but the second is easy – not to mention asking questions like “which packages have ever had a 1.0.0 release?” are impossible in the first case. In the second, just query value.time.version:1.0.0 . As npmsearch has discovered, ElasticSearch likes causing problems , and we’ve been there too . Normalizing our data will ensure our mappings never get too big, so we won’t experience problems related to them. Every time a package is published, updated, or deleted, NPM logs that as a change. Since we’ll be staying in sync with NPM using their changes feed, that means we’ll have to process every one of the hundreds of thousands of changes that NPM has ever accepted. To get up to sync, we’ll want to process these as quickly as possible. Can Orchestrate handle it? Yep! Though to be fair, npm-orchestrate only processes one change at a time, or about 5-10 changes per second. Orchestrate handled just fine. To further ensure I didn’t cause our operations team too much strife, npm-orchestrate quits at the first sign of trouble. Restarting it doesn’t lose progress, so a tool like forever could intelligently keep npm-orchestrate syncing, well, forever . To ensure restarts, crashes, and any other problems don’t lose progress, we use a second collection called checkpoints which we update after every change with its associated sequence value. That way, when npm-orchestrate restarts, it consults the latest sequence value, and starts from there. This is much like how CouchDB uses checkpoints in its replication protocol. If npm-orchestrate fails to update its sequence value, it rolls back the last change and quits. If updating a record fails, it doesn’t update the sequence value, and quits. In both cases, restarting npm-orchestrate will pick up before the last error. Once npm-orchestrate caught up with NPM’s latest state, it went from making 5-10 writes per second to one only every few seconds, in keeping with the writes NPM’s registry processes in realtime. When that happened, nothing changed internally. It continues to consume NPM’s changes feed like always, processing new packages, updating its checkpoints, and so on. I’m proud it works, but more could be done to make this replication process more robust. Consider the current version a proof of concept. Now that we’ve got all of NPM in Orchestrate, we can run search queries against it. For example, how many projects does substack ( maintain? 404! How about with an MIT license? 384! These queries aren’t exactly revolutionary compared to the complex scoring algorithms npmsearch uses, but I’m working on it. Check back soon :D After some initial work on normalization, npm-orchestrate was able to sync all of the NPM registry’s metadata, effectively adding full-text search without any added code. Though to be frank, as easy as this was, it shouldn’t even be this hard. You shouldn’t have to consider the intricacies of your databases to build your applications. That’s the whole reason Orchestrate exists. Over the next few months, we’ll be changing how we index data for search to avoid these problems, so even what normalization we performed may not be necessary. To do all this yourself, follow these steps: Happy coding!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "install-manageiq-on-ubuntu-14-04-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/install-manageiq-on-ubuntu-14-04-on-the-centurylink-cloud", "abstract": "One of the more interesting announcements to come out of the 2014 Atlanta OpenStack Design Summit was Red Hat's statement that they were going to open source ManageIQ, a cloud management platform. You can read the press announcement here . ManageIQ is actually a complete and mature project, having over 8 years of development. ManageIQ the company was founded in 2006 and acquired by Red Hat in December 2012. Subsequent to the acquisition, Red Hat has both integrated ManageIQ into its CloudForms product and spent significant effort to get the source code ready to be an open source project. About a month after the ODS announcement, Red Hat followed up and published the source code and created an open community to shepherd the project. Look here for the ManageIQ community site and here for the sources (hosted on GitHub). From the README: ManageIQ is a Cloud Management Platform that delivers the insight, control, and automation enterprises need to address the challenges of managing hybrid cloud environments. It has the following feature sets: We are going to install the current ManageIQ community code on the CenturyLink Cloud . If you don't already have an account, you can sign up for an account in minutes. When we are done you will have the complete system running on a CenturyLink Cloud server, be able to log into the management console, and be able to access the ReST API. While we go through this process, we'll overcome some \"challenges\" that as we deploy ManageIQ. Sounds like fun? Let's get started... We are going to install on an Ubuntu 14.04 server, so we first need to create the server on the CenturyLink Cloud and install the OS. Instructions for doing this are here . Note that in addition to the standard ports (when adding the public IP address) you should specify port 3000 as well. This is the Rails default port. Reboot your server and ensure that you have a terminal open and are logged into your server as root. While you can do initial development with the built-in SQLite database you will not want to go to production, or even serious testing, without a more industrial strength database. The standard choices are either MySQL or PostgreSQL, today we are going to install and use PostgreSQL as this is the recommended package for ManageIQ. First, let's update our apt information and then get the packages we need: When PostgreSQL was installed it set up a standard default user postgres . For ease of use we are going to remove the database password from this account. What the above commands do is: In order to ensure we can connect locally to the database we need to alter the pg _hba.conf file. Currently this is located in the /etc/postgresql/9.3/main/ directory. Find the following section and update \"peer\" and \"md5\" to \"trust\". Before: After: Lastly, we need to ensure that PostgreSQL is listening correctly for localhost connections. To do this we need to edit the file postgresql.conf , located in the same directory as the pg_hba.conf file. Find the line _listen_addresses_ and ensure it looks like: _listen_addresses = 'localhost' # what IP address(es) to listen on;_ (you should be able to just uncomment this line). Restart the database: The PostgreSQL database should now be running and you should be able to connect and interact with it locally (just as our ManageIQ applications will). To test enter the following: The first command will show you all the tables currently defined. The second command shows the currently defined users on the system. Remember to enter \"q\" to exit. Next we need to create the specific user and databases that ManageIQ will use. We just set up 3 databases, created a database user evm , and set the user evm as the owner of the development database. Exit from the postgres user back to the root user. The last thing we need to do before diving into getting and running the ManageIQ is to install and start the memcache daemon and install git. The following will fetch memcache and start the daemon, then install git. The ManageIQ project currently has a dependency on Ruby version 1.9.3. Also, bundler must be version 1.3.5, so we will need to remove the up-level version and replace it with the expected version. First, let's create a user to host and create a runtime context for the Ruby runtime and development. For ease of use set the password to centurylinkdev as well. In order to give centurylinkdev the correct privileges we must add the user to the sudoers file. Add new user in the ‘User privilege specification’ section. Once the file has been successfully edited press CTRL-x to exit the file; and _Y_ to save the changes. Logout as the root user and then login as centurylinkdev . We now have a normal user with sufficient privileges to install ManageIQ. To start with, we are going to make sure the apt repos are up to date and then install some dependencies. There are a few ways to install Ruby, we are going to use rvm as it one of the most popular install systems. Of course, first we have to install rvm and then we will install Ruby . Finally, we are ready to actually install ManageIQ! And everything is installed. Edit your PostgreSQL config file (at config/database.pg.yml) and add the user evm with the password smartvm to the base definition. When you are done your config file should look like: Lastly, we need to set up the database and start the service: Relax for a few minutes and then point your browser at [public IP]:3000 . You should see the ManageIQ welcome screen: Log in using the username admin and the password smartvm . You will see the global ManageIQ dashboard: Congratulations! You have the open source (community) version of ManageIQ up and running on the CenturyLink Cloud. And have it running on the latest and greatest Ubuntu Server and PostgreSQL database to boot. Future tutorials will build off this base as we explore how to manage your private OpenStack cloud with ManageIQ and also explore the ManageIQ ReST API.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-to-write-a-readme-worth-reading", "author": ["\n              \n            "], "link": "https://www.ctl.io/developers/blog/post/how-to-write-a-readme-worth-reading", "abstract": "Programming is hard, and we need all the help we can give one another to make it easier. I’m blown away when I find projects without readmes, or readmes that only provide a single, cryptic line describing the project, like “A better [other project].” How is a visitor supposed to know what to do with your project? From installation to usage to testing to contributing to licensing, your project needs documentation to be useful. Readmes are the backbone of a project’s documentation. They’re a visitor’s first impression. How do you make them good? Pretty easily. A good readme covers this: First and foremost, inform visitors what your project accomplishes (or hopes to accomplish). This isn’t a sales pitch, just a description. For example: Requests - Requests is an Apache2 Licensed HTTP library, written in Python, for human beings Flask - Flask is a microframework for Python based on Werkzeug and Jinja2. It’s intended for getting started very quickly and was developed with best intentions in mind. Express - Fast, unopinionated, minimalist web framework for node. Each of these examples is minimal, straightforward, and helps you decide whether this project is worth your time. Many projects, including Requests, choose to include code samples to demonstrate their interface, or to briefly list differentiating features. Both help visitors determine whether your project solves their problem. Before a visitor can begin using your project, they need to get it. Show them how. List dependencies, and any instructions you’ll need to run to get the project. At a minimum, include terminal instructions. Express ‘ entire “Installation” section, for example, is just one line: That line presumes that you have NPM installed, that you understand it’s a terminal instruction, and that $ indicates this is a BASH prompt, and thus should not be executed. So, it could be better, but it gets the job done. If your installation instructions are particularly complex, or differ depending on your operating system, then break those differing instructions into multiple sections. CouchDB , for example, has three installation guides . This is the entree. “Usage” sections of readmes should detail the project’s features, interface, caveats, gotchas, tips, etc. Everything a stranger needs to solve problems with your project. If, for example, your project is an API client (like Porc ), you should enumerate every method of every class exposed by the client, along with what parameters they accept, and what they accomplish. This can get pretty lengthy, but welcome to writing usable software. The CouchDB client Nano , for example, includes a table of contents that lists every client method, with code samples showing the library in action. If you find this section getting problematically lengthy, you can link to external documentation. Requests ‘ readme’s “Documentation” section, for example, is a single line which links to their documentation site . Software is buggy. Projects go inactive. Their dependencies change. Their maintainers get disappeared over the Atlantic. So, how can a user assert – before going through the trouble of installing your project and trying it out – that it works at all? You can use badges from services like Travis , Drone , and Coveralls to demonstrate your code worked somewhere other than your computer, but people will want to try it themselves, whether because they run an uncommon system or because they just want to see it for themselves. So, provide testing instructions. If you ship your project without tests, shame on you. Look at your life. Look at your choices. Once you’ve come back and written tests, provide instructions on how to run them. For most of my projects, it’s something like this: To run tests, get the source code and use setup.py: That snippet comes from Porc . If your tests need you to set particular variables (ex: an API key) indicate which, and how. If folks have problems, ideas, or just want to lend a hand, where should they go? Requests has a wonderful contribute section, with special attention to new contributors: Although Requests is an open-source project, these principles are just as important for teams on closed-source projects. A strong contributors section will reduce onboarding time from months to minutes, and reduce how much bandwidth onboarding takes from your existing team. Lastly, answer the question, under what circumstances can I use this project? This can be as simple as a link to opensource.org ‘s page on whichever license your project uses. You could also link to your LICENSE file, which should have the text of your license. Why is licensing important, even for open source? Because by default, you, the author, own the project. Anyone using it is violating your property. Even if you’d never take anyone to court over that sort of thing, how can a visitor be sure? State your license! Know your rights! Your readme is your project’s first impression. Make it great, and your users (and your future self who forgets how this dang project works) will thank you. Happy coding!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "nosql-schema-with-orchestrate", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/nosql-schema-with-orchestrate", "abstract": "So you’ve grown up with SQL, and learned well how to organize your data in sane, relational fashions. Now there’s all this NoSQL business talking about brave new worlds of horizontal scaling and fault tolerance — does what you know about data normalization, robust schemas, and optimizing queries still stand? The short answer, yes. The long answer follows. In a database like PostgreSQL or MySQL, tables contain rows, and you have one table for each schema your rows exhibit, containing all the rows that fit that schema. With Orchestrate ‘s NoSQL database API, collections contain items, but it’s the same! Say you’ve got a blog, with users, articles, and comments. In a relational database, you’d have one table for each. In Orchestrate? Three collections! It’s the same principle: separate items into collections by schema. Who determines that schema? You! How? However your heart fancies! Or, nearly. Here are some tips: We manage the schema of your items behind the scenes to automatically index them for querying, while you still have complete freedom to organize and describe your data however you please. How do we relate data together? Y’know, one-to-one, one-to-many, many-to-many — how do we do that? Relations ! Say you’re working in Python, using porc , and you want to create a comment on a blog article. Let’s see how we would relate the comment to a given article: Then, to retrieve all comments for a given article: There you go! All comments, one request. Say you want every article and comment by a particular user. That’s two different collections; how do we get it in a timely fashion? Parallelism! By performing requests in parallel, waiting for every one of them to finish only takes as long as the slowest request, which would have had to run anyway. Let’s see it: Two requests as fast as the slowest of the two. Yay parallelism! That said, we recognize that as powerful as item and graph searches are, they leave much to be desired in comparison to the power and sophistication of SQL, and we’re working on it. We at Orchestrate want to make databases effortless, and if you have opinions about how to do that, let us know ! We prioritize features based on your feedback, so ask and it shall be given :D Happy coding!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "turning-csvs-into-apis", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/turning-csvs-into-apis", "abstract": "A tremendous amount of data is kept as CSVs, or Comma-Separated Values. Working with spreadsheets through programs like Google Spreadsheets can let you perform pretty magical calculations, but as the spreadsheet grows, it grinds your machine to a halt. Nevertheless, the ubiquity of the CSV means even tremendous files are kept that way. How can we make them easier to process? Enter orc-csv , a command-line tool for uploading CSVs to Orchestrate, bundled with a web server to work with them from your local machine. We’ll use it to explore top Reddit posts. To follow along, you’ll need to install Node.js . Once you have it, in a terminal, run this: Then, if you haven’t already, sign up for Orchestrate ! Once you’re logged in, create an application. Then, with any csv, you can do this: … Where YOUR_API_KEY is the API key of the application you just created, and COLLECTION_NAME is the name of the collection you want to group this data under. Specifically, we’ll be aggregating and analyzing the top posts of r/programming . To do that, we’ll get it from the reddit-top-2.5-million project, like so: That CSV contains one thousand records, so it’ll take a moment to upload. Enjoy the pause with Ingrid Michaelson’s Girls Chase Boys :D Once it’s done, that’s it. You turned some CSVs into a searchable, workable, API. We can use orc-csv to let us explore the data right from your browser, like this: From your browser, you can visit http://localhost:3000/v0/r-programming , and you’ll see the first page of items in that collection. orc-csv server uses your API key to create an authenticated proxy to Orchestrate, so you can work with the data without having to put in your credentials. It’s a lot like the Orchestrate dashboard but handier if you’re used to working on the command line. For example, we can use jq to process the JSON results of searches, like so: … which yields a list like this: As a next step, consider uploading each of the CSVs from the reddit-top-2.5-million project into the same collection using orc-csv . Even just using command-line tools like curl and jq, you can effortlessly extract meaningful data. Happy coding!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-to-document-a-project", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/how-to-document-a-project", "abstract": "Last time, we covered how and why to write readmes worth reading , which focused on writing documentation for new users of your software. This time, we’ll cover best practices for documenting projects for teams, collaborators, and volunteers. First, why should you document your project at all? You’ve already got a good readme, so what else is there? Readmes are good entry points for users and consumers, but folks contributing code to your project need more information to collaborate effectively. It’s not enough that a contributor know how to use the library. They need to understand how the code they contribute will be judged and scrutinized. Folks new at programming will want direction when you won’t have the time to give it, even though their added effort will be critical to the project’s growth and success. The bottom line is, at some point, other humans will want to modify your code. Project documentation makes that painless for all involved. So, how does one write good docs? First, make clear where to find anything related to collaboration. Like what? Python Requests’ contributors section handles most of these up front, and the project’s issues and pull requests demonstrate a vibrant discourse evaluating problems and solutions. But, once you’ve listed resources, how do you make sure they’re used properly? How do you make sure contributions don’t… suck? The first step to creating a safe space is to lay ground rules. Let folks know what’s acceptable, what’s not, and then enforce it. This is common practice in social justice, and as a trans woman it makes the difference for what events and committees I will and won’t attend and condone. It’s the same in programming: set expectations, and everyone will be happier. When folks come on to a project, whether as volunteers or employees, let folks know not just where to find things, but what’s expected of their contributions and of them as contributors. Node.js’s Contributing for Dummies walks newcomers through every last step of submitting a code change for review, and sets expectations about the difficulty of getting code through review: Don’t get discouraged. Node is held to high standards, and it can take months to begin to feel comfortable with how the workflow operates and why things are done the way they are. Jump on IRC. Chances are someone is there ready to help. Statements like “Don’t get discouraged” illustrate the priorities of your project, and how you value the people working on and using the code relative to the code itself. Setting expectations is as much about helping contributors write better code as it is about helping people treat one another right. For bad examples, see Linus Torvalds, all the time . As your project’s community grows, the primary function of documentation shifts from on boarding contributors to community management. High-quality project documentation will help your community grow faster, but will also be unnecessary until your community gains traction. If your team is you and a close buddy, you probably won’t need to spend a lot of time talking about structuring community norms to maximize contributions and prevent burnouts. But if you want your project to grow beyond you and your buddy, you’ll need to put in the time to make coming aboard not merely effortless, but joyous. Happy coding!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "using-manageiq-to-manage-openstack", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/using-manageiq-to-manage-openstack", "abstract": "One of the more interesting announcements to come out of the Atlanta OpenStack Design Summit was Red Hat's statement that they were going to open source ManageIQ, a cloud management platform. You can read the press announcement here . In prior tutorials, we showed you how to install ManageIQ on Ubuntu on the CenturyLink Cloud and how to get a stand-alone OpenStack cloud running on the CenturyLink Cloud . We are now going to demonstrate how to configure ManageIQ to connect to, and manage, your OpenStack cloud. First we need to get both ManageIQ and OpenStack up and running. Follow the instructions for installing ManageIQ (here) and OpenStack (here) . Note: Since we will be running ManageIQ and OpenStack in a single data center and under a single account the programs should share a VLAN by default. You can check your CenturyLink Cloud instances by looking at the private IP and ensure that the first 3 decimal numbers in the IP address are the same: Log into the ManageIQ console (user: admin , password: smartvm ). Navigate to Clouds -> Providers . From the Configuration drop down choose \"Add a New Cloud Provider\". This is the configuration page that will generate the connection to the OpenStack cloud. Under Basic Information select \"OpenStack\" as the Type . This allows the OpenStack specific configuration items to be selected. Give the connection a name, enter the host name from your OpenStack instance, and then enter the local IP of the OpenStack instance. Leave the API Port and Zone as is. We will be using the \"admin\" user in the \"admin\" tenant on the OpenStack cloud to connect to and have sufficient privileges to manage or monitor the cloud. You should verify/set the default admin user password to \"admin\" on the OpenStack cloud. Go to Admin -> Identity -> Users . Select to Edit the Admin user and set the password. Finally, set the ManageIQ credentials for both Default and AMQP to User ID: admin and Password: admin . When you hit the Validate button ManageIQ will connect to OpenStack and verify that the credentials are accepted. Once you have verified the credentials you should be able to get the details on the new Cloud Provider: The last configuration item we need to set is to allow the ManageIQ system to serve as the \"Automation Engine\" role. This is off by default and if you find that your provisioning jobs look good, but are stuck in the \"Pending\" state... You have forgotten to turn on the automation role. Select Configure -> Configuration and under Server Control make sure the Automation Engine box is checked. Now that we have OpenStack set up as a provider to ManageIQ let's demonstrate that we can instantiate instances on the cloud platform through the management system. Using the ManageIQ console select Clouds and Instances and then from the left hand navigation window highlight your new OpenStack cloud. You will notice that there are \"No Records Found\" which is correct, since we have not yet started any VM's. From the Lifecycle drop down select Provision Instances . This will bring up a selection dialog that is populated with the image types from your OpenStack cloud. For this exercise select the _Fedora-x86_64-20-20140618-sda_ image and hit the Continue button. The next set of dialogs specifies the character of the instance you want to create. ManageIQ implements an approval workflow, so the first screen will be the details of the requestor. For expediency, we are going to set our request to be \"auto-approved\" and so the Request Information details won't be applicable (but you have to enter information in all of the starred entry boxes). Select the Purpose tab and select the Auto Approve drop down. Check the \"1\" box, this will allow a request for 1 instance to be auto-approved. Select the Catalog tab, you will note that the Fedora image is highlighted. Finally, give the instance a base name in the Instance Name field. For demonstration purposes I am setting this to \"Tutorial\". From the Environment tab select the \"Choose Automatically\" option for the instance placement. Next, from the Properties tab select the \"m1.small\" for the Instance Type. Since we will accept all of the rest of the defaults now hit the Submit button. You should see the \"Requests\" page with your request at the top of the list, in a \"Pending\" state\". Wait a bit and hit the Reload button. You will see the Request go from \"Pending\" to \"Active\" and the \"Reason\" to \"Auto-Approved\". To ensure that everything is working correctly, go to the OpenStack console and navigate to Admin -> Instances . You should see something like this: And if you go to the ManageIQ console and navigate to Clouds -> Instances -> OpenStack Provider (your name will be here) -> nova -> Tutorial (your name will be here) you will see something like this: Congratulations! You are now running the community version of ManageIQ, connected to an OpenStack private cloud, all being hosted on the CenturyLink Cloud . From here you can explore the many features and options available through the ManageIQ interface. Besides the console UI's both ManageIQ and OpenStack have comprehensive API's to perform their tasks. Other tutorials will explore these programmatic interfaces in more detail.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-gitlab-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-gitlab-on-the-centurylink-cloud", "abstract": "As software developers and devops engineers one of the most important tools in your continuous integration and deployment workflow is the source control management (SCM) system. SCM's have a long and storied history, there is a great overview covering 1972-2010 here (codicesoftware.blogspot.com) . My favorite graphic from this article shows a timeline of SCM systems overlayed on the current state-of-the-art cell phone technology: From the website git-scm.com : As with many great things in life, Git began with a bit of creative destruction and fiery controversy. The Linux kernel is an open source software project of fairly large scope. For most of the lifetime of the Linux kernel maintenance (1991–2002), changes to the software were passed around as patches and archived files. In 2002, the Linux kernel project began using a proprietary DVCS system called BitKeeper. In 2005, the relationship between the community that developed the Linux kernel and the commercial company that developed BitKeeper broke down, and the tool’s free-of-charge status was revoked. This prompted the Linux development community (and in particular Linus Torvalds, the creator of Linux) to develop their own tool based on some of the lessons they learned while using BitKeeper. Some of the goals of the new system were as follows: Since its birth in 2005, Git has evolved and matured to be easy to use and yet retain these initial qualities. It’s incredibly fast, it’s very efficient with large projects, and it has an incredible branching system for non-linear development. With the popularity of Git within the open source development community arose a hosted service to store repositories and encourage collaborative development, this is the popular site GitHub . Seeing a need for a similar solution that could be hosted within a company's firewall GitLab was created, and is available as an open source Community Edition as well as a commercially supported product. A high level view of the features of the GitLab Community Edition server: We are going to install a complete GitLab CE server on the CenturyLink Cloud. It will be based on Ubuntu 14.04 (Trusty) and Ruby 2.1.2. To get started, build a CenturyLink Cloud server and bring it up to date by installing Ubuntu Trusty . You should now have a running Trust server and be logged in as root. We will now install the dependencies: Now, we need to install git and ensure it it the correct version: An important aspect of a distributed SCM is to be able to send email notifications. Since Ubuntu doesn't have a default mailer, let's install postfix : When asked, select Internet Site and hit enter to select the default server name. GitLab CE assumes that you have version 2.1.2 of Ruby installed. By default the Ruby version in Trusty is 1.9.3, so we will first remove the older version and then install the correct version of Ruby. Get Ruby, compile and install it: And, install the Bundler gem: The last thing we need to do before actually installing the GitLab server is to install and configure the database the system will use. It is recommended that PostgreSQL be use, and that is what we will install. First, create a user git for use with GitLab: Next, we are going to install PostgreSQL to the latest version (as of right now it is version 9.3): We will be installing into the home directory of the user git Now we need to configure the system. Copy the example GitLab config, then we will edit it for our purposes. Edit the config/gitlab.yml file and change references to \"localhost\" to the public IP assigned to your server. Additionally, there are several permissions related and example installations that need to be done: Since we have chosen to use the PostgreSQL database, let's copy over the example config file and then edit it. Note that the database user on the production database should be git with no password (this should be the default). Install the required gems: GitLab shell is an SSH access and repository management software developed specially for GitLab. To install run the following: We now have just a few housekeeping items to complete before GitLab is installed and we can validate that it is running correctly. In order to check if GitLab and all of its environments are set up correctly: You should see system information from System Information (OS, Ruby, etc.), GitLab Information , and GitLab Shell Information . Finally, we will compile all of the application assets and start an instance of GitLab. You should see a response that looks like this: Congratulations, you now have GitLab CE edition up and running on the CenturyLink Cloud. However, in order to complete the install we have one last task to accomplish. Nginx is the officially supported web server for GitLab so let's install and configure it. To install: And to configure: At this point the entire system should be installed and running, to double check: All the items should be green. If so, you have successfully installed the Community Edition of GitLab on the CenturyLink Cloud. Point your browser at the public IP of the server and you should see the login screen: Login with user: root and password: 5iveL!fe and have fun exploring GitLab!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-chef-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-chef-on-the-centurylink-cloud", "abstract": "A key element in any Continuous Integration and Continuous Deployment strategy is the automation of infrastructure and application deployment. Over the past few years a discipline called \"DevOps\" has arisen around this philosophy, driven by the availability of automation tooling, such as Puppet from Puppet Labs and Chef from the company of the same name. Being able to codify how infrastructure is setup and how applications are deployed in a repeatable and immutable manner allows for a much higher velocity process without introducing the fallible human element. In other words, let the computers do what they do best. We are going to install the Chef Server and Chef Client/Workstation ; together these comprise the management, control, and authoring systems for Chef. Once you have these servers up and running we will create a Chef Node and initialize it to the server. Note that we are going to be running these systems (Server, Client/Workstation, and Node) on separate servers (so you will end up provisioning 3 CenturyLink Cloud servers). We want to be running on a modern OS, so lets create these CenturyLink Cloud servers and bring the Ubuntu OS up to the latest version. Instructions for doing this are here . Select the server you have chosen to be the Chef Server and login as root. We are going to set up a Chef version 11.1.4 server, you can check to see if there are newer deb packages and substitute the package name where appropriate. Also, since we are not setting up DNS, we will use the public IP as the FQDN to identify the Chef Server and Client/Workstation. Pull up your browser and point it to https://[public ip] . If you get a certificate error don't worry, just indicate you want to continue.You will see the Chef Server logon screen (as below). At this point we have successfully installed the Chef Server and next will install the Chef Client/Workstation. Next, select the server you want to install the Client/Workstation on, and login as root. We need to first grab git : Then, install Chef: And that's all there is to installing the Client/Workstation. Check that everything is OK by running a validation: You will get the version string of the installed version of Chef back. The Chef Client/Workstation is the primary interface to the Chef system, and will contain a local copy of the repository. In order to create the local repo: In order to start using the Client/Workstation we need to set the local system variables and then configure the client to talk to the Chef Server. First, let's configure the local system vars: In order to securely contact and interact with the server we need the private keys for two accounts: admin and chef-validator . These are located on the server we created earlier, to fetch them we will use scp and connect to the server using its private IP . The primary client program is called knife , we now need to create a knife configuration file. Start by: Work through the questions, using the private IP of the server when asked for the server URL. Your screen should look like this when you are done: Finally, make sure your Client/Workstation is installed and can talk to the server: Should return:\nchef-validator\nchef-ui Should return:\nadmin\nknife The final piece of the Chef configuration is the Chef Node. This is the server that will be deploying and running the code as specified by the appropriate run lists, cookbooks, and recipes. This process is very similar to setting up the Workstation. First, install Chef: Copy the chef-validator.pem key from the Chef Server: Register your Node with the Server: Create a client.rb file in /etc/chef (using vi or your favorite editor) and add the following: log_level :info\nlog_location STDOUT\nchef_server_url 'https:// [Server private IP] :443′ Finally, validate your Chef Node installation: You should see a response that looks like: You now have a working 3 server Chef installation running on the CenturyLink Cloud. The next steps will be to create cookbooks for your Chef Node and deploy them using the chef-client. An excellent resource to getting started is the Chef Supermarket , a large collection of cookbooks and recipes that have already been vetted by the Chef community.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "writing-nodejs-data-models-for-nosql-part-1", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/writing-nodejs-data-models-for-nosql-part-1", "abstract": "An important step in building an application is creating an interface to your data. The use cases of your data may vary, but the process through which the data is stored and accessed is largely the same. Models sit between your application's logic and the data store itself. A good model should create a standard interface to the data store, and remove the burden of configuration and implementation from the rest of your application. In this two-part series, we'll create a generic data model for a JavaScript NoSQL solution and discuss common data model patterns. If you're coming from a relational database background , NoSQL models require more attention up front. When building models for a relational database, it's easy for developers to let the database manage type and relationship constraints. In a NoSQL model, there are no database constraints; it's the application that enforces the schema. Developers should take the time to understand their data and how it will be accessed during the life cycle of a request. While it's tempting to throw JSON documents into a collection, you'll end up creating inconsistent documents that will cost you twice the time to fix later. Using Mongoose and MongoDB , we'll create a simple, reusable, and inheritable model object that will set the foundation of our custom application models. Our finished product can be found here . We want to be able to create our own classes that represent each collection in our application. We'll add more features and functionality as we progress through the series. MongoDB is a powerful open-sourced NoSQL document database solution. It focuses on speed and reliability and has advanced capabilities such as full-text search. Mongoose is a Node.js library that provides a schema-based database modeling solution for MongoDB. CenturyLink Cloud Compute servers are high-performance cloud servers. These enterprise-grade virtual machines are easy to deploy and manage from the CenturyLink Cloud Console and via our powerful API. If you don't have a CenturyLink Cloud account yet, head over to our website and activate an account . You'll need it to access CenturyLink Cloud products. Our first step is to deploy a new CenturyLink Cloud virtual server. Follow the steps below. On the left side menu, click Infrastructure and then Servers . On the left-hand side of the server panel, click on the region for the server we will provision. Your server provisioning request will enter the queue. You can watch the progress of your request on the screen. Your server is provisioned when the status of all tasks in the queue is complete. After your new server is provisioned, in the CenturyLink control portal, click Infrastructure on the left side menu, and then click Servers . Type \"27017\" in the blank box to open up the MongoDB server port. Click add public ip address . From a shell on your local machine, connect to your new server with the following command. Replace \"YOUR.VPS.IP\" with your server's public IP address. Install the MongoDB server software by running the following commands. With your favorite text editor, open /etc/mongod.conf . Look for the line that begins \"bind_ip\" and comment it out. The top of your file should now look like this: Start the MongoDB service by running the following command. You can follow along with building the schema model by downloading the finished sample application from this GitHub project . First, we need to make sure that all of the required packages are installed. Run these commands in your shell from the project directory. Note: If you are following along from the GitHub project, you only need to run one command: npm install Next we need to create a database connection. In your project directory, create a file called config.js with the following contents. Replace \"YOUR.MONGODB.HOST\" with the IP address of your MongoDB server. Now we can initialize the database connection and load the libraries needed to build models. The following code goes into your main application file, usually index.js . The Mongoose library can also be configured to use any JavaScript Promise library that follows the Promises/A+ standard . We will load the Bluebird library. Now, we want to connect to the MongoDB database and initialize a model for our movie application. That will look like the following example. You will notice that we have not yet initialized a schema object. Now we can create connections, so it's time to generate models. We need to prepare a model for our movie object. Create a file called lib/schema.js and start it out like the following. This module takes an argument when it's loaded, which allows your application to have multiple MongoDB database connections. In index.js , you can use it like this. Notice how simple it is to make use of Mongoose model methods to create collection-specific functionality! In the movieSchema structure, you see an example of a type constraint to make sure that the year attribute is an integer. This frees your models up from excessive boilerplate and allows you to focus on how data flows through your application. In the next section we'll discuss data modeling in JSON and add features to our model that demonstrate relationships between document collections and records.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "automate-a-centurylink-cloud-server-using-chef", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/automate-a-centurylink-cloud-server-using-chef", "abstract": "Chef is one of the leading DevOps tools that allows infrastructure to be defined by code. This allows \"cookbooks\" and \"recipes\" to be created that then can guarantee that your deployments are automated, repeatable, and invariant. Previously we set up a basic Chef environment; this consists of the Chef Server, the Chef Workstation, and a Chef Node. To start with this tutorial you should have the basic Chef environment setup, as described here . You will also want to SSH into the Workstation and the Node. In order to automate your deployments we need to accomplish the following: In order to get a good understanding of the basics we will start with a very simple Chef recipe. Once we have demonstrated the overall process we will move to a more complicated example involving several related and dependent recipes. In order for the system to work with the local Chef repo on the Workstation we need to tell it: a) where the cookbooks are to be located, and b) what editor we will be using to edit the \"run lists\". Login to the workstation and edit _~/.chef/knife.rb_ . Add the following two lines: Your knife.rb file should look similar to: Cookbooks are the fundamental unit of configuration and policy enforcement that Chef uses to bring a node to a specific state. Chef uses cookbooks to perform work and make sure things are as they should be on the node. Cookbooks are usually used to handle one specific service, application, or function, for instance, a cookbook can be created to use NTP to set and sync the node's time with a specific server. As we will see cookbooks can be much more complicated and built up from a series of base cookbooks. Think of cookbooks as packages and Chef as the package manager for configuring infrastructure. The basic workflow is to create cookbooks on the Workstation and then upload them to the Chef server. From there, recipes and policies described within the cookbook can be assigned to nodes as part of the node's \"run-list\". A run-list is a sequential list of recipes and roles that are run on a node by chef-client in order to bring the node into compliance with the policy you set for it. In a nutshell, this is how the node machine infrastructure is configured via code in an automated fashion. A word about recipes . A recipe is the main workhorse of the cookbook and are used to declare the state of different resources. A cookbook can contain more than one recipe, or depend on outside recipes. Chef resources describe a part of the system and its desired state. For instance, a resource could say \"the package x should be installed\". Another resource may say \"the x service should be running\". A recipe is a list related resources that tell Chef how the system should look when it implements the recipe; while running each resource is checked for compliance to the declared state. If the system matches, it moves on to the next resource, otherwise, it attempts to move the resource into the given state. Let's get to our first simple cookbook. Since we are installing on an Ubuntu 12.04 server we normally will do a sudo apt-get update command to make sure the application repositories are up to date. We can do this with a simple Chef cookbook that we will call apt . Execute the following knife command (on the Workstation): This will create a cookbook, called apt in the directory ~/chef-repo/cookbooks . Go to the apt cookbook directory, note all of the supporting directories and files have been setup for the cookbook. Find the recipes directory and cd into it. You should find only 1 file ( default.rb ), this is the file we will add our instructions to. Edit default.rb and add: Your finished file should look like: Upload your completed cookbook to the Chef Server: Now we are ready to update the run list of the Chef Node. First, get the name of the node by executing: My node name is 'UC1CCCCCC-4401.local' ; yours will vary by what you named the Chef Node server when you created it. Edit the node run list by running the following (substitute your node name): We are going to add \"recipe[apt]\" to the run list, the resulting file should look like: Finally, we can SSH into the Chef Node and run the chef-client program. This will initiate a \"Chef run\" which fetches the run-list and executes the cookbooks specified (in our case it should run sudo apt-get update ). On the Chef Node run: You should see that the apt cookbook runs and executes apt-get update . We have now shown that we can complete the entire Chef workflow; from creating cookbooks and recipes, to uploading them to the server, to executing them in an automated fashion on the target node. Our example was a simple apt-get instruction... However there are experts in the community that have deep knowledge of the apt package manager and have actually codified this into Chef cookbooks. Like all good programmers we are lazy and prefer not to re-invent the wheel, so our next example will show how to get and utilize pre-build cookbooks for common actions. If you go to the Chef Supermarket and search for \"apt\" you will find the following cookbook: First, download and install the remote cookbook: And following the workflow above to upload the cookbook and then run the chef-client on the Chef Node. You will note that there are 8 resources being updated this time (opposed to the single one we had previously). Now that we have demonstrated how to work the end-to-end process let's install a significant application. By now you should have a sense of how easy this will be :). The application will be Wordpress, this is a significant PHP application and has 30 cookbook dependencies. Download and install Wordpress: Upload the Wordpress cookbook to the server (the \"-a\" option says to upload \"all\" cookbooks): Edit the run list for your Chef Node and add a line for Wordpress. Note we will leave the apt cookbook in place: And then on the Chef Node run: Point your browser at the public IP for the Chef Node and you should see the Wordpress configuration screen: We now have a repeatable DevOps setup (using Chef) in the CenturyLink Cloud. In order to create the next Wordpress site (or the next 10) you simply need to install a server, deploy the chef-client , and create a run-list for the node that includes the Wordpress cookbook. If the base Wordpress code (or any of the 30 dependencies) changes you will update the Chef Server and simply re-run the chef-client on each Chef Node for seamless and trouble-free deployments.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "writing-nodejs-data-models-for-nosql-part-2", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/writing-nodejs-data-models-for-nosql-part-2", "abstract": "Welcome back! This is the second part to a two-part series discussing data modeling in NodeJS with Mongoose and MongoDB. In our first part we deployed a CenturyLink Cloud Compute server with MongoDB and created a new application with Mongoose and one object model. We will continue to build upon that work in this tutorial. The end result can be found on GitHub . Please download it so you can follow along. Now that we have a functioning model object we can use to represent our collections we need the ability to create relationships between our models and define schemas. Finally we'll discuss common data modeling patterns in JSON with examples using our object models. Document-based NoSQL databases like MongoDB are highly flexible. The database itself does not impose any type of constraints: documents are stored as is. This flexibility is great for getting something off the ground fast, but as the properties of your data evolve, there is a danger in creating fragmented documents and requiring a growing number of exceptions in your models to fill in the gaps. By creating a schema for your data, you document the structure of your objects and provide a comprehensive plan for data validation. A few minutes up front will save you hours of frustration down the line. While it's possible to build model data validators from scratch, Mongoose comes with a full-featured validation system . Let's expand the data models in schema.js to take advantage of them. First, take a look at this expanded version of movieSchema . It contains validation for the year of the movie and movie ratings. For the year of the movie's release, we want to make sure that it is an integer and that it's not before 1890. The movie ratings are based on those used by the Motion Picture Association of America. Another important aspect of data modeling is the ability to create relationships between different documents across collections. Let's create a schema for an \"actor\" document, so we can create an association between two different models. There are a number of different ways to create relationships between MongoDB documents, but the easiest is to simply store object IDs. Remember, however, that NoSQL databases do not typically feature strong automatic relationship mapping. Add a new schema to schema.js using the following code. Next, update the module.exports function to look like the following example. Using this method, you can create relationships between any documents in the same MongoDB database. However, the burden of managing these relationships falls solely on your application. There are no cascading updates or deletes in a NoSQL database. When designing your data models take the following into consideration: In this example, you can see how models are used with Mongoose and ES6-style promises to add documents to your MongoDB database. Design your data model, enforce it with a schema validator, and build sensible data relationships for your application. Doing these three things up front will save you later heartache by keeping your data consistent. For further practice with expanding data models, write an application to take JSON data from the Open Movie Database , validate and save it in MongoDB.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-to-migrate-legacy-applications-into-docker-containers", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/how-to-migrate-legacy-applications-into-docker-containers", "abstract": "What if your application has been around a long time already? Is it too late for containers? Can you teach an old dog new tricks? Yes you can! Before you throw in the towel and loading up your fatty VM, let's consider what constraints 'containerizing' puts on the application. In this post, I will demonstrate containerizing legacy applications use an application written by Matthias Nehlsen called Birdwatch . His application was not designed for containers but with making a few alterations, Birdwatch can be deployed and configured within a Docker container. If the application relies on external services (i.e. database, nginx proxy, or message queue) these services should now be running within their own container. Of course, this step is not unique to containers; it's a common step for any complex application. The Birdwatch application has two major components. The application is built using the Play framework and relies on an ElasticSearch index. Therefore, I will design the architecture using two containers: one for the application and one for elastic search. This implementation has several advantages: An alternative would be to create a single image which includes all components. While this may appear simpler, it has the same drawbacks as creating a monolithic black box - any alterations to a component will require a new image of the entire application. The larger the image, the larger the download, which will have a negative impact on deployment. A search on the Docker Registry will provide a list of possible images upon which to base the application. The difficulty is determining which to use. The community can star an image, which could be used to determine the popularity of an image. The Registry also shows the number of times an image has been downloaded, but these measures are subjective therefore find images with the following characteristics: Using an image with a tag provides version control for the application. The latest tag is a rolling version consequently pulling the latest image and therefore may introduce a breaking change to the application. The Dockerfile is the complete image recipe to validate all the code and actions contained within the image. Having a common base image will improve the development cycle because they will leverage an image cache. Read Working with the Docker image cache to learn more. Our Birdwatch application needs an ElasticSearch image with version 1.1 or greater. Searching on the Registry for 'elasticsearch' provided 236 results. The first entry is a trusted image, has a Dockerfile, but only provides a latest tag. It also requires the Play2 framework. A Registry search only provides three possible candidates. Inspecting the FROM command in the Dockerfiles for each of the results show none of them have a common base with our trusted elastic search. Initially, I tried the mzkrelx/playframework2-dev but found its startup was slow. I finally settled on the reubenbond/playframework2 image. For an efficient production release of Birdwatch, I would consider creating a new Play framework image which was FROM the common dockerfile/java image used in the trusted ElasticSearch image. Applications require some configuration. This could include: An application running in a container will need the same configuration. This configuration data could be sealed within the container; or, a better approach is to provide the configuration at runtime. Currently, there are three methods to provide runtime configuration. The first is using environment variables . All environment variables provided on the run command are exposed to the image. Therefore any configurations which use environment variables can be set during the run command. The Birdwatch application uses a set of configurations defined in an application.conf file. Assigning environment variables to these items is the only step required to make them configurable. Each of these must be set by a -e option on the docker run command. Using multiple container's requires setting up a communication channel between them. Docker calls this linking . The links are presented to the container as environment variables using specific naming convention. Our Birdwatch application is going to be linked to elasticsearch using --link dockerfile_elasticsearch_latest:db This link command creates several environment variables based on the db alias provided in the link. The TCP address and port are used in the application.conf again to provide the link to the elastic search container. The final method for providing runtime configuration is through parameters to the Dockerfile entrypoint . Our Birdwatch application did not use this configuration process. The current best practice for logging is to write to the standard out. The journal and container logs capture this data and can be viewed in case of an application failure. This subject is far from concluded many discussions have been circling around the ability of a container to provide logging data. The Birdwatch application used system logging and a log file so no alterations to the codebase was needed to access its log data. A Dockerfile must be used to create an image. This file outlines all the steps required to build the image. This article will not discuss the optimization of images. A great discussion about image optimization can be found here . The key command for adding code is the ADD command. It has two parameters a src and dest. The src parameter is a valid path when building the image. The dest parameter is an absolute path within the container. If you want to ignore files or directories when adding files place a .dockerignore in the root directory. It works similar to a .gitignore file. Once you have an image created it can be added to the Docker Hub . Our Birdwatch application has a pretty involved build process. Each of the UI frameworks need to be built first and then the application can be constructed. This could be handled in a script which would be executed on build, but for this demonstration I chose to run those steps outside of the docker build process and only add the resulting compiled code. I added a .dockerignore file to ignore the .git and logs directories: The Dockerfile to assemble the image: The final image is on the Docker Hub: Birdwatch Image Search for Birdwatch and click install. We have shown with the Birdwatch application it is possible to 'containerize' an application after it has completed its development cycles. With some knowledge of how Docker will expose linked containers and using environment variable we also provided runtime controls to the application. Now the Birdwatch application can be deployed and configured for any topic using any twitter credentials. No more excuses. Go forth and Containerize!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "openstack-networking-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/openstack-networking-on-the-centurylink-cloud", "abstract": "This is a follow-up to our previous tutorial setting up an OpenStack Cloud on the CenturyLink Cloud . If you haven't done so, the instructions for setting up OpenStack are here . Once the basic OpenStack system is installed and running the next steps are to create virtual server instances that can route to and from the Internet. In order to make this happen we need to setup the correct networking topology, configure the local instance to utilize the new network, and ensure that the security/privacy settings are defined correctly. At the end of the tutorial we will be able to login to the newly created instance and be able to access the external network from the instance. What about accessing the new instance from the Internet? We are leaving this topic for another tutorial where we will discuss how to setup the pools of floating IP addresses within Openstack and how to configure the correct routing and firewall rules. First, login to the OpenStack controller node using the _stack_ user created earlier, this will be the CenturyLink Cloud instance that owns the public IP and hosts the Horizon dashboard. Note that if you check the network via _ifconfig_ this instance has the public IP and also has an associated private IP. This private IP is managed by the CenturyLink Cloud system, in a minute we will add a second private IP that is managed by the OpenStack Cloud system. Remember, we are running a development version of OpenStack that has facilities to help contributors develop code. As such, it doesn't use the \"standard\" service interfaces for starting/stopping services but rather runs each of the processes via \"screen\". This allows a real time view of the development log messages and allows an easy method to switch between views. From the _devstack_ directory run _./rejoin-stack.sh_ to start the screen program and attach all of the _devstack_ processes. Devstack runs all the processes in different screens, use 'control A' screen # to switch between them. Also, since there are no services we cannot use service restarts. To restart a process go to the appropriate screen, 'control C' out (stop), up arrow (get the start command back), and then press ENTER . Set the following environment variables to allow us to issue \"nova\" and \"glance\" commands. Note that the OS_AUTH_URL is the private IP assigned when the CenturyLink Cloud created the instance. The file _/etc/nova/nova.conf_ should have the following (if not, update to match): Edit the file to add the following: There are a couple of networking changes we need to make to ensure that network traffic is correctly routed to the OpenStack instances: In order to not lose the configuration changes on reboot go ahead and edit (as root) the file _/etc/sysctl.conf_ and add the following: Finally, restart the network (screen #9). Bring up the Horizon dashboard, and ensure you are in the \"admin\" project (not the \"demo\" project). You can check this in the drop down near the top of the screen. From the navigation pane on the left side select \"Project\" | \"Compute\" | \"Access & Security\". We are going to edit the \"default\" security group to make it promiscuous (for demo purposes only and ease of use. Never do this for a production system.). Selecting the \"Security Groups\" tab find the \"default\" security group and hit the \"Manage Rules\" button. Now hit the \"+ Add Rule\" button on the top right of the screen. We are going to add 3 rules: Next, go to the \"Key Pairs\" tab and \"Create Key Pair\". I am naming the key pair TestKey . This will create the public and private keys we will use to login to the OpenStack instances. You'll note that the created key has been downloaded to your desktop so we need to upload this back to the OpenStack controller node, to do so use _scp_ : And from your local desktop scp the key to the stack home directory: And we need to move the key file and set the appropriate permissions: We are almost ready to create our first instance... But first we are going to download an Ubuntu 14.04 image and make it available as an instance type. List images to make sure it is registered, you should see \"Ubuntu 14.04\" as one of the supported images: From the left hand navigation pane select \"Project\" | \"Compute\" | \"Instances\". Find and hit the \"Launch Instance\" in the upper right corner of the \"Instances\" page. On the \"Details\" pane select the following: Hit the \"Launch\" button and in a few moments the new instance will be in a \"Running\" state. Note that since we have only a single key pair the system automatically assigned TestKey.pem key pair: Test that you can access the new instance: Finally, let's SSH into the new instance: Ubuntu will now tell you that the default user is ubuntu and close the connection. You can now shell into the default user: From the new instance check that you can access the network gateway by: From the new instance you can now 'curl 10.0.0.1' to see the HTML for the OpenStack Horizon dashboard. Test that you can see the internet by 'ping www.google.com' or 'curl www.google.com' . You now have a fully setup, configured, and network routable demo instance of OpenStack. Each new instance will inherit the settings made on the network controller and should be able to see all other OpenStack instances, as well as be able to get to the Internet.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "the-joy-of-vagrant", "author": ["\n              Chris Weyl\n            "], "link": "https://www.ctl.io/developers/blog/post/the-joy-of-vagrant", "abstract": "In our previous tutorial Installing Chef on the CenturyLink Cloud , we showed how to set up three hosts: one as a Chef server, one as management workstation (client), and one as a Chef-managed node. However, sometimes it's useful to have the Chef client on your local workstation. Given that the Chef client install feels somewhat...invasive...for a machine used as a primary workstation – you know, the machine you absolutely and without hesitation depend on in order to access and manage everything else – this seems like an excellent opportunity to examine using Vagrant in conjunction with a virtualization provider to provide for an easy, repeatable way to create contained VM's usable as Chef management interfaces. Easy, reproducible build of an Ubuntu 14.04LTS (trusty) VirtualBox VM with the Chef management tools installed. Installed and configured on your system: Note: If you're using Ubuntu Trusty (14.04LTS) the latest level of vagrant available through the Ubuntu apt repositories is 1.4.3, as of this writing. Using a different virtualization provider is left as an exercise for a willing reader. The files (and some others) described in this article can be obtained by cloning this repository: Ok, ready? Go! A Vagrant base box is basically a template box; it's used as the foundation of the box you'll be creating. If you're familiar with how Docker images are built then this concept is very similar to the \"FROM\" line in a Dockerfile. Thanks to the Vagrant Cloud , there are a large number of publicly accessible base boxes available, and the Ubuntu team has an official 64-bit trusty base box they maintain. Create a minimal initial Vagrantfile with: ...this yields a Vagrantfile.minimal file that looks like this: If you want to explore a bit, leaving off the --minimal creates a substantially larger Vagrantfile . You can check it out here . Now comes the fun part. Basically, we take all the steps outlined in the previous tutorial and have Vagrant execute them inside the virtual machine as part of the provisioning process. The initial Vagrantfile created above contains only the barebones minimum required to launch a new virtual machine based off of the ubuntu/trusty64 image. (And a lot of comments !) Let's stick all of our needed provisioning into one script: provision.sh ...and configure Vagrant using our Vagrantfile : Now, all we should need to do is vagrant up in order to launch and provision our VM: vagrant-up.log There's a lot of noise generated by Chef's installation script, so I've omitted it from the log, above. To check the status, you can run vagrant status : To do this you can use the client-configure.sh script. The directory containing the Vagrantfile is exported into the VM at '/vagrant', so we can log in to our newly-created VM and configure the client. Note: You'll need to set the appropriate servername or IP address ...for brevity, the above output has been trimmed; it should mirror the description you see from the previous tutorial . Now, whenever you need to use your Chef management tools, you can simply \"vagrant ssh\" into this VM. There are a great many things one can do to optimize this to meet ones personal tastes – or have the \"provisioning\" stage fully configure for a given Chef environment – and these are left as exercises for the readers. Enjoy! We give you the deployment tools you need to manage your applications quickly and easily. Check out our Knowledge Base of articles on CenturyLink Cloud. We also have several container tools listed in our Developer Center. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "managing-application-code-in-linux-containers", "author": ["\n              Rupak Ganguly\n            "], "link": "https://www.ctl.io/developers/blog/post/managing-application-code-in-linux-containers", "abstract": "Many developers have heard of Linux containers and Docker. But how do you deploy your code to an application running in Linux containers? Let's go over that in this article, using a very simple yet effective workflow. We'll focus on WordPress, but these principles can be used with Ruby, Python, or even Go applications. I think most of the time, users need to manage Wordpress for adding/updating plugins, themes etc. Although, the same can be achieved via the WordPress UI, we will use it as a simple use case to demonstrate propogating code changes to the WordPress application. If you're on a Mac, installing Panamax couldn't be easier: If you are on Linux, it is easy too. Just follow the instructions on the wiki. It might also be helpful to peruse the documentation for Panamax. We will create an application via Panamax, that will have two containers: one for MySQL, and the other for WordPress. WordPress will be linked to the MySQL container. All that part is done for you if you use the Wordpress template we have. We will start with an example of how we can update themes and plugins, from our machine without using the WordPress web UI or without getting into the WordPress container. Creating the Wordpress Application Assuming that you have Panamax installed and you can see the Panamax UI on your browser, we will get going and create our first application. We will use a WordPress application created in Panamax to demonstrate our solution. The reason I picked WordPress is due to the nature of simplicity of the solution, familiarity of WordPress across the community and how the code is laid out. But, the concept can be applied to any Panamax application. From the Dashboard page, click on 'Create a New Application' button to go to the search page. Search for 'WordPress', and pick the official template for WordPress that appears in the search results. Click on the 'Run Template' button, and Panamax should start the application. It is important to note that the images for WordPress and MySql can take a little while to get downloaded and spun up. Once you see the spinners go away, you should have your application running. Fig 1 : Application details screen in Panamax showing WordPress app To be able to view our applications running inside the host VM, we need to forward the port(s) for our application. You can view instructions on port forwarding on our wiki. Let's make sure the application is running and we can browse to the WordPress admin screen at http://localhost:8997 (assumption being you forwarded port 8080 to 8997). Fig 2 : WordPress admin screen setup Here we can configure the required fields and get WordPress installed. We have a few themes and plugins installed by default, but we would like to add our own. Fig 3 : WordPress admin screen showing themes and plugins The lay of the land To understand the different realms of our application and code, we need to understand how they are mapped across different layers. Fig 4 : Mapping of folders in Mac/host VM/containers As shown in the above illustration, the code folder from the physical machine (Mac) is mapped and synced to the host VM (CoreOS) by Vagrant (See Vagrantfile later). Then the code folder from the host VM (CoreOS) is mapped to the folder inside the WP container by the volume mount (See 'Configuring the Application' later). The Mac (physical machine) Code Path: /my/local/path/wp-content The Host VM (the CoreOS VM) Code Path: /Users/username/src/wordpress/app/wp-content The Container (WP) Code Path: /app/wp-content Accessing the code Just accessing the code from within the host VM or within the container, is not sufficient. We want to use our own machine, our own tools, our favorite editor, and manage our code. We need the code on our machine. Let's do that. We can modify the Vagrantfile that Panamax uses to build the host VM, to further share the wp-content folder from inside the host VM to the working folder outside on the physical machine i.e. the Mac in my case. Add the following lines into the Vagrantfile under ~/.panamax , to create a NFS share mount: For the change in the Vagrantfile to have effect, we need to restart Panamax by running panamax restart . Now, we can edit code in the working folder ( /my/local/path/wp-content as configured in the Vagrantfile above) on the Mac, to propogate the changes to the shared folder ( /Users/username/src/wordpress/app/wp-content ) on the host VM. Injecting code into the container Before, we can mount a volume to map the host VM path to a container path, we need to make sure we have content in it. Create a code folder in the Panamax VM host, and put the relevant Wordpress code i.e. the contents of the wp-content folder in there. Get into the Panamax host VM by panamax ssh , and then execute the command at the terminal: After that, we can exit out of the Panamax shell. That will get the contents of the wp-content folder into the /home/core/src/wordpress/app/wp-content folder, and we will later volume mount that folder into the Wordpress container. Note, that the default Wordpress container has all the code that is necessary to run WordPress, but we are selectively replacing the contents of the wp-content folder from our working folder, so that we can change the code, without getting inside the container. Configuring the application Lets get back in Panamax and create a volume mount from our working folder at /home/core/src/wordpress/app/wp-content to the /app/wp-content folder inside the Wordpress container. See the screenshot below to see how to do that in Panamax. Fig 5 : WordPress container details page with volume mount. Click on 'Save all changes' to commit the change. You will notice that Panamax automatically detects the change and restarts the application. Wait until both of the containers are started. Note : Since volume mounting a path on a container will override the contents from the mapped folders, if you do not have the corresponding folders mapped properly, your WordPress application will either not work or the themes/plugins will not show up properly. Since, we have not really injected the code at the folder /home/core/src/wordpress/app/wp-content on the host, the application is broken as of now. I promise it will work soon. Making code updates For making code changes, the easiest examples would be to add a new theme and a new plugin on the working folder and see the changes being reflected all the way on the WordPress site running inside the container. Let's get a theme and a plugin and copy them to the wp-content/themes and wp-content/plugins folder respectively. Installing a theme I picked one of the responsive themes from the WP Featured gallery named Vantage . Let's download and copy the theme over to the wp-content/themes folder by: Also, we wanted to cleanup the other two themes we don't need, so we will delete the twentytwelve and the twentythirteen themes by: Now, lets look at our Wordpress admin screens: Fig 6 : Theme showing up on WordPress app as you can see we have our new theme installed. Installing a plugin I picked the WP Mobile Detector Mobile plugin to be installed. Let's download and copy the theme over to the wp-content/plugins folder by: Also, we wanted to cleanup the other plugin we don't need, so we will delete the Hello Dolly themes by: Fig 7 : Plugin showing up on Wordpress app Viola! We have our theme and plugin i.e. our custom code deployed to the containerized WordPress application running under Panamax. Word of Caution Although, this article talks about mounting a folder from the host into the container, I need to caution you that this makes the container not so portable. If you move your container to a different host system, and if the folder structure on your host that you mounted changes, then your container will not work anymore. This article tries to demonstrate, the ways to make it easy to develop code for your application using the convenience and comforts of your host system or your physical system. My recommendation is to inject the code permanently inside the container while creating the image for your application.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-the-nools-rules-engine-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-the-nools-rules-engine-on-the-centurylink-cloud", "abstract": "Rules Engines can be a very valuable part of an overall business automation and decision making system. In discussing rules engines the simple view is that it is a system that uses rules, in any form, applied to data to produce outcomes. For this tutorial we will be talking about a specific implementation of the Rete algorithm (pronounced as two syllables, reh-te). The Rete algorithm, developed by Charles Forgy in 1974, is able to scale to a large number of rules and facts. For more detail on the Rete algorithm, here is a paper published by Dr. Forgy in 1982. The simple form of a rule is a pattern matching system; implementing \" when condition then action \". As data changes and the condition of the system is altered then one or more actions may be triggered. This is a super simplistic view, for an excellent in depth view check out the Drools project ( here ). This is a complex Business Rules Management System, implemented in Java, and available as open source. An excellent exercise for the reader would be to stand up the Drools system on the CenturyLink Cloud . In this tutorial we are looking at an alternative implementation of a business rules engine. Nools is a Rete based rules engine written entirely in javascript, with a target platform of Node.js. Nools was originally written for Pollenware to allow them to status offers in their market clearing events in a more dynamic way. First we need to create a server on the CenturyLink Cloud. Taking the defaults for number of CPU, memory allocation, and storage is fine. Add a public IP and allow HTTP and SSH port access. I am using the Ubuntu 14.04 server image. SSH into the newly created server. To install Node.js: You can validate that Node.js is installed by running the version command: Due to a naming conflict with the Amateur Packet Radio Node Program package the Ubuntu maintainers have renamed the standard Node.js binary from node to nodejs . This will break some of the standard Nools examples so we will \"fix\" this by: And now you should be able to re-run the version command using the binary node instead of nodejs . The last part of installing the Node.js support is to install the Node.js package manager: [ Running: Should return something that looks like: While we are going to be installing Nools and running it against a Node.js server it is also capable of running in a variety of browsers. Checking the github repository shows the following browser support: Installing the Nools rules engine We will use npm to install Nools: Check to see if you now have the _~/node_modules/nools_ directory. If so, congratulations you have successfully installed Nools! Verifying the install Great, now what can you do with Nools, or... how can I make it do anything? We are going to first run one of the canned example programs (to make sure we can) and then will build a simple set of rules and execute them. Change to the Nools examples directory: And run the following: If everything is OK you will see output like the following: Feel free to explore the \"helloWorld.js\" files to see what we just ran. While you should review and study the Nools documentation to understand all of the terms and interactions, here are a few terms that we will use in our example program. Let's create the Node.js file that will drive the rules process. You can create and run this in any directory, for convenience I am running it out of the Nools example directory. Create a file centurylink.js that looks like: If you run this file it will (correctly) complain that you haven't provided the DSL file centurylink.nools . So lets go ahead and build this file. The DSL will define one object (\"Output\") and two rules (\"Rule1\" and \"Rule2\"). Output will be defined as a text object so we can see the results of the rules evaluation. The rules will manipulate the inputs and trigger the display actions. Create a file centurylink.nools that looks like the following: Now if you run: You will get the output \"nine\". Why is that? Remember our original centurylink.js file has the line: When we ran the code this input of \"six\" matched Rule1... the action made \"six\" become \"nine\". When Rule2 was evaluated the \"nine\" triggered the write to console action, hence we saw \"nine\" printed to the console. And, just for fun, if we change Rule1 to be: And add an additional assert to the centurylink.js file: And then run the file... We now have the Nools Rules Engine answering the question \"Why is six afraid of seven?\" You now have a working installation of Nools. This tutorial has not even scratched the surface of the power and flexibility of the system, but simply has given you a starting place. Check out the Nools documentation and have fun exploring the system!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-strongloop-loopback-api-server-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-strongloop-loopback-api-server-on-the-centurylink-cloud", "abstract": "One of the key concepts in modern applications and systems is the idea of the \"API Economy\". Instead of building monolithic applications that hide their calls and data formats internally we have the concept of exposing these endpoints in a series of API's. This allows for easy reuse and the ability to build new applications by using multiple API providers and \"mashing-up\" the results. Developers have become used to using programming frameworks, such as Rails, to automate much of the boilerplate aspects of creating web apps. For instance, the Model-View-Controller (MVC) pattern expressed in a Rails app defines a clear separation of concerns and also allows a programmatic method for creating the application framework. Building web-exposed CRUD apps can not be simpler. However, as applications have become more sophisticated and require higher scaling and performance some of the earlier frameworks have become less suitable. Built for high levels of concurrency and scaling, today we are going to install a Node.js based API server that will serve as the core for further exploration in building high performance cloud applications. LoopBack is an open source Node.js framework built on top of Express optimized for mobile, web, and other devices. You can connect to multiple data sources, write business logic in Node.js, glue on top of your existing services and data, connect using JS, iOS & Android SDKs. Loopback uses a 'dual license' model. Users may use loopback under the terms of the MIT license, or under the StrongLoop License. Look at the GitHub repo for copies of both licenses . According to the maintainer of LoopBack (StrongLoop): LoopBack is a highly-extensible, open-source Node.js framework that enables you to: LoopBack consists of: For more details, see http://loopback.io/ . Visually, Loopback consists of the following modules: In this tutorial, we will be installing and validating LoopBack on the CenturyLink Cloud. First we need to create a server on the CenturyLink Cloud. Taking the defaults for number of CPU, memory allocation, and storage is fine. Add a public IP and allow HTTP and SSH port access. In addition, open port 3000 which we will use later to access some tools. I am using the Ubuntu 14.04 server image. SSH into your new server instance as root. We are now going to set up a non-root user. Go ahead and now login as your new user <emloopback-user< em=\"\">. In order to get the right permissions for the install of npm :</emloopback-user<> OK, now we can install Node.js: Check that Node.js is installed correctly by: At the time this tutorial was written, the Node.js version call returned v0.10.32 To install the StrongLoop software: This will install: We are now going to create a simple API to show how to get started with the StrongLoop tools and LoopBack. LoopBack uses Yeoman to create applications and then provide the basic framework. From the framework you can add your application logic. Start by running the Yeoman LoopBack generator: You will be asked for a directory to locate the application. Use LoopBackAPI . Take the default for the name of the application and hit ENTER . At this point the application framework will be created. We are now going to create a simple in-memory data model. The generator will ask for a model name. Use Customer . Take the default data source (in-memory db) and the defaults for the pluralization of the model for the ReST API. Every model has properties and you're going to define two properties for the Customer model. For each property you'll enter a name, choose a data type, and choose whether its required. The first property will be Name, it will be a string , and will be required. The second property will be Address , it also will be a string and required. Enter a blank property to leave the model building process. The last step is to actually run the application. It will be attached to port 3000. If you point your browser at [public IP]:3000 you will see a message that looks something like this: {\"started\":\"2014-10-20T16:20:10.628Z\",\"uptime\":854.956} . Congratulations, your API server is now active and running. For more interesting data, point your browser at [public IP]:3000/explorer. This will bring up a detailed API explorer and you will see the Customers model and can drill into the various aspects and properties of the ReST interface. This has been a quick introduction to the StrongBack LoopBack system. Through a series of simple steps using LoopBack you have created a Customer model, specified its properties, and then exposed it through a ReST API.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "how-to-deploy-a-rails-app-with-docker-using-official-images", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/how-to-deploy-a-rails-app-with-docker-using-official-images", "abstract": "The Docker official images have created a canonical way to build Docker images for any web application. In another post , I wrote about the building tool that uses Heroku Buildpacks to create Docker containers. Though the building tool made sense before Docker added the ONBUILD command to Dockerfiles, using a more pure Docker system gives you a few benefits. Let’s create a new Rails app: What does this do? To find out let’s examine the rails: onbuild Dockerfile : So, now it is clear which version of Ruby we are using, where the source code files are going, and how it runs our application. What if you are on a Mac, but need the Gemfile.lock to be tied to the Linux versions of libraries? There is a clever command you can run to do this: This starts a temporary container with a filesystem tied to your local directory so that the Gemfile.lock file that is generated will be written to your local filesystem, not just to your container’s filesystem. Neat! How is using ONBUILD better than using the building tool? For one, you can customize this a lot easier. For example, if you want to run your Rails app using the thin event based/non-blocking server instead of webrick , you can overwrite the CMD in your app’s Dockerfile after adding the thin gem to your Gemfile: (Don't forget to re-run the command mentioned above to avoid Gemfile.lock errors when re-building the image.) This kind of flexibility is what I love about using Docker.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "testing-apis-like-orchestrate-with-go", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/testing-apis-like-orchestrate-with-go", "abstract": "A little while ago Matthew posted a how-to on testing APIs like Orchestrate. It used a library called VCR to record calls made to the API and save them for later test runs. Fantastic! The only downside was that VCR exists for Ruby and Python, but not Go, which is my language of choice. This spurred me into writing DVR . For the uninitiated, VCR/DVR are libraries that can record HTTP calls from a given test, and then use the recorded calls later when re-running the test. This makes it possible to run the tests without having to call out to the actual API, which makes testing much easier. Unless the API you are testing against is public, you will have to embed some kind of credentials into the invocation. If you are using Travis then this would typically come about via secure environment variables that ensure your login isn’t accessible to the world. The problem with this is that Travis can’t allow a pull request against your repo to see those variables, so they go unset, and therefore your tests may not even run against a pull request prior to merging. DVR can help with this situation by removing the private credentials from Travis altogether. Instead, it runs against the recorded requests in the DVR archive which is fast and doesn’t rely on any shared secrets. Since DVR completely captures and intercepts the request, it removes the need for a network connection. This means that your tests will be able to pass, even if a backend service is having issues. You can run the tests on an airplane, and know that you won’t get a random build failure. To run, sometimes a test requires a specific permission outside the access of a developer. For example, a call to spin up new machines in an OpenStack cluster might be something users do in production, but not from development accounts. With DVR, the interaction can be recorded once, and then the recording can be used when testing, working around the need for every developer to have an account capable of starting a new instance. DVR can be use with go 1.1 or higher, and can be downloaded via the following command: go get github.com/orchestrate-io/dvr Then create a test that uses an HTTP call. As an example this test: Since the servers at example.com return a Date header we can use that to evaluate when the request was processed. A normal run looks like this: Now, if we want to record we can pass arguments to the test call (note that we need to make a testdata directory first since that is where the archive file will be stored.) In this run the call will still be made to the server, however the results are saved in testdata/archive.dvr . And now, we can run the test again, only this time we use the archive rather than making calls directly to the example.com. Note that the time displayed in this test is exactly the same. In some more complicated examples you will need to swap out an http.Client object with the RoundTripper returned from dvr.NewRoundTripper() so that the HTTP calls will be intercepted, but the simple import in the above example handles all cases that use the default HTTP client. There are extensive godocs for the package, and it is thoroughly tested as well. Happy coding!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-on-the-mac-without-boot2docker", "author": ["\n              Lucas Carlson\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-on-the-mac-without-boot2docker", "abstract": "As a developer, I'm generally pretty happy with my Mac as a development machine. As I spend more time with Docker though, I certainly wish that I could run my containers natively on the Mac. Unfortunately, OSX is not Linux and it simply doesn't have the kernel features that are required to run Docker containers. As a result, we Mac users typically find ourselves running a Linux distro in a VM in order to get our Docker fix. The boot2docker tool makes this about as easy as it can be by provisioning a Tiny Core Linux virtual machine running the Docker daemon and installing a Mac version of the Docker client which will communicate with that daemon. The end result is that it (almost) feels like we're running Docker containers natively on the Mac -- the client tool runs in the local terminal and transparently interacts with the Docker server running inside the VM. For most people boot2docker is the perfect tool for working with Docker on the Mac (or Windows for that matter). There are, however, a few scenarios where boot2docker doesn't work. Here at CenturyLink Labs we've spent a lot of time recently developing against some of the features unique to CoreOS . So I'm running CoreOS instead of boot2docker's Tiny Core Linux as my Docker host. Similarly, you may already have an existing Ubuntu VM with Docker that you'd like to use. In these cases, you're typically stuck having to log into your VM and executing your Docker commands there. So what do you do when you want that native-ish experience on your Mac but you're running a Docker host that doesn't work with boot2docker? Let's have a look. The key to making this work is recognizing that the Docker client does all of its interaction with the Docker daemon through a RESTful API. Any time you use docker build or docker run the Docker client is simply issuing HTTP requests to the Docker Remote API on your behalf. So the trick to getting a Docker client running on your Mac to interact with a Docker host in a virtual machine is to make sure that the API is accessible and the client knows how to find it. This is work that boot2docker will do automatically but I'll explain how to set it up manually in the sections below. In most installations the Docker API is configured by default to listen on unix:///var/run/docker.sock which only allows local connections by the root user. The first step is to bind the Docker daemon to a TCP port so that the client (running outside the VM) can interact with it. Note: This is most definitely NOT something that you want to do in a production environment as it effectively gives full Docker daemon access to anyone who can reach that port. However, there should be little danger in doing this on a VM which is only accessible to your local machine. To do this in CoreOS (or any other platform where the Docker daemon is being managed by systemd ) you'll need to create a file named /etc/systemd/system/docker-tcp.socket with the following contents: Then enable the socket binding by issuing the following commands: Note: When executing the steps above you may find that the docker service starts again immediately after the systemctl stop docker command. If this happens, the attempt to start the docker-tcp.socket may fail. If you experience this, try stopping any running containers you have before executing these steps. There are instructions on the CoreOS site for doing this set-up automatically via cloud-config in case you don't want to manually set it up every time you start a new CoreOS instance. On Ubuntu you can edit the /etc/default/docker.io config file and add the following line: Then restart the Docker daemon: You can check that your new port binding is functional by running the Docker client with a -H flag that points to the TCP port: If you're using some other platform or manually starting the Docker daemon, there are some general instructions on the Docker site for binding to a different port. Regardless of which method you use to set-up the binding make sure you use port number 2375 as this is the port that has been registered with the IANA specifically for the Docker API and should help ensure that you don't run into conflicts with other services you may be running. Now that you have the Docker daemon listening on a TCP port the next step is to make that port visible to your Mac. If your virtual machine was set-up with private or host-only networking you can skip this step -- the Docker port should already be accessible on the private network. However, if you are using NAT networking (which is the default when using VirtualBox) you'll need to make sure that the Docker port is forwarded from the virtual machine to the host operating system. There are a few different ways to configure this mapping which we'll discuss below (all the instructions that follow assume that you're using VirtualBox as your VM platform). The first option is to do it from the terminal with the VBoxManage command: This invokes the VirutalBox command controlvm to add a port forwarding rule to the VM named vm_name . The rule is named docker , the protocol is set to tcp and port 2375 on the host is forwarded to port 2375 in the guest OS. You can find the names of all your virtual machines by using the VBoxManage list vms command. Note: The double commas ,, in the command above are intentional and used to indicate that we're omitting some optional parameters. Alternatively, the port mapping can be managed via the VirtualBox GUI. Simply open the settings for the virtual machine, navigate to the Network tab and click the Port Forwarding button. From there, insert a new rule that matches the one named docker shown below Finally, if you are using Vagrant to manage your virtual machines, you can add the following port forwarding instruction to the configure block in your Vagrantfile : You can test your port forwarding by opening a terminal on your Mac and issuing the following curl command: If you get back a JSON response containing version information about your Docker installation you'll know everything is working properly. With the Docker API now accessible to your Mac, the next step is to install the Docker client application. The easiest way to get the Docker client is to use the Homebrew package manager: Pay attention to the version of the Docker client that is installed by Homebrew. It is important that the client version match the version of Docker that is running in your virtual machine. If they don't match you may need to update your Homebrew formulas or the version of Docker installed inside the VM. With the Docker client installed, you should be able to test the installation by issuing the following command: The -H flag instructs the Docker client to connect to the API at the specified endpoint (instead of the default UNIX socket). If your VM is using private/host-only networking, you'll want to substitute your VM's IP address for localhost in the command above. At this point you should have a working Docker client which is able to communicate with the Docker API running inside a virtual machine. The only downside is that the -H flag is now needed for each docker command you issue. The good news is that you can omit the -H altogether by simply setting an environment variable that is read by the Docker client. From the Mac terminal, do the following: Again, if you're using private networking, substitute your VM's IP address for localhost in the command above. Now try the docker ps again, but without the -H flag this time. To make this environment variable permanent, simply add the line above to your ~/.bashrc file. When you installed Docker with Homebrew you may have seen a message along the lines of: Using the Bash completion script isn't strictly necessary but does make the Docker client a lot nicer to use on your Mac. If you source this file you'll get tab completion for most of the Docker commands as well as image and container names. To make sure Docker tab completion is available any time you open a new terminal session you can paste the following three lines into your ~.bashrc file: If everything has gone smoothly you should now have a boot2docker-like experience on your Mac with the Docker host of your choice. Enjoy!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "store-omniauth-user-data-with-twitter-login", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/store-omniauth-user-data-with-twitter-login", "abstract": "A key step in developing an application is handling user authentication and datastore. The use cases of your data may vary, but the process through which the data is stored and accessed is largely the same. OmniAuth is a Ruby gem with several strategies, or provider-specific gems, that provides authentication methods for many systems, such as Facebook, Google, GitHub, etc. Each strategy is a Rack middleware, so it’s very easy to integrate into your web framework, whether that’s Rails , [Sinatra, Padrino , or even a less popular one like [Lotus]. Additionally, the Orchestrate Ruby Client is comprehensive, well-documented, and framework-agnostic. In this tutorial, we’ll use Sinatra, a popular Ruby web micro-framework and domain-specific language, with OmniAuth Twitter to build an online community titled Twomnistrate where each user shares their favorite phrase as a profile element, stored in Orchestrate. First, we set up our file/folder directory in a way that Sinatra understands by convention. Our main application file will contain all the Ruby code in this tutorial– app.rb. We’ll include all [ERB templating in our views folder. At the top of our main Ruby file, we require the necessary gems for our project. Note that this assumes the two gems are already installed on your system. Generally, I use Bundler to manage my application’s gems, but I won’t include the code for that in this tutorial since there are alternatives, such as dep and just self-managing with gem install . Then, we enable one of Sinatra’s many built-in settings, sessions, a fork of Rack::Session::Cookies , “simple cookie based session management”. We will use sessions to keep track of whether a user is logged-in or not. Next, we setup the Orchestrate clients. The Orchestrate Gem provides two interfaces currently, the method client and the object client. The method client is a solid but basic interface that provides a single entry point to an Orchestrate Application. The object client uses the method client under the hood, and maps Orchestrate’s domain objects (Collections, KeyValues, etc) to Ruby classes, and is still very much in progress. In this project, we will use both interfaces, as the object client is simpler and includes everything we need except for the delete operation. Note that you must define api_key with the API Key of your Orchestrate application, which can be created or revoked from the Orchestrate Dashboard on a per-application basis. I always recommend defining any API keys on a shell-specific level instead of a code-specific level via an environment variable. You can find more information on working with environment variables in Ruby in documentation for the ENV class . The last thing we need to initialize before writing any application code is OmniAuth with the Twitter strategy. You need to register your app with Twitter. This is easily done – just head over to Twitter Developer Console and login using your Twitter credentials. Then, click on the ‘create a new application’ button and fill in the form. In the callback URL field, you need to append /auth/twitter/callback to whichever URL you used in the website field. If you haven’t configured a domain yet, I recommend that you use ngrok to create a secure introspect tunnel to localhost. For example, ngrok 4567 will create an public subdomain tunnel to your address localhost:4567 , which is the default address for Sinatra’s server. At this time, our app only needs read-only permissions from Twitter, but this can be changed in the “permissions” tab of your app in the developer console. Before progressing to routes and application logic, we must declare a top-level helper method. Helper methods are available in all route handlers and templates. The logged_in? method checks if the user is logged-in via a session variable. Since the method name ends in a question mark, it must return a boolean value. When the user is logged out, we will define session[:authed] nil which is falsey in Ruby and when the user is logged-in, we will define it true. Our application will start off with a splash page. When the user is logged out, this will be serve as their homepage, but when the user is logged-in, /all will take over this role. On /all , we map the users collection to an array of arrays in the format [username, phrase] to @data . Then, we loop through the instance variable in ERB templating to generate an HTML table. On /me, users can update their phrase. Unlike /all , this route is restricted to logged-in users. Looking back at the form on /me, the method attribute is set to POST; therefore, an HTTP POST request is sent to /me with the parameter phrase on submit. If the user isn’t already in our Orchestrate collection and the phrase parameter is not empty, we create it with the Twitter username as it’s key and phrase as one of its values. If it does exist but the phrase is empty, we delete the user from the users collection since phrase since phrase is the only value for any given key in the case of our app. Since the delete function is only present on the method client, we use that here. Lastly, if there is already a phrase associated with the user but another one is submitted, we update the value in the collection. To logout, session[:authed] is defined nil. After authenticating, Twitter will redirect the user to our callback URL set earlier in the Developer Dashboard. Here, we will define session[:authed] true and save the username to session[:username]. In the case of authentication failure, the user is directed to /auth/failure and the full error message is printed. In practicality, you’d want to mask this error message with a coated page via erb :error and log it in full to the filesystem , but this method makes debugging in development much simpler. This application in full has been released as a public, open-source repo on GitHub and deployed to Heroku . In the future, you can create additional collections and interact with them in the same way described here. You can also utilize the power of Orchestrate and its gem to execute powerful search queries by value and order results by a number of factors, including value or timestamp. Additionally, once user authentication is setup with Twitter, you can use the Twitter gem to perform operations using their Twitter account using the accesstoken and accesstoken_secret found in request.env[‘omniauth.auth’] on /auth/twitter/callback , but that is out of the scope of Orchestrate. As you can see, Orchestrate provides a developer-friendly, powerful interface to your application data in an inexpensive service, and integrating user authentication and datastore with it is no different!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "open-data-import-enables-us-executive-order-search", "author": ["\n              Originally appeared on the Orchestrate Blog\n            "], "link": "https://www.ctl.io/developers/blog/post/open-data-import-enables-us-executive-order-search", "abstract": "There’s a lot of chatter about executive orders, where US presidents use their authority to issue directives without congress passing a law. In an effort to understand, I set out to use open data and Orchestrate to shine a light on executive orders of the past three presidents. Between leveraging Orchestrate’s full text search and the flexibility of the Hapi.js framework , I was impressed with how quickly the basic prototype of the site came together with minimal code. The executive order search app is up on Heroku . Search a word or phrase and it quickly runs three Orchestrate queries behind the scenes. Then the count for each of the last three presidents is updated based on the number of executive orders that matched the search. If you want to dig deeper, click the details button to see the dates and brief descriptions of each executive order. The Federal Register maintains a downloadable reference of all executive orders since 1994, available in several formats. This source was sufficient for titles of the executive orders, dates, URL of the publication and a few other bits of metadata. However, the real meat is in the full text of each order. For that, I needed to download the basic text version of nearly 800 executive orders. Once I combined each executive order’s metadata with the full text, I pushed each to Orchestrate as an item in a single collection. All data in Orchestrate is indexed for search, so I was set. The heavy lifting to determine the number of executive orders is all done within the database. It’s important to note that, even though this application appears to only be front-end JavaScript, there’s a thin-yet-important server-side layer in place. Even if it were technically possible to call Orchestrate with client-side JavaScript, it would not be advisable, as it exposes your API Key. In the wrong hands, your API Key is all that is needed to inject inaccurate data or even delete all of your data. For my backend, I chose Node.js and the Hapi framework because they’re lightweight, familiar and great for creating APIs. Since this server-side component is essentially just a way to keep my API Key hidden, an API specific for my front-end is really all I need. With the executive orders already loaded into Orchestrate, the only feature this app consistently uses is Search. Each time a user searches a word or phrase, the backend service makes three quick calls to Orchestrate in succession, one for each President. Since Heroku is within the same data center as Orchestrate (in this example, Amazon US East, each search averages 11ms. Even with three separate calls, the results are extremely fast. My Lucene syntax query for each call looks something like this: value.text:arkansas AND president:clinton And since I don’t need the details of each executive order, I pass a limit=0 parameter so that I receive the total_count and little else. The code notes each response and stores the number for the search completed with all three presidents. Once the backend has all three results, it spits out a simple JSON response for the search, like so: {\"obama\":2,\"bush\":0,\"clinton\":2} Much of the interaction aspects of the application happens in the front-end with jQuery. However, you can look at the Orchestrate calls within the app.js file. The entire project is on GitHub . What data do you want to search? Login to the Dashboard and explore Orchestrate’s features now. Photo by NobMouse", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "temperature-logger", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/temperature-logger", "abstract": "You’ve heard about this Internet of Things (IoT) trend and wanted an excuse to try a project. Well, me too. Actually, I have a goal to create a series of low cost climate/environment modules that capture various types of data like temperature, humidity and more. I want to take all this data and put it in the cloud where I can eventually build out dashboards, alerts and more. Any good programmer knows to make it work before you make it scale. That’s where this post comes in. It's a simple IoT project using four things: You can download all code from this tutorial from the GitHub project . The diagram below shows how the LM35 Temperature Sensor is connected to the Arduino Uno board. I used a breadboard to connect all this together. But I simplifyed the diagram so that you know what is connected to which pin. The LM35 has 3 pins. Now we can write our Arduino code to read that value. Arduino requires a computer to interface via a serial port, such as over USB. Again, we’re keeping this example simple. The Arduino code is straightforward as given below: You will notice in the loop that every 10 seconds we are printing out the temperature value read from the Analog Pin (#0) . If you run the Serial Port Monitor that comes with the Arduino IDE, and if the Arduino is powered up and connected as per the diagram shown, the Temperature value is printed on the Serial Monitor. Once the data appears we know that the Arduino setup is correct. All we need to do now is write a client program on the PC that interfaces with the Arduino, reads the values via the serial port, and then pushes them to the cloud database service. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You'll need it to access CenturyLink Cloud products. Our first step is to deploy a new CenturyLink Cloud virtual server. Follow the steps below. On the left side menu, click Infrastructure and then Servers . On the left-hand side of the server panel, click on the region for the server we will provision. Your server provisioning request will enter the queue. You can watch the progress of your request on the screen. Your server is provisioned when the status of all tasks in the queue is complete. After your new server is provisioned, in the CenturyLink control portal, click Infrastructure on the left side menu, and then click Servers . Type \"27017\" in the blank box to open up the MongoDB server port. Click add public ip address . From a shell on your local machine, connect to your new server with the following command. Replace \"YOUR.VPS.IP\" with your server's public IP address. Install the MongoDB server software by running the following commands. With your favorite text editor, open /etc/mongod.conf . Look for the line that begins \"bind_ip\" and comment it out. The top of your file should now look like this: Start the MongoDB service by running the following command. I decided to use Python to interface with the Arduino connected to my computer’s serial port. Here’s the plan: First, install the pySerial and PyMongo libraries so you can read from the Arduino and write to MongoDB in Python. Next, here is the code needed to read the temperature and record it in MongoDB. Be sure to configure the parameters near the top of the file to match your configuration. Create a new file in your favorite text editor called templog.py and edit it to look like the following. Finally, run the script with the following command: Note: If the Arduino Serial Monitor is running, your python script will be unable to access the serial port. Run only one of them at a time. When using MongoDB, we first connect to the server, then we select a database and a collection in that database. The data is logged with the collection.insert_one() call. The fields being stored are the temperature in degrees Celsius, the timestamp of the measurement, and the location. The final step is to validate if our data is being transmitted successfully and stored in MongoDB. The Python code returns a confirmation message each time it checks the temperature, but to be extra sure we can go to the MongoDB client interface.. From the shell prompt on your virtual server, connect to MongoDB with this command: Run db.templog.find() at the prompt. It should look like this: Arduino makes electronics prototyping fun. With languages like Python and a database like MongoDB, the process of collecting, transmitting, and saving the data in the cloud is made simple, too.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "installing-coreos-rocket-on-the-centurylink-cloud", "author": ["\n              John Purrier\n            "], "link": "https://www.ctl.io/developers/blog/post/installing-coreos-rocket-on-the-centurylink-cloud", "abstract": "In December 2014, a key player in the Docker ecosystem, CoreOS, announced \"Rocket\", a competing container runtime to Docker. They spelled out their motivations for creating Rocket in this blog post . While this caused some controversy amongst the Linux Container community we are simply looking at how to deploy and evaluate Rocket in this tutorial. Rocket is an alternative to the Docker runtime, designed for server environments with the most rigorous security and production requirements. Rocket is oriented around the App Container specification, a new set of simple and open specifications for a portable container format. According to CoreOS the important factors in the design of a container are: As we have done previously with Docker, in this tutorial we will show how to install and run Rocket on the CenturyLink Cloud and use it to deploy containerized applications. Note that Rocket is in a very early and immature state so we need to install component by component, the CoreOS team is soliciting input and contributions by the community. CenturyLink Labs recently did a podcast with Brandon Philips, the CTO of CoreOS and you can hear his view on Rocket, Docker, and the future of containerization . We will create a standard server with the following characteristics: Standard default of 2 cores and 4GB of memory, with Ubuntu 14.04 (64 bit) installed as the base operating system. Create a public ip and open these standard ports: In addition open port 5000 , this is the port that our Rocket contained application will listen on. Login to your server and ensure that you are running the newest base OS bits: If you check the OS version version via: You should see something like: You now have a directory called \"go\" under your home directory. Move this to /opt which is the Go default (or else you need to adjust some of the base defaults). For everything to work like it should, you will need to do the following to complete your Go setup. Create two system variables called GOROOT and GOPATH. These two variables will be used by Golang itself when building your application. You will also need to create a directory to store any dependencies being pulled. Run the following to make these updates persistent after reboots: Create a working directory ~/rkttest and switch (cd) to it. In order to validate the installation we will create a simple \"Hello World\" program. Create a file called goworld.go and enter the following: After saving the file you can run the program by: You should see \" Hello World \" printed to the screen. Go ahead and delete the \"goworld.go\" program as we will be creating a more complex version of the app later. Move back to the home directory. Grab the Rocket release from GitHub: You can validate the Rocket install by: And you should see something like: Move back to the home directory. Next we will be downloading the tools necessary to build the containers that Rocket will deploy. The containers are defined by the _App Container Specification_ that can be found on GitHub here . The \"App Container\" defines an image format, image discovery mechanism and execution environment that can exist in several independent implementations. The core goals include: To achieve these goals this specification is split out into a number of smaller sections. Download the current App Container Specification (ACI) to get the actool program: Change to the new spec directory: Execute the following to build the tools: Add to the path and make persistent: Move back to the home directory. Next we will create yet another \"Hello World\" app in Go in the rkttest directory. This one is a bit more complex, as we will be making it accessible through HTTP. Create a file called _rktworld.go_ and enter the following: Build the static binary by: You will now have two files in your directory: rktworld.go and the binary rktworld . In order to run the container we need to build a manifest file. Create a file called mainifest.json and enter the following: Validate the newly created mainfest file by using the actool : You should see output like: ] Finally, we are going to build the actual container application. To do so we will execute the following steps: And you should see output that looks like: In order to start the rktworld container application, run the following: Validate by logging into a second terminal locally and running: You should see: Finally, point your browser to http://{PUBLIC_IP}:5000 and you should see: ] Congratulations! You are now able to create and run ACI container applications via CoreOS Rocket on the CenturyLink Cloud.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "deploying-to-kubernetes-with-panamax", "author": ["\n              Brian DeHamer\n            "], "link": "https://www.ctl.io/developers/blog/post/deploying-to-kubernetes-with-panamax", "abstract": "Panamax is a Docker GUI that makes deploying multi-container apps as easy as one-click. The project recently introduced support for remote deployments. This feature allows the user to take an application template from their local Panamax installation and send it to a remote machine (or cluster of machines) for execution. Remote deployments are handled by a small, containerized agent which runs in the remote environment and listens for requests from the Panamax client. When composing applications in the Panamax UI, all of the containers are executed within a single, local CoreOS instance which is set-up as part of the Panamax installation process. For remote deployments we didn't want to assume that everyone was going to be using CoreOS/Fleet so we had to find a way to generalize the communication between the Panamax client and the remote agent but allow the agent to communicate with whichever Docker deployment technology was being used in that environment. We settled on a pluggable adapter model whereby the end-user can choose whichever implementation meets their needs (or even build their own ) and it will be guaranteed to work with the Panamax remote agent. At the time of publication, there are three adapters available: Fleet , Marathon , and Kubernetes . The Fleet adapter behaves much like the local Panamax installation does -- containers are scheduled to a cluster of CoreOS machines via Fleet's distributed init system (where \"cluster\" may simply be a single machine). The Kubernetes adapter interacts with Google's Kubernetes container orchestrator. The point of Kubernetes is to manage the deployment of containerized applications across a cluster of hosts so it's a perfect match for Panamax's remote deployment feature. The remainder of this article describes how the containers in a Panamax application template are translated by the Kubernetes adapter for deployment into a Kubernetes-managed cluster. To see how the Panamax Kubernetes adapter translates a template into Kubernetes-specific artifacts, let's look at an example application template: This template describes an application comprised of a WordPress container which is linked to a MySQL container. Both containers receive some configuration data via environment variables and establish port mappings. When using Kubernetes to to deploy an application, there are three different entities that can be created: Pods , Services and Replication Controllers . The following sections describe how the application above maps to these concepts. A Pod represents a group of containers that are deployed together on a host. Typically, if you are deploying an application into a cluster, you don't necessarily want all your containers to land on the same host unless they need to share a local resource or have some other colocation requirement. So, for our example application, we'll end-up with two separate Pods. The Kubernetes adapter will do a pretty straight-forward translation of every container defined in the application template into a Pod. Here is the JSON-encoded Pod description that will be sent to Kubernetes for the WordPress container: Most of the configuration data for the WordPress container in our application template translates nicely into the Pod definition -- the image name, environment variables and port mappings were pretty much copied as-is. The only thing that appears to be missing is the link data, but we'll get to that in the next section. Note that the name value from the application template was used in a few different places in the Pod definition: in the ID of the Pod, as the name of the container, and as a label that is applied to the Pod. The Pod ID is what you reference if you want to manage the Pod via the kubecfg command line tool: The label is used by Kubernetes as a way to select groups of Pods and becomes important when we look at Services in the next section. The Pod description created for the MySQL container would look like the following: When using Docker it's very common to have a container that needs to communicate with some other container. One of the trickiest parts of setting up an architecture with multiple dependent containers is making sure that each container knows how to find its dependencies. Mostly, this boils down to knowing the IP address and port number that you need to use in order to connect to a dependent service. When using Docker directly, this sort of service discovery is facilitated by creating links between containers. If you were to use the Docker client to start the two containers from our sample application you might do something like this: The MySQL container is started and assigned the name MYSQL . The WordPress container is started with a link to the MySQL container and that link is assigned an alias of _DB_. All that the --link flag does here is to cause the Docker runtime to inject some environment variables into the WordPress container with connection info (IP address and port number) for the MySQL container. The environment variables injected into the _WP_ container would look something like this: Each of the variable names is prefixed with the link alias that was specified with the --link flag (the use of the alias provides a level of indirection which prevents the application code from being tightly coupled to the container's assigned name). Docker already knows the IP address for the _MYSQL_container and the port number is determined by looking at the list of ports exposed by the MySQL image. With this information in hand, the process running in the WordPress container should have everything it needs to connect to the service running in the MySQL container. To support this sort of service discovery, the Panamax application template also provides a way to model container dependencies. Much like the docker run command, a link in the Panamax template is specified with the name of the container being linked-to and the alias to be applied to the link. When the Kubernetes adapter sees that a link has been specified between two containers it will automatically create a Kubernetes Service. One of the many things that the Service entity in Kubernetes handles is service discovery. A Service can be associated with a Pod and then any subsequent Pods which are started will automatically receive connection information from that Service. In the case of our example template, the Kubernetes adapter would see the link between the WordPress container and the MySQL container and configure the following Kubernetes Service: The ID of the Service is assigned the same value as the alias specified in the link definition. This ID serves as the prefix for the environment variables injected into the containers when they are started. This ensures that the names of the Kubernetes environment variables match those that would be created when using the --link flag with docker run . The selector field in the Service description is used to select the Pod (or set of Pods) that this Service is representing. Note that the \"name\":\"mysql\" selector shown here matches exactly the label that was assigned to the MySQL Pod. The label is how Kubernetes establishes the relationship between the Service and the Pod it is fronting. Once the Service shown above has been created, any subsequent Pods which are started will receive the following environment variables: The first three variables above are Kubernetes-specific, but the rest of them follow exactly the naming conventions that Docker uses when linking containers running on the same host. By using these environment variables in your application for service discovery you should be able to take containers you're running locally with Docker links and run them in a Kubernetes cluster without any changes. Kubernetes Services have additional uses beyond service discovery, but we'll cover those features in the next section. In our example application template, we have a single WordPress container connecting to a single MySQL container. This is fine for small scale deployments, but what if you were expecting a lot of traffic on your WordPress site and wanted to run a bunch of WordPress instances in order to handle the load. The application template doesn't yet provide for scaling container instances, but when you use the Panamax UI to initiate a remote deployment you have the option of specifying a deploy count for each container. For any container configured with a deploy count greater than 1 the Kubernetes adapter will create a Replication Controller instead of a Pod (in fact, a future release of the Kubernetes adapter may simply default to using Replication Controllers for all services and skip the Pods altogether -- that way you have the option to scale up later if you want). The Replication Controller is pretty much a Pod with an associated instance count. Kubernetes will do its best to ensure that the specified number of instances of that Pod are always running. If we were to deploy our sample application via the Panamax UI and set the deploy count to 3 for the WordPress container, the Kubernetes adapter would create the following Replication Controller: The only things that are really new here are the replicas and replicaSelector fields. The value of the replicas field indicates how many instances of this Pod should be running. The replicaSelector field works much like the selector we saw in the Service example above and creates the association between the Replication Controller and the Pods it is managing. Note that the Replication Controller defined above has the Pod template embedded within it -- there is no need for a separate Pod to be configured. For any container defined in the application template, the Kubernetes adapter will create either a Pod or a Replication Controller (but never both). Obviously, having three instances of WordPress running doesn't do you much good unless you have some way to load-balance your incoming requests across those instances. This is one of the other features provided by the Kubernetes Service. If you define a Service and associate it with Replication Controller it will offer clients an endpoint which, when accessed, will load-balance across the Pods managed by the Replication Controller. Whenever the Kubernetes adapter creates a Replication Controller it will also create a Service to act as the the proxy/load-balancer for the replicated Pod. For the Replication Controller shown above the configured Service would look like this: The service will expose an endpoint on port 8000 which will load-balance all requests to port 80 on the replicated WordPress Pods. The Panamax Kubernetes adapter tries to make the deployment of an application template to a Kubernetes cluster as similar as possible to the execution in the local Panamax environment. However, there are some places where Kubernetes' specific rules/features prevent a perfect translation of the application template. Some of the things to look-out for are documented below: The work we did on the Panamax Kubernetes Adapter represents our first pass at translating Panamax templates for deployment with Kubernetes (and really our first experience with Kubernetes). Undoubtedly, there are things that we could have done differently or improved. If you're a Panamax user and are at all interested in Kubernetes as a deployment target we encourage you to look at the code , submit issues , or make pull-requests . If you're interested in creating adapters for different orchestration systems, we've also created a Panamax Adapter Development Guide which explains how to implement an adapter which will work with the Panamax Remote Agent. If you end up building your own adapter, please drop us a line -- we'd love to hear about it!", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "monitoring-docker-services-with-prometheus", "author": ["\n              Rupak Ganguly\n            "], "link": "https://www.ctl.io/developers/blog/post/monitoring-docker-services-with-prometheus", "abstract": "With the advent of the 'micro-services' architecture and the evolving trend for using Docker, monolithic applications are being broken up into smaller and independent services. The idea is to keep the services small so that small groups of developers can work on them, upgrade or patch them quickly, and build & release them continuously. Although that vision is promising, it introduces complexity as the number of services grow. With that also grows the need to monitor these services around the clock, to maintain the healthy functioning of the application. Here we look at Prometheus , and demonstrate its capabilities by using it to monitor Panamax and its containerized services. Written in Go, Prometheus, is a open-source monitoring service and alerting toolkit build at SoundCloud. It boasts of a variety of features and components that made it really interesting for me to evaluate it internally at CenturyLink Labs. Prometheus was written from the ground up, based on real use cases and experiences at SoundCloud, designed to tackle real problems faced in real production systems. [Pic Courtesy: prometheus.io] In the heart of the system is the Prometheus server , backed up by a local database server. Prometheus is based on a 'pull' mechanism, that scrapes metrics from the configured targets. However, for short-lived jobs, it provides an intermediary push gateway for scraping metrics. It also provides PromDash , a visualization dashboard for the collected data, an Expression browser with a query language to ease filtering of data, and an AlertManager to send notifications based on triggered alerts based on an alert rules engine. You can find more resources on their media page. So to give Prometheus a whirl, I decided to monitor Panamax and its services. The goals were: Note : I am assuming that you have a working Docker installation and a working installation of Panamax on your machine, if you want to follow along. The goal was to run Prometheus as a Docker service although it can be installed as a binary from the available releases , or built from source . Luckily, all the Prometheus services are available as Docker images. In preparation to run Prometheus, we have to create a configuration file named prometheus.yml that allows setting up of jobs and targets for scraping. Create a folder named prometheus and create a new yaml file named prometheus.yml with the contents shown below. The global section describes and overrides some defaults. The labels section, attaches a specific label to this instance of the Prometheus server. The rule_files section lists all rule files ( recording or alert rules ) that Prometheus needs to load and process. We will look at the rule file described here at a later time. The scrape_configs section, describes the job(s) that Prometheus needs to process. In our case, we have a job named panamax , with some config items, including the target_groups sub-section. We add a target that points to the cAdvisor address running as part of the Panamax application. Note : The IP address 10.0.0.200 is my Panamax address also aliased as panamax.local . If you are already using cAdvisor , version 0.11.0 and above has Prometheus integration . Prometheus can leverage the host and container level metrics exposed by cAdvisor. To see the metrics exposed by cAdvisor, go to: Since in the prometheus.yml configuration we specified the target as the cAdvisor address, Prometheus will automatically look for the /metrics endpoint, to expose the metrics. If you looking to capture host and container level metrics, Prometheus also provides a container-exporter , that can be run side by side to your other docker services. Many other exporters and third-party integrations are also provided by Prometheus. In this case, the target_groups section of the prometheus.yml file will have a target that points to the address of the 'container-exporter' like so: This tells Prometheus to leverage the host and container level metrics exposed by the 'container-exporter' service. To see the metrics exposed by the 'container-exporter' service, go to: Note : You only need one target that exposes metrics for your application. In our case, we are using cAdvisor for collecting host and container level metrics for Panamax. Various alert rules can be configured within Prometheus, to detect events that happen based on metric counters that Prometheus tracks. To send notifications based on these alerts, the AlertManager component is used. An AlertManager instance can be configured via the alertmanager.url flag while starting Prometheus, thus enabling notifications to be sent when alerts are triggered. To start off, I wanted to set up a simple alert that detects if Panamax application is down, and notifies me on my Hipchat room. To configure an alert in Prometheus, we need to create an alert rules file. Create a new text file named alert.rules with the contents shown below. Here we are setting up a alert named pmx_down , which specifies a condition up == 0 using the IF clause, and the FOR clause specifying that the alert will be triggered after 5m that the condition remains true. In other words, if Panamax is down for 5m, this alert will be triggered. The WITH clause attaches an additional label of severity=\"page\" to the alert. The SUMMARY and the DESCRIPTION clauses are self-explanatory, but we will soon see that the text in the SUMMARY clause is what gets written as the notification text on Hipchat. Adding an alert as we did above, sets up Prometheus to trigger an alert when conditions are met, but to send notifications, Prometheus relies on the AlertManager component. So, lets set that up so we can send notifications to Hipchat, when our alert is triggered. To do so, we need to create a configuration file. Create a new text file named alertmanager.conf with the contents shown below. We are setting up a notification_config for Hipchat, with some specific keys required by Hipchat. The send_resolved setting is used to trigger an additional notification when the alert condition is 'resolved'. in our case, it would be when the Panamax application is back up. The aggregation_rule sets up an attribute repeat_rate_seconds which configures the notifications to be repeated for the specified duration in seconds. In our case, we want the notifications to be repeated every 2 hours while the Panamax application is down. The notifications are stopped when the alert condition is no longer met or the alert is manually silenced from the Prometheus UI. Now that we have a configuration for the AlertManager, we can run it is as a container service, passing in the alertmanager.conf via the config.file flag. And, we can see our container running: Note : We will record the port where the AlertManager is running as we need it in the next section. With the prometheus.yml setup, the metrics endpoint setup, the alert rules setup and the AlertManager configuration setup, we can finally run the Prometheus server as a container service. As soon as the service starts, it will start scraping the metrics, and make it available on the Prometheus UI. We expose the Prometheus UI at port 9090, and volume mount the local prometheus.yml file & alert.rules file to /etc/prometheus/prometheus.yml , where it is picked up by Prometheus. We also pass the configuration file path via the config.file flag and pass the alert manager url via the alertmanager.url flag. Note : The IP address http://192.168.59.103 is my Docker Host address. And, we can see our container running: This completes our setup and configuration, resulting in running Prometheus server and the AlertManager, both as container services. Now, that we are running Prometheus and scraping metrics off Panamax application, we can head over to the Prometheus UI, to visualize the metrics and query them. Note : There is a separate component PromDash , which is more elaborate Prometheus dashboard, that I talk about at the end of the article. The Prometheus UI is available at your Docker Host address on port 9090. Click on the 'Graph' menu item to open the Expression Browser. The above screenshot shows you the metric counters that were picked up by Prometheus exposed by cAdvisor. In the query field, paste the following query, and hit 'Execute'. Then click on the 'Graph' tab, to see the visualization of metrics for memory_usage_bytes counter for the PMX_UI container. Next, click on the 'Add Graph' button, and paste the following query, and hit 'Execute'. Then click on the 'Graph' tab, to see the visualization of metrics for memory_usage_bytes counter for the PMX_API container. Next, click on the 'Add Graph' button, and paste the following query, and hit 'Execute'. Then click on the 'Graph' tab, to see the visualization of metrics for memory_usage_bytes counter for the WP container. The WP container was actually started by Panamax. Here you can see 5 instances of the WP container starting/stopping at different points in time. Note : You can toggle the 'duration' parameter to zoom in/out on the data points across time. Click on the 'Status' menu item to see the runtime/build information, configuration, rules, targets and startup flags that are active for the Prometheus server. On the Prometheus UI, go to the 'Alerts' menu, and you will see the pmx_down alert inactive and green in color. If you click on it, you can see the actual alert condition that we had setup earlier. To trigger this alert we need to shutdown Panamax. So, lets do that now by doing panamax pause . If you click on the 'Alerts' menu, you will see that the alert has now become active, is red in color and the State shows as firing . You can also open up the AlertManager at http://192.168.59.103:9093 , to see the alerts that have been triggered. And, view the API endpoint for the AlertManager at http://192.168.59.103:9093/api/alerts . Note : The IP address http://192.168.59.103 is my Docker Host address. The expectation is to receive a notification on Hipchat and we do so immediately as shown below. The notifications are repeated every 2 hours till Panamax comes back up. When Panamax is back up, the alert is deemed resolved and a new notification to that effect is sent to Hipchat. The alert status is inactive and green in color again. Update : Based on a few requests, I have created a docker-compose.yml file in Lorry.io for the deployment of the above setup. Note, that the PromDash setup is not included as it needs some manual setup steps. Prometheus also comes with a graphical dashboard named PromDash. Let's setup PromDash as a container service. PromDash needs a database to store its data, so let's create a local file based Sqlite3 database for simplicity. Now, that we have our database created, we need to configure the database with the schema. Since PromDash is a Rails application, we will just run the db migrations. And, now that the database is all setup, let's run PromDash UI as a container, on port 4000. And, we can see our container running: We can now head over to http://192.168.59.103:4000/ to use the PromDash UI. Note : The IP address http://192.168.59.103 is my Docker Host address. Without getting into details, here is what my PromDash UI looks like: In summary, we looked at running a Prometheus server, configured a metric scraping target, created alerts, enabled notifications to Hipchat, and ran an AlertManager. We then looked at the Prometheus UI and PromDash, to visualize the collected data and performed query operations on them. We triggered alerts and got notifications on Hipchat, by shutting down our monitored application. Prometheus is an excellent monitoring service and alerting toolkit, that could help you better monitor your applications and its containerized services. We have just scratched the surface in this article, but you should find Prometheus able to handle most monitoring scenarios.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tomcat-dynatrace-monitoring", "author": ["\n              Bob Stolzberg, Principal Engineer\n            "], "link": "https://www.ctl.io/developers/blog/post/tomcat-dynatrace-monitoring", "abstract": "Spinning up a JBoss or Tomcat server can be time-consuming, and worse; it can interrupt your flow when all you really want to do is jump into a project. At CenturyLink, we decided to automate the entire process of spinning up a server, installing and configuring both the software and dependencies. We took it one step further by providing an easy way to automatically deploy Dynatrace , to monitor your new Java app servers, free for 30 days. The workhorses behind all of this automated magic are our Blueprints , which provide the capability to automatically install and configure applications and servers and even build out whole environments on our Cloud platform. A Blueprint is basically a repeatable workflow. Blueprints allow users with little or no in-depth, technical knowledge to quickly deploy different infrastructure and applications. There are many open-source, totally-free applications in our Blueprint Library (and more on the way.) So, for example, let's say a developer (we'll call him Samson) wants to create a Tomcat server in our cloud and then add performance monitoring to it. Samson logs into our Control Portal and finds the “Install Tomcat on Linux x64” Blueprint. He clicks on the Blueprint, enters some configuration through a wizard (as illustrated below) and then deploys it. The Blueprint runs and Samson’s end result is a fully-configured Tomcat server. Aha!  Samson can take advantage of the free 30-day trial Dynatrace APM Blueprint to get some insight into how his application is performing. He wants to find out where his app's problems and bottle necks are, so he goes back to the Control Portal and finds the “Install Dynatrace Free Trial All In One” Blueprint. Just like the Tomcat Blueprint, Samson goes through the wizard to enter a few lines of information to configure the server and Dynatrace application and register for a free 30-day trial license. Once he deploys, it takes only a couple minutes for the Blueprint to run. Samson’s end result is a fully-functioning performance monitoring solution configured on a new server, ready-to-use. Besides the obvious time savings, what are some of the greatest benefits that Samson gained from using Blueprints together? Low cost and open-source Many of the Blueprints are made for partner products that are open-source. That means all Samson has to pay for is the Cloud space provided. Many other partners bundle a free trial of some kind in with their Blueprints. In the case above, Dynatrace provides an unrestricted, free 30-day trial license for use on the CenturyLink Cloud. A Blueprint that configures your application, server or environment The Blueprint workflows are set up to install and configure a product based on best practices. That means Samson can easily deploy servers and monitoring without doing a \"deep-dive\" just to make sure the configuration is correct. One-stack delivery The Dynatrace Blueprint was developed as a one-stack workflow that delivers an entire process with a few clicks. This means far less time and headaches when trying to implement a complex and customized monitoring system for the Tomcat server or other applications you may run. Repeatable and customizable Every Blueprint allows a customer to deploy it as many times as needed. Each Blueprint acts as a template to provide a standard design or workflow. However, Samson could also easily copy and customize the workflow however he wants to accommodate his specific needs. For example, if the server included in the existing Tomcat Blueprint is too small for Samson’s repeatable process, he can create his own Blueprint with the right-sized server and include the Tomcat installation automation. Manual or automated options Most people use blueprints through the Control Portal, meaning they manually start the deploy process each time they want to create something new. But, there is also the option to use an API to support an automatic, continuous deployment. Samson could use tools we provide, like BPformation , to manage his Blueprints. Additionally, he can use other tools like clc-sdk to turn the install and configure process into one API call. Risk reduction Using Blueprints to install and configure means there are not as many chances for code to be compromised or fall victim to human error. Once a Blueprint has been developed and proven reliable, the code, and everything surrounding it doesn't change unless the owner wants it to. The only chance for error would be from incorrect info entered when using the Blueprint, which is minimal since very little information is required to be entered manually. Check out our Partner List to see what other cool technologies we’ve integrated with the CenturyLink Platform. If you want to play around with these or other Blueprints, visit the CenturyLink Cloud website to get started. You will need an account if you aren't already a customer, but signing up is easy and you only pay for the resources you use. I’d really like to get your feedback on this Blog and learn what software you’d like to see integrated in to a Blueprint. Leave a comment and let’s talk about it. Thanks for reading.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tutorial-create-server-with-clc-java-sdk", "author": ["\n              Chris Sterling, Director of Product Management\n            "], "link": "https://www.ctl.io/developers/blog/post/tutorial-create-server-with-clc-java-sdk", "abstract": "At CenturyLink Cloud, our culture incorporates a DevOps mindset whereby each team owns the services they develop from implementation to production. In order to apply the DevOps mindset, we adhere to these enabling principles: Automate, Self-Service, and Programmable. As an extension of these principles, we also provide public APIs and SDKs for multiple programming languages to enable our customers to also automate, self-serve, and program infrastructure and service interactions. In this tutorial we'll take a look at the CenturyLink Cloud Java SDK and explore how we can use it to create a server using a CentOS 6 operating system template in a specific server group in a US East data center. For scripting in the Java Runtime Environment, I tend to lean towards using the Groovy programming language and Gradle for build automation . In order to execute the build commands later in this tutorial you must install Gradle so it can be used from your terminal. To setup our development environment for using the CenturyLink Cloud Java SDK we can create a Gradle build configuration. The name of this file is build.gradle by default. Create a file named build.gradle in a new project directory in your local development environment with the following content: For those who use Gradle, the above configuration will look familiar. For those who are not familiar with Gradle, we'll describe what this Gradle build configuration is doing. The first line is setting up our project to use the Groovy programming language by applying the 'groovy' plugin. In the 'repositories' section we are telling Gradle where to download project dependencies from during the process. The 'dependencies' we need for this project are the Groovy language ('groovy-all') and the CenturyLink Cloud Java SDK ('clc-java-sdk'). The task named: 'runScript' is a custom task created to run our Groovy script in the JRE (Java Runtime Environment). This task expects a Groovy file named: 'create-server' to be the main class to execute at runtime. So, the next step is to create this Groovy file but Gradle uses a common convention to find source files in the 'src/main/groovy' directory for Groovy. Put the following content into the file src/main/groovy/create-server.groovy : To test that we have our local development configured correctly with Gradle, run the following command in your top-level project directory: You should see the output 'hello world' in the terminal after running this command. The CenturyLink Cloud Java SDK comes with multiple ways to authenticate using your credentials. The SDK configuration section of the documentation describes them in detail. For our purposes, we will use the DefaultCredentialsProvider , which wraps all of the different methods and will try each of the other approaches in the following order: The DefaultCredentialsProvider is the default credentials provider when instantiating the default constructor of com.centurylink.cloud.sdk.ClcSdk . Modify the create-server.groovy file to the following contents: When you run the script again it should say something like: Servers are created using the com.centurylink.cloud.sdk.server.services.dsl.ServerService , which is available from the instantiated ClcSdk class. The create() method on ServerService , as with of its other methods, is asynchronous. Therefore, the return value is com.centurylink.cloud.sdk.base.services.dsl.domain.queue.OperationFuture to allow us to wait for the actual resulting created server. The example code below will create a new standard server in the default server group of the CenturyLink US East Sterling, VA data center with: Now that we have the OperationFuture , we can wait for the result using the waitUntilComplete() method: When you run the script again there will be a delay between when the server creation process is kicked off and the OperationFuture returns with a result. During this time, you will see a server under construction in the CenturyLink Control Portal queue: After the server has been created, you should then see the server in your account's default server group of the US East Sterling, VA data center: The CenturyLink Cloud Java SDK and the SDKs for other languages provide many more capabilities in addition to the basic creation of servers. Some of the capabilities as of this writing are: Take one of our SDKs out for a spin or directly use the CenturyLink Cloud public APIs and let us know what you think.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tutorial-understanding-the-security-risks-of-running-docker-containers", "author": ["\n              Matthew Close, Security Engineer\n            "], "link": "https://www.ctl.io/developers/blog/post/tutorial-understanding-the-security-risks-of-running-docker-containers", "abstract": "We've all been there: You just wanted to try a command line utility, but the install process was as tedious as processing a mortgage application. If you weren't forced to complete too many steps, you were required to clutter up your development environment with libraries you'd never use again. Naturally, when Docker came along, you were thrilled that trying out a new tool became much easier. But is it too easy? Maybe it's time to ask yourself: In your quest for a simple command line tool, are you accidentally giving Docker too much control of your system? After I'd been using Docker containers for a while, I started to run across containerized utilities. I've been thrilled ever since. While not all that different from running a normal Docker app, these containers are very short-lived. The big gain for me is that I don't have to set up an environment to use the tool. All I need is Docker. I'll show you how easy it is with a quick example. In the Dockerfile below, I start with a small Linux distro, install some requirements, and then install the tool, xml2json. I don't need Node.js on my system, and now the tool is portable to other systems so long as Docker is already installed.\nAs an added bonus, clean-up is a snap.\nDelete the Docker container, and you know you are getting rid of every last trace of the tool. However, when I was looking to quench my thirst for easier tool installation, I also ran across a few containers that you'd start up like this, docker run -v /var/run/docker.sock:/var/run/docker.sock , or variations on the same thing, -v /var/run:/var/run or -v /var:/var . Running a container this way shares the volume /var/run/docker.sock with the container. Naturally, I wanted to know: What is /var/run/docker.sock ? What does it mean to share that volume with a container? And is it always safe? Here's what I've learned. /var/run/docker.sock is a Unix domain socket . Sockets are used in your favorite Linux distro to allow different processes to communicate with one another. Like everything in Unix, sockets are files, too. In the case of Docker, /var/run/docker.sock is a way to communicate with the main Docker process and, because it's a file, we can share it with containers. So what do you gain by sharing the socket with a container? Quite a bit as it turns out. When you start Docker and share the socket, you are giving the container the ability to do Docker-like things on the Docker host . Your container can now start or stop other containers, pull or create images on the Docker host, and even write to the host file system. Sharing the Docker socket also makes it possible to use some really great tools. Here's one , and another , and a whole treasure trove of tools . Beyond standard tools, sharing the Docker socket facilitates things like continuous integration of Docker apps. Think of a Jenkins Docker container controlling your Docker development pipeline. Allowing containers access to the socket is very beneficial in some situations. But what's at risk? Possibly the security of the Docker host system. This isn't a new revelation, but it's definitely worth noting that all that power I just described can pose a threat. For example, say I'm going to install Docker inside a Docker container. I don't want to run the Docker process; I just want to have the command line docker available to me. I could do this with a single command: docker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock -v $(which docker):/bin/docker ubuntu:latest /bin/bash . But I'm going to use a Dockerfile for now. More on why I made that choice later. Here's the Dockerfile. After building the container, let's run it and take a quick look around: docker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock mclose/docker-1.8.3 . Note that when I run docker ps inside the container, I'm seeing what is running on the Docker host outside of the container. Now for the fun part. From the running container, let's use the docker command line to start another container: docker run -it --rm -v /:/host busybox . So this is really interesting. I've started yet another container and shared the root file system of the Docker host. See why it might be important to know who can and can't run Docker on your systems? I could create a SUID root shell on the Docker host. I could then run things as root on the Docker host and there wouldn't be any logs of my activity . There are probably a few folks out there thinking, \"But I run this on OS X. Docker-machine totally isolates me.\" Think again. Take a look at that first directory listing above. Recognize anything? Yup: /Users . Your OS X home directory is right there. Keep in mind that docker-machine mounts the /Users directory in every Docker-machine VM. So this container not only has read/write on the Docker host, but it also has read/write access to your home directory on OS X. Oops. If you're starting to panic right now, don't. The rewards of using Docker are still great overall; you simply need to take some precautions. Start by looking at which users have the ability to run the Docker command. You should consider them full system admins. Why? Because, as in the example above, users that can run Docker are able to circumvent access control tools like sudo . When it comes to running containers with access to /var/run/docker.sock , be vigilant with the Dockerfile for the container. You probably got the Dockerfile from someone's repo on github.com . Look for signs that this is a GitHub project you can trust: active development, good documentation, and stars. Next take a look at the Dockerfile. Is the base system, the FROM line, one that you would trust? Would you install all the applications in the RUN lines of the Dockerfile on your own system? If you see something like my Dockerfile above, where Docker itself gets installed, know why. (That's why I used a Dockerfile above instead of a single command.) Do you fully understand each and every line in the Dockerfile? If not, take a look at the command reference . There might even be a few helper shell scripts that get installed, too, and it's a good idea to take a look at them. Make sure you understand the command line arguments that are being passed to docker run . I've covered the -v option here, but you should know what else is going on when you fire up a container. If you see anything mysterious, get a better understanding from the documentation . To be clear, I am not suggesting that you go back to the source code for the  project and review it. No one has that kind of time. At some point after reviewing the Dockerfile, you have to trust the container you are about to run. When you finally do run it, if that container uses a shared Docker socket, the container should be considered a high-value asset along with your databases and management systems. This means using network access restrictions, such as firewalls and VPNs, if the container is exposed beyond the Docker network. Not too distant releases of Docker will probably alleviate some of the risk involved in sharing /var/run/docker.sock with containers. One very promising solution uses namespaces and is in the Docker 1.9.0 experimental build. If you want to know more, here's a great read on Docker namespaces . Docker is an exceptionally flexible tool, but you do need to be careful how you use it. If you aren't, you could be giving away more access than you bargained for. However, at this point, I hope you have a better understanding of what /var/run/docker.sock means when you run containers. If you review and understand a container's Dockerfile, as well as how it gets run, you will be in a much better position to understand the security risks involved and take appropriate action.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tutorial-protecting-sensitive-info-docker", "author": ["\n              Matthew Close, Security Engineer\n            "], "link": "https://www.ctl.io/developers/blog/post/tutorial-protecting-sensitive-info-docker", "abstract": "Dealing with passwords, private keys, and API tokens in Docker containers can be tricky. Just a few wrong moves, and you'll accidentally expose private information in the Docker layers that make up a container. In this tutorial, I'll review the basics of Docker architecture so you can better understand how to mitigate risks. I'll also present some best practices for protecting your most sensitive data. It has become fairly common practice to push Docker images to public repositories like hub.docker.com . This is a great convenience for distributing containerized apps and for building out application infrastructure. All you have to do is docker pull your image and run it. However, you need to be careful what you push to hub.docker.com or you can accidentally expose sensitive information. If you are relatively new to using Docker, they have really great Getting Started Guides to get you comfortable with some of the topics we will discuss in this tutorial. To better understand some of the risks associated with using private data in Docker, you first need to understand a few pieces of Docker's architecture. All Docker containers run from Docker images. So when you docker run -it ubuntu:vivid /bin/bash , you are running the image ubuntu:vivid . Almost all images, even Ubuntu, are composed of intermediate images or layers. When it comes time to run Ubuntu in Docker, the Union File System (UFS) takes care of combining all the layers into the running container. For example, we can step back through the layers that make up the Ubuntu image until we no longer find a parent image. The last docker inspect doesn't return a parent, so this is the base image for Ubuntu. We can look at how this image was created by using docker inspect again to view the command that created the layer. The command below is from the Dockerfile and it ADD s the root file system for Ubuntu. This brings us to an important point in how Docker works. You see, each intermediate image has an associated command, and those commands come from Dockerfiles. Even containers like Ubuntu start off with a Dockerfile. It's a simple matter to reconstruct the original Dockerfile. You could use docker inspect to walk up through the images to the root, collecting commands at each step. However, there are tools available that make this a trivial task. One very compact tool I like is dockerfile-from-image created by CenturyLink Labs. If you don't want to install any software, you can use ImageLayers to explore images. Here's the output of dockerfile-from-image for Ubuntu. As you can see, it's very easy to reconstruct the Dockerfile from an image. Let's take a look at another Docker image I created. This container has nginx running with an SSL certificate. See the problem? If I were to push this image to hub.docker.com, anyone would be able to obtain the private key for my nginx server. Note: You should never use COPY or ADD with sensitive information in a Dockerfile if you plan to share the image publicly. Perhaps you don't need to copy files into a container, but you do need an API token to run your application. You might think that using ENV in a Dockerfile would be a good idea; unfortunately, even that will lead to publicly disclosing the token if you push it to a repository. Here's an example using a variation on the nginx example from above: Spot the problem this time? In the above output, ENV commands from the Dockerfile are exposed. Therefore, using ENV for sensitive information in the Dockerfile isn't a good practice either. So what are some possible solutions? The easiest one is to separate the process of building your containers from the process of customizing them. Keep your Dockerfiles to a minimum and add the sensitive tokens and password files at run time. Taking the nginx example above again, here's how I might solve my problem with SSL keys and environment variables. First, let's take a look at the Dockerfile for this container. There isn't much that's exposed in this file. Certainly, I'm not sharing private keys or API tokens. Here's how you'd then get the sensitive information back into the container at run time. By using volumes at run time, I'm able to share my private key as well as set an environment variable I want to keep secret. In this example, it would be perfectly fine to push the image built from my Dockerfile to a public repository because it no longer contains sensitive information. However, since I'm setting environment variables from the command line when I run my container, my shell history now includes information that it probably shouldn't. Even worse, the command to run my Docker containers becomes much more complex. I need to remember every file and environment variable that's needed to make my container run. Fortunately, there is a better tool called docker-compose that will allow us to easily add run time customization and make running our containers a simple command. If you need an introduction to docker-compose , you should take a look at the Overview of Docker Compose . Here's my basic configuration file, docker-compose.yml , to deploy my nginx container. After running docker-compose build and docker-compose up , my container is up and running with the sensitive information. If I use dfimage on the image that was built, it only contains the basics to make nginx ready to run. By using docker-compose , I've managed to separate the build process from how I customize and run a container. I can now also confidently push my Dockerfile images to public repos and not worry that sensitive information is being posted for the world to uncover. Passwords, private keys, and API tokens in Docker containers can be tricky. After a basic understanding of Docker architecture, with the implementation of some best practices for protecting your most sensitive data, you can mitigate risks. Don't have an account on CenturyLink Cloud? No problem. Just head over to our website and activate an account .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tutorial-using-owncloud-with-centurylink-dbaas", "author": ["\n              Gavin Lai, Senior Cloud Solutions Architect\n            "], "link": "https://www.ctl.io/developers/blog/post/tutorial-using-owncloud-with-centurylink-dbaas", "abstract": "This CenturyLink Cloud tutorial provides a walk-through to install and configure ownCloud on the Linux platform (from scratch and Blueprint ), customize ownCloud to utilize CenturyLink Cloud's Relational Database Service , SMTP Relay and Object Storage. ownCloud is a personal productivity powerhouse. It gives you universal access to all your files, contacts, calendars and bookmarks across all of your devices. Unlike many of the shared repository services out there, with ownCloud, you have your own, private repo. However, just like the public repo companies, with ownCloud you can share your files with friends and co-workers. If you need it, ownCloud even integrates with other storage providers. Best of all, ownCloud is open source and free! Audience CenturyLink Cloud Users - Not a CenturyLink Cloud User? Just head over to our website and activate an account . Prerequisite Postrequisite To access your application over the Internet, please perform the following tasks after the server is deployed successfully: After adding a public IP for OwnCloud perform the following actions:\nEdit the web server configuration files in /etc/apache2/sites-enabled as follows: Restart Apache using sudo service apache2 restart ownCloud setup will prompt to add the new IP address as a \"trusted domain\": Click on \"Add \"IP address\" as a trusted domain\", it will redirect this request to the private IP address to create the necessary entries to the owncloud configuration file: (For Steps using Blueprint, please see Getting Started with ownCloud Blueprint )\nCreate a Linux server in CenturyLink Cloud using the following knowledge articles: Use create a MySQL instance on CenturyLink Relational Database Service to create a database instance Note down the user name and the connection string from the setup: Download the certificate to configure secure connectivity to the Relational Database Service Enable SSL Below is an example of self signed certificate with a 1 year expiration date: Create the certificate ( use /opt/bitnami instead of /etc for Blueprint installation, detail steps are here ): Edit the following two parameters to reflect the location of the certificate: Enable SSL on the web server: To trigger the initial setup in the ownCloud Blueprint installation, move the config.php file from /opt/bitnami/apps/owncloud/htdocs/config directory to a new location (please backup the data for restoration) The ownCloud configuration page will appear: Choose Storage and Database , then select MySQL Using the information from the Relational Database Service to complete the information, the format for the host is IP_Address:port (e.g. 192.168.1.1:45678): Click \"Finish Setup\" , and the Welcome to ownCloud page will display: Download the certificate from Relational Database Service to the ownCloud server to enable secure communication between the database and the ownCloud server. Refer to this Connecting to MySQL instance over SSL-enabled Connection knowledge article. Add the following to section to the config.php file (default location: /var/www/owncloud/config/): Configure ownCloud to utilize SMTP Relay From the owncloud home page, select Admin from the user account: Select Mail Server from the left pane: Configure the SMTP Relay user based on SMTP Relay information from the portal: Use the test function to verify the account information You can utilize Object Storage in ownCloud two ways, one is adding Object Storage as an external storage and the other is to utilize Object Storage as the primary storage for ownCloud Steps to add Object Storage as External storage Access to CenturyLink Cloud storage (S3 compatible) or any other object storage Login to ownCloud portal as Administrator Select Apps from the top left drop down menu: Enable External Storage Support from the Not enabled list: From the owncloud main page, select Admin from the user account: Configure External Storage , Add Storage with \"Amazon S3 and Compliant\" and populate the fields using the credential from Step 1 and set permissions: Once completed, the Object Storage will be part of the storage locations under \"Files\": Steps to add Object Storage as Local storage Access to CenturyLink Cloud storage (S3 compatible) or any other object storage Depending on the version of ownCloud, the options of utilizing Object Storage can vary, learn more here ownCloud Server supports Local storage, GlusterFS/Red Hat Storage, OpenStack Swift as primary storage; Enterprise Edition supports additional primary storage with S3 compatible storage In order to utilize Object Storage for primary storage, edit config.php (default location: /var/www/owncloud/config/) with the Object Storage credential, like the example below: Now, the ownCloud server is set up to consume Relational Database Service, SMTP Relay and Object Storage, this will minimize the administration of the local environment and eliminate resource constraint on the server.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tutorial-deploy-sql-server-blueprint", "author": ["\n              Chris Little\n            "], "link": "https://www.ctl.io/developers/blog/post/tutorial-deploy-sql-server-blueprint", "abstract": "Overview Looking for a fast way to deploy Microsoft SQL Server? CenturyLink Cloud Platform and Blueprints allow customers to easily procure and deploy Microsoft SQL Server licensing within the Control Portal . Microsoft SQL Server is licensed via the Microsoft SPLA program . By utilizing the CenturyLink Cloud Microsoft SQL Blueprint, customers have the versatility to install and consume this business critical database software. Prerequisites Exceptions Go to the Control Portal and choose the Servers Menu. Browse to the Group that houses the VM(s) on which you want to deploy SQL. Select Action , then choose Execute Package . Search for SQL and select Install SQL Server on Windows Blueprint. Select SQL Installation Options .\nInput the appropriate parameters based on the SQL server requirements for your application. Select the VM(s) in the Group on which you want to deploy SQL. Customers can choose an individual VM or multiple. (Quick Tip: Only supported Guest Operating Systems will be shown) Select Blueprints Library in the Control Portal. Search for SQL and select Install SQL Server on Existing Server . Choose deploy blueprint . Input the appropriate parameters based on the SQL server requirements for your application. Select the Virtual Machine to execute the install against. Then, click next: step 2 . Confirm the VM(s), features, and settings are configured correctly and click deploy blueprint . Want to learn more about CenturyLink Cloud offerings that make deployment and execution a snap? Check out some of CenturyLink Clouds newest solutions: Database-as-a-Service Runner", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tutorial-docker-on-bare-metal-servers", "author": ["\n              Bryan Friedman\n            "], "link": "https://www.ctl.io/developers/blog/post/tutorial-docker-on-bare-metal-servers", "abstract": "Docker is open-source software for Linux that is used to deploy applications inside software containers. It does this by providing an additional layer of abstraction within the operating system–level virtualization. CenturyLink Cloud supports Docker on virtual cloud servers as well as on bare metal servers. If you don't have a CenturyLink Cloud account yet, head over to our website and activate an account . You'll need it to access Bare Metal and deploy the server. Follow these instructions for creating a new Bare Metal server . Make sure to select CentOS 6, RedHat Enterprise Linux 6, or Ubuntu 14 from the Operating System (OS) list. All of these operating systems support Docker. Depending on the OS you choose, follow the instructions below for installing Docker on CentOS 6/Red Hat 6 or on Ubuntu 14. Ubuntu 14 Run the following command to install Docker. This will download and install the latest version and configure it to start during system boot. Run the 'hello-world' container to test the 'docker' command and confirm the installation was successful. CentOS 6 and Red Hat 6 Log into the machine via SSH as the root user. In order to follow best practices, run a full update on the system to ensure that it is fully patched. Download the Docker RPM using curl . Install the package with yum . Start the Docker daemon with this command: Run the hello-world command and confirm the installation was successful. To set Docker to start during system boot, run the following command: Learn more about CenturyLink Clouds Bare Metal offerings and use cases. Sign up for our Developer-focused newsletter CODE . Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider — let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "orchestrate-filter-fields", "author": ["\n              Benji Smith, Software Engineer\n            "], "link": "https://www.ctl.io/developers/blog/post/orchestrate-filter-fields", "abstract": "If you've ever worked with a relational database, then you're probably already familiar with the concept of field filtering. Instead of retrieving every field in a complex record, you select only the fields you specifically need for the task at hand. For example, instead of this SQL query: ...you'd probably write something more like this: It's a simple concept, but incredibly convenient and efficient, and today I'm delighted to announce the availability of JSON Field Filtering operations across the entire Orchestrate API. We've given a lot of thought to this feature. JSON fields represent a hierarchy of information with much more potential for complexity than the tabular results of a relational database. But we've come up with an elegant design that supports a wide variety of use cases, and we think you're going to love it. First of all, field filtering is available everywhere in the API where JSON results are served: from simple HTTP GET requests to key-ordered listings and search queries, across all of our data types: key-value items, time-series events, and graph relationships. By using the new with_fields and without_fields querystring parameters, users can whitelist and/or blacklist certain fields according to their fully-qualified names. For example, take a look at this JSON record: An application dependent upon this data might only need to retrieve the value of the \"a\" field in some cases, and we could omit the value of the \"b\" field entirely. It should come as no surprise then that are two different ways to filter the fields: With either query the resultant filtered JSON would look like this: But wait! It gets better! We can actually use both the with_fields and without_fields predicates at the same time to achieve more advanced filtering scenarios. The important thing to understand here is that the with_fields predicate is applied first, causing all the designated field values to be cloned into a new object. If the with_fields predicate is empty, we copy all the fields by default. Then, we apply the without_fields predicate to the resultant JSON and subtract all the designated field values from the cloned subset. Just remember: Whitelisting comes before blacklisting. When used in concert, the two operations are most useful when blacklisting a subordinate field of an object whose values we've already whitelisted. So, given our previous example, we could combine a with_fields=value.b clause with a without_fields=value.b.d to create the following result: By designating the deeply-nested field name value.b.d as a blacklist field, we erase that value from all the deeply-nested children, even within arrays. You can use this functionality to optimize payload sizes, or to avoid retrieving sensitive data when it's not absolutely necessary. The optimization aspects of this feature are especially convenient in our Search API, or when paginating through Event listings, when it's commonplace to iterate over hundreds or thousands of records. In those cases, retrieving only the exact subset of data you need helps keep your application lean and fast. Field filtering also combines nicely with our JSON Patch API , which lets you execute partial updates on stored JSON objects, updating individual fields without having to re-submit the entire record. Now, with field filtering of response JSON, you can effectively query for a single field and then update that same individual field, without having to ever interact with the rest of the record. For example, to query for a user's current phone number, and then subsequently update that field, you might do something like this: ...which would give you a result like this, even if the complete user JSON object stored in Orchestrate has hundreds of different fields: With that information, you could prompt the user for an updated phone number and then issue a PATCH operation to execute the update: As the Orchestrate API continues to grow and evolve, it's nice to see how these kinds of features fit together like puzzle pieces, enabling more and more complex use cases and helping our customers build more sophisticated applications. The field filtering API is available immediately on all our data centers and will be integrated soon into our client libraries. We can't wait to see what you'll build next! Remember: Our lines of code, our technology -- is your platform. We’ll never stop improving and iterating new products and services to better your business.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "automating-sbs-on-api-v2", "author": ["\n              Matt Schwabenbauer\n            "], "link": "https://www.ctl.io/developers/blog/post/automating-sbs-on-api-v2", "abstract": "Earlier this year, CenturyLink Cloud® introduced the Simple Backup Service (SBS) . A fully-integrated component of the CenturyLink Platform, the SBS enables customers to have complete control over the retention policies, file paths, and off-site storage locations of file-level backup protection for their public cloud virtual machines (VM). Much like our other product offerings, you can create and apply SBS retention policies directly from the CenturyLink Cloud Control Portal UI. Using the UI ensures our customers have a simple, user-friendly experience as they roll out this new offering to their infrastructure. A step-by-step guide on creating and applying SBS policies using the UI can be found by clicking this link . While the UI is great for simple tasks like creating a retention policy or applying a policy to one or two servers, having to run through an entire CenturyLink Cloud footprint isn’t the best option for some of our larger customers. So, we have also integrated SBS into the CenturyLink Cloud V2 API. By leveraging our RESTful API, you can 'programmatically' create or modify retention policies for your organization, discover the policies assigned to a server or set of servers, and quickly assign retention policies in bulk. To demonstrate this, I have written a PowerShell script that will execute the necessary API calls to create and apply an SBS retention policy based on the VMs located in a CenturyLink Cloud account alias. The following script will iterate through each of the servers in the specified account alias and create the appropriate SBS policies based on the storage paths being used on the VMs. The correct operating system (OS) policies will be created for Windows and Linux respectively. The script will then iterate through the servers again and apply the appropriate policy based on the OS. There are a number of variables at the beginning of the script that you can manually edit before you execute the script. These are the account alias, the number of days the data will be retained, the backup frequency of the policy, and the storage region where the backups will be stored. These variables begin on line 44. The account alias has to be modified, the other variables are optional. If you don't modify the account alias, the script will prompt you to do so before it executes. If you don't edit the other variables within the script, they can be changed later in the Portal UI. An output file with results of the operation will be stored in: C:\\users\\public\\CLC\\SBSDeployment. First, open a Windows PowerShell ISE window. Copy and paste the block of code at the end of this article into the editor. Note: If you don’t want to edit any of the code prior to execution, you can download the script from the github repo and run the .ps1 file as is. Navigate to line 44. Find the comment # Variables to be modified . The next four lines are variables that you may want to modify before running the script. Your window should look something like this: Note: The unedited script will create backup policies with seven days of retention at a 12-hour interval, pointing to the US West storage location. If you would like different settings for your backup policies, modify these variables accordingly. Change the $accountAlias variable on line 48. This variable tells the script what account alias to apply the SBS policies to. It is currently set to \"XXXX\". If you run the script without changing this variable, it will prompt you to enter an account alias before it executes. Click Run Script in the toolbar at the top of the Windows PowerShell ISE window. This is identified by a green arrow pointing to the right. Alternatively, you can press the F5 function key. A prompt will appear. Log in to the prompt with your CenturyLink Cloud Control Portal credentials. As the prompt implies, these are the same credentials you use to log in to the Control Portal UI, or to make CenturyLink Cloud V2 API calls. Note: If the credentials you entered are correct, the script will continue. If not, the operation will terminate. If you did not modify the $accountAlias variable, enter an account alias in the PowerShell console window. If you modified the $accountAlias variable on line 48, you will see a confirmation of the backup policy. Confirm the details of the SBS policy that will be created. If this information looks correct, type Y and press enter. If you do not want to apply an SBS policy with these settings, type N and press enter to terminate the script. If you confirm the prompt, the main operation begins. You will see information about the policies that are created scroll by in the PowerShell console window. You can view the output to see how the script is applying the Windows-specific policy to Windows machines, and the Linux-specific policy to Linux machines. Note: The script programmatically detects the Windows paths that are in use and creates a backup policy which contains all of them, but for Linux it only backs up the root path \"/\". This is because SBS will reject any backup policies with redundant storage paths, such as \"/\" and \"/boot\". Since \"/boot\" is a sub-path of \"/\", backing it up would be redundant, as it is being backed up when \"/\" is being backed up. In order to ensure we aren’t repeatedly backing up the same data, the script will automatically default to \"/\" as the Linux path that it backs up. The operation completes with a message similar to this: Your specific message will vary based on your account alias and date. Navigate to the listed folder: C:\\Users\\Public\\CLC\\SBSDeployment\\\nto confirm the script and access the .txt output file. The file contains the logging that was captured during the operation. You can also check the Control Portal to see the policies that were created. Navigate to control.ctl.io and login with your CenturyLink Cloud credentials. When your dashboard is loaded, navigate to the Servers section in the menu, and click Policies. In the Policies dashboard, click Simple Backup Service. It is located at the bottom of the navigation options on the left-hand side of the window. The SBS policies that you created will appear here. Click on one of the policies to see the servers that it was applied to, as well as its settings. Note: If you want to remove an SBS policy from a server, just click the server's name on this page and then the delete button. Check out our SBS product page here . The CenturyLink Cloud Knowledge Base also has a number of articles on backup, including Getting Started with Simple Backup and Simple Backup How It Works . We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "deploy-a-minecraft-server-using-docker-and-blueprints", "author": ["\n              David Gardner, Product Owner\n            "], "link": "https://www.ctl.io/developers/blog/post/deploy-a-minecraft-server-using-docker-and-blueprints", "abstract": "This tutorial will explain how to quickly deploy a stateful, persistent Minecraft server using an image from the Docker Hub and a Docker Blueprint on the CenturyLink Cloud . After seeing plenty of WordPress demos displaying stateful persistence between Docker container restarts, I thought it would be interesting to discuss another extremely popular application that uses stateful persistence - Minecraft. In Minecraft, players literally create or destroy almost anything in the game world that they inhabit. As they interact with the environment and the objects within, changes to the world state are immediately saved to the file system. Like WordPress, Minecraft can use Docker to achieve the stateful persistence required for game play. Docker's ability to deploy a pre-configured image into a running container within milliseconds is a huge advantage over VMs. To enable applications to write to the file system, Docker uses a unification file system storage approach. By definition, images are static and composed of multiple layers. When a container is started, a writable top layer is added to the image. All writes to the container that add new data or modify existing data are stored in this writable layer, which is persisted between container stops and restarts. Although it is possible to connect to a running container and interact with the file system belonging to that specific container, a better way is to use a data volume. Data volumes exists outside of Docker's storage area, so even if a container is deleted, data stored in a data volume still remains. (NOTE: Although pausing or stopping a container without a data volume mounted does not destroy the writable top layer, it is much easier and less tedious to manage the state using data volumes) Docker newbies, Minecraft enthusiasts, and anyone interested in seeing how Blueprints can quickly deploy applications. OK. Before we get started, there are a few prerequisites. Besides being comfortable with executing Linux commands on the terminal, you will need: NOTE: If you just signed up for an account, you must first create a server to activate Client VPN access. Follow the Blueprint deployment process below to create your first server, and then setup VPN access. Here's an overview of the steps we'll follow to deploy a stateful Minecraft server using Docker and Blueprints. Ready? Let's get started. Login to the Control Portal and select Blueprint Library from the menu drop-down. In the upper left corner of the Blueprints Library page, select the data center where you want to provision your server. I've chosen IL1 . Most likely you will select your Primary data center. This tutorial assumes that if you choose to provision a server in a data center that is not your Primary data center, you also already know how to create cross-connect data center firewall policies so that your VPN Client can connect to your new server. In the keyword filter, type: docker . Look for the \"Docker Engine\" Blueprint tile. Mouse-over the tile to read the description: \"Installs Docker Engine on Ubuntu 14 with Kernal 4.x\". Click the \"Docker Engine\" tile to display the deployment page. Click the Deploy Blueprint button. (CenturyLink Cloud makes it easy to see your monthly costs and pause your server between Minecraft sessions). The next section will walk through the Blueprint configuration steps. On the Customize page, there are several required fields to update. Under Customize Server Name(s) , give your server a unique 4-6 letter alias. I changed the default DOCK to MINE . The provisioning process generates a complete server hostname using the data center abbreviation, your CenturyLink Cloud account alias, your server alias, and 2-digits. The hostname of my new server is: IL1CLABMINE01 . There is no need to change the Specify Credentials switches or Docker Daemon Options , so leave these settings as they are. When you are done making updates the Customize page should look something like this: Now click the next: step 2 button. This would be a good time to grab a drink of water or purchase a Minecraft license if you don't already have one. Typically, it takes 3-5 minutes to provision the server and install Docker. When it finishes, you're ready to validate your deployment. Now it's time to confirm that your deployment was successful. This is also when you'll find out whether you can securely connect to your newly provisioned server. Before you can SSH into your server, you must establish a secure VPN tunnel to the data center. If you just created a CenturyLink Cloud account, you will need to follow these instructions to download and configure your OpenVPN client for Mac OS or Windows. Once you've finished, continue below. NOTE: You can also add the certificate and configuration to an existing VPN client, but that is outside the scope of this tutorial. Now start the VPN connection. Once your VPN tunnel is connected successfully, you'll see a notification modal. You will need your server's IP address and root password. To view your root password , click show credentials and copy the value. NOTE: In the interest of time, this tutorial doesn't adhere to recommended security best practices. Accessing a server as root isn't a best practice. Please refer to this Knowledge Base article for recommendations on securing your server. Open up your terminal (or PowerShell on Windows) and type: (NOTE: <nn.nnn.nnn.nn> represents the internal IP address of your new server. This is displayed on the server dashboard under Server Info).* Cut and paste the IP address starting with 10. Mine looked like this: Complete the installation by setting the DOCKER_HOST env var to localhost (127.0.0.1). In your terminal, type: Now that the docker daemon is configured, validate your Docker Blueprint deployment from the command-line by typing: A few failure messages appear before the docker engine downloads the missing hello-world image and executes it as an ephemeral Docker container, writing \"Hello World\" and a few more welcome messages before exiting. The entire output is shown below. I switched the filter to Official to check for an Official repo from Microsoft or Mojang (the developers of Minecraft), but none were listed. Instead, I set the filter to Downloads and listed the repositories in descending order -- a good proxy for \"most popular\". Chose a repo. I chose the kitematic/minecraft repo, but any repo could work. Keep in mind, since anyone can upload a repo to the publicly shared Docker Hub, some repos may not work without reviewing the instructions or carefully examining the Dockerfile. This Dockerfile for kitematic/minecraft lists Michael Chiang (a Docker employee) as the MAINTAINER, so I assume it will be one of the easier repos to get started with. Type the following command to run the kitematic/minecraft image in attached mode. Since Docker is unable to find the image locally, it pulls it from the Docker Hub. After all the images are downloaded, Docker starts up the Minecraft server according the last instruction in the Dockerfile: Docker also generates a few Java error messages as the server fails to load the banned user list, banned IP list, and a few other lists. You can ignore these messages. The server prints out a notification when it is ready: Note: Press Control + C on the terminal to signal the running container to stop. The tutorial assumes that your Minecraft client is already registered. Follow these steps to connect to the Minecraft server. Start the Minecraft Launcher and select the profile you want to use. Click the Play button to start the Minecraft Client. Choose Multiplayer from the game types to play a network game. Choose Add Server to specify the server info. Enter in a Server Name (or leave the default: \"Minecraft Server\"). For Server Address , enter the server's IP Address and Port <nn.nnn.nnn.nnn>:<12345> . The default Minecraft port value is: 25565 . My server address is: 10.90.25.12:25565 . Click Done to connect and drop into the world. Feel free to play around and explore until you are ready to move ahead. When ready, stop the container in the terminal by pressing Control-C . Another way to stop a container is to run docker ps in a separate terminal to list running containers, and run docker kill <containerID> to kill a specific container. The docker kill <containerID> approach is helpful when your container is running as a Docker background process. Now let's add the ability to start the game in the same state as when you stopped it. The following docker run command mounts a volume so that the container can access the server's file system, and runs the Minecraft server container in \"detached\" mode as a background process. The docker run parameters do the following: You can save multiple worlds and access them later by renaming the world directory at /opt/craft/ to something else, such as my-new-world . If the world directory is missing when Minecraft starts, a new world folder is created. Try creating new worlds by editing /opt/craft/server.properties and restarting the Minecraft container. When you are done exploring, rename the world directory something else (ex.: # mv /opt/craft/world /opt/craft/my-world-2 ), and repeat the process. Here are a few suggestions: Check out a detailed listing of Minecraft server properties here . These articles helped to inspire and inform my understanding of Minecraft and Docker:", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "search-ranking", "author": ["\n              Adam DuVander\n            "], "link": "https://www.ctl.io/developers/blog/post/search-ranking", "abstract": "Ever wondered what \"sort by relevance\" really means in search results? Maybe you've been frustrated that they aren't actually relevant . Now you can do something about it, at least with your own applications. When we created a way to search Node modules , sorting was on our minds. We wanted the best Node modules to show up at the top of the results, not those with the name that most closely matches the search term. There are many different ways to look at relevance, and in this tutorial you'll see how to apply your own view to create your own search ranking. First, let's consider how search works by default in Orchestrate , our NoSQL database-as-a-service. The search features are built on top of Elasticsearch , and Orchestrate intelligently makes all of the indexing and schema choices for you. When executing a search with no sort specified, Orchestrate uses the search score provided by Elasticsearch. The default score is great for many situations where the goal is to closely match a search term. That method is too simplistic when there are other factors available for sorting. To build ScoutJS , we merged data from NPM with GitHub . Among the fields we cared about from GitHub were stars, forks, and updates. NPM had all the data about each module, as well as a valuable popularity metric--number of downloads. We needed to include all of these factors in our rankings. As a way of showing the difference, let's look at the example of searching for \"auth\" in our combined dataset. The first result is the \"auth\" module. That may seem highly relevant until you realize it was last updated three years ago. It has other signs of non-popularity as well--no forks, one star, and relatively few downloads. But how is Orchestrate or Elasticsearch supposed to know? Every application is different. Your input is needed to identify what matters in your data. You can use the same approach we did with ScoutJS in your applications. If you look at the same search for \"auth\" using our finished search ranking algorithm, you'll see results that are relevant--not due to name matching, but popularity. The top results have hundreds of forks, thousands of stars, and hundreds of thousands of downloads. Clearly, the sort factors for ScoutJS are: What's important to your application? Jot down some potential fields in your dataset that you want to use for sort ranking. You only need a handful, and they should jump out as obviously important. However, they don't need to be equally important, as we'll see in the next section. Now that you've determined which fields in your data count for sorting, you need to decide how much they count . We'll use these weights along with each of the sort factors to determine a popularity value upon which we can sort. We'll add that pre-calculated value to our data so it's ready when we need it. Here's the section of ScoutJS code where the rank is calculated: The first thing we do is store the factors into variables. Next, we create a new variable, updatedWeight , where we make our first decision about how much each factor matters. We decided that we cared only if a module was updated within the last six months or so. If it's older than six months, it gets no credit for being updated. Otherwise, it does get credit. We also realized that modules that have very few downloads, but have many stars, are anomalies that should be dropped in the rankings. So, we reset stars and forks to be zero. Lastly, the weights are applied based on how close each module is to the maximum value for its field. The weights and maximums themselves are static values declared elsewhere: We tried various values before arriving at these weights. You'll do the same thing with your data, because you know it best. If something seems amiss, adjust your weights, or even try removing some factors. They might not be as important as you thought. Remember that ScoutJS merges disparate datasets, which themselves are always updating. Periodically we  grab the latest downloads from NPM and popularity data from GitHub. Each of those is stored separately, which is \"normalized data\" in database terms. Then our application \"denormalizes\" the data into a central collection. That's when our ranking is calculated. Wondering why we don't calculate rankings or join the data on the fly? That's a perfectly natural question, especially if your background is mostly relational databases. It can be a little strange to transition from relational to NoSQL databases . Our applications benefit in several ways from planning ahead. First, we're able to update our data from different sources at different times. We also gain the ability to look only at the data from GitHub, for example, and to look back at its changes by using Orchestrate's powerful Refs feature . Finally, we see performance gains by calculating the static ranking value a single time and storing that value for usage when we need it. Hopefully you've been thinking about your own data. Are there ways you can sort it in a more relevant way? Do you know which fields of data matter in your applications? Use this search ranking algorithm approach to put some weight into your results.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "terraform-tools-infrastructure", "author": ["\n              Albert Choi\n            "], "link": "https://www.ctl.io/developers/blog/post/terraform-tools-infrastructure", "abstract": "ter·ra·form (verb) (literally, \"earth-shaping\") from Latin terra \"earth\" + forma \"shape\", (used especially in science fiction), a hypothetical planetary engineering process so as to make the planet inhabitable for human life. Build, evolve, and manage your infrastructure with confidence. HashiCorp Terraform provides a common interface to infrastructure management — from servers and networks to email and DNS providers. Terraform is a deployment toolkit that provisions to multiple cloud vendors via a common interface and evolves with you over time, all under programmatic control. Terraform allows you to stitch together a wide range of cloud providers via high-level interfaces in one common paradigm. For instance, you can launch a VM on one cloud provider, then associate its IP with a DNS record on another provider. Built-in dependency resolution gives you assurance that related resources are operated on in proper sequence. As requirements on your infrastructure change, modifications happen on the Terraform configuration and are realized for you via the toolchain. This enables you to have confidence that your infrastructure is orchestrated centrally. As an analogy, we'll use a sample terraform project. We'll launch a Terraforming payload onto an empty planet , sculpting the environment to sustain life for your team's applications. Typically, provisioning to any given cloud provider entails implementing code to call proprietary REST-based APIs, often via custom scripts. The scripts might be vendor-distributed SDKs in the language of your choice or a cloud provisioning library like fog or jclouds . They might be integrating against the provisioning plugins available from configuration engines like chef , puppet , or Ansible . Terraform proves to be an outstanding way of managing an evolving deployment, orchestrating inter-related graphs of cloud resources, drift, and mutation. Let's get started building our planet . Terraform works by translating a declarative description of the infrastructure deployment and realizing it through a process of state comparison (what does and does not yet exist in the target infrastructure) and graph traversal (which related resources will be affected by the requested changes). The following Terraform configuration declares a CenturyLink Cloud server group named frontends and provisions a single virtual machine into it: You can pull the latest binaries for Terraform off the downloads page to get rolling with this example. You might also check out the provider documentation for CenturyLink Cloud resources. Invoking the terraform plan will cause Terraform to do the following: Next, terraform apply will analyze the configuration, detect that no such server group exists, then proceed to create it. Only then does it create the child VM under the group. Similarly, if the group does exist and the VM doesn't, Terraform would skip the creation of the group and infer it needs to create the VM only. Finally, once the group and server are created, artifacts for these resources are stored to a local cache file, which is keyed by resource identifier so that subsequent calls can operate on the same resources. As you may have guessed, modifying the count of servers will either create or destroy VMs in order to converge into the desired state. Launching a VM is generally pretty straight-forward using any tooling. Things get more interesting when we look at scaling the number of resources up or down and adding dependent resources. Let's look at a slightly more complicated version of the above example that logically follows provisioning a VM. Similar to the first example, a group is created. But this time a variable number (N) of VMs are placed into it. In tandem, every server created receives the following: The provisioner transfers a local file to the remote box via the generated public IP. As the N of servers increases, new VMs are created. Additional IPs are allocated according to the same specification. Similarly, if N is decreased, Terraform de-allocates both the VM and the associated public IP. Adding or removing ports triggers all affected IPs to mutate into the desired state. Doing all of this the old way involved managing this state manually by orchestrating the changes via sequences of script invocations. It's a tedious and time-consuming process. Fire off the above configuration and, with the next command, watch it mold the infrastructure of your planet into an ecology of resources arranged just as you designed. Reverting the planet back to its pristine state is accomplished by using one command: terraform destroy . This isn't likely to happen on your production deployments, but it's certainly helpful for disposable environments. All generated resources are tidied up and your planet is restored to its original state. In the event that one of your systems appears to have drifted, or is otherwise in an undesirable state, terraform taint can be very helpful. This command triggers the destruction and re-creation of the resource. Additionally, any related resources are also pulled into the state transition. Under ordinary circumstances you'll be modifying your server resources, adding or removing capacity, scaling compute resources, or setting up new groups of servers as determined by your applications. As you make modifications to your Terraform configuration, terraform apply will re-analyze and formulate the convergence plan to realize the changes. For instance, exposing an additional port would involve modifying the source config to add the additional port and then apply ing the changes: When converged, Terraform will detect that the resource may be modified in place and will simply update the firewall rules to expose the additional port, as demonstrated by this plan: The Terraform execution plan has been generated and is shown below. Resources are listed in alphabetical order for quick scanning. Note: If you didn't specify an \"-out\" parameter to save this plan, when \"apply\" is called, Terraform can't guarantee this is what will execute. Note: You might decide that cloud servers aren't powerful enough for your workload. CenturyLink Cloud also supports Bare Metal servers for enabled accounts to maximize compute capacity. See the Terraform supported server types for more details. One of the strengths of the the Terraform toolkit is the wide range of providers included. You might stitch together a configuration that spans CenturyLink Cloud, OpenStack, or AWS/GCE, and wire public IPs from each into a global DNS pool configured through one of the supported DNS providers. Further, you might extend your build-out to include SSL certificate generation and registration via the list of available providers . Note the following from the Terraform docs on Multi-Cloud Deployment: Realizing multi-cloud deployments can be very challenging as many existing tools for infrastructure management are cloud-specific. Terraform is cloud-agnostic and allows a single configuration to be used to manage multiple providers, and to even handle cross-cloud dependencies. This simplifies management and orchestration, helping operators build large-scale multi-cloud infrastructures. In this tutorial you've learned how to get started with Terraform, how to work with configuration files, and why it's helpful to integrate into your DevOps toolchain. Peruse a real-world example of Terraform in the community with the mantl project . This project brings together Terraform and Ansible to deploy a  microservices orchestration framework on the cloud provider of your choice. Mantl supports several cloud targets facilitated by the Terraform toolkit. Provider source code is available here . Check out other articles related to terraform and other partner products on our Developer Center . Don't have an account on CenturyLink Cloud? No problem. Just head over to our website and activate an account . Sign up for our Developer-focused newsletter CODE . Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why. Thanks for reading, Albert Choi", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "docker-tornado-alley-data-1", "author": ["\n              Matthew Close, Security Engineer\n            "], "link": "https://www.ctl.io/developers/blog/post/docker-tornado-alley-data-1", "abstract": "Data is a pain. There's a ton of it; it's in a billion different formats, and trying to make it behave the way you want so you can find patterns or answers to questions is difficult. But it's still important to figure out how to manage data sets because data analysis helps companies in just about every industry across the globe analyze the past to predict the future and get a leg up on competitors. Data is the “thing” right now. Of course, that's a very broad interpretation of what one does with large data sets. The reality is that most of us working with data on a daily basis are mired in very specific areas, and we have specific questions to answer. This often means that we start with massive amounts of information  and we need simple ways to trim it all back so we can focus on managing only the stuff we need. Additionally, we need to make sure we're using repeatable processes — just like every other science out there. If you can pull something off once, it's not science. You have to be able to replicate what you find, over and over. I think one of the best ways to handle these challenges is by using Docker . I know you're probably thinking Docker is good for building web apps, but managing data sets? How would that work? Trust me — it's pretty cool. In this two-part series, I'm going to present an example that will show you why Docker is great in this kind of scenario. The first post will show you how to get your data ready and into a database (we'll use MongoDB — it's awesome at geospatial references), and how to start your basic analysis. The second post will show you how to visualize the data analysis you perform and how you can then interact with your data in a more intuitive and useful way. For the purposes of this example, I needed to pick a data set that wasn't ridiculously large, but big enough. It needed to be free and open to the public (because, well, obviously). And, I wanted the data to lead us to ask natural questions, something everyone understands to a point. I settled on tornadoes. To be specific, I took a data set from Census.gov outlining all county borders in the United States, and another set of data from NOAA.gov covering all tornadoes recorded in the United States from 1950 to the present. My objective was this: I wanted to end up isolating all tornadoes that happened in my home county, Tarrant County — cute little spot on the edge of Tornado Alley in the Dallas/Ft. Worth area of Texas) and all tornadoes that happened within 100 miles of the center of Tarrant County. This seemed like a good way to start with two sets of unrelated data and find a way to narrow them meaningfully, so both would ultimately contribute to answering the same question. Let's get started. While Shapefiles are the gold standard for obtaining government map data, you have to convert them to either GeoJSON or KML formats in order for them to be usable. And while you have a few options for converting them, the typical method (directly downloading and compiling the Geospatial Data Abstraction Library (GDAL) is one I find frustrating and overly-complex. Instead, I prefer using a Docker container to deal with the standard tools. Docker is efficient and self-documenting. As an added bonus, if you are using geocoded information for data science, Docker gives you an easy way to reproduce your environment and tidy up data when you're done. The main tools we will need are: (And, of course, Docker. If you don't have Docker installed already, you can find instructions here .) Let's start by cloning (or forking) my GitHub Dockerfile repo and building a couple of Docker images. The -t flag is the tag used to name the image and the final parameter to build is the directory where the Dockerfile is located. The build for GDAL is a somewhat lengthy process, so you can plan on it taking about 10 minutes to complete. (Comparatively, if you were creating the Dockerfile from scratch, it would take a bit longer — you're capitalizing off the work I've already done here. And if you were going about this the old-fashioned way, you might as well set aside at least a day, or more.) Let's verify that it worked. The output of docker images should now look something like this: The last piece of the puzzle is to make sure we can use our tools from the command line as if they were any other binary installed on our system. In my Dockerfiles repo, there is a file called all_aliases . By sourcing this within a command line shell, you will have access to all the Dockerized geocoding resources as if they were any other tool on the CLI. Seem too easy? It should — and that's a good thing. Your environment is now ready to convert and process Shapefiles. Let's start working with some data. Normally, the next step would be to gather all your data sources for processing. For this tutorial, the easiest route is to clone or fork the tornadoes-gis repo to get all the data and code samples I've used. If you want to download the data sets directly, I've included my sources at the end of this post. We can now interact with some of our Shapefiles directly. Let's take a look at the Shapefile with county boundaries specifically. The last command, ogrinfo , is running in a Docker container and will give us a summary of what is in the file. Your output should look something like this: I'm emphasizing some of the the more important output with Shapefiles. The GEOGCS portion of the output describes the coordinate system or spatial reference system that is used in the Shapefile. For the most part, we don't need to worry about the coordinate system of the source file. However, we do need to worry about converting it into something we can use. The coordinate system for GeoJSON, our goal, is WGS84. So our next step is to change the Shapefile coordinate system and format. We'll do this in one step and again use a Dockerized command. ogr2ogr is a very flexible command and can do many things, but it can be a little complicated. Let's take a second to run through what is going on above in detail. Now that we have the US county boundaries converted to GeoJSON, we need to tidy up the file a bit. Since we plan on using MongoDB, we need to make sure each GeoJSON feature is smaller than 16MB or we will exceed the per document size limitation. There are a number of ways we could get this done. Two simple ones would be either reducing the number of coordinates inside a county boundary or reducing the precision of the coordinates. In either case, our Dockerized tool mapshaper is the right choice for the job. For my example, I'm simply going to reduce the precision of the coordinates. So a coordinate like [-86.12254952, 31.6125044] will become [-86.125, 31.615] . If you're worried about loss of accuracy, the change isn't very significant when you're working with objects the size of a US county. The worst case scenario for a continental US county is that the new coordinate will be a quarter mile away from the actual one. For our purposes, that's just fine. Here's the command: We have one last step before we can import the GeoJSON file into MongoDB. Normal GeoJSON format looks like this: While MongoDB has native support for GeoJSON, mongoimport will interpret the normal GeoJSON file as a single JSON document. That's not what we want. What we really need is to have each county as a JSON document. The easiest way to get this done is with one of my favorite JSON tools, jq . We'll use our Dockerized version of jq . The parameter .features[] is a query to jq and will extract each county from the features array in the original file. It's finally time to import our file into MongoDB using mongoimport. If you don't have MongoDB installed, please take a look at these instructions to do so. In the command below, the target database is named tornadoes and the collection name will be counties . If the database doesn't exist, MongoDB will automatically create it. What about the tornado data? Let's quickly run through the same steps for getting the NOAA data into MongoDB using our Dockerized commands. Start in the root directory of the GitHub repo. Since we're working with fairly compact JSON objects in the tornado file, we can skip the mapshaper step. However, we do need to make one change in MongoDB so that our geospatial queries work. The tornado collection has to be changed so that the geometry key is a 2D spherical system. To do this, we'll just use the MongoDB shell. If you don't change the geometry, MongoDB won't be able to properly determine distances from a point. This is particularly important if you want to find out how many tornadoes have occurred within a set radius from a point on the map. Note : You might be wondering why I didn't do this for the US county data. I have a good reason. Boundaries in Shapefiles are called polygons. Most Shapefiles when you convert them to GeoJSON have self-intersecting polygons. Basically, the boundary crosses itself kind of like a bow tie. This is not allowed with the GeoJSON specification. If you were to set up a MongoDB collection with a 2dsphere geometry and try to import a self-intersecting polygon, it would fail. I have yet to find the perfect tool to clean up these situations. (If you know of one, please leave me a comment below.) So for now, I do not set a 2dsphere geometry on large data sets like the US county boundaries. We are on the home stretch. Let's run some interesting queries on the data we've just imported. First, let's find out how many tornadoes have happened in Tarrant, TX, since 1950. As I said earlier, Tarrant is my home county — so that's why I find the boundary interesting. The MondoDB query will look like this: The basic idea here is that we're querying for any tornado in the tornadoes collection that falls within the boundary of Tarrant, TX. The output will be on a single line, printjsononeline , and will be a JSON array, toArray() . I also don't want the MongoDB document ID to be returned. So I've added { _id: 0 } to my query. If you want to change counties to the one where you live, you'll need your state FIPS code. You can look that up on this page . Here's how you run the query from the command line: The above query is stored in the file query_tarrant_tornadoes.js . The output from MongoDB is passed to our Dockerized jq to reformat the output as proper GeoJSON. Let's do one more quick example to illustrate a slightly different query in MongoDB. Say I want to find all the historical tornadoes within 100 miles of a given point. I'm picking the geographic center of Tarrant, TX, for this example. Let's start with the query: The main differences to note here are, first, that we use the $near query. For more information, check out the MongoDB documentation . Then, instead of finding something near a county, we want to find all tornadoes near a point. So, the $maxDistance is in meters here, and 100 miles is about 160,934 meters. Running the query happens exactly like we did above. This is just the first part of our process. We've got the data cleaned up and ready to use. Next, we need to visualize it. That will be the focus of Part Two in this post series. You might think it's easiest to just paste everything in Google My Maps and call it a day. But I've got a better (and much more useful) idea. We'll be looking to Docker again to create our own Node.JS app and then view and even interact with our data in a browser. Stay tuned — and in the meanwhile, post your comments and questions below. If you've found that I'm discussing tools or processes you're not familiar with, please don't hesitate to ask how to do something. I'm accelerating a few things here based on assumption of experience, but this is stuff that anyone can learn with the right preparation. Thanks for reading, Matthew Close - Security Engineer Sign up for our Developer-focused newsletter CODE . Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider — let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "jenkins-multi-tier-application-management", "author": ["\n              Oscar Serna\n            "], "link": "https://www.ctl.io/developers/blog/post/jenkins-multi-tier-application-management", "abstract": "You may be familiar with how Cloud Application Manager enables you to deploy components of an application or how Jenkins can be used to automate steps of an application build; but how can you deploy and manage an entire application with multiple components and tiers all at once?  Glad you asked. With application box that was recently introduced and our integration with Jenkins, you can now automate this process in a predictable manner without manual steps.  To recap, an application box deploys several instances that executes an application simultaneously. To deploy most applications, you need several instances cooperating together but application boxes are a way to define and reuse how several boxes will work together to run an application. To learn more, check out the Application Boxes documentation here . Today, we are going to deploy an Application Box using the Cloud Application Manager Jenkins plug-in.  Below is a simple 3-tiered Java application that we will deploy which is comprised of a JBoss server and a mySQL database (If you want to learn how to create one, please check the documentation ). Now, we need to go to the Jenkins server to configure the Cloud Application Manager environment. This is how it should look like if every step is successfully configured in the Jenkins management section: Now, we will create a Job called “ApplicationBoxTest” for deployment of an application box using the Jenkins plugin. Now that we have created the Job, we are going to configure the Deploy Box build step. We have already chosen the box we want to deploy (JBoss Sample Application), the workspace and the Cloud that we previously set. All we need to do now is run the job and check if it works. We can see how the application box is being deployed in the Jenkins Console. Alternatively you also can see how this deployment in the Instances tab of the Cloud Application Manager portal: After some time, we can visually see that everything is performing as expected: All instances up and running! That easy?? Let’s go check it, anyway! To access our application all we need to do is to enter the correct endpoint IP address (to get it, you can use the Jenkins Console link or through Cloud Application Manager UI) of one of the load balancers in the portal. Cloud Application Manager is a powerful, scalable platform for deploying applications into production across any cloud infrastructure – private, public or hosted. It provides interactive visualization to automate application provisioning, including configuration, deployment, scaling, updating and migration of applications in real-time. Offering two approaches to cloud orchestration — Cloud Application Manager and ElasticKube — enterprise IT and developers alike can benefit from multi-cloud flexibility. Explore ElasticKube by visiting GitHub (curl -s https://elastickube.com | bash). Visit the Cloud Application Manager product page to learn more.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "survey-software-saas-or-iaas", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/survey-software-saas-or-iaas", "abstract": "One of the benefits provided by cloud services is rapid server and application deployment. Cloud services are well-suited to applications with short lifespans that have a defined end date. Survey software is a good example of that. Cloud services also provide different paths for survey deployment, such as virtual servers in an Infrastructure-as-a-Service (IaaS) environment or a managed application installation in a Software-as-a-Service (SaaS) environment. Survey software is a powerful tool for marketing, customer relations, and gathering information. Many different options exist, but choosing the right solution for your needs doesn't have to be rocket science. We will look at three options in this tutorial to help you pick the approach that is right for you. LimeSurvey is one of the most popular survey software packages. LimeSurvey can either be self-hosted or managed through an SaaS. It can be installed very quickly on a CenturyLink Cloud virtual server using the LimeServer Blueprint . LimeSurvey can also be deployed as a SaaS product through LimeService themselves or a similar provider. SurveyMonkey is an online survey SaaS providing both free and paid service tiers. SurveyMonkey also provides a suite of enterprise-class features including data analysis, consumer-focused marketing, and brand management. Both methods of deploying and running survey software have their advantages. So let's compare the two by examining the requirements for survey software an enterprise might have and how the requirements might be met. No project is complete without a list of requirements. Deciding between a self-hosted solution or SaaS doesn't have to be a painful experience. Both cloud-based approaches have their strengths and weaknesses, and learning to evaluate them empowers you to choose a solution with confidence. Many of the requirements can be used to evaluate future software deployment choices as well. Both solutions provide a reasonable amount of front-end customization. SurveyMonkey has a limited amount of front-end customization, allowing free-tier users to select pre-defined themes and make small adjustments along the way. On the other hand, LimeSurvey has a built-in template editor with a full range of customizations, including customized CSS, JavaScript, and HTML. However, self-hosted software offers an additional bonus to customization: access to the back-end and source code. With self-hosted LimeSurvey, you can customize every aspect of the application. Of course, many elements of back-end customization will need to be done by a front-end or back-end developer, but the flexibility provides a lot of power. With a SaaS solution, you will only be able to integrate services to the extent that the provider allows. Integration capabilities vary widely by SaaS provider, but will always be limited to their choices. Self-hosted software gives you the ability to add plug-ins or make configuration changes with all manner of integrations. With survey software, this could mean integration with existing customer or client lists, analytic packages, or custom database drivers. The self-hosted option has the potential to be tightly integrated with your existing cloud infrastructure. Both the CenturyLink Blueprint and the SaaS option are easy to deploy without additional technical support. However, ongoing maintenance requirements are different for each. The SaaS is completely managed by a third-party. So any technical issues are dealt with through the SaaS provider's support structure. This experience can be a mixed bag and depends entirely on the SaaS provider. On the other hand, self-hosted software has a higher technical staff requirement. Staff may be required to do additional system configuration or conduct the initial cloud service deployment. Some customization might require assistance from developers or DevOps personnel. In addition, any software upgrades will require manual intervention. SurveyMonkey has a straightforward, predictable pricing plan for commercial users. However, SaaS pricing tiers can cause unpredictable bottlenecks or cost spikes as usage ceilings are reached. Self-hosted software can have less-predictable pricing, but it also allows scaling of services allocated for a particular task to stay within a project's budget. CenturyLink Cloud provides a pricing estimator that gives project planners an accurate idea of what a service's requirements will cost. The CenturyLink Cloud Blueprint integration provides a click-through solution to install and configure LimeSurvey on a virtual Linux server. Follow the steps below to deploy your own self-hosted LimeSurvey solution. CenturyLink Cloud Servers allow you to create highly configurable, high-performance virtual servers, while paying only for what you use. These enterprise-grade virtual machines are easy to deploy. You can manage the servers from the CenturyLink Cloud Portal or via API calls. CenturyLink Cloud Blueprints offer deployable configurations that can bring a new environment online in a few clicks. Custom apps and pre-packaged software are always deployed to their ideal settings, thus eliminating problems related to misconfiguration and mismanagement. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to access CenturyLink Blueprints and deploy virtual servers. Try our most popular products, or MarketPlace Partners offerings, allowing you to create a combination of products that best fits your business needs. Follow these steps to start the deployment process. Select Blueprints Library from the pull-down menu. Enter LimeSurvey into the Refine Results box and click go . Click Install LimeSurvey on Linux x64 . The LimeSurvey Blueprint customization only requires that you fill out a form and wait for provisioning before deployment. Follow the steps below to complete the process. The \"Deploy Blueprint\" screen appears with a progress indicator. Note: If the LimeSurvey Blueprint fails to deploy, return to Step 1 of this section and try again. In order to allow access to your new LimeSurvey installation from the Internet, you will need to add a public IP address and allow Internet traffic to a few specific ports. Follow the directions below to get your server ready for the public. Your server's new public IP address will now be displayed on the summary screen. Access that IP address in your web browser to be taken to the new LimeSurvey installation. SaaS is a powerful platform, but self-hosted software is more flexible and customizable. Both options have their benefits. It is important to carefully consider your requirements before selecting which approach to take. The CenturyLink Cloud Blueprints library has many helpful self-hosted, automatically deployed software packages. Explore them today and find a solution that meets the challenges you face. Sign-up for our Developer-focused newsletter CODE . It's designed hands-on by developers, for developers. Keep up-to-date on topics of interest, including: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tutorial-backing-up-your-bitbucket-repositories-to-object-storage", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/tutorial-backing-up-your-bitbucket-repositories-to-object-storage", "abstract": "Software development teams rely on distributed revision control systems to keep from descending into unmanageable chaos. These days, teams often favor web-based revision control platforms offered as Software-as-a-Service (SaaS) products. The data stored on these repositories is the lifeblood of a company, so it's important to keep backups elsewhere. In this tutorial, we will outline a method for mirroring Bitbucket repositories and backing them up in CenturyLink Cloud Object Storage. CenturyLink Cloud Object Storage offers enterprise-grade object storage. Our cloud servers store and manage your files in a highly-scalable, fault-tolerant, distributed datastore. For large-scale cloud applications, object storage is far more efficient than hierarchical file systems. Learn more about Object Storage . Bitbucket is a web-based hosting service for software projects using Git or Mercurial revision control systems. It is part of Atlassian's powerful suite of SaaS tools for software development and project management. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to access CenturyLink Object Storage. This tutorial assumes that you have a Bitbucket account with at least one Git repository to back up. If you don't already have a Bitbucket account, go to http://bitbucket.org and sign up for free. The Bitbucket CLI allows you to interact with Bitbucket without visiting the website. The CLI allows you to execute a number of useful operations, such as creating new repositories, managing repository access, or view metadata about your account. Run the following command to install the Bitbucket CLI. sudo pip install bitbucket-cli In a text editor, create a file called .bitbucket in your home directory. Edit $HOME/.bitbucket to look like the following, replacing \"your-username\" and \"your-password\" with your Bitbucket username and password. Run chmod 0600 $HOME/.bitbucket to restrict permissions on your configuration file. Before you can use Object Storage to back up your repositories, you need to add an Object Storage user and bucket. Go to your CenturyLink Cloud Control Portal at https://control.ctl.io/ . From the Navigation Menu, click Services > Object Storage . Click the Users tab. Click Create User . Fill out the form. Note: Keep in mind that the email address you use will be unique across the Object Storage platform and cannot be changed or reused with a different CenturyLink Cloud account. Click save . You will return to the \"Users\" tab. Click on the name of the user you just created. The user screen displays the access key id and secret access key . Note: Save these for later! You will need them to configure S3cmd to access Object Storage. Click on the Buckets tab to create a new Object Storage bucket. Click create bucket . From the Owner drop-down menu, select the user you created in Step 3. Record the bucket name, as you will need that to run your backup with S3cmd. If you have never used Object Storage or other S3-compatible systems, some of the terminology might be unfamiliar. Throughout this article, we will use the \"Canada\" region in the examples. However, if you choose to use US-East as the data center for your buckets, you will need to change all references to \"canada.os.ctl.io\" to \"useast.os.ctl.io\". We will use S3cmd to copy data into an Object Storage bucket. This handy command line utility was originally built to access Amazon Simple Storage Solution (S3), but it works with any compatible service. Before we configure it to interact with CenturyLike Cloud Object Storage, you will need to install it. Check out the S3cmd website for installation instructions for any Linux or OS X system. For example, to install S3cmd on an Ubuntu 16.04 system, use the following command: sudo apt install s3cmd Next, we need to configure S3cmd to use Object Storage. Follow these instructions: Run the following command: s3cmd --configure Enter the key and secret key from the Object Storage user creation process. \"Test access with supplied credentials?\" appears. Type \"no\". \"Save settings?\" appears. Type \"yes\". This creates the file .s3cfg in your home directory. Open ~/.s3cfg in a text editor. Find the lines starting with host_base and host_bucket . Edit them to look like this: Note: For an in-depth look at installing and configuring S3cmd, check out our Knowledge Base article on the topic . The next step is to mirror all of your Bitbucket repositories. Create a directory to hold your repository mirrors with the following commands: With a text editor, create a file called mirror.sh in the new directory and edit it to look like this: Run the new file as a script to mirror all of your Bitbucket repositories with this command: bash ./mirror.sh Finally, sync your backup directory with your Object Storage bucket. Run the following command, replacing $MIRROR_DIR with the full path of the directory in which you mirrored your Bitbucket repositories. Replace $BUCKET with the name of your Object Storage bucket from step 12 of \"Create a Bucket to Back Up Your Repositories\": s3cmd sync --delete-removed \"$MIRROR_DIR\" s3://$BUCKET/ You can schedule your backup to run automatically using Cron. The following example will schedule a backup to run every Sunday. Run the following command to open your default text editor and edit your crontab file: crontab -e Edit the file to look like the following example, replacing $MIRROR_DIR with the full path of the directory in which you mirrored your Bitbucket repositories, and $BUCKET with the name of your Object Storage bucket. If Cron isn't available on your system, you can use your favorite job scheduler to do something similar. You have set up a basic backup scheme to mirror your Bitbucket repositories to Object Storage. Now you can customize the solution to address your particular requirements. A more sophisticated script might use all bare Git repositories, or might take advantage of the Bitbucket CLI to mirror both Git and Mercurial repositories. S3cmd and the Bitbucket CLI are powerful tools for sharing data between cloud services. Be sure to explore our knowledge base and the tool documentation to learn how to unleash their full potential and leverage other hybrid cloud solutions.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "automation-coreos-cluster-deploy", "author": ["\n              Daniel Morton\n            "], "link": "https://www.ctl.io/developers/blog/post/automation-coreos-cluster-deploy", "abstract": "CoreOS is a lightweight, Linux-based open source operating system that provides infrastructure for clustered deployments. The focus of the operating system is on automation, ease of application deployment, security, reliability, and scalability. CoreOS bundles code into an application container along with the dependencies that it needs to run properly, which includes built-in mechanisms for service discovery and configuration sharing. There are several advantages to using a CoreOS cluster deployment. Our Blueprint technology automates the process. Basically, all you do is specify the data center location, refine your parameters, and determine how many servers you want in your cluster. There are basically three steps in deploying a CoreOS cluster on the CenturyLink Cloud solution. This tutorial will take you through the sequence. If you don’t have a CenturyLink Cloud account yet, no problem. Head over to our website and activate your account . You’ll need it to access CenturyLink Cloud Blueprints for the DHCP-PXE Server and the CoreOS Server. Now that you have an account, let's get started. You can reference our Creating and Deleting VLANs article if you like for additional details on this procedure. After successfully deploying the Blueprint you will see a new server listed in the group and data center you specify during the deployment. The new server acts as the DHCP and PXE server for the CoreOS servers to be deployed in subsequent steps. You can go back and change any of the setting by clicking the outline on the left of the Blueprint. Click deploy blueprint . Now that the network and the DHCP-PXE Server are in place, it's time to build the CoreOS server and install CoreOS on a remote server. A major benefit of the CoreOS is that distributing software is now so easy. CoreOS gives you the ability to manage services across an entire cluster from a single point. CenturyLink Cloud Blueprints and the Control Portal provide the tools that make the process of building out a cluster very simple. Here are some other topics that may be of interest to you. Sign up for our Developer-focused newsletter CODE . Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. CenturyLink Cloud – We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "private-cloud-file-sharing-object-storage", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/private-cloud-file-sharing-object-storage", "abstract": "File sharing is perhaps one of the most common cloud services. Many popular services are available, such as Dropbox, Google Drive, Box, and Microsoft OneDrive. However, an enterprise often needs file sharing services that are strictly-controlled internally. This tutorial will outline how to use the ownCloud universal file access platform to deploy private cloud file sharing by using CenturyLink Cloud Object Storage. ownCloud Community Edition is the Open Source version of ownCloud. It provides users with seamless access to files on many different devices and services, all from a centralized location that is under the control of your enterprise. It uses an open, modular architecture to provide extensibility and expansive federated cloud sharing capabilities. You can store and manage your files in a highly available repository with CenturyLink Cloud Object Storage . The product is designed for the enterprise. That means high availability with automatic replication. Our cloud servers store and manage your files in a highly-scalable, fault-tolerant, distributed data store. For large-scale cloud applications, object storage is far more efficient than hierarchical file systems. CenturyLink Cloud Compute servers are high-performance cloud servers. These enterprise-grade virtual machines are easy to deploy and manage from the CenturyLink Cloud Control Portal and via powerful API calls. CenturyLink Cloud Blueprints offer deployable configurations that can bring a new environment online in a matter of minutes. Custom apps and pre-packaged software are always deployed to their ideal settings, thus eliminating problems related to misconfiguration and mismanagement. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to access CenturyLink Cloud Blueprints and Object Storage. Follow the steps below to use the CenturyLink Cloud Console to add an Object Storage bucket and user to your account. To create a new Object Storage user, follow the steps below: To create a new Object Storage bucket for storing digital assets, follow the steps below: For more information on managing Object Storage buckets and users from the Control Portal, check out this Knowledge Base article. If you have never used Object Storage or other S3-compatible systems, some of the terminology might be unfamiliar. Throughout this article, we will use the \"Canada\" region in the examples. However, if you choose to use US-East as the data center for your buckets, you will need to change all references from \"canada.os.ctl.io\" to \"useast.os.ctl.io\". In this tutorial, we will deploy ownCloud using a CenturyLink Cloud Blueprint. To start the deployment process, follow these steps: The ownCloud Blueprint customization only requires that you fill out a form and wait for provisioning and deployment to complete. Follow the steps below to complete the process. Once your ownCloud Blueprint has finished deploying, we can move on to configuring it for Object Storage. Note that the Blueprint uses ownCloud Community Edition. With this free version of ownCloud, we can only add a shared directory that is Object Storage backed. However, ownCloud Enterprise Edition , a commercial product, allows Object Storage to be its primary storage method . To configure ownCloud, follow the directions below: Just like other popular cloud file sharing services, ownCloud has a fully-featured desktop client available for Windows, MacOS, and Linux. The desktop client gives you the ability to sync folders across machines, with everything backed by CenturyLink Cloud Object Storage. Clients are also available for iOS and Android. ownCloud has a number of powerful features. Among them are user and group-based access control, cross-server federated file sharing, and server-side encryption. All of these can serve to enrich your file sharing experience while taking full advantage of Object Storage capabilities. CenturyLink Cloud Object Storage is already a powerful tool for storing and delivering digital assets. Its capabilities can be used to power a fully-featured secure enterprise file sharing solution with ownCloud. The CenturyLink Cloud Blueprints library has many helpful self-hosted, automatically deployed software packages. Explore them today and find a solution that meets the challenges you face. Sign-up for our Developer-focused newsletter CODE . It's designed hands-on by developers, for developers. Keep up-to-date on topics of interest, including: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "private-cloud-enterprise-calendar", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/private-cloud-enterprise-calendar", "abstract": "Every enterprise needs a central calendar solution with collaboration support. However, many Software-as-a-Service (SaaS) solutions host this information in a public cloud. Many enterprises prefer to host sensitive calendar information in a private cloud. In a previous tutorial, we showed you how to how to use the ownCloud universal file access platform to deploy private cloud file sharing using CenturyLink Cloud Object Storage. In this tutorial, we will explain how to deploy an enterprise calendar system using the ownCloud Calendar application. ownCloud Community Edition is the open-source version of ownCloud. It provides users with seamless access to files on many different devices and services, all from a centralized location that is under the control of your enterprise. It uses an open, modular architecture to provide extensibility and expansive, federated cloud sharing capabilities. CenturyLink Cloud Compute servers are high-performance cloud servers. These enterprise-grade virtual machines are easy to deploy and manage from the CenturyLink Cloud Control Portal and via powerful API calls. CenturyLink Cloud Blueprints offer deployable configurations that can bring a new environment online in a matter of minutes. Custom apps and pre-packaged software are always deployed to their ideal settings, thus eliminating problems related to misconfiguration and mismanagement. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to access CenturyLink Cloud Blueprints. In this tutorial, we will deploy ownCloud using a CenturyLink Cloud Blueprint. To start the deployment process, follow these steps. From the Navigation Menu on the left, click Orchestration > Blueprints Library . Enter \"ownCloud\" into the Refine Results search box and click go . Click Install ownCloud on Linux x64 . When the Blueprint summary and estimate are displayed, click deploy blueprint . The ownCloud Blueprint customization only requires that you fill out a form and wait for provisioning to complete. Follow the steps below. Fill out the Deploy Install ownCloud on Linux x64 Blueprint form. The values in the Login and Password fields are used for your ownCloud administrator account. Make sure you remember them! Pay particular attention to the Password field in the Build Server(s) section, which needs your CenturyLink Cloud account password. Click next: step 2 . After reviewing your settings, click deploy blueprint . You will now see the Deploy Blueprint screen with a progress indicator. Wait for the progress indicator to indicate that your deployment has finished. Once your ownCloud Blueprint is deployed, you can configure your ownCloud instance and enable the Calendar application. To configure ownCloud, follow the directions below. Log in to the Control Panel . From the Navigation Menu, click Infrastructure > Servers . Locate the new ownCloud server. In the SERVER INFO section, look for IP ADDRESS(ES). The IP address in public address space is your ownCloud server's public IP address. In a web browser, visit the server's IP and click Access ownCloud . Enter the Login and Password values from Customizing the Blueprint into the Username and Password fields on the ownCloud login form. In the upper left-hand corner, click Files > Apps . In the Apps screen, click Productivity . Scroll down the list of apps until you find Calendar . Click Enable under the External storage support app. In the upper left-hand corner, click Apps > Calendar . The Calendar interface allows you to add new calendars and share them with ownCloud users and groups. Multiple calendars are supported, with common scheduling features such as inviting and tracking appointment attendees, alerts, and event categorization. The ownCloud Calendar has a powerful CalDAV interface that allows it to be integrated into other popular applications. To access the CalDAV link for a calendar, follow the steps below. In the Calendar app, click on the three dots next to the calendar. Click on the chain link icon to access the CalDAV link. Check out this list of CalDAV clients to find software for many different platforms. Integrate your calendar with iOS, Android, and popular office software such as Microsoft Outlook and Mozilla Thunderbird. The ownCloud apps collection has dozens of different products that serve to transform it into a private cloud collaboration portal. These include webmail, a contacts manager, and collaborative document editing using LibreOffice. The CenturyLink Cloud Blueprints library has many helpful self-hosted, automatically-deployed software packages. Explore them today and find a solution that meets the challenges you face. Sign-up for our Developer-focused newsletter CODE . It's designed hands-on by developers, for developers. Keep up-to-date on topics of interest, including: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "relational-db-hosted-cloud-dev-test", "author": ["\n              Daniel Morton\n            "], "link": "https://www.ctl.io/developers/blog/post/relational-db-hosted-cloud-dev-test", "abstract": "While leveraging a cloud service provider (CSP) to run a dev/test environment is nothing new, it’s probably easier now than ever before to make it happen. Lots of companies (large, medium, or small) leverage the flexibility and scalability of the cloud to run these environments. From a development perspective, the advantages of cloud dev/test make the proposition an attractive one to consider: In short, an agile development environment in the cloud allows developers to scale lab infrastructure rapidly. Cloud testing provides a safe and simple way to help transition a database through the application development process while shifting all of the costs from CapEx to OpEx. CenturyLink’s Relational DB is a MySQL-compatible Database-as-a-Service product that can be leveraged in test and dev environments to help with speed and agility. Part of the value of Relational DB is that the developer can quickly spin up a database instance to support their application development needs without having to worry about the compute layer or database configuration details. Once a developer has their application running as they like in their dev environment, they can easily move their database into production with additional resource and availability configurations simply by leveraging the service's backup and restore capabilities. Once an application is in production, users can similarly pull back production data into a test database quickly and easily to support regular testing needs. High Availability Larger application development environments may require more high availability. Relational DB offers in-data center replication with auto-failover. When purchased, a secondary instance is provisioned on a different host in the same data center. Data is replicated from the primary instance to the secondary instance in real-time and the service monitors the health of the primary database instance. If the primary instance is unavailable, we will fail over to the secondary instance. This tutorial walks you through how to provision a MySQL-compatible database instance on CenturyLink Cloud and then move that data to a new production database instance using backup and restore functionality. CenturyLink Cloud Relational Database supporting rapid software delivery needs by providing instant access to a high performance, enterprise-hardened MySQL-compatible Relational DB Service instance built on our cloud platform with 100% flash storage. Control Portal is our intuitive management interface that reduces the time required to administer cloud environments. Self-service functionality enables you to set up environments, add, modify, or remove capacity at will. Note : You can also use API  calls to manage your dev/test environments. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to access our Relational Database service. We provide all the tools and functionality developers love, from a robust open Platform-as-a-Service (PaaS) to dev/test environments that spin up and tear down quickly. Check out our products and services. We’ve designed them to abstract the complexity of enterprise development away, so it’s easier for you to accelerate innovation for your apps. Sign-up for our Developer-focused newsletter CODE. Designed hands-on by developers, for developers. Keep up-to-date on topics of interest: tutorials, tips and tricks, and community building events.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "interactive-api-docs", "author": ["\n              Adam DuVander\n            "], "link": "https://www.ctl.io/developers/blog/post/interactive-api-docs", "abstract": "APIs are like opinions: everybody has one... and most of them are bad. Luckily, the worst of these APIs are not for public consumption, just intended for a company's own engineers or partners. But why should they suffer? While there is plenty within API design that can affect quality, the most common reason for a bad experience is poor documentation. Developers have been raising the bar on what is expected of docs. Among the almost standard fare now is interactive documentation. In this post, we'll show what it is, how you can get it for your own APIs, and how you can make your developers really happy. Typical documentation explains an API's endpoints or methods, what data is expected with a request, and what is returned in the response. This is important information and should be available from any API. Interactive documentation takes it a step further. Developers can see examples, but also fill in their own inputs. With a click of a button (that's the interactive part), developers see results on an actual API call to the service. Everyone wants complete, accurate documentation. When your understanding can be augmented by seeing it in action, you're more likely to grasp what can be accomplished. The best way to add interactive features to your documentation is to first describe your API. In the days before RESTful APIs, SOAP web services were documented with WSDL files. Standing for Web Service Definition Language, the XML format covered methods that an API can invoke and the type of data it receives. While SOAP is still used, most APIs now are based on REST (REpresentational State Transfer). There are a handful of ways to \"describe\" REST APIs: Each description or definition format has a bunch of tools around it, some of which are included in the section below. There isn't a compelling reason to create your own interactive documentation. You can choose from many projects that read in one or more description formats to make your documentation interactive. Unless you have incredibly customized needs for your interactive documentation, it makes sense to use something off-the-shelf. DreamFactory , a mobile app backend, comes bundled with a Swagger-based interactive documentation framework. By connecting your database and creating a description, you get an API and documentation that can interact with it. You can try a developer trial account or deploy the DreamFactory blueprint on any CenturyLink Cloud virtual machine. Similarly, you can use the Swagger-based tool directly. Called ReDoc , it's the next version of what was previously called SwaggerUI. If you've chosen a description other than Swagger, you might look elsewhere: While the tools are often related to the descriptions, they don't have to be. You can find tools to convert between description formats. Or use an online tool, like API Transformer , which supports many different input and output formats.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tutorial-how-to-create-a-docker-swarm-on-centurylink-cloud", "author": ["\n              Richard Case\n            "], "link": "https://www.ctl.io/developers/blog/post/tutorial-how-to-create-a-docker-swarm-on-centurylink-cloud", "abstract": "The release of Docker Engine v1.12 introduced swarm mode for creating and managing a cluster of Docker Engines, which is referred to as a swarm. This replaces Docker Swarm, which was previously a separate application. A Swarm is a collection (cluster) of nodes running Docker Engine following a decentralized design where services can be deployed. Swarm mode includes scaling, service discovery, desired state and many other features. For an overview of Swarm mode highlights, see our post on Docker Swarm Integrated into Docker Engine . By using the CenturyLink Cloud plugin for Docker Machine, customers can create a Docker swarm on the CenturyLink Cloud by running a small number of Docker machine commands to create the nodes. Thus allowing the ability to run the Docker CLI commands required to create the swarm by setting the machine context using Docker Machine. The following requirements are needed to create a Docker swarm in CenturyLink Cloud: If you don’t have a CenturyLink Cloud account yet, no problem. Just head over to our website and activate an account . Creating a Docker swarm is composed of many steps.\nFor this tutorial the steps have been divided into the following sections: Environment Setup Node Creation Creating the Swarm Adding Manager Nodes Adding Worker Nodes Validating the Cluster 1) Decide how many manager and worker nodes you want in your swarm. The following table contains some advice: By default the manager nodes will be used to run tasks in addition to the worker nodes. The manager nodes are used to submit tasks and they perform the management and orchestration function of the swarm. If you are new to the ideals behind the protocol for implementing distributed consensus(RAFT), visit this slideshow for more information. 2) Open a command prompt and set the CLC_USERNAME, CLCPASSWORD and CLC_ALIAS environment variables for your CLC account. For example on Linux & OSX: The notes from step 1) apply to the worker nodes as well. 3) Once the nodes have been created run the following command to view all of the Docker hosts that have been created: The output should be similar to the image below: 1) Set the Docker environment variables for the first of the manager nodes by running: Depending on your OS the command will output a command that can be used to set the required environment variables. For example: 2) Create the swarm by running the following command: The command output will contain a command that will be used later to join worker nodes to the swarm. Save the command for later use: IMPORTANT: There are a number of command line options that can be specified when creating the swarm. Visit this document for more information. 1) While the Docker environment variables are still set to the first manager node run the following command: This output will contain the command that will be used to join additional manager nodes to the swarm: 2) Set the Docker environment variables for the manager node to join to the existing swarm by first running: Once again, depending on your OS the command will vary. This output will contain a command that can be used to set the required environment variables: 3) Run the command that was output in step 1). For example: The output of the command should indicate that the node joined the swarm as a manager. 4) Repeat steps 2) & 3) for each of the remaining manager nodes that need to join the swarm. 1) Set the Docker environment variables for the worker node to join to the existing swarm by first running: Based-on your OS, run the command from the output to set the require environment variables. For example: 2) Run the command that was the output in step 2) of the Create the swarm section. For example: The output of the command should indicate that the node joined the swarm as a worker. 3) Repeat steps 1) & 2) for each of the remaining worker nodes that need to join the swarm. 1) Run the following command to list all the nodes in a swarm with their status: Output should be similar to the image below: This output shows important information including the status of each node and which manager has been elected the leader. Now that you have a working Docker swarm running in CenturyLink Cloud, you are ready to start managing and deploying services to the swarm. It is recommended that you follow the swarm mode tutorial . You can find more Docker tutorials and Resources below, or just search 'Docker' or 'Containers' in the search field on Developer Center home page .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tutorial-streamline-your-application-development-with-cloudsoft", "author": ["\n              Brandy Smith\n            "], "link": "https://www.ctl.io/developers/blog/post/tutorial-streamline-your-application-development-with-cloudsoft", "abstract": "If you're a developer looking to streamline the process of launching your applications in the cloud, Cloudsoft offers a software that helps cut down on the provisioning and manual operations of runbooks, allowing developers the ability to design, deploy, and operate at high velocity. Cloudsoft Application Management Platform (AMP) is designed to integrate the development and operation aspects of an application. AMP orchestrates services, platforms, and infrastructures to ensure they directly meet the needs of applications, dynamically and in real-time. This results in more reliable operations, productive development, and greater agility for IT to respond to their business needs. AMP is built on the open-source project Apache Brooklyn , with Cloudsoft's commercial support and additional enterprise features. CenturyLink Cloud works with Cloudsoft to provide agility and reliability throughout the application lifecycle. There are many things that Cloudsoft AMP brings to the cloud, including cloud-native modeling and steady platforms that provide consistency across environments. In addition, Cloudsoft AMP helps to expand a developer's capacity for productivity by allowing he/she to focus on the reliability of environments, rather than just developing the basic features for an environment to successfully operate. The key to Cloudsoft AMP's management environment is consistency, which plays a big role in how customers can operate within the environment. Applications are viewed within the same perspective, which allows for the simplification of any environment, from traditional on-prem to hybrid cloud and platforms. The modeling tools provide a full, up-to-date view of your new and existing applications. Highlights Include: Prerequisites Access to the CenturyLink Platform as an authorized user. If you are ready to get started, but are not yet a CenturyLink Cloud customer, no problem. Just head over to our website and activate an account . Impact After completing this tutorial, the user will be able to install and use Cloudsoft AMP Open Edition in the CenturyLink Platform. Postrequisite If you are deploying from \"CenturyLink Blueprints Library\" all the prerequisites are configured. There is no need to perform any tasks. Steps to Deploy Blueprint Login to the Control Portal . From the left side navigation menu, click Orchestration > Blueprints Library . Search for \"AMP\" in the keyword search on the right side of the page and filter for \"Monitoring\" Solution. Locate the \"Cloudsoft AMP 4\" Blueprint. Click the \"Cloudsoft AMP 4\" Blueprint, then in the following page click Deploy Blueprint . Deploy Cloudsoft AMP 4 Blueprint section where you will complete the following tasks: Specify password Confirm password Select Primary DNS (e.g., \"Manually Specify\") Optionally update Network, Secondary DNS, and Server type. Specify AMP Username (e.g., admin). Specify AMP Password. Specify CenturyLink Cloud account. Specify CenturyLink Cloud password. Select one or multiple CenturyLink Cloud data centers to configure as AMP locations, or \"(None)\" if none are needed. Accept the Cloudsoft Trial Software License Agreement Terms and Conditions. Click next: step 2. Verify your configuration details. Once verified, click the deploy blueprint button. You will see the deployment details along with an email stating the Blueprint is queued for execution.\nThis will kick off the blueprint deploy process and load a page to allow you to track the progress of the deployment. Monitor the Activity Queue Monitor the Deployment Queue to view the progress of the Blueprint.\nTo monitor progress, click Queue from the Navigation Menu on the left.\nOnce the Blueprint completes successfully, you will receive an email stating that the blueprint build is complete. Please do not use the application until you have received this email notification. After your Blueprint deploys successfully, please follow these instructions to access your server: Click on the link that brings you to the VM that hosts AMP 4 and grab the public IP address of that VM. Refresh the page if you cannot see the link, or just browse to the VM in the Control Portal. Browse to http ://PublicIpAddress:8080 and use the credentials specified at \"AMP 4 Install Script on AMP4-\" section. Start deploying AMP blueprints to the CenturyLink Cloud Data centers specified above. Immerse yourself in tutorials , Knowledge Base articles, and more .", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "user-authentication-saml-sso-linux", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/user-authentication-saml-sso-linux", "abstract": "Handling user authentication across multiple systems, networks, and applications is one of the most time-consuming IT tasks. Hybrid cloud environments make this more challenging as the complexity of cross-network security increases. Single sign-on (SSO) technologies provide a variety of solutions that aim to make user management and authentication simpler across all systems. Many SSO solutions have been developed over the years, from MIT Kerberos to Microsoft Active Directory. With web applications becoming more and more common, additional SSO solutions have become popular. For instance, any website that allows you to log-in using your Facebook or Twitter account is using an SSO-based protocol solution, such as OAuth. For enterprise applications, the industry has developed an XML-based open standard called Security Assertion Markup Language (SAML) to distribute authentication and authorization information to facilitate SSO. In this article, we will look at Shibboleth and SimpleSAMLphp , two SAML options available for adding SSO to your cloud-hosted web applications. There are a number of terms that need to be understood in order to get a firm grasp of SAML and SSO systems. The following terms are used by SAML specification and are needed to talk about deploying SSO solutions. Before knowing what elements of SAML-based SSO you will need to deploy, you need to know what a typical SAML session looks like. The process below outlines what a typical web application looks like when using SAML SSO. Remember that the IdP can use any sort of backend as long as it provides a SAML response. This means that authentication information can be stored in LDAP, Active Directory, or an arbitrary database. This extensibility means that SAML SSO can be built on top of your existing enterprise authentication system. If you don’t have a CenturyLink Cloud account yet, just head over to our website and activate an account . You’ll need it to access CenturyLink Cloud Compute servers . In a typical hybrid cloud configuration, the IdP resides on a private network in a secure segment, usually inside the enterprise. In this configuration, web applications in the cloud are service providers. However, there are times when you may want to deploy an IdP. For instance, if you are providing an SSO solution used by external clients or services outside your enterprise, it makes sense to deploy an IdP to a secure cloud host. Shibboleth has both SP and IdP packages. The Shibboleth Service Provider integrates seamlessly with Apache HTTPD and works with all SAML implementations. A Shibboleth SP on a virtual server can integrate with a wide variety of applications and can be used to authenticate against a secure enterprise IdP. Our first step is to deploy a new CenturyLink Cloud virtual server. Follow the steps below: In order to make your SP ready for production, you have to configure Apache HTTPD with additional information about your enterprise. These items are specific to your application configuration. So, while we can't give you additional steps, note the following: Check out the Shibboleth SP Apache configuration guide for more information. The Shibboleth SP configuration is located in /etc/shibboleth/shibboleth2.xml . Your enterprise provides you with configuration parameters for its IdP. Most of the changes you need to make will be located in the section labeled \"ApplicationDefaults\" of the configuration file. Read more about adding a new Identity Provider in the Shibboleth Configuration Guide . Finally, when you are ready to add Shibboleth SP authentication, read about how to \"Shibbolize\" your application . The SimpleSAMLphp library is a well-maintained, scalable SAML authentication platform that can be deployed quickly. Compared to Shibboleth, it might seem fairly simple and feature-light. It is a powerful and effective solution for integrating federated, enterprise-grade authentication into a PHP application. Our first step is to deploy a new CenturyLink Cloud virtual server. Follow the steps below: As with the Shibboleth SP configuration, this will vary widely based on your particular needs and environment. For details on particulars of Apache configuration, check the SimpleSAMLphp configuration guide . The following covers a very basic configuration. With a text editor, create a file called: /etc/httpd/conf.d/simplesamlphp.conf and edit it to look like this: As with the Shibboleth SP configuration, you have to make sure that you deploy SSL support for Apache HTTPD and double-check your configuration against your enterprise requirements. Read more about the specifics of SP configuration at the SimpleSAMLphp Service Provider QuickStart guide . SAML SSO requires a lot of configuration to get running, but it is the most robust way to securely extend your enterprise into the cloud. In addition, deploying an SP allows you to integrate with third-party SSO providers, such as OneLogin . Understanding the way that SAML SSO integrates with applications gives you the ability to deploy services that use external enterprise IdPs to authenticate users and clients. For an example of how one Software-as-a-Service product is offering SASL SSO integration successfully, read about Slack's SAML support . Sign-up for our Developer-focused newsletter CODE. Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "linux-lxd-hypervisor-containers", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/linux-lxd-hypervisor-containers", "abstract": "Reading Time : about 8 minutes. Virtualization covers a range of related topics, all having to do with emulation of servers or other computer systems. It runs the gamut from fully virtualized hardware, such as KVM or VirtualBox, to lightweight containers like Docker. Somewhere in between those two is operating system-level virtualization. Currently, the most exciting OS-level virtualization for Linux is LXC , or Linux Containers. Linux Containers (LXC) aim to create a virtualized container that offers an environment as close as possible to a fully virtualized machine. However, because it's a container, an LXC instance doesn't have the overhead of a separate kernel and full hardware emulation. This means that LXC instances can be deployed and managed quickly and efficiently. While LXC is great at what it does, its low-level command-line interface is not very user friendly. It requires a lot of specialized knowledge and complex configuration on top of that to use it effectively. Enter LXD . LXD is the newer, better way to interface with LXC. LXD provides a system-wide daemon, a new LXC command-line client, and an OpenStack Nova plugin. The daemon exports a REST API, which makes the entire LXD experience very powerful and extremely simple to use. Since it uses LXC as its underlying layer, LXD is able to provide robust security with underprivileged containers and resource restrictions. Its new command-line client is simple to use and intuitive. From multiple containers on a laptop to thousands on a production server, LXD is highly scalable. In this tutorial, we will deploy a CenturyLink Cloud Server and install LXD on it. We will then cover a few common LXD tasks and show you how to provision, deploy, and configure containers remotely. CenturyLink Cloud Compute servers are high-performance cloud servers. These enterprise-grade virtual machines are easy to deploy and manage from the CenturyLink Cloud Console and via our powerful API. These servers are designed for high-performance workloads, with top-tier CPU, RAM, and 100% flash storage. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to access CenturyLink Cloud Servers. Before we get started with LXD, we need to provision and configure a new Cloud Server. Follow the instructions below: The next step is to add a public IP address to your server. Follow the directions below. Follow the steps below to install and configure LXD: From a shell on your local machine, connect to your new server with the following command. Replace \"YOUR.VPS.IP\" with your server's public IP address: From the shell prompt on your server, run the following commands to install LXD: To configure LXD, run the following command: For Name of the storage backend to use , type \"dir\". Before we can deploy any LXC instances using LXD, we need to import images. LXD can import images via three different methods: remote LXD servers, the built-in image stores, and raw image files. We will import images for CentOS and Ubuntu using the built-in image stores. With this method, we can import and launch a new image with one command. Follow the directions below. To import and launch an Ubuntu container, run the following command: To import and launch a CentOS container, run the following command: To get a list of running images, run the following command: The command-line client makes interacting with containers simple. For instance, to update all packages on your Ubuntu container, run the following shell command: To execute an interactive shell on your CentOS container, run the following command: You can see a list of available options with lxc help . If you have a Linux workstation, you can manipulate containers in a remote LXD daemon. Follow these steps on your local Linux machine to connect the remote LXD instance. Replace YOUR.VPS.IP with the public IP address of your LXD server. You will also need the password you entered in Step 12 of Installing LXD. Install the LXD command-line client. In Ubuntu 16.04, this can be done with the following command: Add your remote LXD server to the client configuration with the following command: At the Admin password for ctl: prompt, type the password from Step 12 of Installing LXD. You can now manipulate containers running on your virtual server's LXD instance by prefixing their image names with ctl: . For example, to execute an interactive shell on the Ubuntu container you previously created, run the following command: Note: For non-Linux workstations that need to access LXD instances remotely, check out the list of API clients for LXD . You will see even more performance with a CenturyLink Cloud Bare Metal Server . Bare Metal servers are fully integrated into CenturyLink Cloud, allowing you to use LXD to create a powerful, container-based public cloud with full scaling and provisioning support. LXD also allows you to create your own images. The containers can be useful for provisioning and deploying development environments. Check out our article on moving from cloud-based development to on-premises installation for ideas on how hosted virtual servers and on-premises deployment can be used to create a hybrid cloud solution. Sign-up for our Developer-focused newsletter CODE. Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "boost-productivity-via-the-centurylink-cloud-api-chatops-and-hubot", "author": ["\n              Matt Schwabenbauer\n            "], "link": "https://www.ctl.io/developers/blog/post/boost-productivity-via-the-centurylink-cloud-api-chatops-and-hubot", "abstract": "One of the biggest advantages of working on a team with a DevOps culture is the ability to quickly react to the needs of the business. If demand for a particular piece of tooling or automation is identified, a solution can be rapidly delivered to end users. By quickly creating automation for common operational tasks, a team can greatly improve their productivity.\nWhile this is great for quickly responding to a business’s needs, it is easy for teams to become overwhelmed with the amount of tools they have available to them. Worse yet, what if you are on a cross-functional team with a variation in technical skill-sets? It can be challenging to create solutions for a common denominator, especially with the short development times that can accompany projects being rapidly released. Having to dig through documentation to relearn a tool you used three months ago can be a huge time sink. Ironically, tools that were meant to increase productivity can sometimes reduce it. One of the ways DevOps teams have solved these problems is through a concept called ChatOps. Originally coined by the GitHub team , ChatOps is a way to standardize and organize a library containing multiple tools and simple documentation into a chat client interface. By consolidating a user’s interaction with tooling into a format many people are comfortable with (i.e.: chat clients), teams can quickly leverage a wide array of tools.\nIn addition to the benefits provided by the standardization and organization of a team’s tooling, exposing your toolset in a chat client is also a great way to promote collaboration and education for available functionality. By integrating ChatOps with clients that have the concept of team channels, such as Slack , teams can see how their colleagues are using available tools and what they are working on.\nAs a Devops shop ourselves, the team at CenturyLink Cloud is no stranger to the rapid development of software. Many of our engineering teams have adopted ChatOps tools to automate their workflows. Things like monitoring, deploying, and reporting have been automated and exposed by our Slack instance. Being on the Customer Excellence team at CenturyLink Cloud, I was often quickly developing reporting tools, mostly via PowerShell scripts, that would take some simple input from a user, query our APIs for data, then parse everything together and export a user-friendly report. Having a wide customer base with diverse reporting needs, I found myself managing a repository of dozens of tools. To make matters worse, we had to frequently support a sales force that sometimes wasn’t technologically comfortable with having to run a script or command line tool to generate a piece of information they needed. Instead of burning a large amount of time developing intricate front end interfaces for our customer reporting tools, I decided to follow the example set by our engineering teams and use ChatOps to solve our problems. Meet Jarvis . Jarvis is a Slackbot that has functionality built on top of GitHub’s Hubot to enable anyone in our organization to quickly retrieve information about any of our customers. For example, what if I wanted to know how many servers a particular customer has running? Or what data centers a customer has a footprint in? Maybe I just want a quick snapshot of a customer’s environment. Users are able to quickly access a list of functionality via Hubot’s built in help command. Hubot is written in CoffeeScript on Node.js. Since most of the tools I was developing for the team were reports created using data retrieved from the CenturyLink Cloud API using PowerShell, I either needed to redevelop all of the tools I created from scratch, or find a way to kick off and retrieve data from PowerShell scripts from within Node.js. Luckily, there is a resource called PoshHubot created by Matthew Hodgkins that facilitates this exact functionality. After installing PushHubot by following the documentation on GitHub, developing new functionality requires a PowerShell script and an accompanying CoffeeScript file. Let’s use Jarvis’s server count functionality as an example. The PowerShell code to retrieve the data from the CenturyLink Cloud API looks like this: First we have to declare the function, parameters and a hash table to store a response: Next we need to log in to the CenturyLink Cloud APIs . I do this via functions within a PowerShell module I made in order to reuse the same code in all of Jarvis’s scripts: In order to get a customer’s server count across each data center they use, we need to call the API for the list of locations the customer has access to: Now we can iterate through our data center list and query the API for the customer’s servers in each location: Finally, we can send a successful result back to Hubot, or a message if we were unsuccessful. In order for things to look correct in the eventual Slack output, we need to follow Slack’s formatting guide with our output: Now we need to create the accompanying CoffeeScript file. We start by including Edge.js which allows Node.js to call PowerShell, and add instructions for Edge to call Powershell: After that, we can do a regex capture to store a specific Slack message in a variable, which will trigger the Hubot listener and can be stored into a variable to be sent to PowerShell: Next, we build the PowerShell callback: One last thing, we need to add some logic to notify the user whether or not the script was run successfully: Finally, we can call PowerShell: By following the PoshHubot framework I laid above, we are able to create powerful functionality with the CenturyLink Cloud API, such as reporting on customer utilization. If you would like to learn more about Hubot, you can read Matthew Hodgkin’s guide about PoshHubot . If you’re curious about Jarvis, you can see all of the functionality we have developed in this GitHub repository . Get started on CenturyLink Cloud in minutes. Just activate an account !", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tutorial-drive-a-full-lifecycle-with-jenkins-kubernetes", "author": ["\n              Guillermo Sanchez\n            "], "link": "https://www.ctl.io/developers/blog/post/tutorial-drive-a-full-lifecycle-with-jenkins-kubernetes", "abstract": "This time we are going to explain you how the Kubernetes -CI Jenkins plugin works, taking advantage of containers and Kubernetes (K8s). In summary, this plugin provides: Jenkins slave lifecycle: it manages Jenkins slaves with Pod definitions into Kubernetes clusters. It is no longer necessary to have slaves deployed in advance, as the time it takes to deploy them using containers and Kubernetes is minimum. Autodiscovery: here is a smooth integration between the plugin and K8s; if the Jenkins Master is deployed into a K8s cluster, the plugin discovers it and populates the cloud configuration for you. Moreover, a default Pod configuration and a default repository configuration pointing to Helm chart are also provided. CI/CD pipeline: our vision is that charts are a key element to build them, so you can deploy and delete charts through build steps provided by the plugin. As an ElasticKube believer, I’m going to use it in order to show how the things are going, but you can also use kubectl to do that. Our starting point is a Kubernetes cluster up and running. Have a look at this doc in case you need help to create a cluster, but keep in mind that by using ElasticKube (Getting started),  all the processes will be easier. Just by executing: We are going to use the Jenkins chart provided by Helm in order to deploy our Jenkins Master into the Kubernetes cluster. Using ElasticKube you can configure the repository of your choice, and here we are pointing to a Helm repository fork, where we configured the Jenkins Chart with a fixed NodePort at 30800. Let’s now deploy the Jenkins chart. When we click on play button, this is what you see: (We configured a new label called “jenkins-type”, with “master” as value). Click on Deploy button. As you can see above, after a few seconds we find a new Jenkins service, a replication controller and the Pod that will be the Jenkins Master is still deploying. Again, using kubectl, you will be able to see the same in the command line. Eventually, the Pod finishes to deploy: Now, we will be able to access to our Jenkins service through 30800 port: To continue, we will go to Manage Jenkins area and we are going to install our Kubernetes-CI plugin. Below, this is how Jenkins installs the plugin and restarts. Once the restarting has been completed if we go again to the Manage Jenkins area and in the system configuration page, you will find how the plugin has auto-discovered the Kubernetes Cloud Configurations for you. Also, note that you will find the previously mentioned default Pod and charts repository configurations. Now that we have everything we need, we will create our Jenkins jobs.  Once you have created it, if you go to Build Steps area, you will be able to deploy the Charts from the repository that you have configured in the Cloud configuration. Once you click on Kubernetes-Deploy Chart, you have to configure the proper Build step. In this case, we will deploy a rabbitmq chart: Once we have completed the Build Step, we can enqueue a new build. To run a build, Jenkins will provision a new Jenkins slave. In the following steps you will see how this new slave is provisioned and executes the Build. As a result of the Build, you will be able to see how the rabbitmq has been deployed in the Kubernetes cluster. Note how the new slave has been provisioned first. Once the slave is up and running it executes the rabbitmq chart deployment in Kubernetes. The build finishes removing the slave, as well as the rabbitmq chart as we have configured it in the build step. You can check both components have been deleted in the ElasticKube instances screen, as it is also stated in the Jenkins Console, where you can see the Output of the job execution: Want to Learn More About Cloud Application Manager and ElasticKube? Cloud Application Manager is a powerful, scalable platform for deploying applications into production across any cloud infrastructure – private, public or hosted. It provides interactive visualization to automate application provisioning, including configuration, deployment, scaling, updating and migration of applications in real-time. Offering two approaches to cloud orchestration — Cloud Application Manager and ElasticKube — enterprise IT and developers alike can benefit from multi-cloud flexibility. Visit the Cloud Application Manager product page to learn more or contact us for a no-obligation, custom consultation.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "multi-tier-load-balancing-cedexis", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/multi-tier-load-balancing-cedexis", "abstract": "When your web application grows in popularity, it becomes more and more difficult to efficiently provide fast service to all of your users. This tutorial explores ways of using Cedexis Openmix to intelligently route traffic across multiple application origins, providing the fastest possible service to each end user. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to access CenturyLink Cloud Compute servers and other services. You will also need a Cedexis Portal account. Visit the Cedexis Portal new account page and create one. You will need this to set up a new Openmix load balancer. CenturyLink Cloud Network Load Balancer is a multi-tenant, scalable and programmable infrastructure presented to the customer as a service. It is based on Citrix® NetScaler®. Cedexis Openmix is a scriptable global traffic-management service that reacts to real-time data. It can be used to balance traffic across cloud providers, CDNs, and bare metal servers. A web application that needs to be high performance but also serves a large audience frequently needs to be hosted across multiple services. Dynamically generated content may be processed by application engines such as CenturyLink AppFog , while static assets such as images, PDFs, and JavaScript are served by a content delivery network (CDN). However, because of the complexity of the Internet, it's nearly impossible for any single service provider to reach every end user with the most speed and efficiency. This challenge has prompted the development of a number of innovative solutions for spreading application traffic across multiple channels. The most sophisticated solutions, such as Cedexis Openmix, implement performance-based cross-provider load balancing using real user monitoring (RUM). Performance-based load monitoring uses a JavaScript tag or other monitoring code embedded in the web application to figure out which providers offer the fastest, most reliable service to each end user. This tutorial gets you started with a simple multi-tiered load balancing setup. Closest to your content, we will set up a CenturyLink Load Balancer to handle distribution of traffic across a number of services in a specific data center. Closest to the user, we will deploy a Cedexis Openmix application to balance traffic across content providers and CDNs. A CenturyLink Load Balancer can be used to distribute network traffic across multiple virtual private servers, network applications, or any service that can run over TCP. In this section, we will set up a load balancer to distribute traffic across virtual servers that all live in the same data center. Select the data center to host your new load balancer. Add a name and description. Click create load balancer . You will be returned to the \"Load Balancer\" page. It might take a few minutes for your new load balancer to be provisioned. Once your new load balancer is ready, click on its name on the \"Load Balancer\" page. You will be taken to a status and configuration page. Note: Record the IP address of your load balancer. We will need it to configure the Openmix Application later in the tutorial. In this example, we will only be adding one content delivery platform to our Cedexis Openmix application. However, note that this is a second tier of load-balancing that operates between the CenturyLink load balancer and the end-user. To take full advantage of Openmix, you will need to configure multiple delivery platforms so it has options to pick from. One solution that takes full advantage of CenturyLink Cloud's multi-homed services is to create a separate Load Balancer for each data center. For example, this means hosting web applications on multiple virtual servers in both VA1 and WA1, for example. Create a load balancer in each data center using the directions in the previous section to handle traffic distribution across each data center individually. Cedexis Openmix relies on delivery platforms to measure web application performance, and an application to distribute traffic across multiple platforms. To add platforms to Cedexis Openmix, follow these steps. Select the CenturyLink Cloud data center from the PLATFORM drop-down menu. Click NEXT . Repeat this process for each CenturyLink Load Balancer data center or other content delivery platform you wish to add. The Openmix application routes traffic to your CenturyLink Load Balancers based on the algorithm you tell it to use. Before you dive into configuration, let's take a moment to understand what the different options are. A Custom Javascript Application uses the Cedexis API to make complex decisions based on any number of metrics and variables collected by Cedexis tools. Optimal RTT uses Cedexis Radar and real user measurements to intelligently route network traffic along the fastest possible path. This is the option we will use. Round Robin is the simplest form of traffic management. It simply hands network traffic to each provider in turn. Static Routing routes one Cedexis Openmix application to a single service provider. To add a new Openmix application and configure it to use Optimal RTT, follow these steps: For APPLICATION TYPE , select \"Optimal RTT\" from the drop-down menu. Enter NAME , DESCRIPTION , and TAGS to suit your project requirements. Note: Record the CNAME for your new Openmix application for later configuration. Cedexis Openmix uses DNS to route traffic. In your DNS configuration create a CNAME record for your web application and point it at the CNAME for your Openmix application. If you look up your web application's address from the command line, it should look similar to the following: Note that the ending IP address (which is 206.128.136.10\" in this example) will change according to Cedexis's realtime performance measurements from your Internet provider. The next step is to separate the static elements of your web application and deliver them through a separate Cedexis Openmix application. This step usually includes deploying multiple CDNs, so it can be a little more complicated. The process can vary widely based on the CDNs you select, but most of the steps in this tutorial will still guide you to a successfully configured performance-based load balancer. Sign-up for our Developer-focused newsletter CODE. Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "self-hosting-git-repository-gogs", "author": ["\n              Erik Arneson\n            "], "link": "https://www.ctl.io/developers/blog/post/self-hosting-git-repository-gogs", "abstract": "Change tracking and version control are essential tools for software development. In fact, they are one of the pillars upon which projects succeed, both in the short and long term. Today, Git remains the version control system of choice for many teams, with almost half of all professional developers reporting that it was their primary choice. In spite of its widespread adoption, Git can be difficult to learn. As one software engineer put it, \"Git makes difficult things possible, and simple things so frustrating that I've thought about becoming a llama rancher.\" Online tools like Github and Bitbucket attempt to make Git easier for individuals and teams, but these services come with additional costs and risks . These risks and costs can be mitigated by self-hosting a Git service. In this tutorial we will show you how to use CenturyLink Cloud services to deploy Gogs , a lightweight, full-featured, and self-hosted Git service. If you don’t have a CenturyLink Cloud account yet, head over to our website and activate an account . You’ll need it to use CenturyLink Cloud tools. CenturyLink Cloud Compute servers are high-performance cloud servers. These enterprise-grade virtual machines are easy to deploy and manage from the CenturyLink Cloud Console and via our powerful API. Our first step is to deploy a new CenturyLink Cloud virtual server. Follow the steps below. Log-in to the CenturyLink Cloud Control Portal . From the Navigation menu on the left side, click Infrastructure > Servers . On the left-hand side of the server panel, click on the region for the server you want to provision. Click Create , and then click Server . Fill out the form for your new server. For Operating System , select Ubuntu 16 | 64-bit . Click Create Server . Your server provisioning request will enter the queue. You can watch the progress of your request on the screen. Your server is provisioned when the status of all tasks in the queue is complete. After the new server is provisioned, return to the Control Portal and click Infrastructure > Servers . Navigate to the region, find the new server and click on its name. Click the More menu, and then click add public ip . Check the boxes for HTTP (80) and SSH/SFTP (22) . Click add public ip address . To update your virtual server and prepare it for running Gogs, follow these steps. Log-in to the Control Portal . From the Navigation Menu, click Infrastructure > Servers . Navigate to the virtual server you provisioned above and click on its name. On the right hand side of the server status page, look for IP ADDRESS(ES) . Your server's public IP address will be underlined. From a shell or terminal on your local machine, connect to your virtual server with the following command. Replace \"YOUR.VPS.IP\" with your server's public IP address. Install the required Ubuntu packages with the following commands. Now you have to configure the NGINX web server to act as a reverse proxy for Gogs. Open /etc/nginx/sites-available/default with a text editor. Below the location / block, add the following code. At the shell prompt, run the following command to restart NGINX. Now we install Gogs to run under a dedicated Git user. This allows you to expand your Git self-hosting capabilities in the future with additional software. It also keeps users from accessing restricted accounts on your machine. To create the user account and install Gogs, run the following: Create the user and switch roles using the following commands. Download and install Gogs with these commands. Open the custom/conf/app.ini file with a text editor and edit it to look like the following. Replace YOUR.VPS.IP with the IP address of your virtual server. Run the Gogs server with the following command. You should see output similar to the following, which indicates that Gogs is up and running. In your web browser, go to http://YOUR.VPS.IP/gogs/ . Note: Replace YOUR.VPS.IP with the IP address of your virtual server. This takes you to the Gogs installation screen. From the Database Type drop-down, select SQLite3 . Set the application hostname to the IP address of your virtual server. Configure the rest of the settings according to your requirements. Click Install Gogs . In your web browser, go to http://YOUR.VPS.IP/gogs/ . Replace YOUR.VPS.IP with the IP address of your virtual server. This takes you to the Gogs log-in screen. Log-in and use the application (which is similar to how you would use Github). Now that you have Gogs set up and configured, it's time to get your team to use it! Try creating new repositories and cloning them, the same as you do with other Git services. You might also want to try configuring Gogs to use CenturyLink Cloud MySQL , our cloud-hosted managed relational database solution. In addition, you can use tmux to keep Gogs running while you're not logged into your server. Check out our tutorial on deploying NEM securely for hints on using tmux. Sign-up for our Developer-focused newsletter CODE. Designed hands-on by developers, for developers. Keep up to date on topics of interest: tutorials, tips and tricks, and community building events. We’re a different kind of cloud provider – let us show you why.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "converting-app-bootstrap4", "author": ["\n              Jim Phillips\n            "], "link": "https://www.ctl.io/developers/blog/post/converting-app-bootstrap4", "abstract": "Twitter Bootstrap has recently shipped an alpha release for version 4. There has been much written on the new and updated features since the August release. I decided to get my hands dirty and convert an existing application from version 3 to version 4.  As the release moves to beta and production, I will update this article.  My experience, hopefully, will make the process easier for others looking to do the same. The application I am converting uses AngularJS for the front end. The js and css files are managed using gulp to dynamically build index.html with minified css and js files. First things first, let's get the code.  One option is to just use the compiled CSS and js.  This is fine for the CSS if you just want to use the default styles. However, I want to customize the look and feel with our own branding, so I downloaded the source files.  For the JavaScript, I just use the compiled js in the dist directory. This is where I ran into my first problem; how to customize the CSS.  In the source file, there is a _variables.scss file. It would be easy enough to just edit the Bootstrap source file and compile the CSS with each build.  There are two problems with this approach: Bootstrap will inevitably upgrade its versions. We use Bower to manage client-side vendor code. Editing a copied version of Bootstrap would mean manual updates with any version change.  That's no fun.  Also, we would be checking vendor code into our repository.  That is just wasteful and potentially confusing. Instead, I utilize gulp to build the CSS for us: You will notice I have separated out custom-bootstrap.scss and main.scss . This is because the application also uses Angular plugins that depend on Bootstrap. I want to customize some of the CSS, both in Bootstrap and other vendors' CSS.  In order to build the CSS in the proper order, the compiled Bootstrap (with our variables definitions) must be separate.  The order of CSS in the minified file is: \" Hang on ,\" you say. \" I thought you could just set the variables to customize Bootstrap .\"  At least that's what I thought when I started, but it turned out not to be the case. The first thing I change is the primary brand color. This is easy enough. It is the $brand-primary variable.  Bootstrap uses a blue. We use a green. To allow easy customization of the major variables, Bootstrap uses $[variable-name]: [value] !default; the !default is what allows for overriding the variables.  Unfortunately, this is not used throughout the SCSS code. For example, in the new card component , the card-header background color uses a variable that is not overridable. Instead, you have to override the styling traditionally. I want to use the primary brand color for the card-header background. In order to do this, I created 3 SCSS files: _variables.scss , custom-bootstrap.scss , and main.scss . I separated out _variablesin order to include it in the other two: _variables.scss: custom-bootstrap.scss: Note that the local _variables import must be before the Bootstrap import. main.scss: Remember to include both your local _variables and bootstrap _variables for access to all. Perhaps this will change with the beta or prod releases. But even if all variables could be overridden in a local _variables file, there will be other components you will want to customize. This is a clean way to incorporate all variables in your custom main.scss file, allowing you to extend Bootstrap. This is where I get into the HTML changes that become necessary with the conversion. The first thing I did once I got Bootstrap installed and set up was to see how it looked with the existing HTML. Some elements were laid out wrong, others just did not display. First, I will go through some classes that are missing. Next, I will explain the edits necessary for some of the major components that changed. This is far from an exhaustive list, but it should hit on some commonly used components and give you an idea of what to look for. In some instances, Bootstrap decided to just drop the class altogether. The following sections should give you guidance for a global search and replace strategy. For each item, I suggest starting with a single instance to get the exact changes you want, and then doing the search and replace. I used the card component as an example of something that needed standard CSS overriding. It is also a new feature that replaces panels, wells, and thumbnails from Bootstrap 3. Our app uses panels extensively, meaning a rewrite of the HTML. This is simple enough. becomes Note : I use .card-inverse because of the earlier customization that uses our dark $brand-primary background-color.  This class will display a white font. This may make a comeback. The documentation mentions it and other sections have an -inverse , but it is not in the CSS as of the writing of this article. becomes When I started the app without any edits the first thing I noticed on the login page was that the form was all jacked up. .form-horizontal was a simple way to lay out forms with a label on the left and the form field on the right within a .form-group . It removed the need to add .row to the form-group div. Luckily, this is an easy fix. Also, it is probably a more clear way to lay out the form. In retrospect, I probably should have used .row from the start. becomes Removing this speaks to a larger philosophy with Bootstrap v4 of avoiding the use of margin-top. This is discussed in their reboot documentation. Basically, they say to avoid it because it may yield unexpected results. I can live with that.  Goodbye .page-header . Goodbye .margin-top . This was a gray background that we used for our 'cancel' buttons, with the 'save' using .btn-primary. Simply replace btn-default with btn-secondary .  See also the other button classes for more options. Gone. No replacement. This was perhaps the most disheartening change. I personally loved the glyphicons. Alas, I will live. There are several options, but all require rewriting the HTML to use the new implementation. The most similar that I found was Font Awesome . They are still font-based, and the implementation is practically identical. A simple Bower install with an update to the gulp build includes font-awesome. The rest is just a matter of finding a comparable image, doing a global search, and replace. Some components changed dramatically, at least in terms of the HTML used to render them. The changes were made with the intent to simplify the usage and allow for more flexibility on what element to use. Our application uses a common Navbar for accessing the various sections. Once I logged in, the Navbar did not show. Oops! I can't use the app. This is because in v3, the Navbar forced you to use a ul. With v4, this is no longer the case. That is a nice feature allowing flexibility. It also means rewriting HTML. Since there is no assumed field, you must explicitly set the nav-item. becomes You will notice in the previous example that the dropdowns also changed. The difference follows the same pattern as the Navbar. You have to explicitly set the .dropdown-item within the dropdown menu. Finally, other vendors provide plugins that rely on Bootstrap. Since the current version of Bootstrap v4 is in alpha, it is unlikely the vendors will convert their code any time soon. The current versions of some plugins use v3.  This provides a dilemma as an early adopter of v4. For the plugins that use features that are now gone (glyphicons) or dramatically changed (dropdowns), what to do?  Do we just hope the third-parties will update their code in time? Do we write our own components to duplicate the functionality? Or should we just find a non-Bootstrap alternative? Most likely, you are using the plugin because it uses Bootstrap and matches the look and feel of the rest of the application.  Ultimately, you must decide what is best for your use case. Personally, I love coding, so creating the functionality appeals to me.  However, my manager may have a problem with me spending time on writing something that already exists. Most likely I will try to find comparable plugins that do not conflict with the Bootstrap 4 changes.", "date": "2020-07-30"},
{"website": "CenturyLink", "title": "tutorial-automation-ansible-runner", "author": ["\n              Benjamin Harristhal\n            "], "link": "https://www.ctl.io/developers/blog/post/tutorial-automation-ansible-runner", "abstract": "CenturyLink's Runner enables teams, developers, and engineers to quickly provision, interact, and modify their environment on the CenturyLink Platform or third-party clouds, as well as on-premise and in private data centers. Runner brings together state-based and parallel job execution with multi-cloud and multi-data center execution. All in one powerful engine. Our goal is to make automation and orchestration easy and accessible to everyone, from the most advanced users to those new to the Cloud. Runner makes it easy to create jobs and execute them, regardless of your environment or skill level. This tutorial guides you through the steps necessary to build and execute your first Runner job. Runner works against any publicly accessible system and any system in an environment that supports VPN connectivity. This guide builds a Runner job for systems in the CenturyLink Cloud. This allows us to defer conversations about distributed executions for a later tutorial and just focus on developing the execution job. Outline If you don't have a CenturyLink Cloud account, no worries! Just head over to our website and activate an account . Once your account is set up we will collect a few credential related items before proceeding. We are going to need a Bearer Token to pass in with our job definitions. The Bearer Token provides the user info necessary to authenticate to the CenturyLink Platform prior to executing any tasks. The API URL for the request is: https://api.ctl.io/v2/authentication/login We can get our Bearer Token in an adhoc manner by executing the following CURL command: The result is a JSON formated collection of account information. In this collection you will see the Bearer Token we're after: Our example here creates a simple playbook which uses the CLC Ansible modules to create a server in our default group. The tutorial assumes that you have some experience with developing Ansible playbooks . So we focus primarily on the CenturyLink Cloud module necessary to create the server without going into any depth on the surrounding playbook. Here is our playbook: Now that the playbook is written, it needs to be stored in a GitHub repo. Please see Github for instructions on creating and publishing a project. We will create a Github project based on the following directory structure for our project: The GIT URL for the project, which we make reference to through the tutorial, is [https://github.com/clc-runner/runneruprunning.git] . The project can be found here . The CenturyLink Runner team has an online utility that helps create playbooks, but for this instance we build ours manually. We start with a basic job skeleton: Callbacks Callbacks allow you to provide a URL endpoint for a webhook. Each status message that is generated for internal logging will also be sent to the URL. You can set the level of messages you would like to receive. For example, if you wanted to receive all messages you can set the level to DEBUG. These messages are sent out real-time so you can follow along with the execution of your Job. TIP: There are several free webhook and debugging sites available. This example uses Webhookapp . Description The description attribute allows you to provide a detailed description of the purpose of the job. Repository The repository section is where you provide the information necessary to retrieve your playbook from Github. The first item needed is the URL to your Github repository. Next is the branch name where your playbook resides. If you do not provide a branch name, the default branch, as set in GitHub, is used as the branch. You also have to provide the name of the top level playbook you would like to have executed.  Last, you have to provide your GitHub credentials, unless the GitHub repo is a public repository.  The GitHub repo for this example is public so you don't have to provide the 'credentials' section. This flag signals the execution-runner to create an inventory of all the items in your account alias at runtime. Setting this flag to True isn't always necessary, especially if you are providing the name of the hosts you would like to execution the playbook against. Next, we ask for an immediate execution of our playbook once we submit the job definition. We are also going to use CURL to submit our Job request so that you can see what HTTP headers are necessary. Before we move onto submitting our job for execution, let's capture the job definition from above into a local file named JobDef.json . This way, we can reference our job through a file reference, which keeps our CURL command a little cleaner. So now our job submission request is as follows: ##The Elements of Our Request Are: The Method To create a Runner job we use the PUT method: The HTTP Headers As part of the HTTP Headers we specify that the content type of our request is JSON and then pass in our Bearer Token. Our Job Definition We captured our job definition into a file so we simply make reference to that file. The file should be local to the directory in which we are running the CURL command. The \"URL\" And finally, the URL endpoint: Note some key elements which we had to reference in the URL. First, we had to identify the service we wanted to process our request: Next, we had to provide our AccountAlias identifier: And the last bit of the URL signals the Job service to immediately execute the defined Job: And the result from our request is: Taking a closer look at the response you can see that it simply echo's back the job definition we just submitted, along with a couple of timestamps, an ID which is unique to this job definition, and some reference links we can use if we want to directly query for the status of the Job. Validation Now that the Job has completed you can go into your account where you should see a newly minted VM in your 'Default' group. To learn more on how to integrate Ansible Playbooks and CenturyLink Cloud's Runner, which is an automation and orchestration engine that we exposed as a service, check out How to Ansible with Runner on our CenturyLink Cloud blog . Go to the GitHub repo for Runner playbooks to get started writing more elaborate Playbooks and to learn more about Runner and its use cases .", "date": "2020-07-30"}
]