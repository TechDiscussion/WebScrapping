[
{"website": "Envato", "title": "\n          ", "author": ["Julian Doherty", "Jack \"chendo\" Chen", "Envato"], "link": "http://webuild.envato.com/blog/page/5/", "abstract": "November 28 , 2012 by Julian Doherty Using Stats To Not Break Search How do you change around pretty much everything in your search backend, but still remain confident that nothing has broken? (at least not in a way you didn’t expect). We can use statistics to do that. In particular, a technique called Spearman’s rank correlation coefficient . Let’s have a look at it, and see how we can use it to compare search results before and after a change to make sure relevancy rankings haven’t gotten screwed up in the process. Read on… November 26 , 2012 by Jack \"chendo\" Chen Moving the Marketplaces to Elasticsearch TL;DR: How we got from the top chart to the bottom chart. We have been playing with adding new facets to search on the Marketplaces, but Solr was not making it easy for us due to slow indexing and annoying XML schema files. I experimented with Elasticsearch during a dev iteration and decided it was worth switching. So we did it. And I’m going to tell you how we did it. Read on… November 25 , 2012 by Envato Welcome! We are the people who build the many Envato sites and products. We think we build great products in interesting ways, and this is our place to talk about it. At Envato, we help our community learn and earn online and in that spirit it is important for us to share what we learn. You’ll find out more about the technical choices we make, including the ones that don’t turn out as we hope. We’ll describe our approach to solving problems rather than simply adding features. Along the way, you will meet the team members and find out what makes them tick. Welcome to webuild.envato.com . The Team", "date": "2012-11-28"},
{"website": "Envato", "title": "\n          ", "author": ["Jordan Lewis", "John Viner", "Alan Skorkin", "John Viner", "Jordan Lewis", "Ben Smithett", "Mario Visic", "Lucas Parry", "Jack \"chendo\" Chen", "Lucas Parry"], "link": "http://webuild.envato.com/blog/page/4/", "abstract": "February 11 , 2014 by Jordan Lewis Making the Envato Marketplaces Responsive The first Envato Marketplace was started in 2006 (and 7 others have followed since then) and now we’re a good sized online business with a lot of traffic (well over 100m page views per month). Over the same period the mobile revolution has and continues to happen and like many other online businesses we want the user experience of our sites to be far better on mobile devices by making them responsive. While broadly this is not a unique challenge, we do have some nuances that we think make us a little bit different. In addition to a fairly large and evolving code base, some of the key pages on our sites like our item page (the equivalent of a product page on a typical e-commerce site and thus super important for conversion) are based around user generated content and assets. Although we are a while off being fully responsive I’d like to share our journey and some solutions we have developed to progressively convert the Marketplaces to be responsive. Read on… January 03 , 2014 by John Viner 10 Deploys a Day - A Case Study of Continuous Delivery at Envato I recently presented to the Bankwest Solutions Delivery Group on the processes and technology we use that allows us to deploy our sites up to 10 times a day. 10 Deploys a Day - A Case Study of Continuous Delivery at Envato from John Viner September 13 , 2013 by Alan Skorkin How Not To Fail When Presenting At An International Developer Conference A couple of months ago I presented at LoneStarRuby 2013 in Austin, Texas. While I’ve done a few presentations for work and locally in\nthe past, I’ve never presented at a proper developer conference before. I knew\nfrom experience that the standard of presentations at Ruby conferences will tend\nto be pretty high, so rather than risk being the token crappy presenter\n(that you occasionally see - and feel sorry for), I set out to do a bunch of\nresearch on what makes a decent presentation. Watching videos by such\npresentation luminaries as Ben Orenstein and Scott Hanselman ,\nas well as reading old favourites like Presentation Zen and Confessions of a Public Speaker .\nIn the next few paragraphs, I’ll show you my talk as well as try to distill\neverything I learned and hopefully save some fortunate developers hours of\neffort in the future. Read on… August 22 , 2013 by John Viner Ruby on Rails, still scaling at Envato after all these years Last week the Envato Marketplaces reached a milestone: for the first time we handled over 70 million page requests on our Rails stack in one week … that’s 10 million a day! Our Rails app powers seven separate Marketplace sites, the most well-known of which is Themeforest.net and based on this article it ranks in the top 5 Rails sites in the world by traffic. The marketplaces are for buying and selling a wide range of digital items, and we sell an item roughly every 10 seconds. A couple of our sellers–we call them authors–have sold $2,000,000 worth of items and another six have sold over $1,000,000. Read on… July 30 , 2013 by Jordan Lewis Techniques for mobile and responsive cross-browser testing: An Envato case study. Not so long ago, cross-browser testing meant firing up different versions of Internet Explorer, Chrome, Safari, Firefox and (possibly) Opera on multiple operating systems. Add in the ever-growing multitude of mobile devices now available, and it can be a real challenge developing your site to deliver a consistent experience to all. Read on… June 26 , 2013 by Ben Smithett How to Scale and Maintain Legacy CSS with Sass and SMACSS We’ve been big fans of SMACSS for a long time, however a\npure SMACSS approach works best with plain old CSS on a brand new project. On\nour marketplace sites , we write style sheets in Sass and\nhave several years’ worth of legacy CSS to deal with. We’ve recently been refining the way we do CSS to make it easier for our\ngrowing team to maintain our style sheets without throwing away our existing\nfront end & starting from scratch. What we’ve ended up with is an approach loosely based on SMACSS that\nstill solves the problems originally tackled by SMACSS & OOCSS , but with a few modifications\nand some ideas cherrypicked from places like BEM and Toadstool . Read on… March 27 , 2013 by Mario Visic Keith and Mario's Guide to Fast Websites At the first Australian Ruby Conference, Keith Pitt and I were lucky enough to present a talk on Fast Websites.\nWe were joined by over 20 other Envato developers attending the conference, in the sunny city of Melbourne. We wanted to demonstrate all of the performance gains that could be made using a \nreal application, so we created Giftoppr : a site that \nlets users share animated gif images using Dropbox. We also created a website \nbenchmarking tool called WBench , which we \nuse together with other existing tools throughout our presentation. We released \nboth Giftoppr and our WBench tool as open source software. In our presentation we take our slow loading Giftoppr application\n(initially it loaded in around 9 seconds) and set a goal to reduce the loading\ntime to be under 2 seconds. Using a variety of simple techniques we slowly chop\naway at the loading time until we reach our goal. Read on… March 19 , 2013 by Lucas Parry Happily upgrading Ruby on Rails at production scale The Envato marketplace sites recently upgraded from Rails 2.3 to Rails 3.2. We \ndid this incrementally at full production scale, while handling 8000 requests \nper minute, with no outages or problems. The techniques we’ve developed\neven let us seamlessly and safely experiment with mixing Rails 4 servers into \nour production stack in the near future. We wanted to be able to confidently make the huge version jump without having\nto do an all-or-nothing cutover. In order to achieve this we made a number of\nmodifications that allowed us to run Rails 3.2 servers side-by-side on our load\nbalancer with all of our 2.3 servers. This let us build confidence in our\nupgrade to the new version gradually with far lower risk of our users receiving\na bad experience. We’ve released the patches we used as a gem rails_4_session_flash_backport ( github ) that \nmagically lets Rails 2, 3, and 4 servers live happily side by side. If you are still stuck back on a Rails 2.3 app, this should help kick start your\nupgrade progress to Rails 3 (and beyond to 4 if you’re ready). This post will go into the technical details around making this upgrade as\nsmooth as it was. Read on… February 07 , 2013 by Jack \"chendo\" Chen Rails 3.2.10 Exploit and Slow Read Attacks Charlie Somerville and I presented a talk at the Melbourne RORO (Ruby on Rails Oceania) meetup regarding the recent Rails 3.2.10 security hole , as well as the Slow HTTP Read Attack and how it affects certain Rails stacks. We decided to experiment with the structure of the talk by weaving in a narrative. We called it “chendo’s 11”, parodying Ocean’s Eleven . The story follows us planning and executing a revenge heist against a fictitious illegal gambling website called “Casino King On Line”. Read on… January 31 , 2013 by Lucas Parry How Slow Searches Killed the Marketplaces (and How We Fixed it) On Thursday the 10th of January at around 3:25am AEDT (UTC+11) the Envato Marketplaces began\nsuffering a number of failures. These failures caused searches to stop working,\nbuy now purchases to be charged but not recorded, newly submitted items to not be\nprocessed, our review queues to be blocked by error pages and the site to be\ngenerally unstable. We’d like to take the time to explain what happened, why so many seemingly\nunrelated areas of the app failed simultaneously and the measures we’ve put in\nplace to try to prevent similar problems from occurring in the future. Read on…", "date": "2014-02-11"},
{"website": "Envato", "title": "\n          ", "author": ["Mario Visic", "Alan Skorkin", "Mary-Anne Cosgrove", "Jordan Lewis", "Shevaun Coker", "Mary-Anne Cosgrove", "Jordan Lewis", "Justin French", "Eaden McKee", "Julian Doherty"], "link": "http://webuild.envato.com/blog/page/3/", "abstract": "April 10 , 2015 by Mario Visic Push Button Deployments Envato is becoming a large company, with several teams working on many different products spread across various technology stacks. Each of our teams are responsible for their own deployment process, which means each has ended up with its own way to deploy changes.  It’s complicated to grant a new developer access to all the tools and services they need to deploy a single project, let alone multiple projects. Read on… March 23 , 2015 by Alan Skorkin A Better Way to do Rails with Aldous About a year ago the Tuts team encountered an issue with our Rails codebases becoming more unwieldy as they got bigger. Our development pace was slowing down and new features and fixes would cause regressions in unexpected parts of the system. This is a problem that many of us have encountered before, and we often treat it as part and parcel of doing business. This time, however, we decided to see if we could find ways to improve the situation. Read on… March 10 , 2015 by Mary-Anne Cosgrove Making the Most of BDD, Part 2 Hi, I’m Mary-Anne. I’m a senior Ruby developer here at Envato. One of the things that I love about my job is that it gives me the opportunity to use one of the practices I am most passionate about - Behaviour Driven Development, or BDD. In Part 1 of this 2-part series I described what BDD is and explained how it is more than simply a way to improve code quality. Today, let’s look at how BDD becomes the living documentation of your system, and how it informs your system architecture. Read on… February 16 , 2015 by Jordan Lewis Chainable BEM modifiers Envato Market has been using a css naming convention loosely based on BEM block__element--modifier syntax for the past two years and it has been instrumental in helping us update our front-end codebase and freeing us from legacy constraints. About a year in, a problem was still bugging us. We had two different ways of using modifiers: single class and multiple classes . Whilst both techniques are valid, they lend themselves for use in different scenarios. Plus, having two conventions using identical syntax in the same codebase is a recipe for confusion. Read on… February 06 , 2015 by Shevaun Coker A Case For Use Cases What’s the problem? Most Ruby on Rails applications start off as simple web applications with simple requirements. As a developer, you take those requirements, map out the business domain and then implement it using the MVC pattern of models, views and controllers. Over time, more requirements are added and simple controllers can become bloated with complex logic. The obvious next step is to create public methods on your models and move the logic into them, thus keeping the controllers simple. However, this inevitably leads to fat models. One of the guiding principles of Software Design is the Single Responsibility Principle and fat models have too many responsibilities. In fact, you could argue an empty model that extends ActiveRecord::Base already has multiple responsibilities: persistence and data validation/manipulation. Furthermore, an empty ActiveRecord model contains 214 methods (not counting any of the methods on Object). So these classes are already tightly coupled to the Rails framework and if you put your logic in them too, they’re also coupled to the business domain. Read on… January 22 , 2015 by Mary-Anne Cosgrove Making the Most of BDD, Part 1 Hi, I’m Mary-Anne. I’m a senior Ruby developer here at Envato. One of the things that I love about my job is that it gives me the opportunity to use one of the practices I am most passionate about - Behaviour Driven Development, or BDD. BDD is the practice of writing functional tests before you write unit tests. It incorporates the older practice of test driven development (TDD), the art of writing unit tests before you write code. At Envato, our code is written in Ruby, our unit tests in RSpec and our functional tests are in Cucumber, but many of these principles can be applied to other languages and frameworks. Read on… August 13 , 2014 by Jordan Lewis Styleguide Driven Development Styleguide Driven Development (SDD) is a practice that encourages the separation of UX, Design & Frontend from Backend concerns. This is achieved by developing the UI separately in a styleguide. By separating the UI and backend tasks so they don’t rely on each other, it allows teams to iterate fast on prototypes and designs without having to make changes to the backend. With careful planning they should plug-and-play together nicely. SDD isn’t just limited for big teams working on large applications, but the core concepts of developing UI elements separately in a styleguide (living or static) can still benefit a sole developer working on a single page app. Read on… July 31 , 2014 by Justin French Formtastic: Creating and Maintaining a Popular Open Source Plugin Interview by Natasha Postolovski. Justin French heads up the Design & UX team at Envato . In addition to being a product mastermind, Justin is an accomplished Rails developer, passionate contributor to open source, and frequent wearer of black T-shirts. In the open source community he is best known as the creator of Formtastic , a super popular plugin for the Ruby on Rails framework that helps Rails developers build better forms faster, with less boiler plate code. The repository has more than 4,400 stars in Github, and over 180 contributors. In this post we’ll delve into Justin’s experiences creating Formtastic, from idea to public release. We’ll also learn about what it takes to maintain and grow a popular open source project. Read on… July 10 , 2014 by Eaden McKee Learning from pull requests at Envato One of the things I really enjoy about working in the Envato Marketplace development team is the opportunity to learn and to teach via pull requests (PRs) and code reviews. Although the main reason we use pull requests is to obtain approval for production deployment, a useful indirect benefit is the transfer of knowledge within the development team. Pull Request Lifecycle In the development team we use GitHub to manage our code and the pull request functionality is a large part of our process. Every change in the codebase is put up for review on GitHub before it is merged into the master codebase and deployed to the web servers. The main reason we use PRs is to get other developers’ approval for the code to go into production. This approval is usually given via a “plus one” (+1). It is up to the developer to decide how many +1s they need before they merge and deploy their code. Generally, the more large and complex the piece, the more +1s are required. How does one get a +1? When a developer comments with a +1, they are saying “I am willing to support this code in production”. It must meet their idea of quality. From the overall architectural decisions made in your code, right down to whitespace formatting - everything is up for comment. This graph is of our PR comments, the thickness of the lines reflect the number of comments from one developer to another’s pull request. However, what it represents is of more importance. It represents knowledge sharing, teaching and learning. Read on… April 01 , 2014 by Julian Doherty Envato is hiring! Ninjas! Rock stars! Clerics! Envato is a fast growing company and we’re expanding both our Melbourne, Australia office and our remote team. Check the job listings below or visit our Careers page to learn about working at Envato. We are especially interested in talking to Ninjas, Rock Stars, and Clerics. In addition, we’re also looking for Ruby developers, devops, UX, and more! Read on…", "date": "2015-04-10"},
{"website": "Envato", "title": "\n          ", "author": ["Patrick Robinson", "Ross Simpson", "Jacob Bednarz", "Jacob Bednarz", "Fraser Xu", "Steve Hodgkiss", "John Viner", "Peter Rhoades", "Jordan Lewis", "Dennis Matotek"], "link": "http://webuild.envato.com/blog/page/2/", "abstract": "August 25 , 2016 by Patrick Robinson To The Cloud in-depth In a previous post, Envato Market: To The Cloud! we discussed why we moved the Envato Market websites to Amazon Web Services (AWS) and a little bit about how we did it. In this post we’ll explore more of the technologies we used, why we chose them and the pros and cons we’ve found along the way. To begin with there are a few key aspects to our design that we feel helped modernise the Market Infrastructure and allowed us to take advantage of running in a cloud environment. Where possible, everything should be an artefact Source code for the Market site Servers System packages (services and libraries) Everything is defined by code Amazon Machine Images (AMIs) are built from code that lives in source control Infrastructure is built entirely using code that lives in source control The Market site is bundled into a tarball using scripts Performance and resiliency testing Form hypotheses about our infrastructure and then define mechanisms to prove them We made a few technical decisions to achieve these goals along the way. Here we’ll lay those decisions out and why it worked for us, as well as some caveats we discovered along the way, but first. Read on… August 16 , 2016 by Ross Simpson Envato Market: To The Cloud! This is the story of how we moved Envato’s Market sites to the cloud. Envato Market is a family of seven themed websites selling digital assets.  We’re busy; our sites operate to the tune of 25,000 requests per minute on average, serving up roughly 140 million pageviews per month.  We have nearly eleven million unique items for sale and seven million users.  We recently picked this site up out of its home for the past six years and moved to Amazon Web Services (AWS).  Read on to learn why we did it, how we did it, and what we learned! Read on… August 05 , 2016 by Jacob Bednarz Getting Envato Market HTTPS everywhere Last month we announced that we had\nfinally completed the move to HTTPS everywhere for Envato\nMarket . This was no easy feat since we are serving over\n170 million page views a month that includes about 10 million products\nlisted and are all user generated content. Along the way we have learnt\nmany valuable lessons that we want to share with the wider community and\nhopefully make other HTTPS moves easier and encourage a better adoption\nof HTTPS everywhere. Read on… May 19 , 2016 by Jacob Bednarz How we tracked down Ruby heap corruption in amongst 35 million daily requests Back in November 2015, one of the Envato Market developers made a\nstartling discovery - our exception tracker was overrun with occurrences\nof undefined method exceptions with the target classes being NilClass and FalseClass . These type of exceptions are often a\nsymptom that you’ve written some Ruby code and not accounted for a\nparticular case where the data you are accessing is returning nil or false . For our users, this would manifest itself as our robot error\npage letting you know that we encountered an issue. This was a\nparticularly hairy scenario to be in because the exceptions we were\nseeing were not legitimate failures and replaying the requests never\nreproduced the error and code inspection showed values could never be\nset to nil or false . Read on… January 29 , 2016 by Fraser Xu Running Headless JavaScript Testing with Electron On Any CI Server Background Since the end of 2015, the Envato Front End team has been working on bringing a modern development workflow to our stack. Our main project repo powers sites like themeforest.net and serves around 150 million Page Views a month, so it is quite a challenge to re-architect our front end while maintaining a stable site. In addition, the codebase in 9 years old, so it contains the code from many developers and multiple approaches. We recently introduced our first React based component into the code base when we developed an autosuggest search feature on the homepage of themeforest.net and videohive.net . The React component was written with ES6, and uses Webpack to bundle the JavaScript code. As I mentioned above, it’s a 9 year old code base and nobody can guarantee that introducing something new won’t break the code, so we began all the work with tests in mind . This post documents our experiences developing the framework for testing the React based autosuggestion component . Read on… November 04 , 2015 by Steve Hodgkiss Introducing StackMaster - The missing CloudFormation tool CloudFormation is an Amazon (AWS) service for provisioning infrastructure as\n“stacks”, described in a JSON template. We use it a lot at Envato, and\ninitially I hated it. Typing out JSON is just painful (literally!), and the\nAPIs exposed in the AWS CLI are very asynchronous and low level. I wanted\nsomething to hold my hand and provide more visibility into stack updates. Today I’d like to introduce a project we’ve recently open-sourced: StackMaster\nis a tool to make working with multiple CloudFormation stacks a lot simpler. It\nsolves some of the problems we’ve experienced while working with the\nCloudFormation CLI directly. The project is a refinement of some existing\ntooling that we have been using internally at Envato for most of this year, and\nit was built during one of Envato’s previous “Hack Fortnights”. Read on… September 22 , 2015 by John Viner How Envato defined the expectations of our developers The Envato development team has always had a strong sense of what we stand for, how we work together and what we expect of each other … at least that is what many of us thought. Around 9 months ago our company participated in the Great Places to Work survey, which gauges how our employees feel about Envato as a place to work. Each department received a breakdown of their feedback, and whilst much of our feedback was great, one statement was a clear outlier “Management makes its expectations clear”. This was a trigger to question our assumptions about those expectations. This post tells the story of that journey. Read on… July 08 , 2015 by Peter Rhoades How to organise i18n without losing your translation_not_found I’ve written before about Working with Locales and Time Zones in\nRails , but I often feel the i18n library (short for\ninternationalisation) is underused (appreciated?). Perhaps it is even avoided\nbecause of the perception it is more effort to develop with and harder to\nmaintain. This article will, I hope, open your mind to the idea that you will be better\noff using i18n in your application (even for a single language) and that it\ncan be maintainable with some simple organisational pointers. Read on… June 17 , 2015 by Jordan Lewis Envato Market Structure Styleguide Today we have released the Envato Market ‘Structure Styleguide’ to the public. https://market.styleguide.envato.com A Structure Styleguide is a special breed of living styleguide, designed to document an application’s UI logic and all of the possible permutations of the UI. The goal is to have a complete and shared understanding of how an application behaves in any situation, even in the most uncommon edge cases, without having to laboriously recreate each scenario by hand. Read on… May 27 , 2015 by Dennis Matotek Your puppet code base makes you fear the apocalypse Let me paint you a picture. At some point in time someone said ‘hey, wouldn’t it be great if we could manage our servers with that new puppet thing’. ‘Great’, said everyone, ‘Let’s do that and the way we have always done it.’. And that, my friends, is how you end up where we are. Reading our puppet code base reads much like a bad folding story . Everyone had a plot line and tried to weave into a flowing narrative. And like all badly written stories it has many dead ends, twists, continuity issues and obscure meanings. You get that from many different authors - all code bases face the same problem. Read on…", "date": "2016-08-25"},
{"website": "Envato", "title": "\n          Working From Home - An Envatian Perspective\n        ", "author": ["Pete Johns"], "link": "http://webuild.envato.com/blog/working-from-home-an-envatian-perspective/", "abstract": "March 23 , 2020 by Pete Johns Working From Home - An Envatian Perspective Why are we writing this? Envato has been a remote-friendly workplace since the beginning. Our founders\nare keen travellers and understand that work is something we do, not somewhere\nwe go. The authors of this article are well-versed in working remotely both from\nhome and from abroad. Working from home for a day can seem pretty sweet: you\ndon’t have to take a day off work when you’re expecting a delivery or a\ntradesperson (“tradie” to the Australians) but working from home longer-term\ncomes with more challenges. Recent events that have overtaken the world mean\nthat our entire workforce is now working from home and for some people this is\ntheir first time experiencing this for more than a day or so. We’re writing this\nfor them. One of Envato’s core values is, “When the community succeeds, we\nsucceed”, so we’d like to share this with you, our community, too. Who are we? Jaymie Jones - Engineer\nliving in Northern NSW. Walking the path of the generalist diving a bit\ndeeper into areas such as Ruby and front-end work. Lucas Parry - Engineer in\nMelbourne, hates winter and has worked from Thailand, USA, Honduras, and\nNorthern NSW in the past to avoid it. Mary-Anne Cosgrove (aka MAC) - Senior\nDeveloper living in Canberra. I’ve worked from home for the last 5 years. Patrick Robinson -\nSenior Dev Ops Engineer from Wangaratta VIC. Working from home for 8 years. Pete Johns - Development Team\nLead in Melbourne. Has been leading distributed teams since 2013 and recently\nworked while travelling in Europe for three months. Travis King - Community Management\nSpecialist in Saskatchewan, Canada. Don’t worry he has no idea where that is\neither. Cool. How do you work effectively when you’re not at Envato’s HQ? Patrick Online, video-based, daily “stand-ups” are an important touch point for remote\nworkers. Working remotely creates barriers to communication, I can’t turn to a\nco-worker to see if they’re busy and attract their attention naturally. I have\nto interrupt them, potentially while they’re in a meeting or deep in thought.\nBeing aware of what each other are working on, and establishing if someone is\nfree to be interrupted at a certain part of the day helps that process work a\nbit more freely. A simple “Can you help me after lunch with X ?” can go a long\nway. Lucas Setting aside a dedicated workspace if possible, and only using it when you’re\nworking, can really help with the blurred boundaries between work/home. Change your expectations around communication, expecting near-instant\nresponses to emails or Slack messages is setting yourself up for\ndisappointment. Don’t send “Hi <coworker> ” and wait for a response, just ask\nyour question and the answerer will get to it when they get to it, I’m sure\nyou are capable of finding something else productive until they do. If it’s\ntruly urgent, maybe call the person, but really stop and think if it actually\nis since you’ll be interrupting someone else who is probably in the middle of\ntheir own important thing. Meetings over video are great, but delays and varying connections can\nsometimes cause unintended talking over one another. Stop and agree who is\ngoing to speak. Sometimes a meeting facilitator can be helpful for ensuring\neveryone gets to say their piece in larger meetings. Getting all those waiting-intensive chores done during the week (e.g. washing\nclothes) means less time wasted on weekends. Travis I only come to Envato HQ once or twice a year so I actually find my\nproductivity drops when I have actual people around to chat with. The great\nthing about being in community is that it is active 24/7 from all sides of the\nglobe, so I can drop in anytime I like to see what’s going on. The big\ndownside to that is that I can drop in anytime I like to see what’s going on.\nSo I’ve had to find a good work/life balance. MAC The big question for me is how do I work effectively when I AM at Envato’s HQ.\nDuring work time I tend to work much more intensely than when I am in the\noffice. I like working without interruption, and am lucky to have a dedicated\nspace that makes that easy. Occasionally I turn off my Slack notifications for\nfocus. I tend to make the assumption that others will also turn off Slack\nnotifications if they want to focus, so I generally ping people on Slack\nwithout worrying about interrupting them. I always make sure to include my\nquestion as well as a greeting, so that they can get back to me with the\nanswer without another delay. I have been in teams where different members had\ndifferent expectations around communication. It’s very worthwhile having a\nchat with your team to figure out what works best for each person and for the\nteam as a whole. Sounds great! What are the benefits of working from home? Pete Having young family, the most obvious benefits relate spending\nless time commuting and more time with them, whether that’s playing with\nthe kids, preparing meals, or helping with homework. I also enjoy the\nbenefits of having access to my own kitchen at lunchtime. Work-wise, the\nenvironment at home is much quieter than our open-plan office, which allows\nfor more deep thought and focus. Jaymie The flexibility, whether that is changing up my environment, using the time I\nwould normally be commuting to do chores or time with family. The more I think\nabout it, the list could go on and on. Travis For me being able to focus on my work from a quiet home office really allows\nme to get a great deal of work done quickly. I only have to really worry about\na cat who seems to only want to go into my office when I close the door for a\nmeeting. I also really love the flexibility of my work day when I work from\nhome. Lucas Not commuting to the office saves at least 1-1.5 hours out of the day, which\nmakes taking care of myself in the way of daily exercise much easier. I like\nto get in a run before or after work every second day or so. I find I also\nmake much healthier food choices when I’m at home with access to my own\nkitchen than I ever would buying lunches at the office. Finally (and probably\nnot so helpfully right now), I’m a natural introvert and not spending my\n“social points” in the office leave me with more points to spend with friends\nbefore I need to stop and recharge. Patrick For me it’s having the freedom I get about structuring my day. I can go to my\nDaughter’s assembly on a Friday morning, pick my son up from daycare early for\nswimming, go to the Gym or for a run at lunch. When you have to commute, it\nmakes it harder to organise and i’m not great at being organised. MAC In addition to all the benefits others have mentioned, working remotely gives\nme the opportunity to work for a modern, progressive, caring company. It’s\nvery hard to find places like that in my location. I bet it’s not all beer && skittles. What are some of the pitfalls when working from home? [ Fun and enjoyable. “Skittles” is a British game that is similar to bowling. -Ed ] MAC The big pitfall for me is getting so wrapped up in what I’m doing that I end\nup stiff and sore. Must keep moving! Patrick Not having a routine for starting and ending your day inhibits the mind’s\nability to start and stop working. Some people have trouble focusing, others\nhave trouble switching off, sometimes you get both in the same day. Building a\nroutine for yourself and having triggers that gently guide you towards the\ndesired outcome helps. My routine is to get dressed while getting the kids\nready for school and daycare, go for a walk or run most days once they’ve left\nand sit down with a freshly brewed coffee ready to work. Once the kids get\nhome it’s often distracting, they want to talk about their day and show me a\npicture they drew. Giving them 15 minutes of attention, for me often this\ninvolves making them food, avoids hours of broken attention as they interrupt\nme constantly. Pete If you’re not used to video or teleconferencing, it can take some adjustment.\nConsider joining a call a few minutes before the scheduled start. Checking\nthat your microphone and camera are working properly before others join can\nsave five awkward minutes of people asking, “can you hear me now?” Travis I’ve never tried beer and skittles together so I think I need to get on that.\nMy biggest pitfall is not being able to disconnect. The community is always\ntalking and I have a strong curiosity to check in even when I shouldn’t. So\nI’ve had to set my devices and tools like Slack to automatically enter a Do\nNot Disturb mode at night. It helps reduce the temptation to check in when I\nget a notification. Now I just have to find something to reduce the temptation\nto drink beer and play skittles… Lucas I definitely sometimes struggle with finishing up on time when I’m deep in\nproblem solving mode; Not being in an emptying office means there’s fewer\nexternal signals that it’s time to stop. I sometimes need to set alarms for\nmyself when I start in the morning to remind me when to stop. If you don’t\nconsciously focus on keeping active, it’s extremely easy to be waaaay too\nsedentary; Get a fitness tracker of some sort, so you’ve got stats to feel\nguilty about. What about the tech ? Are there any good tools to use with remote teams? 1Password : For keeping secrets secret. An external microphone: the built-in microphone in most laptops picks up way\ntoo much background noise. If you’re running video conferencing software on a\nlaptop, the cooling fan may sound like a jet engine to other meeting\nparticipants. Comfortable headphones: these will dramatically improve your online meeting\nexperience and if your schedule is meeting-heavy, you’ll appreciate the\npadded ear cups by the end of your working day. External webcam so you can still look at your main screen and be seen to be\nlooking at folks. GitHub : We’ve written about this before . Google Docs : We drafted this article in Google\nDocs. It’s so ubiquitous in our organisation that Pete forgot to list it\nwhile we were talking about our tools. Google Meet : This is our go-to tool for team\nmeetings. Krisp : for audio, reduces external/ambient noises in both\ndirections, e.g. street noise, fans, etc . Slack : Mostly used for text-based chat, operations, and\npairing over video. Trello : Most of our teams use Kanban-style boards for\nvisualising our work. Zoom : For larger company meetings, we get to see each\nothers’ faces on the big screen. Comedy backgrounds are very much an optional\nextra. Working remotely sounds isolating. How do I stay connected? Some of our colleagues have recently written about Staying social during social\nisolation , be\nsure to check that out on the main Envato Blog . Travis I’m pretty introverted, so I don’t get my tanks filled up talking to people\nlike those crazy extroverts seem to do, but I still enjoy social interaction.\nSo I try a schedule a night or two a week to just enjoy hanging out with\nfriends. I also fortunately seem to have a lot of extroverted friends so they\nusually make all the plans and I can just show up…and now I feel a bit bad\nfor calling them all crazy. Lucas I’m like Travis, if I’m in the office that’s introvert points I can’t spend\nwith my social friends. Probably not helpful. Patrick When I first started working from home I also moved to a new town where I\ndidn’t know many people. At first it was incredibly isolating. Prior to that\nI’d leant on my work colleagues to provide most of my social connections. I\nrealised I needed to make an effort to build those relationships and through\nsports clubs and not-for-profits I’ve managed to. It’s hard to replace unplanned interaction with people you know, at the\nsupermarket, gym, parkrun or the coffee shop. MAC I attend virtual lunches or afternoon teas with three different groups (my\n team, women engineers, and remote workers). I also have regular one on ones\n with some people. Locally, I run a Humanist group and sing with a choir.\n During the pandemic lockdown I have helped both these groups to continue to\n meet via Zoom. We need to replace Social Isolation with Physical Isolation\n plus Social Connection! Pete Keeping connected with my team is really important to me. One-on-one meetings\nare the most important meetings of my week, where I get to spend time with the\nindividuals on my team and make sure they’re supported. Each Wednesday\nafternoon we have “Working Together Time” where we all join a Google Meet and\ngo about our normal work, as if we were co-located. Every other Wednesday we\nget together via Google Meet for a team lunch, which is a work-free social\noccasion. Outside of work, as an ex-pat, I’ve grown accustomed to using FaceTime to keep\nin touch with family members and friends overseas. Like Patrick, I’m a keen\nrunner and being in the open air with my running buddies keeps me healthy\nphysically and mentally. Jaymie Working from home has its challenges, what I have found works quite well is\nhaving general conversations with the team (not work talk), as well as one on\nones, then attending meetups when I can. What about health and safety? Pete Workplaces tend to have health and safety teams, who look after things like\nergonomics, risk assessments, first aid and so on. If you work from home,\nthese concerns still need to be taken care of. Using your laptop at the\nkitchen table may work for you when you work from home for a day but\nlonger-term you’re going to want a dedicated workspace that is as comfortable\nas the one your employer provides. Office furniture can be expensive; consider\nchecking out your local second-hand office furniture store for an ergonomic\nchair and a comfortable desk. Reusing these items is better for the bank\nbalance and the environment than buying new and also is more readily\navailable. Ask your IT team whether you can borrow peripherals like an\nexternal display, keyboard and mouse to use with your laptop while you’re not\nin the office as this will be better for your posture. Check your home working environment for hazards, such as trailing power cords\nto make sure there are no trip hazards for you or for anyone else who might be\nnearby. Take regular breaks from your screen. When you work in an office, you are\nlikely to spend time walking between meeting rooms, when working from home,\nall of your meetings are at your desk. Try to finish meetings a few minutes\nearly, and use that as a cue to get up, stretch, walk around your home and\nrefill your water (hydrate, hydrate, hydrate). MAC Pay attention to your body. Notice if you have any aches and pains, and see if\nyou can figure out what’s causing them. For me, I get neck pain and a back of\nthe head headache if I work too long on a screen that is not at eye level, and\npain in my left shoulder if I’m leaning on my left elbow (a bad habit!). Jaymie Sometimes you can get lost in your work and forget to take breaks. Using the pomodoro technique can be\na great way to ensure you take regular breaks. In addition to breaks, it is important to set boundaries on when you start and\nfinish your work. If you don’t set boundaries, it can be easy to overwork\nyourself. For example, your working boundaries may be starting at 8am and finishing at\n4pm. When 4pm strikes, ensure that you log off for the day. When attending meetings, ensure to have your webcam enabled so others can see\nyou. Travis I’ve recently purchased a standing desk and started a short daily exercise\nprogram. I’ve never been a fan of exercise because yuck! So I had to find a\nroutine that worked for me that I wouldn’t try to get out of every morning. So\nit’s coffee in the morning, 30 minutes on the stationary bike while listening\nto a podcast and then a nice soak in the tub before work starts. It’s gotten\nto be so enjoyable that I miss it when I can’t do it. I can’t believe I could\nactually miss exercise. Lucas Ergonomics! Don’t just sit on your couch and stare down at your notebook. At a\nminimum get an external keyboard and pointing device and sit your notebook on\nsomething to make it eye level. Ideally use an external display at the correct\nheight. Get a decent chair, you’ll be sitting in it enough hours to justify\nspending on a comfortable one. Get outside, be active. It’s going to take some time to adjust Like so many things, we’re not going to get this right the first time. Adjusting\nto working from home if you’re used to working in an office may be hard,\nespecially when you’re dropped into it at short notice. Communication here is\nimportant: regularly discuss what’s going well with your teammates and look at\nwhat needs to be improved for you to be effective. Iterate and adapt. You’ve got\nthis!", "date": "2020-03-23"},
{"website": "Envato", "title": "\n          The tale of the missing semaphore\n        ", "author": ["Stergio Moschogiannis"], "link": "http://webuild.envato.com/blog/the-tale-of-the-missing-semaphore/", "abstract": "August 12 , 2019 by Stergio Moschogiannis The tale of the missing semaphore A number of our services were until recently running on Ubuntu 14 (Trusty Tahr), for which long term support (LTS) was coming to an end. Trusty Tahr was released in 2013 and has since served us quite effectively, —so effectively in-fact, that we near completely forgot about it. That is until one morning, an announcement from Ubuntu declaring that LTS for good ol’ Trusty was coming to an end, demanded attention. Our engineering team, being the enthusiasts that we are, popped the hood, blew the dust off and diligently set upon the task of taking stock, discussing and carding up requirements for the great migration. It made sense —we reasoned, that since we were performing a recondition, we might as well bump it all the way up to the present in one go and shepherd in the era of Ubuntu 18 (Bionic Beaver). The rub —we all agreed, was that there was a fairly large surface area of change between versions, and with layers of provisioning and configuration in the mix that would have to be adapted, the likelihood of this going smoothly was pretty slim. Now I can not say whether it was the looming LTS deadline, anarchic tendencies or enthusiasm that had the biggest influence on our decision, but decide to do this we did, and over the ensuing sprints juggled learning about and adapting our infrastructure to the ways of Bionic. After all the chopping, changing, tests and refinements, it was time to direct production traffic over, fire extinguishers in hand we flipped the switch and.. —you’re expecting to read about the fireworks right about now, aren’t you? Yup, we were expectant too, nothing fell over. Bionic hummed along, all the gauges and readouts were claiming normalcy, and after doubtingly observing this strange phenomenon for a few hours, we hesitantly accepted the possibility of success. For good measure, we decided to let this new and wondrous machine run for a full 24 hours before declaring victory. Twenty-four hours later, with an evidently well-tuned infrastructure upgrade in production, we made all changes permanent and left the building in victory stride. The migration was done and we were moving on to the next thing. It wasn’t until a couple of days later, when one of our web instances fell over —complaining about missing a semaphore, that we began to wonder about what we might have overlooked. What follows is a walk-through of our investigation into the case of the missing semaphore, the rabbit-hole it sent us down and the anagnorisis that hit, revealing the unexpected culprit. See if you can call it before the big reveal. We started looking at backtraces after a semaphore went missing for a second time, the missing semaphores were used by a part of our code-base that utilises Semian , a library that has been a salutary part of our code-base for a good few years. Semian provides circuit breaker and bulkhead functionality which can be used to improve the resilience of a service that needs to call out to other services. Semian in turn utilizes semaphores to implement this functionality, and what the backtraces were claiming, was that our service was referencing a semaphore that did not exist. Our initial investigation did not give us much more than this to go by, Bionic was fresh, but it had been running smoothly for a few days and nothing was pointing to it being the perpetrator. Moving through the expected steps; we were still running the same version of Semian, associated code had not been changed, logs and monitoring had nothing and there were no co-occurring events to read into. Going by the little we had, this would have to be an exercise in abductive reasoning. Besides ipcs confirming that semaphores were indeed missing, and some early speculation about the possibility of bottlenecks, starvation and resource constraints, we had nothing. Opting to garner more information before trying our hand at hypothesising, we set logging up to capture what semaphore activity could be traced, expecting to catch a complaint if/when another semaphore went missing. The pager went off early the following morning, all of our web instances had hit this same snag, between log data and some manual testing we began ruling possible causes out, no complaints, no timeouts, no workers were being killed. Everything we looked at was behaving as advertised. Being unable to incite this misbehaviour, and with nothing revealing in the logs, we turned our attention to the only other lead we had, which was from observing that all web instances were hit, and were hit at the same time, the time that daily cron jobs run. We went on to explicitly invoke those of the daily cron jobs, that could be run on a more frequent basis without consequence/side-effects, this fell flat with nothing misbehaving. Perhaps the job we were after was not in this selection. Our investigation till this point had revealed very little, some cron jobs could not be run till the following morning, others were invoked yet no semaphores disappeared. Inspecting these cron jobs —some of which were contemporaries of Trusty, only served to raise more questions about their relevance and function. Left with few options, we decided to disable a single, different, daily cron job per web instance and waited for things to get noisy the following morning, the instance that lost none of its semaphores would tell us which cron job could instigate misbehaviour on account of it being the one that was disabled on said instance. This played out as expected and a specific cron job, which had the purpose of rotating log files was implicated. The plot thickened a little here, besides being a fairly straightforward job, it did not interact with Semian, semaphores, endpoints or any part of the service that we could use to connect the dots. What it did reveal was that it only performed its job when log files were either old or large enough to warrant rotation, inferring that this was a plausible explanation for why we did not encounter odd behaviour in the 24 hours that immediately followed the Bionic upgrade. Besides the mystery of how this cron job was instigating misbehaviour, when forced to run, it ran successfully, no misbehaving here either. Suspicion in Bionic’s direction was mounting by this point, but there was no evidence to go by. All questions, no answers and no leads, a couple of us got up early the following morning, hopped onto a few web instances, got our tracing tools of preference out and collected a few gigabytes of system event data right as semaphores began to disappear. There’s plenty to see in a couple of minutes of system event data, making sense of it all is an entirely different matter. We filtered for errors, carefully inspected warnings, looked into the sequence of semaphore related messages, focused on log rotation events and came up with a number of hypotheses that could rival the best of conspiracy theories, then disproved them all. Having looked at all the meaty, noisy, interesting or otherwise cryptic events and having learnt very little —grasping at both straws and sanity, we set out to trace our steps one more time in the hopes of finding something that we had missed. As fate would have it, the thread that served as catalyst to unraveling this mystery was hidden in plain sight between the grasped straws, having looked at everything else we were now reading the event messages that were in plain English, a message from logind announcing a logout event in particular. Now we knew that the log rotation job, when running, would sudo in to a specific account and log out once done. Looking at this logind logout event spurred thought in a different trajectory, which ended in a question; what does logind clean up at logout? Things unraveled pretty quickly from here, a search revealed a number of articles on the topic, systemd is the best example of Suck caught my attention, which linked to systemd-logind deletes your message queues , next up was Linux Database crashes when semaphores get removed . In TL;DR, logind is a daemon that Systemd employs to manage logins, although not the standard in older versions of Ubuntu, this is the defacto system and services management solution that ships with Bionic, and is configured by default to remove IPC related resources when a user account logs out. A quick check-in with the service’s provisioning code confirmed that the account in question did in-fact live in user space, and remedial action would be to change default behaviour by setting RemoveIPC=no in /etc/systemd/logind.conf , or move the account to system space where it would be unaffected. Summed up succinctly, the service’s Unix account was in user space, whenever it was logged into then logged out of, all Inter Process Communication (IPC) resources would get cleaned up, of which semaphores are a part of. From the system’s perspective, semaphores were not missing, they were cleaned up as expected and it was the service that was misbehaving by trying to refer to semaphores that were no longer there. Takeaways.. Hindsight tends to be 20/20, reflecting on the decisions we made on the journey to solving this conundrum, there are a couple of things that I think are worth calling out; Diversity amplifies synergy, solving this would have taken longer had it not been for all the unique contributions. We might have been able to solve this sooner, perhaps by having tested every aspect of the instigating cron job once we identified which job it was, but debugging with not much to go on is difficult, you do not know what you do not know, thus all inference is weighed equally until something new is discovered, and you do not know how far, how close or in which direction that next discovery is.", "date": "2019-08-12"},
{"website": "Envato", "title": "\n          Automating the migration of lodash to lodash-es in a large codebase with jscodeshift\n        ", "author": ["Glenn Baker"], "link": "http://webuild.envato.com/blog/automating-the-migration-of-lodash-to-lodash-es-in-a-large-codebase-with-jscodeshift/", "abstract": "April 04 , 2019 by Glenn Baker Automating the migration of lodash to lodash-es in a large codebase with jscodeshift Recently the Elements team needed to make a reasonably large change to the codebase: migrating over 300 files which imported lodash to instead import from lodash-es . To automate this change we chose to write a codemod for jscodeshift , a tool by Facebook. The power of jscodeshift is that it parses your code into an Abstract Syntax Tree (AST) before transforming it, allowing you to write codemods that are smarter than regular expression based codemods. jscodeshift is a toolkit for running codemods over multiple JS files. It provides: A runner, which executes the provided transform for each file passed to it. It also outputs a summary of how many files have (not) been transformed. A wrapper around recast, providing a different API. Recast is an AST-to-AST transform tool and also tries to preserve the style of original code as much as possible. Fortunately, we diligently imported Lodash functions from their direct entry points (eg. lodash/<function-name>\" ), which makes this modification easier. A typical change we needed to make looks like this: 1 2 3 4 import React from \"react\" -import isEmpty from \"lodash/isEmpty\" +import { isEmpty } from \"lodash-es\" import { connect } from \"react-redux\" In many cases, there was more than one import from Lodash to consider. 1 2 3 4 5 import React from \"react\" -import isEmpty from \"lodash/isEmpty\" -import mapValues from \"lodash/mapValues +import { isEmpty, mapValues } from \"lodash-es\" import { connect } from \"react-redux\" Writing the jscodeshift transformer The starting point of any jscodeshift codemod is the transformer function. The transformer function gives you the fileInfo of the file that the CLI is operating on, and the api.jscodeshift API . It’s common to see examples of this API reference aliased to j . 1 2 3 4 5 6 7 8 9 10 11 12 13 export default function transformer ( fileInfo , api ) { // j is a reference to the api we will use later on const j = api . jscodeshift // Create a jscodeshift Collection from the source string const root = j ( fileInfo . source ) // Do some sort of transform on the Collection // .. omitted .. // Return the new code string return root . toSource () } When working with the API j , you pass it the file source and it returns a Collection . A Collection is an object containing an array of NodePath objects. The docs describe it as jQuery-like: jscodeshift is a reference to the wrapper around recast and provides a jQuery-like API to navigate and transform the AST. Finding the import declarations The first thing we want to do is find all the import declarations that are sourcing Lodash modules. To better understand how we might do this, it’s helpful to first explore what the AST looks like. The AST for our sample code contains many ImportDeclaration nodes. Each one contains a source string literal with the value, which we will check for Lodash. The above screenshot was captured in AST Explorer . Check it out, it’s an excellent online tool for exploring ASTs (Turn on Transform -> jscodeshift ). 1 2 3 4 5 6 7 8 9 10 const lodashImports = root . find ( j . ImportDeclaration ) // Find only lodash NodePaths . filter ( nodePath => { return nodePath . value . source . value . startsWith ( \"lodash\" ) }) lodashImports . forEach ( nodePath => { // ... }) Analyzing the import declarations Now that we have all of the Lodash import declarations, we can start to analyze them. We want to understand what the module name is, so let’s further analyze the source literal. 1 2 3 // Eg. source \"lodash/mapValues\" const id = nodePath . value . source . value . replace ( \"lodash/\" , \"\" ) // => \"mapValues\" We also want to understand if the import declaration’s specifier is the same as the module name. If it’s not the same, it is important to capture this, because we might need to be able to import the named functions with the as directive in the future. For example: 1 2 -import myMapValues from \"lodash/mapValues\" +import { mapValues as myMapValues } from \"lodash-es\" To do this we need to explore what the import specifier AST nodes look like. 1 2 3 4 // Get the first specifier... const [ specifier ] = nodePath . value . specifiers // ...and save the name const name = specifier ? specifier . local . name : id With that, we now have enough information to create new import specifiers. As we loop over each import declaration, we’ll populate an array of replacement specifiers we intend to use on our final import declaration. 1 2 3 4 5 const replacementSpecifier = j . importSpecifier ( j . identifier ( id ), // eg. isEmpty or mapValues j . identifier ( name ) // eg. isEmpty or myMapValues ) replacementSpecifiers . push ( replacementSpecifier ) Removing and replacing import declarations Now that we have all of the replacement specifiers, we can look at replacing all the lodash/* import declarations we found, with a single import declaration for lodash-es . To do so we’ll need maintain a reference to the first import declaration for later. All the other Lodash import nodes can be removed. To remove nodes in jscodeshift, we’ll need to wrap it in a j(nodePath) call and use remove() . 1 2 3 4 5 if ( ! first ) { first = nodePath } else { j ( nodePath ). remove () } Using that first Lodash import reference, we can create the new import declaration. To replace nodes in jscodeshift, we’ll need to wrap it in a j(nodePath) call and use replaceWith() 1 2 const newImport = j . importDeclaration ( replacementSpecifiers , j . literal ( \"lodash-es\" )) j ( first ). replaceWith ( newImport ) Alternatively you can modify the the first Lodash import reference itself. This is my preferred way to make this change. 1 2 first . value . specifiers = replacementSpecifiers first . value . source . value = \"lodash-es\" Dealing with comments The modifications we’ve applied so far are, are enough for our application code to be functional, but we will lose all the comments from the nodes with removed and replaced since, in the AST, comments are attached to nodes. In order to fix this problem, we’ll need to collect up all the comments and assign them to the final lodash-es import declaration. 1 2 3 4 5 6 7 let replacementComments = [] // Save all the comments lodashImports . forEach ( nodePath => { replacementComments = comments . concat ( nodePath . value . comments || []) }) // Replace the comments first . value . comments = replacementComments Putting it all together That’s it, now lets assemble it into our final transform function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 export default function transformer ( fileInfo , api ) { const j = api . jscodeshift const root = j ( fileInfo . source ) const lodashImports = root . find ( j . ImportDeclaration ) // . filter ( nodePath => { return nodePath . value . source . value . startsWith ( \"lodash\" ) }) let first let replacementComments = [] const replacementSpecifiers = [] lodashImports . forEach ( nodePath => { const id = nodePath . value . source . value . replace ( \"lodash/\" , \"\" ) const [ specifier ] = nodePath . value . specifiers const name = specifier ? specifier . local . name : id const replacementSpecifier = j . importSpecifier ( j . identifier ( id ), // the import id j . identifier ( name ) // the import \"as\" name, it might be the same as id. ) replacementSpecifiers . push ( replacementSpecifier ) replacementComments = replacementComments . concat ( nodePath . value . comments || []) if ( ! first ) { first = nodePath } else { j ( nodePath ). remove () } }) if ( first ) { first . value . specifiers = replacementSpecifiers first . value . source . value = \"lodash-es\" first . value . comments = replacementComments } return root . toSource () } Finally, run the codemod over your entire codebase with the CLI: 1 jscodeshift src -t lodash-es-imports.js --extensions = js,jsx And the result? Checking the git diff , we see many changes like this: 1 2 3 4 5 6 7 import React from \"react\" // isEmpty used because it works on arrays and objects -import isEmpty from \"lodash/isEmpty\" // Note that myMapValues is different from mapValues -import myMapValues from \"lodash/mapValues\" +import { isEmpty, mapValues as myMapValues } from \"lodash-es\"; import { connect } from \"react-redux\" You can copy the complete example above and play with it in AST Explorer . You’ll need to turn on Transform -> jscodeshift in order to see the codemod output. Happy codemod’n. Links jscodeshift Github repo jscodeshift API docs AST Explorer ast-types", "date": "2019-04-04"},
{"website": "Envato", "title": "\n          Speeding up CI in AWS\n        ", "author": ["Michael Pearson"], "link": "http://webuild.envato.com/blog/speeding-up-ci/", "abstract": "December 06 , 2018 by Michael Pearson Speeding up CI in AWS One of our development teams highlighted that their build was taking too long to run. We obtained a near three times speed improvement in most part by using newer AWS instance types and allocating fewer Buildkite agents per CPU. Envato use the excellent Buildkite to run integration tests and code deployments. As a bring-your-own-hardware platform, Buildkite offers us a lot of flexibility in where and how these tasks run. This means that we’re able to analyse how that build is using its hardware resources and try to work out a better configuration. The build in question is for the “Market Shopfront” product: a React & node.js application written in TypeScript, built with webpack, and tested using Jest and Cypress. On-branch builds were taking between ten to twenty-five minutes. master builds, which also include a separate build of a production container and a deploy to a staging environment, were taking between fifteen and forty minutes. Builds should take less than five minutes: any longer and the waiting for a build becomes a reason to switch to something else, forcing an expensive context switch back once the build has finished. Worse, a consistently failing build can easily consume an entire day, especially if it’s only repeatable in CI. The efforts described below are one part of a larger project to improve this build’s performance and use what we learned to improve other builds at Envato. Investigation The first thing that stood out to me was the very high variance in build times. This hinted either that: the build was relying on third party APIs with varying response times, or the build’s performance was being affected by other builds stealing its resources The first possibility was quickly ruled out: the parts of the build that talk to things on the internet or in AWS showed the same level of variance as other parts of the build that are entirely local. We don’t (yet) have external instrumentation on the build nodes, so we ssh’d into them individually and used the sysstat toolkit to watch the instance’s performance. We found that CPU was almost entirely utilised while memory, disk bandwidth, disk operations per second, and network throughput still had a fair amount of headroom. We also found that the CPU being fully utilised by concurrent builds on the same node was the cause of the large variance in build times. This confirmed what several people in the team already suspected: we needed more CPU. Exploratory Research Dedicated spot fleets and Buildkite queues were created to perform indicative testing on the effect of different node configurations and classes on build performance. The existing configuration was c3.xlarge and m3.xlarge spot instances with one agent per AWS virtual CPU. We tried: increasing the instance size from xlarge to 2xlarge halving the number of agents per virtual cpu moving to current generation c5 and m5 instances using the newly released super-fast CPU z1d instances We found that: current generation instances provide a 50% speed increase over their previous generation counterparts the difference between m5 and c5 instances was minimal z1d instances provided a further 30% performance increase, but at double the cost halving the number of agents per virtual cpu provided a performance increase using smaller instance types meant steps more often needed to docker pull cache layers, which randomly increased build times However, these results and findings are indicative only: only one sample was taken for each instance class. Modern Instances We isolated two steps from the build that were not network-dependent and were idempotent: the initial webpack build and the first set of unit tests. They were run multiple times using the avgtime utility on a set of instance types: This confirmed (at least, for these two steps) the indicative findings on the performance improvements offered by the newer instance types: c5 & m5 instances are approximately 50% faster than their older c3 counterparts for this type of work. It is also interesting that c5 and m5 instances are almost exactly as fast as each other for this step, despite the c5 ’s reported 3ghz versus the m5 ’s 2.5ghz. Virtual vs “Real” CPUs AWS advertises its instances as having a certain number of “virtual CPUs” or vCPUs. This can be misleading if you’re not already familiar with Intel’s Hyperthreading , where for every processor core that is physically present, two “logical” cores are made available to the operating system. AWS’ vCPUs map directly to logical cores, not physical ones, Our instances were configured to run one agent per logical core, not physical. This meant that even single-threaded build steps could take up to twice as long to run if the instance’s CPU was fully taxed. This was originally a cost saving measure that was based on the assumption that most build steps would spend their time waiting on network resources or other tasks. For this build queue this assumption proved to be incorrect. We ran two benchmarks on a single c3.large : one with a single webpack build running, and one with two running in parallel. We also ran the same benchmark on a c5.large to determine whether the newer instance type provided better Hyperthreading optimisations: On both classes of instance, running two identical steps at the same time on the same physical CPU nearly doubled the execution time versus running only one, despite the benefits offered by Hyperthreading. Other findings: Docker COPY vs Bind Mounts All of the tests above were run via docker run on a container without volumes or bind mounts: node_modules and the project’s source were baked into the image via COPY . /app . Running the webpack build with these files instead bind mounted (via -v $(pwd):/app ) showed us a significant performance improvement: Unfortunately, this isn’t something that we can easily take advantage of in our builds without making them significantly more complicated. Bind mounts also gave us no performance improvements when running the unit test step. Configuration Changes Based on the above results, we decided on two initial actions: moving to c5d.2xlarge and m5d.2xlarge instances halving the number of agents per virtual cpu We opted for the d class instances as we wished to keep using the instance storage provided by the c3 and m3 class instances. Doubling the instance size while halving the number of agents per virtual CPU meant that we still had the same number of agents per spot instance, meaning that we’d increase build performance without increasing cache misses on docker image layers. This was recorded as a set of Architectural Decision Records in the git repository containing the StackMaster configuration for this fleet so that future maintainers would know the context and thinking behind these changes. Costs Prediction of how much this would increase costs was difficult: we anticipated that while each individual instance cost twice as much, we’d ultimately need less of them, as faster builds would mean that the spot fleet autoscaling rules would be triggered less frequently. We expected that the change would increase costs as our fleet is configured to always have one instance running regardless of load, and we’d be doubling that instance’s size. We found that the change more than doubled the cost for this fleet: the newer instance types are in higher demand and therefore attract higher spot prices. Fortunately for us the original costs were very low, so this level of increase was not a big worry! Impact This change had an almost immediate and significant effect on branch builds, as shown in the scatter plot below in the middle of November. Master builds have also improved, but less so, as the deploy to staging adds a significant chunk of time: Through this change builds have become both much faster and much more consistent: branch builds that previously took between ten and twenty five minutes now take between four and ten, and master builds that took between fifteen and thirty five minutes now take between seven and thirteen. Other improvements have been made to this build, but of all of them it was this change that had the highest impact. We’re now hoping to take what we’ve learned here and roll it out to a single consolidated fleet of agents that can be shared by all projects, rather than a single fleet per project. This will allow us to consider faster instance types (like the lighting-fast z1d instances) as we’ll have less “idle” agents, offsetting costs. Eagle-eyed readers will notice that the times in the scatter plot above are faster than the speculative improvements we expected in our initial runs. The above improvements aren’t the whole story, just the change we made with the highest impact: additional improvements were made in our webpack configuration, balancing of E2E tests between nodes, and docker layer caching strategies. More on these further changes soon!", "date": "2018-12-06"},
{"website": "Envato", "title": "\n          Migrating edge network providers\n        ", "author": ["Jacob Bednarz"], "link": "http://webuild.envato.com/blog/migrating-edge-providers/", "abstract": "March 06 , 2018 by Jacob Bednarz Migrating edge network providers Unknown to our users, we recently migrated edge network providers. This\ninvolved some particularly interesting problems that we needed to solve\nin order to migrate without impacting availability or integrity of our\nservices. Before we get into how we made the move, let’s look at what an edge\nnetwork actually does. An edge network allows us to serve content to users from a physically\nnearby location. This allows us to deliver the content in a fast, secure\nmanner by avoiding sending every request to our origin infrastructure,\nwhich may be physically distant from users. On the security front, using an edge provider allows us to perform\nsecurity mitigations without tying up our origin infrastructure\nresources. This becomes quite important when we start talking about\nDistributed Denial of Service (DDoS for short) attacks that aim to\nsaturate your network and consume all of your compute resources making\nit difficult for legitimate users to visit your site. By offloading the\ndefense against malicious traffic and mitigation work to a set of\npurpose built servers distributed across the globe, you free up your\norigin resources to do what they need to do and service your users. DDoS + WAF Malicious users are a very real threat and something we deal with on a\ndaily basis. The majority of these attacks aren’t volumetric however\nthey can impact other users if they manage to generate enough requests\nto slow down a particular part of our service ecosystem. Depending on the type of malicious traffic, we have two options: a Web\nApplication Firewall (WAF) and DDoS scrubbing. WAF is used for most of our mitigations. This is a series of rule\nsets that have been developed over the years based on attacks that we’ve\nseen against our services. We “fingerprint” a large sample of requests\nand then extract out common traits to either block the traffic, tarpit the request or perform a challenge that\nrequires human interaction to proceed. DDoS scrubbing comes into play when we have a highly distributed\nattack or we are seeing high volumes of network traffic. The goal of\nthis mitigation is to filter out the malicious requests (much like using\nWAF) however it is usually done far more aggressively and involves\ninspecting other aspects than just HTTP. Prior to the move, these were two separate systems and the request flow\nlooked like this: This setup wasn’t perfect and lead to a few issues. Debugging was very difficult. To get to the bottom of any request\nissues, we needed to use both systems to piece together the full\npicture. While both systems had correlation IDs that we could map\nagainst each other, it was easy to get confused which part of the\nrequest/response you were looking at in either system. Coordinating changes was hard . As we added additional features to\neither our DDoS defense or WAF we needed to do some extra work to\nensure that rolling out changes in one would continue to work with the\nother. API differences . We are big users and advocates for Infrastructure\nas Code, however only some of our service providers offered this\nfacility and even then only for limited portions of their services.\nThis resulted in us either needing to use the UI with manual reviews\nor only storing part of it in code which added confusion on what went\nwhere. Getting blocked in one system could be misunderstood by the other\nsystem . If a user managed to trigger one of our WAF rules, the DDoS\nsystem that had some basic origin healthcheck capabilities could read\nthe response as the origin being under load and start throwing\nconfusing errors. This would get in the way of finding the real issue\nas you would get errors from both systems instead of just a single\none. So, we set out to combine the two systems into a single port of call for\nour traffic mitigation needs. DNS We maintain both internal and external DNS services. For our internal\nDNS, we use AWS Route53 as that is already well integrated with our\ninfrastructure. However, externally we needed something that would do\nall the standard stuff plus cloak our origin and prevent recursive\nlookups from finding the origin. Something else we wanted to improve was the auditability of our DNS zone\nchanges. Our existing DNS provider didn’t lend itself very well to\nmanaging the records as code. This resulted in changes needing to be\nstaged in a UI and then posted to Slack channels for review from other\nengineers before being committed. Managing our DNS in code would help us\nlevel up in our security practices because it would keep DNS changes\neasily searchable and aid mitigating vectors like dangling DNS\nvulnerabilities. Preparing for the move One of our biggest concerns with migrating these services was conformity\nbetween the new and old. Having discrepancies between the two systems\ncould cause a bunch of issues that if not monitored, would create bigger\nissues for us. We decided that we would address this in the same way we prevent\nregressions in our applications; we would build out a test suite. Our\nengineering teams are very autonomous which meant that this test suite\nneeded to be easy to understand and use by the majority of the\nengineering team since they could be potentially making changes and\nwould need to verify behaviour. After some discussions, we landed on RSpec . RSpec\nis already a well understood framework in our test suites and the\nmajority of our teams are using it daily. Even though using RSpec would\nget us most of the way, we would still need to extend it to add\nsupport for the expected HTTP interactions and conditions. To do this,\nwe wrote HttpSpec . This is our HTTP RSpec library that performs the\nunderlying HTTP request/response lifecycle and has a bunch of custom\nmatchers for methods, statuses, caching, internal routing and protocol\nnegotiation. Here is an example of something you might see in our test\nsuite: 1 2 3 4 5 6 7 8 9 10 11 12 # HTTP/S RSpec . describe 'themeforest.net' do it { is_expected . to redirect_http_to_https } it { is_expected . to support_tlsv1_2 } end # Caching RSpec . describe 'themeforest.net/category/all' do it { is_expected . to be_browser_cachable } it { is_expected . to be_shared_cachable } it { is_expected . to serve_stale_while_revalidating ( ttl : 60 ) } end This solved the issue for most of the functionality we were looking to\nport however we still didn’t have a solution for DNS. We started putting\ntogether a proof of concept that relied on parsing dig responses and a\nshort while later decided that wasn’t scalable to our configuration due\nto the number of variations that could be encountered. This prompted us\nto go in search for a more maintainable tool. Lucky for us, Spotify had\nalready solved this issue and open sourced rspec-dns . rspec-dns was a great option for us since it could be integrated into\nour existing RSpec test suites and gave us the same benefits we wanted\nin our edge test suite. This is what our DNS tests looked like: 1 2 3 4 5 6 7 8 9 10 RSpec . describe 'themeforest.net' do # CNAME it { is_expected . to have_dns . with_type ( 'CNAME' ) . with_rdata ( 'origin.hostname' ) } # TXT it { is_expected . to have_dns . with_type ( 'TXT' ) . with_data ( 'v=spf1 include:spf.mandrillapp.com -all' ) } # MX it { is_expected . to have_dns . with_type ( 'MX' ) . with_exchange ( 'aspmx.l.google.com' ) . with_preference ( 1 ) } end Now that we had a way of confirming behaviour on both systems, we were\nready to migrate! Making the move The second big issue we hit was that the two providers didn’t use the\nsame terminology. This meant that a “zone” in provider A wasn’t\nnecessary going to be the same thing in provider B. Remedying this wasn’t a straight forward process and required a fair\namount of documentation diving and experimentation with both providers.\nIn the end, we built a CLI tool that took the API responses from our old\nprovider and mapped then to what our new provider expected to manage\nthe equivalent resources. This greatly reduced the chance of human error\nwhen migrating these resources and ensured that we would be able to\nreliably create and destroy resources over and over again. An upside of\ntaking this automated approach is that we could couple resource\ncreation with spec creation. For instance, if the CLI tooling found a\nDNS record in provider A it would also update our specs to include an\nassertion based on what was going to be created in provider B (Yay, for\nfree test coverage!) Our approach to cutting over sites followed standard Test Driven\nDevelopment process: Create the expected tests Run our test suite and see them all fail Port over the functionality to the new provider Re-run the test suite and see what is missing Rinse and repeat As an additional safety measure, we configured our edge and DNS test\nsuite to run hourly (outside of regular Pull Request triggered builds)\nand trigger notifications for any failures. This ensured that we were\nconstantly getting feedback on a quickly changing system if we broke\nanything. To keep the blast radius of changes as small as possible while we were\ngaining confidence in the migration process, we migrated the systems in\norder of traffic and their potential for customer impact. By taking this\napproach, we were able to give stakeholders confidence we were able to\nbring over larger systems without impacting users. Once we were happy the site was working as expected we would release it\nto our staff using split horizon DNS to gain\nsome confidence that there wasn’t anything missed. If a regression was\nfound, we’d go back through the TDD process until we were completely\nconfident in our changes. After we were happy with the testing, we’d schedule some time to perform\nthe public cut over. On cut over day, the migration team would jump into\na Hangout and start stepping through the runbook and monitoring for any\nabnormal changes. A caveat to note about DNS NS record TTLs: Despite taking precautions\nsuch as lowering the NS TTLs weeks before hand, the TTL for NS records\nare pretty well ignored by most implementations. This means that while\nyou may cut over at 8am on a Monday, the change may see a long tail\nuntil full propagation is achieved. In our case, up to 3 or 4 days in\nsome regions. For this reason we introduced a new 24x7 on call roster\nthat would help the system owners mitigate this issue should we have\nneeded to roll back the cut over. Final thoughts Embarking on a migration project for your edge network provider is no\nsmall feat and it definitely isn’t without risks. However, we are\nextremely pleased thus far with the improvements and added\nfunctionality that we have gained from the move. In the future, we will be looking to integrate our edge provider closer\nwith our origin infrastructure. The intention behind this is to automate\naway some of the manual intervention we currently perform when applying\ntraffic mitigation. In the long term this will help us build a safer and\nmore resilient Envato ecosystem.", "date": "2018-03-06"},
{"website": "Envato", "title": "\n          Building a scalable ELK stack\n        ", "author": ["Patrick Robinson"], "link": "http://webuild.envato.com/blog/building-a-scalable-elk-stack/", "abstract": "February 11 , 2018 by Patrick Robinson Building a scalable ELK stack An ELK stack is a combination of three components; ElasticSearch, Logstash and Kibana, to form a Log Aggregation system. It is one of the most popular ways to make log files from various services and servers easily visible and searchable. While there are many great SaaS solutions available, many companies still choose to build their own. When we set about building a log aggregation system, these were the requirements we had: Durability: we need to persist logs long term with a very high level of confidence. Integrity: we need to ensure logs cannot be tampered with in the event of a security breach. Maintainability: we have a small team with limited resources to operate and manage the platform. Scalability: we need to be able to deal with a very large number of events that may spike during peak periods, outages and when bugs are introduced on the producer end. ELK stacks are not known for their Durability or Integrity, so we had to think outside the box to solve these problems. But they are fairly easy to maintain and scale, when they are designed and implemented thoughtfully. Logstash can become particularly unruly if not implemented carefully. Structured Logs “You want structured data generated at the client side. It isn’t any harder to generate, and it’s far more efficient to pass around.” - Charity Majors We decided early on to enforce the complexity of making logs easy to consume on to the application or server that produced them. Each server has an agent that collects logs from a file or network socket. Either the logs are structured or not, if they are not the agent parses them and forwards them on.\nThis greatly simplifies the Logstash configuration. Instead of having dozens of rules that covered a variety of applications (nginx, HAProxy) and frameworks (Rails, Phoenix, NodeJS) we have a single JSON format. Queuing “(A Queue) is imperative to include in any ELK reference architecture because Logstash might overutilize Elasticsearch, which will then slow down Logstash until the small internal queue bursts and data will be lost. In addition, without a queuing system it becomes almost impossible to upgrade the Elasticsearch cluster because there is no way to store data during critical cluster upgrades.” - logz.io Since building our modified ELK stack we have experienced incidents where the number of logs being created was greater than we were able to process. Having a queue in place was invaluable to buffer the logs whilst we caught up. It also allows us to easily take down any part of our stack for maintenance without losing logs. For the queue we chose AWS Kinesis, because it scales well beyond what we will need and we don’t have to manage it ourselves. Logstash now has Persistent Queues , which are a step in the right direction, but they are not turned on by default. Persistent Queues also have an important side effect, without them Logstash is a stateless service that can be treated like the rest of your Infrastructure; built up and torn down whenever you want. With persistence they become stateful and your Logstash instances become harder to manage. They have to be managed like a database; backed up, disk space carefully measured and data replicated somehow between multiple data-centres to make them highly available. Many people use Kafka or Kinesis to form a queue in front of Logstash, which comes with in built replication. Logstash may be more manageable than Kafka, particularly on a smaller scale, but Kafka and Kinesis are a lot more robust and have been developed with durability as a primary concern. Durability While having a queue improves the durability of our logs in ElasticSearch there’s still a risk we could lose them from there. We can replicate our shards across multiple availability zones, but there’s still the risk, through human error or catastrophic failure, that logs could be lost. So we configure all our Log Agents to forward logs to both a Kinesis Stream and a Kinesis Firehose Delivery Stream. This Delivery Stream persists the logs to an S3 bucket for long term archival.\nIn the event of a failure we can always retrieve logs from S3. Integrity One of the many benefits of Centralised Logs is providing an audit trail of actions taken by users in any given system. During a security breach they form an essential piece of evidence for establishing what was breached, how and potentially by whom.\nBut if the log servers themselves are breached the audit trail could be modified, rendering them useless. This forms the basis of the PCI DSS 10.5.2 requirement: “Protect audit trail files from unauthorized modifications” Therefore all the S3 buckets we use for long term archival are stored in a dedicated AWS Account with much tighter security controls. Amazon provide more details on how this works . A side note about Logstash Plugins While Logstash core is a robust service, there are many community maintained plugins with varying levels of maturity. In an early implementation of this platform we attempted to load some logs from S3 using the logstash-input-s3 plugin. However we had many issues: The plugin doesn’t support assuming a role to read logs, which was necessary since the S3 bucket is in a different account and the objects are sometimes owned by a third account (such as ELB and CloudTrail logs). We had to write the code ourselves and raised a Pull Request upstream The plugin assumes if the log is JSON the message field will be a string, which may not be the case. The fix for this was pushed upstream and is fixed in newer versions We also tried to use a number of Codec plugins to parse CloudTrail and CloudFront logs, but had many issues including a lack of compatibility with Logstash 5.\nIn the end we dumped all plugins except logstash-input-kinesis . The curious case of the blocked pipeline Another issue we faced was Logstash seemed unable to keep up with CloudTrail logs. This manifested as all logs being delayed several hours at regularly intervals. We tried to report metrics from the S3 input plugin, but this proved difficult to get working. To understand why we need to look at the structure of a CloudTrail payload as delivered to S3: 1 2 3 4 5 { \"Records\": [ ... ] } Reference Inside the Records array can be hundreds of events. This means Logstash has to deserialise a large (several megabyte) JSON file before passing it down the pipeline. In testing we found that Ruby, even JRuby which is used by Logstash, would take dozens of seconds to load such a large JSON string.\nUltimately Logstash is designed to deal with streams of logs and not large serialised payloads. Our theory is the CloudTrail logs were choking all the worker threads causing all logs to be delayed.\nThe number of Logstash instances, with their RAM and CPU requirements, needed to ingest all our CloudTrail logs was cost prohibitive. To solve this we looked at ways to pre-process the events before they were consumed by Logstash. Writing a small program to chunk the JSON into smaller events and feed them to the Kinesis Stream also simplified our architecture since all events now come from Kinesis and reduced the responsibility and complexity of the Logstash implementation. We decided to write this program in Golang, as benchmarks showed it was six times faster at deserialising large JSON strings than Ruby or JRuby. Tuning Logstash for performance The issue with CloudTrails was just one of the performance issues we have experienced with Logstash. Over time we developed a better understanding how Logstash performs and how to troubleshoot it. The Elastic guide provides some insights into how you can scale worker threads and batch sizes to get better utilisation and throughput. However these are very one-dimensional in tuning performance, they only generally affect filter and output components and not performance issues in the input plugin. For instance the previous issue discussed is exacerbated by the fact the S3 input plugin is single threaded . Adding more worker threads or increasing the batch size does not improve this issue. Given a Logstash Pipeline, consisting of input, filter and output plugins, how do we find the bottleneck? A simple way is to start with the guide provided by Elastic and see if this improves performance. The pipeline.workers setting determines how many threads to run for filter and output processing. If increasing the pipeline.workers setting improves performance then great! If not though, the issue could still be an input, filter or output plugin. To determine which we can dive into the Logstash API: $ curl localhost:9600/_node/stats/pipeline?pretty=true This API, available in Logstash 5.0 and greater, gives a breakdown of the number of events that have executed in the pipeline and how long each step has taken. To understand what these numbers mean we first need to understand a bit more about how the Logstash Pipeline works: Each input stage in the Logstash pipeline runs in its own thread. Inputs write events to a common Java SynchronousQueue. This queue holds no events, instead transferring each pushed event to a free worker, blocking if all workers are busy. Each pipeline worker thread takes a batch of events off this queue, creating a buffer per worker, runs the batch of events through the configured filters, then runs the filtered events through any outputs. With this in mind, and some basic queuing theory, we can see how increasing the pipeline.batch.size and pipeline.workers may improve throughput. If we process more events at a time we can reduce the number of network calls made to the output we are feeding. If we have more workers, we can process more events at a time. However if the input or output plugin are single threaded, or a filter plugin takes a long time to process each event, we need more CPU and RAM to be dedicated to Logstash (either by creating more instances, or adding more resources to the existing instance), which becomes cost prohibitive. A blocked pipeline, in which the input worker is waiting for a free thread to handle a new batch of events, can be discovered by looking at the queue_push_duration_in_millis statistic from the node pipeline stats. For this example we’ll look at a Logstash instance we use that takes inputs from syslog and forwards them to a Kinesis Stream and a Firehose Delivery Stream: 1 2 3 4 5 6 7 8 \"pipeline\" : { \"events\" : { \"duration_in_millis\" : 3764814011, \"in\" : 28471136, \"out\" : 28471136, \"filtered\" : 28471136, \"queue_push_duration_in_millis\" : 3758074993 }, By dividing the queue_push_duration_in_millis by the duration_in_millis we can determine the percentage of time that logstash spent waiting for a free worker: 1 3758074993 / 3764814011 = 0.9982 This means Logstash spent 99.82% of it’s time waiting for a free worker. To see which pipeline step is taking the most time, we can extract the duration_in_millis of each step: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \"plugins\" : { \"filters\" : [ { \"events\" : { \"duration_in_millis\" : 1767002, \"in\" : 28471136, \"out\" : 28471136 }, \"matches\" : 28471136, \"name\" : \"date\" } ], \"outputs\" : [ { \"events\" : { \"duration_in_millis\" : 1735364, \"in\" : 28471136, \"out\" : 28471136 }, \"name\" : \"kinesis\" }, { \"events\" : { \"duration_in_millis\" : 1459221995, \"in\" : 28471136, \"out\" : 28471136 }, \"name\" : \"firehose\" } ] Here we can see the “firehose” output plugin is taking a lot longer than others. 99% of time is spent in the firehose plugin! As it turns out the Firehose output plugin we were using was sending each event one at a time in a single thread, limiting it to about 20 events per second . Summary Building any kind of event processing pipeline is not an easy task and Log Aggregation is no exception. But as with any pipeline, decoupling components to reduce their responsibilities greatly simplifies the architecture and makes it easier to scale and troubleshoot issues. By taking this approach to the problem of Log Aggregation we have been able to scale to consistently processes 1000 events per second, with regular spikes exceeding 1300 events per second.", "date": "2018-02-11"},
{"website": "Envato", "title": "\n          A real world story of upgrading react-router to v4 in an isomorphic app\n        ", "author": ["Fraser Xu"], "link": "http://webuild.envato.com/blog/a-real-word-story-of-upgrading-react-router-to-v4-in-an-isomorphic-app/", "abstract": "May 08 , 2017 by Fraser Xu A real world story of upgrading react-router to v4 in an isomorphic app While working on the new Envato Market Shopfront app, the team agreed to always keep all the dependencies in the project up to date. Sometimes it was just straightforward patch or minor version upgrade, but sometimes it could also be breaking changes that need a whole lot of thought. The upgrade to react-router v4 happened to be a good example. The story First a little bit background on our current stack (and version): React for the view layer (v15.4.2) Redux for state management (v3.5.2) React Router for routing (v2.8.1) Node.js for server-side rendering and providing a simple proxy layer to our APIs (v6.10.1) Webpack (v2.3.3) and Babel for bundling JavaScript for server and browser My original plan was to upgrade all the dependencies in one pull request. But when it comes to the React related package families, things start to get out of control. For those who are using React in their project already, you may have already heard about the latest changes to React v15.5.0. The biggest change is that we’ve extracted React.PropTypes and React.createClass into their own packages. This means, for every single component that is using those two packages or methods, it will have to be updated to using the new packages to get rid of all the deprecation warnings. Luckily, the React team always provide nice codemod with react-codemod to automatically migrate the code. But what about third party React related modules? If you’ve chosen your project’s packages wisely and with a little luck, the package author would have already released a new version to support the latest release of React and, even if that’s not the case, this might be a good opportunity to give back by sending a pull request to the repo. Everything went pretty smoothly until it came to upgrade React Router. We are currently on v2.8.1, do we want to upgrade to v3 or v4 now? Considering all the changes we’ve already made to the other React packages, I thought that there might be too many changes in one pull request, so in the end I decide to try to only update to v3 (as I’ve heard React Router v4 has changed dramatically since the previous version) in a separate pull request. It seems to me that the biggest change from v2 to v3 for React Router is withRouter according to the change logs. Add params , location , and routes to props injected by withRouter and to properties on context.router It turns out to be a big problem for us because we depends on the location object heavily for critical search query filters, SEO and other stuff. Previously, location was not injected by withRouter , we were passing a modified version of it from the very top page level down to component which need access to location . And not by coincidence, those components also use withRouter to do props.router.push for page transitions (router here is injected by withRouter to component props). The newer version however providing that, we will now have lots of conflicts regarding the location object. Because the code is heavily dependent on React Router and we can’t change the internal API provided by it, we can only modify or rename the location we are passing down which is not a small amount of work. Considering the amount of work from v2 to v3, why not upgrade to v4 directly?\nI decided to have a try. The how Before reading this I highly recommend reading the migrating guide from the official Github Repo first. The first change I made is to install the new react-router-dom and update all the references in the code from react-router to react-router-dom . For those who don’t know what react-router-dom is and the difference between them, short answer is that react-router includes both react-router-dom and react-router-native . For a web based project react-router-dom is usually what you need. 1 2 - import { withRouter } from 'react-router' + import { withRouter } from 'react-router-dom' Another big difference is that instead of having a centralised route configuration for your application and rendering children based on router, now you can define a child component as a normal one inside the component where you need to render content based on current location. However this doesn’t really work for us because we are doing isomorphic rendering. The key for achieving isomorphic rendering is the ability to pre-fetch data before calling React.renderToString so the content(HTML) you sent to browser will have the required data. In the previous version, we normally have a central routes config like this 1 2 3 4 5 6 7 8 9 10 export default ( < Route path = '/' component = { AppContainer } > < IndexRoute component = { SearchPageContainer } /> < Route path = 'category/*' component = { SearchPageContainer } /> < Route path = 'tags/:tags' component = { SearchPageContainer } /> < Route path = 'attributes/:key/:value' component = { SearchPageContainer } /> < Route path = 'search' component = { SearchPageContainer } /> < Route path = '*' component = { NotFoundPageContainer } /> < /Route> ) On the server side, when the request comes in, we can have an express.js middleware like this to handle and render the content 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 export default ( req , res ) => { match ({ routes , location : req . url }, ( error , redirectLocation , renderProps ) => { // here we assume we have defined a `loadData` static method on the component where we want to pre-fetching data const prefetchingRequests = renderProps . components . map ( component => { if ( component && component . loadData ) { return component . loadData ( renderProps ) } }) Promise . all ( prefetchingRequests ) . then ( prefetchedData => { const HTML = React . renderToString ( < App data = { prefetchedData } >< /App>) res . send ( HTML ) }) }) } What about v4? In v4, there is no centralized route configuration. Anywhere that you need to render content based on a route, you will just render a component. There’s no central routes config anymore, how do we co-locate the static loadData method on the render component tree? Luckily there is someone already doing this for us! There’s a package named react-router-config from the react-router team. To achieve the same purpose, now we just have to adjust our routes config into something like this 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 export default [ { path : '/' , component : AppContainer , routes : [ { path : '/category/*' , component : SearchPageContainer }, { path : '/tags/:tags' , component : SearchPageContainer }, { path : '/attributes/:key/:value' , component : SearchPageContainer }, { path : '/search' , component : SearchPageContainer }, { path : '/*' , component : NotFoundPageContainer } ] } ] And in the client side, load it like this 1 2 3 4 5 6 7 render (( < Provider store = { store } > < Router > { renderRoutes ( routes )} < /Router> < /Provider> ), document . querySelector ( '#react-view' )) And on server side, load it like this 1 2 3 4 5 6 7 8 9 10 const componentHTML = renderToString ( < Provider store = { store } > < StaticRouter location = { req . url } context = { context } > { renderRoutes ( routes )} < /StaticRouter> < /Provider> ) With this setup in place, we are now ready to co-locate fetch calls, we can use the matchRotues provided by the package 1 2 3 4 5 6 7 8 9 10 11 12 const prefetchingRequests = matchRoutes ( routes , parsedUrl . pathname ) . map (({ route , match }) => { return route . component . loadData ? route . component . loadData ( match ) : Promise . resolve ( null ) }) Promise . all ( prefetchingRequests ) . then ( prefetchedData => { const HTML = React . renderToString ( < App data = { prefetchedData } >< /App>) res . send ( HTML ) }) And 💥, we now have server side data pre-fetch working with React Router v4.\nThe last thing we need to fix is client side data fetching. This happens when user switches route inside the browser, we will also need to trigger the same requests to load new data to render new content. In the previous version, we can use browserHistory.listen to watch client router change and trigger the network request 1 2 3 4 5 browserHistory . listen ( location => { match ({ routes , location }, ( error , redirectLocation , renderProps ) => { // same as what we did for server side }) }) This could still work with the new version, but we can also follow the example given in react-router-config repo to create a special component, and use withRouter to attach the location object to the component props , then we can use componentWillReceiveProps to listen on location change and trigger the network request call. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 componentWillReceiveProps ( nextProps ) { const navigated = nextProps . location !== this . props . location const { routes } = this . props if ( navigated ) { // save the location so we can render the old screen const prefetchingRequests = matchRoutes ( routes , window . location . pathname ) . map (({ route , match }) => { return route . component . loadData ? route . component . loadData ( match ) : Promise . resolve ( null ) }) Promise . all ( promiseRequests ) . then (( prefetchedData ) => { // do things with new data }) } } Another benefit of using componentWillReceiveProps over browserHistory.listen is that you have a context of the previous location and the current location so you can implement shouldFetchNewData to prevent making expensive network requests. Conclusion Voila! That was pretty much what we needed to do to upgrade an isomorphic React app from React Router v2 to v4. I’ve definitely learned a lot from it: It was probably a bad idea to depend so much on a routing library so deeply nested in application component tree. What we should probably do next is make the location part of the redux store—this way, the next time the location object changes, we simply update it in the redux store without having to modify things all over the code base. Some of you who read this article may be wondering why am I doing the upgrade, same question for me when I was in the middle of doing this. Is it just because we want to keep everything up to date? I’m not sure. Maybe not. I was able to figure out that there are still quite a lot of places in our code which could be improved. Last but not least, I wrote this article because by the time I went to do the upgrade, I couldn’t find any existing example I could refer to, and I hope you may find this one helpful.", "date": "2017-05-08"},
{"website": "Envato", "title": "\n          Remedying the API gateway\n        ", "author": ["Jacob Bednarz"], "link": "http://webuild.envato.com/blog/rememdying-the-api-gateway/", "abstract": "March 22 , 2017 by Jacob Bednarz Remedying the API gateway To expose our internal services to the outside world, we use what is\nknown as an API Gateway. This is a central point of contact for the\noutside world to access the services Envato Market uses behind the\nscenes. Taking this approach allows authors to leverage the information\nand functionality Envato provides on its marketplaces within their own\napplications without duplicating or managing it themselves. It also\nbenefits customers who want to programmatically interact with Envato\nMarket for their purchases instead of using a web browser. The old API gateway The previous generation API gateway was a bespoke NodeJS application\nhosted in AWS. It was designed to be the single point of contact for\nauthentication, authorisation, rate limiting and proxying of all API\nrequests. This solution was conceived one weekend as a proof of concept\nand was quickly made ready for production in the weeks that followed. This solution worked well and allowed Envato to expose a bunch of\ninternal services via a single gateway, removing the need to know which\nunderlying service it was connecting to and how to query it correctly. Here is an overview of how the infrastructure looked: Whilst building a Ruby client for the Envato API I noticed some\nniggling issues that I fixed internally however throughout the whole\nprocess, I was getting intermittent empty responses from the\ngateway. This was annoying but at the time I didn’t think much of it\nsince my internet connection could have been to blame and there wasn’t\nany evidence of this being a known issue. March 2016 saw Envato experience a major outage on the private API\nendpoints due to a change that incorrectly evaluated the authorisation\nstep, resulting in all requests getting a forbidden response. You can\nread the PIR for full details however during this outage we had\nmany of our authors get in touch and conveyed their justified\nfrustrations. Due to this incident, we implemented a bunch of improvements\nto the API and created some future tasks to address some issues that\nweren’t user facing but would help us answering some questions we had\nabout the reliability of our current solution. Following on from these discussions, in April a couple of our elite\nauthors got in touch regarding some ongoing connectivity issues with\nthe API. They were experiencing random freezes in requests that would\neventually just time out without a response or warning. During the\nconversations, they also mentioned they would see an occasional empty\nbody in the responses. We spent a great deal of time investigating these\nreports and working with the elite authors to help mitigate the issue as\nmuch as possible. We finally managed to trace down some problematic\nrequests and begin trying to replicate the issue locally. Even though we were able to eventually reproduce the issue locally, it\nwas very difficult to isolate the exact cause of the problem for a\nnumber of reasons: The single API gateway application had so many responsibilities and\ntracing requests showed it crossing concerns at every turn. We were using third party libraries for various parts of functionality,\nhowever the versions we were running were quite old and included many\ncustom patches we added along the way to fit our needs. The proxying functionality (used for sending requests to the backends)\ndidn’t perform a simple passthrough. There was a great deal of code\ncovering discrepancies in behaviour between backends and the content\nwas rewritten at various stages to conform to certain expectations. All of the above points were made even more difficult since we have very\nlittle in-house support for NodeJS and those who are familiar with it\nare primarily working on the front end components, not the backend so\nthis was a new concept for them too. After spending a few weeks trying to diagnose the issue, we realised\nwe weren’t making enough headway and we needed a better strategy. We got\na few engineers together and starting working on some proposals to solve\nthis for good. During the meeting we decided that going forward, NodeJS\nwasn’t going to work for us and it needed to be replaced with a solution\nthat handled our production workload more effectively and we knew how to\nrun at scale. The meeting created the following action items: Throw more hardware into the mix with the aim of reducing the chance\nof hanging requests by balancing the load over a larger fleet of\ninstances. While this didn’t solve the issue entirely, it would allow\nour consumers hit this issue less often. Find a replacement solution for the NodeJS gateway. It needed to be\nbetter supported, designed in a way that allowed us to have better\nvisibility, be highly scalable and fault tolerant. The new API gateway Immediately after the meeting we scaled out the API gateway fleet and\nsaw a drop off in the hanging requests issue. While it wasn’t solved,\nwe saw significantly fewer occurrences and eased the pressure. We started assessing our requirements for the new API gateway and came\nup with a list of things that we set as bare minimums before a solution\nwas considered viable: Must isolate responsibilities. If a single component of the service\nwas impaired, it should not impact the rest. Must be able to be managed in version control. This was important for\nus since we are big fans of infrastructure as code and all of our\nservices take this approach to ensure we can rebuild our\ninfrastructure reliably each time, every time. Must be able to maintain 100% backwards compatibility with existing\nclients so that our consumers don’t need to redo their whole\napplications to fit our changes. Have great in-house support. If something goes pear-shaped, we have\nthe skills to solve the problems. Following some trialling of PaaS and in-house solutions we landed on AWS API gateway . This met all of our criteria and employed many\nAWS products we were already familiar with which made the transition far\nsmoother. However, a problem for us was that much of the functionality we\nneeded was still under development by AWS and for a long time, we were\nbuilding against a private beta of the service and hit various bugs that\nwere still being addressed by the AWS teams. We finally managed to ship a private beta of the service to a select few\nelite authors in late November and after ironing out a few bugs we\nfound, we dark launched the new gateway to public use in January. Here is what the infrastructure and request flow looks like (as of this\nwriting): This new infrastructure has allowed us to meet all the requirements we\nset out to while also removing a bunch of the confusion around which\ncomponents are associated with which responsibilities. When we go to\nperform changes to a piece of this infrastructure, we know exactly what\nthe impact will be and how to best mitigate it. The move has also given\nus a bunch of improvements around scalability and resiliency. Now if we\nexperience a request surge the gateway infrastructure is able to scale\nto meet the needs instead of throwing errors because all the available\nresources have been exhausted. While it’s still early days, we are far more confident in the API\nGateway’s reliability. Since the move we have full visibility into each\ncomponent, which was lacking before and a major cause of frustration.\nConsequently we are able to measure the availability and act quickly\nwhen a component fails. P.S. If you haven’t already, why not check out the Envato API ?", "date": "2017-03-22"},
{"website": "Envato", "title": "\n          Cloudbleed and its impact on users of Envato websites.\n        ", "author": ["Andrew Humphrey"], "link": "http://webuild.envato.com/blog/cloudbleed-impact-on-envato-dot-com-users/", "abstract": "February 27 , 2017 by Andrew Humphrey Cloudbleed and its impact on users of Envato websites. You may have recently heard reports or seen news about a security bug called “Cloudbleed” affecting sites served by Cloudflare. Envato delivers some websites using services provided by Cloudflare, however Cloudflare have confirmed that none of our websites are directly affected by this security bug. Cloudflare published a detailed explanation of what the bug is and how it came to be, you can read it on their blog . UPDATE Since the original publication of this post,  Cloudflare have released a follow up blog post with information they have learned in their investigations.  The second article focuses more on explaining the real-world impact of the bug, rather than the technical details. How does the security bug impact you? The security bug has caused a very tiny percentage of requests served through Cloudflare to contain information from other unrelated sites. In an even smaller percentage of cases, some of this leaked information included usernames, passwords, and other private information. Even though Envato serves some of our websites, Envato Elements , Envato Tuts+ and Envato Studio through Cloudflare, your data and credentials were not leaked on any Envato websites. Envato takes security very seriously, so as a precautionary measure we have: Expired all current login sessions on all Envato websites that use Cloudflare services. Despite being extremely confident session data was not exposed by this bug, we took this step to make 100% sure that even if session data was exposed it was no longer valid and could not be used to access your account. Replaced all credentials that Envato systems use with other service providers that may have also been affected by this bug. Whilst we are confident no usernames or passwords to Envato websites were leaked through Cloudflare if you used the same password somewhere else it may have been compromised. If you are at all unsure we recommend changing your password. Change your Envato Elements password here .  For more information about this process, feel free to visit our help page updating your account details If you use your Facebook account with Envato Studio, please follow the instructions in this Facebook help post to change your password, otherwise change your Envato Studio password here . Change your Envato Tuts+ password here .", "date": "2017-02-27"},
{"website": "Envato", "title": "\n          Post Mortem report: 19 October 2016\n        ", "author": ["Ross Simpson"], "link": "http://webuild.envato.com/blog/post-mortem-report-19-october-2016/", "abstract": "October 23 , 2016 by Ross Simpson Post Mortem report: 19 October 2016 On Wednesday 19 October, Envato Market sites suffered a prolonged incident and were intermittently unavailable for over eight hours.  The incident began at 01:56 AEDT (Tuesday, 18 October 2016, 14:56 UTC) and ended at 10:22 AEDT (Tuesday, 18 October 2016, 11:22 UTC).  During this time, users would have seen our “Maintenance” page intermittently and therefore would not have been able to interact with the sites.  The issue was caused by an inaccessible directory on a shared filesystem, which in turn was caused by a volume filling to capacity.  The incident duration was 8 hours 26 minutes; total downtime of the sites was 2 hours 56 minutes. We’re sorry this happened.  During the periods of downtime, the site was completely unavailable.  Users couldn’t find or purchase items, authors couldn’t add or manage their items.  We’ve let our users down and let ourselves down too.  We aim higher than this and are working to ensure it doesn’t happen again. In the spirit of our “Tell it like it is” company value, we are sharing the details of this incident with the public. Context Envato Market sites recently moved from a traditional hosting service to Amazon Web Services (AWS).  The sites use a number of AWS services, including Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), and the CodeDeploy deployment service.  The sites are served by a Ruby on Rails application, fronted by the Unicorn HTTP server.  The web EC2 instances all connect to a shared network filesystem, powered by GlusterFS . Timeline (All times are in AEDT / UTC+11) [01:56] Load balancer health checks start failing and terminating web EC2 instances [02:00] “Site down” alerts from Pingdom (one of our monitoring systems) [02:09] The on-call engineers begin investigating [02:19] Web instances are replaced and serving traffic, site appears healthy [02:23] Health checks begin failing again, terminating web instances [02:40] Automatic termination is disabled to avoid flapping and facilitate investigation [02:50] We notice all Unicorn worker processes are tied up accessing a Gluster mount [02:55] Sites are isolated from Gluster by way of an outage flip [03:05] We recognize the Gluster mount is at 100% utilization [03:13] All web instances are replaced to clear stalled processes, restoring site functionality [03:30] Woke up the Gluster subject matter expert from our Content team for help [04:02] Health checks begin failing once again [04:05] We recognize one particular code path doesn’t respect the shared filesystem outage flip [04:14] All web instances are replaced again to clear workers and restore the site [04:55] The next round of health check failures begins [05:25] A fix to the broken code path above is deployed, but the deployment fails [05:44] “Maintenance mode” is enabled, blocking all users from the site and showing a maintenance page [06:22] We notice that the maintenance mode has disrupted CodeDeploy deployments, preventing them [06:47] Maintenance mode is disabled, and we block user access to the site in a different manner [06:56] The deployment of the previous fix is finally successful, users are allowed back into the site [08:57] Once again, health checks start failing [09:01] We notice another code path which doesn’t respect the shared filesystem outage flip [09:13] Our Gluster expert identifies a problem with one directory in the shared filesystem [10:22] A fix to use a different shared directory is deployed, restoring the site to service. Analysis This incident manifested as five “waves” of outages, each subsequent one occurring after we thought the problem had been fixed.  In reality there were several problems occurring at the same time, as is usually the case in complex systems.  There was not one single underlying cause, but rather a chain of events and circumstances that led to this incident.  A section follows for each of the major problems we found. Disk space and Gluster problems The first occurrence of the outage was due to a simple problem which went embarrassingly uncaught: our shared filesystem ran out of disk space. As shown in the graph, free space started decreasing fairly quickly prior to the incident, decreasing from around 200 GiB to 6 GiB in a couple of days.  Low free space isn’t a problem in an of itself, but the fact that we didn’t recognize and correct the issue is a problem.  Why didn’t we know about it?  Because we neglected to set an alert condition for it.  We were collecting filesystem usage data, but never generating any alerts!  An alert about rapidly decreasing free space may have allowed us to take action to avoid the problem entirely.  It’s worth mentioning that we did have alerts on the shared filesystem in our previous environment but they were inadvertently lost during our AWS migration. An out-of-space condition doesn’t explain the behavior of the site during the incident, however.  As we came to realize, whenever a user made a request that touched the shared filesystem, the Unicorn worker servicing that request would hang forever waiting to access the shared filesystem mount.  If the disk were simply full, one might expect the standard Linux error in that scenario ( ENOSPC No space left on device ). The GlusterFS shared filesystem is a cluster consisting of three independent EC2 instances.  When the Gluster expert on our Content team investigated he found that the full disk had caused Gluster to shut down as a safety precaution.  When the lack of disk space was addressed and Gluster started back up, it did so in a split brain condition, with the data in an inconsistent state between the three instances.  Gluster attempted to automatically heal this problem, but was unable to do so because our application kept attempting to write files to it.  The end result was that any access to a particular directory on the shared filesystem stalled forever. A compounding factor was the uninterruptible nature of any process which tried to access this directory.  As the Unicorn workers piled up, stuck, we tried killing them, first gracefully with SIGTERM , then with SIGKILL .  The only option to clear these stuck processes was to terminate the instances. Resolution One of the biggest contributors to the extended recovery time was how long it took to identify the problem with the shared filesystem’s inaccessible directory–just over seven hours.  Once we understood the problem, we reconfigured the application to use a different directory, redeployed, and had the sites back up in less than an hour. GlusterFS is a fairly new addition to our tech stack and this is the first time we’ve seen errors with it in production.  As we didn’t understand its failure modes, we weren’t able to identify the underlying cause of the issue.  Instead, we reacted to the symptom and continued trying to isolate our code from the shared filesystem.  Happily the issue was identified and we were able to work around it. Takeaway: new systems will fail in unexpected ways, be prepared for that when putting them into production Unreliable outage flip In order to isolate our systems from dependent systems which experience problems, we’ve implemented a set of “outage flips” – basically choke points that all code accessing a given system goes through, allowing that system to be disabled in one place. We have such a flip around our shared filesystem and most of our code respects it, but not all of it does.  Waves 3 and 5 were both due to code paths that accessed the shared filesystem without checking the the flip state first.  Any requests that used these code paths would touch the problematic directory and stall their Unicorn worker.  When all the available workers on an instance were thus stalled the instance was unable to service further requests.  When that happened on all instances the site went down. Resolution During the incident we identified two code paths which did not respect the shared filesystem outage flip.  Had we not identified the underlying cause, we probably would have continued the cycle of fixing broken code paths, deploying, and waiting to find the next one.  Luckily, as we fixed the broken code the frequency with which the problem reoccurred decreased (the broken code we found in wave five took much longer to consume all available Unicorn workers than that in the first wave). Takeaway: testing emergency tooling is important, make sure it works before you need it. Deployment difficulties We use the AWS CodeDeploy service to deploy our application.  The nature of how CodeDeploy deployments work in our environment severely slowed our ability to react to issues with code changes. When you deploy with CodeDeploy, you create a revision which gets deployed to instances.  When deploying to a fleet of running instances this revision is deployed to each instance in the fleet and the status is recorded (successful or failed).  When an instance first comes into service it receives the revision from the latest successful deployment. A couple of times during the outage we needed to deploy code changes.  The process went something like this: Deploy the application The deployment would fail on a few instances, which were in the process of starting up or shutting down due to the ongoing errors. Scale the fleet down to a small number of instances (2) Deploy again to only those two instances Once that deployment was successful, scale the fleet back to nominal capacity This process takes between 20-60 minutes, depending on the current state of the fleet, so can really impact the time to recovery. Resolution This process was slow but functional.  We will investigate whether we’ve configured CodeDeploy properly and look for ways to decrease the time taken during emergency deployments. Takeaway: consider both happy-path and emergency scenarios when designing critical tooling and processes Maintenance mode script During outages, we sometimes block public access to the site in order to carry out certain tasks that would disrupt users.  To implement this, we use a script which creates a network ACL (NACL) entry in our AWS VPC which blocks all inbound traffic.  We found that when we used this script, outbound traffic destined for the internet was also blocked.  This was especially problematic because it prevented us from deploying any code. CodeDeploy uses an agent process on each instance to facilitate deployments: it communicates with the remote AWS CodeDeploy service and runs code locally.  To talk to its service it initiates outbound requests to the CodeDeploy service endpoint on port 443.  When we enabled maintenance mode the agent was no longer able to establish connections with the service. As soon as we realized that the maintenance mode change was at fault, we disabled it (and blocked users from the site with a different mechanism).  After the incident, we investigated the cause further, which turned out to be an oversight in the design of the script.  Our network is partitioned into public and private subnets.  Web instances live in private subnets, and communicate with the outside world via gateways residing in public subnets.  Traffic destined for the public internet crosses the boundary between private and public subnets, and at that point the network access controls are imposed.  In this case, the internet-bound traffic was blocked by the NACL added by the maintenance mode script. Resolution As soon as we realized that the maintenance mode script was blocking deployments, we disabled it and used a different mechanism to block access to the site.  This was effectively the first time the script was used in anger, and although it did work, it had unintended side effects. Takeaway: again, testing emergency tooling is important Corrective measures During this incident and the subsequent post-incident review meeting, we’ve identified several opportunities to prevent these problems from reoccurring. Alert on low disk space condition in shared filesystem This alert should have been in place as soon as Gluster was put into production.  If we’d been alerted about the low disk space condition before it ran out, we may have been able to avoid this incident entirely.  We’re also considering more advanced alerting options to avoid the scenario where the available space is used up rapidly. This action is complete; we now receive alerts when the free space drops below a threshold. Add monitoring for GlusterFS error conditions When Gluster is not serving files as expected (due to low disk space, shutdown, healing, or any other type of error) we want to know about it as soon as possible. Add more disk space Space was made on the server by deleting some unused files on the day of the incident.  We also need to add more space so we have an appropriate amount of “headroom” to avoid similar incidents in the future. Investigate interruptible mounts for GlusterFS The stalled processes which were unable to be killed significantly increased our time to recovery.  If we could have killed the stuck workers, we may have been able to recover the site much faster.  We’ll look into how we can mount the shared filesystem in an interruptible way. Reconsider GlusterFS Is GlusterFS the right choice for us?  Are there alternatives that may work better?  Do we need a shared filesystem at all?  We will consider these questions to decide the future of our shared filesystem dependency.  If we do stick with Gluster, we’ll upskill our on-callers in troubleshooting it. Ensure all code respects outage flip Had all our code respected the shared filesystem outage flip, this would have been a much smaller incident.  We will audit all code which touches the shared filesystem and ensure it respects the state of the outage flip. Fix the maintenance mode script The unintended side effect of blocking deployments by our maintenance script extended the downtime unnecessarily.  The script will be fixed to allow the site to function internally, while still blocking public access. Ensure incident management process is followed We have an incident management process to follow, which (amongst other things) describes how incidents are communicated internally.  The process was not followed appropriately, so we’ll make sure that it’s clear to on-call engineers. Fire drills The incident response process can be practiced by running “fire drills”, where an incident is simulated and on-call engineers respond as if it were real.  We’ve not had many major incidents recently, so we need some practice.  This practice will also include shared filesystem failure scenarios, since that system is relatively new. Summary Like many incidents, this was due to a chain of events that ultimately resulted in a long, drawn out outage.  By addressing the links in that chain, similar problems can be avoided in the future.  We sincerely regret the downtime, but we’ve learned a lot of valuable lessons and welcome this opportunity to improve our systems and processes.", "date": "2016-10-23"},
{"website": "Envato", "title": "\n          To The Cloud in-depth\n        ", "author": ["Patrick Robinson"], "link": "http://webuild.envato.com/blog/moving-the-marketplace-to-aws/", "abstract": "August 25 , 2016 by Patrick Robinson To The Cloud in-depth In a previous post, Envato Market: To The Cloud! we discussed why we moved the Envato Market websites to Amazon Web Services (AWS) and a little bit about how we did it. In this post we’ll explore more of the technologies we used, why we chose them and the pros and cons we’ve found along the way. To begin with there are a few key aspects to our design that we feel helped modernise the Market Infrastructure and allowed us to take advantage of running in a cloud environment. Where possible, everything should be an artefact Source code for the Market site Servers System packages (services and libraries) Everything is defined by code Amazon Machine Images (AMIs) are built from code that lives in source control Infrastructure is built entirely using code that lives in source control The Market site is bundled into a tarball using scripts Performance and resiliency testing Form hypotheses about our infrastructure and then define mechanisms to prove them We made a few technical decisions to achieve these goals along the way. Here we’ll lay those decisions out and why it worked for us, as well as some caveats we discovered along the way, but first. The implementation Auto Scaling Groups (ASG) We rely heavily on Auto Scaling Groups (ASGs) to keep our infrastructure running, they are the night watchmen keeping our servers running so we don’t have to. At the core of designing infrastructure to run in the cloud is the concept of designing for failure; only when you embrace failure do you enable yourself to take advantage of the scalability and reliability of cloud services. Every server lives in an Auto Scaling Group; which defines a healthcheck to ensure the server is running. If the server fails it is terminated and replaced with a new one. We also run our ASGs across three Availability Zones (different data centres in the same region). If an Availability Zone fails, the failed servers are launched automatically in another. In order to use ASGs we must define a server artefact to launch. To provide operational efficiencies we want that artefact to be built automatically. Packer and Puppet For simple servers, like our log forwarder, we use vanilla Packer with embedded bash code to build AMIs. The JSON files are our code and the AMIs our build artefact. We’ve been using Puppet for a number of years to manage our servers and we’re comfortable with it. Since the migration took many months it was also good to use the same code we used to define our servers in both our old and new environments, so we didn’t miss any updates or fixes. So for our application servers (which by far have the most complex requirements) we decided to build our AMIs with Puppet and Packer, using BuildKite to do the build for us to ensure consistency. We also have a lot of ServerSpec tests we run locally on our laptops to test our Infrastructure Code. Running them locally on our machine was sometimes a slow and buggy process, especially for those of us who work from home and don’t have such fast internet. Also it’s not entirely accurate as the virtual machine on our laptop doesn’t exactly replicate the AMI we are building. So we developed AmiSpec to help us utilise our Continuous Integration systems to test our servers before they go into production. We build as much of the software and configuration as we can into our AMIs. This enables us to launch replacement instances quickly, but we don’t bake our application into AMIs, as Netflix does , a concept called “immutable AMIs”. This gives us a degree of flexibility to deploy as often as we do at lower cost, while allowing us to launch new servers relatively quickly (generally within a few minutes). Code Deploy In a previous post we discussed how we implemented automated deploys. During the migration we moved our aging Capistrano deployment code to CodeDeploy. We make upwards of 40 changes and 18 deployments to our website a day and we need the deployments to be reliable and fast. The change was significant but necessary, our existing deployment code had many problems: It had grown organically over many years and resembled spaghetti code The mixture of Bash and Ruby made the code difficult to read, write and reason about It had zero unit tests For all these reasons it was extremely fragile which further prevented us from refactoring it. This led to a very brittle deployment approach that everyone wanted to avoid touching. With CodeDeploy the deployment code continued to live in our source code repository, but since CodeDeploy handled downloading the source to every server we were able to write most of it in Bash. Some more complicated parts required Ruby, but even with Bash we are able to write tests using Bats . This differs from our previous approach of “shelling out” to Bash from Ruby, because we are defining specific functions in one language. Each component can then be unit tested and swapped out easily for another if necessary. While we found CodeDeploy was a great choice for us, we also had a couple of issues that tripped us up more than once. Diagnosing launch failures It can be tricky to diagnose why a new instance fails to launch; CodeDeploy will automatically fail the launch of any instance to which it fails to deploy, causing the instance to be terminated. If your ASG is trying to scale up to meet desired capacity and this continually happens you end up launching new instances in a loop. This is very expensive if it goes unchecked since you’re paying for 1 hour of time for each instance launch and can launch many instances an hour. We highly recommend anyone using CodeDeploy to ensure they monitor for this scenario and wake someone up if necessary to resolve it, to do this we chose DataDog , but there are other solutions we won’t cover in this article. To troubleshoot this you should first check the CodeDeploy deployment log, available in the AWS Console. You can also use the get-console-output cli command to see the output from your instance at boot time to help understand if the server started correctly. Creating a new CodeDeploy application with no successful deployment revisions If you have to re-create your CodeDeploy application then there are no healthy revisions of your application. When you have no healthy revisions it is impossible to get CodeDeploy to deploy if you use an Elastic Load Balancer (ELB) healthcheck. Your instances won’t get the application deployed to them on boot because you have no previous “healthy” revision (that is a revision that was successfully deployed). Because they have no application deployed you can’t deploy the application because you have no healthy instances, they will be stuck in a respawn loop as described above. We chose to switch to EC2-based checks to work around this situation. Concurrent deploys There’s a limitation of 10 concurrent deploys per account. Each instance you launch in a CodeDeploy deployment group is one deployment. When we wanted to scale up our ASG by more than 10 instances at a time, the rest of the instances fail to launch and are terminated because their heartbeat times out (10 minutes by default). The maximum number of concurrent deployments is a service limit you can ask AWS to raise. Starting the CodeDeploy agent on boot If you require your user data to run before your app can be deployed, you need to start CodeDeploy from your user data. The CodeDeploy agent can start before user data runs, resulting in a race condition bug. We found the AWS CodeDeploy Under the Hood blog post extremely valuable for understanding how CodeDeploy works and troubleshooting these types of issues. Elastic Load Balancers (ELB) This is a no brainer for our core web servers. ELBs scale to support hundreds of thousands of requests per minute and are at the core of almost every major AWS deployment. We also spent a lot of time planning and creating our healthcheck endpoint. We chose Rack ECG , an in house developed open source tool, to create a simple endpoint for the ELB to check. We were deliberate about only checking hard dependencies of our application, like our databases and cache. We ensure our databases are writable so if our database fails and Rails does not reconnect or re-resolve the DNS entry, the instance is terminated and a new one provisioned. We did lots of testing with different failure scenarios to make sure we could recover automatically where possible and as quickly as possible. ELB connection draining over application reloads One decision we made, without measuring the performance impact, was how we stop serving traffic on a web server in order to deploy a new revision of our code without impacting users. We use Unicorn as our backend HTTP server. It supports a reload command that will allow existing connections to finish while stopping old threads and starting new ones with our new code. This resulted in a four fold increase in response times for a brief period, until Unicorn settled down. Using the ELB to drain connections to our web servers instead we’ve noticed our response time only increases 50% during deployments. Route53 For hosts not directly part of our application server group that don’t need to be, or cannot be, load balanced we use Route53 to register domain names with one or more IPs that we associate to our Auto Scaling Group instances on boot. CloudFormation and StackMaster, a match made in heaven Last year, as part of a hackfort project, some of our developers put together a tool called StackMaster . You can read more about it in a previous blog post . Initially we reviewed Terraform as well as StackMaster, but chose StackMaster for its flexibility combined with the maturity of CloudFormation. Time and again we’ve found the modularity of SparkleFormation dynamics combined with StackMaster Parameter Resolvers and many other features produce small re-usable stacks, with little repetition that enable us to reduce the amount of code needed and make that code easier to reason about. We like smaller stacks because from our previous experience it’s possible, through human error or software bugs, for a stack to become “wedged” in a state that’s unrecoverable. That’s also why we chose not to define our database resources in CloudFormation, but using scripts. When a stack is wedged you have little choice but to destroy it and re-create it. By creating smaller stacks we reduce the impact of having to destroy a CloudFormation stack. Also by splitting certain resources out, like our Elastic IPs and Domain Names, we decouple the infrastructure in a way that allows us to more easily make changes in the future. At the moment we’re considering adding another Load Balancer to an Auto Scaling Group, an operation that requires the Auto Scaling Group be destroyed (along with all its instances) and recreated. This would normally be a change that would cause downtime, but by defining the Domain Name that points to our Load Balancer in a separate stack we can stand an exact copy of that stack up, swap the domain name over to it and delete the old stack, similar to a Blue-green deployment. In Summary We created what we like to call “semi-immutable” machine images, to balance speed of scaling up with cost and flexibility. We chose to restructure how we deploy our infrastructure and application in order to take advantage of a cloud platform. We spent time investigating our technology choices and design decisions to validate they solved the right problems All this would be for nothing if there was no impact on users. No amount of fancy cloud buzzwords would make it valuable if our customers were not better off. Thankfully we’ve already started to see impressive performance improvements on our site, mostly because we recognised during early performance testing some bottlenecks and were able to quickly resolve them. Here’s a chart of our backend response time (in blue) with response times from before the migration in grey. That is how fast our server responds to queries from users. Our agility is what allowed us to make this improvement and it’s what drove us to move the Envato Market sites in the first place, so it’s already paying dividends.", "date": "2016-08-25"},
{"website": "Envato", "title": "\n          Envato Market: To The Cloud!\n        ", "author": ["Ross Simpson"], "link": "http://webuild.envato.com/blog/envato-market-to-the-cloud/", "abstract": "August 16 , 2016 by Ross Simpson Envato Market: To The Cloud! This is the story of how we moved Envato’s Market sites to the cloud. Envato Market is a family of seven themed websites selling digital assets.  We’re busy; our sites operate to the tune of 25,000 requests per minute on average, serving up roughly 140 million pageviews per month.  We have nearly eleven million unique items for sale and seven million users.  We recently picked this site up out of its home for the past six years and moved to Amazon Web Services (AWS).  Read on to learn why we did it, how we did it, and what we learned! A short history of hosting at Envato Back in 2010, Envato was hosted at EngineYard , and looking to move.  EngineYard was then a Ruby-only application hosting service.  The Market sites were growing to the point where the EngineYard service was no longer suitable, and in addition Envato wanted to focus on the core business of building marketplaces rather than running servers.  In August 2010 the Market sites moved to Rackspace ’s managed hosting platform. From 2010 to 2016 the Market sites were hosted by Rackspace.  While managed hosting was a good choice for the Envato of that time, the company and the community have grown significantly since then.  Around 2013, we found ourselves looking once again for a platform that better fit our needs. Like many tech companies, Envato runs “hack weeks”, where we pause our normal work and spend a week or two trying out new ideas. In a hack week in September 2014, a team wondered if it was possible to move Market to AWS in one week.  In true hack week style, this project focused solely on that goal, and was successful.  The team had one Market site running in AWS within the week, and proved that the “lift and shift” strategy was feasible.  While we’d have loved to migrate to AWS then and there, the work was only a proof of concept and nowhere near production-ready. Flash forward nearly two years from that first try, and we’ve made it a reality for all the Market sites! Why we moved A strong element in the development culture at Envato is a “do it yourself” attitude – rather than waiting for someone else to do something for us, we’d prefer to do it ourselves.  Managed hosting was no longer such a good fit, because we had so much to do and were constantly being constrained by the delays inherent in a managed service. The managed nature of the service means we were effectively hands-off of the infrastructure.  While we had access to the virtual machines that ran our sites, everything else – physical hardware, storage, networking – was controlled by Rackspace and required a very manual support ticket process to change.  This process was often lengthy, and held us back from operating with the speed we desired. Working in AWS requires a paradigm shift.  While Amazon still manages the physical infrastructure, everything else is up to you.  Provisioning a new server, adding storage capacity, changing firewall rules or even network layout: these tasks, which would have taken days to weeks in a managed hosting environment, can be accomplished in seconds to minutes in AWS. Sometimes we run experiments to prove or disprove an idea’s feasibility, often in the form of putting a new website up and observing how people use it.  That means taking the idea from nothing to a functional site in a short period of time.  With managed hosting, that could take weeks or even months to accomplish.  In AWS, we can build out a site and its supporting infrastructure very rapidly.  This ability to quickly run an experiment is crucial to developing new products and features. Finally, there is a cost incentive to moving to AWS.  In Rackspace, we leased dedicated hardware and paid a fixed cost, no matter how much traffic we were serving.  We had to pay for enough capacity to handle peak traffic load for our sites at all times; at non-peak times we paid the same rate.  In AWS, “you pay for what you use” – you’re only billed for the actual use of the resources you provision.  The ease with which you can add or remove capacity means that we’ll be able to add capacity during peak times, and remove it during non-peak times, saving money.  After an initial settling period, we’ll be able to model our usage and we expect to see cost savings on the order of 30-50%. How With a limited timeframe to accomplish the migration, we had some tough decisions to make.  Rebuilding the application from scratch to work in the cloud was not an option, due to the enormous amount of time that would take.  Instead, we chose a common strategy in software: MVP, minimum viable product.  We did just the amount of work required to deliver the new, migrated platform, without rebuilding every component.  This reduced the time to market and let us focus on the core problems. A big choice faced by companies moving workloads to the cloud is to “lift and shift” or rearchitect.  Lift and shift refers to picking up an entire application and rehosting it in the cloud.  This has the advantage of speed and reduced development effort, however applications migrated like this can’t leverage the capabilities of the cloud platform and often cost more than they did pre-migration.  Rearchitecting, on the other hand, is cost and time intensive, but results in an application built for the platform which can benefit from all the features it provides. Envato performed a lift and shift migration of our Single Sign-on system ( account.envato.com ) a couple of years ago; we learned that while this approach can be accomplished quickly in the short term, it requires significant work after the fact to get the systems involved running as desired.  Had we realized that up front, we may have chosen to do that work as we migrated. Why not both? Instead of picking one or the other, we chose a hybrid approach in moving the Market.  Functionality that could easily be left unchanged, was.  Only changes that were required or that would be immediately beneficial were made. Amongst the more important changes made were the following: We replaced our aging Capistrano -based deployment scripts with AWS CodeDeploy .  CodeDeploy integrates with other AWS systems to make deployments easier.  While Capistrano can be made to work in the cloud, it falls short supporting rapid scaling. Scout , our existing Rails-specific monitoring system, has been replaced by Datadog for monitoring and alerting.  Monitoring in the cloud requires first-class support for ephemeral systems, and Datadog provides that along with excellent visualization, aggregation, and communication functionality. The key component of the Market sites, our database, was moved from a self-managed MySQL installation to Amazon Aurora , a high performance MySQL-compatible managed database service from AWS.  Aurora offers significant performance increases, high availability, automated failover, and many other features. For some core services, we opted to use AWS’ managed versions, rather than managing ourselves.  We chose Amazon ElastiCache for application-level caching; the Aurora database mentioned above is also a managed service; and we make use of the Elastic Load Balancing service for our load balancers. The application now runs on Amazon EC2 instances managed by Autoscaling groups, effectively removing the concept of a single point of failure from our infrastructure.  If a problem affects any given instance, it is easily and quickly replaced and returned to service.  Adding and removing capacity literally takes nothing more than the click of a button. As a counterpoint, some specific things which didn’t change: Shared filesystem (NFS) for some content: while we really wanted to get rid of this part of our architecture, it would have been too time consuming to remove our reliance on it.  We’ve instead marked it as something to address post-migration. Logging infrastructure: we had a good look at Amazon Kinesis which looked to provide a new AWS-integrated log aggregation system.  However, it turned out that there were irreconcilable problems with this approach, so we left the current system unchanged.  Again, we’ll review this at a later date. The vast majority of the Market codebase was untouched during the migration.  Any code that didn’t need to be changed, wasn’t. A key decision we made early on in the project was to manage our infrastructure as code.  Traditionally, infrastructure is defined by disparate systems: routers, firewalls, load balancers, switches, databases, hosts, and rarely do these systems share a common definition language or configuration mechanism.  That’s a major difference in AWS; everything is defined in the same way.  We chose the AWS CloudFormation provisioning tool, which lets you define your infrastructure in “stacks”.  The benefit is that our infrastructure is under source control; changes can be reviewed before being applied, and we have a history of all changes.  We use CloudFormation to such an extent that we’ve written StackMaster to make working with stacks easier. In Rackspace, our systems were spread over a small number of physical hosts, on which we were the only tenants.  Contrast that to AWS, where our systems are spread out over hundreds of physical hosts which we share with other AWS customers.  A consequence of the increased number of systems is an increased failure rate of individual servers.  However, this can be mitigated by architecting with expected failures in mind: As mentioned previously, all our instances are members of Autoscaling groups, which means they are automatically replaced if they become unhealthy. Most systems are deployed to multiple physical locations, ensuring a problem (e.g. loss of power, cooling, or internet connectivity) at any one location does not affect the availability of the site.  Those systems deployed to only a single location are able to run in any location, and when disrupted in one location can launch in another. Managed services (Aurora and ElastiCache, most notably) are also configured to run in multiple locations, and are tolerant of the loss of a location. Not only have we followed the cloud best practice of designing for failure, we’ve taken it a step further by researching possible failure scenarios, validating our assumptions, and where possible, optimizing our designs for quick recovery.  Additionally, we’ve worked to create self-healing systems; many problems can be resolved without human intervention.  This gives us the confidence that not only can we tolerate most failures, but when they do occur we can quickly recover. Readers familiar with cloud architecture may ask, “why not multi-region?”  This refers to running applications in multiple AWS regions .  Even though we’ve architected for availability by running in multiple locations (availability zones) and storing our data in multiple regions, we still only serve customer traffic from a single region at a time.  For availability and resiliency on a global scale, we could run out of multiple regions concurrently.  Running a complex application like Market simultaneously in multiple locations is a hard problem, but it is on our roadmap. Execution The mandate from our CTO was clear: “optimize for safety.”  Many of our community members depend on Market for their livelihoods; any data loss would be unacceptable.  This requirement led to a hard decision: the migration would incur downtime – Market sites would be entirely shut down during the actual cutover from Rackspace to AWS. While we would have liked to keep the Market sites open for business the entire time, there was no way to guarantee that every change – purchases, payments, item updates – would be recorded appropriately.  This is due in large part to the fact that the source of truth for all this data, our primary database, was moving at the same time.  Maintaining multiple writable databases is a very difficult problem to solve, and we opted to take the safer route and temporarily disable Market sites. Months of planning led to the formation of a runsheet: a spreadsheet containing the details of every single change to be made during the cutover, including timing, personnel, specific commands, and every other detail required to make each change.  Multiple rollback plans were made, instructions for undoing the changes in the event of a major failure. The community was notified; authors were alerted, vendors were consulted, Envato employees informed.  Preparation for the cutover day, scheduled for a Sunday morning (our time of lowest traffic and purchases), began the week prior.  On Sunday morning, the team arrived (physically and virtually) and ran the plan.  Market was taken down, the move commenced, and four and a half hours later, the sites were live on AWS!  Not only live, but showing a small performance increase as well! In the following app-level view from one of our monitoring systems, you can clearly see the spike in the middle of the graph showing the cutover, and the decreased (faster) response time following it: In this browser-level view, you can again see the cutover at the same time, and following that the better-than-historical behavior of the new site: Next steps While the sites have successfully been moved to AWS, we’re far from done.  There is plenty of clean-up work to do, removing now-unused code and configuration.  Our infrastructure at Rackspace needs to be decommissioned. Another major task which will continue for some time is to start modifying the Market to take advantage of the AWS platform – or as it’s more commonly known, “drinking the kool-aid.”  AWS provides many services, and we’ve only scratched the surface during the migration.  As we continue to develop and operate the Market sites in AWS, we’ll evaluate these services and use them where it makes sense. Lessons Learned A factor that really contributed to the success of this migration was having the right team involved.  The migration team had representatives from several parts of the business: the Customer group (owners of the Market sites themselves), the Infrastructure team (responsible for company-wide shared infrastructure), and the Content group (who look after all the content we sell on the sites).  Having stakeholders from each area involved in the day-to-day work of the migration meant that we had confidence that everyone was up to speed and we weren’t missing any major components. Another contributing factor was the “get it done” strategy we employed – the team was empowered to make the necessary decisions to complete the project.  That’s not to say that we didn’t involve other people in the decision-making process, but we were able to avoid the “analysis paralysis” problem by not asking each and every team their opinion on how to proceed. With a project of this scale, there will certainly be things that don’t go right.  One area where we could have improved is communication.  This project affected many teams at Envato, but our communication plan didn’t reflect that.  Notifications were left until later in the project, and we didn’t communicate every detail we should have.  Given another chance, communicating early and often to the rest of the company would have helped ensure everyone was on the same page and had all the information they required.  Similarly, we didn’t communicate our plan to the community until the project was nearing its end; more lead time would have been helpful. On cutover day, we had trouble with the database.  Indeed, migrating the database was far and away the most complex part of the move.  We had a detailed plan for it, but due to the fact that it contained live data and the complexity around it, we had no opportunity to practice that part of the migration.  Finding a way, however difficult, of practicing the database migration may have mitigated some of this trouble.  Ultimately, though, we found solutions to the problems and the database was safely migrated without ever putting data at risk of loss or corruption. Were we to offer any tips to the reader thinking about a similar migration, they’d be these: First and foremost, understand your application.  A solid understanding of what the app does and how it works is critical to a successful migration.  Our biggest fear, happily unrealized, was of some unknown detail of our ten-year-old system that would show up and stop the show. Get AWS expertise on board.  There’s no substitute for experience, and having that experience in the team was critical.  Send team members to training, if necessary, to get the knowledge, but also practice it. Beware the shiny things!  There are a lot of cool technologies in AWS, and it’s tempting to use them anytime you see a fit.  This can be dangerous and distract from the migration goal.  You can always revisit things once the project is complete. Consider AWS Enterprise Support.  It may seem expensive, but having a technical account manager (TAM) on call to answer your questions or pass them off to internal service teams when required will save your team valuable time.  The TAM will also analyze your designs, highlight potential problems, and help you address them before they become real problems.  AWS provides a service called IEM , where the TAM will be available during major events (e.g. migrations), proactively monitoring for issues, and liaising with internal service teams in realtime to address actual problems. Conclusion As this post has hopefully demonstrated, a lot of thought went into this migration.  Due to the comprehensive planning the move went relatively smoothly.  We’re now in a position to start capitalizing on our new platform and making Envato even better! A follow-on post, To The Cloud in-depth , provides more in-depth detail on how our new systems work. – Update 2016-08-18: Thanks to John Barton for his correction .", "date": "2016-08-16"},
{"website": "Envato", "title": "\n          Getting Envato Market HTTPS everywhere\n        ", "author": ["Jacob Bednarz"], "link": "http://webuild.envato.com/blog/getting-envato-market-https-everywhere/", "abstract": "August 05 , 2016 by Jacob Bednarz Getting Envato Market HTTPS everywhere Last month we announced that we had\nfinally completed the move to HTTPS everywhere for Envato\nMarket . This was no easy feat since we are serving over\n170 million page views a month that includes about 10 million products\nlisted and are all user generated content. Along the way we have learnt\nmany valuable lessons that we want to share with the wider community and\nhopefully make other HTTPS moves easier and encourage a better adoption\nof HTTPS everywhere. Behind the scenes, the groundwork for the HTTPS rollout started back in\n2014 with a couple of the engineers implementing a feature toggle which\nallowed staff to opt-in for HTTPS. For a long time, this sat dormant and\nunused by most staff until earlier this year when a few engineers got\ntogether and decided it was time to give HTTPS everywhere another push\nand get it to general availability. But Why? HTTPS isn’t just about the having a padlock or green indicator shown in\nthe browser. It’s about creating a trusted connection between the end\nuser and your services via three protection layers: Encryption: Securing the exchanged data to prevent eavesdropping\non the connections. Data integrity: Confidence that the data has not been altered mid\ntransit without being detected. Authentication: Assuring the website you are connecting to is who\nyou expect them to be. An added side effect of migrating to HTTPS is that you can unlock HTTP/2\nand features like request multiplexing and server push which are great\nnews for performance! Last year, Google announced HTTPS as a ranking\nsignal so by migrating to HTTPS you get a boost in\nyour search results too! User managed content We have a lot of user managed content. The problem here is that many\nof our authors don’t have time or additional funds to implement things\nlike content delivery network (CDN for short) caching so most of our\nuser managed content requests would end up needing to hit their origin\nservers to fulfill requests. This was bad for a few reasons: Many authors use shared hosting or very small instances for storing\nthese assets. During the testing phase, we generated a low amount of\ntraffic for a particular set of assets that would sometimes take over\n20 seconds to complete! The result of these slow load times is a very\npoor experience for buyers and would result in many people looking\nelsewhere because they couldn’t see previews or screenshots of the\nproduct quickly enough. Very little HTTPS adoption. If we intended to serve our pages under\nHTTPS, we needed to ensure the assets on the page were also served\nsecurely. The issue here is that it’s very unrealistic for Envato to\nforce users to spend time (and potentially money) on updating all of\ntheir assets to be served via HTTPS to avoid seeing mixed content\nwarnings on the item pages. To solve both of these issues, we decided to use an approach that\nconsisted of an image proxy and a CDN. The image proxy would rewrite all\nof the non-secure links at render time to point at our CDN which would\nhelp speed up response times and allow us to take some of the load off\nauthor origins by caching the assets. Initially we used camo which was built by Corey Donohoe\nwhen he was at GitHub where they needed to solve a similar\nissue . This worked well for us until we started\ntrying to scale it to handle more traffic. GitHub solved the scaling\nissue by adding more worker processes however we decided to try adding\nclustering support so that we could utilise more of\nthe hardware we already had in place. This didn’t solve the problem for\nlong and we eventually ended up back in the same position and needed to\nresize our hardware to account for the additional load. We determined\nthis wasn’t going to be a viable path going forward and we needed a\nbetter solution. After some looking around we found go-camo which is a Go port of Corey’s original project. For a while we ran the\ntwo implementations side by side and discovered that go-camo was able\nto better utilise all of the existing hardware (due to its ability to\nuse more than a single operating system thread) and was easier to debug\nwhen issues popped up. After a couple of weeks of load testing we\ndecided to completely swap to go-camo and start ramping up the number\nof users who were using this. Sharing cookies As you may know, Envato Market is built using Ruby on Rails and out of the box Rails offers the ability to define how you handle your cookies . To continue with our\nincremental rollout, we needed to allow user cookies to be able to be\naccessible on HTTP or HTTPS depending on which protocol their request\nwas served via. This was achieved by omitting the Secure flag on\ncookies until we were confident post rollout that we were not going to\nroll back. Performance One of the big concerns from teams looking to undertake HTTPS migrations\nis that they will incur a performance hit once it’s live and in most\ncases, it’s just not true. Deploying to modern hardware/software setup\nand using a suitable cipher suite mitigates many of the performance bottlenecks that used to be associated\nwith HTTPS. This definitely doesn’t mean you shouldn’t collect metrics\naround these areas and monitor them, it just means that “HTTPS is slow”\nis not a valid excuse. In Envato’s case, we haven’t seen any performance impacts and our end\nuser time is consistent with the weeks prior to the HTTPS rollout. Monitoring One of the most important things you can do during a HTTPS migration is\nMonitor All The Things. By having insight into the changes within your\nstack during the migration you can quickly detect an issue before all\nyour users do. During our rollout some of the metrics we kept a very\nclose eye on were: Exception rate Time spent in network requests End user response time Application response time Instance resource utilisation (CPU specifically) Total number of requests Edge network requests and the count by status code During our rollout we identified a couple of issues, most notably a load\nbalancer misconfiguration. We were seeing a CPU spike on a small subset\nof web instances that were missed in all of our testing which we managed\nto catch before rolled it out to all of our users. Here are two that we put together to keep everyone informed about how\nfar through the rollout we were. The top one was the initial rollout\n(mostly just staff usage) and the second is when we decided to cut\neveryone over to HTTPS. SEO 2016 has been a big year for SEO at Envato. We’ve kicked off many\ninitatives targeting better visibility for search engines into our\nauthor products and during early discussions it was decided we needed to\nbe extra careful during the migration not to undo all of the hard work\nwe’ve put into the last 7 months. To ensure we didn’t do go backwards we\ntook a couple of steps that has helped us stay on top and improve our\nsearch engine rankings: Submit both HTTP and HTTPS sitemaps to Google webmaster tools : In\nthe week leading up to the swap over, we took a snapshot of our\nsitemaps and uploaded them into Google webmaster tools as a new set of\nsitemaps. This was done to ensure that when we swapped over to HTTPS\nGoogle would have access to a HTTP and HTTPS sitemap source and would\nallow Google to continue crawling the HTTP sitemap but at the same\ntime be lead into the HTTPS version of the site. Ensure we maintained 1:1 redirects: This helped ensure our users\n(and bots) still knew where to find us even though we moved to HTTPS. In taking these steps, 61% of high volume terms we track have remained\nstable or improved their rankings since the HTTPS release. The remaining\nterms that have moved backwards were not on page 1 and have not actually\nlost us traffic or revenue. The migration wasn’t completed overnight and took longer than we would\nhave liked but we’ve managed to roll this out without any negative\nimpacts on our users or application which is something we are extremely\nproud of. Hopefully by publishing our journey this will give others\ninformation about migrating to HTTPS along with the added benefits that\ncome with it and remove the stigma that HTTPS is only a painful\nexperience.", "date": "2016-08-05"},
{"website": "Envato", "title": "\n          How we tracked down Ruby heap corruption in amongst 35 million daily requests\n        ", "author": ["Jacob Bednarz"], "link": "http://webuild.envato.com/blog/tracking-down-ruby-heap-corruption/", "abstract": "May 19 , 2016 by Jacob Bednarz How we tracked down Ruby heap corruption in amongst 35 million daily requests Back in November 2015, one of the Envato Market developers made a\nstartling discovery - our exception tracker was overrun with occurrences\nof undefined method exceptions with the target classes being NilClass and FalseClass . These type of exceptions are often a\nsymptom that you’ve written some Ruby code and not accounted for a\nparticular case where the data you are accessing is returning nil or false . For our users, this would manifest itself as our robot error\npage letting you know that we encountered an issue. This was a\nparticularly hairy scenario to be in because the exceptions we were\nseeing were not legitimate failures and replaying the requests never\nreproduced the error and code inspection showed values could never be\nset to nil or false . It’s worth noting that during the assessment of these exceptions we were\nable to confirm that the security of Envato Market and that of our\ncommunity’s data was not impacted. This was a notion we continuously\nchallenged and ensured was still true throughout every step of our\ninvestigations and if at any stage that was not clear, we stopped and\ntightened third party checks and monitoring to make sure we were\ncertain. These exceptions were harder to track down than the regular errors that\nmanifest in a single point of the application and our error tracker\nshowed the errors had begun sometime in October, although we could not\nisolate a specific deployment or change that matched that timeframe.\nInitially we tried upgrading and downgrading newly introduced gem\nversion changes and when none of these stopped the errors, we also\nrolled back our Gemfile to previous months with no success. We also\nread close to every article on the internet related to Ruby heap\ncorruption available and tried all the proposed solutions without any\nluck. To add to the problem this issue was only occurring in production. Any\nattempts to replicate this locally or in our staging environments never\ncreated the error - even attempting to replay the production traffic\nthrough another environment didn’t create the issue. In production this\nwas very difficult to create a reproducible test case as we did not know\nexactly where the problem originated from. About mid investigation we\ndid think we had a script to reproduce it however it turned out that it\nwas just running into the exceptions much like our usual traffic was. Our suspects Premature garbage collection From the early investigations, we suspected a premature GC due to valid\npointers disappearing mid call. This became more suspicious as upgrading\nto MRI 2.2.x and introducing generational GC made our situation worse.\nTo dive further into this we instrumented Ruby GC stats using Graphite and watched for any unusual\nchange in object allocations and heap health. However, we could never\nfind anything that pointed us to a GC problem despite spending a large\namount of our time investigating and tuning this Ruby’s GC behaviour. Unicorn Envato Market runs unicorn as the\napplication server and suspicion was raised after we traced a series of\nrequests back to the same parent process. Since unicorn uses forking and copy on write we thought\nthere could be a chance that the parent process was becoming corrupt and\nthen passing it onto its children processes. We lowered the worker count\nto 1 and attached rbtrace to the\nparent and child processes but came up with nothing that looked like\ncorruption and never managed to capture a segmentation fault in the\nwatched processes so we were able to rule this out. C extensions We use a handful of gems that rely on C extensions to perform the low\nlevel work in a speedy and efficient manner. Using C is a double edged\nsword - using it well results in excellent performance whereas using it\npoorly (especially unintentionally!) results in very difficult to\ndiagnose issues unless you are well versed in the language and\nenvironment. For the most part we relied on valgrind and manual review to identify anything\nthat we thought could be the cause. To get a list of all gems that we\nneeded to inspect we use the following snippet which returns a list of\nthe gems which have a ext directory. 1 puts `bundle show --paths`.split(\"\\n\").select { |dep| File.directory?(\"#{dep}/ext\") }.map { |dep| dep.split('/').last }.join(\"\\n\") Once we had a list of potential culprits we set off reviewing each one\nindividually and running it through valgrind looking for memory leaks\nand anything that didn’t quite look right that might lead us to a heap\ncorruption scenario.  Our investigations here\ndidn’t solve our specific issue however we did submit some upstream patches to the projects that\nwe identified potential issues with. Our Ruby build process Before the Envato Market application got onto MRI 2.x we ran a custom\nRuby build with a series of patches that required us to build and\nmaintain our own fork of Ruby. Ruby is in a far better place now so we\nrarely need to patch it but we still maintain this process as it allows\nus to package up our version of Ruby and ship it to our hosts as a\nsingle package and eliminate attempting to download and build on\nindividual hosts. During the investigations there were concerns that the version of Ruby\nwe were building and deploying was being corrupted at some stage and we\nwere seeing that as the segmentation faults in production. To\ntroubleshoot this further, we tried to build our custom version with\nadditional debugging flags but quickly discovered they were not being\napplied as we thought they were in the past. After spending a bit of\ntime digging into the issue we identified the cause was the position we\nwere passing in the flags and inadvertently ended up getting trampled on\nby ruby-build default options\ninstead. We fixed this within our packaging script and shortly after we\nwere able to verify that our version of Ruby matched the upstream Ruby\nonce it had been packaged up manually and deployed to our hosts. Getting help After a few months of trying and eliminating everything we could think\nof, we reached out for external help. We had spent a bit of time\nanalysing our C extensions but we didn’t feel we went deep enough with\nour investigations. To get more insight into the C side of things we\nteamed up with James Laird-Wah and started\ngoing through our core dumps and gems that relied on C extensions. After many long hours of debugging and stepping through core dumps, we\nhad found a smoking gun in the form of a Nokogiri\nrelease .\nNokogiri RC3 introduced a patch for a nasty edge case where if you have libxml-ruby and Nokogiri\nloaded in the same application, you could encounter segmentation\nfaults due to the way some fields were managed. Updating to Nokogiri\nRC3 saw us drop the number of segmentation faults to a third of their\nprevious counts!  Looking at this behaviour further we identified the\ncause of this edge case was the way libxml-ruby managed the fields\ndespite the ownership on them. In order to address the remaining\nsegmentation faults, we got together with James and formulated a patch\nthat would ensure libxml-ruby would only manage the fields it\nexplicitly owned.  We tested it and deployed it to production where we\nmonitored it closely for 24 hours. Lo and behold, 0 segmentation\nfaults! We’d finally found the cause to our issues!  Excited with our\ndiscovery we pushed the patch upstream and it’s now available at xml4r/libxml-ruby#118 . Ensuring this doesn’t happen again Like most regressions, proactive monitoring and insights into your\nnormal application behaviour are the best solution for avoiding long\nrunning issues like this. To make sure we are not stung with this again,\nwe are taking the following measures: Implementing alerting based on any occurrences of segmentation faults\nwithin our applications. Our applications should not ever be hitting\nsegmentation faults and if they are, we need to mark them as a high\npriority and assign engineers to resolve them. Dependency graph review on each bundle update to see when we have a\nlingering dependency and possible removal. The gem we were using that\nleveraged libxml-ruby has since been refactored out but the\nrelationships and dependencies were never cleaned up once it was\nremoved. Better monitoring and roll ups of exceptions on a team by team basis.\nWe are aiming to better integrate our developer tooling with our\nexception tracker to ensure  we can quickly identify an increase in\nexceptions and work on a resolution before they become a bigger issue.", "date": "2016-05-19"},
{"website": "Envato", "title": "\n          Running Headless JavaScript Testing with Electron On Any CI Server\n        ", "author": ["Fraser Xu"], "link": "http://webuild.envato.com/blog/running-headless-javascript-testing-with-electron-on-any-ci-server/", "abstract": "January 29 , 2016 by Fraser Xu Running Headless JavaScript Testing with Electron On Any CI Server Background Since the end of 2015, the Envato Front End team has been working on bringing a modern development workflow to our stack. Our main project repo powers sites like themeforest.net and serves around 150 million Page Views a month, so it is quite a challenge to re-architect our front end while maintaining a stable site. In addition, the codebase in 9 years old, so it contains the code from many developers and multiple approaches. We recently introduced our first React based component into the code base when we developed an autosuggest search feature on the homepage of themeforest.net and videohive.net . The React component was written with ES6, and uses Webpack to bundle the JavaScript code. As I mentioned above, it’s a 9 year old code base and nobody can guarantee that introducing something new won’t break the code, so we began all the work with tests in mind . This post documents our experiences developing the framework for testing the React based autosuggestion component . One issue I had while writing unit test code is that some of the code depends on a browser based environment because they need to access to some browser only object or APIs. The first solution Most of the unit tests nowadays are running with Nodejs, so in order to emulate a browser environment, jsdom showed up. A JavaScript implementation of the WHATWG DOM and HTML standards, for use with Node.js . Here’s a handy snippet that you could use before your testing code to prepare a DOM environment: 1 2 3 4 5 6 7 8 9 10 11 12 13 import jsdom from 'jsdom' // This part inject document and window variable for the DOM mount test export const prepareDOMEnv = ( html = '<!doctype html><html><body></body></html>' ) => { if ( typeof document !== 'undefined' ) { return } global . document = jsdom . jsdom ( html ) global . window = global . document . defaultView global . navigator = { userAgent : 'JSDOM' } } And in your test code, you could just import it and use it by calling the function. 1 2 3 import { prepareDOMEnv } from 'jsdomHelper' prepareDOMEnv () If your code depends on some DOM helper function like jQuery, you may also need to include the source code of jQuery into the prepared environment, you could do: 1 2 3 4 5 6 7 8 9 10 11 12 import fs from 'fs' import jsdom from 'jsdom' import resolve from 'resolve' const jQuery = fs . readFileSync ( resolve . sync ( 'jquery' ), 'utf-8' ) jsdom . env ( '<!doctype html><html><body></body></html>' , { src : [ jQuery ] }, ( err , window ) => { console . log ( 'Voilà!' , window . $ ( 'body' )) // your actual test code here. }) Notes: in the official jsdom github repo, they give an example of loading jQuery from the CDN which needs an additional network request and can be unreliable and not work if without network. They also have an example loading jQuery source code with nodejs fs module but it’s not clean as you have to tell the path to jQuery. Everthing looks OK so far, but why do we bother to having a real browser environment? The reason is that once things get compliated, your code may depend on more browser based APIs. Of course you could fix your code but what if you are using 3rd party moudles from npm , and one of them happen to depends on XMLHttpRequest , it’s nearly impossible to “mock” everything, and to be honest, I feel uncomfortable doing it this way as it’s really tricky and kinda dirty. Let’s run it in a browser Why not Phantomjs From the problem we saw above, it’s pretty straight forward to think about just running all the tests in a real browser. If you search “headless browser testing” on Google, the first result will be PhantomJS. I haven’t used phantomjs a lot and I’m not familar with how it works, but I’ve been heard bad things about it, “lagging behind more and more from what actual web browser do today”, “have 1500+ opened issues on Github”, “unicode encode issue for different language”. The last concern is actually from my own experince and I mentioned it in my another blog post PDF generation on the web . Last but not least, I’m not quite confident about how Phantomjs deals with Nodejs code. As my testing code is actually not browser only code, it needs to access to nodjes fs module as well. Let’s talk about Electron What is Electron ? Build cross platform desktop apps with web technologies. Formerly known as Atom Shell. Made with <3 by GitHub. It would take another blog post to explain what Electron is and what it does, I have built a few projects with it and also have written a few blog posts about it. The short version, and what really matters to me, is A Nodejs + Chromium Runtime , actively maintained by fine folks from Github and used by Atom editor, Slack etc . To conclude I’ll quote from one of my favourite JavaScript developer dominictarr Electron is the best thing to happen to javascript this year.\nNow we get root access to the browser! Let’s run our code in browser Please read the quick start guide and make sure you know how to write your first Electron App . Since we are not building a real Electron app here but only want to run our JavaScript code in it, there’s a project called browser-run . You can install it with npm install browser-run and use it like this: 1 2 $ echo \"console.log('Hey from ' + location); window.close()\" | browser - run Hey from http : //localhost:53227/ Run test in Electron And if you are writting your test with tape , you could even pipe your testing result to a test reporter like faucet 1 browserify - t babelify test . js | browser - run - p 2222 | faucet There are also a tool specific designed for tape named tape-run A tape test runner that runs your tests in a (headless) browser and returns 0/1 as exit code, so you can use it as your npm test script. With this tool, it is even easier to run your test. 1 browserify - t babelify test . js | tape - run | faucet Tip: There’s also one module to run mocha test named electron-mocha . Important notes As the title indicate, this post is about running tests on any CI server . The reason is that most of the CI servers are neither Mac or Windows, and there’s a known issue with running Electron on Linux, you need a few setup steps to get it running. Here’s a few notes copied from the repo and thanks to juliangruber for including my section on running it on gnu/linux there. To use the default electron browser on travis, add this to your travis.yml: 1 2 3 4 5 6 7 8 addons : apt : packages : - xvfb install : - export DISPLAY = ':99.0' - Xvfb : 99 - screen 0 1024 x768x24 > /dev/null 2>&1 & - npm install Source . For Gnu/Linux installations without a graphical environment: 1 2 3 4 $ sudo apt-get install xvfb # or equivalent $ export DISPLAY = ':99.0' $ Xvfb :99 -screen 0 1024x768x24 > /dev/null 2 > & 1 & $ browser-run ... There is also an example docker machine here . Final step Once we have all setups ready, our test will be much simpler without the need to “hack” a browser like environment: 1 2 3 4 5 6 7 8 9 10 11 12 13 import test from 'tape' import React from 'react' import jQuery from 'jquery' import { render } from 'react-dom' test ( 'should have a proper testing environment' , assert => { jQuery ( 'body' ). append ( '<input>' ) const $searchInput = jQuery ( 'input' ) assert . true ( $searchInput instanceof jQuery , '$searchInput is an instanceof jQuery' ) assert . end () }) And you can put the test code in npm script and call it on your CI 1 2 3 4 5 6 7 { // ... \"scripts\" : { \"test\" : \"browserify -t babelify test.js | tape-run | faucet\" } // ... } Conclusion Voilà! That’s all we needed to get headless JavaScript test running on any CI server. Of course your testing environment may different from mine but the idea is there. As front-end development is changing rapidly recently with things like single page application, isomorphic universal apps, also front-end tooling system like npm, Browserify, Babel, Webpack, testing will become more complex. I hope this setup will make your life suck less and be significantly eaiser. Last but not least, if you have any questions or better way for testing setups, let us know!", "date": "2016-01-29"},
{"website": "Envato", "title": "\n          Introducing StackMaster - The missing CloudFormation tool\n        ", "author": ["Steve Hodgkiss"], "link": "http://webuild.envato.com/blog/introducing-stackmaster-the-missing-cloudformation-tool/", "abstract": "November 04 , 2015 by Steve Hodgkiss Introducing StackMaster - The missing CloudFormation tool CloudFormation is an Amazon (AWS) service for provisioning infrastructure as\n“stacks”, described in a JSON template. We use it a lot at Envato, and\ninitially I hated it. Typing out JSON is just painful (literally!), and the\nAPIs exposed in the AWS CLI are very asynchronous and low level. I wanted\nsomething to hold my hand and provide more visibility into stack updates. Today I’d like to introduce a project we’ve recently open-sourced: StackMaster\nis a tool to make working with multiple CloudFormation stacks a lot simpler. It\nsolves some of the problems we’ve experienced while working with the\nCloudFormation CLI directly. The project is a refinement of some existing\ntooling that we have been using internally at Envato for most of this year, and\nit was built during one of Envato’s previous “Hack Fortnights”. See the changes you are making to a stack before you apply them: When applying a stack update with StackMaster, it does a few things. First\nyou’ll see a text diff of the proposed template JSON and the template that\ncurrently exists on CloudFormation. This helps sanity-check the changes and\nabort if something doesn’t look right. It also shows a diff of any parameter\nchanges. After confirming the change, StackMaster will display the log of stack\nevents until CloudFormation has finished applying the change. Easy ways to connect stacks: StackMaster provides a number of helper functions to deal with parameters.\nOne allows you to easily take the output from one stack and use it as the input\nto another, without having to hardcode it.  We call these helpers parameter\nresolvers. Make it easy to keep secrets secret. Another parameter resolver transparently decrypts encrypted parameters before\nsupplying them to CloudFormation, meaning you don’t need to worry about\nplain-text secrets. Make your parameters easier to understand by using names instead of IDs. Another set of parameter resolvers StackMaster offers allow you to refer to\nAmazon Simple Notification Service (SNS) topics and security groups by\ndescriptive names, instead of obscure and hard to maintain ID numbers. Here’s an example parameter file using those features: 1 2 3 4 5 6 7 8 9 InstanceType: t2.micro VpcId: stack_output: my-vpc/VpcId DbPassword: secret: db_password ssh_sg: security_group: SSHSecurityGroup notification_topic: sns_topic: PagerDuty Make it easy to customise stacks in different environments StackMaster will load and merge parameters for a given stack from multiple YAML\nfiles to allow for region- or environment-specific overrides. You can, for\nexample, set defaults in one YAML file and then use an environment specific\nYAML file to tailor as required. We use this to do things like use a smaller\ninstance type in our staging environment. Apply descriptive labels to regions Think in terms of environments instead of region names. StackMaster allows you\nto operate on your staging stack, rather than on your ap-southeast-2 stack,\nreducing the chance of applying changes where they are not desired. For more details and examples check out our StackMaster\nrepository on GitHub.", "date": "2015-11-04"},
{"website": "Envato", "title": "\n          How Envato defined the expectations of our developers\n        ", "author": ["John Viner"], "link": "http://webuild.envato.com/blog/how-envato-defined-the-expectations-of-our-developers/", "abstract": "September 22 , 2015 by John Viner How Envato defined the expectations of our developers The Envato development team has always had a strong sense of what we stand for, how we work together and what we expect of each other … at least that is what many of us thought. Around 9 months ago our company participated in the Great Places to Work survey, which gauges how our employees feel about Envato as a place to work. Each department received a breakdown of their feedback, and whilst much of our feedback was great, one statement was a clear outlier “Management makes its expectations clear”. This was a trigger to question our assumptions about those expectations. This post tells the story of that journey. The Response Step 1 - Review What We Have We reviewed where we stated expectations, how consistent and available that information is, and how we applied that information. The conclusion was it is patchy and not consistent. Our position description alluded to some expectations, our annual review question didn’t at all, and were broad and generic across the whole company, and goals that line managers set for developers were unique to each developers and did not relate to a common set of expectations. Step 2 - Organise a Working Group Envato has around 60 developers right now, and about 45 when we started this work, so consulting everyone as a large group was not going to be productive. We got together a working group of 7 developers that was a cross section of disciplines and seniority. Step 3 - Come Up with a Framework / Classification We reviewed all the information we had and came up with a basic classification system of expectations: Tech Competency Learning and Teaching Collaboration and Ownership Envato Values Step 4 - A Starting Set of Values / Expectation Next the working group had a go at coming up with a set of these statements, as a template for more broader consultation. We came up with a way of stating them which was a first person assertion, so that a developer could “ask themselves” if they satisfy this expectation, not just so that a manager could “judge” the developer … e.g. “I define the problems I am solving before the solutions” We came up with 13 statements in this first draft, although we chose not to be exhaustive, but more to provide starting points for the rest of the team. Step 5 - Get Ideas all Team Members The entire dev team of 35 staff members was split up into groups and and assigned to one working group member. There were 9 separate sessions ran. They ran a workshop to come up with a unique set of statements themselves. Ideas were collected in individual trello boards, and categorised either with the existing categories or new categories. There were 220 statements, around 20 from each group, generated in these sessions. Step 6 - Merging all Input to One Master List The working group got back together to attempt to synthesise all this feedback. We created a new “Master” board with 5 major lists. The working group moved cards from their workshop boards into the Master board. The lists we came up with, with number of cards in each list was: Technical Competence - 86 Collaboration - 57 Learning and Teaching - 42 Envato Values - 19 What Makes and Awesome Team Mate - 8 Miscellaneous - 7 With everyone’s separate lists on the one board it looked like this! Step 7 - Consolidating Ideas The working group split into 3 groups to consolidate one list each. The outcome of the consolidation was to represent common themes in the input. For example the Collaboration list had 57 cards and was consolidated to 10 major themes, covering about 50 of the cards, with 7 being marked for review. We linked these consolidated cards back to the original card so we could trace individual input through to final statements. Consolidating all our ideas revealed some outliers that were not common across the working groups. We wanted to reduce the final set of expectations to a workable number, so outliers were cut. Step 8 - Finalise the List After much re-wording we came up with our final set of lists and cards that we considered a small enough but not too large list of expectations. Step 9 - Convert to a Github Repo and Request Reviews Once we were happy with our trello cards, it was time to publish these expectations and open them up for comment and review. We decided to use the development process that serves us well for code, which is github hosted repositories and pull requests. After presenting our content to the entire team at a ‘code party’ we found team members started to start conversations via Pull Requests. And finally, publishing our living set of developer expectations to the public. You can find them here at Developer Expectation .", "date": "2015-09-22"},
{"website": "Envato", "title": "\n          How to organise i18n without losing your translation_not_found\n        ", "author": ["Peter Rhoades"], "link": "http://webuild.envato.com/blog/how-to-organise-i18n-without-losing-your-translation-not-found/", "abstract": "July 08 , 2015 by Peter Rhoades How to organise i18n without losing your translation_not_found I’ve written before about Working with Locales and Time Zones in\nRails , but I often feel the i18n library (short for\ninternationalisation) is underused (appreciated?). Perhaps it is even avoided\nbecause of the perception it is more effort to develop with and harder to\nmaintain. This article will, I hope, open your mind to the idea that you will be better\noff using i18n in your application (even for a single language) and that it\ncan be maintainable with some simple organisational pointers. Organisation The number of i8n keys that your application accumulates can become\noverwhelming as your application develops. In my experience, the biggest pain\npoint has been finding the key(s) you’re looking to update if everything is in\none huge file such as the default en.yml . Thankfully, you are not restricted to using a single file. We can adjust the\ni18n load_path in Rails and break up our translation files into more\nlogically grouped files. Rails doesn’t care or place special significance on this structure, all it\ncares about is the key hierarchy it eventually stores after parsing and merging\nthe resulting data structure. 1 2 # config/application.rb config . i18n . load_path += Dir [ Rails . root . join ( 'config' , 'locales' , '**' , '*.{rb,yml}' ) ] Now we organise our locale files to mirror the structure of our app/ directory. So for every view we can have a corresponding locale file, for\nexample: 1 2 3 4 5 6 7 8 9 10 11 |- app |- - views |- - - home |- - - - _widget.html.erb |- - - - index.html.erb |- config |- - locales |- - - views |- - - - home |- - - - - _widget.en.yml |- - - - - index.en.yml This pairing applies even to partials; in fact I prefix them with an\nunderscore as well. Now I can easily find the translations for any view. Tip: When you add a new locale file you will need to restart your Rails\nserver as it will not be loaded automatically. The “Lazy” lookup To compliment our new directory structure we can make a hierarchy in our locale\nfile the same as the path to the view and use a more convenient way to to look\nup locales. This removes the need for you to think up a hierarchy yourself and instead take\nadvantage of a convention everyone can understand/lookup in documentation. 1 2 3 4 5 # config/locales/views/home/index.en.yml en : home : index : heading : Welcome to my homepage 1 2 # app/views/home/index.html.erb <h1> <%= t ( '.heading' ) %> </h1> The same pattern works for partials too. For example: 1 2 3 4 5 # config/locales/views/home/_widget.en.yml en : home : widget : heading : My Widget 1 2 # app/views/home/_widget.html.erb <h3> <%= t ( '.heading' ) %> </h3> Global translations What if you have some translations that need to be “global” to your application\nand don’t fit in a particular view or class, perhaps they are changeable and\nappear in many locations so having them repeated would be inconvenient. The idea can be carefully applied at a global level too when it’s needed. 1 2 3 |- config |- - locales |- - - global.en.yml If you’ve only got a few keys then a single file is fine, if you start finding\nthe file hard to read then break it up into smaller files inside a global directory or whatever makes sense for your domain. Using i18n outside of views This approach is not limited to views, I find it really useful to use for\nvalidation messages in form object and models as well. For example: 1 2 3 4 5 6 7 # app/forms/user/sign_up_form.rb class User :: SignUpForm include ActiveModel :: Model attr_accessor :email validates :email , presence : true end Given a simple form object with basic presence validation as an\nexample, the locale file below shows how you might customise the validation\nmessage. This makes use of Rails “lazy” lookup in a similar way to views and\nwill affect this form only. Global error messages can be set as well, check out the rails-i18n\ngem for an exhaustive list of the defaults you can change in\nRails. 1 2 3 4 5 6 7 8 9 # config/locales/forms/user/sign_up_form.en.yml en : activemodel : errors : models : user/sign_up_form : attributes : email : blank : \"must be supplied or you can't sign up!!!\" Taking care to create our locale file in a corresponding location to our class\nmakes it easy to find these translations in the future. Naming keys Naming things is hard. So don’t over think it, with our file structure this key is already confined to a single view and locale file. If it turns out to be a poor choice you can confidently go and change it. My recommendation for naming your keys is to choose a name after the purpose\nof the key and not just use an underscored version of your translation. One\nexception to this would be things like models attribute labels where it makes sense to just use the translation as the key too. 1 2 3 4 5 6 7 en : avoid : keys_like_this_are_fragile : Keys like this are fragile good : heading : This is a better key call_to_action_message : \"I expect this translation to change so I haven't based my key on it\" Hopefully in the example above you can see the point I am trying to make. This\nisn’t a rule; more of a guide to help you make a more meaningful choice early\non. Remove text from HAML and Slim templates If you use, or have ever used a template engine such as Slim or HAML they are great for structure but I find if you want to start adding text things can get out of hand. 1 2 3 4 5 6 7 8 9 10 # app/views/home/_widget.html.haml %section %h1 My #{ article . name } %p Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut <strong> #{ article . important_info } </strong> magna aliqua. %ul %li Ut enim quis nostrud exercitation %li ullamco laboris nisi ut aliquip ex ea commodo %li non proident, sunt in culpa qui While the example above does not look like chaos it should be seen as a code\n“smell” in my opinion and one that’s easily solved with i18n: 1 2 3 4 5 6 7 # app/views/home/_widget.html.haml %section %h1 = t ( '.heading' , name : article . name ) %p = t ( '.body_html' , important_info : article . important_info ) %ul - t ( '.features' ) . each do | key , translation | %li = translation That’s better! You might notice I call #each on the .feature translation,\nthis is so I can quickly mention namespace lookups : if you call a\nkey with children it will return a Hash with the translation as the value so\nit can used to create the list. Also .body_html is considered a HTML safe translation so\nRails won’t escape the <strong> tag. In my opinion this keeps your focus on good structure without the noise and\ndistraction of interpolated strings. This can be used for ERb templates too. Bonus level: Pluralization I just wanted to share one of my favourite uses for i18n, handling\npluralization. Not simply changing a singular form to a plural, I’m talking\nabout adapting the entire sentence based on the count. For example, given the\nfollowing locale file: 1 2 3 4 5 6 en : comments : number_of_comments : zero : No one has left a comment yet one : There is %{count} comment other : There are %{count} comments You can probably see what’s going on just from that example; depending on the count you pass to the translation it will select the appropriate response. It should be noted that you don’t need to include all those options. You might not need to include a special case for zero in which case it will fallback to other . 1 I18n . t ( 'comments.number_of_comments' , count : @comments . size ) This can help reduce a lot of unnecessary code in your view, not to mention the\nbenefit of providing users with a better message. I suggest taking a look at the documentation for i18n in Rails to\nlearn more about its features and creative uses. This article is more about\nhow to better manage your locales and really only scratches the surface of\nwhat you can do with i18n.", "date": "2015-07-08"},
{"website": "Envato", "title": "\n          Envato Market Structure Styleguide\n        ", "author": ["Jordan Lewis"], "link": "http://webuild.envato.com/blog/envato-market-structure-styleguide/", "abstract": "June 17 , 2015 by Jordan Lewis Envato Market Structure Styleguide Today we have released the Envato Market ‘Structure Styleguide’ to the public. https://market.styleguide.envato.com A Structure Styleguide is a special breed of living styleguide, designed to document an application’s UI logic and all of the possible permutations of the UI. The goal is to have a complete and shared understanding of how an application behaves in any situation, even in the most uncommon edge cases, without having to laboriously recreate each scenario by hand. What does it do? The Envato Market Structure Styleguide is the very same tool that our developers, designers, user experience experts and product team use day-to-day to build Envato Market. It is a living, breathing work in progress and its various elements are likely to change at any time without warning. Although at this stage only a small percentage of the application lives in the styleguide, we are continually adding more as we develop new and exciting features or have the chance to improve existing ones. Why do this? Although the content itself may not be super useful to anyone else, we hope that by making it public we can share our knowledge and experience of a development technique that we have been refining for the past two years and found tremendously beneficial in building complex user interfaces: Styleguide Driven Development. By making it public, we hope that other people can learn first-hand the benefits of a Structure Styleguide and how to effectively document an application’s UI so that nothing is out-of-sight, out-of-mind and ultimately neglected. Resources To learn all about ‘Structure Styleguides’ as well as ‘Styleguide Driven Development’, here are a bunch of resources we’ve put together: Styleguide Driven Development (blog post) Simplifying your workflow with SDD - Agile Australia Conference 2015 (slides) Example Component (gist) Update : We have been actively working on a new design for Envato Market to provide better design and user experience, as a result the market styleguide will no longer be maintained. Please use our new, updated site envato.design .", "date": "2015-06-17"},
{"website": "Envato", "title": "\n          Your puppet code base makes you fear the apocalypse\n        ", "author": ["Dennis Matotek"], "link": "http://webuild.envato.com/blog/your-puppet-code-base-makes-you-fear-the-apocalypse/", "abstract": "May 27 , 2015 by Dennis Matotek Your puppet code base makes you fear the apocalypse Let me paint you a picture. At some point in time someone said ‘hey, wouldn’t it be great if we could manage our servers with that new puppet thing’. ‘Great’, said everyone, ‘Let’s do that and the way we have always done it.’. And that, my friends, is how you end up where we are. Reading our puppet code base reads much like a bad folding story . Everyone had a plot line and tried to weave into a flowing narrative. And like all badly written stories it has many dead ends, twists, continuity issues and obscure meanings. You get that from many different authors - all code bases face the same problem. So in this particular story I’m going to tell how we began solving some of the mysteries within this Odyssey like tail. Without mixing my metaphors, it starts with two families living in a village. The Webs the Web-Trans. Web-Trans were an old family of servers that had been managed with the ‘old ways’ of building servers. Rather than being a respected family of web servers, people started to fear them because they are just not like other web servers. The Webs on the other hand, were a new addition to the village. Much more certain in themselves. But still with that titillating vulnerability that you don’t quite know what you’re getting. Conformity became a very strong movement in the village - no one liked odd things anymore. Something had to change. Now the simple way of fixing this conflict would have been to start a village war where hopefully one side would be wiped out and forever erased from the commit history. But no, not yet at least. The messy problem is that these two web families sometimes used different and sometimes the same paths through the village to get to the end of whatever they are getting to the end of; let’s just call it a happy wedding where they serve web requests. And along each of these pathways that the families take, they often stop and ask the village Gods for directions because they forgot their maps, or lost their way or whatever. So the people that have planned the wedding don’t always know if one of the web families will turn up, be dressed the right way, or have their heads on. So how do we make certain that whatever path our web families take they turn up to the wedding the way we want them? We first of all describe to them how they should look and then we test that they are what we expect. Now I want to talk about Zombies - see it is very hard to stay in your storyline, but I do want to break out of this villagey weddingy metaphor and just start getting to the real story. So we have two ‘types’ of web servers that do exactly the same thing but take two different paths to get there, one with more certainty than the other. Okay, pick the more certain one and start testing against that. In puppet world (man, should have used a story about muppets) we can test modules with things like puppet-rspec and other tools including using vagrant to provision a host for us. That is a good way of doing some quick testing of your manifests. But we wanted to test that the two types of web server actually produced the same web server configuration state. So when the village war started we could be assured that the victor would be what we wanted. With that information we can then even wipe out the entire village, make a new village with completely different paths and lights and stop signs and discos and everything and still have the same type of web server at the end. We decided to introduce ServerSpec to the village. As you may know it is a way of testing your actual server state with rspec code. It ties in nicely with vagrant and is a great addition to your TDD infrastructure. You can use the documentation for serverspec to set up your testing, but here is how we test both types of web hosts without duplicating the testing. Having the same expected state means we can begin to remove the obvious differences and validate that we still get what we expect in a web server. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 spec/ ├── mp_web │   ├── alerting_spec.rb │   ├── common_spec.rb │   ├── logrotate_spec.rb │   ├── nginx_spec.rb │   └── splunk_spec.rb ├── mp_web_trans -> mp_web ├── shared │   ├── common │   │   ├── init.rb │   │   └── mp_common.rb │   ├── logging │   │   ├── init.rb │   │   └── mp_logrotate.rb │   └── monitoring │       └── init.rb └── spec_helper.rb In the above layout, a vagrant host called mp_web will execute a bunch of tests including some shared tests. By using the symlink for mp_web_trans we can perform the same tests for mp_web as we do for mp_web_trans hosts. Now when a change is made to the mp_web_trans hosts we can validate that they will work the same as a mp_web host. Of course this makes deploying changes a much more assured thing. So when the Zombie apocalypse comes and the village is under attack and the story starts taking a surprising turn, our wedding can still go on and be a happy event driven experience. source", "date": "2015-05-27"},
{"website": "Envato", "title": "\n          Push Button Deployments\n        ", "author": ["Mario Visic"], "link": "http://webuild.envato.com/blog/push-button-deployments/", "abstract": "April 10 , 2015 by Mario Visic Push Button Deployments Envato is becoming a large company, with several teams working on many different products spread across various technology stacks. Each of our teams are responsible for their own deployment process, which means each has ended up with its own way to deploy changes.  It’s complicated to grant a new developer access to all the tools and services they need to deploy a single project, let alone multiple projects. We have just finished one of our quarterly “hack weeks” here at Envato, where we take time out from our usual programme of work to spend time on solving new or interesting problems.  Our team of 5 developers decided to try out Zendesk’s open source deployment tool Samson to improve our deployment process. Rather than our current process of having each developer deploy from their local machine, using a centralised deployment tool has some benefits. You don’t need a complex set of permissions, nor have to add SSH keys to a bunch of servers to deploy – you simply need web access to Samson.  Some further benefits: Logins to Samson are authenticated via OAuth, from either Github or Google Deploys are logged Multiple users can view an ongoing deployment Our deploy process now looks like this: A developer merges a pull request into the master branch in GitHub The merge triggers a build of the master branch in our CI service Buildkite Passing builds notify Samson to create a release Releases can be deployed by Samson by either the click of a button, or automatically Samson updates the status in the GitHub deployment API As a finishing touch for this project, we wanted to perform deploys by pushing a physical button.  We got our hands on a wireless physical pushbutton which is connected to a given project, and pressing it triggers a deploy of that project’s latest pending release.  Here is our CEO Collis eager to deploy a new feature. By a press of a button he now can. We’re actually going to get him to deploy this very blog post by pushing the button during our presentation.", "date": "2015-04-10"},
{"website": "Envato", "title": "\n          A Better Way to do Rails with Aldous\n        ", "author": ["Alan Skorkin"], "link": "http://webuild.envato.com/blog/announcing-aldous/", "abstract": "March 23 , 2015 by Alan Skorkin A Better Way to do Rails with Aldous About a year ago the Tuts team encountered an issue with our Rails codebases becoming more unwieldy as they got bigger. Our development pace was slowing down and new features and fixes would cause regressions in unexpected parts of the system. This is a problem that many of us have encountered before, and we often treat it as part and parcel of doing business. This time, however, we decided to see if we could find ways to improve the situation. Our goals were: keep a constant development pace regardless of the size of the codebase ensure that new features and fixes don’t cause regressions in unrelated parts of the system Focusing our attention on OO design helped to address some of these concerns, but along the way we found a few common threads which could be codified into some helpful patterns. We created a gem and slowly started augmenting Rails, trying things out on our own codebases as we went. A little while ago, we decided that our gem was finally useful enough to release publicly. Meet Aldous - a Brave New World for Rails with more cohesion, less coupling and greater development speed. The main issues common to larger Rails codebases that we try to address are: bloated models which don’t respect the SRP controllers that contain a lot of logic, spread around many before_actions the lack of proper view objects, having to make do with templates and helpers Have a glance over the README, it is pretty extensive. We also plan to write a few blog posts about how to use Aldous , as well as give more detail about some of the motivations behind it. As the blog posts appear, we will add them to the README. We hope you’ll give it a try. Bug reports, pull requests and general comments are welcome.", "date": "2015-03-23"},
{"website": "Envato", "title": "\n          Making the Most of BDD, Part 2\n        ", "author": ["Mary-Anne Cosgrove"], "link": "http://webuild.envato.com/blog/making-the-most-of-bdd-part-2/", "abstract": "March 10 , 2015 by Mary-Anne Cosgrove Making the Most of BDD, Part 2 Hi, I’m Mary-Anne. I’m a senior Ruby developer here at Envato. One of the things that I love about my job is that it gives me the opportunity to use one of the practices I am most passionate about - Behaviour Driven Development, or BDD. In Part 1 of this 2-part series I described what BDD is and explained how it is more than simply a way to improve code quality. Today, let’s look at how BDD becomes the living documentation of your system, and how it informs your system architecture. A Living Document An invaluable benefit of having well written functional and unit test suites is that they are the only form of documentation for the system under test that is guaranteed to be correct. When you are starting work on a complex system for the first time the tests are often the best way of learning how the system works. For this reason it is critical to pay attention to writing your tests expressively. Make sure that the important information is clearly visible, and unnecessary details are hidden away. In this example, we create helper methods with parameters that showcase the important variables. mock_default_account creates a mock payment account with a given payment service (PayPal or Skrill). any_valid_params is a set of parameters that works for the form. We are only interested in two of these for the test, so we merge in the two we really care about. We place these helper methods out of the way at the bottom of the RSpec block, so we don’t have to read them unless we want the details. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 require 'rails_helper' RSpec . describe Withdrawal :: RequestForm , :type => :model do subject ( :form ) { Withdrawal :: RequestForm . new ( user , params , request_forensics ) } let ( :user ) { User . make! ( :earnings_balance => Money . new ( 800_00 )) } before do allow ( Withdrawal :: DefaultWithdrawalAccount ) . to receive ( :default_for_user ) . with ( user ) . and_return ( default_account ) end describe '#save' do context 'successful save' do context 'and the author has chosen to use their default account' do let ( :default_account ) { mock_default_account ( 'paypal' ) } let ( :params ) { any_valid_params . merge ( :use_default_account => true , :service => 'skrill' ) } it 'creates a withdrawal based on the default account' do expect ( form . save ) . to be true expect ( Withdrawal . last . payment_service ) . to eq ( 'paypal' ) end end context 'and the author has not chosen to use their default account' do let ( :default_account ) { mock_default_account ( 'paypal' ) } let ( :params ) { any_valid_params . merge ( :use_default_account => false , :service => 'skrill' ) } it 'creates a withdrawal based on the form parameters entered by the user' do expect ( form . save ) . to be true expect ( Withdrawal . last . payment_service ) . to eq ( 'skrill' ) end end end end def any_valid_params { :type => 'single_fixed' , :amount => '500' , :service => 'paypal' , :paypal_email_address => 'test@envato.com' , :paypal_email_address_confirmation => 'test@envato.com' , :skrill_email_address => 'test2@envato.com' , :skrill_email_address_confirmation => 'test2@envato.com' , :taxable_australian_resident => true , :hobbyist => true } end def mock_default_account ( payment_service ) mock_model ( Withdrawal :: DefaultWithdrawalAccount , :user => user , :payment_service => payment_service , :payment_email_address => 'me@here.com' , :swift_detail => nil ) end end Good RSpec tests form technical documentation that provide a detailed view of how your system works. In contrast, good Cucumber Scenarios are a window on the business value of your application.\nEvery value that appears in your test, whether it’s an RSpec or a Scenario, should be important, and it should be clear where it came from and what has happened to it.\nHere’s a Scenario that is giving more detail than someone who’s looking for an overview on the functionality will want: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Scenario: Request a single withdrawal with a fixed amount Given I have earned $ 1000 When I go to the withdrawal page And I fill in \" Full Name \" with \" Alan Somebody \" And I fill in \" Withdrawal Amount \" with \" 100 \" And I select \" PayPal \" from \" Payment Service \" And I fill in \" Email Address \" with \" alan@somewhere.com \" And I fill in \" Billing Address Line 1 \" with \" Somewhere \" And I fill in \" Billing Address Line 2 \" with \" Melbourne \" And I fill in \" Billing Address Line 3 \" with \" Victoria \" And I fill in \" City \" with \" City \" And I fill in \" State \" with \" Here \" And I fill in \" Postcode \" with \" 1234 \" And I submit the withdrawal request Then I should see \" Your withdrawal request has been sent. \" And I should see my balance as $ 900 And I should have a withdrawal with the following details: | amount | 100.00 | | payment_email_address | alan@somewhere.com | | maximum_at_period_end | false | In our first refactoring attempt, we hide the detail behind a single step: 1 2 3 4 5 6 7 8 9 10 11 Scenario: Request a single withdrawal with a fixed amount Given I have earned $ 1000 When I go to the withdrawal page And I fill the form And I submit the withdrawal request Then I should see \" Your withdrawal request has been sent. \" And I should see my balance as $ 900 And I should have a withdrawal with the following details: | amount | 100.00 | | payment_email_address | alan@somewhere.com | | maximum_at_period_end | false | Now the $900 balance and the 100.00 withdrawal amount appear as if by magic in the assertions. The reader has to dive into the steps to figure out where those values came from.\nLet’s refactor that so that the important values are called out: 1 2 3 4 5 6 7 8 9 10 11 12 Scenario: Request a single withdrawal with a fixed amount Given I have earned $ 1000 When I go to the withdrawal page And I fill the form to request a single $ 100 withdrawal And I select to withdraw via PayPal with the email \" alan@somewhere.com \" And I submit the withdrawal request Then I should see \" Your withdrawal request has been sent. \" And I should see my balance as $ 900 And I should have a withdrawal with the following details: | amount | 100.00 | | payment_email_address | alan@somewhere.com | | maximum_at_period_end | false | Tips Make the interesting data the hero. Hide the boring stuff. Don’t be afraid to use literal strings. They catch the eye. Avoid magic values Describe … is what you are testing. Describe can be: an instance method name ( #my_method ) a class method name ( .my_method ) an English description Context … is the environment it’s running in. Use subject {…} and it {…} for succinctness. Do not use should . In code, use expect(...) instead. In descriptions, describe what it does, not what it should do. A Sound Architecture When you write tests before code, you are by definition writing code that is testable. Testable code is more cohesive and less coupled, and that will benefit your architecture. Moreover, when your application has close to 100% test coverage, you can feel confident to refactor code, knowing that if you break anything your tests will probably catch it. Without the freedom to refactor, the application architecture becomes brittle over time. Eventually your app will be unmaintainable. Ideally your unit tests will have a sharp focus on the class that is being tested. Classes rarely operate in isolation, however. What is the best way of creating the dependencies of the class that’s under test? There are two main ways to handle this problem: test doubles and object factories. Each approach has its pros and cons. If you are working on a new system, introducing a test object creation factory like Machinist or Factory Girl can detract from the architectural benefits of TDD by making it just too easy to create complex object structures. This can allow your models to quickly get out of hand. If, however, you are working in a system that already has a complex model structure, object factories can be invaluable for helping you write very readable tests very quickly. This is a trade-off you need to be pragmatic about. Use test doubles when the setup is not too difficult. Use object factories when necessary. In Rails applications, there are many different kinds of test doubles you can use. Here are some examples: Double A simple double is fine when you need a parameter and don’t particularly care about its behaviour.\neg let(:presenter) { double } Instance Double Create an instance double by passing in a class. When you specify its behaviour any methods that would not be available on an object of that class will cause a test failure. This helps ensure your tests are realistic.\neg let(:item_variant) { instance_double(ItemVariant, :cost => Money.new(800)) } Mock Model Use mock_model if you need a double for an active record class. mock_model automatically supplies active record methods like #id . You can also add .as_null_object() to avoid having to specify all internal method calls. Any methods that are called on the mock object that have not had a return value specified for them will return the mock (not null as you might expect!).\neg: let(:item) { mock_model(Item).as_null_object } Stubbing methods You can replace the behaviour of a single method on either a real object or a mock object by using allow(x).to receive(:y).and_return(z) .  (These replace the older syntax x.stub , x.should_receive and x.should_not_receive .)\neg:  allow(Buying::DepositAndCheckout::FormPresenter).to receive(:new).and_return(presenter) Stubbing method chains allow(x).to receive_method_chain(:a, :b, :c …).and_return(q) is a simple way of replacing the behaviour of a single method on a deeply nested dependency. (This is the new syntax to replace stub_chain() .) \neg: allow(Collection).to receive_message_chain(:not_favorite, :find).and_return(collection) Tips: Use a simple double when you don’t really care about the behaviour of the dependency Use an instance double when the dependency’s behaviour is important and you want to be sure your test setup is correct Use method stubs to: verify the calling behaviour of the class under test avoid interactions with dependencies, even if they are not injected control the behaviour of the class under test to allow you to test edge cases easily Use allow().to receive_method_chain() to avoid having to create deeply nested dependencies Use manufactured objects when attempting to use a double results in a test that is overly complex and difficult to read Use manufactured objects for integration tests that test the interaction across classes A Refactoring Here is an example refactoring putting into practice some of the things I’ve talked about. Before: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 describe '#recommended_items_for_item' do let ( :item ) { double ( :item , :id => 7917671 ) } let ( :response ) do { \"class\" => [ \"recommendations\" , \"collection\" ] , \"properties\" => { \"facts\" => { \"item_id\" => \"7917671\" , \"Site\" => \"themeforest.net\" , \"category\" => \"wordpress\" }, \"skipped\" => [] , \"included\" => [ \"item-items\" ] }, \"entities\" => [ { \"class\" => [ \"grouping\" ] , \"rel\" => \"http://www.envato.com/rels/recommended/recommendation-grouping\" , \"properties\" => { ... }, \"entities\" => [ { \"class\" => [ \"item\" , \"recommendation\" ] , \"rel\" => [ \"http://www.envato.com/rels/recommended/marketplace/item\" ] , \"properties\" => { \"id\" => 7668951 , \"recommender\" => \"item-items\" } } ] } ] } end let ( :expected_url ) { \"http://localhost/recommender_api/get_recommendations?category=wordpress&item_id=7917671&restrictions=item-items&site=themeforest.net\" } let ( :options ) { { :timeout => 1 } } it 'calls the item item recommender' do item . stub_chain ( :category , :root , :site , :domain ) . and_return ( \"themeforest.net\" ) item . stub_chain ( :category , :root , :path ) . and_return ( \"wordpress\" ) expect ( RecommenderApi :: Recommender ) . to receive ( :get_recommendations ) . and_return ( response ) result = RecommenderApiAdapter . recommended_items_for_item ( item ) expect ( result ) . to eq ( [ { :title => \"Suggested items\" , :url => \"http://themeforest.net\" , :ids =>[ RecommenderApiAdapter :: Suggestion . new ( 7668951 ) ] } ] ) end end After: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 describe '#recommended_items_for_item' do let ( :item ) { mock_model ( Item ) . as_null_object } let ( :response ) do { \"class\" => [ \"recommendations\" , \"collection\" ] , \"entities\" => [ { \"class\" => [ \"grouping\" ] , \"properties\" => { \"name\" => \"Suggested items\" , \"href\" => \"http://themeforest.net\" }, \"entities\" => [ { \"class\" => [ \"item\" , \"recommendation\" ] , \"properties\" => { \"id\" => 7668951 , }} ] } ] } end it 'returns the recommended items as suggestions' do expect ( RecommenderApi :: Recommender ) . to receive ( :get_recommendations ) . and_return ( response ) result = RecommenderApiAdapter . recommended_items_for_item ( item ) expect ( result ) . to eq ( [ { :title => \"Suggested items\" , :url => \"http://themeforest.net\" , :ids =>[ RecommenderApiAdapter :: Suggestion . new ( 7668951 ) ] } ] ) end end Be Pragmatic I’d like to conclude with one final piece of advice. In everything you do in BDD and TDD, be pragmatic. David Heinemeier Hansson, in his controversial ‘TDD is Dead’ speech , made the flawed assumption that all TDD practices must be followed religiously for the practice to be called TDD. In reality, there are many times when you should feel free to be pragmatic. If it’s hard to create a mock object, feel free to use a real object. If it’s hard to create a real object, feel free to use a factory like Machinist or FactoryGirl. Don’t feel that your unit tests always have to be strictly isolated to the class under test - though that should be your aim most of the time. You don’t always have to write a test before you change code. If you don’t have a clear picture in your mind of the change you need to make, then give yourself the freedom to do some code experimentation before you write your tests. If you are changing a configuration value, eg a constant,  do not write a test that ensures it has that value. A test that is a mirror of the code is no help; it just adds to maintenance burden. Being pragmatic doesn’t mean you’re not doing TDD. You can still write your tests first most of the time, and take advantage of all the great benefits that TDD brings. Want more? Check out the Prezi", "date": "2015-03-10"},
{"website": "Envato", "title": "\n          Chainable BEM modifiers\n        ", "author": ["Jordan Lewis"], "link": "http://webuild.envato.com/blog/chainable-bem-modifiers/", "abstract": "February 16 , 2015 by Jordan Lewis Chainable BEM modifiers Envato Market has been using a css naming convention loosely based on BEM block__element--modifier syntax for the past two years and it has been instrumental in helping us update our front-end codebase and freeing us from legacy constraints. About a year in, a problem was still bugging us. We had two different ways of using modifiers: single class and multiple classes . Whilst both techniques are valid, they lend themselves for use in different scenarios. Plus, having two conventions using identical syntax in the same codebase is a recipe for confusion. Single Class (aka @extend) A single class modifier is delimited with a double hyphen -- to indicate that it is one of a set of possible variations of a module. HTML 1 2 <!-- Large green button --> < button class = \"btn--primary\" > This method pushes all the logic into the CSS and makes use of Sass’ @extend to make the modifications. It’s ideal for modules that only need to make one modification at a time, plus it’s easy to understand at a glance. Sass 1 2 3 4 5 6 7 8 9 .btn font-size : 20 px background-color : grey .btn--primary @extend .btn background-color : green font-size : 30 px padding : 10 px CSS (generated) 1 2 3 4 5 6 7 8 9 . btn , . btn--primary { font-size : 20 px ; background-color : grey ; } . btn--primary { background-color : green ; font-size : 30 px ; padding : 10 px ; } Multiple classes (aka chaining) When chaining modifiers together we start with a base class and then add modifiers to set or override properties. These are also delimited with a double hyphen -- to indicate that they make modifications. This approach keeps the logic in the HTML and offers us the flexibility to configure any given module on the fly. It is best suited for modules with multiple modifiers that are designed to be mixed and matched. This is especially useful for elements that make up a core UI Library, such as buttons, icons, and typography. The downside is that the HTML becomes verbose and repetitive and you may clobber your styles if you are not careful with source ordering. HTML 1 2 <!-- Large green button --> < button class = \"btn btn--color-green btn--size-large\" > Sass 1 2 3 4 5 6 7 8 9 10 .btn font-size : 20 px background-color : grey .btn--color-green background-color : green .btn--size-large font-size : 30 px padding : 10 px Ben Smithett, a fellow Envatonaut, has written an article entitled BEM modifiers: multiple classes vs @extend where he examines the two approaches in more detail. Finding a better solution While looking for a solution to our problem, I stumbled across Dan Telos’ article Sassier (BE)Modifiers in which he proposes a slight variation to the BEM syntax and introduces the concept of BEVM: block__element--variation -modifier . This was an “ aha !” moment for us. All along, we had been treating both ‘variation’ and ‘modifier’ as the same without realising. With this newfound knowledge, we set about updating our modifiers to fit the BEVM model but with a few modifications. Our ‘single class’ approach would be known as a ‘variation’ and not require any change to syntax or implementation, whereas ‘multiple classes’ would become known as ‘chainable modifiers’ and bring a new set of rules and syntax. Chainable Modifiers Chainable modifiers are denoted by a leading hyphen - , a namespace and a descriptor for the modification. HTML 1 2 <!-- Large green button --> < button class = \"btn -color-green -size-large\" > As the name would indicate, chainable modifiers provide us with the ability to configure a module in the HTML with a short, concise syntax. The golden rule is that chainable modifiers should never modify the same CSS property twice for a given module . This is to ensure that styles don’t get clobbered and that the order in which they are applied is irrelevant. Below are some examples of how chainable modifiers are used in Envato Market’s UI Library. HTML 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 <!-- Icon --> < i class = \"e-icon -icon-envato -color-green -size-xl -margin-right\" ></ i > <!-- Typography --> < h2 class = \"t-heading -size-m -color-light\" > Heading </ h2 > < p class = \"t-body -size-s\" > Paragraph </ p > <!-- Inputs --> < input class = \"f-input -type-string -width-full\" > <!-- Notifications --> < div class = \"alert-box -type-success\" > < div class = \"alert-box__icon\" > < i class = \"e-icon -icon-ok\" ></ i > </ div > < div class = \"alert-box__message\" > < p class = \"t-body -size-m h-remove-margin\" > Success!! </ p > </ div > </ div > But won’t that make things hard to find? In the above example we have 3 different modules that use the -size namespace. We have been using chainable modifiers for almost a year now on Envato Market and have yet to encounter any issues with this, but in the rare instance that you are unable to find what you’re after, the following regular expression is your friend. Regular Expression 1 2 btn .+ -color-green /* module.+modifier */ Most text editors should support performing a regular expression search, below is how to achieve this using Sublime Text 3: State and Utility classes ‘State’ and ‘utility’ classes are also a form of modifier and due to their high priority nature should always be applied after other modifiers so that their styles take precedence (with state classes at the very end). The class order that we use in Envato Market is: js-hook block__element--variation -modifier h-helper is-state . State State classes are prefixed with is- and used to indicate that a module has been modified in some way by application logic, be it server-side code or JavaScript. HTML 1 < input class = \"f-input -type-string is-invalid\" > While we have a handful of global state classes such is is-hidden , we prefer to scope them directly to a module. Sass 1 2 3 4 .f-input & .is-invalid color : red border : 1 px solid red Utility (aka Helper) Utility classes are not your traditional modifier, instead they are single-use helper classes designed to perform a simple repeatable task. Prefixed with h- (or u- in other toolkits such as SUIT ), they can be used stand-alone or chained to a module. HTML 1 < h2 class = \"t-heading -size-xl h-text-align-center\" > Special mention (JS Hooks) JS Hooks are prefixed with js- and do not have any styles. Their whole purpose is to be used in JavaScript to target elements in the DOM. Conclusion Discovering that there is a clear distinction between a ‘modifier’ and a ‘variation’ has led us to re-evaluate and challenge our practices. Migrating from BEM to BEVM was a relatively straight forward process that happened over a long period of time, and is still ongoing. Whilst we have mostly limited ‘chainable modifiers’ to the core UI library, there is nothing stopping us from using them more widely across the application. Adopting the BEVM convention and (more importantly) introducing chainable modifiers has been instrumental in allowing us to build a super flexible UI Library. This has enabled us to configure our modules in the HTML with a short, concise syntax, reducing the amount of CSS that we need to write and increasing our development speed.", "date": "2015-02-16"},
{"website": "Envato", "title": "\n          A Case For Use Cases\n        ", "author": ["Shevaun Coker"], "link": "http://webuild.envato.com/blog/a-case-for-use-cases/", "abstract": "February 06 , 2015 by Shevaun Coker A Case For Use Cases What’s the problem? Most Ruby on Rails applications start off as simple web applications with simple requirements. As a developer, you take those requirements, map out the business domain and then implement it using the MVC pattern of models, views and controllers. Over time, more requirements are added and simple controllers can become bloated with complex logic. The obvious next step is to create public methods on your models and move the logic into them, thus keeping the controllers simple. However, this inevitably leads to fat models. One of the guiding principles of Software Design is the Single Responsibility Principle and fat models have too many responsibilities. In fact, you could argue an empty model that extends ActiveRecord::Base already has multiple responsibilities: persistence and data validation/manipulation. Furthermore, an empty ActiveRecord model contains 214 methods (not counting any of the methods on Object). So these classes are already tightly coupled to the Rails framework and if you put your logic in them too, they’re also coupled to the business domain. This means a class has many reasons to change, which makes future changes harder and which also makes it more prone to bugs and merge conflicts. It also means you’re going to end up with huge classes that contain many public and private methods. Large classes make it very hard to see which methods are related to each other as well as where and how they are used in the codebase. If you think ActiveSupport concerns are going to help with that by moving methods into their own files, well now you’ve only made the code even harder to find. Another problem is that now your business logic is obfuscated inside the ORM layer. If you look at the structure of the source code of a typical Rails application, all you see are these nice MVC buckets. They may reveal the domain models of the application, but you can’t see the Use Cases of the system, what it’s actually meant to do . What is a Use Case? Like most other definitions in our industry, the term “Use Case” is overloaded but I really like the definition from Usability.gov : A use case is a written description of how users will perform tasks on your website. It outlines, from a user’s point of view, a system’s behavior as it responds to a request. Each use case is represented as a sequence of simple steps, beginning with a user’s goal and ending when that goal is fulfilled. Written Use Cases are a great tool for explaining how the system should behave by providing a list of the goals. But once the application has been built, the documentation starts to drift away from the implementation and becomes less and less useful. Simon Brown gives a great talk on The Essence of Software Architecture where he talks about the importance of reflecting the architecture in our code. How can we reflect the Use Cases of our application in our code? Implementing Use Cases at Envato Envato Market has a large codebase with a lot of developers working on it, so we needed our implementation to be simple, consistent and conventional. A UML Use Case usually depicts an entire user flow, which may involve multiple actions. We decided to reduce the scope and define a Use Case as a plain Ruby class which defines one user action and all of the steps involved in completing that action. Each Use Case is named after the action the user takes, e.g. PurchasePhoto or ResetPassword . We created a directory app/use_cases so that all of the Use Cases are easy to find (namespaced appropriately under their domains) and a module called UseCase which defines a simple public interface for our Use Cases to implement. use_case.rb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 module UseCase extend ActiveSupport :: Concern include ActiveModel :: Validations module ClassMethods # The perform method of a UseCase should always return itself def perform ( * args ) new ( * args ) . tap { | use_case | use_case . perform } end end # implement all the steps required to complete this use case def perform raise NotImplementedError end # inside of perform, add errors if the use case did not succeed def success? errors . none? end end As you can see, the module consists of a class method .perform that takes as many arguments as required and returns an instance of itself. It wraps the call to #perform in a tap block to ensure that perform remains a command method (any return value is ignored). It also has an instance method #success? , which bases success on whether the Use Case has accumulated any errors. Here’s an example of a class that includes this module; a plain old ruby object named after the operation it carries out. All the collaborating objects are passed in at initialization and the perform method contains all the steps required to execute its goal. purchase_photo.rb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class PurchasePhoto include UseCase def initialize ( buyer , photo ) @buyer = buyer @photo = photo end def perform if create_purchase send_invoice increment_sales_count allow_buyer_to_review_photo end end private attr_reader :buyer , :photo , :purchase def create_purchase @purchase = Purchase . new ( buyer : buyer , photo : photo ) @purchase . save . tap do | success? | errors . add ( :base , \"Purchase failed to save\" ) unless success? end end def send_invoice Buyer :: InvoiceMailer . enqueue ( purchase : purchase ) end def increment_sales_count Photo :: SalesCount . increment ( photo : photo ) end def allow_buyer_to_review_photo Photo :: ReviewPermissions . add_buyer ( buyer : buyer , photo : photo ) end end The main method, #perform , contains the high level steps (which are defined as private methods) required for the action. This means if another developer wants to know what happens when a purchase is completed, they can go straight to the perform method of the PurchasePhoto Use Case. If they need to know the detail behind any step, they can look at the relevant private method to see what it’s doing/calling. The Use Cases are designed to be called from controllers (or from other use cases), creating a clear boundary between a controller, a model, and the business logic. What lessons have we learned? We’ve refactored our Use Cases a few times now and we’ve learnt a few valuable lessons along the way. 1. Don’t mix Queries and Commands Command Query Separation is an important concept in software design, and one that we originally ignored by making the Use Case #perform method return a result object as well as producing side effects. This made our use cases harder to test and harder to refactor (see Sandi Metz’s great testing talk for advice on how to test objects that obey the CQS rule). 2. A great fit for Integration Tests We started unit testing our Use Cases but quickly realised that they were actually a great layer to write integration tests in. This way, we could test the actual side effects the perform method produced, and be confident our business logic was correct. Generally speaking, we unit test the classes in the layers below our use cases, and have a smaller layer of full stack acceptance tests to ensure the system works from the front end down. Here’s a really simple example of what I mean by integration test. purchase_photo_spec.rb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 RSpec . describe PurchasePhoto do describe “ #perform” do subject ( :use_case ) { PurchasePhoto . perform ( buyer , photo ) } it “ creates a purchase record ” do expect { use_case } . to change { Purchase . count } . by 1 end it “ sends an invoice ” do use_case expect ( Mailer . deliveries . last ) . to have_attributes ( subject : “ Purchase Invoice ” ) end end end In our subject we run perform on our use case, so we can make expectations on its side effects. For example, we expect it to change the number of purchase records in the database and we expect an email to be created with the subject “Purchase Invoice”. 3. Don’t put all your logic in your Use Case! This introduces multiple levels of abstraction and gives the use case too much responsibility and knowledge of the rest of your system. Instead, only call high level commands that delegate to lower level objects. This will increase the readability and changeability of your code. 4. Don’t make your Use Cases generic. One of the key benefits of a Use Case is that it reflects a single use case of the system. If you try to make it generic enough to handle multiple use cases, then you lose the benefits of clarity and context . For example, we actually have more than one way to complete a purchase, depending on how the user chose to pay for it. Initially, we tried to make one Use Case handle all the ways to complete a purchase, which only made it complex, confusing and error prone. We replaced this Use Case with multiple single purpose Use Case classes that are simple to read and understand. Don’t be tempted to reuse code for the sake of it . What does the Use Case pattern provide? A consistent interface: All Use Cases are easily identifiable because they include the UseCase module, meaning they share a common interface (namely the .perform method). Self documenting and readable code: Use Cases provide explicit, living documentation of the important user driven actions of your system. They are easy to read because they list out each step at a single level of abstraction. Context and encapsulation: Because a Use Case maps to a single user action and the code inside it encapsulates everything you need to carry out that action, the Use Case creates a contextual wrapper for that operation. Extensibility: It’s easy to extend the behaviour of a Use Case because of the Command-Query separation. It’s easy to change the logic because each step is implemented by a specific object. Decoupling your domain from your framework: Use Cases allow you to keep your rails controllers and Active Record models very simple and decoupled from your business logic. A codebase that reveals its features: Developers can look in one place, app/use_cases , to see all of the user features of the application. In Conclusion Of course, the Use Case pattern is not a silver bullet (yes, I said it); don’t expect it to solve all of your design problems. There are also many similar patterns out there which provide equivalent benefits - always be pragmatic to use the best tools for the problem at hand. However, we can confidently say that introducing use cases in our codebase has greatly helped us to reduce coupling, increase visibility of features and provide context to our complex business domain. Resources There are already a few gems out there that provide similar functionality: Mutations focuses on input validation and sanitisation. Use Case was inspired by the mutations gem but has lot of other features such as step pipelines and pre-conditions. Interaction is a gem that my colleague Steve has just released, based on our Use Cases. Interactor has a slightly different implementation to our Use Cases but virtually identical motivations.", "date": "2015-02-06"},
{"website": "Envato", "title": "\n          Making the Most of BDD, Part 1\n        ", "author": ["Mary-Anne Cosgrove"], "link": "http://webuild.envato.com/blog/making-the-most-of-bdd-part-1/", "abstract": "January 22 , 2015 by Mary-Anne Cosgrove Making the Most of BDD, Part 1 Hi, I’m Mary-Anne. I’m a senior Ruby developer here at Envato. One of the things that I love about my job is that it gives me the opportunity to use one of the practices I am most passionate about - Behaviour Driven Development, or BDD. BDD is the practice of writing functional tests before you write unit tests. It incorporates the older practice of test driven development (TDD), the art of writing unit tests before you write code. At Envato, our code is written in Ruby, our unit tests in RSpec and our functional tests are in Cucumber, but many of these principles can be applied to other languages and frameworks. It’s not (just) Mortein* We all know that automated testing is a great way to kill bugs. But BDD is about so much more than that… It can be the driver of your programming cadence It focuses your attention on your users’ experience It is the living documentation of your system It informs the structure of your application Let’s explore each of these in turn. Along the way, I’ll give you some tips for getting the most out of Cucumber and RSpec. *A popular Australian brand of bug-spray Feel the Rhythm BDD drives your programming cadence like the pendulum drives a grandfather clock. You start with a clean build; imagine the clock striking 1. The first thing you do is write a failing feature test - a Cucumber scenario in our case. Your first job is to get one step to pass. To do that, you’ll need to \nwrite some code - but first you write a failing unit test. You make that pass, then refactor the code.  In getting a single Cucumber step to pass you’ll go through many red - green - refactor cycles in the unit tests, just like the second hand will tick many times before the minute hand moves on. Eventually the whole feature will pass and you can run the entire build; you’ve progressed to 2 o’clock! You can use the inner cycle as a fun way to make sure each partner gets a turn when you’re pairing. One person writes a test, the other writes the code to make it pass, the first person refactors, then it’s the second person’s turn to write a test. When you’re in a good rhythm here it’s like playing ping pong, each person bouncing ideas off the other. The secret to productivity in BDD is getting your second hand to run quickly so you can write your tests in tiny increments. This means you’re never far away from working code. The feature test gives you guidance throughout this process, helping to make sure you’re always progressing towards your end goal. If you’re working in a large Rails application, RSpec startup time is too slow to develop this rapid fire cadence. You really need to be running something like Zeus or Spring. These are gems that pre-load your rails environment for a rapid start to your tests. Keep that rapid-fire rhythm going by only running the single spec you are currently working on, until it goes green. Then run the whole file, before going back to the cuke. It’s also important to keep your hour hand - your whole build - running smoothly. Functional tests run orders of magnitude more slowly than unit tests. Write functional tests only for the major happy path scenarios. Work through the edge cases in the unit tests. Write some integration tests to make sure nothing has fallen through the gaps between your unit tests. If your whole build takes more than 10 minutes to run, set it up to run multiple tests in parallel. Tips: Use a pre-loader like Zeus or Spring Start zeus in one terminal pane and run your tests in another Run a single test by line number, e.g. zeus test spec/models/my_model_spec.rb:50 Write the most unit tests, write the least functional tests Try test ping-pong when you are pairing Cuke it Out Here at Envato, some of our teams work together to develop Cucumber scenarios for our sprint work at the beginning of the sprint. When you write scenarios together you’ll find the whole team develops a good understanding of the requirements for your sprint, and you will discover many more of the ‘gotchas’ than you would if you wrote scenarios individually. Writing the scenarios from the user’s perspective gives you much more empathy for them than you would otherwise have, and helps identify areas where the user experience needs more attention. You can write out the whole set of scenarios at your sprint start. They then become a good guide to your progress throughout the sprint. Here at Envato we’ve added a few tools to our Cucumber toolbox to help with debugging. For example, we have defined a step ‘Then I debug’ as follows: 1 2 3 4 5 6 7 8 9 10 11 Then /^I debug$/ do begin require \"pry\" pry binding rescue LoadError require 'irb' require 'irb/completion' ARGV.clear IRB.start end end This definition reverts to IRB for those who are not using Pry yet. If you’re still using IRB, this is worth a read: Rubyists, It’s Time to PRY Yourself Off IRB! Then there’s ‘Then show me the page’: 1 2 3 Then /^show me the page$/ do save_and_open_page end Tags are a great way to organise your tests. \nYou can tag your tests with anything you want then filter the set of tests that are run according to your tags. You can also define actions to take before or after your tag, to help make your tests more readable. Here’s an example that ensures a feature flip is turned on for a test: 1 2 3 Before '@special_feature_flip_on' do Flip::FeatureSet.instance.strategy('global').switch!(:special_feature, true) end Tips: @wip allows you to check in your work-in-progress without breaking the build Run all @wip tests with rake cucumber:wip @selenium @inspect allows you to view the application at each step Next Time… In Part 2 of Making the Most of BDD we’ll look at BDD as documentation, how it impacts your architecture, and why it’s important to be pragmatic. Want more? Check out the Prezi", "date": "2015-01-22"},
{"website": "Envato", "title": "\n          Styleguide Driven Development\n        ", "author": ["Jordan Lewis"], "link": "http://webuild.envato.com/blog/styleguide-driven-development/", "abstract": "August 13 , 2014 by Jordan Lewis Styleguide Driven Development Styleguide Driven Development (SDD) is a practice that encourages the separation of UX, Design & Frontend from Backend concerns. This is achieved by developing the UI separately in a styleguide. By separating the UI and backend tasks so they don’t rely on each other, it allows teams to iterate fast on prototypes and designs without having to make changes to the backend. With careful planning they should plug-and-play together nicely. SDD isn’t just limited for big teams working on large applications, but the core concepts of developing UI elements separately in a styleguide (living or static) can still benefit a sole developer working on a single page app. SDD in the real world We have recently embraced SDD in the Envato Market ‘Purchase’ team and have spent the last 6 months building a new shopping cart using this approach with great success. SDD evolved slowly out of our team’s growth and the need to visualise complex features and their scenarios at a glance. By introducing such a wide-reaching and sophisticated feature into an already complex application it spurred a change in our development and agile practices. As a frontend developer working on a cross-functional team with 4 backend developers, a UX designer and a product manager, it is easy to become a bottleneck. The UI has many moving parts and strong dependencies on each team member. Living Styleguides Styleguides and pattern libraries do a great job of visualising an application’s framework and static UI library, however when developing data rich applications the ‘views’ (aka partials) are mostly dynamic and require data to bring them alive. In order to render these dynamic views in the styleguide we need to use sample data to trigger the scenarios we want to test. By using real views, we can be confident that the styleguide truly reflects our application. This confidence allowed us to change our approach and develop our views directly in the styleguide as the first step in assembling a new page or adding a feature. Using sample data means that we can iterate quickly on prototypes and designs before requiring the backend to be complete or even exist. Developing the views early helps to expose any gaps in the designs or wireframes. It also helps us figure out what sort of logic we need from the backend. Having it all separated like this offers us the freedom to complete the backend and frontend work asynchronously. Living styleguides are the key to SDD and they also provide a fantastic base for regression testing so that we can avoid breaking our UI when working on new features. Scenario Visualisation User interfaces are often developed for best case scenarios, however they can unknowingly break in a handful of cases when a possible scenario hasn’t been considered or is ‘out of sight, out of mind’. Simple things like the length of the data or how it responds to mobile breakpoints can turn an otherwise well designed UI into an amateurish mess. Having a styleguide that documents all the possible scenarios for a view is a tremendously powerful asset. It directly benefits each team member’s role as well as other members of the business. Historically, testing a view across all of its various states was very manual process and occasionally required database manipulation. By documenting all scenarios in a central location the styleguide essentially becomes a frontend spec and helps identify edge-cases, gaps in the UX and breakages. Below is an early prototype of the user-nav view and its possible scenarios . Bringing the views alive Developing views that can handle sample data required us to change our development practices. Historically our views pulled data directly from our database backed models, which meant that they were not very reusable outside of their primary context. In order to decouple the views from the models, we create a layer of objects responsible for asking our data models for information and converting it into whatever form will make our views happy. In Ruby on Rails there are several ways to do this; the way that we currently use is for each view to have a presenter object sitting behind it. Essentially this makes our views ‘dumb’ as they only care about the values that they require. Removing the bulk of their logic and their direct relationship with our data models makes them reusable in other contexts, similar to the way that javascript templates work. The following is a simplified example of the ‘user-nav’ view and presenters. Styleguide Presenter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class UserNavPresenter def signed_in_cart_with_a_few_items OpenStruct . new ( :user_signed_in? => true , :shopping_cart_enabled? => true , :username => \"michael-jordan\" , :balance => \"$314.15\" , :shopping_cart_count => 2 , :shopping_cart_empty? => false , :sign_in_url => \"#\" , :sign_out_url => \"#\" , :sign_up_url => \"#\" ) end def signed_out_cart_with_a_few_items # etc end end Application Presenter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class UserNavPresenter def user_signed_in? @signed_in_user . present? end def shopping_cart_enabled? feature_on? ( :shopping_cart ) end def username @user . username end def balance @user . balance . format end def shopping_cart_count @cart . entries_count end def shopping_cart_empty? @cart . entries_count == 0 end def sign_in_url routes . sign_in_path end def sign_out_url routes . sign_out_path end def sign_up_url routes . sign_up_path end end Our View (user_nav.html.erb) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 <div> <div> <% if presenter . user_signed_in? %> <span> <%= presenter . username %> </span> <span> <%= presenter . balance %> </span> <% else %> <a href=\" <%= presenter . sign_up_url %> \">Create an Envato Account</a> <% end %> </div> <% if presenter . shopping_cart_enabled? %> <div> <div class=\"cart-summary <%= \"--empty\" if presenter . shopping_cart_empty? %> \"> <span> <%= presenter . shopping_cart_count %> </span> </div> </div> <% end %> <div> <% if presenter . user_signed_in? %> <a href=\" <%= presenter . sign_out_url %> \">Sign Out</a> <% else %> <a href=\" <%= presenter . sign_in_url %> \">Sign In</a> <% end %> </div> </div> This approach does mean we do need to manage two presenters for each view which we add to the  styleguide, however we are currently investigating different methods to decouple our views from the models and are trialling Cells to reduce the need for duplication. Change in agile practices As we experimented with SDD our agile practices evolved to compliment our new approach. Historically we included both frontend and backend tasks in one self contained and estimated user-story, but this often meant that we were creating blockers and bottlenecks for ourselves on day one as often a frontend task couldn’t start until the backend was completed or vice versa. Using SDD we can now split our larger user-stories into smaller more focussed stories with smaller estimates and less dependencies. The benefit of this is that they can be tackled in any order even over the course of several sprints as the resources become available. With good communication the UI and backend will seamlessly come together. An added benefit of this approach we have found is that our Git branches and Pull Requests are much smaller as they don’t combine as much frontend and backend code and as a result makes them easier to get reviewed. Conclusion By developing directly in the styleguide we immediately have to start thinking about the user experience and how we want our view to interface with the backend. While this approach requires changing some development and agile practices, the process directly benefits and compliments the UX, design, frontend and backend stages of development. The added benefit of this approach is that we are documenting our application and all its views, thus bringing light to otherwise hidden or broken scenarios. Any edge cases or gaps in the UI are identified early and the frontend and backend devs can be confident they are developing towards a consistent interface.", "date": "2014-08-13"},
{"website": "Envato", "title": "\n          Formtastic: Creating and Maintaining a Popular Open Source Plugin\n        ", "author": ["Justin French"], "link": "http://webuild.envato.com/blog/formtastic-creating-and-maintaining-a-popular-open-source-plugin/", "abstract": "July 31 , 2014 by Justin French Formtastic: Creating and Maintaining a Popular Open Source Plugin Interview by Natasha Postolovski. Justin French heads up the Design & UX team at Envato . In addition to being a product mastermind, Justin is an accomplished Rails developer, passionate contributor to open source, and frequent wearer of black T-shirts. In the open source community he is best known as the creator of Formtastic , a super popular plugin for the Ruby on Rails framework that helps Rails developers build better forms faster, with less boiler plate code. The repository has more than 4,400 stars in Github, and over 180 contributors. In this post we’ll delve into Justin’s experiences creating Formtastic, from idea to public release. We’ll also learn about what it takes to maintain and grow a popular open source project. NP: Why did you initially decide to create Formtastic? JF: There were a few key things happening at the same time that pushed me towards it. I was working on a new product with a lot of data entry screens — it was a core part of the experience, so I was intensely focused on designing and building these forms every day, thinking about this stuff a lot, and getting frustrated. The built-in form helpers in Rails are powerful, but they’re also quite basic building blocks. It makes for nice neat demos, but when you start building fully-featured and well-designed forms you quickly find yourself adding a lot of structural code. Building forms in Rails was better than any other web framework I’d tried, but writing all that repetitive code got really old, really fast. I started digging into the Rails code and discovered that it was written with this in mind. Rails came with a default form builder, but you could write an alternative form builder that suited your application. None of the existing examples in the wild suited my needs, but I now knew what was possible, and that there was a supported place in Rails. Then Aaron Gustafson gave a talk at Web Directions called Learning to Love Forms . He showed us a way to use semantic mark-up and existing CSS layout techniques to achieve semantically rich, accessible and beautiful forms without resorting to tables or other hacks. This was way back before Bootstrap, Foundation or any other CSS framework, so I also viewed Aaron’s work like a proposal for a standard — an opinion about how forms should be marked-up and styled that didn’t already exist. This was way back before Bootstrap, Foundation or any other CSS framework, so I also viewed Aaron’s work like a proposal for a standard — an opinion about how forms should be marked-up and styled that didn’t already exist. I sat down and designed the syntax I’d like to use to achieve that mark-up inside a Rails app. I wrote a few lines of code, which lead to a few more and a few more — the rest is history. NP: How long did it take to build the initial version? JF: I have no idea! I think I had a really rough proof of concept within a few days, and I think we were using it internally straight away. There was a 5 month gap between Aaron’s talk and the first public commit, so I guess it took a couple of months of occasional work to get it to the point where I was prepared to extract and share it. Looking at the commit logs and tags, things got pretty stable and interesting in October 2009 when we shipped v0.9.0, but it looks like it took a long time to finally ship 1.0 in August 2010. The reality is that so many people were using it in real-world apps we should have shipped 1.0 much much earlier, but this is what happens with side projects! NP: Did it start as a personal project, or did you always plan to release it as a plugin? JF: It did start out as something reusable and portable (like a gem or a plugin), but this was never a personal project. From day one, the code was attached to the realities of a real production application with a real team, real requirements and tight deadlines. Much like Rails itself, Formtastic wasn’t an academic exercise with imagined requirements — it was built alongside and extracted out of real working software. I think this is crucial to the success of a library or framework. Much like Rails itself, Formtastic wasn’t an academic exercise with imagined requirements — it was built alongside and extracted out of real working software. I think this is crucial to the success of a library or framework. By the time I extracted the code to a separate repository and made the first public commit on Github, I knew I’d use it in multiple projects, and I hoped it might be useful for others. We converted it to a gem when Rails started embracing gems for its plugin architecture. NP: Formtastic now has more than 4.4k stars on GitHub. What would you suggest other developers can do to help their open source projects get more attention? JF: Most importantly, I think Formtastic solved a real problem or pain point for a lot of Rails developers — especially those with less experience designing and building forms. Formtastic was opinionated from the beginning, and offered you a great boost in productivity if you had no idea what you were doing, or if you liked the trade-offs and decisions we made for you. For my own projects, I was building new, fully-featured, production-ready forms in minutes instead of hours. It felt great to be shipping features faster and more consistently, so I can only imagine this is what other Formtastic users felt. I also treated and marketed Formtastic like a real product. If you look way back to the README from the first public commit, I was selling benefits instead of features. I focused on what Formtastic meant to Rails developers — how much better it could be for them. I spoke at local Ruby meet-ups, I did some private coaching and consulting to onboard teams, I blogged, I offered help on Twitter and kept focusing on what Formtastic meant to developers — the benefits and even the downsides they would have from using it. The README itself was one of the first things I wrote. I spent a lot of timing thinking about how I would want to build forms and the way I would explain it to my co-workers. There’s a whole movement around README Driven Development these days, but it was novel for me at the time. I also treated and marketed Formtastic like a real product. If you look way back to the README from the first public commit, I was selling benefits instead of features. When we talk about “attention” I guess we’re also talking about adoption a bit as well. For adoption, I think it was important that we didn’t modify Rails’ default behaviour in any way. This made it really easy to try it out in a new or existing application with very little risk or downside. It was also important to be really open to contributions. Quality can drop and you have to make more compromises, but when people are ready and willing to help, you have to let them run with it as much as possible. There’s probably a lot more to it that was out of my control, like luck, timing, being featured in blogs, podcasts, tutorials, etc. NP: In the gem docs, you mention that over 180 developers have contributed to Formtastic. What are your tips for managing such a wide variety of contributors? How do you ensure the process is as smooth as possible? JF: Github’s pull request feature and the “Github Workflow” is the real enabler here — it’s a pretty consistent process for contributing to almost all Github projects, and I just had to make sure developers knew their contributions were welcome, and what we expected in terms of documentation and testing. The big challenge is in curation. Many people are willing to contribute a quick patch, but they can lose energy and momentum quickly if their patches aren’t accepted right away, or if changes are required. Many people are willing to contribute a quick patch, but they can lose energy and momentum quickly if their patches aren’t accepted right away, or if changes are required. I’ve definitely struggled with this, and it’s a tough balance to get right. Holding out for perfect patches often leads to a lot of waste and unfinished work, which obviously isn’t great. On the other hand, accepting everything leads to bad design decisions, a drop in quality, missing documentation, and features that have to be supported for a long time. Well-structured, approachable code helps too — I haven’t looked at the hard numbers, but it feels like we lost a lot of momentum in the middle, but it picked up again after the big rewrite where we focused on maintainability, extensibility and clarity. NP: How has being open-source benefited Formtastic? JF: Without extracting Formtastic to a plugin, the code would never have made it outside the original Rails app. Without an open source public release, the code would still be stuck inside that first company, so I would no longer have access to it. When Rails or Ruby released a major new version that needed us to make changes, the community was there to help and I didn’t need to be the expert on every tiny change. When HTML5 introduced a whole bunch of new form inputs, the community pitched in. There’s no way Formtastic would be as fully featured, well tested or complete today without those. Progress would have been much slower, quality would have dropped and development would have stalled. NP: What would you suggest every developer include with any plugin they plan to publicly release? Start with README Driven Development: showing the syntax, describing how developers would use it, and what the benefits are. Extract it from a real world application that’s proven the need and only accept features driven out of other real-world use cases. Use Semantic Versioning to communicate the significance of the changes to your users and tie-in nicely with gem dependency resolutions. Have some automated testing in place to ensure no bugs are introduced. Use Continuous Integration . With Formtastic, Travis CI runs our suite of 1500+ tests against the 27 combinations of Ruby and Rails versions that we currently support every time a new pull request is proposed or merged. Include instructions for how to contribute - usually just a few lines at the bottom of the README. A big thanks to Justin ( @justinfrench ) for sharing his insights!", "date": "2014-07-31"},
{"website": "Envato", "title": "\n          Learning from pull requests at Envato\n        ", "author": ["Eaden McKee"], "link": "http://webuild.envato.com/blog/learning-from-envato-pull-request-comments/", "abstract": "July 10 , 2014 by Eaden McKee Learning from pull requests at Envato One of the things I really enjoy about working in the Envato Marketplace development team is the opportunity to learn and to teach via pull requests (PRs) and code reviews. Although the main reason we use pull requests is to obtain approval for production deployment, a useful indirect benefit is the transfer of knowledge within the development team. Pull Request Lifecycle In the development team we use GitHub to manage our code and the pull request functionality is a large part of our process. Every change in the codebase is put up for review on GitHub before it is merged into the master codebase and deployed to the web servers. The main reason we use PRs is to get other developers’ approval for the code to go into production. This approval is usually given via a “plus one” (+1). It is up to the developer to decide how many +1s they need before they merge and deploy their code. Generally, the more large and complex the piece, the more +1s are required. How does one get a +1? When a developer comments with a +1, they are saying “I am willing to support this code in production”. It must meet their idea of quality. From the overall architectural decisions made in your code, right down to whitespace formatting - everything is up for comment. This graph is of our PR comments, the thickness of the lines reflect the number of comments from one developer to another’s pull request. However, what it represents is of more importance. It represents knowledge sharing, teaching and learning. We are now approaching pull request number 6000 on our main Marketplace codebase with many comments on each - in effect creating a large database that provides a platform for knowledge transfer. Outlined below are some of the ways we use pull requests comment at Envato to enhance learning and teaching. How we use pull requests to share knowledge At Envato, PRs are not just a request to merge code, they can and do have many more purposes. I’ll outline a few of many below… 1. Explaining the purpose of the code One of the most important parts of a PR is to outline the purpose of the code upfront and any relevant assumptions. This is useful to reviewers of the code as it allows them to pick up any issues related to how the code relates to the bigger picture, rather than mere technicalities with the code itself. 2. Collaborating on our engineering culture and coding guidelines Discussions in comments can form the nucleus of future coding guidelines, such as this suggestion from Warren on a piece of code that contained some meta programming. I would say as a general rule, we should favour explicit, readable solutions over meta-programming - it’s more maintainable over the long term. 3. Asking Questions… Asking questions is a great way to provide feedback. Asking a question like this allows for open discussion. It is a suggestion,\nbut in a question format. Very diplomatic. It is important to be able to\nhandle such detailed and sometimes pedantic feedback, without taking it personally. However, we still have to be pragmatic and ship code. 4. Manual Static Analysis Forgot to close a bracket, didn’t use that variable you assigned? No worries,\n we have Steve. 5. Suggesting alternative ways of doing things Ruby is a fun language to write in, and there are always new tricks to learn. 6. Meta This blog is powered by the Octopress static site generator, with the repository hosted on GitHub - this blog post also went through the pull request process! 7. Having Fun Envato is a fun place to work, and pull requests are no exception. It is not uncommon to come across a GIF or two. 8. Continuous Delivery and Scale PRs and the collaboration, discussion, and validation they allow us are an essential part of our ability to deploy over 10 times a day . Upon deploy that code will be out on the marketplace handling millions of requests a day! :shipit:", "date": "2014-07-10"},
{"website": "Envato", "title": "\n          Envato is hiring! Ninjas! Rock stars! Clerics!\n        ", "author": ["Julian Doherty"], "link": "http://webuild.envato.com/blog/hiring-ninjas-rock-stars-clerics/", "abstract": "April 01 , 2014 by Julian Doherty Envato is hiring! Ninjas! Rock stars! Clerics! Envato is a fast growing company and we’re expanding both our Melbourne, Australia office and our remote team. Check the job listings below or visit our Careers page to learn about working at Envato. We are especially interested in talking to Ninjas, Rock Stars, and Clerics. In addition, we’re also looking for Ruby developers, devops, UX, and more! Go apply! Ninja We’re growing our team and we’re looking for great ninjas who would love the opportunity to stalk and strike fear from the shadows. We’re calling out to ninjas with deep commercial infiltration experience who have built great honour, and like working in cross-functional teams. You’d be primarily doing espionage, sabotage, and infiltration work, but we also expect that you’re a pretty comfortable working on the full stack.  We’d expect you know how do wire work, you value combat and honour, want to move silently like a cat and care about arriving and exiting in secrecy.  That’s what we do. What you’ll be doing Writing clean, well thought out infiltration plans Lead small, functionally oriented projects containing team members with different skills and levels of knowledge such as ninja stars, and crossbows You’ll work on big projects, small maintenance tasks, alone in the dead of night and anything else that our customers love Providing input into stealthily upgrading our infrastructure without being detected by customers We don’t have testers or sysadmins so you’ll be ensuring your work is done with honour and secrecy Essential Requirements Be a nice person Communicate well in person and in writing and with the threat of your shadow Be a curious person committed to continual learning Be able to follow a project through from the original fuzzy brief through rooftop assault and stealthy entry and beyond with minimal detection Be comfortable in a ninja development and deployment environment Preferred Requirements Be a nice person & be funny & don’t kill coworkers (unless honour bound) Have commercial ninja experience Have solid ninja star and sword experience Have experience of other high stealth/secrecy missions Contribute to clan ninja projects Past experience in other martial arts environments Excellent wirework knowledge Rock Star Headquartered here in Melbourne, we’re a completely online company with an ecosystem of sites and services to help people get creative. We run a set of digital marketplaces that includes ThemeForest,the 120th hardest rocking site in the world. On our marketplaces thousands of people clap and sing along in unison. But we only clap on the back beat. Front beat clappers need not apply. About the Position We value people who can pump out a number one rock anthem while recovering from another night of hard living.  As a rock star, you’ll be working in a hands on capacity as part of our team of rock gods to make sure sell out shows bring the house down. You will have woken up in a tour bus forgetting what city you are in, summoned diva-level tantrums over jars of M&Ms , band meltdowns, and stalker fans. You understand that configuring and managing your delicate ego requires a full time entourage.  You need to have a broad knowledge of going to 11 , partying, trashing hotel rooms, publicity scandals, rehab and recovery, and have a deep technical expertise at shredding solos in at least one of these disciplines. You align with the goals of the rock movement in living life to the edge. Responsibilities In a partnership with the rest of the band and the sound crew, ensure the creation, configuration and maintenance of our sound is pure rock and roll Working closely with the fans and groupies to ensure we live fast and die young Help build and manage the monitoring of our live sound systems focusing on staying close to the metal, performance, and making sure your mike comes through louder than everyone else Self diagnose and self medicate performance issues with your choice of single malt and/or prescription or non-prescription substances Communicate clearly with both technical and non-technical members of the team about why you are the most important, have the best hair, and deserve the most adoration Essential Requirements Be a nice person… but only to people who can be useful to you Have an advanced level of rock knowledge Has used musical instruments or vocals in a previous role Have managed to cause a scandal in both public and private performances Know how to manage and maintain your bad-boy/bad-girl rock image Know how to select or build sound monitoring and tools that go to 11 Have implemented number one hits to support a world wide public facing tour handling millions of concert goers Have proven experience scaling stadium concerts or other high volume events Communicate well in person with screaming vocals Be a curious person committed to continual experimentation Be a natural diva who is able to develop intense resentment from your band members at your increasingly ridiculous demands Be on values with Envato’s passionate and supportive culture. Preferred Skills and Experience Previous experience raging against a system designed to be robust in a disaster situation and at The Man in general The ability to provide security and risk assessment headaches for your support crew Has participated in a culture of shared blame between bandmates during studio sessions Contribute to collaboration projects Past experience in other rock environments Cleric Due to phenomenal growth we are in need of a Cleric. This role will assist in curing ailments and healing allies, and helping teams through encounters with your high wisdom and intelligence, while encouraging improvement through XP gain. The Cleric will assist our teams, keeping them moving and leveling, helping them to control undead and navigate toward their quests. So many job ads for code ninjas, wizards, and warriors... but none for clerics.\n\nThis is why your entire startup is going to get eaten. — Evan Goer (@evangoer) March 21, 2014 You will have experience working with, or in dungeon party teams in any capacity. This role will be non-technical, the caretaker of party health and not an active member of any one team. Chaotic good experience is preferred, but not essential, however a support-class attitude and a visible belief within one step of their aligned domain/deity as a way of working life is. We are looking for someone who will be a cleric role model, dogmatically minded, disciplined, a dedicated life-learner. You must have strong divine magic, and encourage resurrection spells, acting as an enabler to team success and never a barrier. Carrying a truly healing mindset, look to instill the same aura in everybody you come in contact with, being genuine in contributing to the development of an inclusive, creative, adaptive party group If these roles sound interesting, we’d love to have a talk to you !", "date": "2014-04-01"},
{"website": "Envato", "title": "\n          Making the Envato Marketplaces Responsive\n        ", "author": ["Jordan Lewis"], "link": "http://webuild.envato.com/blog/making-the-envato-marketplaces-responsive/", "abstract": "February 11 , 2014 by Jordan Lewis Making the Envato Marketplaces Responsive The first Envato Marketplace was started in 2006 (and 7 others have followed since then) and now we’re a good sized online business with a lot of traffic (well over 100m page views per month). Over the same period the mobile revolution has and continues to happen and like many other online businesses we want the user experience of our sites to be far better on mobile devices by making them responsive. While broadly this is not a unique challenge, we do have some nuances that we think make us a little bit different. In addition to a fairly large and evolving code base, some of the key pages on our sites like our item page (the equivalent of a product page on a typical e-commerce site and thus super important for conversion) are based around user generated content and assets. Although we are a while off being fully responsive I’d like to share our journey and some solutions we have developed to progressively convert the Marketplaces to be responsive. Where to begin? “A journey of a thousand miles began with a single step” - Lao Tzu Our first goal was to take one page and make it responsive. We decided to build a new comments dashboard page as there was demand from authors wanting an easy way to manage comments on mobile devices, plus we could limit this new page to beta access. At first this may not sound like a few months worth of work, but in order to make one page responsive it required a complete re-think and re-engineering of our entire front-end. Another important decision we made was to adopt a mobile-first approach. The added complexity of converting a legacy grid-less website to be mobile-first is mind boggling as there is a long period of time that most pages are a hybrid. “Mobile first means designing an online experience for mobile before designing it for the desktop Web — or any other device” uxmatters.com With all this in mind we sat down and identified the all the key tasks required to meet our goal: Build a switch to limit which pages are responsive Select a mobile-first grid framework Serve separate CSS for responsive pages Conditional media-queries to force desktop styles on non-responsive pages Complete re-build header with off-canvas nav for mobile devices Complete re-build of footer Build responsive modules Build and launch first responsive page Responsive switch We prefer to release smaller features in quick cycles, so building a new responsive front-end from the ground up wasn’t an option. So whenever we are updating a page or module we are making sure that we are coding it to be responsive even though we may not enable the responsiveness for a while. The ‘switch’ is actually a very simple variable in each page’s controller which looks a little something like this: 1 2 3 def show @responsive = true end Using this approach we are able to enable the switch based on conditions such as environment, page, feature-flip, user roles, A-B testing etc. Although we can set individual pages to be responsive like the comments dashboard or envatomarketplaces.com , the reality is for UX and browsing consistency the Marketplaces will go responsive by flipping a switch when every page has been converted. Choosing a grid framework Deciding on an appropriate grid framework was our first big decision. We needed a mobile-first grid framework without all the extra bells and whistles like Bootstrap or Zurb Foundation. After reviewing lots of grids we settled on Susy as it best met our needs: Pure grid framework Grids applied with mixins (no grid classes required in HTML) Highly configurable Works with Rails + Compass Written with Sass IE7+ compatibility (legacy fallback is desktop version) Can change grid columns at different breakpoints Actively developed and maintained Conditional CSS Leveraging our responsive switch we serve up different CSS files depending if the page has the responsive switch enabled or not. Until the site goes fully responsive, users will only download the one CSS file in most cases. layouts/application.html.erb 1 2 3 4 5 6 <%= if @responsive %> <link href=\"application-responsive.css\" rel=\"stylesheet\" /> <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"> <% else %> <link href=\"application.css\" rel=\"stylesheet\" /> <% end %> The responsive and non-responsive style sheets are generated from the same Sass files and the only difference is that the responsive version has a variable set in the config to enable media-queries. application-responsive.sass 1 2 $responsive : true @import 'application.sass' Conditional media-queries In order to progressively adopt a mobile-first approach we needed the ability to compile both the desktop style sheets (no media-queries) and the responsive style sheets (with media-queries) from the same Sass files. After a bit of head scratching we achieved this using Susy’s powerful inbuilt helpers. Susy is a fantastic responsive grid framework and it gives you a lot of magic out of the box with the ability for advanced configuration. “Susy provides the power-tools, what you build is up to you.” We rely on Susy’s at-breakpoint Sass mixin to handle all our media-queries and as an added bonus this allows us to take advantage of it’s $ie-fallback feature . Although it’s called $ie-fallback this feature allows us to remove any media-query applied with the at-breakpoint mixin if the <html> has a certain class. This essentially allows us to force render the desktop version of any page on any browser or device. In our case we use our switch to set a class of fixed-layout on the <html> when the responsive switch is enabled or for legacy browsers (< IE9). 1 2 <!--[if lte IE 8]> <html class=\"lt-ie9 fixed-layout\"> <![endif]--> <!--[if gt IE 8]><!--> <html class=\" <%= 'fixed-layout' unless @responsive %> \"> <!--<![endif]--> Example Sass module module.sass 1 2 3 4 5 6 7 8 9 10 11 .module // Mobile default background-color : red // Tablet +at-breakpoint ( $tablet-only ) background-color : green // Desktop +at-breakpoint ( $desktop-and-above ) background-color : blue Output application-responsive.css 1 2 3 4 5 6 7 8 9 . module { background-color : red ; } @ media ( min-width : 569px ) and ( max-width : 1024px ) { . module { background-color : green ; } } @ media ( min-width : 1025px ) { . module { background-color : blue ; } } application.css 1 2 3 4 5 6 7 . module { background-color : red ; } . fixed-layout . module { background-color : green ; } . fixed-layout . module { background-color : blue ; } /* With breakpoints removed the desktop style (background-color: blue) will take precedence */ The drawbacks of this approach is that we still use a couple of media-queries on our desktop version which we want to keep, but this can be overcome by using raw media-queries rather than Susy’s mixins as they will remain untouched. Ben Smithett has done a great job isolating our base Marketplace styles (minus responsive switch) into an open source github repo if you would like to look under the hood. Rebuild header with off-canvas nav The header is by far the most complex module we have in the Marketplaces as by it’s nature it is built to adapt to multiple user types and ten site variations with unique navigation scenarios. Rebuilding the header was our biggest challenge and therefore what we focussed most of our attention on to get right. We purposely built the new header to look identical to the existing version, and since then we have been continually refining the design as part of a bigger site-wide visual refresh. It’s very obvious there is a lot of navigation going on in the header and we knew pretty much straight away that we needed to implement an off-canvas nav that would allow easy navigation on a mobile device. We first prototyped an off-canvas nav in a Middleman app before bringing it across into the main Rails application. Whilst prototyping the off-canvas nav we ran into many browser edge-cases related to animation issues and in the end decided to opt for no sliding transitions. We initially tried to reflow the navigation for the off-canvas version, but this proved to be an unmaintainable nightmare which would hinder any future updates. So on responsive pages we decided that outputting the navigation twice in different locations would be a good trade-off. We then use basic Javascript to toggle the classes for the active nav and let CSS do all the positioning and styling work. Also currently in development we are trialling the use of the off-canvas nav to replace modals at certain breakpoints. This allows us to overcome all issues relating to modals on mobile devices and not to mention is much better for the user experience. I will go into more depth on our solution for conditional modals in another blog post soon. Rebuild the footer The footer was relatively straightforward to convert and because of this we ended up leaving it until last as we were able to do everything on our list without it. Like the header, the mobile version is a completely new design and only the most important information is visible. Thanks to our modular approach we are able to serve a slightly different version for envatomarketplaces.com . Responsive modules A responsive module is coded to work anywhere and doesn’t care about or inherit styles from it’s parent container. At most, the parent container controls the position and dimensions of where the module will be used. The header and footer are examples of some very complex responsive modules and in fact most of our site is broken up into much simpler self-contained modules which are designed to be used anywhere and adapt to any breakpoint. A great example of a responsive module is the pagination module. On desktop there is room to display the first, last, next, previous and surrounding page numbers, however as we are restricted for space on smaller screens we just display the next, previous and a summary. This module is used on many pages but is currently only responsive on pages that contain the responsive switch, such as the comments dashboard. Putting it all together After re-engineering our front-end codebase and then creating all the responsive elements we needed we were finally able to assemble the first fully responsive page on the Marketplaces. All along the way we did extensive browser and device testing which I wrote a whole article on - ‘ Techniques for mobile and responsive cross-browser testing ’. Once we were ready we invited a group of authors to beta test the new comments dashboard page and provide feedback. We incorporated much of this feedback to improve the page and mobile experience. Along our journey there were many other mobile best-practices which we also took the opportunity to address: Icon fonts Hi-res logos for retina SVGs Living style-guide Reduced background sprites usage Ongoing Challenges One of the biggest challenges we still face in delivering an optimised responsive experience is how we present the generated user content. Most authors have image-heavy item and portfolio pages and a lot of these images are text-heavy and when resized to fit in a mobile view-port the text becomes illegible. We have a couple of ideas on how we tackle this, but we’re still working through this difficult problem. Conclusion Currently the Envato Marketplaces’ are 7 years old, and as we work on new and existing features we are taking every opportunity to bring the front-end code inline with our modern coding practices . In most cases this is a complete rebuild of a module’s HTML and CSS. While we are still a while off making every page responsive the hard work going towards this goal is having massive benefits straight away. As we progressively update, modularize and responsify our front-end code we are freeing ourselves from any legacy front-end constraints, enabling us to ship new features and design changes much quicker, and eventually a mobile friendly Envato Marketplaces.", "date": "2014-02-11"},
{"website": "Envato", "title": "\n          10 Deploys a Day - A Case Study of Continuous Delivery at Envato\n        ", "author": ["John Viner"], "link": "http://webuild.envato.com/blog/10-deploys-a-day-a-case-study-of-continuous-delivery-at-envato/", "abstract": "January 03 , 2014 by John Viner 10 Deploys a Day - A Case Study of Continuous Delivery at Envato I recently presented to the Bankwest Solutions Delivery Group on the processes and technology we use that allows us to deploy our sites up to 10 times a day. 10 Deploys a Day - A Case Study of Continuous Delivery at Envato from John Viner", "date": "2014-01-03"},
{"website": "Envato", "title": "\n          How Not To Fail When Presenting At An International Developer Conference\n        ", "author": ["Alan Skorkin"], "link": "http://webuild.envato.com/blog/how-not-to-fail-when-presenting-at-an-international-developer-conference/", "abstract": "September 13 , 2013 by Alan Skorkin How Not To Fail When Presenting At An International Developer Conference A couple of months ago I presented at LoneStarRuby 2013 in Austin, Texas. While I’ve done a few presentations for work and locally in\nthe past, I’ve never presented at a proper developer conference before. I knew\nfrom experience that the standard of presentations at Ruby conferences will tend\nto be pretty high, so rather than risk being the token crappy presenter\n(that you occasionally see - and feel sorry for), I set out to do a bunch of\nresearch on what makes a decent presentation. Watching videos by such\npresentation luminaries as Ben Orenstein and Scott Hanselman ,\nas well as reading old favourites like Presentation Zen and Confessions of a Public Speaker .\nIn the next few paragraphs, I’ll show you my talk as well as try to distill\neverything I learned and hopefully save some fortunate developers hours of\neffort in the future. First thing first, here is the video of my talk , I’d like to think I did OK\nand all my preparation paid off. The title of my talk was “Why You Should\nLove The Command-Line And Get Rid of Your Rake Tasks Once and For All” and\nit was about the shortcomings of Rake and what we would use instead if we\nwanted to abandon it as a community (here is the full conference program description ). How To Actually Get To Speak At A Conference In my case it was just a fortunate chain of events. LoneStarRuby decided to\ndo things a little differently this year and asked the community to nominate\npeople they’d like to see present at the conference. People would then vote\nfor the nominees and the most requested ones would get invited to speak.\nI was lucky enough to get nominated and then even luckier to get some votes,\nso this opportunity essentially fell in my lap. However getting invited to speak at a regular conferences is actually not\nthat difficult. Most conferences have a ‘Call for Papers’ (e.g. RubyConf Australia 2013 ) a\nfew months before the conference actually runs, people submit descriptions of\ntheir talks and the conference committee then decides who to invite to speak.\nUsually a conference would get a couple of hundred submissions at most,\nsometimes significantly fewer: Wow, 80 talk proposal pull requests for RubyConf AU! Thanks to all who submitted. It's going to be tough to choose 22. — RubyConf Australia (@rubyconf_au) October 31, 2012 So if you have a good talk idea and submit it, you have a reasonable chance of\nbeing accepted. To put things in perspective, a decent software job gets\nseveral hundred applications for one position. Here you have a couple of\nhundred proposals for a couple of dozen spots! So, pick a few conferences and\nsubmit your ideas, the odds are good. Writing A Presentation That Kicks Ass Before you start writing your presentation there are a few things you need to\ndo, a few things you need to get, and a bunch of choices you need to make.\nLet’s look at these one by one. Use a computer you will actually be using when you deliver your\npresentation . Seriously, don’t write the presentation on one machine, and\npresent using another! Use a machine you know really well. It may not seem\nlike much, but if things go wrong, it can be the difference between knowing\nexactly what to do and hunting around (that is the difference between looking\nlike a boss and looking like an idiot). Get a presentation remote straight away and make sure it works well with\nyour set-up, it’s just one less thing to worry about allowing you to focus on\nthe presentation. I use the Logitech Professional Presenter R700 : It’s small, has a nice laser pointer, and works well. Not all features work\nwith a Mac, but it’s definitely good enough. Before you even start to write any slides, write out your presentation in\nlong-form as if you were writing a blog post. Don’t restrict yourself in\nany way here, put down everything you can think of. Do this over the course of\na few days to let your ideas percolate. It will let you organise your thoughts,\nyou’ll have all the text you need for the presenter notes and you can print it\nout for reference when writing your slides. In fact, if you read this document\nslowly enough, slides will tend to just fall out of it, making your job that\nmuch easier. At this point find out how long your presentation should be , this is not\nalways obvious. Sometimes they need to be 30 minutes, sometimes 40, 50 or 1\nhour. E-mail the conference organisers if you have to, but find out as early\nas you can. It doesn’t really matter for now, but when you start to practice\nyou need to know what to aim for. I spent hours trying to pick the right Keynote template and the right fonts.\nWe have a whole marketplace for this kind of stuff at Envato. I got it down to a shortlist of over a dozen templates which is when\nI realised that my time is probably better spent focusing on my content\nrather than comparing the merits of slide templates. So I ended up going with generic slides, white background, black text, and Helvetica for the font . You’re not going\nto wow anyone with your design acumen, but you also won’t make anyone puke too\nmuch. And it’s totally liberating not having to worry about this stuff, focus\non writing your presentation instead. And if this doesn’t convince you,\nremember that it’s all about high contrast - black on white is perfect in\nthat regard. The projector is FAR crappier than your nice retina screen.\nSubtle colour differences are lost, and what stands out on an LCD is often\nnot visible at all on a projector. And while you’re at it, find out what resolution the projectors at the\nconference venue are (once again you may need to e-mail the organisers),\nuse a slide template that matches the projector resolution or it will\ndefinitely look bad. Presenting Without Making People Cringe There are no shortcuts for this, it all comes down to practice. I don’t mean\nexperience, I mean practicing this particular presentation that you have just\nwritten. When you finish the first draft of your slides, start running through\nit, out loud, in front of a mirror. The first 2-3 times you can use to tinker\nwith your slides, remove things that don’t work, make sure the whole thing\nflows well. When you start, your talk will seem way longer than it should be,\nbut don’t prune anything just yet. The first time I ran through my presentation\nit took me over an hour (aiming for 35 minutes), but I’ve subsequently done it\nin less than 30 minutes. The lesson is, don’t prune anything until you can\nrun through your presentation twice in a similar amount of time . After a few times practicing out loud on your own, you will feel better about\nit, this is when you can present to a couple of friends, or your wife, or your\nmum. It doesn’t matter if they won’t understand it. The idea is to make it a\nlittle awkward for you, so that you’re more confident in a slightly\nuncomfortable situation. The people you’re presenting to are just\na rubber ducky . After this you can present to your co-workers, or a local user group which\nwill let you get some feedback and some more nice practice time. At this point,\nyour slides will be getting close to their final state. Now, you can go back\nto practicing on your own again. The goal here is for you to get as close as\nyou can to memorising your notes . Overall I practiced my talk about 12-15 times before I presented it for real\n(that’s at least 6 hours of pure talk time). The more practice you get under\nyour belt, the harder it will be for any unforeseen circumstance to throw you\noff your game. And if you think that’s too much time, watch the LoneStarRuby 2013 opening keynote by Sandi Metz , there is a moment where she talks about\nresearching that keynote for the last six months - hard to compete with that\nif you try and ‘wing it’. Demos Many people are under the impression that if you’re doing a developer\npresentation you always have to have a demo. I totally disagree with this,\njust about every presentation with a demo that I have ever seen has had\nsomething go wrong with the demo. From minor glitches/syntax errors, to total\nand complete failure. So, unless you’re very confident, have had a lot of\npractice and are extremely good at recovering on your feet, don’t use demos\nduring a conference talk (save it for local user groups where people will\ncut you a bit more slack). Take static screenshots and put them in the slides,\nor if you really want to show a working something, record a video then play it\nand talk people through it. Whatever you do never ever rely on internet\nbeing present , I am yet to go to a developer conference where there were\nno issues with the internet! Fitting Into The Time Frame People often speak faster when they are nervous ,\nso their talk will run short. If you’ve practiced enough, you should have no\ntrouble with this. Going over is another matter entirely, when you know your\nmaterial well, you tend take detours because you know you can always pick up\nwhere you left off. Don’t deviate from your notes, if you want to take a\ndetour or make a joke, put it in your notes . A trick that some people use is to make all your important points upfront\nthen just expand on them one by one subsequently, that way you can stop at\nany time since you’ve already made your main arguments and therefore you\nnever run over. I don’t like this, it makes for an unsatisfactory story that\nends nowhere. A presentation should have a beginning, middle and an end. Fit into your time frame by practicing . General Tips For Succeeding (Or At Least Not Failing) Here are some general tips that will save you some headaches. If you can find one, hook your computer up to a projector with the same\nresolution as the one you’ll be using at the conference. This way you can\nwork out all the connectivity issues so when you connect at the conference\nthings should just work. Your presentation should entertain first, inform second and teach a distant\nthird . It’s very hard to get people to learn anything in 30-60 minutes. But\nyou can make it fun while showing them something interesting. Imagine you’ve\nbeen awake for 48 hours, and you just had lunch and now you have to listen to\nsome guy talk about code when all you want is a siesta ( not too far from\nthe truth ). Will you put this guy to sleep, or perk him up a little bit? If you have serious stage fright, you can take some beta blockers (you’ll need a\nprescription), but don’t do it unless you know how they affect you i.e.\nyou’ve taken them before. I don’t tend to get much stage fright, but I\ncertainly feel much more anxious if I don’t know my material well. So, if you’re prone to stage fright, practice again and again , it will help\nmore than you think. Lastly, know what kind of flyer you are . It turns out when I cross over\nenough time zones in a short period of time, I feel extremely unwell for\nseveral days afterwards. I suspected this would be the case, so I arrived a\nfew days before I was due to speak, which gave me just enough time to recover\n(barely). So if you’re flying a long way to a conference, make sure you give\nyourself enough recovery time (if you’re like me and need it). No matter how\nmuch you’ve practiced, if you’re feeling sick while presenting, it will not\ngo well. I hope these tips help you anticipate anything that can possibly go wrong so\nthat you kick ass next time you present. Image by Reality Side B", "date": "2013-09-13"},
{"website": "Envato", "title": "\n          Ruby on Rails, still scaling at Envato after all these years\n        ", "author": ["John Viner"], "link": "http://webuild.envato.com/blog/rails-still-scaling-at-envato/", "abstract": "August 22 , 2013 by John Viner Ruby on Rails, still scaling at Envato after all these years Last week the Envato Marketplaces reached a milestone: for the first time we handled over 70 million page requests on our Rails stack in one week … that’s 10 million a day! Our Rails app powers seven separate Marketplace sites, the most well-known of which is Themeforest.net and based on this article it ranks in the top 5 Rails sites in the world by traffic. The marketplaces are for buying and selling a wide range of digital items, and we sell an item roughly every 10 seconds. A couple of our sellers–we call them authors–have sold $2,000,000 worth of items and another six have sold over $1,000,000. The marketplace code base has been around for a while. In fact, the first commit was made before Rails 1.0 was released back in 2005. Since then, we’ve lovingly cared and tended to the app, with no wholesale rewrites. This certainly makes for challenging and lengthy major framework upgrades as this post details. The reason we managed to scale Rails consistently is because we’ve been keeping our stack simple and stayed focused on maintaining our back-end performance. Today, our app is running faster than ever before, averaging a 117 millisecond response time across more than 70 million weekly requests. Our front-end performance isn’t too shabby either, as we average an end-user page load time of around 2.5 seconds in the US. Even more important to us than these performance figures, is that we want to be able to manage this traffic while moving fast. One of the ways we measure this by how often we ship new versions of our app to our end users. The week we processed 70 million requests, we also deployed a new version of the app 17 times and over the last year we’ve deployed our app 753 times! How are we doing this? The truth is: because this is the way it’s always been. We’ve been fortunate that there has been no “Dev Ops” divide to bring together or lengthy release cycles to shorten. The development workflow started with small changes deployed multiple times a day and supported by the developers that wrote the code from the day, 5 years ago, that the application was created. The key ingredients are: The developers “build and run” the sites and they participate in a primary on-call rotation, so they are “wearing the beeper”. If they cowboy changes, they take the call when things go wrong. We have no operations team to watch our back. We have a huge and comprehensive automated test suite. We have an engaged, tech savvy and vocal user base. If things go wrong, they tell us and we can revert quickly. We have substantial monitoring tools in place, and the development team keeps an eye on application performance continuously. The number of deploys we do a day is one of the key metrics to gauge the agility of the team. Wrapped up in this number is a whole lot of information about how we go about our work. If our build tooling is flakey or slow, fewer builds leads to fewer merges into master and fewer deploys. If our test suite is not picking up bugs, this leads to failed deploys, which leads to fewer deploys. If our internal agile delivery processes become too heavyweight, this leads to fewer deploys. If our site is not reliable, it leads to fewer deploys as we fear changes will break the site. In lean thinking, the rate of deployments is a classic measurement of flow. When continuous delivery is at the heart of your process—which it is at Envato—this number is a key metric to watch. The development team has nearly tripled in size in the last year, and we’ve kept a close eye on this deployment metrics during that time. As we focus on getting new team members up to speed on our codebase and domain, we’ve managed to maintain that throughput, despite the significant changes to personnel and process: In subsequent posts we’ll discuss our architecture in more detail and the developer workflow that underpins our continuous delivery approach. John Viner\nSoftware Development Manager, Envato Marketplaces", "date": "2013-08-22"},
{"website": "Envato", "title": "\n          Techniques for mobile and responsive cross-browser testing: An Envato case study.\n        ", "author": ["Jordan Lewis"], "link": "http://webuild.envato.com/blog/techniques-for-mobile-and-responsive-cross-browser-testing/", "abstract": "July 30 , 2013 by Jordan Lewis Techniques for mobile and responsive cross-browser testing: An Envato case study. Not so long ago, cross-browser testing meant firing up different versions of Internet Explorer, Chrome, Safari, Firefox and (possibly) Opera on multiple operating systems. Add in the ever-growing multitude of mobile devices now available, and it can be a real challenge developing your site to deliver a consistent experience to all. On the Envato Marketplaces we get over 16 million monthly visits, 6% of which are from mobile devices. We’re currently working on making the sites responsive, so having the right tools and cross-browser testing on a range of mobile devices is incredibly important. I’m going to take you through some of the options I’ve explored while setting up the cross-browser testing suite we use for the marketplaces, and some of the workflows and techniques that make us more efficient. Goals and Challenges When testing mobile and responsive sites on mobile devices there are basic factors to test for. Ask yourself: Does the site look ok? Are there any bugs? Does the user interaction feel good? Is the site optimised for mobile? The ultimate goal is to deliver a consistent experience to users on a variety of devices and browsers. We’re using traffic data and survey results to help narrow down the devices on which we focus our mobile browser support, so that the changes we make assist the bulk of our users. Reviewing your own mobile traffic data and researching device stats usage is a great place to start deciding which devices and browsers to support. When developing the marketplaces we run multiple local development servers which makes normal cross-browser testing complicated. Until recently we used a combination of Pow and xip.io on our development machines to serve the marketplaces to our testing devices and VMs. Since beginning our journey to responsive enlightenment, this testing process has become even slower and more complicated. Due to the nature of the marketplaces, we want to focus on testing the same actions across a range of mobile devices and browsers. This means being signed in simultaneously on each device and simulating different scenarios. Luckily there are some quality tools and techniques available to help alleviate the pain of developing and testing responsive websites, whether you own any testing devices or not. Synchronized Testing Synchronised testing is an efficient way to automatically perform the same action across a variety of browsers and devices simultaneously. Products like Ghostlab and Adobe Edge Inspect CC solve this problem and help speed up testing with the added bonus of being able to remotely inspect and debug each connected browser. Ghostlab Earlier this year I came across Ghostlab and was immediately attracted to the product by their promise that it “synchronizes scrolls, clicks, reloads and form input across all connected clients.” Available for Mac, Ghostlab allows you to connect any device (computer, tablet, smartphone, etc.) as long as you are on the same network. If a device has JavaScript enabled then Ghostlab can run on it. Ghostlab includes the Weinre remote web inspector that lets you inspect the DOM and debug JavaScript on any connected device. To remotely inspect a device it’s as simple and double-clicking the device listed in the sidebar and then clicking ‘debug’. I haven’t had a device yet that Ghostlab couldn’t remotely inspect, however, I did try to use it to inspect desktop versions of Internet Explorer with no luck. Ghostlab has lived up to its promise and is now an invaluable tool in our cross-browser testing toolkit. Not only do we use it for testing on mobile devices, but we also use it for our regular desktop cross-browser testing as well. Adobe Edge Inspect CC A few years ago Adobe released a product called Adobe Shadow which was revolutionary as it was the first that easily allowed you to synchronise and remotely inspect your testing across multiple devices. Since then it has since been rebranded as Adobe Edge Inspect CC and requires a paid subscription to Adobe Creative Cloud for all the features. “Wirelessly pair multiple iOS and Android devices to your computer. With Edge Inspect, you browse in Chrome, and all connected devices will stay in sync.” It is still pretty awesome, however, the downside is that it is limited to Chrome on iOS and Android. It also requires installing software, a browser plugin and apps on each device. Other mentions Adobe Edge Inspect vs Ghostlab by Andrew “Atlanta” Jones Grunt - Synchronised Testing Between Browsers/Devices by Matt Bailey shim by marstall Web-based Cross-browser Testing Web-based services such as BrowserStack and Crossbrowsertesting.com offer the ability to test your site across a massive collection of mobile OS and browser combinations. BrowserStack We have a subscription to BrowserStack and find it really handy for when we want to test devices we don’t have physical access to. Although it’s great to have access to almost every conceivable OS and browser combination, I find it can be very sluggish and hard to test interactions. I usually resort to using it for bug fixing rather than regular website testing. BrowserStack relies on mobile device emulators and is served with Flash, so it can’t be relied on 100% for a native experience. However, the benefit of nearly no installation makes it another great tool in our cross-browser testing toolkit. BrowserStack Automate is a separate service which allows you to automatically run your Selenium test scripts across multiple browsers in parallel. “Automate removes the need for manual testing of common workflows such as registrations and logins, search queries, web-based admin tasks etc” Crossbrowsertesting.com Crossbrowsertesting.com is another fantastic cross-browser testing tool and with over 1,000 combinations of browsers, OS, and plugins it markets itself as having “More features than any competitor.” Other mentions There are a few similar services that offer desktop browsers testing such as Browserling , however, currently they don’t offer mobile browser testing. Browser Developer Tools All modern browsers have really good built-in developer tools and in the last couple of years a few have added mobile specific features. Chrome The Google Chrome DevTools are my go-to inspector tools for developing the marketplaces. An awesome addition has been the ‘override’ settings, which allow Chrome to emulate a mobile device. In DevTools > Settings > Overrides you can currently override the following settings: User Agent Device Metrics Override Geolocation Device Orientation Emulate touch events Emulate CSS media (i.e. handheld, tv etc) Protip: The ‘User Agent’ override is really handy if you want to inspect a website that uses device detection to serve up the mobile site. However, since it’s just mimicking a different browser it will not replicate the associated bugs and inconsistencies. Firefox Firefox has a ‘ Responsive Design Mode ’ which allows you to change the size of the content area without having to change the window size. On the Tools menu, choose Web Developer , and then Responsive Design View . On the keyboard, press ctrl-shift-M on Windows or Linux, or cmd-option-M on Macintosh. This is great as it doesn’t squash the window and the developer tools whilst testing out different viewport sizes and orientations. Remote Debugging As web-developers we have taken for granted the ability to right-click and ‘inspect element’ since Firebug hit the scene in 2006. Developing on mobile devices brings its obvious challenges and debugging would be at the top of the list. Fortunately there are a handful of solutions which solve this problem for us, some easier than others. Using Ghostlab is definitely the easiest way to remotely inspect and debug almost any device and browser. Although it’s possible to use the Weinre remote web inspector on it’s own, it’s probably not worth the headache. Other mentions Using Web Inspector to Debug Mobile Safari (Tuts+) Remote Debugging on Android (Google Developers) Remote Debugging on Firefox for Android (Mozilla Hacks) Viewport Resizers If you want to test or show-off how your website looks and responds at different viewport sizes, there is no shortage of online tools to help. Bookmarklets http://lab.maltewassermann.com/viewport-resizer http://codebomber.com/jquery/resizer http://responsive.victorcoulon.fr/ Interactive resizers http://responsive.is http://screenqueri.es http://bradfrostweb.com/demo/ish http://designmodo.com/responsive-test http://cybercrab.com/screencheck http://responsivepx.com Side-by-side comparison http://mattkersley.com/responsive http://www.studiopress.com/responsive http://responsive.pixeltuner.de http://ami.responsivedesign.is/ Protip: Almost all work with localhost addresses, but if you’re developing a static site then you’ll need to run a server. If you have a copy of Ghostlab then this should be no problem, however another simple way is to startup a basic server. For Mac users open Terminal and to cd into the directory that your files are located in and then run: python -m SimpleHTTPServer You should now be able to access your static site at http://localhost:8000/ Screenshots If you need to rapidly test how your site renders on different devices and browsers then screenshots are a great way to automate testing. BrowserStack In addition to its Live Testing and Automate products, BrowserStack offers another product called BrowserStack Screenshots . Screenshots’ pricing is independent of Live Testing but does includes BrowserStack Responsive , which is another useful responsive screenshot tool. Crossbrowsertesting.com Crossbrowsertesting.com offers automated screenshots across all browsers, operating systems and devices as part of a regular subscription. It seems to offer a few more features than BrowserStack Screenshots and some examples can be seen here . PhantomJS PhantomJS is a* “headless WebKit with JavaScript API”,* which means that it can run web pages and capture screenshots without actually loading a browser. This is great for testing that your website renders correctly in WebKit, but it’s not ideal for detecting device and browser specific issues. Here are a couple of good write ups on using PhantomJS to automate responsive screenshot capturing: Responsive Screenshots With Casper (Tuts+) Testing your responsive design with PhantomJS (Adnane Belmadiaf) Simulators & Emulators If you can’t get your hands on physical devices to test on, then a great option is to test your site on a simulator/emulator. This is what BrowserStack uses and if you are after speed and responsiveness I would use these first. Mobile Emulators & Simulators: The Ultimate Guide Maximiliano Firtman has put together a fantastic write up of all mobile emulators and simulators and is definitely worth bookmarking. http://www.mobilexweb.com/emulators Other mentions Keynote MITE “MITE is a desktop tool that lets you interactively test and verify mobile content by emulating over 2,200 devices and 12,000 device profiles” If you have a spare $5,000 per licence then Keynote MITE Pro looks like a promising tool, however it only runs on Windows. Open Device Labs Open Device Labs (ODLs) are a non-profit community driven collection of devices that are open to developers to visit so that they can physically test their work on a broader range of devices. According to opendevicelab.com there are 66 ODLs across 22 countries with 1,052 devices accessible at the time of writing. For more information about visiting, sponsoring or contributing to a ODL, check out opendevicelab.com and lab-up.org . Related articles Open Device Labs: Why Should We Care? (Smashing Magazine) Envato’s Testing Suite After lots of experimenting my current cross-browser testing setup looks a bit like this: Machine: 27inch iMac (OS X Mountain Lion) Dev browser: Google Chrome and the DevTools Testing browsers: Mac: Latest Chrome, Safari, Firefox & occasionally Opera Windows: Same as Mac but also Internet Explorer 7-10 (VMs thanks to www.modern.ie ) Devices: iPhone 4 (iOS 5.1) iPhone 5 (iOS 6.1) iPad 3rd gen (iOS 6.1) Nokia Lumia 800 (Windows Phone 7.8) Nokia Lumia 920 (Windows Phone 8.0) Samsung Galaxy S4 (Android 4.2) MacBook Pro Retina We are continually growing our collection Synchronised testing: Ghostlab Remote debugging: Ghostlab Edge case browsers: BrowserStack Screenshots: BrowserStack Viewport resizing: Viewport Resizer Conclusion There is no right or wrong way to test a mobile or responsive site. Having good tools is definitely an advantage but there are plenty of free resources available to achieve the same goal. If you’re unable to get your hands on testing devices then using a device simulator/emulator is great for speed. Otherwise a web-based tool like BrowserStack or Crossbrowsertesting.com is fine. Whether you’re testing on physical devices, emulator/simulators, or VMs, I highly recommend using Ghostlab to ease the pain of synchronised testing and remote inspection and debugging. If your goal is to simply review your site at different viewport dimensions rather than as rendered by specific browsers; I recommend using a tool like Viewport Resizer. For taking browser specific screenshots, BrowserStack Screenshots or Crossbrowsertesting.com are both good options. If you’re trying to take screenshots at a range of viewport dimensions, then a solution that uses PhantomJS is ideal. Envato & the Responsive Design Community One of Envato’s key values is to help people learn by sharing our knowledge and engaging with a wide community of designers and coders. I co-founded Be Responsive Melbourne and am hosting the third meetup here in the cool new offices at Envato — come along if you’re local!", "date": "2013-07-30"},
{"website": "Envato", "title": "\n          How to Scale and Maintain Legacy CSS with Sass and SMACSS\n        ", "author": ["Ben Smithett"], "link": "http://webuild.envato.com/blog/how-to-scale-and-maintain-legacy-css-with-sass-and-smacss/", "abstract": "June 26 , 2013 by Ben Smithett How to Scale and Maintain Legacy CSS with Sass and SMACSS We’ve been big fans of SMACSS for a long time, however a\npure SMACSS approach works best with plain old CSS on a brand new project. On\nour marketplace sites , we write style sheets in Sass and\nhave several years’ worth of legacy CSS to deal with. We’ve recently been refining the way we do CSS to make it easier for our\ngrowing team to maintain our style sheets without throwing away our existing\nfront end & starting from scratch. What we’ve ended up with is an approach loosely based on SMACSS that\nstill solves the problems originally tackled by SMACSS & OOCSS , but with a few modifications\nand some ideas cherrypicked from places like BEM and Toadstool . Note: You’ll need to be somewhat familiar with SMACSS for the rest of this\npost to make any sense. Here’s an overview . Our take on SMACSS SMACSS defines five categories of styles: Base Layout Module State Theme Our approach looks more like this: Base Layout Module State Theme Base In newer projects our base styles are just Normalize plus some basic default\nelement styles (colors, typography, margins & padding). Unfortunately, the marketplace CSS comes from a time when aggressive CSS\nresets were cool, as well as\nhaving some unfortunate default typography styles that we pretty much always\noverride. These styles are difficult to change without affecting all the other\nCSS that has been built on top of them over the years. Even with those drawbacks, we can still treat our base styles just like SMACSS\nbase styles. Layout In our approach everything that is not a base style or a global state class\nis a module . SMACSS draws a distinction between major layout components (header, sidebar,\ngrid, etc) and everything else. We’ve found this distinction unnecessary for a\ncouple of reasons: Modules often “lay out” their child components in the same way that major\ncomponents are laid out on the page. Even if we’re 100% certain a component will never be reused, there is no\nbenefit in treating it any differently to reusable components. The line between layouts and modules is too fuzzy for it to be worth keeping layouts\naround as a special category. Module Modules are standalone, reusable components that have no knowledge of their\nparent container. Their only dependencies are the app’s base styles. We can\nsafely delete a module when it’s no longer needed without causing changes\nelsewhere in our CSS. BEM double underscore syntax is used to scope child components to a \nmodule, and we use CSS child selectors liberally to minimise the depth of applicability . BEM double hyphen syntax is used as a modifier to indicate subclasses or, when\nused in combination with the is- keyword, module-specific state classes. Since setting a module’s width, position or margins would require knowledge of\nthe context it appears in, our modules are always either full-width block\nelements or inline elements. Here’s a simple example: modules/_my_module.sass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 .my-module background-color : maroon position : relative > a color : aqua .my-module--important @extend .my-module border : 3 px solid fuchsia .my-module--is-active background-color : red .my-module__close-button position : absolute right : 0 top : 0 When I first started writing modules like this I’d often end up with huge\nmodules with complex class names like .my-module__child-component__grandchild-component--modifier . But aside from position and dimension properties, most child components can be\nextracted out into their own standalone modules. So if you leave the\npositioning up to the parent, we end up with 3 smaller, standalone modules. modules/_my_module.sass 1 2 3 4 5 .my-module // ... .my-module__child-component width : 100 px modules/_child_component.sass 1 2 3 4 5 6 .child-component // ... .child-component__grandchild-component position : absolute top : 10 px modules/_grandchild-component.sass 1 2 3 4 5 .grandchild-component // ... .grandchild-component--modifier // ... example.html 1 2 3 4 5 6 7 8 9 10 11 12 13 < div class = \"my-module\" > < div class = \"my-module__child-component\" > < div class = \"child-component\" > < div class = \"child-component__grandchild-component\" > < div class = \"grandchild-component--modifier\" ></ div > </ div > </ div > </ div > </ div > .grandchild-component and .child-component are now independent of their\nparent containers. It’s up to a module to position containers for its\nchildren. You do end up with a tiny bit more structural HTML, but with the\nbenefit that nested UI components are now completely decoupled from each\nother. State Module-specific state classes are defined in the same file as the module\nitself (see .my-module--is-active above) but we keep global state classes\nseparate, e.g. .is-hidden . Theme We do “theme” each of the eight marketplaces\n(e.g. different color schemes on ThemeForest and GraphicRiver ), \nbut we achieve this by setting variables for site-specific styles in a config file. Some Sass magic to bring it all together Configuration _config.sass is the very first file included in our main application.sass .\nIn it, we assign to global variables all the common values we’ll use. This\nincludes colors, font sizes & families, responsive breakpoints and more. We also include a marketplace-specific config file to set variables for each\nmarketplace’s color scheme. Mixins All of our mixins are kept in their own files in a mixins directory and are\navailable globally. We also import a few vendor mixin libraries, like Compass which we use for spriting and vendor\nprefixing. Grid Even our grid framework is just a module. We’re steering clear of using grid classes like .span-5 in HTML and instead\nusing Susy to keep column-based widths in CSS\nalongside the modules they belong to. modules/_page.sass 1 2 3 4 5 6 7 8 .page // ... .page__sidebar @include span-columns ( 3 , 12 ) .page__content @include span-columns ( 9 omega , 12 ) We wrap everything in a .grid module that’s just a Susy grid\ncontainer . Internet Explorer We’re still supporting IE 7 & 8. We used to use the HTML5 Boilerplate method of targeting those browsers, but that meant we were sending lots of styles\nscoped to .lt-ie8 down to other browsers that would never use them. Now we’re using this trick to\ngenerate standalone application-ie8.css and application-ie7.css style sheets for those browsers. IE-specific styles (like everything else) live right inside the module they\nbelong to: modules/_my_module.sass 1 2 3 4 5 .my-module color : chartreuse @if $ie7 position : relative zoom : 1 Good browsers are served a nice clean application.css without any IE junk,\nwhile old IE users get their own special version instead. What we’re not doing We’re not aiming for readable CSS output. When you write modules like this\nyou can look at the class name and go straight to the correct Sass file\ninstead of digging around in DevTools trying to figure out where a certain\nstyle is coming from. Also, Source Maps . We’re not aiming to remove every last bit of duplication from our compiled\nCSS. We want to make development as easy as possible without impacting\nperformance. So far including mixins in modules (and thus adding some\nduplicate styles) instead of @extend ing everything in sight hasn’t hurt us\nin terms of raw file size (gzip chews up most of the duplication - we’re\nsending about 39kb of CSS over the wire). Conclusion We started a few months ago by adding a single module file to our somewhat\ndirty style sheets directory. As we add new features & convert existing ones,\nour modules directory is steadily outgrowing what’s left of our old CSS. The main application.sass file for ThemeForest now looks like this: application.sass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Config @import config_global @import config_themeforest // Vendor mixin libraries @import compass @import susy // Our mixins @import mixins/**/* // Old crud. Our base styles, plus everything else that will eventually // be converted into modules. @import old_stuff/**/* // Modules @import modules/**/* // Global state classes @import state We were successfully able to modernise our CSS architecture without throwing\nout our existing front end and starting from scratch. Further reading style is a reference starter project\nfor this approach. Try it out and let us know what you think! If you haven’t read SMACSS, most of it is free to read on smacss.com . Harry Roberts has a whole series of great posts on scalable CSS. Start with this one on BEM syntax and work your way through the archives. Nicolas Gallagher has also done a ton of work in this area. About HTML Semantics & Front End Architecture is a good overview of some of the thoughts that led to our approach.", "date": "2013-06-26"},
{"website": "Envato", "title": "\n          Keith and Mario's Guide to Fast Websites\n        ", "author": ["Mario Visic"], "link": "http://webuild.envato.com/blog/keith-and-marios-guide-to-fast-websites/", "abstract": "March 27 , 2013 by Mario Visic Keith and Mario's Guide to Fast Websites At the first Australian Ruby Conference, Keith Pitt and I were lucky enough to present a talk on Fast Websites.\nWe were joined by over 20 other Envato developers attending the conference, in the sunny city of Melbourne. We wanted to demonstrate all of the performance gains that could be made using a \nreal application, so we created Giftoppr : a site that \nlets users share animated gif images using Dropbox. We also created a website \nbenchmarking tool called WBench , which we \nuse together with other existing tools throughout our presentation. We released \nboth Giftoppr and our WBench tool as open source software. In our presentation we take our slow loading Giftoppr application\n(initially it loaded in around 9 seconds) and set a goal to reduce the loading\ntime to be under 2 seconds. Using a variety of simple techniques we slowly chop\naway at the loading time until we reach our goal. Our presentation focuses primarily on frontend website performance as that’s\nwhere we can make the biggest impact. We go on to explain why frontend\nperformance is important and why it’s usually best to optimise the frontend\nfirst before the backend. A copy of our slides are available at Speaker Deck , and you can view some more Ruby Conf Australia presentations online at Vimeo .", "date": "2013-03-27"},
{"website": "Envato", "title": "\n          Happily upgrading Ruby on Rails at production scale\n        ", "author": ["Lucas Parry"], "link": "http://webuild.envato.com/blog/upgrading-ruby-on-rails-at-production-scale/", "abstract": "March 19 , 2013 by Lucas Parry Happily upgrading Ruby on Rails at production scale The Envato marketplace sites recently upgraded from Rails 2.3 to Rails 3.2. We \ndid this incrementally at full production scale, while handling 8000 requests \nper minute, with no outages or problems. The techniques we’ve developed\neven let us seamlessly and safely experiment with mixing Rails 4 servers into \nour production stack in the near future. We wanted to be able to confidently make the huge version jump without having\nto do an all-or-nothing cutover. In order to achieve this we made a number of\nmodifications that allowed us to run Rails 3.2 servers side-by-side on our load\nbalancer with all of our 2.3 servers. This let us build confidence in our\nupgrade to the new version gradually with far lower risk of our users receiving\na bad experience. We’ve released the patches we used as a gem rails_4_session_flash_backport ( github ) that \nmagically lets Rails 2, 3, and 4 servers live happily side by side. If you are still stuck back on a Rails 2.3 app, this should help kick start your\nupgrade progress to Rails 3 (and beyond to 4 if you’re ready). This post will go into the technical details around making this upgrade as\nsmooth as it was. History The Envato Marketplaces are actually quite an old code base, started back in\nFebruary 2006 on Rails 1.0. It has seen it’s fair share of hairy Rails upgrades\nover the years, but the changes in the framework between Rails 2.3 and 3.0 were\npretty much a rewrite.  For a long time it felt like we’d accrued too much\ntechnical debt from previous upgrades to make the jump without giving the code\nbase a lot of love first. We were stuck on 2.3 quite a while, and we let 3.0, 3.1 and a large number of\n3.2 releases slip by. Eventually the pain from being on such an old unsupported\nversion of Rails became enough that we were able to get the approval to pay\nback some of our technical debt and do some longer overdue upgrades. One of the things we did to build up our own confidence, and the confidence of\nthe business, in our upgrade to Rails 3.2 was to create some patches both to\nRails 2.3 and to Rails 3.2 that would allow us to have requests bounce between\nservers of both versions seamlessly. This let us run a single Rails 3 server\namongst the many other Rails 2 servers for short bursts, so that even if the\nRails 3 server failed catastrophically, it would still only cause a small\npercentage of user-visible failures, and would allow us to test for performance\nand reliability without having to do an all-in switch at the first smoke test. Problem 1 - Changes to SessionHash The SessionHash is where details are stored that let us identify who a user is\nlogged in as, so we can determine what parts of the site they can access, show\nthem their own profile settings, etc. In Rails 2, the SessionHash\n( source )\nwas much closer to a plain old hash, meaning if you store something in session[:foo] and try to access it via session[\"foo\"] , you’d get nil . On\nRails 3, the SessionHash (provided by Rack, source )\nstarted acting like a HashWithIndifferentAccess so both session[\"foo\"] and session[:foo] would return the same thing. This meant that taking a session from Rails 2 to 3 worked correctly, the newer\nversion of Rails didn’t care if we stored the session_id as a string or a\nsymbol, it could happily find it and we’d end up with the same session_id we\nhad on Rails 2. The problem was that once a session goes through Rails 3, all of the keys to\nthe SessionHash are then stored as strings, meaning if we take the session back\nto Rails 2 and it looks for session[:session_id] , we get nil and a new session_id is generated, ignoring the old session_id and logging out the user. This is obviously not desirable as it means any user who hits our Rails 3 test\nserver is very likely going to be logged out on their next request.  This is\nbecause our load balancer is not configured to do “sticky sessions” where a\nuser’s requests will always hit the same server, and consequently the next\nrequest will probably hit a Rails 2 server. To deal with this, we wrote a patch for ActionController::Session on Rails\n2.3 which approximated the behaviour of Rails 3 closely enough that requests could\nbounce back and forth between versions and the user remained logged in with\nthe same session_id . The basic idea is that we store everything in the SessionHash as strings, and\nthen when looking things up we seek using a string key first, fall back to\nseeking with a symbol key if it’s not found (ie. If it’s a SessionHash from a\nvanilla Rails 2.3 server). Problem 2 - Changes to How Flash Messages are Stored In Rails, there is the concept of a flash, which is a short term storage place for\nputting messages/errors/etc that will be displayed for a user on their next\npage view. An example of such is when you edit your profile and it\nsuccessfully saves, a message is passed on to the next page via the flash,\nwhich is then displayed to say everything went according to plan. The class used to do this is marshalled into a binary format in the session,\nand then unmarshalled on the next request. The problem that occurred here was\nthat the class used for flashes completely changed between Rails 2 and 3,\nmeaning that if you attempted to unmarshall a Rails 2 session with a flash\nobject stored in it on a Rails 3 server (or vice versa), the request would blow\nup with a ActionDispatch::Session::SessionRestoreError, complaining that an\nobject of a class was found, but the class isn’t defined anywhere. In Rails 4 pre-release builds, this practice has stopped and the flash is now\nstored in the session as a basic Ruby hash, meaning it’ll happily unmarshall\non any version of Rails, even if the version of Rails doesn’t know how to get\nthe flash message out from that data structure. We think this is a much better approach, and thus we back-ported this new\nmethod of flash serialization back to both Rails\n2.3 and Rails\n3.2 We also got some hacks working that let us unmarshall the missing flash class\non each Rails version even without a full class definition, meaning we could\nuse our knowledge of how things were stored internally in those objects and\n#instance_variable_get to pull out the messages and bring them into the new\nformat. With these patches in place on both our Rails 2 servers and our Rails 3 test\nserver, it was possible for a user to bounce between servers of different\nversions without being logged out, and without seeing error pages because the\ncurrent server couldn’t understand the flash message from the previous server.\nIt would theoretically even be possible to add a Rails 4 box to the pool and\nhave the session be happily understood on any server in the pool. Problem 3 - Consistent URLs Between Versions The final hurdle in running Rails 2 and 3 concurrently in production was making\nsure all our URLs remained the same between Rails 2 and 3. The format for the\nroutes file completely changed between these versions and we took this as an\nopportunity to kill off a lot of overly-permissive routes that let through too\nmany HTTP verbs, many dating back to a time before Rails even spoke REST. The initial work on this involved lots of manual testing to ensure URLs and form\nactions were still matching up. Once we had fairly high confidence that most\nthings aligned, we developed some time charts in our log aggregation tool Splunk which allowed us to see when a request came\nthrough and was unable to be routed to a particular controller. We always see a\nlevel of background noise as we gets LOTS of requests for random php files,\netc.  Some of these requests are obviously malicious, some just innocently bad\nURLs, but by graphing them on a time chart we are able to determine what is\nnormal background noise and what are new routing problems caused by Rails 3. Rollout With these patches in place and the Splunk charts at our disposal, we were in a\nvery safe position to silently start serving requests on Rails 3 in short\nbursts, and get an even better level of confidence that all our URLs matches up\ncorrectly. Initially we added one Rails 3 server to the load balancer for a 1\nminute smoke test, which revealed very few problems, just a few unusual routes\nwe’d missed surrounding the API.  These were fixed and we did progressively\nlonger tests, each time fixing any problems that were revealed, and eventually\nwe could have a Rails 3 server in rotation for 30+ minutes with no obvious\nchange in error rate. Conclusion The extra work involved in getting things into a position where we could run a\nRails 2 and Rails 3 version of our app concurrently was definitely worthwhile.\nIt allowed us to detect more problems without the site appearing broken to all\nusers than we otherwise could have. This gave us a huge degree of confidence that\nthe final cutover would go smoothly and it allowed us to properly assess the\nperformance of our Rails 3 app with production levels of traffic without having\nto “bet the farm” so to speak. I ended up being the on-call person for the first night after the full\ncut-over, but because of all the work we’d put in beforehand to make sure\nthings went smoothly, the night was so quiet that I started to worry if my\nphone was actually working. You would expect with an upgrade this big that even when you think of everything\nthere’ll still be something that slips through, but the things that did were\ndecided minor enough that I got an uninterrupted nights sleep after what was\nprobably our most high-risk upgrade to date.", "date": "2013-03-19"},
{"website": "Envato", "title": "\n          Rails 3.2.10 Exploit and Slow Read Attacks\n        ", "author": ["Jack \"chendo\" Chen"], "link": "http://webuild.envato.com/blog/rails-3-dot-2-10-exploit-and-slow-read-attacks/", "abstract": "February 07 , 2013 by Jack \"chendo\" Chen Rails 3.2.10 Exploit and Slow Read Attacks Charlie Somerville and I presented a talk at the Melbourne RORO (Ruby on Rails Oceania) meetup regarding the recent Rails 3.2.10 security hole , as well as the Slow HTTP Read Attack and how it affects certain Rails stacks. We decided to experiment with the structure of the talk by weaving in a narrative. We called it “chendo’s 11”, parodying Ocean’s Eleven . The story follows us planning and executing a revenge heist against a fictitious illegal gambling website called “Casino King On Line”. chendo’s 11 The slow read portion of the talk was based on our experience of encountering the slow read attack in the wild in late 2011 — although the bit about it being used for a distraction as part of the heist isn’t quite true to life.", "date": "2013-02-07"},
{"website": "Envato", "title": "\n          How Slow Searches Killed the Marketplaces (and How We Fixed it)\n        ", "author": ["Lucas Parry"], "link": "http://webuild.envato.com/blog/how-slow-search-killed-the-marketplaces/", "abstract": "January 31 , 2013 by Lucas Parry How Slow Searches Killed the Marketplaces (and How We Fixed it) On Thursday the 10th of January at around 3:25am AEDT (UTC+11) the Envato Marketplaces began\nsuffering a number of failures. These failures caused searches to stop working,\nbuy now purchases to be charged but not recorded, newly submitted items to not be\nprocessed, our review queues to be blocked by error pages and the site to be\ngenerally unstable. We’d like to take the time to explain what happened, why so many seemingly\nunrelated areas of the app failed simultaneously and the measures we’ve put in\nplace to try to prevent similar problems from occurring in the future. Initial Symptoms On Thursday at around 3:00am our Solr server\n(the search platform used by most of the Marketplaces) began responding to\nrequests very slowly. This presented itself in a number of different ways. Users performing searches experienced slow response times, and several other features of the site (seemingly unrelated to search) began to have issues: Buy now purchases failed to actually record the purchase and give buyers\ntheir items. Our Delayed Job queues that\nhandle pushing authors’ files from our servers up to Amazon\nS3 and sending out emails began to blow out to huge\nlengths, causing many jobs to not be processed. Reviewers were unable to approve items. Page load time rose to unacceptable levels, since all the app servers ended\nup waiting for the search server to time out before being able to serve other\nrequests. To top it all off, our support system crashed. Initial Response At around 3:00am, the on-call developer for the week was paged by an automated\ntrigger for excessively high CPU usage on our Solr server and immediately\nstarted to look into what was happening. He quickly discovered that search\nresponses were extraordinarily slow, mixed in with many other seemingly\nunrelated failures across the application. His first action was to disable Solr. This made the rest of the site work, but with reduced functionality—we were handling errors returning quickly, but not slow responses. After an hour the second in line on-call developer came online to help. In order to isolate Solr from the the application for diagnosis, they firewalled off Solr traffic. Under these\nconditions the Solr server appeared to respond to locally generated searches\nwith normal response times but the rest of the app continued to be broken in\nweird and wonderful ways. We realised later that this was due to dropping the\nSolr traffic instead of rejecting it, which led to the same behaviour as slow\nresponses. Eventually, we shut down the Solr server completely which solved the response\ntimes, the review queues and allowed new buy now purchases to complete\ncorrectly. This was possible because our code that communicates with Solr gives up right\naway if the server is offline. What we hadn’t accounted for was slow\nresponses. The problem now was that this left us with most of the Marketplaces with no\nsearch, our Delayed Job queues were still growing and support was still broken. Another team in the business maintain our support server, and they were separately\nalerted of it being down. They found it in a frozen state, probably caused by\nall of the extra load from new support requests due to the other failures. Their\non-call developer forced the machine to reboot, which brought it back up but its\ndatabase came back online corrupted. A reload of the latest database backup was\ndone, bringing it back online and allowing people to tell us that everything\nelse was broken. At about 8AM developers started arriving in the office, allowing our pair of\non-call developers to hand over to some fresh minds and recuperate with some\nwell deserved sleep. They wrote up a gist containing the night’s IM conversation history, a list of known problems and what had been figured out to\ndate. That made it much simpler to bring each developer up to speed as they\narrived in the office. We discussed the situation and organised to have pairs of developers\nworking to concurrently diagnose and fix the many problems we were seeing. Buy Now Purchase Problems The investigation into our buy now purchase problems discovered that inside the\ndatabase transaction, just after the system completed a buy now purchase and\nacknowledged it with PayPal, it incremented the sales count on the item. This seemingly innocent action triggered a callback to re-index the item in\nSolr. Solr responded very slowly, slower than our maximum allowed time for a web\nresponse, causing the request to be dropped and the database transaction to be\nrolled back. The immediate fix for this was to push the Solr re-indexing into Delayed Job so\nthat it no longer mattered how long indexing took, the purchases would\ncomplete quickly. The guys working on this problem then devised a way to find the buy now\npurchases affected by this, re-verify with PayPal that they had made the charge,\nand complete the purchase at our end giving buyers access to their items. Item Processing and Asset Uploads We examined the contents of the Delayed Job queue and it immediately became\nobvious why our asset upload jobs were not completing, and why submission\nprocessing had stopped.  Solr re-index jobs had been queued with default\npriority, which happens to be higher priority than our item submission\nprocessing, S3 uploads etc. The Solr re-index jobs would tie up a worker until the Solr server\nresponded or completely timed out, until eventually every worker was tied up\ntrying to update Solr. This exacerbated the load situation on our Solr server,\nand stopped our Delayed Job queues from carrying out useful work. We fixed this by temporarily lowering the priority on this type of job, so that\nany other type of job was worked off before workers started getting blocked\ntrying to update Solr. Review Queue Errors Finally, we examined the review queues and noticed that on approval of an item,\nits de-normalized representation is updated or created. This triggers a\ncallback to index the item in Solr in-request, which again was taking so long\nthat the request would time out and the reviewer would get an error page. Root Cause The final piece of the puzzle, why the Solr server was slowing down so\ndramatically, wasn’t solved in such a satisfying manner. Load looked normal and\nindividual requests to the machine were responded to normal times. After poking around with it for the better part of the day, and still unable to explain\nwhy it was behaving the way it was, we decided to reboot the server “just in case”.\nAfter the reboot, we let traffic from a single web server through to Solr and\nfound that performance seemed normal again, so we slowly ramped up how many web\nservers we allowed through to Solr until it was again serving full load with\nnormal response times. Thus proving you should always try turning it off and on again. Various theories floated around the office as to the root cause; was it the leap\nsecond from 7 months earlier,\nsuddenly causing problems on a machine that was never rebooted? Had we crossed\nsome invisible point above which load becomes unstable? Was it caused by cosmic\nrays ? We’re sad to say we’re still not sure. Unexpected failures can and do happen no\nmatter how much engineering you do. This event highlights our need to do better\nat handling those failures. Preventing it from Happening Again The take-home lesson for us is that we need to engineer the Marketplaces to be\nbetter isolated from the services they depend on, even the ones that are on our\ninternal network and run by us. Solr may slow down on us again in the future (and\nwe’ll be much quicker to try rebooting the server if it does), but it\nshouldn’t affect the Marketplaces. This incident really highlighted how a slow response from a service can be\nworse than no response at all. There’s some great sections on why this is and\nhow to defend against it in “Release it!” by Michael\nNygard , specifically chapters 4.9 , 5.1 and 5.2 . We’ve since taken the time to prevent a repeat situation by wrapping the calls\nto modify Solr’s index with much shorter timeouts, allowing web requests to\nfinish in a timely manner even if Solr is misbehaving, and circuit\nbreakers so we\ndon’t waste time making calls to Solr when it’s already responding too slowly\nto be useful. We’re taking this lesson and applying it to all of our services, to hopefully\nprevent any similar failures in the future.", "date": "2013-01-31"},
{"website": "Envato", "title": "\n          Using Stats To Not Break Search\n        ", "author": ["Julian Doherty"], "link": "http://webuild.envato.com/blog/using-stats-to-not-break-search/", "abstract": "November 28 , 2012 by Julian Doherty Using Stats To Not Break Search How do you change around pretty much everything in your search backend, but still remain confident that nothing has broken? (at least not in a way you didn’t expect). We can use statistics to do that. In particular, a technique called Spearman’s rank correlation coefficient . Let’s have a look at it, and see how we can use it to compare search results before and after a change to make sure relevancy rankings haven’t gotten screwed up in the process. Solr -> elasticsearch We’ve been in the process of moving our site search engine from Solr to elasticsearch . There are many nice new features we can build on the marketplace sites using elasticsearch, and it’s a lot faster to index and query, with lower resource requirements. Our guinea pig was search on Envato’s marketplace site 3docean.net . Initial results were promising. Items turned up where expected, our ad-hoc random clicking looked good, and chendo was raving about how awesome it was. But it was very unscientific. We were just holding up a finger in the air, and saying “yeah, seems about right”. I’m no expert on search relevancy, and I don’t know if what we had before was perfect, but it worked, and people were used to it. So we needed to make sure that we hadn’t rocked the boat too much. At the very least, we wanted to make sure that when you search for cute pig you don’t end up with Lego Kratos (as awesome as that might be), or vice-versa. I will do science to it - Spearman’s Coefficient. We need a way to calculate, in bulk, how differently the same items are ranked between the two search backends. Given a search term, and a list of results from Solr and a matching list from elasticsearch, how have items moved around between the two? Ideally items won’t move much at all, but if they do, we want some metric that will tell us about it so we know where to focus our attention. We can use a statistical model to help figure this out. In particular, “Spearman’s rank correlation coefficient” works well for this task. Here’s what Wikipedia has this to say about it: In statistics, Spearman’s rank correlation coefficient or Spearman’s rho, named after Charles Spearman and often denoted by the Greek letter rho, is a non-parametric measure of statistical dependence between two variables. It assesses how well the relationship between two variables can be described using a monotonic function. If there are no repeated data values, a perfect Spearman correlation of +1 or −1 occurs when each of the variables is a perfect monotone function of the other. Spearman’s coefficient can be used when both dependent (outcome; response) variable and independent (predictor) variable are ordinal numeric, or when one variable is an ordinal numeric and the other is a continuous variable. However, it can also be appropriate to use Spearman’s correlation when both variables are continuous. Which is just a complicated way of saying “give me a number between 1 and -1 to indicate if two lists of items are ordered the same, in reverse, or not in the same order at all” . Spearman’s coefficient is expressed mathematically as: Where x i and y i are the rank of item i in samples X and Y respectively, and x̅ and y̅ are the mean of all ranks of items in X and Y . Show me the code Being coders, it makes more sense to us like this: spearman.rb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # xs & ys are arrays of results in order of rank def spearman ( xs , ys ) # remove items that aren't common to both result sets # these are mostly outliers anyway which we want to ignore xs_common = xs . select { | i | ys . include? i } ys_common = ys . select { | i | xs . include? i } # calculate the mean of each set of ranks (simple sum/length calculation) # as both are just the sum of ranks [1,2,3,4...] and have same length, # we can figure it out based on an arithmetic sum total = 0 . 5 * xs_common . length * ( xs_common . length + 1 ) x_mean = y_mean = total / xs_common . length # initialize totals that we'll need sum_mean_diff = 0 sum_x_mean_diff_sq = 0 sum_y_mean_diff_sq = 0 # sum the differences of the items xs_common . each_with_index do | x , x_rank | x_rank = x_rank + 1 # ranking is 1-based, not 0-based # grab the corresponding item from the other set of ranked items y_rank = ys_common . index ( x ) + 1 # work out the error of each item from it's mean x_mean_diff = x_rank - x_mean y_mean_diff = y_rank - y_mean # aggregate totals for final calc sum_mean_diff += x_mean_diff * y_mean_diff sum_x_mean_diff_sq += x_mean_diff ** 2 sum_y_mean_diff_sq += y_mean_diff ** 2 end # final coefficient sum_mean_diff / Math . sqrt ( sum_x_mean_diff_sq * sum_y_mean_diff_sq ) end And we can drive it with some super simple test data spearman_example.rb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 require 'spearman' ranks1 = [ :a , :b , :c , :d , :e ] ranks2 = [ :a , :b , :c , :d , :e ] puts spearman ( ranks1 , ranks2 ) # = 1.0 (in exactly the same order) ranks1 = [ :a , :b , :c , :d , :e ] ranks2 = [ :e , :d , :c , :b , :a ] puts spearman ( ranks1 , ranks2 ) # = -1.0 (in exactly reverse order) ranks1 = [ :a , :b , :c , :d , :e ] ranks2 = [ :b , :a , :c , :d , :e ] puts spearman ( ranks1 , ranks2 ) # = 0.9 (a & b are out of order) ranks1 = [ :a , :b , :c , :d , :e ] ranks2 = [ :b , :d , :c , :a , :e ] puts spearman ( ranks1 , ranks2 ) # = 0.3 (stuff is all over the place) The function will return 1.0 if both lists are ordered exactly the same. -1.0 if they are exactly reversed , and something in between if they are partially ordered one way or the other. Using Spearman’s Coefficient On Search Results Now that we have a way of quantifying differences in order between the same search done against different search engines, can can systematically compare relevancy between our old Solr implementation, and the new elasticsearch implementation. The rough approach is: Find the top 1000 most frequently searched terms Run each of those search terms against each search engine, record the items, and their ranking Given two lists of results for each term, feed them into our spearman function. Each term has a Spearman coefficient ideally close to 1.0 . Investigate any outliers, tweak search settings Repeat till happy Results So how did we go? There were a few interesting patterns that we found. Terms for specific things, where the relevancy actually meant something were the most similar between Solr and elasticsearch. These were terms where there was a range of relevancy with some obvious matches at the top, then gradually trailing off to partial matches that were less relevant. Terms in very broad categories that applied loosely to a large number of items, and/or terms with tight competition of a number of highly relevant items were the ones that moved the most. Lets look at a few examples graphed out. Solr Rank for each item on horizontal axis, elasticsearch on vertical. A straight diagonal line means everything is one-to-one identical to where it was before. The more fuzziness around the line, the more things have moved. “earth” - high Spearman coefficient An example of a term that was virtually unchanged between the two search backends is ‘earth’ . This is a term where there are a number of hits, but the top matches are obviously a better fit than the lower ranked items. The top items are models of an earth globe, then maps, then earth moving equipment, and finally down to less  relevant items like rocks and soil. ‘earth’ has a Spearman coefficient of 0.99 - practically identical (remember, 1.0 indicates an absolute perfect match of rankings). Things are roughly the same order, and when they have moved, it’s only a few places up or down. “low poly” - low coefficient ‘low poly’ is a term with very high competition. There are a lot of items that are tagged with this term. This is a hard one for a search engine to rank based on the term alone. In conjunction with other search terms it’s useful, but by itself there is a too much noise for the search algorithm to contend with. The reason results between the two are so different is that we add a slight random boosting to item indexing. There are a number of reasons for this, but it’s partly to avoid gaming of search results by authors tweaking item tags/descriptions to artificially end up at the top of search results for popular terms. Normally this is drowned out (by design) by the other relevancy calculations. But when there are so many items that all have very similar relevancy, the random boosting wins out and things get shuffled around quite a lot - and that shuffling is different between search engines. “robots” - typical coefficient At the end of the analysis, the mean coefficient for the top search terms was around 0.70 . This means that we’ve kept order pretty well. Some things have moved up, some have moved down, but most things are in roughly the same place. ‘robots’ was pretty typical. It has a coefficient of 0.75 . Some winners, some losers, but mostly unchanged. Conclusion We’re pretty happy with the move. Doing this analysis has given us the confidence that we haven’t royally screwed things up (at least not too much…), and gives us a good starting point for further tweaking and new features built around search. Where items have moved, it’s due to tweaks we made to relevancy boosting make results more useful where we noticed they didn’t make sense. Big differences are due to the “low poly” effect described above where things with similar relevancy move around a lot due to random boosting. And at the end of the day, both engines do things slightly differently internally, which can add differences.", "date": "2012-11-28"},
{"website": "Envato", "title": "\n          Moving the Marketplaces to Elasticsearch\n        ", "author": ["Jack \"chendo\" Chen"], "link": "http://webuild.envato.com/blog/moving-the-marketplaces-to-elasticsearch/", "abstract": "November 26 , 2012 by Jack \"chendo\" Chen Moving the Marketplaces to Elasticsearch TL;DR: How we got from the top chart to the bottom chart. We have been playing with adding new facets to search on the Marketplaces, but Solr was not making it easy for us due to slow indexing and annoying XML schema files. I experimented with Elasticsearch during a dev iteration and decided it was worth switching. So we did it. And I’m going to tell you how we did it. But first, some background. What is Elasticsearch? Most people seem to think it’s the Amazon search product, but it’s not. Amazon’s search product is called CloudSearch and was released April 2012, whereas the Elasticsearch I’m talking about was first released in February 2010. Like Solr, Elasticsearch is powered by Lucene, written in Java, and is licensed under Apache License 2.0. However, unlike Solr, Elasticsearch was designed to scale from the get-go. Why is it better? As stated above, Elasticsearch was written with scaling in mind rather than being tacked on. We were investigating sharding with Solr and found out that you lose some features like MoreLikeThis support which we are using for PhotoDune’s similar search. The list below is indicative of why I personally think Elasticsearch is better. Super easy to get set up and started It was super easy to set up and get started. For example, on OS X: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ brew update # To ensure latest version $ brew install elasticsearch # Follow launchd steps if you want to start on boot $ elasticsearch # If you aren't running it with launchd already $ sleep 5 # Let's give it some time to spin up $ curl localhost:9200 # Make sure it's working { \"ok\" : true, \"status\" : 200 , \"name\" : \"Baron Brimstone\" , \"version\" : { \"number\" : \"0.19.9\" , \"snapshot_build\" : false } , \"tagline\" : \"You Know, for Search\" } REST API This is one of my favourite features. It’s super easy to interact with Elasticsearch. Let’s add “some_item” with the id of 1 to “some_index”: 1 2 3 4 5 6 7 8 $ curl -XPOST localhost:9200/some_index/some_item/1 -d '{\"foo\": \"bar\"}' { \"ok\" : true, \"_index\" : \"some_index\" , \"_type\" : \"some_item\" , \"_id\" : \"1\" , \"_version\" : 1 } You just take your documents, convert them to JSON and throw it at Elasticsearch. Searching looks like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ curl localhost:9200/some_index/_search -d '{\"query\": {\"term\": {\"foo\": \"bar\"}}}' { \"took\" : 0 , \"timed_out\" : false, \"_shards\" : { \"total\" : 5 , \"successful\" : 5 , \"failed\" : 0 } , \"hits\" : { \"total\" : 1 , \"max_score\" : 0 .30685282, \"hits\" : [ { \"_index\" : \"some_index\" , \"_type\" : \"some_item\" , \"_id\" : \"1\" , \"_score\" : 0 .30685282, \"_source\" : { \"foo\" : \"bar\" } } ] } } Deletes are simply DELETEs. 1 2 3 4 5 6 7 8 9 $ curl -XDELETE localhost:9200/some_index/some_item/1 { \"ok\" : true, \"found\" : true, \"_index\" : \"some_index\" , \"_type\" : \"some_item\" , \"_id\" : \"1\" , \"_version\" : 2 } Of course, you shouldn’t actually use curl to implement searching in any application. Super easy replication and sharding This is probably the main reason why we decided to switch to Elasticsearch. When I was experimenting with it, I was able to get another node to automatically join a cluster by simply running elasticsearch on the same machine (not that this would provide any actual benefits). With the default shard count of 5 and replica count of 1 per index, it immediately started to copy over the data to the new node. Once it was ready, it started to handle indexing and search queries. I decided to kill off the new node I booted up. Elasticsearch shrugged and kept on running. This is while it was indexing and serving queries. Scaling Elasticsearch horizontally is as simple as just spinning up another Elasticsearch instance on another box. With the right config, it’ll automatically join the cluster, get some data and start serving requests. We’ve found that people tend to scale Solr vertically due to replicating being horrendously painful to set up or didn’t work. Replicating with Elasticsearch was ridiculously easy. Solr 4.0-beta can scale and replicate much easier than 3.x, but I’m personally a bit wary of retrofitting something as complicated as scaling and replication. Near real-time One of our major annoyances with Solr was how long newly pushed documents would take to appear in the index. In Solr, you need to perform a commit which creates a new “searcher”, but with empty caches, so performance takes a hit while the caches warm up. We decided on an autoCommit time of 5 minutes, so it would take up to 5 minutes for an updated item to appear in the index, which confused some authors. Elasticsearch has a default refresh interval of one second. That means within a second of pushing a document to an index, it would be searchable. Which is pretty cool. Performance Preliminary testing showed Elasticsearch to be significantly faster than Solr on the same hardware. I’ll be discussing what the performance was like for us further down in the blog post. Scripting functionality Elasticsearch has the ability to let you use scripts from anything from custom score queries to faceting. It defaults to mvel, but there are plugins that let you code in Javascript, Python or Groovy. Real-time GET Elasticsearch has the ability to let you fetch the document you stored in the index by ID without needing to do a refresh. This is useful for using Elasticsearch as a (secondary) data store. Note: Solr 4.0-beta has real-time GET now. Aliases You can add as many aliases as you want to an index in Elasticsearch, and it will resolve the index to the appropriate index(es) so implementing cross-index searches is as easy as adding aliases. We use it to easily swap live indexes without the search layer needing to care which index is live or not. Downsides Unfortunately, perfect software does not exist. There are downsides to Elasticsearch. At the time of writing, Elasticsearch is at 0.19.9, which is pre-1.0. Initially, the lack of a ‘stable’ release was an issue for us, but after talking to a few people who have been using Elasticsearch in production, we decided to go ahead with the switch. The documentation is pretty sparse and lacks good examples, but I think it’s better organised better than Solr’s wiki. The Elasticsearch community is definitely much smaller and there are far fewer resources online compared to Solr, which is probably due to the massive age difference between the two. Solr has been around since 2004 whereas Elasticsearch was first released in 2010. However, the speed that bugs get fixed is pretty astounding. We found two bugs when we were adding it to our stack ( #2045 , #2197 ), and they were fixed within two days. Which is pretty bloody amazing. That said, I don’t know how fast the Solr team fixes bugs. How we did it The first step was getting our data into Elasticsearch. This was fairly easy since all we needed to do was turn our data into a hash and peg it at Elasticsearch. The hardest part was figuring out how we wanted to have our indexes and how many shards we wanted. We settled on having an index per site for easier maintenance and selective scaling, and kept the shard count to five as per defaults. The next step was re-implementing the search functionality we had with Solr. Figuring out what our existing searches mapped to in Elasticsearch took a while, mainly because the documentation was a bit sparse, but Elasticsearch Head is a great tool to quickly browse and query an ES cluster. Once we were able to do an Elasticsearch query for every marketplace, we wanted to verify that it can handle the existing load. We added a shim where it would push every single search with Solr into a Redis queue so workers outside of the web request can replay the same search against Elasticsearch. This worked pretty well, apart from a bug in the shim that actually caused the workers to replay three times the production load. Elasticsearch handled it gallantly. Performance details are below. The next step was improving the search interface and adding new facets for 3DOcean. This was by far the hardest part, and not a Solr or Elasticsearch problem. After some internal testing, we switched over to Elasticsearch on 3DOcean for everyone. Monitoring We tried out Sematext’s SPM for monitoring ES since it was the easiest to set up. However, we were seeing one to two hour delays for the graphs to update and it left logs on the disk which it didn’t clean up, so we ended up ditching it. We added app-level metrics on how long search requests were taking in to NewRelic in the same way that we do for Solr to have a good comparison point. Each metric is broken down by site (AudioJungle, PhotoDune, etc) and type of search (standard search or a similar item search). Performance Take everything below with a grain of salt since this is the performance we get with our dataset. Your mileage may vary. For these tests, we had one Solr instance and one Elasticsearch node. Both the Elasticsearch and Solr boxes are specced the same: 4 cores on a Intel Xeon CPU E5645 @ 2.40GHz 12GB RAM 15GB of SAN-backed storage With Solr, we’ve tuned the caches a bit to ensure not too many evictions happen (field cache tends to not evict for us). We are using an autoCommit time of 5 minutes and 100k documents (which we never reach). An optimize happens every 20 minutes. With Elasticsearch, we are using a refresh interval of 1 second. We haven’t added automated optimizing yet mainly because it seems to perform fine without having to do it all the time, where Solr definitely needs it. We left the cache stuff at defaults. Each index uses the default 5 shards. Query performance The graph above shows the response times of Solr and Elasticsearch of a day’s worth of production search traffic. Each colour is a different site. There is a tiny delay (<5s) between a search happening on Solr and that search being replayed on Elasticsearch. Solr tends to average between 100ms to 190ms between sites. With Elasticsearch, most sites average between 25ms to 45ms, with the 120ms to 190ms for the biggest index. MoreLikeThis performance The graph below shows the MoreLikeThis performance between Solr and Elasticsearch. As you can see, there is a massive difference. Solr tends to average 400ms, with the fastest averaging 60ms. Elasticsearch handles everything in under 100ms and as low as 25ms! Another interesting chart is the CPU usage on each box. The only things running on each box is Solr/Elasticsearch and the Scout agent. Elasticsearch averages between 3-5% CPU but Solr uses 20%-50%! This could be attributed to the optimize calls happening for every data point, but if we look at a smaller timescale… … We see a similar trend. ES is 2-5% for the most part, with a spike to 13%. Solr averages around 25% with a spike to 42%. Indexing performance Indexing performance was significantly improved with Elasticsearch. We started with a request per document to index (very very sub-optimal) and found it was already matching our bulk index strategy for Solr. Once we moved to using _bulk to submit updates in batches of 1000, we were indexing 4x faster than Solr. Parallelisation brought this up to 16x. Given we weren’t parallelising indexing for Solr, this is an unfair comparison, but 4x is still a significant improvement. We use a refresh interval of 60s when we’re doing bulk indexing. Infrastructure We are running two ES data nodes on same-specced boxes as stated above. Each one is configured to use the local gateway for simplicity, and each one can become the master. For accessing the ES data nodes, we had a few options: HAProxy to load balance between the two nodes Implement our own failover code Run a client node instance on every machine that needs to access the data nodes We decided against HAProxy because it would become a single point of failure. Implementing our own failover code would be re-inventing the wheel. Both of these would require managing the list of ES nodes, which was not ideal. Using client nodes means running an instance of Elasticsearch on each machine you would want to use to talk to data nodes, which means losing 100-250MB RAM to Java, but it makes everything much easier. Rather than having to define a list of ES hosts to talk to, you simply talk to the client node on localhost:9200. The client nodes become part of the cluster but don’t store any data or perform searches, but they know which node has what shard, and are notified of nodes going up or down. This means that when one of the data nodes go down, all the clients are notified and will direct queries to another node with minimal interruption. Also, when there are shards that only exist on a particular node, the client will talk to that node directly rather and avoid the potential extra hop. What’s next? The next step is to flip other sites to Elasticsearch one by one until we can ditch Solr. The ultimate goal is to have the browsing aspect of the Marketplaces be backed by Elasticsearch completely with its real time GET so we can take some load off our DB and have the site usable when we do migrations. Conclusion If you’re looking for a straightforward and scaleable search solution but don’t mind the smaller community, give Elasticsearch a go.", "date": "2012-11-26"},
{"website": "Envato", "title": "\n          Welcome!\n        ", "author": ["Envato"], "link": "http://webuild.envato.com/blog/welcome/", "abstract": "November 25 , 2012 by Envato Welcome! We are the people who build the many Envato sites and products. We think we build great products in interesting ways, and this is our place to talk about it. At Envato, we help our community learn and earn online and in that spirit it is important for us to share what we learn. You’ll find out more about the technical choices we make, including the ones that don’t turn out as we hope. We’ll describe our approach to solving problems rather than simply adding features. Along the way, you will meet the team members and find out what makes them tick. Welcome to webuild.envato.com . The Team", "date": "2012-11-25"}
]