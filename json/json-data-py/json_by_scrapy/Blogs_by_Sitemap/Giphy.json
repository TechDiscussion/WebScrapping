[
{"website": "Giphy", "title": "Search @GIPHY: Keeping up with the Zeitgeist", "author": ["GIPHY"], "link": "https://engineering.giphy.com/search-giphy-keeping-up-with-the-zeitgeist/", "abstract": "Search @GIPHY: Keeping up with the Zeitgeist February 12, 2019 by GIPHY February 12, 2019 GIPHY HQ – New York, NY While on the outside GIPHY search may seem like a standard search problem, being on the top of the zeitgeist is an increasingly viral world is a constant challenge. GIPHY has devised ways to make sure their users are getting the Most relevant search via models that allow them to exploit known performant content while experimenting with potentially successful content. This model ensures relevant content is surfaced to the end user while still trying to explore the space to ensure that new and exciting content has a chance to propagate up towards the top of the page. Register for free on the official New York Data Science meetup page! Previous Post Next Post", "date": "2019-02-12"},
{"website": "Giphy", "title": "gRPC Adoption in GIPHY", "author": ["Nima Khoshini"], "link": "https://engineering.giphy.com/grpc-adoption-in-giphy/", "abstract": "gRPC Adoption in GIPHY December 11, 2018 by Nima Khoshini GIPHY’s API servers handle over a million requests a minute. To handle this load, we divide the work between a variety of microservices, each of which fulfills part of the request. For example, one service performs searches, another supplies metadata about GIFs, yet another validates user credentials, and so on. Our first iteration of microservices was initialized as standard REST servers with JSON payloads, but more recently we started to experiment with gRPC. We’ve seen improvements across the board in terms of performance, ease of development and cost. What is gRPC gRPC is an open source Remote Procedure Call (RPC) library that is well supported in numerous languages, such as Python, Java, PHP, etc. It is based on Google’s internal RPC infrastructure, entitled Stubby, but rewritten to be more generic for public usage. Unlike the REST/JSON paradigm, in which a contract between the client and server is defined in the documentation, gRPC enforces strict contracts using an Interface Description Language (IDL). Within each language, gRPC’s protoc binary generates stub code for creating a client or a server, thereby making the integration drop-dead simple. RPC is not a new concept by any means. It has existed for over 20 years in different formats (Corba, Soap, Java RMI, etc). gRPC, however, is perhaps the most elegant and simplest implementation. It also leverages popular technologies under the hood, such as Protobuf for fast object serialization/deserialization, and HTTP/2 for persistent connections. Migrating to Protobuf Protocol Buffers, or Protobuf for short, is Google’s popular object serialization library that serves as the foundation for defining all gRPC objects and services. Some of our services already used Protobuf for incoming request payloads and outgoing response objects, making the transition straightforward. The remaining services had to be retrofitted to use Protobuf as a means of communication within the services. Most of our services are written in Scala; fortunately for us, we were able to leverage the ScalaPB library to facilitate gRPC integration. First, we mapped our REST endpoints into corresponding gRPC services. Along the way, we organized logical groups of functionality (say managing a User’s profile), into distinct services. Then, we implemented the generated server interfaces in our code. For example, suppose we had this service for profile management: service UserProfile { rpc CreateProfile (CreateProfileRequest) returns (UserResponse) {} rpc UpdateProfile (UpdateProfileRequest) returns (UserResponse) {} } Implementing the server-side portion of the generated ScalaPB trait would look something like: // Base trait for an asynchronous client and server. object UserProfile extends UserProfileGrpc.UserProfile { def updateProfile(request: UpdateProfileRequest): Future[UpdateProfileRequest] = { // call update profile code and return a future } def createProfile(request: CreateProfileRequest): Future[CreateProfileRequest] = { // call create profile code and return a future } } Since most of our code was structured around returning a Future , the changes in the existing codebase were trivial. The last step for us was to wire up the client portion of the codebase. Turns out, this is equally a simple to set up as well: val channel = ManagedChannelBuilder.forTarget(\"localhost:5000\").build() val client = UserProfileGrpc.stub(channel) client.createProfile(CreateProfileRequest()) As you can see, it’s only a couple of lines to generate a client and server. The simplicity of integration alone makes it much easier to create and consume a service with gRPC instead of REST/JSON. Deploying to Kubernetes At GIPHY we use Kubernetes for pretty much every user-facing and internal deployment. Integrating gRPC services into Kubernetes was a very easy process at first. However, as we’ll show below, there are several huge differences worth noting between gRPC and REST deployments. Client-Side Load Balancing via GIPHY Anyone that has deployed a gRPC services onto Kubernetes will know that doing proper load balancing is not as straightforward as with a RESTful service. Due to gRPC’s usage of HTTP/2, which maintains long-lived connections, a service will never properly load balance between different pods using the same techniques usually used with a RESTful service. However, there are several other load balancing techniques that work well for gRPC, either at the infrastructural level or code level. We chose to tackle this problem using client-side load balancing for now. There are a couple examples on Github for Java-based services to implement what is known as a NameResolverProvider. We created a NameResolver that watches the Kubernetes APs for changes to a deployment. Watching the Kubernetes API means that when new pods are added or removed, the client almost instantly knows about it and can adjust calls accordingly. The sample code we used as inspiration can be found here. While this approach is nifty, it’s unfortunately language specific. Leveraging a service mesh like Linkerd or an Envoy proxy would go much further in addressing our concerns. If you’re looking for more info on load balancing gRPC with Kubernetes, you can head here . Health Checks via GIPHY One of the problems we faced when deploying these new services to Kubernetes was the lack of the simple health checks we previously had with the REST services. Since gRPC uses Protobuf to communicate, there is no simple way of adding a simple liveness or readiness probe within our Kubernetes deployment to check the health. There are however, some libraries present that formalize and facilitate the creation of health check services. We implemented grpc-health-probe as outlined in this blog post. The integration of health checks required us to: – Update our gRPC servers to implement the generic Health service. – Implement the health checks on the server. The checks are unique to each service. For example, one service was validating results from a SQL call while another was simply performing a DNS host name check. – Update our Kubernetes manifests to make a binary call to grpc_health_probe . Prior to implementing health checks, we would occasionally see spikes in latency for some deployments. We tracked this down to Kubernetes draining a node and thus killing a gRPC pod, which was brought up in another node. After implementing the Health checks, we were happy to see consistent response times and no more latency spikes. Deadlines and Circuit Breakers via GIPHY Timeouts are important for both a client and a server. As a client, you do not want to wait indefinitely to hear back from the server. The gRPC client libraries have a built in notion of deadlines, making timeouts quite simple to enable. In our Scala code, we made sure every client call specified a deadline by using the withDeadlineAfter method. This alleviated client calls waiting around forever. Unfortunately in our testing, enabling a client call deadline did not kill the request on the server and subsequently free up all of its resources. In other words, if a client makes an expensive call to the server (think of a slow DB call), it will give up after its deadline interval is surpassed, but the server may sit around waiting indefinitely until it generates a response. To address this, we introduced some very lightweight circuit-breaking logic to timeout requests after a specified interval. Additionally we specified a backoff interval period to fail requests quickly should the server be under stress. This helped us ensure that in disaster scenarios, in which communication with a shared resource is unable to be resolved, we can reliably fail requests quickly while maintaining consistent levels of CPU and memory usage. There are numerous Circuit Breaker libraries out there; Netflix’s Hystrix is the most popular. The Akka library has a very clean and simple implementation for Scala microservices as well. https://github.com/Netflix/Hystrix https://doc.akka.io/docs/akka/2.5.4/scala/common/circuitbreaker.html Conclusion As you can see, migrating from REST to gRPC is more involved than simply swapping out libraries. However, the migration is worthwhile: we have noticed gRPC deployments using up to 10x less CPU than their REST counterparts. This can be attributed to the use of Protobuf’s across the board and lack of JSON serialization/deserialization. With the savings in CPU usage, our service deployments can handle higher loads with fewer replicas. That, plus not having to write boilerplate integration code in consuming services, makes it a winning strategy. No doubt about it, gRPC makes writing microservices fun, fast, scalable and easy. — Nima Khoshini, Services Team Lead Previous Post Next Post", "date": "2018-12-11"},
{"website": "Giphy", "title": "Python Software Foundation’s Startup Row @ GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/python-software-foundations-startup-row-giphy/", "abstract": "Python Software Foundation’s Startup Row @ GIPHY December 4, 2018 by GIPHY December 4, 2018 GIPHY HQ – New York, NY Join us for an evening mashup of Python and Startups! At GIPHY HQ we’re hosting Python Software Foundation’s Startup Row. At the event, startups will five minutes to pitch and another five minutes of live Q&A from the audience. The winning team will be awarded two conference badges and a free booth in Startup Row at PyCon US 2019 in May. The winning New York startup will join an alumni list that includes companies like Docker, Mixpanel, X.ai, Plotly and Quantopian, among over 100 others. You can find more information and register for the event here! Previous Post Next Post", "date": "2018-12-04"},
{"website": "Giphy", "title": "Queer Tech Meetup @ GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/queer-tech-meetup-giphy/", "abstract": "Queer Tech Meetup @ GIPHY December 6, 2018 by GIPHY December 10, 2018 GIPHY HQ – New York, NY Join us for an exclusive look under the hood at GIPHY! GIPHY Engineering manager Bjorn Roche will provide insight into the microservice architecture and exciting engineering challenges the GIPHY team faces each day. As an engineering manager, Bjorn is responsible for all things career development and engineering culture. He also serves as a product manager for the Content Engineering and Core Services teams. So if you want to learn more about GIPHY Engineering, this is the perfect opportunity! For more information and to register for the event, head over to the official meetup page here ! Previous Post Next Post", "date": "2018-12-06"},
{"website": "Giphy", "title": "How to increase performances on custom UICollectionsViewLayout", "author": ["Giorgia Marenda"], "link": "https://engineering.giphy.com/how-to-increase-performances-on-custom-uicollectionsviewlayout-using-binary-search/", "abstract": "How to increase performances on custom UICollectionsViewLayout November 2, 2018 by Giorgia Marenda GIPHY’s iOS app grid design includes a particular layout called the “Waterfall Layout.”It allows differentsized GIFs and Stickers to fit together in a continuous steam.An issue with this kind of layout is that we need to dynamically calculate the frame-size of every cell because we don’t know what the height of the GIF will be in advance. This could be very inefficient without applying any optimizations, so in this article we’ll describe how to build a waterfall layout  efficientl Collection view layouts are subclasses of the abstract UICollectionViewLayout class, and they serve the purpose of telling the UI CollectionView where the content is displayed. They define the visual attributes of every item in your collection view. The individual attributes are instances of UICollectionViewLayoutAttribute and contain the properties of each item in your collection view, such as the item’s frame or transform.The implementation of a custom layout is made of three core steps: 1. Prepare method. This is called every time the layout is invalidated. This is the time when we calculate our attributes. 2. Cache. While computing the attributes we want to cache them so we don’t need to do it again until the layout is invalidated. As a first optimization, we will maintain an array of UICollectionViewLayoutAttributes . After that, we’ll see how much better we can do with a binary tree. 3. LayoutAttributesForElements(in rect: CGRect). This method is called periodically by the UICollectionView , asking for a set of attributes that match a certain region. So we need to filter the cached array of attributes and return an array that contains all the attributes that correspond to all the items that are going to appear within that rect in our UICollectionView . The green rectangle is the rect representing the screen, so the attributes we need to return are just the ones which need to be displayed (purple rectangles). In other words we want to find all the items that intersect the rect. A simple implementation could be something like: override func layoutAttributesForElements(in rect: \nCGRect) -> [UICollectionViewLayoutAttributes]? {\n  return cache.filter {\n    $0.frame.intersects(rect)\n  }\n} Now this solution works fairly well when the number of items are limited, since the filter method has a linear complexity with the number of elements in the array. The worst case complexity of linear search is O(n). In the GIPHY’s case we potentially have a huge number of GIFs, and the performance degrades as the user scrolls deeper and deeper. At the WWDC18, Apple presented a Tour of UICollectionView , explaining optimization to save a considerable amount of CPU work, simply by using a different algorithm to filter the attributes. We can observe that our cache array of attributes is sorted by nature, since our layout demands every cell next to or below its preceding cell. A much more performant algorithm to search for an item in a sorted array is Binary Search . The worst case complexity of binary search is O(log n). Here a generic Swift implementation takes an array a and returns the index of the first element that matches the criteria described by the compare function: func binarySearch(_ a: [T], where compare: ((T)-> ComparisonResult)) -> Int? {\n    var lowerBound = 0\n    var upperBound = a.count\n    while lowerBound < upperBound { \n        let midIndex = lowerBound + (upperBound - lowerBound) / 2\n        if compare(a[midIndex]) == .orderedSame { \n            return midIndex \n        } else if compare(a[midIndex]) == .orderedAscending { \n            lowerBound = midIndex + 1 \n        } else { \n            upperBound = midIndex \n        } \n    } \n    return nil \n} Now we can use the binary search to find an index that match our criteria: (attribute.frame.intersects(rect) ). let index: Int? = binarySearch(cache) { (attribute) -> ComparisonResult in\n    if attribute.frame.intersects(rect) {\n        return .orderedSame\n    }\n    if attribute.frame.minY > rect.maxY {\n        return .orderedDescending\n    }\n    return .orderedAscending\n} Starting from this index we can start looping forward and backward to find all the other items that match our criteria. for attributes in cache[..<index].reversed() { \n    guard attributes.frame.maxY >= rect.minY else { break }\n    attributesArray.append(attributes)\n}\n\nfor attributes in cache[index...] {\n    guard attribute.frame.minY <= rect.maxY else { break }\n    attributesArray.append(attributes)\n} Now, let’s see how much that helps. We can use some tests to compare the performance using an array of 10,000 sorted elements versus a binary tree. In this case, we’ll search for the last item: func testPerformanceWithoutBinarySearch() {\n    self.measure {\n        _ = sortedArray.filter { $0 == 99999 }\n    }\n} measured [Time, seconds] average: 0.0215 func testPerformanceWithBinarySearch() {\n    self.measure {\n        _ = binarySearch(sortedArray, key: 99999)\n    }\n} measured [Time, seconds] average: 0.00000645 via GIPHY Conclusions On the Apple Performance Tips , the number one tip is reduce your app power consumption. This optimization may not result in a noticeable change in the final product. The scroll experience will be smooth using a linear search as well, especially on newer phones , but reducing the amount of CPU usage will improve the device’s battery life. -Giorgia Marenda, Lead iOS Engineer Previous Post Next Post", "date": "2018-11-02"},
{"website": "Giphy", "title": "GIPHY @ ‘Microservices  & Microwave Dinners’", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-microservices-microwave-dinners/", "abstract": "GIPHY @ ‘Microservices  & Microwave Dinners’ October 22, 2018 by GIPHY October 24, 2018 Splash HQ – New York, NY We’re excited to be part of the Splash ‘Microservices & Microwave Dinners’ meetup, where we’ll have an open discussion on microservices – the nuances, hype, benefits, and more. GIPHY’s Director of Engineering API, Laurence Girard, will be moderating a discussion on “Continuous Health and Monitoring in Microservice Architecture.” We hope to see you there! Register HERE ! Previous Post Next Post", "date": "2018-10-22"},
{"website": "Giphy", "title": "Themed Hacknight: Coding for Gender Equality @ GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/themed-hacknight-coding-for-gender-equality-giphy/", "abstract": "Themed Hacknight: Coding for Gender Equality @ GIPHY November 28, 2018 by GIPHY November 29, 2018 GIPHY HQ – New York, NY GIPHY Engineering is hosting Women Who Code + genEquality’s themed hacknight “Coding for Gender Equality.” This is a general hacknight that will also allow participants who want to work on a genEquality project the opportunity to form a group and start a project for the hackathon. Feel free to participate in the genEquality hackathon or simply come to code on your own projects and ask other members for help with your questions! For more info head over to the official meetup page ! Previous Post Next Post", "date": "2018-11-28"},
{"website": "Giphy", "title": "GIPHY @ HackNY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-hackny/", "abstract": "GIPHY @ HackNY September 25, 2018 by GIPHY September 29 – 30, 2018 NYU – New York, NY GIPHY is a proud sponsor of HackNY’s Fall hackathon! You know the drill, you can find our mentors there handing out GIPHY swag, helping teams with their projects and sharing info on how to use the GIPHY API. See below for information on HackNY’s Hackathon and we hope to see you there! “Every spring and fall, hundreds of students from scores of universities around the country flock to hackNY’s Student Hackathons, where they participate in collaborative and creative coding challenges in a 24-hour coding sprint. The events open with API demos from New York City startups. and then students work in teams to build projects. Students work around the clock, and the event culminates in a presentation before a panel of judges the next day. Our next Student Hackathon will take place this fall at New York University.” – HackNY Previous Post Next Post", "date": "2018-09-25"},
{"website": "Giphy", "title": "GIPHY @ PyData NYC", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-pydata-nyc/", "abstract": "GIPHY @ PyData NYC October 16, 2018 by GIPHY October 17 – 19, 2018 Microsoft Conference Center Times Square – New York, NY Come join the GIPHY Engineering team at PyData NYC to hear GIPHY CTO, Anthony Johnson give his talk “Word2Vec for GIFs”! PyData conferences are amazing community based events that bring together users and developers of data analysis tools to share ideas and learn from each other. In Anthony’s talk, “Word2Vec for GIFs,” he’ll discuss how GIPHY used word2vec to create better suggestions for searches and GIFs and, in turn, increased user engagement on our site. Previous Post Next Post", "date": "2018-10-16"},
{"website": "Giphy", "title": "Giphy Android App and Fresco", "author": ["Cristian Holdunu"], "link": "https://engineering.giphy.com/giphy-android-app-and-fresco/", "abstract": "Giphy Android App and Fresco September 27, 2018 by Cristian Holdunu GIPHY’s Android team recently released our third major version of the GIPHY app. In the latest release, we focused on delivering new features, redesigning the home page to support GIPHY Stories, and improving our app’s performance. We also delivered an entirely  new set of GIF creation tools! Behind these front-facing features for our users, we focused on fundamentally improving our app’s performance by converting the codebase to Kotlin, refactoring the navigation stack, and animating screen transitions.. Testing the new app codebase showed promising results, except for the GIF rendering. Renditions took a lot of time to load and feedback we’ve collected from our users suggested this is a common issue. The Android team headed out into the  wild to find a suitable replacement for our current image rendering technology. We soon had to decide between the two major image loading libraries offering support for GIF and WebP format: Glide – A popular choice for image loading in Android, supported by Google. Fresco – Created by Facebook for efficient and fast image loading. via GIPHY As both libraries offer support for disk caching, efficient image loading, and post-processing, we made our decision based on animated drawing performance. We quickly drafted a demo app into which we loaded a ton of GIFs, similar to GIPHY’s functionality. We noticed that Glide encountered issues when rendering multiple sources at once, dropping frames from the GIF animation compared to Fresco. via GIPHY Doing a quick search about Glide’s performance with GIFs, we found multiple issues on GitHub related to this issue. Developers made it clear that performance can’t be improved, unless they rewrite their rendering engine from scratch. At this point, it was clear we have a winner: Fresco Integrating Fresco Setting up Fresco was easy. Because we use several different types of GIF renditions (i.e. thumbnails, gif details, story feed), we decided to setup two cache configs. In this way, Fresco will not be forced to evict a lot of small thumbnails, because the cache will quickly fill with high quality images. val previewsDiskConfig = DiskCacheConfig . newBuilder( this ) . setMaxCacheSize( 250L * ByteConstants . MB ) . build() val qualityDiskConfig = DiskCacheConfig . newBuilder( this ) . setMaxCacheSize( 250L * ByteConstants . MB ) . build() val config = ImagePipelineConfig . newBuilder( this ) . setSmallImageDiskCacheConfig(previewsDiskConfig) . setMainDiskCacheConfig(qualityDiskConfig) . build() Fresco . initialize( this , config) For rendering, we used the SimpleDraweeView class. With just some small additions to support our SDK models out of the box, we had Fresco rendering our GIFs in no time. Loading a image into a DraweeView basically means creating a new controller and assigning it to the view, like so: val newController = Fresco . newDraweeControllerBuilder() . setUri(uri) . setOldController(draweeView . controller) . setControllerListener(getControllerListener()) . build() draweeView . controller = newController Easy, fast, fun! Launch it! via GIPHY Fresco’s Limitations Browsing the GIPHY app means exploring thousands of GIFs. Small animated images (thumbnails) are used to populate the home feed in GIPHY Stories or presently trending GIFs. After tapping a thumbnail, the app will open a page to present the selected GIF at full size, or to play a Story feed. For the GIF detail page and the content of a story, the app will progressively load a set of renditions, starting from low-quality images to high-quality ones. The first image to be loaded from the set is the low-quality thumbnail the user tapped. Concurrently, in the background, a high quality version of the same GIF is loaded. When it is complete, we replace the thumbnail. We soon discovered that when making a simple request to change the resource of a DraweeView , Fresco will clear the current content, display the placeholder, then load the new resource. Even with the resources preloaded, there is a noticeable flickering. Searching for answers on Fresco’s GitHub project, we found similar problems being discussed, but for static images. We found two possible solutions: Fresco provides a low-quality/high-quality schema for loading images. Unfortunately, in this schema, the low-quality image is not animated. Use a RetainingDataSource . This allows us to keep the same DraweeController and replace its content. Unfortunately, we couldn’t make animated images play. None of the above solutions worked out of the box, so we decided to solve the issue for this particular case of animated images. As Fresco is an open-source project, we forked their GitHub repo and started to analyze the problem.  After an extensive debug session with different kind of images, we identified the following actors responsible for loading and rendering images: While RetainingDataSource looked like what we were looking for, it turns out that this data source works its magic by lying to the underlying Fresco implementation, keeping it in a forever PROGRESS state. That means the DraweeControllers will never be notified when an GIF is loaded and they will never start to play. Fixing RetainingDataSource Solving the issue of GIFs not being played turned out to be an easy fix by following these steps: Let DataSource provide multiple results. In our loading flow, a new result meant a GIF image of a higher quality. Mark RetainingDataSource as a data source capable of delivering multiple results. Modify the Fresco DraweeController base class to deliver each result to its DraweeControllers. At this point, controllers will start playing the animated drawables. via GIPHY As the open source community is extremely important for everyone here at GIPHY, we pushed all our changes to GitHub and opened a pull request to Fresco, excited to  solve an issue in a library used by thousands of projects. The Fresco team was great to work with and after a few exchanged comments, they accepted the pull request. You can check our work on the Fresco repo, here! – Cristian Holdunu, Senior Android Developer Previous Post Next Post", "date": "2018-09-27"},
{"website": "Giphy", "title": "GIPHY @ HackMIT", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-hackmit/", "abstract": "GIPHY @ HackMIT September 10, 2018 by GIPHY September 15 – 16, 2018 Massachusetts Institute of Technology – Cambridge, MA For the second year running, we’ll be at HackMIT! With two GIPHY Engineers heading back to their alma mater for the weekend, we’ll be sponosring the hackathon and awarding a prize to the team with the best use of the GIPHY API. We hope to see you there! Previous Post Next Post", "date": "2018-09-10"},
{"website": "Giphy", "title": "GIPHY @ PennApps", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-pennapps-2/", "abstract": "GIPHY @ PennApps September 3, 2018 by GIPHY September 7 – 9, 2018 University of Pennsylvania – Philadelphia, PA GIPHY is a proud sponsor of this year’s PennApps hackathon! You can find our mentors there handing out GIPHY swag, helping teams with their projects and sharing info on how to use the GIPHY API. See below for information on this year’s theme! “As disaster risk grows, families and communities need to become more resilient; that is, they must improve their ability to prepare for, absorb, and recover from extreme events. In this year’s PennApps “Hack-for-Resilience” route, hackers have the chance to help people do just that, by developing apps, tools, and other technologies to help at-risk families. Examples could include hacks to help people better understand the risks they face, the potential or historical impacts of disaster, the benefits of risk-reduction measures or insurance, or how to improve preparedness.” Previous Post Next Post", "date": "2018-09-03"},
{"website": "Giphy", "title": "Contextually-Aware Search: GIPHY Gets Work-Specific", "author": ["Zachary Hay"], "link": "https://engineering.giphy.com/contextually-aware-search-giphy-gets-work-specific/", "abstract": "Contextually-Aware Search: GIPHY Gets Work-Specific September 4, 2018 by Zachary Hay GIPHY has popular integrations on many platforms like Facebook Messenger, Twitter, and Slack. We serve tens of billions of requests a month to all integrations via our API. Users search our industry-leading database of GIFs and stickers to find the perfect one to send in all of these messaging platforms. GIPHY’s search engine calculates CTR rates via giphy.com and mobile apps. We are optimizing the search engine for these contexts. I found that users’ content preferences on Slack differ significantly than the preferences of mobile and website users, meaning that the best GIFs for Slack were not always in the top of search results. To solve this problem, I introduced a contextually-aware search algorithm for our Slack integration. This resulted in the significant reordering of many search results, and a large relative increase of 7.4% in the overall send rate of users in the Slack integration. THE GIPHY SLACK INTEGRATION GIPHY’s Slack integration lets users send GIFs and stickers to coworkers using the translate endpoint in our API. The integration has two modes: shuffle and random. Sending the “/giphy query” command in shuffle mode starts a search session where users shuffle through a list of GIFs until they find the perfect one to send. My project revolved around calculating the send rates for specific GIFs using click-actions from shuffle mode more on that later). The random mode is a bit more exciting. Users enter the slack command and a GIF is immediately sent from a pool ofrelevant GIFs. On GIPHY.com or the mobile app, users can scroll through pages of GIF search results to make a selection. However, our Slack integration presents a unique search context since users are only presented with a single GIF at a time. This poses a challenge because we have to inject a bit of randomness in the mix to make things novel and fun while keeping the results relevant. Two ways of celebrating in Slack at work Shuffle Mode Random Mode Celebrating on Giphy.com Many GIFs – One Page CONTEXTUALLY-AWARE SLACK SEARCH GIPHY engineers previously implemented a creative solution around the constraints of the Slack integration called the translate endpoint. The previous release of the translate endpoint selected a GIF from the top 25 search results for a query, with a larger probability of selecting a GIF with a higher ranking. This way, users received a variety of relevant GIFs, instead of the same one or two ad nauseum. The goal of contextually-aware search was to release a new version of the translate endpoint featuring Slack-specific search weights. To do this, we utilized click-action data collected from our Slack integration to calculate the send rate of a GIF with a given query. I wrote a Spark job to calculate the send rate of a GIF with its query over an arbitrary period of time. By calculating the send rate over several months, there’s enough example data (query/ GIF/ user action) to make an informed estimate of the send rate over an enormous number of searches. What is the difference between these two GIFs? The angry doctor GIF on the left is the highest-ranked GIF in our database for the query “monday” (according to multiple factors, including CTR) while the escalator GIF on the right is ranked 16th. Their Slack send rates, however, vary greatly. The escalator GIF is sent 46% of the times it’s seen, while only 25% for angry doctor GIF. Prior to my project, the Slack Integration would be five times more likely to return the angry doctor GIFthan the elevator GIF. This large  discrepancy affected hundreds of other popular queries like “mad,” “sweet,” and “thanks.” Clearly, Slack users have different content preferences than users of our other products, and we had a great case for creating Slack-specific search scores in the integration. Our next step was to update the translate endpoint to utilize the powerful send rate data we calculated for a given GIF/query pair. We decided to build on top of the base logic of the previous version of the translate endpoint, but expanded the pool of GIFs from 25 to 50. Now, GIPHY’s search engine returns the top 50 GIFs for a search, then the translate endpoint orders them according to their Slack-specific send rate. My blog post would not be complete without revealing all new features of the translate endpoint available to developers here in addition to the Slack-specific search score detailed above: – N o duplicates in shuffle mode. We now keep shuffle session state so the same GIF will not be returned within 10 shuffles. – Weirdness as you shuffle. The start of a shuffle session returns highly relevant GIFs; they get weirder (or more random) as you shuffle. – A weirdness param can be passed to the translate endpoint. It determines how weird (or random) the GIFs we return will be. – New GIFs or stickers added to GIPHY’s database that Slack users enjoy will quickly “bubble up” to the top of the search results due to continuously recalculating send rate data. We implemented all of these features into a compact mircroservice with a queryable API that could handle Slack’s daily throughput. A Redis datastore was used to hold all send rate information. Below is a diagram laying out the architecture and flow of a request: MEASURING IMPACT There were three Key Performance Indicators (KPIs) established for measuring success of the revamped Slack integration: shuffle mode session length; overall send rate; and the “one GIF, one submit” rate of shuffle mode sessions. The “one GIF, one submit” rate is important because it is a proxy for the quality of random mode GIFs. We can’t directly measure random mode sessions because the first GIF is always returned. There is no notion of shuffling, canceling, or sending. A high “one GIF, one submit” rate in shuffle mode, though, indicates that users are getting a good GIF in random mode because shuffle mode and random mode use the same underlying process. After deploying Slack-specific search for a week, we saw dramatic improvements in all three metrics. These increases are calculated across all queries, not just a high performing subset. Furthermore, A higher proportion of sessions ended in 0-1 shuffle, which means shuffle mode users aren’t having to work as hard to get the GIFs they want. Contextually aware search had a huge impact on our KPI! HOURLY ONE GIF, ONE SEND RATE (AUG 13-AUG 21) MY SUMMER AT GIPHY I worked on this project as a Search Engineering Intern on the R&D team. The R&D team works on a variety of long-range projects to improve search from content moderation to GIF recommendations using machine learning and adaptive systems. We are tasked with introducing new, cutting edge technology and forward thinking products. I want give a big shout out to Nick Hasty, my PM and mentor for the summer. He introduced me to the GIPHY tech stack and gave me support throughout the project from product vision to productionalizing the code. Ihor Kroosh and Denis Sergienko from Rails Reactor were both essential members of the team. Their expertise in Luigi scheduling, Kubernetes, and deployment was invaluable throughout the process. Anthony Johnson, Sixuan Liu, and Sean Quigley all took time to ensure the project’s success even though they are not members of the R&D team. Thank you all! – Zach Hay , Search Engineering Intern Previous Post Next Post", "date": "2018-09-04"},
{"website": "Giphy", "title": "Building GIPHY’s Extension for Twitch", "author": ["Nick Santaniello"], "link": "https://engineering.giphy.com/building-giphys-extension-for-twitch/", "abstract": "Building GIPHY’s Extension for Twitch August 21, 2018 by Nick Santaniello We’re super excited about the recent launch of our GIPHY extension for Twitch! Twitch has put a lot of and work and attention to detail into creating their Extensions platform and we’re excited to be a part of it. If you’ve used GIPHY on Slack, GIPHY for Twitch works in much the same way except with a slim, graphical interface. Viewers can insert a term and GIPHY will automagically deliver the perfect GIF and layer it atop a broadcaster’s stream for all of the audience to see. This integration offers a new way for viewers to visually comment on a broadcaster’s stream . In the past, streamers have had to set up complicated pipelines with their professional broadcast software to bake visual effects into their streams and adding an interactive effect was impossible. Twitch’s new extension platform allows developers like GIPHY to make native HTML5 widgets that overlay atop a stream, thus simplifying the process for streamers and allowing for greater interactivity with viewers. Technically, the GIPHY for Twitch extension is composed of two parts: a lean HTML5 widget that interacts with both the public GIPHY API and the Twitch API, and a new GIPHY API. The existing public GIPHY API provides most of the functionality we need, while the new API is used for queueing up GIFs to be played on Twitch. When a user enters a search term on Twitch, the extension fetches an appropriate GIF from the public GIPHY API and queues it up in the new service, which stores the queues in a Redis database. The Twitch API then interacts with the new service and the extension to display the GIFs on a broadcaster’s channel. The new service periodically connects to Twitch’s PubSub system to deliver a list of all pending GIFs for a channel to each viewer’s browser. The PubSub system delivers the list to the frontend of every client watching the stream, so that the GIFs can be shown to all viewers. One problem we noticed while testing was what I like to call “busy jukebox” syndrome . Have you ever requested a song in a popular jukebox only to end up waiting forever, not knowing when your song would play, or if it would even play at all? The same problem could occur with our extension on Twitch if too many GIFs are queued on a given stream. We solved this problem in our frontend by shortening the duration of GIFs as the queue grows. This ensures that all GIFs are played in a timely manner, shortly after they were initially requested. Finally, we wanted to ensure that artists and partners were properly attributed when their GIFs were shown on Twitch. We do this by allowing users to hover over the GIF which will then display which GIPHY user the GIF comes from. We’re looking forward to hearing users’ feedback and response to the extension and ways we can make it even more useful and fun for Twitch streamers and viewers in the future! Previous Post Next Post", "date": "2018-08-21"},
{"website": "Giphy", "title": "Doing it Live at GIPHY (with AVFoundation)", "author": ["Andy Hin"], "link": "https://engineering.giphy.com/doing-it-live-at-giphy-with-avfoundation/", "abstract": "Doing it Live at GIPHY (with AVFoundation) July 27, 2018 by Andy Hin At GIPHY, we create a lot of GIFs from live broadcasts –- TV shows, world events, sports games, and more. One of the tools we use to do this is GIPHY Capture – a MacOS app that lets you easily record your screen. Click to record, and click again to stop. Easy! But one unique challenge with live broadcasts is that it’s near impossible to predict when something is about to happen – and if you forget to click the record button you’ve missed the moment! We set out to build a feature to address this specific problem. Introducing Loop Recording Various products including video surveillance, dash cams, and broadcast servers have solved this problem using a process called loop recording . Video is recorded continuously to an endless tape (or computer memory). When the tape or storage runs out, the recording process continues at the beginning, looping back to overwrite previous content. This allows recording to go on indefinitely without the need for endless amounts of storage. When something eventful happens, the last chunk of recorded data can be retrieved. The Product: Infinite Capture For our solution, we shipped a feature update to GIPHY Capture called “Infinite Capture”. Here’s how it works: When the app launches, users can drag and resize the capture window over the video they are trying to capture. When they click the record button, the app starts recording the screen. When the progress bar completes, it will begin overwriting old content – but the app will always have the last 30 seconds of video. The app can continue to record in this state indefinitely. When a significant event occurs in the live feed, the user can click the capture button to output the last 30 seconds of recorded video – guaranteeing they will never miss out on a GIF-able moment again! Implementing it in Swift (and Objective C) To implement the “Infinite Capture” feature, we needed to recreate this process using Swift and Apple’s AVFoundation framework . Circular Buffer First, we implemented a way to store the frames in the same fashion as a “looped tape”. Fortunately, there is an analogous data structure to help us achieve this: it’s called a CircularBuffer or RingBuffer . A circular buffer is essentially an array that is connected end-to-end to form a circle (see image above). It works by having special logic to handle both: Wrapping the writeIndex around to the beginning when the buffer is full Reading from the buffer in the correct order. Let’s create a class for our CircularBuffer structure – backed by a primitive array . We also need to keep track of the current writeIndex . class GGCircularBuffer {\r\n    private var buffer = [Any?]()\r\n    private let capacity : Int\r\n    private var writeIndex : Int = 0\r\n    \r\n    ... We need a way to write data into the buffer: /*\r\n * Appends a new value to the buffer.\r\n */\r\nfunc write(newElement: Any) {\r\n    // Calculate the index to write to, account for wrap-around\r\n    let i = self.writeIndex % self.capacity\r\n    \r\n    // Write the new value\r\n    self.buffer[i] = newElement\r\n    \r\n    // Increment write index\r\n    self.writeIndex += 1\r\n} Warning: For simplicity, the example above is not thread-safe. To support concurrency, you should use a serial dispatch queue or semaphore to control write access. The main trick is that the writeIndex needs to wrap back to the beginning when the buffer is full. We can use the mod operator to handle that. The new element will then overwrite any data that was previously there. We’ll also need a way to read back the data we’ve written: /*\r\n * Returns a copy of the current data in the the ring buffer, in the order that it\r\n * was written, as a sequential array.\r\n */\r\nfunc readAll() -> [Any] {\r\n    if self.writeIndex <= self.capacity {\r\n        // If we haven't made a full wraparound yet, return data from index 0..<writeIndex\r\n        return Array(self.buffer[0..<self.writeIndex].map { $0 as Any })\r\n    } else {\r\n        // Otherwise, if we have wrapped around, we need to start from writeIndex (accounting\r\n        // for wrap-around).\r\n        var orderedArray = [Any?]()\r\n        \r\n        for i in 0..<self.capacity {\r\n            let readIndex = (self.writeIndex + i) % self.capacity\r\n            orderedArray.append(self.buffer[readIndex])\r\n        }\r\n        \r\n        return orderedArray.map { $0 as Any }\r\n    }\r\n} For reads, there are 2 cases. The buffer isn’t full yet. In this case we simply read from the beginning ( self.buffer[0] ) up to the last written index ( self.buffer[self.writeIndex - 1] ). Buffer has been filled up and writes have “wrapped around”. In this case we need start reading at self.buffer[self.writeIndex] which will be pointing at the “least recently written” element in the buffer. That’s it! Now that we’ve built our CircularBuffer class, we can start using it to store our recording. Writing frames to our CircularBuffer The typical process to record video in AVFoundation is: AVCaptureSession orchestrates the recording process AVCaptureVideoDataOutput outputs the captured frames as CMSampleBuffer objects. AVCaptureVideoDataOutputSampleBufferDelegate captureOutput:didOutputSampleBuffer:fromConnection: can then feed the frames to AVAssetWriter which will save out the video to a file. For our use-case, we want to store the frames in a CircularBuffer instead of writing directly to a video file. When the user wants to save the current buffer, we feed the frames stored in our CircularBuffer to AVAssetWriter to write the video file. It would look something like this: - (void)captureOutput:(AVCaptureOutput *)captureOutput didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection {\r\n    if(CMSampleBufferDataIsReady(sampleBuffer)) {\r\n        [self.circularBuffer write:sampleBuffer];\r\n    }\r\n} Note: Because AVCaptureSession allocates and reuses the CMSampleBufferRef in a pool, you’d actually need to clone it before retaining it – otherwise you will run into memory management errors. For more info, see this StackOverflow thread . If we then take the frames stored in CircularBuffer and write it out to a video file, it would yield the expected results. However, you will notice that recording uses up a lot of memory resources – depending on your recording this could easy go into the gigabytes! Optimize: Encoding CMSampleBuffers using Video Toolbox The problem is that we are storing uncompressed frames. A 10 second, 1080p video, at 30 FPS would result in 1.8 GB of memory usage ( 10s x 60fps x 1920px x 1080px * 3bytes/px = 1.8GB )! To optimize, what we want to do is encode/compress the frames before storing them in our CircularBuffer . Unfortunately, AVFoundation is too high-level to support “on-the-fly” encoding like this, so we’ll need to drop down into Apple’s Video Toolbox framework. The Video Toolbox framework provides hardware-accelerated video encoding and decoding capabilities. I highly recommend these 2 resources to learn more: Apple’s documentation . WWDC presentation We’ll be using VTCompressionSession to perform the encoding. To keep things organized, we’ll put code inside a helper class called GGH264Encoder.mm : @interface GGH264Encoder()\r\n\r\n@property VTCompressionSessionRef encoderSession;\r\n\r\n@end\r\n\r\n@implementation GGH264Encoder\r\n\r\n- (instancetype)init {\r\n    // Specify callback for the encoder\r\n    VTCompressionOutputCallback callBack = encoderSessionDidOutputSampleBuffer;\r\n    \r\n    // Initialize encoder\r\n    VTCompressionSessionCreate(nil, (int)width, (int)height, kCMVideoCodecType_H264, nil, nil, nil, callBack, (__bridge void *)(self), &_encoderSession);\r\n    \r\n    // Encoding settings\r\n    VTSessionSetProperty(_encoderSession, kVTCompressionPropertyKey_AverageBitRate, bitRateRef);\r\n        \r\n   // Startup encoder\r\n    VTCompressionSessionPrepareToEncodeFrames(_encoderSession);\r\n}\r\n\r\n... Note: Video Toolbox contains classes and functions that are written in C++ so we will need to use the .mm extension and some of the syntax may look funny. Above, we initialize our VTCompressionSession with H264 as our desired codec. You can also use VTSessionSetProperty() to specify additional encoding settings (bitrate, framerate, pixel formats, etc.). We also provide a VTCompressionOutputCallback which will get fired automatically after each frame is successfully encoded. To perform the actual encoding: CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);\r\nCMTime presentationTime = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)\r\n   \r\nVTEncodeInfoFlags flags;\r\nVTCompressionSessionEncodeFrame(_encoderSession, pixelBuffer, presentationTime, nil, nil, nil, &flags); A CMSampleBufferRef contains 2 main components: the actual pixel data for the frame, and the timestamp of when the frame should be displayed. We extract these 2 properties and call VTCompressionSessionEncodeFrame() to encode the frame. Once the frame has been encoded, VTCompressionSession will fire off the callback method we specified earlier. We can then take the encoded CMSampleBuffer and append it to our CircularBuffer : void encoderSessionDidOutputSampleBuffer(void *outputCallbackRefCon, void *sourceFrameRefCon, OSStatus status, VTEncodeInfoFlags infoFlags, CMSampleBufferRef sampleBuffer) {\r\n    //  Append the encoded sampleBuffer to the CircularBuffer\r\n} Here is a visual representation of what we just built: Running the new implementation we notice significant improvements in performance. The actual % gains depend mostly on the video content and how “compressible” it is. But from our own testing, we saw memory savings in excess 10X. On current Macbooks, we were able to store a 30 second buffer containing 1080p, 60FPS video frames without issue! Try out the new feature for GIPHY Capture and let us know what you think! Previous Post Next Post", "date": "2018-07-27"},
{"website": "Giphy", "title": "MusicTech @ GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-musictech/", "abstract": "MusicTech @ GIPHY July 16, 2018 by GIPHY July 18, 2018 GIPHY HQ – New York, NY GIPHY is excited to host this month’s MusicTech Meetup! MusicTech is a community that supports the people who are defining what music technology will look like in the future. Demos by: Merch Cat, WR1, Radiate and Indie.Ninja Performance by: Kelsey Pyro To sign up and find more information, click here ! Previous Post Next Post", "date": "2018-07-16"},
{"website": "Giphy", "title": "How GIPHY uses Fastly to Achieve Global Scale", "author": ["Ethan Lu"], "link": "https://engineering.giphy.com/how-giphy-uses-fastly-to-achieve-global-scale/", "abstract": "How GIPHY uses Fastly to Achieve Global Scale April 20, 2021 by Ethan Lu GIPHY serves a lot of GIF media content. Over 10 billion pieces of content per day, in fact. In addition to media requests, which represent the actual download of GIFs, we also provide public API and SDK services for developers to use in their products, which gives their users access to our enormous library. As with many tech companies with large daily traffic volume, we have scalability challenges. Our systems have to be able to handle a high volume of requests (in the 100K requests per second range) and have a low latency response. There’s nothing worse than waiting for something to load — especially a GIF! This is where an edge cloud platform plays a role: instead of making our AWS servers handle every request that comes our way, an edge cloud platform caches as much of the media content and search result JSON payload as possible. This works well because neither media content, nor API responses change often. The edge cloud platform servers also distribute the request load among various regions. We use Fastly as our edge cloud platform provider to help us serve billions of pieces of content to our users. Fastly Solution Fastly provides a variety of features that allow us to deliver content at scale. These features can be broadly categorized as: – Cache Layering – Cache Management – Edge Computing Cache Layering A basic edge cloud platform set up has the content cached at the edge . These server nodes are distributed globally and deliver the cached content to users making requests in their region. In the event the edge node does not have the content, it will make a request to our origin server to retrieve it. This single layer setup has a drawback. Each edge node maintains its own cache based on the requests from its region. So a new piece of content may not be cached on any of the edge node which could lead to surges in traffic to our origin servers as each edge node repeats a request for the same content. Viral content often exhibits this behavior as its popularity gains traction. Fastly offers a second layer of cache service called the Origin Shield . Edge nodes that do not have the requested content in cache can now retrieve it from the Origin Shield layer with the request only needing to reach our origin server if the Origin Shield does not have it. Cache Management Now that the content is cached on the edge and Origin Shield, we need ways to manage their caching policies. Not all content should stay cached for the same duration, or TTL (Time to Live). For example, the information on an individual GIF will not change that much, so its API response can be cached over a relatively long period of time. On the other hand, the API response for the Trending Endpoint , which returns a continuously updated list of currently trending GIFs, would need to be on a short TTL due to the nature of trends. Fastly is powered by Varnish, so all of the configurations are executed as Varnish Configuration Language (VCL) code. Both the edge and Origin Shield runs VCL code, so we are able to set up various cache TTLs based on API endpoint paths with some simple VCL code: # in vcl_fetch\nif (req.url ~ \"^/v1/gifs/trending\") {\n # set 5 minute ttl for trending responses\n set beresp.ttl = 600s;\n\n return(deliver);\n} The cache TTL does not always have to be set by VCL code. API requests that reach origin, when a cacheable item is missing or stale, can have cache control instructions encoded in their responses from Origin. We just need to simply setup the VCL code so that it can be overridden. From origin, we can propagate this decision to Fastly’s Origin Shield and edge nodes by setting cache control headers in the API response. Specifically, the Surrogate-Control header because this header will only be for Fastly nodes. So, we can update the above VCL to prioritize the Surrogate-Control over the endpoint cache policies like this: # in vcl_fetch\nif (beresp.http.Surrogate-Control ~ \"max-age\" || beresp.http.Cache-Control ~ \"(s-maxage|max-age)\"\n) {\n  # upstream set some cache control headers, so Fastly will use its cache TTL\n  return(deliver);\n} else {\n  # no cache headers, so use cache policies for endpoints\n  if (req.url ~ \"^/v1/gifs/trending\") {\n   # set 10 minute ttl for trending responses\n   set beresp.ttl = 600s;\n\n   return(deliver);\n  }\n} With this setup, we can have cached content invalidate themselves with dynamic TTL policies that meet our needs — but we also need to invalidate cache explicitly if we don’t want to wait for them to naturally expire. We could simply invalidate the cache by the cache key (URL). This works well for media, but it is a bit more complicated for API responses. For example, our API search endpoint can return the same GIF for different queries, but it isn’t feasible for us to know every possible URL that yielded that GIF if we wanted to invalidate it: # same GIF can appear in the response of all of these API calls\nhttps://api.giphy.com/v1/gifs/search?api_key=__KEY1__&q=haha\nhttps://api.giphy.com/v1/gifs/search?api_key=__KEY1__&q=hehe\nhttps://api.giphy.com/v1/gifs/search?api_key=__KEY2__&q=lol\nhttps://api.giphy.com/v1/gifs/search?api_key=__KEY3__&q=laugh For this situation, we take advantage of Fastly’s Surrogate Keys! As the name suggests, a surrogate key can uniquely identify cached content, in much the same way the cache key does. Unlike the cache key, there can be multiple surrogate keys per stored result, and we can set the surrogate keys. Using the GIF IDs that appear in each API response gives us a way to identify multiple pieces of cached content that contain a given GIF: # same GIF (gif_id_abc) can appear in the response of all of these API calls\nhttps://api.giphy.com/v1/gifs/search?api_key=__KEY1__&q=haha\n\tAssign Surrogate Key: gif_id_abc\nhttps://api.giphy.com/v1/gifs/search?api_key=__KEY1__&q=hehe\n\tAssign Surrogate Key: gif_id_abc\nhttps://api.giphy.com/v1/gifs/search?api_key=__KEY2__&q=lol\n\tAssign Surrogate Key: gif_id_abc\nhttps://api.giphy.com/v1/gifs/search?api_key=__KEY3__&q=laugh\n\tAssign Surrogate Key: gif_id_abc We can even attach multiple surrogate keys to the same content: https://api.giphy.com/v1/gifs/search?api_key=__KEY1__&q=haha\n\tAssign Surrogate Key: gif_id_abc gif_id_def key_KEY1 q_haha\nhttps://api.giphy.com/v1/gifs/search?api_key=__KEY1__&q=hehe\n\tAssign Surrogate Key: gif_id_abc gif_id_123 key_KEY1 q_hehe\nhttps://api.giphy.com/v1/gifs/search?api_key=__KEY2__q=lol\n\tAssign Surrogate Key: gif_id_abc, gif_id_321 gif_id_456 key_KEY2 q_lol\nhttps://api.giphy.com/v1/gifs/search?api_key=__KEY3__&q=laugh\n\tAssign Surrogate Key: gif_id_abc key_KEY3 q_laugh Surrogate keys are a powerful feature that allows us to select the appropriate cache to invalidate with great precision and simplicity. With this setup, we are able to invalidate cache for situations such as: – Invalidate all cached API responses that contain a specific GIF – Invalidate all cached API responses that are for a specific API key – Invalidate all cached API responses that queried for certain words Running Code At The Edge VCL provides a lot of versatility in what we can do in the edge cloud platform configuration. We showed before how the configuration can set various cache TTL policies for the edge and Origin Shield nodes, but we can also use VCL to set the request information. We can have code to rewrite the incoming request URL. This comes in handy when we need to make changes to our API endpoints without troubling our consumers to update their calls. # in vcl_recv\nif (req.url ~ “^/some-old-endpoint”) {\n\t# rewrite to the new endpoint\nset req.url = regsub(req.url, “/some-old-endpoint”, “/new-and-improved-endpoint”);\n} We can even select a percentage of the incoming requests to test experimental features. By using Fastly’s randomness library we can add a special header to some of our requests that enables new behaviour in our origin server. # in vcl_recv\nset req.http.new_feature = 0\nif (randombool(1,10000)) {\n\t# .01% of the traffic gets to see the new feature\nset req.http.new_feature = 1;\n} This combined with Fastly’s edge dictionaries allow us to set up different behaviors with minimal code. # API keys that will have a percentage of their request use the new feature\ntable new_feature_access {\n\t“__API_KEY1__”: “1”,\n\t“__API_KEY2__”: “5”,\n\t“__API_KEY3__”: “1000”,\n}\n\n\nsub vcl_recv {\nset req.http.new_feature = 0\n\n# check if request has an api key that is setup to have a percentage of its requests use the new feature\nif (randombool(std.atoi(table.lookup(new_feature_access, subfield(req.url.qs, \"api_key\", \"&\"), \"0\"))\n,10000)) {\nset req.http.new_feature = 1;\n}\n\nreturn(lookup);\n} This is just scratching the surface of what VCL enables. Fastly’s documentation can be found here if you want to see what else is possible! Tips and Tricks We use a lot of Fastly features to power the world with animated GIF content. However configuring the edge cloud platform can be quite complex when there is so much functionality at your disposal, so here are some tips and tricks we recommend to help you along the way. VCL Execution In Edge and Origin Shield With a two layer cache setup, one key thing to remember is the same VCL code will execute on both the edge and Origin Shield. This can cause unexpected outcomes if the VCL code is changing request/response state information. For example, our VCL code from before would set our cache TTL, based on cache control headers from upstream or specified in the VCL code itself, for both the Origin Shield and edge nodes: # in vcl_fetch\nif (beresp.http.Surrogate-Control ~ \"max-age\" || beresp.http.Cache-Control ~ \"(s-maxage|max-age)\"\n) {\n  # upstream set some cache control headers, so Fastly will use its cache TTL\n  return(deliver);\n} else {\n  # no cache headers, so use cache policies for endpoints\n  if (req.url ~ \"^/v1/gifs/trending\") {\n   # set 10 minute ttl for trending responses\n   set beresp.ttl = 600s;\n\n   return(deliver);\n  }\n} Suppose for the trending endpoint , we also set the response’s Cache-Control header so we can instruct the caller on how long to cache the content on their side. This could simply be done as: # in vcl_fetch\nif (beresp.http.Surrogate-Control ~ \"max-age\" || beresp.http.Cache-Control ~ \"(s-maxage|max-age)\"\n) {\n  # upstream set some cache control headers, so Fastly will use its cache TTL\n  return(deliver);\n} else {\n  # no cache headers, so use cache policies for endpoints\n  if (req.url ~ \"^/v1/gifs/trending\") {\n   # set 10 minute ttl for trending responses\n   set beresp.ttl = 600s;\n   # set 30 second ttl for callers\n   set beresp.http.cache-control = \"max-age=30\";\n\n   return(deliver);\n  }\n} The Origin Shield would have executed this VCL code and added the Cache-Control header to the response’s header and returned it to the edge. On the edge however, it would have seen that the Cache-Control is set in the response and would have executed the if-statement. This would have resulted in the edge nodes using a cache TTL of 30 seconds instead of the intended 10 minutes! Fortunately, Fastly provides a way of distinguishing between the edge and Origin Shield by setting a header (Fastly-FF) in the request: # in vcl_fetch\nif (req.url ~ \"^/v1/gifs/trending\") {\n   # set 10 minute ttl for trending responses\n   set beresp.ttl = 600s;\n\n   return(deliver);\n}\n\n# in vcl_deliver\nif (!req.http.Fastly-FF) {\n   # set 30 second ttl for callers\n   set resp.http.cache-control = \"max-age=30\";\n} With this addition, the Cache-Control header would only be set at the edge node and our cache policies are behaving as expected again! Debugging and Testing The pitfall we just mentioned can be quite difficult to detect and debug. The VCL code would just run on a server and present you with the response and response headers. We can simply add debugging information into custom headers and view them in the response, but this can get unwieldy quick. Fortunately, there is the Fastly Fiddle tool , which provides better visibility into what the VCL code does when it executes. We can simulate the various VCL code parts in this tool and get more information on how Fastly’s edge and Origin Shield servers will behave with the VCL code. Here is the fiddle of the above example that shows the double execution of the VCL can affect the cache TTL. We set up the VCL in the appropriate sections on the left, and execute it to see how Fastly would have handled the request on the right: The picture above shows a lot of useful information about the life cycle of the request as it goes through the edge and Origin Shield node. In a real world setting, the VCL code can be very complex and this tool really shines in situations like this. — Ethan Lu, Tech Lead, API Team Additional Information Fastly Shielding Fastly Cache Control Fastly Surrogate Keys Fastly Fiddle Fastly Documentation Varnish Configuration Language Previous Post", "date": "2021-04-20"},
{"website": "Giphy", "title": "GIPHY Text is Now Live in Our SDK and Chrome Extensions", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-text-is-now-live-in-our-sdk-and-chrome-extensions/", "abstract": "GIPHY Text is Now Live in Our SDK and Chrome Extensions March 18, 2021 by GIPHY GIPHY *literally* has a gazillion pieces of content indexed in its library. Just head over to the website GIPHY.com and type in your fav search terms to get a sense of the incredibly vast amount of results (GIFs, Stickers, Clips, etc.) that we surface. However, sometimes what you want a GIF to say is quite specific and search results aren’t getting it exactly right. What if you could just make a GIF for those special occasions? The new GIPHY Text feature is a creation tool for generating messages that pop, catch fire, or jiggle for whatever text you input — and it’s now available in the GIPHY SDK as well as GIPHY Chrome extensions! We provide a few dozen different animated font styles, each paying homage to a different vintage internet aesthetic. The Design When designing the experience for delivering Text Stickers — inside the mobile SDK and Google Chrome Extensions — we stripped away anything that might slow down the communication process and aimed to deliver beautiful expressive text as quickly as possible inside the existing primary use case: GIPHY Search. Users simply search like they always have and a helpful suggestion will appear with search results. If they tap the Text Icon,  they will see their search terms automatically animated and styled for them to share or they can continue creating elegantly from inside of the search bar. We also designed a core set of text styles that would instantly add more than just trendy looks, but also emotional context to any phrase you like. You might want to send a flirty “hello” or perhaps a friendly “thanks” with a playful bounce. This is all possible using GIPHY Text Styles. Some styles may even look familiar as they borrow from text GIFs of the internet’s past. Some reference beloved movie and TV titles to help us all inject a little bit of personality into our presentations, web documents, emails, and chats. Behind the Scenes Under the hood, ThreeJS is the magic that brings your stylized text input to life! This nifty Javascript library encapsulates WebGL functionalities into a high-level API that gives developers a variety of ways to create animated 3D objects. At a glance, we created GIPHY Text by representing a font as a text geometry, and then give it the look by applying various colors, textures, lighting, and cameras. Finally, we animate it by adding different kinds of rotations, translations, and transformations. And we only scratched the surface on what ThreeJS can do, so check it out if you are curious! Where can I find it? While these “results” are dynamically generated on the fly, they still appear in the GIF grid just like regular ol’ search results, enabling users to animate their words through the standard GIPHY search experience they know and love. Whether you’re building a chat app or a creation tool, these Text GIFs can add a new dimension to your users’ expression and communication. They certainly have for us over here at GIPHY HQ, where we often use them to add a lil extra pizazz to our slack messages and emails. GIPHY Text is available in our Chrome and Gmail extensions, apps like Prequel (and coming soon to byte !), and now in the SDK for developers to integrate. Haven’t yet checked out the GIPHY SDK? Head over to our developer portal and github repos ( iOS , Android ) to get started. You can find details specific to GIPHY Text implementation in our documents too ( iOS , Android ). p.s. The coolest thing about working on the GIPHY SDK is getting to collaborate with developers all over the world building and growing different apps. As we continue to drop new features like GIPHY Text, we want to see what you think and learn how we can help power the best possible GIPHY experience in your app. No idea’s too wild! Drop an issue on Github or say hello @ developers@giphy.com. Previous Post Next Post", "date": "2021-03-18"},
{"website": "Giphy", "title": "GIPHY2Vec: Natural Language Processing @ GIPHY", "author": ["Nick Hasty"], "link": "https://engineering.giphy.com/giphy2vec-natural-language-processing-giphy/", "abstract": "GIPHY2Vec: Natural Language Processing @ GIPHY February 26, 2021 by Nick Hasty At Betaworks Studios’  “ RENDER(lite); GPT-What? ” conference, Director of Machine Learning, Nick Hasty spoke on our in-house NLP tool called “GIPHY2Vec” (a modification of Word2Vec). To learn more, check out the full talk below! For more great content, keep an eye on Betaworks Studios’ Event Calendar . Previous Post Next Post", "date": "2021-02-26"},
{"website": "Giphy", "title": "Part 2: Computer Vision @ GIPHY: How we Created an AutoTagging Model Using Deep Learning", "author": ["Dmitry Voitekh"], "link": "https://engineering.giphy.com/part-2-computer-vision-giphy-how-we-created-an-autotagging-model-using-deep-learning/", "abstract": "Part 2: Computer Vision @ GIPHY: How we Created an AutoTagging Model Using Deep Learning January 11, 2021 by Dmitry Voitekh This is part two of the GIPHY Autotagging blog post series, where we’ll cover modeling, configuration of our training environment, and share our results. In part one , we outlined our motivation for this product and provided an overview of existing related approaches. Additionally, we described the training and evaluation data we have at hand and our custom methodology for filtering and enrichment. If you haven’t read part one, you should check it out (here ) before reading this post. Our Training Environment For this project, we conducted all our experiments on Amazon Sagemaker , and primarily relied on architectures written in PyTorch because of its simplicity, convenience for prototyping, and optimization features. All the code was written using this framework. To ensure reproducibility and full control over different experiments, we leveraged the Catalyst framework , which allows us to structure PyTorch experiments in a clean and concise way, enabling engineers to omit rewriting the same training loop every time. Furthermore, Catalyst provides an intuitive high-level interface over extremely handy PyTorch tools we benefited from: Distributed Data Parallel – distributed mode for PyTorch, allow use of multiple GPU devices Apex – PyTorch extension with NVIDIA utilities for mixed precision and distributed training Torch JIT – modern way to serialize and optimize PyTorch models for inference As you can see, components for training need to be defined in separate files: model, dataset, custom metrics (if any) etc. Afterwards, they can be organized into a training pipeline via config.yml file. To start a corresponding experiment we can use catalyst-dl command-line tool like this: catalyst-dl run -config=all_layers_fine_tuning_v2_snorkel_fb_resnet/config.yml --verbose This command will launch the training routine and record actual code that was executed with all dependencies, checkpoints, and metrics in TensorBoard format. Training and Evaluation As mentioned previously, we decided to consider this task as a multilabel classification, therefore, model outputs sigmoids and Binary Cross Entropy Loss are used as a main criterion. For the learning rate policy we picked Cyclic Learning Rate and Adam for the optimizer — training tools which have worked best for us in the recent Computer Vision projects. A majority of our experiments were done using the ResNext101 model from FB WSL research. Additionally, we came up with the following list of metrics to evaluate our training progress: IOU (Intersection Over Union) For each GIF we take its actual tags (say, N tags) and take top N classes from the model’s output (sigmoids). IOU is an intersection between actual tags and those top N predicted tags (value from 0 to 1) normalized by the size of their union. To aggregate this metric for the batch, we simply compute the mean value of all these intersections. “At-Least-One-Correct” This metric is similar to IOU, but instead of the actual intersection value we calculate the percentage of GIFs from the batch for which IOU is not zero (value from 0 to 1). “All-Correct“ This metric is also based on the intersection described above, but here we count GIFs with 100% IOU between actual and predicted tags (value from 0 to 1). Semantic Accuracy As observed within our experiments, aforementioned metrics values are not as high as we would expect. This can be explained by generalization capabilities of the model: for example, when a GIF depicts a soccer scene and actual tags are “soccer“ and “goal,“ but our model predicts “soccer“ and “ball.“ IOU value for this case is 0.5, which doesn’t reflect that overall the tags are pretty relevant. It means that exact matching results in lower metrics than we subjectively perceive. To tackle this problem we came up with a metric which estimates similarity between actual and predicted tags in the latent semantic vectors space. For this purpose we used an internal semantic model called Tag2Vec that provides meaningful embeddings for tags. We simply map labeled and predicted tags onto this embedding space, compute pairwise cosine distances, and find minmax distance value for the final metric. Experiments Head Fine-Tuning We started with a simple idea: remove the last fully-connected layer of pre-trained ResNext101 model and replace it with our fully-connected layer to map latent vectors onto a set of our tags (not Imagenet labels). Only this new layer is trained — the CNN is frozen. After that, we added more fully-connected layers with RELU activations and batch norms, but it didn’t result in significant improvements. Full Fine-Tuning To go further, we unfreeze all layers in the neural network including convolutional blocks. This change brought the model to a completely new quality level — metrics went up more than 3%. You can see some of predictions made by our model below: Comparison of Facebook WSL ResNet vs Torchvision Imagenet Resnet Since the FB WSL models were fine-tuned on Imagenet, the natural question would be: “why do we use a FB WSL model instead of a more common model, pre-trained on Imagenet, taken from torchvision package?“ To address this concern, we repeated the previous experiment for Imagenet pre-trained ResNet50 model from torchvision and compared it to the ResNet50 WSL model we fine-tuned. What we observed is that FB WSL outperforms a pure Imagenet pre-trained model. Therefore, it seems that representations of the FB WSL model are easier to fine-tune on a GIPHY dataset than a plain Imagenet model. Semi-supervised learning Looking at the results of the previous experiments, we have an assumption that the low performance of the model is related to the noisy training dataset. To verify this, we cleaned our dataset with predictions of our SOTA model by taking the following steps: We take original labels and predictions (sigmoids) from the model for the given training sample If the original label is available in prediction with confidence value bigger than some threshold value, we leave it unchanged Otherwise, the score assigned to the original label is reduced by half All predicted labels with confidence ≥ another threshold value are added to the sample (if not present already) with corresponding score By doing this, we both filter out unconfident labels and generate new ones by using our current best model. We trained a new model on this updated dataset. This strategy allowed us to improve our metrics a bit (approximately 1% improvement of IOU). Teacher-Student learning Since our final goal is to serve this model’s predictions to actual GIPHY users, addressing performance concerns is critical to delivering the best experience to our users. So far, we’d primarily utilized a huge FB WSL model — ResNext101, which inference time on a CPU instance is around 1 second (c5.4xlarge ec2 instance, 16 cores, 32GB of RAM). Therefore, we thought we might follow a modern knowledge distillation approach to get the most out of our SOTA model and transfer its behaviour to a more lightweight model — ResNet50 (also from WSL research) which has much better timing (~200 ms on the same c5.4xlarge instance) and size (approximately 3 times smaller than ResNext101 in terms of parameters number). Our assumption was, this new model’s quality should be on par with a large WSL model. Hereafter, in this section we reference ResNext101 as “ Teacher model” and  ResNet50 as “ Student model .” To train the Student model we used a multi-task loss that is calculated as a weighted sum of two components: Loss based on original labels for the sample Loss based on predictions (sigmoids) of Teacher model for the same sample As it turned out, the Student model performs considerably worse (by 2-3%) according to all metrics. This can also be seen when the model is tested manually. What’s even more interesting, when any component is removed from our combined loss (labels or predictions) it doesn’t affect the metrics. The same effect can be observed when the Student is trained on the original labels only. We can’t sacrifice such a considerable precision gap for the sake of performance, but we’re actively investigating ways to optimize inference speed through various compression techniques. Deployment To serve the AutoTagging model in the production environment we export it via PyTorch JIT . This has multiple benefits, such as compact serialized checkpoint and ability to load and use a model without having an actual model definition in code. To wrap the model into a real-time service we use the Seldon framework . It automatically generates both REST and GRPC services given only a Python class where behaviour of your model is defined. Apart from that, these services are already instrumented in collecting metrics that can be easily exported to Prometheus, Jaeger etc. Also, Seldon provides out-of-the-box ability to run A/B tests to compare different models. Our Seldon environment is running in a Kubernetes cluster and we leverage GPU powered instances for faster inference. The first user-facing interface on the GIPHY platform to integrate the AutoTagging model is the “GIF Edit” modal, which is a popup allowing users to edit one of their uploaded GIFs. As it can seen in the example below, a user is provided with a set of tags suggested by the model. If a GIF doesn’t have tags, all the suggestions come from our AutoTagging model. If there are pre-existing tags on the GIF, we supplement Autotagging Model suggestions with suggestions from a custom NLP model which are semantically similar to the pre-existing tags. To track performance, we track “seen,” “add,” and “reject” events for all tags so we know which tags a user finds appropriate or irrelevant. This data will be used to improve the model over time. We did a preliminary analysis of this feature’s impact on the newly added content for the last month. As a result, we report that following the launch for some of the verified users cohorts: Percentage of uploaded GIFs without tags dropped down by 45% Percentage of uploaded GIFs with one tag dropped by more than 13% Percentage of uploaded GIFs with multiple tags increased by 2% Average number of tags per uploaded GIF increased by almost 39% Even by looking at these simple metrics we can clearly see a positive trend after the model was released. What these numbers basically mean is that new content will be handled more effectively by our search engine and bring more fun to our users. Furthermore, this is just the first iteration of the AutoTagging project and we’ll keep on iterating on both model and UI/UX to provide an even better experience. Creating this AutoTagging model was one of the most challenging and rewarding projects our team has ever worked on, and we’re extremely proud of our results and excited to keep iterating on this model. We hope you found this article helpful, and perhaps it can provide guidance to anyone else working on a similar project. Please reach out to us on Twitter with any questions or comments. — Dmitry Voitekh, AutoTagging tech lead Additionally, many thanks to Proxet and their engineers for their valuable contributions to the project! GIPHY Engineering Signals Team — Nick Hasty — Dmitry Voitekh — Ihor Kroosh — Taras Schevchenko — Vlad Rudenko Previous Post Next Post", "date": "2021-01-11"},
{"website": "Giphy", "title": "GIPHY Search Trends During the 2020 Presidential Election", "author": ["Nick Hasty"], "link": "https://engineering.giphy.com/giphy-search-trends-during-the-2020-presidential-election/", "abstract": "GIPHY Search Trends During the 2020 Presidential Election December 14, 2020 by Nick Hasty Major social and cultural events mean “all hands on deck” here at GIPHY, as millions of people across the world use GIFs to express themselves as these events unfold. Award shows, championship games, and holidays (such as New Year’s Eve) require resources across the company — from our Editorial team “ live-GIFing ” video feeds in real-time to our SRE team making sure everything is running smoothly during big traffic spikes. These major events never fail to introduce new memes and cultural phenomena, like catchphrases, quotes, and video clips, that become popular GIPHY searches. Likewise, existing memes are often recontextualized to provide commentary on, or a reaction to, the new phenomena. In both cases, we have to identify these emerging search trends and understand the user intent driving the trend so we can return the most relevant content. Given the anticipation and uncertainty surrounding the 2020 Presidential Election, we fully expected a multi-day event rife with intense and oscillating emotions and no shortage of new search trends. To prepare for the undoubtedly epic week, we overhauled our internal Trending Search tools and developed new cross-team workflows to create a search experience synchronized in real-time with the fast evolving zeitgeist. Improving Trending Search Identification Search query analysis is one of the primary methods we use to understand how GIPHY users express themselves during major events. Our most common approach is retroactively examining the search queries we received while the event took place to discover trends in the data, and then sculpt a narrative around these trends which explains what people were thinking and how they were feeling. Given the sheer volume of search requests we get in a single day, we are able to identify aggregate emotional states for “society-at-large,” and we’ve published articles demonstrating this. Discerning trends in real-time , however, has historically been more of a challenge since it takes considerable time and effort to process and sift through such a large amount of data. We do track real-time trending searches, and we make these findings available to the public through our trending searches API endpoint , but these results are generated from a relatively small sample set of search data, and do not always provide actionable results for internal use. The Signals Team, which includes myself, took to the task of transforming our internal Trending Searches tool to be more comprehensive, accurate, and helpful in understanding what content our users want. Our team extracts vital “signals” from GIPHY’s vast data stores to improve existing products or create new ones. We primarily use machine learning techniques to do so, things like unsupervised learning for recommendations and CNNs for computer vision. My colleague Taras Schevchenko was the tech lead for updating our Trending Searches tool, and did everything from developing the algorithms we used, to building an internal web interface. Our first step in this process was to revisit what we consider to be a “trending” search, as we had to know exactly what we were looking for in the data before we could start any real work. We discussed this topic with other teams, like Editorial and Search, and came up with a consensus on what we consider to be a “trending” search. Qualitatively speaking, we’re interested in search queries demonstrating a clear relationship to happenings in pop culture, nascent memes, or major events involving sports, entertainment, or politics. Statistically speaking, we’re looking for queries whose recent volume counts reflect a significant change relative to the recent past. To identify volume spikes like this, we treat search volume data for a given query as stationary time-series data generated by a stochastic process. Time series data is a sequence of numbers ordered successively over time, and data can be considered stationary if the numbers “look the same” at any given point in time, i.e. there are no predictable patterns or any sort of seasonality and the values are constant throughout the timeline. A stochastic process is a process that produces numbers with some inherent randomness. So search volume for most queries is best understood as a series of random numbers fluctuating over time driven by a static source, which is user demand for that query. While the exact number of times people search for a particular query in a time-period isn’t predictable per se, as long as demand remains static you can estimate an expected value for volume using an autoregressive model. If the current volume is significantly different than the expected average value outputted by the autoregressive model, usually in respect to some threshold, we can assume something has changed in the process generating the data, i.e. that user demand has dramatically increased (or decreased). When this happens, the data is no longer stationary, and this spike indicates the emergence of a trend. As part of the update, we now look at an 8-day window of non-sampled search activity to examine our time-series data. As mentioned above, most of our query volume data is stationary. We do, however, have a number of queries whose volumes are non-stationary because they have repeatable trends, usually a daily or weekly periodicity, and we want our model to account for these patterns when calculating an expected mean for these queries. For example, “ good morning ” queries spike every morning, and searches for “ TGIF ” spike every Friday. Technically, auto-regressive models are not as reliable for non-stationary data analysis, but it’s well enough for our purposes as the model does help highlight these repeating trends and lets us know if the trend breaks out of its normal pattern. We tested many variations of the model described above by backtesting it on search logs dating to the presidential debates and college football games. We examined the results of the various models until we found one that did a balanced job of identifying spikes in both popular, high-volume queries and unique, low-volume queries. With this, we were ready to take on the live election! 2020 Election Trends We began trend analysis Tuesday November 3rd, when in-person voting officially began. While we expected election-based searches to be prevalent amongst trending searches, we were floored by just how dominant and diverse these types of queries were. Searches related to individual candidates (even Kanye West) had massive volume increases as compared to the previous day, up to 400 times normal volume. Searches around voting in general, such as “ i voted, ” “ did you vote ” and “ waiting in line ” had increased percent changes in the thousands. There was a similar surge in queries expressing anxiety, like “ hyperventilating, ” “ nail biting, ” “ stressed out, ” “ drinking wine ” and “ self-care. ” Likewise, queries expressing anticipation were up tremendously, such as “ hold onto your butts, ” “ fasten your seatbelts, ” “ let’s get ready to rumble, ” “ helm’s deep ” and “ raising head to look. ” It was fascinating to see, and we felt like we had tapped directly into some sort of shared cultural consciousness. Sentiments swirled as in-person voting wrapped-up and it became clear the election was going to last multiple days.. For the period of November 4th, 5th and 6th, we saw lots of queries commenting on the ballot counting process, some humorous (“ Count Von Count, ” “ Zootopia sloth, ” “ slow motion, ”) some reactionary (“ stop the count, ” “ cheating, ” “ land doesn’t vote, ”) and some emotional (“ fat lady sings, ” “ don’t give me hope. ”) Anxiety was still in full force (“ what is happening?, ” “stress eating, ” “ Canada watching US news,” ) and the memorial quote “Looks like I picked the wrong week to stop sniffing glue, ” citing a running gag from the classic comedy Airplane, picked up significant volume as people turned to their vices to cope. As vote totals began reflecting a shift in the electoral college, GIPHY users turned to the Arrested Development meme “ I’m afraid I just blue myself, ” recontextualizing it as a funny commentary on many states changing from red to blue. Searches for individual states and cities trended heavily during this time as well. Georgia-related searches were abundant when that state turned blue (“ John Lewis dancing, ” “ Stacey Abrams, ” “ Georgia on my mind, ” “ Outkast, ” “ the South got something to say, ” “ Designing Women, ”) and Pennsylvania-related searches similarly spiked when that state turned as well (“ flipadelphia, ” “ flip Always Sunny, ” “ It’s Always Sunny in Philadelphia, ” “ Philadelphia freedom, ” “ Philly Phanatic, ” “ Gritty ”). Late Saturday, November 7th, it became clear that Joe Biden was highly favored to win the election and our search trends reflected this news. Searches for the GIF of Vince Carter saying “ its over, its OVER “skyrocketed, as did queries like “ so relieved, ” “ sweet relief ” and “ it’s finally ove r,” as people expressed thankfulness for some closure. Searches for “ Frodo it’s done, ” referencing the final moments of the Lord of the Rings film trilogy, also shot upwards as people needed a gif that captures exhausted relief, as with release of some heavy burden. Searches for scenes from the Star Wars movie franchise were also extremely popular to express a sense of triumph, “ Return of the Jedi ending, ” “ Death Star explodes, ” “ Star Wars celebration, ” “ Ewok party, ” and even “ Admiral Ackbar ” for the skeptics. More general celebratory searches, such as “ dancing in the streets, ”, “ party in the USA, ” “ popping champagne, ”, “ we are the champions, ”, and “ brand new day ” also had tremendous boosts. A New Cross-Team Workflow Having real-time trending data unlocked a series of new cross-team workflows within GIPHY. Eugene Kong , from our Data Science team, tracked trending queries throughout the week, rolled them up into 12 categories, and compared the trends against general search volume. We found that GIPHY’s overall search volume doubled during election week, peaking on Saturday11/7 when the election was called for Biden. Over 50% of all the trending searches we identified during this time period were election related, and nearly 20% of those searches were related to Donald Trump, whether positive, negative, or neutral in sentiment. He plotted the number of trending searches per each of the 12 identified categories during the course of the week. The resulting chart is fascinating, as it captures the week’s rollercoaster of emotions and interests and reinforces the narrative created with the example searches listed above. We can see the week began with lots of general searches regarding the election and voting in general, followed by a spike in searches negative emotions (like anxiety), and ended with lots of searches about individual states, remarks on the actual results, and finally a big spike in searches related to Trump himself. Other teams used this data as well. Our Partnerships team was able to alert our media partners when their content was getting increased views due to being included in search results for trending queries. Our Editorial team used the endpoint to monitor trends as well. For example, they noticed that the query “ So You’re Saying There’s a Chance ” was gaining traction and, realizing this query was a slight variation of the quote “ so you’re telling me there’s a chance ” from 90s hit Dumb and Dumber , they ensured users could also find relevant content for the misquote. The team was able to perform similar actions like this for dozens of queries throughout the week, and the ability to tweak in-demand search results in real time can make huge improvements in our core metrics, like click-through rate (CTR), as users are more likely to click on a GIF when search results better match their expectations. All in all, we’ve been very pleased with our updated Trending Searches tool and how performed during the elections. Our next step is to pass these insights on to our users and integrations through the GIPHY API. Our primary focus now will be on battle-hardening our infrastructure and improving the product based on internal feedback, with the goal of deploying the updated data stream to the Trending Searches API endpoint in early 2021. — Nick Hasty, Director of Machine Learning, Signals Team @GIPHY Previous Post Next Post", "date": "2020-12-14"},
{"website": "Giphy", "title": "Part 1: Computer Vision @ GIPHY: How we Created an AutoTagging Model Using Deep Learning", "author": ["Dmitry Voitekh"], "link": "https://engineering.giphy.com/computer-vision-giphy-how-we-created-an-autotagging-model-using-deep-learning/", "abstract": "Part 1: Computer Vision @ GIPHY: How we Created an AutoTagging Model Using Deep Learning December 3, 2020 by Dmitry Voitekh Motivation GIPHY search is powered by ElasticSearch, where GIFs are represented as documents with various features, such as tags and captions. Tags in particular have a great impact on the search performance. Therefore, at GIPHY Engineering, we continuously track and improve their quality with both manual and automated verification. However, for the last couple of years, the number of GIFs in our library and size of search query space has grown dramatically, making feature maintenance difficult. Since tags are added manually by users who upload GIFs, a portion of the GIPHY catalog is missing relevant tags. Missing tags happen when the user uploading the GIF fails to add tags, assigns tags that are too general, or assigns irrelevant tags. Without tags, this content is unlikely to surface as a result of relevant search queries, depriving creators of well-deserved attention and making some of GIPHY’s search results less complete than they could be. Our approach to alleviating this problem was to create a deep learning model able to predict tags for a GIF, and to present these predicted tags to our users so they can make the final judgement about which tags to include. The goal for the model is to increase the discoverability of our users’ content. Consequently, the tags we predict need to be both relevant and reflect our most popular search queries so that their GIFs might show up in results for those searches. We call this model the AutoTagging Model, and in this two-part blog post, we’ll break down the whole process of its creation, from training, to deployment. Selecting a Base Model Considering the abundance of open-source, performant Computer Vision models, we decided to leverage a pre-trained solution which could be fine-tuned on the GIPHY dataset. Even though existing models are designed to predict labels for a single image, they can be extended to make predictions for an entire GIF by sampling a fixed number of frames and aggregating these frame-based outputs into a single output, either by applying average pooling or learnable pooling (MLP, CNN, RNN, Transformers). We’ll cover our architectural solution in the next sections. We started with a torchvision package that provides a variety of models pre-trained on Imagenet dataset, from tiny and less accurate (MobileNet V2) to massive and performant (ResNext and NasNet). As an alternative, we followed public, open-source research done by Facebook’s AI Team which showcases that by utilizing a huge amount of data with weak supervision (in this case – IG videos and images with noisy hashtags added by authors) in combination with clean labeled data (e.g. Imagenet) we can get an even more accurate model. This can be achieved by using a Teacher-Student training setup . A Teacher model is pre-trained on a weakly supervised dataset, and fine-tuned on a clean dataset. After that, a Student model is pre-trained on a weakly supervised dataset automatically labeled by a Teacher model and fine-tuned on a clean dataset. Facebook evaluated these models on public benchmarks (Kinetics, Imagenet) and proved that this training approach significantly improves accuracy compared to more straightforward strategies. Facebook AI’s WSL models are available via torchhub . We tested both video and image-based models (aggregation is done via average pooling) on our GIFs and couldn’t observe any substantial difference in results subjectively and on our mini benchmark. Moreover, a video model is much slower in both training and inference procedures. Therefore, we decided to limit our further experiments to a model trained on images. “Examples of predictions for GIFs made by FB WSL model fine-tuned on Imagenet dataset” In summary, we kept two options for the base learner: pre-trained models from torchvision (Imagenet) and FB WSL (Instagram + Imagenet). Labels Engineering In the case of FB WSL, pure Imagenet labels were used and extremely diverse Instagram hashtags were projected onto this set of 1K general labels. We can’t follow the same track since we’re not trying to beat any benchmarks: our goal is to build a model that will speak in GIPHY-specific vernacular. In other words, we do not need narrow labels from Imagenet, like “cuirass”, but instead we need popular terms describing general emotions, reactions and other adjectives, such as “happy”, “mic drop”, and “thumbs up.” Also, we don’t want our labels to be too specific assuming chances of failure are considerably high (e.g. classifying a particular celebrity or movie). Apart from that, our tag set should be clean (no misspellings, synonyms, etc) and compact (up to 2-3K labels because of computational reasons). To collect such a set, we started with a complete list of all GIPHY tags and applied various transformations and filtering procedures to obtain our final refined set: → Used our own custom Language Detection Service to leave only English tags → Applied deduplication (removed misspellings, prefixes etc) → Removal of people and movie-related tags via Google Knowledge Graph → Grouping of remaining terms based on lemmatization, stemming, and clustering via domestic semantic tag space → Each cluster is represented by a most common tag As a result, we obtained around 2K tags. This set was manually verified and enriched by our editors with missing popular GIPHY terms leading to approximately 2.5k labels in total. GIFs Dataset To compile our dataset, we collected about 6M of GIPHY’s most popular GIFs and their tags, and then discarded GIFs that were too short or too long. For each GIF, we kept a maximum of 16 frames. We then reduced our initial dataset by only using GIFs that had at least one of the tags from our label set, which left us with a dataset of approximately 3M of GIFs. In order to track performance of models during training, we created a validation dataset based on GIFs with “editorial tags”–tags that were added by GIPHY’s in-house editors–as opposed to tags added by GIPHY users. Our editorial tags are cleaner and more accurate, so metrics measurement on this dataset is more trustworthy than on the random hold-out split or cross-validation. This set includes around 200K GIFs, which are excluded from the training dataset. For the test dataset, we have a small subset of validation dataset that was manually reviewed. Since our dataset is highly imbalanced (see a below diagram), we created a custom training data loader that performs stratified sampling of GIFs on every epoch to ensure even distribution of samples per class. The problem of low-frequency classes is addressed via various augmentations applied for input frames. Since the majority of GIFs have more than one tag, we treated this task as a multi-label classification problem. Therefore, our training sample is an (X, Y) pair, where X = GIF frame and Y = the list of correct labels with confidences. “Distribution of GIFs in the training dataset: number of GIFs (Y axis) annotated with each tag (X axis)” From the very first experiments we could see substantial downsides of annotations in our original training dataset: many GIFS lacked relevant tags while others had irrelevant tags. A small percentage of such content is enough to negatively influence the training, so we performed some automatic enrichment and adjustments of labels by applying an approach we call the “ Experts Voting.” “Experts Voting” Dataset To create this automated pipeline for dataset generation, we collected vector representations for GIFs from various sources: → ResNet50 Imagenet embeddings → TRN MIT visual embeddings → Internal semantic model for GIFs representations → OpenGPT2 representations of OCR results These representations and corresponding original tags are used to create kNN classifiers separately for each modality (based on NMSLIB library ). This simple model allows us to leverage similarity measures in those embedding spaces of GIFs to infer GIF’s tags by looking into the tags of its neighbours. After these models are built we can get the top 10 closest GIFs for each given GIF from each of those models and then compute frequencies for their tags (e.g. tag “cat” appeared in nearest GIFs from the Imagenet model 3 times and 2 times in GIFs for TRN MIT model, therefore the final frequency is 5). Then, in order to give more trust to models that provide more accurate predictions, we adjust our frequencies by multiplying them by weights calculated for each model – the percentage of GIFs for which each model correctly predicted at least 1 tag from a GIF. Model Weights Semantic Model vectors .54 Imagenet vectors .33 MIT vectors .28 OCR vectors .2 After that, modified frequencies are filtered out by a static lower bound to limit the amount of noise and normalized from 0 to 1 to match confidence values in our training dataset. This allows us to significantly extend our training dataset (almost to the size of the full collection of GIFs that we initially processed – 6M) with GIFs with reasonably confident tags. Apart from that, using these models’ results we are (to some extent) able to fix original GIPHY tags in the following way: if a given original tag was present in at least one model’s prediction for a given GIF, we leave it confidence of 1, if it was not – we reduce the initial confidence to 0.7. This processing doesn’t affect validation and test sets as we wanted to keep the original tags intact. “Samples from “Experts Voting” dataset” Stay tuned for our second post where we’ll dive into our experimentation environment, training and evaluation, and finally, deployment. — Dmitry Voitekh, AutoTagging Tech Lead Additionally, many thanks to Proxet and their engineers for their valuable contributions to the project! GIPHY Engineering Signals Team — Nick Hasty, Team Lead — Dmitry Voitekh — Ihor Kroosh — Taras Schevchenko — Vlad Rudenko Previous Post Next Post", "date": "2020-12-03"},
{"website": "Giphy", "title": "GIPHY SDK Debuts its Latest Service, GIPHY Text, with Select Launch Partners", "author": ["Dan Burke"], "link": "https://engineering.giphy.com/giphy-sdk-debuts-its-latest-service-giphy-text-with-select-launch-partners/", "abstract": "GIPHY SDK Debuts its Latest Service, GIPHY Text, with Select Launch Partners October 28, 2020 by Dan Burke GIPHY users have relied on us to help them better express themselves since day one. First, with GIFs, and later followed by Stickers, animated emoji & text, Video, etc. We know, based on the frequent use of captions on GIFs, that sometimes text is important to contextualize visual media. This led the way to the next evolution of our media library — unleashing animated text creation to users via GIPHY Text! Historically, using text has required the written word to handle all the effort of expression. We asked ourselves why is this the case; is it possible to let the text itself enhance your expression? As non-verbal and unwritten cues are lost in our digital world, written communication often requires the reader to interpret, and thus make assumptions, about the underlying meaning of the words sent.  We feel that creative and animated font styles can improve both the sender and the reader’s quality of communication.  Instead of simply using a fire emoji to let someone know what they said was, well.. fire , why not put that in words with flaming text?  And, just in time for Halloween, GIPHY has you covered for all your spooky messages with this creepy style of text. Check it out, below! GIPHY’s goal is to animate the internet, and it’s long past due that text becomes more fun. GIPHY Text will soon be available in a select set of partner apps, including: Clash , Prequel , and TextNow . We expect to roll out GIPHY Text to all SDK users in the coming months. If you’d like to be the first to know when that happens, get on the waiting list here ! — Dan Burke, Director of Product Previous Post Next Post", "date": "2020-10-28"},
{"website": "Giphy", "title": "Spinnaker @ GIPHY", "author": ["Bryant Rockoff"], "link": "https://engineering.giphy.com/spinnaker-giphy/", "abstract": "Spinnaker @ GIPHY August 28, 2020 by Bryant Rockoff Like many companies, GIPHY Engineering has been using Kubernetes for the past several years to help our teams quickly build, compile into containers, and distribute applications to our AWS servers. One of the problems with any Kubernetes distribution is: well, the distribution. There is an amalgamation of tools out there vying for your attention (and, in many cases, your $$$) to help with getting your code out the door in Kubernetes. There are also a plethora of tools to help template your Kubernetes manifests , including Helm, which GIPHY Engineering has been developing internally, specific to our deployment mechanisms. Why Spinnaker? This all leads to the question, both for Kubernetes and CI/CD newcomers: which tools are best to get you up and running? At GIPHY Engineering, prior to Spinnaker, we had largely been handling deployments through a combination of Jenkins and kubectl (manual), via static manifests in a Github repo maintained by the GIPHY Engineering team. However, for the past year the Site Reliability Team has been working to transition our team to Spinnaker , in conjunction with Jenkins and Helm , to help deploy our applications across our multiple K8s clusters and environments in an automated fashion. While security and validation/auditing played a part in this decision, our primary reasons for choosing Spinnaker were: It was easy for us to integrate Spinnaker into our already developed Jenkins pipelines and tooling. (More on this to come.) Spinnaker allowed us the ability to quickly, and easily, distribute our code across multiple Kubernetes clusters efficiently, and to onboard and stand up new clusters in the tool with minimal work. GIPHY Engineering had been looking for a way to handle automated Canary testing and releases, both of which Spinnaker supported out of the box via its Kayenta microservice. There are many ways a team can trigger pipelines in Spinnaker, including Github webhooks, Pub/Sub, and GCS Artifacts. We chose a combination of Jenkins and General Webhooks for our use-case due to the ability for us to quickly iterate and add these into our already developed pipelines. Running the Spinnaker Jenkins and the Jenkins Shared Library In order to get Spinnaker out the door quickly and easily with our current infrastructure, we utilized a proprietary Jenkins Shared Library . This Library to helped facilitate the transition to deployment via our new Spinnaker tooling by allowing us to make simple tweaks to our already developed Jenkinsfile s in our repositories that looked similar to the following: @Library(\"giphy-jenkins-library@master\") _\n...\n...\nstages {\n    stage('Define Spinnaker variables') {\n       steps {\n           script {\n               variablesSpinnaker(environment: spinEnv,\n                   helmDir: \"helm/${env.SERVICE_NAME}\",\n                   imageName: \"${env.JENKINS_JOB_NAME}\",\n                   imageTag: \"${BUILD_VERSION}\",\n                   primaryBranch: \"develop\"\n               )\n           }\n       }\n    }\n\n    stage('Prepare helm package') {\n         steps {\n             kubernetesHelmPush(helmDir: \"helm/${env.SERVICE_NAME}\",\n                 helmPackageName: env.jslHelmPackageName,\n                 helmPackageVersion: env.jslHelmPackageVersion,\n                 helmPackageVersionSha: env.jslHelmPackageVersionSha,\n                 imageTag: env.jslImageTag\n             )\n         }\n     }\n\n     stage('Trigger Spinnaker') {\n         steps {\n             spinnakerTriggerPrep(spinAppName: \"${env.SERVICE_NAME}\"\n             )\n         }\n     }\n} While this is a condensed version of our live Jenkinsfile s, the shared library in its current state handles the following logic: Generating a list of predefined values we send to Spinnaker ( variablesSpinnaker ) Determining if an update to an application’s helm chart has been made, and pushing it to our internal helm repository if necessary Triggering a Spinnaker deployment (either via Jenkins Artifact or Webhook JSON,depending on the needs of the app) Here’s an example of an artifact the Jenkins Shared Library generates (which is archived at the end of the pipelines so Spinnaker can retrieve it): {\n    \"helmPackageVersion\": \"0.1.13\",\n    \"helmPackage\": \"bartender\",\n    \"helmRepository\": \"stable\",\n    \"branchName\": \"develop\",\n    \"environment\": \"staging\",\n    \"imageTag\": \"develop-dc67395-f38b4f1\",\n    \"longCommit\": \"f38b4f17726839a02896846d884fcc9e28bcce0e\"\n} By using a Jenkins Shared Library, the work our developers had put into their pipelines did not need any kind of rewriting, and only about 30-40 additional lines of Groovy to implement. Our Pipeline Once our Jenkins Shared Library was in a state that allowed for our deployment mechanisms, we began working on additional tooling to help get our applications out the door. This included tooling to implement secret distribution via Vault , as well as the introduction of Helm into our ecosystem (including base Helm templates our developers could work off of when migrating their applications). We settled largely on three types (I sense a theme here) of Spinnaker pipelines. Basic Pipelines This is our most basic and simplest pipeline: It is triggered by the completion of a Jenkins pipeline, grabs that pipeline’s generated/archived artifact, installs our secrets from Vault, “Bakes” (Spinnaker speak for “templating”) our Helm chart, and deploys the baked chart to the cluster of our choice. At GIPHY Engineering, we primarily use these pipelines for deploying single-use, or long term, environments, such as staging , which don’t usually involve additional testing suites to run at launch. In regards to the “baking” of our chart, Spinnaker uses a specific Parameter Expression language to help facilitate passing information from your Spinnaker triggers and Jenkins jobs to the bake process, as well as between stages within a pipeline. In practice, this looks something like the following in Spinnaker’s pipeline JSON for Jenkins Artifact pipelines (for our Bake Helm Chart step): {\n  \"expectedArtifacts\": [\n    {\n      \"defaultArtifact\": {\n        \"kind\": \"default.s3\",\n        \"type\": \"s3/object\"\n      },\n      \"displayName\": \"helm-artifact\",\n      \"id\": \"helm-artifact\",\n      \"matchArtifact\": {\n        \"kind\": \"base64\",\n        \"name\": \"helm-artifact\",\n        \"type\": \"embedded/base64\"\n      },\n      \"useDefaultArtifact\": false\n    }\n  ],\n  \"inputArtifacts\": [\n    {\n      \"account\": \"giphy-aws-mgmt\",\n      \"id\": \"initial-artifact-id\"\n    }\n  ],\n  \"name\": \"Bake Helm Chart\",\n  \"namespace\": \"${ trigger[\\\"properties\\\"][\\\"namespace\\\"]}\",\n  \"outputName\": \"${ trigger[\\\"properties\\\"][\\\"helmPackage\\\"]}-${trigger[\\\"properties\\\"][\\\"branchName\\\"] }\",\n  \"overrides\": {\n    \"deployment.labels.github\\\\.com/revision\": \"${ trigger[\\\"properties\\\"][\\\"longCommit\\\"] ?: '' }\",\n    \"env.secretRefs.primary\": \"${ #stage(\\\"Create Secrets\\\")[\\\"context\\\"][\\\"generatedPrimarySecretName\\\"] ?: '' }\",\n    \"environment\": \"${ trigger[\\\"properties\\\"][\\\"environment\\\"] }\",\n    \"fullnameOverride\": \"${ trigger[\\\"properties\\\"][\\\"helmPackage\\\"] }\",\n    \"image.tag\": \"${ trigger[\\\"properties\\\"][\\\"imageTag\\\"] }\"\n  },\n  \"templateRenderer\": \"HELM2\",\n  \"type\": \"bakeManifest\"\n} The key point here is that we are passing a fair amount of information to our Helm chart from our trigger properties sent from Jenkins, which were generated by our Jenkins Shared Library. These properties are rendered and set as Helm Overrides during the bake process. Environment Pipelines Our second type of pipeline here at GIPHY Engineering is the “Environment Pipeline.” This is a Webhook type pipeline. Why? Because we send Spinnaker some additional information, outside of that listed above, then use that information to generate unique environments. Unlike our basic pipeline, which has a minimal amount of “overrides” for Helm, this pipeline type can have many overrides assigned to an application to help facilitate the creation of an entirely new environment. Currently, these environments are unique to a specific application, but work is currently ongoing to allow for our entire stack, and specific applications, to be deployed in tandem. Canary Pipelines Finally, the last type of pipeline we have is the Canary Pipeline. At GIPHY Engineering, this Pipeline has a pretty substantial amount of customization, including several Custom Jobs we created to facilitate copying current Deployment objects for use as “Baseline” versions for canary analysis, as well as the actual Canary Analysis step (which pulls in specific metrics from New Relic and Datadog). The General Pathway for Canary Pipelines As you can see, the Canary pipeline duplicates the currently running production image (v1.0) as a “baseline” for our analysis, and creates a new “canary” deployment to compare the metrics from both. Once the analysis completes, Spinnaker works to determine if the changes deployed are positive or negative based on a score comprised of several metrics. If the score is above a certain threshold, the canary passes, and is deployed. An example for our “web” pipeline can be seen below: In a successful canary test, the baseline is indistinguishable from the canary. How Spinnaker handles a typical release with a successful canary. Of course, not every Canary goes so smoothly. For example, we recently had a Canary test for our web application that saw a large spike in the error rate of our application upon deployment. Spinnaker was able to determine this was problematic, and halted the deployment of that code, based on the generated report. Our developers were able to work on the bug after the canary deployment ended, and ship the change two days later: A failed canary that Spinnaker was able to automatically halt before releasing to 100% production. How Spinnaker handled a failed canary. In the above screenshots, you can see the report that Spinnaker generated for our “failed” canary (which saw a large spike in the application’s error rate), and below it, how our Pipeline reacted to the failed report. When the canary completed, rather than shipping the code out to Production fully, as occurred in our first example, it noted the failure within the Pipeline. It then deleted the Baseline and Canary deployments in production, keeping the currently deployed production code active and intact. Spinnaker at GIPHY in 2020 Moving forward, our team is working on completing onboarding all of our applications into Spinnaker. This includes work on even simpler pipelines for our developers to help migrate applications that haven’t been ported to our new helm based bake systems. These pipelines use the githubFile artifact type in Spinnaker to pull in updated files from Github, based on push events for that file, and deploy them out to Spinnaker. In this case, our compliled.yaml file is generated by Jenkins, and is a compiled version of all of our Kubernetes static objects ( deployments, service, ingress, etc.). We’re also working on internal POCs to help onboard our developers even faster, with improved processes for onboarding new applications into Spinnaker. More on this to come in a future GIPHY Engineering blog post. 😉 Is this for me? Tl;dr — Spinnaker will be a major time investment for your team. Like any Kubernetes tooling, or tooling in general, make sure it fits your team’s needs before going down this path! Our SRE team at GIPHY Engineering is relatively small, so the development time for us to get Spinnaker out the door, with the tooling we envisioned and needed, took about a year in total. This is a substantial time investment into a tool, and one that many startups and small organizations may simply not have the time to invest in. Spinnaker is a beast. Plain and simple. In fact, many large organizations have entire teams dedicated to just maintaining Spinnaker in their infrastructure. It’s something our small team has had to grapple with, and certainly one of the pitfalls of the tool. That being said, when Spinnaker works, it just works; and it works well . While our initial launch was slower than expected, we’ve since had a lot of success with the tool, including catching bugs before they hit production via our Canary pipelines, as well as reducing human error in our previous deployment systems. It has also substantially automated our processes in our deployment pipelines. Additionally, because Spinnaker is so extensible , the amount of customization you and your team can pour into it is nearly limitless. For example, we’ve developed full integration test suites surrounding our pipelines, which can be triggered based on commit messages sent with our Jenkins artifact payloads. More Information You can find all of the information you need on Spinnaker at its OSS website, https://spinnaker.io For teams looking for enterprise level solutions and support, we highly recommend Armory , who we partnered with throughout our development process. Finally, if you want to learn more about how our Spinnaker infrastructure works, including a live demo, you can find a presentation I gave last fall on the topic at a Meetup event in New York City on Youtube . Stay well, and healthy <3 SRE@GIPHY — Bryant Rockoff, Site Reliability Engineer Previous Post Next Post", "date": "2020-08-28"},
{"website": "Giphy", "title": "How GIPHY’s Public API Integrates with gRPC Services", "author": ["Serhii Kushch"], "link": "https://engineering.giphy.com/how-giphys-public-api-integrates-with-grpc-services/", "abstract": "How GIPHY’s Public API Integrates with gRPC Services August 13, 2020 by Serhii Kushch We always work hard at GIPHY to help people find the right GIF at the right time. Adoption of gRPC helps us continue to keep our services fast , stable, and fault-tolerant as we scale to over 10 billion pieces of content a day. When the GIPHY API, which serves content to our third party integrations, was initially developed in 2013, it was monolithic. In 2017, we began splitting it into micro-services to help scale and maintain our service. In the first stage of this separation, the integration between the original PHP app and the new micro-services was done via HTTP. In 2019, we started moving to gRPC. With limited resources and a highly scaled application, the process of integrating gRPC in PHP was a challenging task. In this post, we’ll share our learnings from the process so that your integration will be a bit less tricky. Step 1: generate the client. gRPC comes with a great tool, protoc , that helps with code generation. All you need is to define your service using proto syntax: syntax = 'proto3';\npackage gif.proto;\n    \nservice Gif {\n    rpc getGif(GetGifRequest) returns (GetGifResponse) {}\n}\n\nmessage GetGifRequest {\n    string id = 1;\n    Options options = 2;\n}\n\nmessage GifGifResponse {\n    string id = 1;\n    string title = 2;\n} protoc will take care of everything else: // GENERATED CODE -- DO NOT EDIT!\nnamespace Gif\\Proto;\n\nclass  GifService extends \\Grpc\\BaseStub\n{\n    /**\n     * @param string $hostname hostname\n     * @param array $opts channel options\n     * @param \\Grpc\\Channel $channel (optional) re-use channel object\n     */\n    public function __construct($hostname, $opts, $channel = null)\n    {\n        parent::__construct($hostname, $opts, $channel);\n    }\n\n    /**\n     * @param \\Gif\\Proto\\GetGifRequest $argument input argument\n     * @param array $metadata metadata\n     * @param array $options call options\n     */\n    public function getGif(\\Gif\\Proto\\GetGifRequest $argument, $metadata = [], $options = [])\n    {\n        return $this->_simpleRequest('/gif.proto.Gif/getGif', $argument, ['\\Gif\\Proto\\GetGifResponse', 'decode'], $metadata, $options);\n    }\n} To automate this process we setup Jenkins to generate a composer package with all the clients we need in the API, so when definitions change we just need to update the package version. Step 2: HTTP to gRPC switch GIPHY’s API is a very high-load service and mistakes are very expensive. The first thing we had to think about was achieving an easy and fast switch between HTTP and gRPC services, so we could easily canary the service and roll back if we had any trouble. We added an internal “throttling strategy” for requests distribution: class GifService\n{\n    /** @var GifClientInterface */\n    private $client;\n\n    public function getClient()\n    {\n        if ($condition) {\n            return $this->grpcClient;\n        }\n\n        return $this->httpClient;\n    }\n\n    public function getGif(string $id): ?Gif\n    {\n        return $this->getClient()->getGif($id);\n    }\n} $condition represents the percentage of requests we want to send to a particular service. Depending on our needs, it could be stored either as Redis key or environment variable. But simple solutions are not always easy to implement. Incompatible interfaces, like this, were a common situation for us: class GifHttp\n{\n    public function getGif(string $id, Params $metadataParams, array $options = []): ?Gif\n    {\n    }\n}\n\n// GifServiceGRPC here\nclass GifGRPC\n{\n    public function getGif(string $id, array $fetchOptions = [], bool $forceReadFromMaster = false): ?Gif\n    {\n    }\n} Because this was a frequent issue, we introduced a RequestOptionsInterface interface to resolve incompatibilities: interface RequestOptionsInterface\n{\n    public function getHttpOpts(): array;\n\n    public function getGrpcOpts(): array;\n}\n\ninterface GifClientInterface\n{\n    public function getGif(string $id, RequestOptionsInterface $requestOptions): ?Gif;\n}\n\nclass RequestOptions implements RequestOptionsInterface\n{\n    public function getHttpOpts(): array\n    {\n        return ['http-param' => 'value']; // http-call specific args\n    }\n\n    public function getGrpcOpts(): array\n    {\n        return ['grpc-param' => 'value']; // grpc-call specific args\n    }\n}\n\nclass GifHttp implements GifClientInterface\n{\n    public function getGif($string $id, RequestOptionsInterface $requestOptions): ?Gif\n\t{\n\t\t$options = $requestOptions->getHttpOpts();\n\t\t...\n\t}\n}\n\nclass GifGRPC implements GifClientInterface\n{\n    public function getGif(string $id, RequestOptionsInterface $requestOptions): ?Gif\n    {\n        $options = $requestOptions->getGrpcOpts();\n        ...\n    }\n}\n\nclass GifService\n{\n    /** @var GifClientInterface */\n    private $client;\n\n    public function getGif(string $id): ?Gif\n    {\n        $opts = new RequestOpts();\n        // fill in $opts depending on a client;\n        return $this->getClient()->getGif($id, $opts);\n    }\n} As a result, we were able to define a common interface and make client-specific calls. Error handling Generated clients only have request logic, so we had to implement our own error handling. We use Guzzle as our HTTP client, so we wanted something similar to minimize changes to service layers. The gRPC error model is based on status codes , so it was easy to map them to HTTP codes. For example: We found Guzzle’s handlers and middlewares were well documented, so we reused this concept not only for error handling but also for tracing (e.g., retrying requests, etc.). In general, we have a chain of middlewares: class ErrorMiddleware\n{\n    public function __invoke(): array\n    {\n        [$reply, $status] = ($this->next)();\n        ... // detect, log, raise error\n        throw new $exception($message, $code);\n    }\n}\n\nclass GrpcService\n{\n    public function call(string $method, ...$arguments)\n    {\n        $callStack = function () use ($method, $arguments) {\n            return $this->client->{$method}(...$arguments)->wait();\n        };\n\n        $callStack = new RetryMiddleware($callStack, $metadata);\n        $callStack = new ErrorMiddleware($callStack, $metadata);\n        $callStack = new OpentracingMiddleware($callStack, $metadata);\n        [$reply, $status] = $callStack();\n\n        return $reply;\n    }\n} Benefits: simple pluggable testable Testing Prior to transitioning to gRPC, there were already many existing acceptance tests in API.  We utilized PHPUnit TestListener interface and custom @throttle annotation to manage our throttling strategy and run them over new service: class ThrottleAnnotationListener implements TestListener\n{\n    public function startTest(Test $test)\n    {\n        $annotations = $test->getAnnotations();\n        ... //get throttle annotation value\n        $this->setSwitch($key, $value);\n    }\n\n    public function endTest(Test $test, $time)\n    {\n        // revert switch value\n    }\n}\n\n// Test\nclass ServiceTest extends TestCase\n{\n    public function testSomething()\n    {\n        ...\n    }\n\n    /**\n     * @throttle switch-grpc=1\n     */\n    public function testSomethingWithSwitch()\n    {\n        $this->testSomething();\n    }\n} Step 3: rollout and results The first thing we noticed was how stable communication to gRPC service is. Connection errors were gone after the switch: But there was also something unexpected: request balancing. HTTP/2 maintains long-lived TCP connections, so it does not load balance naturally out-of-the-box. This is what we got when we tested it the first time (throughput distribution per pod of Scala gRPC server): As you see, we had different throughput on each pod. Some pods are overloaded, while others are doing almost nothing. This is because PHP is always reusing existing underlying connections rather than creating new connections for each request. Ideally, L7 load-balancing would be delegated to a service mesh, and our services would not have to be aware of their runtime environment. Istio and Linkerd are two popular service meshes that can be run on Kubernetes and provide L7 load-balancing as a core feature. At GIPHY, we’ve been keenly following the Istio project, and are actively working on its rollout. However, since we are currently tackling the last set of hurdles around minimizing tail latencies and connection exhaustion when communicating with some non-mesh services, we aren’t running all our production traffic through an Istio service mesh. In an engineering organization, the development and rollout of projects can be incongruous. Rather than making a service mesh a prerequisite for rolling out gRPC, we decided to come up with direct solution to the load balancing issue. The solutions we describe here illustrate the considerations that need to be made in the absence of native L7 load-balancing. Our Scala microservices are using k8s API for load balancing. Each Scala gRPC service subscribes to k8s state modification events. (For more about using the k8s API, see https://kubernetes.io/docs/reference/using-api/api-concepts/ .) So Scala gRPC ↔︎ Scala gRPC communication was not suffering from this issue. Throughput and CPU load per pod were pretty even, as we were keeping up-to-date information about what pods we can connect to. Our goal was to achieve the same results with minimal changes in our Kubernetes cluster. So we started digging on the server-side of gRPC services. It turned out, that NettyServerBuilder allows setting of maxConnectionAge property: Sets a custom max connection age, connection lasting longer than which\nwill be gracefully terminated. An unreasonably small value might be \nincreased. A random jitter of +/-10% will be added to it. \nLong.MAX_VALUE nano seconds or an unreasonably large value will\ndisable max connection age. public NettyServerBuilder maxConnectionAge(long maxConnectionAge, TimeUnit timeUnit) Setting this value to 10 minutes causes the server to close connections older than that. When the connections close, it causes PHP to reconnect and re-resolve the service name, thus obtaining up-to-date information about pods. After applying these changes, these were the results: In practice, you need to find a value for maxConnectionAge that satisfies your use-case. Setting it too high will cause load-balancing issues for extended periods of time because the service will re-use the existing connections as long as possible and new pods will barely get any traffic. On the other hand, setting this value too low will cause frequent reconnections and will kill all the benefits of gRPC and HTTP/2. In our case, 10 minutes for maxConnectionAge was a good trade-off. After service deployments, it took around 10 minutes to get an even load on all new pods. Step 4: Adjustments gRPC is flexible, especially when it comes to request settings. You are able to control request time using deadlines. $options = [\n    'update_metadata' => function (array $metadata) use ($app) {\n        $metadata['grpc-timeout'] = [GRPC_DEADLINE];\n        return $metadata;\n    }\n];\n$client = new GifService($host, $options);\n\n// or send it as metadata\n$client->getGif(new GifRequest(), [], ['timeout' => GRPC_CDEADLINE]); So the server is able to detect if the client has canceled request or timeout exceeded and stop execution: 2020-02-02 09:24:27,044 [grpc-default-executor-8479] WARN c.g.s.g.s.SlowCallDumperServerInterceptor$ - Canceled gRPC method WrappedArray(gif.proto.getGif origin=api took 100ms with params gif_id: \"3o6gbbuLW76jkt8vIc\") Conclusions So what was the result? First of all, we improved application health: connection errors decreased dramatically when we rolled out the new integrations. Generated code with additional type checks resulted in more strict types in app, which makes all our code safer. (At the time we were working on integrations, PHP didn’t have typed properties.) We are able to integrate new services faster, as the team only has to learn the Interface definition language and focus only on business logic. — Serhii Kushch, Senior Software Engineer Serhii comes to us via Proxet Previous Post Next Post", "date": "2020-08-13"},
{"website": "Giphy", "title": "Fixing Bugs in FFMPEG GIF Encoding", "author": ["Jacob Graff"], "link": "https://engineering.giphy.com/fixing-bugs-in-ffmpeg-gif-encoding/", "abstract": "Fixing Bugs in FFMPEG GIF Encoding July 23, 2020 by Jacob Graff Here at GIPHY Engineering, we frequently use FFmpeg to resize and reformat GIFs. We generate around 40 different renditions for each GIF uploaded to our platform, so it’s important we do so as efficiently as we can. While FFmpeg is powerful, it was designed for processing MP4 files, and its support for the GIF format is limited. In fact, FFmpeg support for transparency was implemented by GIPHY Engineering, which you can read more about here . From time to time, an editor will report a bug with our rendition processing. One particularly vexing bug first appeared in certain renditions of this lightning GIF . Original lightning GIF (Left), and rendered version (Right). There are a couple issues we see here: In the top right corner of this GIF, the frame appears to be stuck — the pixels from the first frame stay still for the duration of the GIF. The timing is off — the original GIF has a long pause at the end, whereas the resulting GIF has no pause at all, and doesn’t flash convincingly. We soon noticed many of our Stickers had broken renditions — small pieces on the sides would get stuck, just like the lightning. We also noticed this when launching GIPHY Emoji, and the most important Emoji was exhibiting this bug: Notice that some parts of the image get stuck (eg, the top of this GIF) Unwilling to let the world go without pixel-perfect poop, I started hunting down the bug. Bug Hunting The buggy renditions were traced back to an FFmpeg command that we use to re-scale GIFs while retaining the original color palette: ffmpeg -i in.gif -filter_complex \"[v:0]scale=100:-1, split [a][b]; [a] palettegen [p]; [b][p] paletteuse\" out.gif This bug only affected Stickers, which are GIFs with some transparent pixels. This indicated something was going wrong when processing transparent pixels. What was strange, however, was that non-transparent pixels were being affected. The FFmpeg codebase is large and complex, and the filter above is fairly complicated as well. Using a nifty tool in the repo called graph2dot, I generated a plot of each step in the process: Looks complicated. I decided to narrow down where this problem was originating before diving into the code. I started by converting the image to an MP4 file, and then converting it to GIF: ffmpeg -i in.gif -filter_complex \"[v:0]scale=100:-1, split [a][b]; [a] palettegen [p]; [b][p] paletteuse\" out.mp4 && ffmpeg -y -i out.mp4 out.gif The emoji converted to MP4 and then a GIF. Success!… sorta. The stuck pixels were gone, but unfortunately the MP4 format does not support transparent pixels, and so every transparent pixel was set to a default color (green in this case). While this doesn’t solve our problem, it does tell us something important: this bug happens after all filtering, and occurs while converting to the final output. While we usually think of a file format (such as GIF) as telling us how an image is encoded, a format can actually specify several different methods for compressing and encoding image data. These methods are called codecs. Since this bug seemed to be a problem with encoding (given individual pixels are broken, rather than the whole file), the GIF codec (defined in https://github.com/FFmpeg/FFmpeg/blob/master/libavcodec/gif.c ) was a natural place to start searching. This file describes a series of processing steps needed to take FFMpeg’s internal representation of video and translate it into a final GIF output. One reason FFMpeg is so powerful is that all encoders declare a struct (of type AVCodec) that defines the various steps necessary to translate an image into the desired format. This means image formats can be encoded fairly easily (without modifying too much existing FFMpeg code) and it also means, if you understand the various steps that FFMpeg takes to encode an image, you can fairly quickly read and understand an encoder. When FFMpeg uses an encoder, there are three important steps: Initialization (AVCodec.init), in which the various file headers are defined, the media is checked for compatibility with the encoded file format, and memory is allocated. Encoding (AVCodec.encode2), a function run on each frame of the media to encode, which converts each frame into the desired codec. Closing (AVCodec.close), which writes any file trailers and cleans up the memory used. The GIF AVCodec defines these functions as gif_encode_init, git_encode_frame, and gif_encode_close. This bug seems to affect the frame encoding step only — the GIFs are written out appropriately and there are no memory issues, but individual frames have dead pixels. The gif_encode_frame function performs a lot of memory management and setup for each frame, and calls one function to actually process and encode the image — gif_image_write_image . This function defines several steps for processing images. One optimization allowed in the GIF codec is cropping – if the outer pixels of a GIF don’t change between one frame and the next, they don’t need to be included in the next frame – instead, the frame defines a rectangle containing all of the pixels that needs to be updated, leaving out the pixels that can remain the same. This saves space in the file, especially for GIFs which have large blocks of pixels that don’t ever change (such as most stickers). Due to differences in how this cropping works with transparent and non-transparent pixels, there are two functions that do this work: gif_crop_translucent, and gif_crop_opaque. Recall the bug we’re trying to solve only affects transparent GIFs, and it seems to only affect the sides of the image — two strong signs this is a transparent cropping bug. Since cropping is an optional optimization, the easiest way to verify this was the cause of the bug was to simply comment out the call to gif_crop_translucent, and re-render: Transparent GIF without optimizations. It worked! So the gif_crop_translucent function is the source of the bug. Unfortunately, because this fix turns off optimization, it’s not suitable for production systems (even a small increase in file size can make a huge difference in cost at scale). So we need to fix the cropping function. Root Cause While FFMpeg has many strengths, one weakness is that the code is sparsely documented. In this case, all we have is a function signature and our best guess as to what each argument is for. Using the body, we can piece together what the function is trying to do. Let’s take a look at the first cropping step, crop top : // crop top\n// loop from the first line of the image until the end\nwhile (*y_start < y_end) {\n\n    // We haven't yet seen a non-transparent pixel\n    int is_trans = 1;\n    \n    // loop through each pixel on this line\n    for (int i = 0; i < w; i++) {\n    \n        if (buf[w * *y_start + i] != trans) {\n            // if the pixel at the buffer location isn't transparent,\n            // we've found the location to crop to\n            is_trans = 0;\n            break;\n        }\n    }\n\n    if (!is_trans) {\n        // this line contains a visible pixel, \n        // so we can't crop any further downwards\n        break;\n    }\n    \n    // since we've made it here, the entire line is transparent\n    // we can safely crop this line, so let's move on to the next one\n    (*y_start)++;\n}\n\n// at this point, y_start will point to the lowest line of the image we can\n// crop to from the top down See the bug? When I first read this code, I sure didn’t. Nothing in here jumps out as obviously incorrect. In fact, without knowing exactly what data is being passed to this function, I don’t think it’s really possible to debug. At this point, my first instinct was to use gdb , the standard debugging tool for C developers. But, after poking around at the various parameters and trying to find anything obviously incorrect, I was empty handed. The biggest problem was that I couldn’t see the actual image data — binary data is not particularly amenable to command line inspection. What if I could see the actual image as it was being modified? Writing images in C can be difficult, especially since we’re already fighting with an image processor! However, there is a file format that is particularly amenable to the kind of quick hack we’re looking for here: PPM . Plain PPM files are textual representations of image, which look like this: P3\n# feep.ppm\n4 4\n15\n 0  0  0    0  0  0    0  0  0   15  0 15\n 0  0  0    0 15  7    0  0  0    0  0  0\n 0  0  0    0  0  0    0 15  7    0  0  0\n15  0 15    0  0  0    0  0  0    0  0  0 The first line is the filetype identifier, the next two numbers the image dimensions, and the next one the total color depth. After that, each three numbers represent the red, green, and blue components of a pixel. The file above defines this image: The image defined in the above PPM file (Enlarged) This format is so simple, and yet it allows you to write out arbitrarily large images with only a few lines of C code. For a good idea of how powerful it is, I’d recommend reading “ Deciphering the Business Card Raytracer “, which uses PPM as a final output format. I decided to modify the cropping function quite a bit: static void gif_crop_translstatic void gif_crop_translucent(AVCodecContext *avctx,\n                                 const uint8_t *buf, const int linesize,\n                                 int *width, int *height,\n                                 int *x_start, int *y_start)\n{\n    GIFContext *s = avctx->priv_data;\n    int trans = s->transparent_index;\n\n    /* INITIALIZE PPM FILE */\n    char poop_file_name[256];\n    sprintf(poop_file_name, \"poop_ppm_%d.ppm\", avctx->frame_number);\n    FILE *fp = fopen(poop_file_name, \"w\");\n\n    fprintf(fp, \"P3\\n%d %d\\n255\\n\", *width, *height);\n\n    /* Write PPM data*/\n    while (*y_start < y_end) {\n        for (int i = 0; i < *width; i++) {\n            if (buf[*width * *y_start + i] != trans) {\n                fprintf(fp, \"0 0 0 \");\n            } else {\n                fprintf(fp, \"255 255 255 \");\n            }\n        }\n\n        (*y_start)++;\n        fprintf(fp, \"\\n\");\n    }\n    fclose(fp);\n} In this case, I kept only the first while loop; it can loop over the entire image, so there’s no need to do so again. I only slightly modified the body of the loop – it prints a white pixel for every transparent pixel it sees (PPM doesn’t have transparency), and if it sees a non-transparent pixel it prints a black pixel instead. This goes on until the end of the image. For each frame, this would generate a PPM image. I ran the command, and looked at the first frame (rendered here as a PNG): This does not look like poop! This… does not look like poop. Although it’s frustrating to not see the right image, this is actually a good sign because it looks like the bug is still being reproduced (albeit with a different symptom), so we’re on the right track! In fact, although it’s not immediately apparent, the generated image shows us everything we need to know to solve the problem. Readers with sharp eyes may have noticed that the image creates an optical illusion — two sets of lines, criss-crossing the image, seem to appear if you unfocus your eyes slightly and look for patterns. If you’re having trouble visualizing this, I’ve added guidelines showing where some of the lines appear: The non-poop image, enlarged. While it may seem like an irrelevant detail, this phenomenon is the key to understanding the bug. To understand why, we need to understand the most common cause of artifacts like these in computer generated imagery: aliasing . In general, aliasing occurs as a result of converting a continuous signal (such as a sound or picture) into a discrete signal (such as an mp4 or GIF file). Specifically, it appears when a signal is “undersampled”: the computer samples the signal at such a low rate it misses vital information, and patterns (that don’t exist in the original signal) can appear. In this case, we’re not undersampling; all the data provided is faithfully reproduced in our output. However, there is an important frequency component that we control here: the width of the image. PPM files are just a continuous stream of pixels, with the width specified at the beginning determining when the image wraps around. This means by simply altering the width of the image, we can alter the frequency of line wraps, and thus the final sample frequency. In theory, this should alter the artifacts that appear on the image, and I decided to use this to gather additional information. I started with a big jump, setting the image size to 200 x 50: Still not poop. Interesting! Still not what we want, but the pattern there is now showing some significant white gaps – since the poop image has large sections of transparency on either side, I took this as a good sign. On a whim, I decided to try setting it to 170 x 50: Hints of Poop! Woah! Obviously, this isn’t exactly what we want, but there is a clear representation of poop in there. At this point, it’s pretty clear that the width is the issue, but I was getting impatient with generating these images over and over, and so I wrote a script to iterate over a wide range of possible widths and generate an animation: Sometimes poop! It’s there! Specifically, one frame: The poopy frame. This image had a width of 128 pixels – so that’s the “correct” sampling frequency. As it happens, 128 is a power of 2, and powers of 2 are important in computer science, because computers represent numbers in binary. Although the number could have been anything, the fact that it was 128 tells us that a programmer probably chose that number, meaning we can probably find that number in the code. As it turns out, we don’t have to look far. Recall there are two different cropping functions that can be called – gif_crop_translucent and gif_crop_opaque. They work pretty similarly, but gif_crop_opaque doesn’t exhibit this bug. If we can figure out how gif_crop_opaque calculates the width, we can figure out what we’re missing. The bug comes from the line if (buf[w * *y_start + i] != trans) . This is a common method of storing and accessing two dimensional data sets in computers. Just like the PPM file we looked at earlier, the data is stored as a single, one-dimensional list, since computer memory only has one dimension. This means  an image with a width W and a height H could be stored in memory as a list of pixels with length W * H. Then, to access a pixel at location (x, y), the address in the list would be calculated as x + y * W The y * W calculates an offset into the list so that the line is correct, and x is added to it to get the correct pixel in that line. So in this case, the image is laid out in the array buf, and we’re accessing the pixel at (i, *y_start). Not really! Whoever wrote the gif_crop_translucent function assumed, quite logically, that buf, the two dimensional array we’re using, had the same width and height as the image itself, and used that width in the calculation: const int w = avctx->width; . However, gif_crop_opaque calculates this differently : buf[y*linesize + *x_start] The variable linesize ultimately comes from a struct called AVFrame, and a quick look at the documentation tells us the problem: * @note The linesize may be larger than the size of usable data -- there\n     * may be extra padding present for performance reasons. So the buffer has been passed to us with padding. Since the image width is different from the width of the buffer, we’ve been looking in the wrong place for pixels! This makes the fix rather simple – replace all uses of w with linesize. The result: Perfect poop! Success! And 509 bytes smaller than the uncropped version, so we know the cropping has worked. As a final test, let’s run the lightning GIF through again: Lightning is close. Uh oh — the cropping bug has disappeared, but the timing is still off. As it turns out, this was a different bug, related to how GIFs define the timestamp of each frame – we may consider how this bug was solved in a future post. — Jacob Graff, Software Engineer, Content Engineering Previous Post Next Post", "date": "2020-07-23"},
{"website": "Giphy", "title": "Modifying FFMPEG to Support Transparent GIFs", "author": ["Bjorn Roche"], "link": "https://engineering.giphy.com/modifying-ffmpeg-to-support-transparent-gifs/", "abstract": "Modifying FFMPEG to Support Transparent GIFs July 9, 2020 by Bjorn Roche A sticker (left) is just a GIF (right) with transparent pixels. Here at GIPHY, we differentiate between GIFs and Stickers in our business language, as the two products are served to different searches and customers. However, we still use the GIF format to store stickers – all they really are is GIFs with transparent pixels. The distinction exists in our engineering toolchain as well – some tools struggle to correctly support Stickers (usually due to transparency). One of those tools is FFMPEG, an extremely popular package for working with video in a wide variety of formats. At GIPHY, we use FFMPEG to process all uploaded content – including GIFs and Stickers. FFMPEG is fast, high quality, and open source. Until recently, however, FFMPEG did not properly support writing GIFs that are animated and have transparent pixels. Although we have other tools at our disposal, such as Gifsicle , we want to use as many of the same tools as possible, especially ones that are powerful, popular and fast like FFMPEG. For this reason, we decided to tackle the issue ourselves. Why FFMPEG Doesn’t Support Transparencies To understand the issue that FFMPEG had writing transparent GIFs, you need to understand exactly how transparencies work in the GIF format, and how FFMPEG was handling it. In a GIF, any pixel can take on any one of 256 colors defined in a palette. Optionally, one of those colors can be transparent. Non-transparencies are 100% opaque, meaning the GIF format does not support an “alpha” in the way that some formats, like PNG, do. If the GIF is a single-frame (i.e., a still image), how to handle the “transparent” color is pretty obvious: the tool rendering the GIF just needs to know to not draw that pixel. However, it turns out that if there are multiple frames, (i.e., an animated GIF), there are several possible ways to handle transparencies. These are called “disposal methods.” The first disposal method is called “Restore to background color” in the GIF specification. We’ll call it RTB, for short. It does what you’d expect: when a new frame is drawn, the area of the image is set to be completely transparent and only non-transparent pixels get drawn on top of that. The second disposal method is called “Do not dispose,” and it turns out that using the “Do not dispose” (or DND) method, GIF viewers can, instead, build a new frame using the last frame as the starting point. With this method, transparent pixels serve the purpose of preserving the color of the pixel from the previous frame. For example, in the following GIF, a lot of pixels don’t change color between frames, so we can use transparency for all the unchanged pixels. Transparency can be used to optimize GIFs like this. When using the DND method, only pixels that were transparent in the previous frame can be made transparent again in the current frame. DND could be used for Stickers if the pixel transparency doesn’t change from frame to frame, but that’s an unusual use-case for animation. Most stickers have different transparent pixels from frame to frame. This sticker is a rare exception. Although this method isn’t generally useful for stickers, it can be very useful in cases where GIF encoders want to use different sets of colors on different frames ( which can be used to improve the quality ), and for reducing file size because multiple transparencies in a row are easier to compress than a bunch of different colors. The problem in FFMPEG was that it only supported the DND method, so single frame GIFs and GIFs without transparency both worked fine, but GIFs that had multiple frames and transparency did not. If you are thinking that clearly the solution was to add support for the RTB method, you’re right, and that’s just what we did. (There are some other disposal methods. You can read the full spec if you are curious and this article has more non-technical info on disposal methods and how they work.) How We Fixed It The reduction in file size that comes from the DND method can be significant, so we want to be able to use it when possible, but also use RTB when needed. The solution we came up with was to write a function that analyzes each frame to determine which method to use, and then write the frame using the correct method. The function for analyzing is fairly straightforward: after some setup, it simply loops through all the pixels in the buffer, and compares the color of each pixel to the transparent color. As soon as it finds a transparent pixel, it knows that the frame is translucent (i.e., partially transparent) and returns. If it is unable to find a transparent pixel, it knows the entire frame is opaque. // returns true if any of the pixels are transparent static int is_image_translucent(AVCodecContext *avctx,\n                                 const uint32_t *palette,\n                                 const uint8_t *buf, const int linesize)\n {\n     GIFContext *s = avctx->priv_data;\n     int trans = s->transparent_index;\n     int p;\n     const int m = avctx->width * avctx->height ;\n\n     if (trans < 0) {\n         return 0;\n     }\n\n     for (p=0; p < m; ++p) {\n         if (buf[p] == trans) {\n             return 1;\n         }\n     }\n     return 0;\n } A slightly more elaborate check is possible (e.g., we could see if the pixel is transparent in the previous frame as well), but for the vast majority of cases this works well. Once we added this function, we also needed to create a method for writing translucent frames, and rename the existing frame-writing function to indicate that it only works for opaque frames. This change is a lot of code, but it’s fairly straightforward. You can find the new method in the Github diff . The only hard part that remained was getting FFMPEG to store the correct frame disposal method in the GIF file itself, so that the right method would be used when rendering the GIF. In principle, this is pretty simple, but the way FFMPEG divides up the work makes it a bit tricky. The raw frame data and frame metadata are written in two very different places, and communicating between those two places is not straightforward. FFMPEG is structured this way to allow a clean separation between the container (which defines the overall structure of the file) and the codec (which defines how the actual image data is compressed). Since some containers support many codecs and many codecs can go in more than one container, this structure makes FFMPEG extremely flexible. However, in the case of GIFs, which don’t have as clear a distinction between the codec and the container it can get confusing, and the way FFMPEG splits the two is somewhat arbitrary. To resolve the issue, we had two options. The first was to move all the relevant code into the file that handles the codec. There’s a good case to be made that disposal method and other frame header data is part of the codec and therefore should be moved. However, we decided to go for a more surgical approach that would have less impact on the code overall. We added a new type of “sidecar” data, which is a structure in the FFMPEG code that is specifically designed to allow the codec and container code to communicate. Since the sidecar data is logged, tested and compared, this option required changing existing tests to accommodate the new data. In retrospect, it’s still not completely clear which approach would have been best. The complete changes we made are in Github . Try This At Home Now that we’ve made the changes needed to support writing GIF Stickers with FFMPEG, we thought it would be fun to demonstrate one way it might be useful, even if you don’t have any Stickers handy. So, we wrote a shell script that takes video input, analyzes it, and outputs a GIF Sticker. This script is purely a proof of concept -- we only tested it on a few green-screened images -- but hopefully it's good enough to see how this might be useful. If you are looking for something more robust and production-ready, check out unscreen.com which has a service that does this using much more sophisticated analysis. Above are some green-screen GIFs (left) and the stickers we made from them (right) using our simple script. For these examples to work, you’ll need ImageMagick and a new version of FFMPEG installed on your machine. The script uses FFMPEG’s built-in ability to identify colors and replace them with transparency. Now that GIF transparency is supported in FFMPEG, the only hard part is figuring out what color (or, as we’ll see, range of colors) should be removed. The rule of thumb we came up with is as follows: Segment the corners of the first frame of the image. (Which corners we use is an argument passed to the script at the command line.) Find the average color of the corners. Get some additional statistics on the image to establish the full range of colors we need to remove. Replace the range of colors with transparency . STEP 1: Segment the corners To do all this in shell, we start by pulling out the first frame of the image using FFMPEG and save it as a PNG: ff=${WORKDIR}/frame1.png\nffmpeg -i \"$input\" -vframes 1 -an -ss 0.0 $ff The next step is to cut out the corners we want: convert $ff -crop ${CROPPERCENTAGE}%x+0+0 ${WORKDIR}/crop1.png STEPS 2 AND 3: Find the Average Color and Get some additional Statistics Analyzing the corners with ImageMagick to get the numbers we need is a bit of a trick, but once you combine all of the corner pieces into one using montage and setup the arguments correctly we can get all the data we need. #now montage the corners into one:\nmontage -geometry ${width}x${height}+0+0 -tile 1x ${WORKDIR}/crop?.png ${WORKDIR}/montage.png \n\n#get stats for the montaged image\nfmt=\"%[fx:int(255*mean.r)] %[fx:int(255*standard_deviation.r)]\"\nfmt=\"$fmt %[fx:int(255*mean.g)] %[fx:int(255*standard_deviation.g)]\"\nfmt=\"$fmt %[fx:int(255*mean.b)] %[fx:int(255*standard_deviation.b)]\"\nfmt=\"$fmt %[fx:int(255*mean)] %[fx:int(255*standard_deviation)]\"\nvals=(`convert ${WORKDIR}/montage.png -intensity average -format \"${fmt}\" info:-`)\nfor i in 0 1 2 3 ; do\n\tave[$i]=$(( ave[i] + vals[i*2] ))\n\tdev[$i]=$(( dev[i] + vals[i*2+1] ))\ndone STEP 4: Replace the range of colors with transparency Now that we know what colors to remove, we can finally build our Sticker. This is not as straightforward as you might think, because outputting a GIF with FFMPEG is a two step process. In the first step, we need to analyze the colors in the image and find the 256 colors that can be used to best represent the GIF. This set of colors is called the “pallette.” \"[0:v]chromakey=$hexcolor:$similarity[a];[a]palettegen[b]\" -map \"[b]\" $WORKDIR/palette.png || die \"Can't make palette\" Then we use the palette to actually output the GIF. ffmpeg -v error -i \"${input}\" -i $WORKDIR/palette.png -filter_complex \"[0:v]chromakey=$hexcolor:$similarity[trans];[trans][1:v]paletteuse[out]\" -map \"[out]\" -y \"$output\" || die \"can't make final video\" The -filter_complex \"[0:v]chromakey=$hexcolor:$similarity[trans];[trans][1:v]paletteuse[out]\" part of the command to FFMPEG sets up two filters: the first is a chromakey filter that replaces colors close to $hexcolor (which is the average color from the above step) with transparency. The second filter, paletteuse, changes RGB color to indexed color using the color pallet we built in the previous step. One thing we haven’t explained is where that $similarity value comes from or what it does. It’s a signal to FFMPEG about how close a color needs to be to the average color. We use the standard deviation we find from the corner images. We multiply that by a “spread” value given at the command line, which is really just a “fudge factor” for our hastily made script that doesn’t always work perfectly. Some more tweaking with the statistics output by ImageMagick might be able to eliminate this fudge factor and make the script easier to use, but for demonstration purposes, we thought this was a good place to start. Here’s the complete script if you want to try it yourself. What Remains As we alluded to above, the script is not perfect — among other things, it still requires some fiddling to get the right value that removes all the background and none of the foreground, but we thought it was a good way to show off something cool that you can’t do easily with another command-line tool. There’s another problem, though: if you look at the Sticker we made, you’ll notice some glitches. We’ll talk in a future blog post about what those are and how we fixed those as well. — Bjorn Roche, Head of Engineering, Management Previous Post Next Post", "date": "2020-07-09"},
{"website": "Giphy", "title": "How we made FastText faster", "author": ["Taras Shevchenko"], "link": "https://engineering.giphy.com/how-we-made-fasttext-faster/", "abstract": "How we made FastText faster April 24, 2020 by Taras Shevchenko FastText is a library for efficient text classification and representation learning. Like its sibling, Word2Vec , it produces meaningful word embeddings from a given corpus of text. Unlike its sibling, FastText uses n-grams for word representations, making it great for text-classification projects like language detection, sentiment analysis, and topic modeling. Here at GIPHY, we use FastText for search query analysis tasks like language prediction and query clustering. Given the tremendous volume of distinct queries we receive every day, we need these tasks to be as performant and low-latency as possible so that our analytics update within a reasonable and useful time-frame. In this post, we break down how we optimized FastText in various ways, from compilation to training to inference. Follow along to learn how we did it so you can optimize FastText wherever you’re using it. FastText Overview Fastext supports both supervised and unsupervised  (cbow, skip gram) training modes, model quantization and automatic hyperparameter tuning. Facebook has published pretrained English word vectors, as well as multilingual word vectors for 157 different languages. Out of the box we can use FastText from bash, C++, and Python. In terms of architecture, FastText is written in C++ and can be compiled with C++11 and newer versions of C++. Typical C and C++ applications have multiple translation units. This allows developers to have independent compilation of different parts of the program, but it blocks some optimizations. What slows down C++ applications Slow algorithms, inefficient data structures, function calls, memory allocations, and cache-misses make our applications slow. With better algorithms, we can do fewer basic operations, which leads to better performance. Polylogarithmic, linear, and quasilinear algorithms guarantee the scalability of our applications. With efficient data structures, we can save memory and guarantee predictable memory access patterns. Predictable memory access patterns are particularly important. For modern CPUs, RAM behaves pretty much like a hard drive. L1 cache reference costs half a nanoseconds, branch misprediction is ten times more expensive, but the main memory reference is 200 times slower than L1 cache access. So, the performance is dictated by memory access patterns, pluses are free, divisions are expensive, function calls and cache-unfriendly data structures are catastrophic for the performance of our software, and algorithms with the same asymptotic complexity can have surprisingly different performance . Performance Enhancements We will introduce two kinds of performance improvements: build-level and code-level. Let’s start our journey and see how deep the rabbit hole really is. We will take the first 35,653,488 bytes from English Wikipedia and perform some initial measurements. The dataset can be found on Matt Mahoney’s website , and we will do some preprocessing as suggested on the FastText website: $ git clonegit@github.com :facebookresearch/fastText.git $ cd fastText $ wget -c http://mattmahoney.net/dc/enwik9.zip -P data $ unzip data/enwik9.zip -d data $ perl wikifil.pl data/enwik9 > data/fil9 $ head -c 35653488  data/fil9 > data/fil9.tiny For all our experiments we will use: – gcc (GCC) 9.2.1 20190827 (Red Hat 9.2.1-1) – Fedora 31 – Linux 5.5.7 kernel – Intel Core I7 7700HQ – HyperX Impact 32GB Kit (2x16GB) 2400MHz DDR4 CL14 260-Pin SODIMM Laptop Training We will measure training time for a single core. This allows us to get reproducible models, because even with the same random seed it is impossible to get identical results on multiple cores. It will also reduce any uncertainty in our measurements. Let’s start with a Makefile section, which captures useful information out of the execution process: bm: sync; echo 1 > sudo tee /proc/sys/vm/drop_caches sync; echo 2 > sudo tee /proc/sys/vm/drop_caches sync; echo 3 > sudo tee /proc/sys/vm/drop_caches mkdir -p benchmarks\n\n/usr/bin/time -p  -o benchmarks/$(BENCHMARK_NAME).time.txt perf record -o benchmarks/$(BENCHMARK_NAME).perf.record  -g ./fasttext skipgram -input data/fil9.tiny -output result/skipgram.fil9.tiny.1.2147483563.$(BENCHMARK_NAME) -thread 1 -seed 2147483563 -verbose 0\n\nperf stat -o benchmarks/$(BENCHMARK_NAME).perf.stat.v1 -g ./fasttext skipgram -input data/fil9.tiny -output result/skipgram.fil9.tiny.stat.1.2147483563.$(BENCHMARK_NAME) -thread 1 -seed 2147483563 -verbose 0\n\nperf stat -o benchmarks/$(BENCHMARK_NAME).perf.stat.v2 -g ./fasttext skipgram -input data/fil9.tiny -output result/skipgram.fil9.tiny.stat.1.2147483563.$(BENCHMARK_NAME) -thread 1 -seed 2147483563 -verbose 0\n\nperf stat -o benchmarks/$(BENCHMARK_NAME).perf.stat.v3 -g ./fasttext skipgram -input data/fil9.tiny -output result/skipgram.fil9.tiny.stat.1.2147483563.$(BENCHMARK_NAME) -thread 1 -seed 2147483563 -verbose 0\n\nperf script -i benchmarks/$(BENCHMARK_NAME).perf.record > benchmarks/$(BENCHMARK_NAME).perf.record.script\n\nstackcollapse-perf.pl benchmarks/$(BENCHMARK_NAME).perf.record.script > benchmarks/$(BENCHMARK_NAME).folded\n\nflamegraph.pl benchmarks/$(BENCHMARK_NAME).folded > benchmarks/$(BENCHMARK_NAME).folded.svg stat ./fasttext > benchmarks/$(BENCHMARK_NAME).stat size ./fasttext > benchmarks/$(BENCHMARK_NAME).size This script does a few useful things: Drops caches of the operating system. Records the execution process with perf. Keeps track of the performance of counters. Generates a flame graph from the information, which was recorded by perf. Experiments We’ve added -fno-omit-frame-pointer to CXXFLAGS. This allows us to record the call graph. Before making progress, we’ll introduce six groups of compilation flags and add them to the Makefile. It is possible to use other optimization techniques such as AudoFDO , which is also known as Profile-guided optimization( PGO ), but it is out of the scope of the article. Since the training process takes a long time, we didn’t have time to launch all the configs many times. We did ten measurements and took the median. We have not observed serious variance in the results. Let’s measure the speed of training process: export BENCHMARK_NAME=opt__fb FASTTEXT_CONFIG=fb && make clean && make opt && make bm This script will generate a few files: We will generate similar files for each group of CXXFLAGS. The training process takes 312.663296 seconds. Perf had enough time to gather all the useful run-time information from our CPU.  So, we can take a look at our flame graph. From the flame graph above we can see that most of the time we are doing linear algebra and other mathematical operations. The flame graph is a little bit noisy because Perf tries to be as friendly to our program as possible. Another perf app, called perf-stat, tells us about task-clock, context-switches, cpu-migrations, page-faults, number of CPU cycles, instructions, branches, branch_misses, and the time of execution. We can find this information in *.perf.stat.v* files. The simplest way to improve the performance of floating-point operations is to allow the compiler to generate SIMD instructions. In the case of gcc we can add an option -ffast-math. It encapsulates several other optimizations. But let’s start with a safer operation: link-time optimization. This option runs the standard link-time optimizer. When invoked with source code, it generates GIMPLE (one of GCC’s internal representations) and writes it to special ELF sections in the object file. When the object files are linked together, all the function bodies are read from these ELF sections and instantiated as if they had been part of the same translation unit. To use the link-time optimizer, -flto and optimization options should be specified at compile time and during the final link. It is recommended that you compile all the files participating in the same link with the same options and also specify those options at link time. OBJS = args.o autotune.o matrix.o dictionary.o loss.o productquantizer.o densematrix.o quantmatrix.o vector.o model.o utils.o meter.o fasttext.o\nINCLUDES = -I.\n \nopt-flto: CXXFLAGS += -O3 -funroll-loops -DNDEBUG -flto\nopt-flto: fasttext\n \ncoverage: CXXFLAGS += -O0 -fno-inline -fprofile-arcs --coverage It is important to know the FastText team added the -DNDEBUG option, and according to our measurements, the absence of this option increases the training time 1.44X. This option discards unnecessary run-time checks, like assert calls. With -flto we won an extra 1.04% in performance, nothing impressive, but keep reading to see some more significant improvements. Let’s add the fast-math option to the Makefile. OBJS = args.o autotune.o matrix.o dictionary.o loss.o productquantizer.o densematrix.o quantmatrix.o vector.o model.o utils.o meter.o fasttext.o\nINCLUDES = -I.\n \nopt: CXXFLAGS += -O3 -funroll-loops -DNDEBUG -flto -ffasth-math\nopt: fasttext\n \ncoverage: CXXFLAGS += -O0 -fno-inline -fprofile-arcs --coverage With a few keystrokes we’ve achieved 1.29X performance improvement. Let’s gather the final results in a table. We’ve  sped up the training process by 1.29X, but what about the inference stage? Can we do even better? Inference Here comes a hard part — because we will be dealing with microseconds and nanoseconds. With such tiny numbers, we’ll need microbenchmarks to support our results. We’ll use Google Benchmark, a library to benchmark code snippets similar to unit tests, and combine it with  FastText and Perf.  In all cases, the number of iterations for which the benchmark is run is governed by the amount of time the benchmark takes. Concretely, the number of iterations is at least one, not more than 1e9, until CPU time is greater than the minimum time, or the wallclock time is 5x minimum time. The minimum time is set per benchmark by calling MinTime on the registered benchmark object [ google benchmark ]. For our microbenchmarks, we will use a pretrained model for language identification and test out three functions from the FastText library: BM_get_word_vector BM_get_nn (for this task we use an amazing library ) BM_predict_line (prediction performance in supervised mode) #include #include #include \"fasttext.h\"\n \nfasttext::FastText& get_fasttest_model() {\n\tstatic bool is_initialized = false;\n\tstatic fasttext::FastText fasttext_model;\n\tif (is_initialized) { return fasttext_model;}\n\tconst char* path_to_fasttext = getenv(\"FASTTEXT_MODEL\");\n\tif (path_to_fasttext == nullptr) {\n       \t\tstd::cerr << \"There is no model\\n\";\n    \t\tstd::exit(1);\n\t}\n\tfasttext_model.loadModel(path_to_fasttext);\n\treturn fasttext_model;\n}\n\n// the code benchmark\n\nBENCHMARK(BM_get_word_vector);\nBENCHMARK(BM_get_nn);\nBENCHMARK(BM_predict_line);\nBENCHMARK_MAIN(); BM_get_word_vector For this first function benchmark, we will preload the FastText model and initialize a word vector. We’ll run a loop and search the vector for related words for our given string “happy.” static void BM_get_word_vector(benchmark::State& state) {\n\tconst auto& fasttext_model = get_fasttest_model();\n \n\tfasttext::Vector word_vector(fasttext_model.getDimension());\n\tstd::string word = \"happy\";\n \n\tfor (auto _ : state) {\n          \t            fasttext_model.getWordVector(word_vector, word);\n    \t\tescape(&word_vector);\n\t}\n} Thanks to these compiler options, we’re already 23% faster without any changes to the code. Now let’s look at the code itself. The original function is full of implicit conversions and contains a constant reference to a temporary object, which forces gcc to copy the object (most of the time we don’t need to do that). void FastText::getWordVector(Vector& vec, const std::string& word) const {\n  const std::vector & ngrams = dict_->getSubwords(word); \n  vec.zero();\n  for (int i = 0; i < ngrams.size(); i++) {\n\taddInputVector(vec, ngrams[i]);\n  }\n  if (ngrams.size() > 0) {\n\tvec.mul(1.0 / ngrams.size());\n  }\n} Let’s get rid of implicit conversions and improve the way we’re handling copying the array of ngrams: void FastText::getWordVector(Vector& vec, const std::string& word) const {\n  auto word_id = dict_->getId(word);\n  vec.zero();\n  if (word_id == -1) {\n  \tconst std::vector ngrams = dict_->getSubwords(word);\n  \tfor (int32_t ngram : ngrams) {\n    \t\taddInputVector(vec, ngram);\n  \t}\n  \tif (ngrams.size() > 0) {\n    \tvec.mul(1.0 / ngrams.size());\n  \t}\n  } else {\n  \tconst std::vector & ngrams = dict_->getSubwords(word_id);\n  \tfor (int32_t ngram : ngrams) {\n    \t\taddInputVector(vec, ngram);\n  \t}\n  \tif (ngrams.size() > 0) {\n    \tvec.mul(1.0 / ngrams.size());\n  \t}\n  }\n} This, along with the right set of compiler options, gave us 1.48X speed up. BM_get_nn Here is the function for this benchmark: static void BM_get_nn(benchmark::State& state) {\n\tauto& fasttext_model = get_fasttest_model();\n\tint32_t k = 10;\n\tconst std::string word = \"happy\";\n\tfor (auto _ : state) {\n    \tstd::vector<:pair std::string>> results = fasttext_model.getNN(word, k);\n    \tescape(&results);\n\t}\n} Similar to the previous case, we can improve the performance without any code changes using compiler flags: Can we do better? Of course! Method FastText::getNN takes a std::set as the last argument. We don’t need it in our scenario, so we can get 2.13X speed up instead of 1.22X: std::vector<:pair std::string>> getNN(\n   \tconst DenseMatrix& wordVectors,\n   \tconst Vector& queryVec,\n   \tint32_t k,\n   \tconst std::set<:string>& banSet); Std::set is implemented as a red-black tree. Red-black-trees searches have logarithmic complexity, and are slower than binary search and hash-table lookup. In our scenarios we don’t need any look-ups, so we can simplify the original code. std::vector<:pair std::string>> FastText::getNN(\n\tconst DenseMatrix& wordVectors,\n\tconst Vector& query,\n\tint32_t k,\n\tconst std::string& word) {\n  ++k;\n  std::vector<:pair std::string>> heap;\n  heap.reserve(k);\n  int32_t nwords = dict_->nwords();\n \n  int32_t i = 0;\n  for (; i < nwords && heap.size() < k; ++i) {\n\tstd::string word = dict_->getWord(i);\n\theap.emplace_back(wordVectors.dotRow(query, i), std::move(word));\n  }\n  std::make_heap(heap.begin(), heap.end(), [](const auto& x, const auto& y) { return x.first > y.first; });\n \n  for (; i < nwords; i++) {\n\tstd::string word = dict_->getWord(i);\n\treal similarity = wordVectors.dotRow(query, i);\n\tif (similarity >= heap.front().first) {\n    \tstd::pop_heap(heap.begin(), heap.end(), [](const auto& x, const auto& y) { return x.first > y.first; });\n    \theap.pop_back();\n    \theap.emplace_back(similarity, std::move(word));\n    \tstd::push_heap(heap.begin(), heap.end(), [](const auto& x, const auto& y) { return x.first > y.first; });\n\t}\n  }\n  std::sort(heap.begin(), heap.end(), [](const auto& x, const auto& y) { return x.first > y.first; });\n  std::remove_if(heap.begin(), heap.end(), [&](const auto& pair) { return pair.second == word; });\n  heap.pop_back();\n \n  real queryNorm = query.norm();\n  if (std::abs(queryNorm) < 1e-8) {\n\tqueryNorm = 1;\n  }\n \n  for (auto& word_similarity : heap) {\n\tword_similarity.first /= queryNorm;\n  }\n \n  return heap;\n} And the results…. Now we don’t do string comparison for every word from our vocabulary, and, instead of one complicated loop, we have multiple simpler loops. Here’s the breakdown: Build a heap. Use this heap to get top k + 1 results. Sort the heap. Remove the query from the results. Normalize similarities. We could improve this code even further by replacing the heap with the nth_element function call, but there is a CPU vs Memory trade-off. Since the vocabulary can be huge, we decided to keep the heap-based implementation, which uses less RAM. In the next section we will see how to get a faster algorithm for partial sort. BM_predict_line Again, we can speed this function up just with compiler flags and achieve a 1.25X speed up: In the code, we can change HierarchicalSoftmaxLoss::predict function to give us a speed boost: void HierarchicalSoftmaxLoss::predict(\n\tint32_t k,\n\treal threshold,\n\tPredictions& heap,\n\tModel::State& state) const {\n  dfs(k, threshold, 2 * osz_ - 2, 0.0, heap, state.hidden);\n#ifdef GIPHY  \n  auto middle = heap.begin() + std::min(heap.size(), size_t(k));\n  std::nth_element(heap.begin(), middle, heap.end(), [](const auto& x, const auto& y) { return x.first > y.first; });\n  heap.resize(middle - heap.begin());\n  std::sort(heap.begin(), heap.end(), [](const auto& x, const auto& y) { return x.first > y.first; });\n#else\n  std::sort_heap(heap.begin(), heap.end(), comparePairs);\n#endif\n} And we can get rid of the heap in dfs within the implementation of Hierarchical Softmax Loss: void HierarchicalSoftmaxLoss::dfs(\n\tint32_t k,\n\treal threshold,\n\tint32_t node,\n\treal score,\n\tPredictions& heap,\n\tconst Vector& hidden) const {\n  if (score < std_log(threshold)) { return; }\n  if (tree_[node].left == -1 && tree_[node].right == -1) {\n\theap.emplace_back(score, node);\n\treturn;\n  }\n \n  real f = wo_->dotRow(hidden, node - osz_);\n  f = 1. / (1 + std::exp(-f));\n  dfs(k, threshold, tree_[node].left, score + std_log(1.0 - f), heap, hidden);\n  dfs(k, threshold, tree_[node].right, score + std_log(f), heap, hidden);\n} We don’t need a heap sort to get top K predictions; it’s slow, and we don’t need to save memory. Instead, we can load the data and use std::nth_element and std::sort to implement partial sort. With a few code changes we’ve achieved 1.48X speed up instead of 1.25X. How can you repeat the results? For microbenchmarks: $ # Download a model for language identification $ wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin $ bash scripts/run_microbenchmarks.bash $ # Observe the results in Jupyter Notebook $ jupyter notebook  # Open Benchmark.micro.ipynb For training experiments: $ wget -c http://mattmahoney.net/dc/enwik9.zip -P data $ unzip data/enwik9.zip -d data $ perl wikifil.pl data/enwik9 > data/fil9 $ head -c 35653488  data/fil9 > data/fil9.tiny $ bash scripts/run_macrobenchmarks.bash Conclusion As you can see, choosing the right compiler flags can have a massive impact on performance. Sometimes that’s good enough, but double-checking your algorithms can also help you squeeze out even better enhancements. Algorithms with the same asymptotic complexity can have dramatically different performance, so it is important to use cache-friendly algorithms. Our improvements allowed us to get: - Faster Training - Faster Inference Process - Faster Hyperparameter tuning, which can lead to better models Here are the final numbers: Performance enhancements like these are crucial when dealing with huge datasets that need recurring or real-time analysis. Let us know if you were able to utilize these recommendations by reaching out to us on twitter ( @giphyeng ). Previous Post Next Post", "date": "2020-04-24"},
{"website": "Giphy", "title": "GIPHY SDK: The Grid", "author": ["Chris Maier"], "link": "https://engineering.giphy.com/giphy-sdk-the-grid/", "abstract": "GIPHY SDK: The Grid April 15, 2020 by Chris Maier A GIF feature in your app can increase engagement and give your users a valuable new way to express themselves, but building it can be hard! You have to figure out how and where to get the content, design and implement a UI, test, iterate…the list goes on. That’s why we built the GIPHY SDK , which comes with pre-designed UI templates that take care of all the work for you. However, while the templates are great for some, we also know creators out there may want to customize how  GIPHY looks in their product… Update: The Grid We’re excited to announce a brand new feature of the GIPHY SDK for iOS and Android: the Grid. A couple of our favorite apps using the GIPHY SDK In addition to the slick template experiences offered by the GIPHY SDK, you can now use the Grid to customize your GIF search UI to match the look and feel of your app. It’s the best of both worlds: the ease of use offered by the GIPHY SDK and full control over the design. You can tweak the padding and number of columns, and build your own search bar and buttons to control the experience. Learn more here: https://developers.giphy.com Have any thoughts on this feature, or the GIPHY SDK in general? Slide into our DMs at sdk@giphy.com Previous Post Next Post", "date": "2020-04-15"},
{"website": "Giphy", "title": "Engaging Endpoints: 4 Ways to Supplement GIF search", "author": ["GIPHY"], "link": "https://engineering.giphy.com/engaging-endpoints-4-ways-to-supplement-gif-search/", "abstract": "Engaging Endpoints: 4 Ways to Supplement GIF search April 1, 2020 by GIPHY We’d like to highlight four public API endpoints designed to increase engagement and create better user experiences for products and projects integrating GIPHY. At GIPHY, we use these endpoints to help users discover content and as ancillary features for our core GIF search experience — as we know a blank search input on a platform with hundreds of millions of GIFs and Stickers can be a bit daunting to users. Perhaps they don’t know what they want to search, or would rather passively consume content rather than actively search for it. The endpoints are designed specifically with these use cases in mind, and offer fun and simple ways to pique curiosity and unlock novel user interactions. GIF Categories The GIF Categories endpoint returns the list of high-level content categories and their respective subcategories, which we use to classify our GIF content. These categories were assembled by our internal team of GIF experts, and each one includes a representative GIF ideal for display, providing the perfect foundation for building interfaces for browsing and exploring. Check out how we use this data on our categories page . Trending Searches The Trending Searches endpoint returns a set of search terms that are “trending” across the GIPHY network. These results are updated hourly, and are sourced from both humans and algorithms. Some terms surface due to statistical significance, like a recent volume surge in search queries for that term, while others are hand-picked by our Editorial team for their social or cultural relevance. Given GIPHY’s standing as a cultural barometer of sorts, this endpoint is a great way to get timely, meaningful content to your users. We use it to power our “popular searches” dropdown in our mobile app available on iOS and Android . Search Suggestions The Search Suggestions endpoint returns a set of related search queries for a given query. These results are generated by a neural network embedding trained on user activity able to group tags/queries by their semantic similarity. Given the nature of how people use GIPHY, we’re able to capture powerful relationships amongst terms stemming from pop culture, internet slang, sports, emotions and expressions. You can see this endpoint in action with the related search terms listed on a search results page. Autocomplete The Autocomplete endpoint returns a list of searches matching a given string of characters. While there are many ways to utilize this endpoint, one of the most useful means is for search query autocomplete. Query autocomplete is a key component of any modern search engine, and is proven to provide a better overall user experience by reducing typing, reducing spelling errors, and promoting higher-quality search results. We use this endpoint to power the search autocomplete results in our apps and website, and it’s a fantastic way for helping users find the perfect query that captures what they’re thinking or feeling. Give them a try! Whether updating a preexisting product, or starting from scratch, we think you’ll find these endpoints extremely useful for increasing engagement by helping people find and share great GIF content. Head over to our API Documentation to to read more about these and other endpoints available in the GIPHY public API. Previous Post Next Post", "date": "2020-04-01"},
{"website": "Giphy", "title": "#BuildforCOVID19 Global Online Hackathon", "author": ["GIPHY"], "link": "https://engineering.giphy.com/buildforcovid19-global-online-hackathon/", "abstract": "#BuildforCOVID19 Global Online Hackathon March 27, 2020 by GIPHY We’ve joined Facebook and many other partners such as Microsoft, Pinterest, Slack, TikTok, Twitter, and WeChat to support and participate in the #BuildforCOVID19 Online Hackathon — a new global hackathon to tackle the challenges and problems related to the current coronavirus (COVID-19) pandemic. We want to encourage you – our global developer community – to join in as well and #BuildforCOVID19 using technologies of your choice across a range of suggested themes and challenge areas – some of which have been sourced through health partners including the World Health Organization (WHO). These areas include: health, vulnerable populations, businesses, community, education, and entertainment. The hackathon welcomes locally and globally focused solutions, and is open to all developers. Here’s how you can participate: Register and view full guidelines on Devpost here . Join the #BuildforCOVID19 Slack chat here . Submit your project by Monday, March 30th at 9am PST. Whether you use GIPHY’s API / SDK in your submissions or not, we’re here to support and help all developers. You can find us in the Slack chat (link above!). For further information about the hackathon, visit their event site here . Previous Post Next Post", "date": "2020-03-27"},
{"website": "Giphy", "title": "Elasticsearch: Custom Analysis", "author": ["Utah Ingersoll"], "link": "https://engineering.giphy.com/elasticsearch-custom-analysis/", "abstract": "Elasticsearch: Custom Analysis March 2, 2020 by Utah Ingersoll GIPHY uses Elasticsearch to deliver all the best GIFs. Elasticsearch is an extremely fast, open source search engine supported by a great community. It has a robust Query API which allows us to quickly iterate our search algorithm. The Mapping API enables us to prototype new signals and account for the quirks in GIF metadata. This article describes text analysis as related to Elasticsearch, covers built-in analysis and introduces the development of custom analysis. While we will not exhaustively cover text analysis we aim to provide solid tools for further exploration. You are encouraged to follow along using the docker environment described below. Docker Setup To follow the exercises in this tutorial you will need to install the following: Docker Desktop Community Edition HTTPie Installing Docker Desktop may require a full restart. Once Docker is running, let’s pull the Elasticsearch container by entering the following in your console: docker pull docker.elastic.co/elasticsearch/elasticsearch:7.5.2 Now let’s start your local container: docker run -p 9200:9200 -e \"discovery.type=single-node\" \\\ndocker.elastic.co/elasticsearch/elasticsearch:7.5.2 From a new console confirm Elasticsearch is running using HTTPie http localhost:9200 You should receive a response similar to the following HTTP/1.1 200 OK\n\n {\n    \"cluster_name\": \"docker-cluster\",\n    \"cluster_uuid\": \"_LQxOs63Rte4xlC8AQqLvw\",\n    \"name\": \"cce47c60c1fd\",\n    \"tagline\": \"You Know, for Search\",\n    \"version\": {\n        \"build_date\": \"2020-01-15T12:11:52.313576Z\",\n        \"build_flavor\": \"default\",\n        \"build_hash\": \"8bec50e1e0ad29dad5653712cf3bb580cd1afcdf\",\n        \"build_snapshot\": false,\n        \"build_type\": \"docker\",\n        \"lucene_version\": \"8.3.0\",\n        \"minimum_index_compatibility_version\": \"6.0.0-beta1\",\n        \"minimum_wire_compatibility_version\": \"6.8.0\",\n        \"number\": \"7.5.2\"\n    }\n} Congratulations! You are all set up for the exercises below! Introduction to Text Analysis Text Analysis is the process of decomposing text into small components called tokens. Frequently, tokens are just words. Tokens produced by analysis are used to build the inverted indices which Elasticsearch uses to retrieve and rank documents. Analysis also collects term counts, positions, and other data for ranking documents. Elasticsearch documents are composed of fields. Each field is assigned a data type either by mappings or through inference . Each data type has an implicit analyzer, but you may configure custom analyzers when the defaults do not suit your needs. Incoming queries are parsed using the same analysis used at index time to ensure searches are operating on the same set of tokens. How Analysis Works Analysis consists of three parts: Character Filters transform the original string by replacing or adding characters. A Tokenizer decomposes the text into tokens, usually splitting on whitespace to form words. Token Filters then remove or transform the tokens created by the Tokenizer. Common Token Filters include stop-word removal and stemming. The Analysis Pipeline: Analysis in Action You can inspect analysis before indexing using the Analyze API . Example 1: Standard Analysis Use HTTPie to post the phrase “lost in translation” to your local Elasticsearch Analyze API: Enter the following command in your terminal: http localhost:9200/_analyze <<< '{\n  \"text\": \"lost in translation\"\n}' You should receive the following in response: {\n\t\"tokens\": [\n    \t{\n        \t\"end_offset\": 4,\n        \t\"position\": 0,\n        \t\"start_offset\": 0,\n        \t\"token\": \"lost\",\n        \t\"type\": \"<ALPHANUM>\"\n    \t},\n    \t{\n        \t\"end_offset\": 7,\n        \t\"position\": 1,\n        \t\"start_offset\": 5,\n        \t\"token\": \"in\",\n        \t\"type\": \"<ALPHANUM>\"\n    \t},\n    \t{\n        \t\"end_offset\": 19,\n        \t\"position\": 2,\n        \t\"start_offset\": 8,\n        \t\"token\": \"translation\",\n        \t\"type\": \"<ALPHANUM>\"\n    \t}\n\t]\n} Since we did not specify an analyzer, we received the Standard Analyzer . The phrase “lost in translation” has been broken into the three tokens “lost”, “in” and “translation”. Built-in Analysis Elasticsearch has default analysers for each data type. The Text data type defaults to the Standard Analyzer. There are also language specific analyzers which will outperform the default when the language is known. Example 2: English Analysis Let’s try analyzing “cats in space” using the English Language Analyzer . The English analyzer has no Token Filters, uses the standard tokenizer and passes the resulting tokens through a stop word filter, a stemmer, and a lowercase filter. Enter the following in your terminal: http localhost:9200/_analyze <<< '{\n  \"analyzer\": \"english\",\n  \"text\": \"lost in translation\"\n}' This time we will receive only two tokens, “lost” and “translat”. {\n\t\"tokens\": [\n    \t{\n        \t\"end_offset\": 4,\n        \t\"position\": 0,\n        \t\"start_offset\": 0,\n        \t\"token\": \"lost\",\n        \t\"type\": \"<ALPHANUM>\"\n    \t},\n    \t{\n        \t\"end_offset\": 19,\n        \t\"position\": 2,\n        \t\"start_offset\": 8,\n        \t\"token\": \"translat\",\n        \t\"type\": \"<ALPHANUM>\"\n    \t}\n\t]\n} The english analyzer removed the stop word “in” and stemmed “translation” to “translat” (stemming is funny like that). Stopwords are very frequently occurring words like “a” or “it.” Adding stopwords to the index adversely impacts performance while doing little to improve the relevance of results. Stemming folds words with similar meaning like “translate” and “translation” down to one word, “translat” which has the overall effect of improving recall . Example 3: Phrase Matching using English Analysis Let’s post mappings defining a single field named caption with English analysis. http PUT localhost:9200/gifs <<< '{                        \n  \"mappings\": {\n      \"properties\": {\n        \"caption\": {\n          \"type\": \"text\",\n          \"analyzer\": \"english\"\n        }\n      }\n  }\n}' Next, let’s add some documents using the bulk API. http PUT localhost:9200/_bulk <<< '\n  { \"index\" : { \"_index\" : \"gifs\", \"_id\" : \"1\" } }\n  { \"caption\": \"Happy birthday my love\" }\n  { \"index\" : { \"_index\" : \"gifs\", \"_id\" : \"2\" } }\n  { \"caption\": \"happy birthday to me\" }\n  { \"index\" : { \"_index\" : \"gifs\", \"_id\" : \"3\" } }\n  { \"caption\": \"happy birthday my friend\" }\n' Now lets run a query: http GET localhost:9200/gifs/_search <<< '{\n  \"query\": {\n    \"match_phrase\" : {\n      \"caption\" : \"Happy birthday to\"\n    }\n  }\n}' You should receive the following results: [\n  {\n    \"_id\": \"2\",\n    \"_index\": \"gifs\",\n    \"_score\": 0.28852317,\n    \"_source\": {\n      \"caption\": \"happy birthday to me\"\n    },\n    \"_type\": \"_doc\"\n  },\n  {\n    \"_id\": \"1\",\n    \"_index\": \"gifs\",\n    \"_score\": 0.25748682,\n    \"_source\": {\n      \"caption\": \"Happy birthday my love\"\n    },\n    \"_type\": \"_doc\"\n  },\n  {\n    \"_id\": \"3\",\n    \"_index\": \"gifs\",\n    \"_score\": 0.25748682,\n    \"_source\": {\n      \"caption\": \"happy birthday my friend\"\n    },\n    \"_type\": \"_doc\"\n  }\n] The query “happy birthday to” matches all documents. This is because the English analyzer removed the stopword “to,” both at index time and at query time. Our actual query was “happy birthday” which matched all three documents. If we wanted to match with more precision we could switch to an analyzer without a stop word filter. Let’s explore that further in the next example. Example 4: Standard Analysis Let’s post mappings with the caption field set to standard analysis. http PUT localhost:9200/gifs-standard <<< '{                        \n  \"mappings\": {\n      \"properties\": {\n        \"caption\": {\n          \"type\": \"text\",\n          \"analyzer\": \"standard\"\n        }\n      }\n  }\n}' Let’s add the same documents as before: http PUT localhost:9200/_bulk <<< '\n  { \"index\" : { \"_index\" : \"gifs-standard\", \"_id\" : \"1\" } }\n  { \"caption\": \"Happy birthday my love\" }\n  { \"index\" : { \"_index\" : \"gifs-standard\", \"_id\" : \"2\" } }\n  { \"caption\": \"happy birthday to me\" }\n  { \"index\" : { \"_index\" : \"gifs-standard\", \"_id\" : \"3\" } }\n  { \"caption\": \"happy birthday my friend\" }\n' Now let’s rerun our query against the new index: http GET localhost:9200/gifs-standard/_search <<< '{\n  \"query\": {\n    \"match_phrase\" : {\n      \"caption\" : \"Happy birthday to\"\n    }\n  }\n}' This time we should receive only the result matching the entire phrase: [\n  {\n    \"_id\": \"2\",\n    \"_index\": \"gifs-standard\",\n    \"_score\": 1.247892,\n    \"_source\": {\n      \"caption\": \"happy birthday to me\"\n    },\n    \"_type\": \"_doc\"\n  }\n] Custom Analysis If we wanted the query “Happy birthday” to only match the document tagged “ Happy birthday my love” we would need to write a custom mapping without the lowercase filter found in the standard and English analysers. http PUT localhost:9200/gifs-custom <<< '{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_custom_analyzer\": {\n          \"type\": \"custom\", \n          \"tokenizer\": \"standard\",\n          \"char_filter\": [],\n          \"filter\": []\n        }\n      }\n    }\n  },\n  \"mappings\": {\n      \"properties\": {\n        \"caption\": {\n          \"type\": \"text\",\n          \"analyzer\": \"my_custom_analyzer\"\n        }\n      }\n  }\n}' Now let’s add our documents: http PUT localhost:9200/_bulk <<< '\n  { \"index\" : { \"_index\" : \"gifs-custom\", \"_id\" : \"1\" } }\n  { \"caption\": \"Happy birthday my love\" }\n  { \"index\" : { \"_index\" : \"gifs-custom\", \"_id\" : \"2\" } }\n  { \"caption\": \"happy birthday to me\" }\n  { \"index\" : { \"_index\" : \"gifs-custom\", \"_id\" : \"3\" } }\n  { \"caption\": \"happy birthday my friend\" }\n' Now run the query: http GET localhost:9200/gifs-custom/_search <<< '{\n  \"query\": {\n    \"match_phrase\" : {\n      \"caption\" : \"Happy birthday my\"\n    }\n  }\n}' And you will receive the document we expect: [\n  {\n    \"_id\": \"1\",\n    \"_index\": \"gifs-custom\",\n    \"_score\": 1.5843642,\n    \"_source\": {\n      \"caption\": \"Happy birthday my love\"\n    },\n    \"_type\": \"_doc\"\n  }\n] You can combine different tokenizers and filters to achieve different text analysis styles to match your needs. Inspecting Mappings Let’s take a closer look at what happened in Example 3. We can invoke the analysis defined on a specific mapping like this: http GET localhost:9200/gifs/_analyze <<< '{\n  \"field\" : \"caption\",\n  \"text\" : \"Happy birthday to\"\n}' You should receive two tokens: [\n    {\n        \"end_offset\": 5,\n        \"position\": 0,\n        \"start_offset\": 0,\n        \"token\": \"happi\",\n        \"type\": \"<ALPHANUM>\"\n    },\n    {\n        \"end_offset\": 14,\n        \"position\": 1,\n        \"start_offset\": 6,\n        \"token\": \"birthdai\",\n        \"type\": \"<ALPHANUM>\"\n    }\n] “Happy birthday my” and “birthday” were stemmed to “happi” and “birthdai” respectively. The algorithm that produced these odd stems is called the porter stemmer. Most importantly, the stop word filter removed the word “to”. Let’s now see what happens when we use standard analysis: http GET localhost:9200/gifs-standard/_analyze <<< '{\n  \"field\" : \"caption\",\n  \"text\" : \"happy birthday to\"\n}' You will receive three tokens: [\n  {\n    \"end_offset\": 5,\n    \"position\": 0,\n    \"start_offset\": 0,\n    \"token\": \"happy\",\n    \"type\": \"<ALPHANUM>\"\n  },\n  {\n    \"end_offset\": 14,\n    \"position\": 1,\n    \"start_offset\": 6,\n    \"token\": \"birthday\",\n    \"type\": \"<ALPHANUM>\"\n  },\n  {\n    \"end_offset\": 17,\n    \"position\": 2,\n    \"start_offset\": 15,\n    \"token\": \"to\",\n    \"type\": \"<ALPHANUM>\"\n  }\n] The standard analyzer has only separated the words at word boundaries. This allows phrase match to find the phrase “happy birthday to” on the document we expect to be returned. Further Exploration With the tools outlined above in hand, you should be well prepared to dive into custom analysis. Elasticsearch has extensive documentation on analysis , which when paired with these examples, will help you craft custom analysis pipelines suited to your data. — Utah Ingersoll, Senior Software Engineer Previous Post Next Post", "date": "2020-03-02"},
{"website": "Giphy", "title": "NY MusicTech @ GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/ny-musictech-giphy/", "abstract": "NY MusicTech @ GIPHY December 3, 2019 by GIPHY December 10, 2019 GIPHY HQ – New York, NY Join us and the MusicTech meetup for a night of demos and musical performances! MusicTech is a community that’s supports the people who are defining what music technology will look like in the future. Demos by: – Christian Rutledge, Muzooka – Hazmin Valdes, BillFold POS – Deane Marcus, RIFF VR – Music by Mankind! For more information, and to register head to the official meetup page ! Previous Post Next Post", "date": "2019-12-03"},
{"website": "Giphy", "title": "Pupline: GIPHY’s Media Metadata Pipeline", "author": ["Jacob Graff"], "link": "https://engineering.giphy.com/pupline-giphys-media-metadata-pipeline/", "abstract": "Pupline: GIPHY’s Media Metadata Pipeline October 29, 2019 by Jacob Graff Here at GIPHY, we receive thousands of GIF uploads each day. Like any tech company, we love data: we want to attach as much data as we can to these GIFs so that we can improve their search performance and the user experience. We also want to do so in an organized, centralized, and asynchronous way so that we can collect the content data quickly and easily after the upload is completed. For example, we run our GIFs through Google Cloud Vision in order to get a variety of object detection data, and run everything through our moderation platform (more on that later). To do all of this, and more, we built Pupline. Pupline allows us to observe each and every GIF in our library and quickly write scalable jobs for analyzing or processing GIFs immediately after they are ingested. Pupline has a simple, straightforward interface which makes writing a custom task to perform almost any image or metadata processing as simple as writing a Python function and deploying the changes. As a result, our team can quickly add new processing for GIFs as needed and other engineering teams can submit pull requests (PRs) to our codebase with minimal guidance from our team. Celery At its core, Pupline uses Celery to schedule custom tasks. Celery is a task queue framework, written in Python, which offers a powerful yet simple system for asynchronous task scheduling right out of the box, with minimal configuration. Celery tasks are run by workers, which listen on one of a number of supported message brokers (in our case, Redis). Workers monitor the broker for Celery messages (which can be generated by any application which follows the protocol) and, when they receive one, invoke the appropriate function with the arguments passed. If both your tasks and your scheduling application are written in Python, task invocation is a breeze: simply write your tasks as functions, add one of a few Celery decorators, import your task functions into your application, and voila — you can now invoke your tasks directly from your application without writing any worker code . In fact, your worker and application tasks can be deployed using the same docker image! This means that you can spend most of your time writing business logic, and much less time worrying about scheduling and infrastructure because Celery handles most of that. Infrastructure: Our infrastructure is fairly typical of a Celery application. As our task scheduler, we use a Django application. This application exposes a few different endpoints for invoking the various processing pipelines that we’ve written for GIFs. Once one of the endpoints has been called (usually with nothing more than a single identifier indicating which GIF to operate on), Pupline collects various other pieces of information, such as user information and metadata, and sends task messages to Redis. Finally, the workers pick up task messages and invoke the relevant functions. Batched Uploading In our standard workflow, GIFs are passed to Pupline as soon as they are uploaded and tasks start running almost immediately. However, sometimes we identify large batches of GIFs that we would like to do additional processing on. While it is easy to pass huge batches of data to the Celery queue (as it’s backed by Redis, the only limit is the disk size) this can cause a problem; GIFs that are freshly uploaded will get stuck waiting for all of the jobs in the batch to finish before they can start. Since managing new GIF uploads is a high priority, we want to make sure that they are always processed before batch jobs. To solve this, we implemented priority queues in Pupline. Dedicated workers scan the high priority queue exclusively, and any GIFs that require immediate processing, such as freshly uploaded GIFs, are sent to that queue. Whenever we wish to operate on a batch of GIFs, we add them to the low priority queue, and they are processed by batch processing workers. What do we use it for? Pupline is an ongoing project, and one of the great features of this tool is that it is very easily extendable by engineers. As a result, we can often use it to write quick, one off jobs that let us do GIF processing on the fly as needed. In addition, engineering teams around GIPHY are in the process of writing tools in Pupline to take advantage of its features. Today, Pupline runs on every GIF uploaded to the platform, and performs a few important tasks to keep our content safe, relevant, and useful. Moderation One of the primary tasks that Pupline was built for is content moderation. Here at GIPHY, we take content moderation very seriously, and our moderation pipelines can be pretty complex. Furthermore, new moderation jobs can suddenly be required and we want to be able to quickly and safely provide the most in-demand content. When GIFs are uploaded to GIPHY, they are first sent to our third party crowdsourcing provider for basic content evaluation. This provider allows us to run Human-in-the-loop (HITL) jobs on GIFs – these jobs allow automated pipelines to deliver content to trained participants who can classify and understand the context of GIFs much better than a computer can. After that, GIFs are separated into different content types and passed to various pipelines for further moderation steps. Whenever we want to create or modify a job like this, we write a Pupline task to send GIF data to the appropriate HITL job, plug our source of data into Pupline, and deploy; the job will automatically start filling with data. In addition, we’ve built a set of custom webhooks into Pupline for the HITL provider to call once the job has been completed, meaning the entire moderation flow can be written within a single application. Because these jobs have a Human-in-the-loop component, they take a long time relative to entirely automated tasks – on the order of 10 minutes or longer. In order to not tie up resources, rather than wait on these tasks to complete, the Celery workers do not keep track of which GIFs are in transit or moderation — when a HITL job completes, a webhook is called which spawns a new task to further process the GIF. Machine learning and computer vision In addition to some contracted human assisted moderation, there are many tasks that we can trust solely to computers. Tasks like face and object detection, action and scene analysis, and automatically generating titles for GIFs, can all be entrusted to machine learning or other automated systems. Pupline exposes several pathways for this, running GIFs through both our internal systems, and through cloud providers such as Google Cloud Vision. These tools allow us to enrich our data about uploaded GIFs and help improve our search performance by exposing information about the content of the GIFs. Because the tools we use for automated processing are fast, these tasks can run synchronously: we chain multiple automated tasks together, and the process can complete within seconds of upload. What’s next? As we continue to expand the number of tasks that Pupline executes, we hope to make it more flexible and add more sources of data. Currently, data is only input through REST calls to Django endpoints — eventually, we would like to provide GRPC endpoints and the ability to send input through a queue, allowing data from more sources without hurting application performance. In addition, as we improve our testing and monitoring, we hope to enable every GIPHY engineer to quickly and safely add new tasks with minimal oversight. — Jacob Graff Software Engineer, Services Previous Post Next Post", "date": "2019-10-29"},
{"website": "Giphy", "title": "GIPHY Gets Tagged by K-Nearest Neighbors", "author": ["Alice Phan"], "link": "https://engineering.giphy.com/giphy-gets-tagged-by-k-nearest-neighbors/", "abstract": "GIPHY Gets Tagged by K-Nearest Neighbors October 16, 2019 by Alice Phan When a brand new GIF gets uploaded onto GIPHY.com, there’s usually not a lot of data associated with it to make sure that it’s discoverable via search. While we do rely on machine learning for tag generation, we also allow uploading users and GIPHY’s content team to manually add “tags”, which are keywords that help ensure good content is easy to find. However, how does one decide on the most effective tags? “manually adding tags on GIPHY” Tagging can be a fairly laborious and semantic process. Additionally, some GIFs aren’t as straightforward to tag as others which makes the process even more time consuming. What if there was a way to suggest tags to a GIF using underlying data? Not only would automating the process save time, it would also increase the likelihood that every GIF would appear in relevant search results. To test this idea, I built a service called GIPHY Tagz, which recommends tags to a GIF based on its metadata and a K-Nearest Neighbor model. What is K-Nearest Neighbor Algorithm? K-Nearest Neighbor (KNN) is a machine learning algorithm that classifies unlabeled data based on labeled data given to the model. For example, the model can be trained to recognize that GIFs with hand waving motions have been previously tagged with “hello”. Therefore, the model should tag “hello” for new GIFs with hand waving motion. To determine which GIFs have hand waving motions (or other features), we use Google Cloud Vision, which detects features in images. In addition to the feature list, GCV data includes a “confidence score” for each feature which conveys how confident the model is that the image contains that feature. In this simplified model, the X- and Y-Axes are the confidence values for two features. In a real KNN model, there would be one dimension for each GCV feature. The trained KNN model can take a new GIF and find the K most similar GIFs based on feature similarity, and recommend tags of those GIFs. Training and Testing the Model As with other machine learning models, before we can use the KNN model, we need to train it with a set of training data. For GIPHY Tagz, I created a training set by starting with a list of the top 300 tags/search queries on GIPHY. I used this list to write a SQL query that looked through GIPHY’s Amazon Redshift database to select the top 10 GIFs that perform well for each search term based on click through rate difference. After fetching the metadata for each GIF, I dumped the metadata into a huge JSON file which was used for features and confidence scores. Fortunately, there is a Python’s Scikit-Learn library that implemented K-Nearest Neighbors. The training data was in the form of a sparse matrix where the rows are GIF ids, the columns are features, and the values are the confidence scores. I loaded the training data into the KNN model and was ready to test it. Major props to anyone who specializes in machine learning because the testing part is incredibly painful. The first time I entered a cat GIF into my KNN model, the tags returned were “spa,” and “food.” Of course, my KNN model was not fully accurate due to the noise in the data and the limitation of working with only the top 300 search queries. However, I spent weeks trying to improve the precision and recall of the tags returned and I lost track of how many times I had to ask our data engineers for different approaches. I reduced the dimensions of the training data through principal component analysis and normalized the scores of the features through TF-IDF which both led to a decrease in the model’s accuracy. One successful approach was to use cosine similarity instead of euclidean metric when measuring the distance between each GIF. When I was finally satisfied with the accuracy of the tags returned by my KNN model, I collaborated with our internal React expert, Kyle, on a simple UI so that the service could be used by anyone in the company. Final Thoughts “GIPHY Tagz in action” GIPHY, at its core, aims to deliver the perfect GIF for every situation and GIPHY Tagz demonstrated that it could play an effective role in future tagging efforts. I would like to give a special shoutout to Jesse Ling, who was my mentor, and the Ad Products team for all the support they have given me throughout my internship whether that be giving career and life advice, introducing data engineering 101, explaining React basics, or even fixing my very many git conflicts. — Alice Phan , Engineering Intern Previous Post Next Post", "date": "2019-10-16"},
{"website": "Giphy", "title": "DevOps and Drinks : Discussing Spinnaker with GIPHY and Armory", "author": ["GIPHY"], "link": "https://engineering.giphy.com/devops-and-drinks-discussing-spinnaker-with-giphy-and-armory/", "abstract": "DevOps and Drinks : Discussing Spinnaker with GIPHY and Armory October 4, 2019 by GIPHY October 10, 2019 GIPHY HQ – New York, NY Come join us on October 10th for another iteration of DevOps and Drinks, this time featuring our GIPHY Engineering’s very own Site Reliability Engineer, Bryant Rockoff, who will discuss how GIPHY has deployed and is using Spinnaker to help ensure we help to keep the internet weird at a fast pace, as well as an overview of the platform and its pitfalls. For more information, and to register head to the official meetup page ! Previous Post Next Post", "date": "2019-10-04"},
{"website": "Giphy", "title": "The Round", "author": ["GIPHY"], "link": "https://engineering.giphy.com/the-round/", "abstract": "The Round October 1, 2019 by GIPHY October 3rd, 2019 @ 26 Bridge, Brooklyn NY Come see us at The Round NYC on October 3rd where we’ll be participating in a panel, sharing what it’s like to work at GIPHY Engineering and chatting with candidates about opportunities to join the team! The event is completely free. You can find more info and register here ! Previous Post Next Post", "date": "2019-10-01"},
{"website": "Giphy", "title": "3D Print Fashion x WOW: Fashion of the Future @ GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/3d-print-fashion-x-wow-fashion-of-the-future-giphy/", "abstract": "3D Print Fashion x WOW: Fashion of the Future @ GIPHY September 18, 2019 by GIPHY September 25, 2019 GIPHY HQ – New York, NY Join us at GIPHY where 3D Print Fashion, in conjunction with Women of Wearables, are putting on a visual presentation and interactive panel dedicated to the future of fashion tech. Learn more about the people behind fashion’s most future forward innovations and exciting designs. oin us at GIPHY where 3D Print Fashion, in conjunction with Women of Wearables, are putting on a visual presentation and interactive panel dedicated to the future of fashion tech. Learn more about the people behind fashion’s most future forward innovations and exciting designs. For more details, head to the official event page . Previous Post Next Post", "date": "2019-09-18"},
{"website": "Giphy", "title": "The Brooklyn iOS August Meetup @ GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/the-brooklyn-ios-august-meetup-giphy/", "abstract": "The Brooklyn iOS August Meetup @ GIPHY August 14, 2019 by GIPHY August 20, 2019 GIPHY HQ – New York, NY Join us at GIPHY on Tuesday, August 20th for two exciting talks featuring our very own iOS developer, Chris Maier as well as Mike Sanderson of Tendigi. Doors open at 6:30, and presentations begin at 7. In Chris’ talk, “Building a GIF Carousel with UICollectionViewLayout” he’ll discuss the development of the GIPHY app’s “GIF carousel.” This talk will go into some of the technical challenges of implementing the design, the custom UICollectionViewLayout solution, and more generally touch on some useful tricks for breaking down complex problems into smaller pieces. In Mike’s talk,  “UIKit Will Disappear: A look at iOS app development in the near future” will look at Swift UI, the Combine framework, Catalyst/Marzipan, “Scenes”, and the Swift Package Manager and attempt to envision what development for Apple platforms will look like in just a few years—and how we get there. Register for free on the official Brooklyn iOS Develpoer meetup page! Previous Post Next Post", "date": "2019-08-14"},
{"website": "Giphy", "title": "Containerizing & Scaling Luigi in Kubernetes", "author": ["GIPHY"], "link": "https://engineering.giphy.com/containerizing-scaling-luigi-in-kubernetes/", "abstract": "Containerizing & Scaling Luigi in Kubernetes July 26, 2019 by GIPHY July 30, 2019  GIPHY HQ – New York, NY At GIPHY we love data because it lets us make data-driven decisions that improve the quality of results our users get from our search engine. Recently, GIPHY moved their Luigi pipelines from legacy infrastructure on managed AWS EC2 instances to a new containerized ecosystem inside of Kubernetes to increase the overall latency of our most critical ETLs. In this talk, Walter will share the exact limitations of GIPHY’s old infrastructure, how they identified these bottlenecks, and how decided to overhaul their system as they containerized it. Additionally, the speaker will cover how they redesigned Luigi pipelines to work around its core limitations while maximizing its strengths. Register for free on the official New York Data Science meetup page ! Previous Post Next Post", "date": "2019-07-26"},
{"website": "Giphy", "title": "What’s that Spike?", "author": ["Alex Anderson"], "link": "https://engineering.giphy.com/whats-that-spike/", "abstract": "What’s that Spike? August 13, 2019 by Alex Anderson Weekly Search Trends at GIPHY The Product Analytics team at GIPHY sits at the interface of technical data engineering teams and the rest of our business. We’re tasked with making data easily accessible across the org and uncovering insights to enhance our product and sales strategies. This can take the form of building data visualizations, automating routine pulls, and, most importantly, performing deep dives into search behavior and content. Ultimately, our goal is to craft stories out of data that can be easily understood by all. When I started at GIPHY as a Product Analyst, many of the first data analyses I completed were related to enhancing our knowledge of what our users were searching for. Understanding users’ desires impacts nearly all teams across the organization. While there is already a robust set of internal tools and products that make our Search algorithm first class, it’s not always easy for less technical teams to sift through the data and extract insights without getting in the weeds with SQL. Supplying teams like editorial and marketing with easily digestible, summarized information about search trends was a high priority, but something we quickly learned that nearly all teams wanted. To mitigate one-offs and equip the entire GIPHY org with useful data, Product Analytics decided to craft a “Weekly Search Trends” newsletter. This allows us to push information to all stakeholders, rather than being reactive to requests or asking them to monitor existing dashboards. The newsletter presents high level categories of what users searched for over the previous week, as well as keywords that spiked, by building a more automated reporting framework. What are users searching for? Every time a user makes a request to our API , from any integration we partner with, that request is logged into our databases. We’re able to extract multiple pieces of information about each of these requests, but for this exercise, we wanted to include search requests coming from the U.S. across our API network. The U.S. is our largest market, and these searches are generally in English which makes it simpler to assess trends. Once we have this set of data, we apply some functions to cleanse it for easier readability. Sanitizing the queries involves a number of methods, including utilizing an existing remapping framework for semantically similar and misspelled phrases (i.e. “happybirthda” maps to “happy birthday;” “laugh out loud” maps to “lol”). Finally, we filter on searches that fall into the top by volume per day to ensure we’re focusing on statistically significant results. How do we determine what constitutes a trend? Although we get millions of unique queries per language per day, certain terms perennially appear in the list of top terms, such as “love,” “happy,” and “sad”. Upon further analysis, we also noticed that there are over 500 terms that appear multiple times in the top searches of the day over the course of a given month. To establish a benchmark of what constitutes an average top term, we crafted a “high volume score.” To calculate this, we take four weeks worth of data of the top terms for each day. Every day a keyword appears in the top terms, this gives it an additional point, for a max score of 28. We then look at these points by day of the week to determine if this term actually should be included in the high volume keywords list. For example, “tbt” generally always is a top search on Thursdays so would only have a score of 4 in a given four-week span, but is something we want to include as a benchmark. Once we have this top terms benchmark list, we compare it with our most recent weekly search dataset. This makes it simple to filter and find terms that are “new” to the top terms list. Aside from finding “new” top terms, we also want to see if any of the benchmark high volume terms spiked in a given week. As a point of comparison, we look at the volume of each term for the previous week, then divide by the total search volume for the week to come up with an index that represents a week over week delta. Though uncomplicated, this has proven to be an effective way to spot anomalies without the need for a more complex algorithm for this simple newsletter. Once we have a list of terms that spiked, the real work begins! At GIPHY, we believe that combining machine learning algorithms with culturally aware (and diverse!) humans yields the most powerful and effective insights. Our algorithm will export a list of trending terms for the week, but our cultural insight about the latest events and internet memes can provide more context about the intent of the user. The next step after identifying terms that spiked is to bucket them into categories or trends. Finally, we package it up into a quick, easy-to-read newsletter that gets sent out company-wide. Concluding thoughts While the newsletter has only been distributed for a few months, it has already been helpful to nearly all GIPHY teams. It’s also been a great tool to expose our fairly new Product Analytics team to GIPHY to show examples of the types of analyses we can create. Some of the fun trends and categories we’ve identified users searching for include entertainment releases (Game of Thrones was particularly popular), major holidays (think Mother’s Day, Easter, and Cinco de Mayo), sporting events (soccer nearly always appears in top searches), and more. Moreover, by sharing this simple newsletter, we’ve empowered our editorial team to take action and improve content around popular themes. Lastly, we’ve reduced the amount of ad-hoc requests by proactively pushing out information, instead of reacting to one-off requests asking if specific events are showing up in search. Going forward, there are a few ways I can foresee this newsletter evolving to take initial feedback into consideration and add additional value to the company. Right now, the emails are sent out manually and insights are not catalogued. In the future, I’d love to make a simple web application to visualize this data and add the ability to look into historical reports by various content verticals. Teams could see which holidays, sporting events, seasons, or other categories of keywords that users were searching for in the past to inform their strategy. Additionally, our method of determining spikes, while effective, remains simple, but has the potential to become more sophisticated. We could test out time-series models to see if it improves the number of events we’re able to spot, and potentially expose more niche spikes. The most important thing is that the newsletter is simple to create and spreads knowledge to teams that were previously not empowered with easily accessible insights, and can now understand the story we extract from the numbers. — Alex Anderson, Product Analyst Previous Post Next Post", "date": "2019-08-13"},
{"website": "Giphy", "title": "Fixing Django Fixtures", "author": ["Chris Hranj"], "link": "https://engineering.giphy.com/fixing-django-fixtures/", "abstract": "Fixing Django Fixtures July 2, 2019 by Chris Hranj Here at GIPHY we have a lot of services. These services vary in language, visibility, and scale. Since I started a few months ago, I’ve been working on a newer internal service called Bouncer. Bouncer is a CMS built on top of the Django framework and primarily talks to another internal Scala service. When I started working on Bouncer, my team and I ran into a recurring issue with generating sandbox data that consumed a lot of our time. After researching multiple approaches I was able to solve this problem by combining a number of tools/libraries into one clean implementation. This post assumes some working knowledge of the typical Django app structure. If a refresher is needed, the Django Tutorial (which I admittedly referenced while writing this post) should cover it. The Problem Spinning up a local instance of Bouncer was simple thanks to the wonders of Docker . However, Bouncer was difficult to develop and test locally because new containers would connect to an empty database and present a sparse user interface. We needed a bunch of sandbox data to work with, but generating useful, human-readable data was difficult and time consuming without a solid understanding of Bouncer’s complicated data models. Additionally, I wanted the output to be replicable and idempotent, so new engineers could run the same script and share the same starting point. Note : If you just want to skip the journey and just see the code, head to the ‘Implementation’ section below, or check out some example code on GitHub . Attempted/Possible Solutions SQL Dumps My initial approach was to load a SQL dump from Bouncer’s staging environment into my local database container. The SQL dump loaded successfully but also produced a bunch of errors (e.g., relation already exists, random sequence issues, etc). A lot of the data in Bouncer’s staging environment survived a ton of migrations and manual manipulation, so my gut feeling was that tracking down these tiny issues would not be worth the time and effort. Additionally, most of the data in the staging environment looked like nonsensical garbage, as it was quickly and manually created to test one specific feature of Bouncer (it is staging, after all). Perhaps it was my OCD, but this approach left me with a lot of insecurity about the accuracy and usefulness of my data, so I searched for another solution. A Pre-Loaded Docker Image Since Bouncer runs on Docker locally, another possible solution was to create a Docker image with the necessary data already loaded into the database. Developers could then simply pull down the pre-loaded database image and be ready to go. For these reasons, the approach was not ideal: What if a new use case required additional data for testing? Bouncer is then faced with the same inconvenience of manually creating data with no standardized way to do so. Any time the data model changes a new Docker image would need to be built, pushed, and pulled down by all the other Bouncer devs. Bouncer is a newer service undergoing refactors, so the data model is constantly in flux. What if Bouncer wasn’t Docker-ized? If we ever decided to move away from Docker we would be forced to revisit this problem. Selfishly, I’d prefer to write Python scripts over Docker commands any day. 😉 Fixtures A fixture (in the Python/Django world) is a module for loading and referencing test data. Fixtures are basically synonymous with JSON although they can also be XML, YAML, etc. (see more info on fixtures here ). I was convinced fixtures would be the solution to my problem. The Django documentation even recommends them. Basic use cases, like a single unit tests with a simple data model can and should be dealt with using fixtures. Fixtures become troublesome as soon as you start dealing with increasingly complex data models. “Complex” in this case refers to models containing datetimes, UUIDs, and especially foreign keys. Bouncer’s data model contains all of these, including several one-to-many and many-to-many relationships. After I tried to populate my database using fixtures, I came across the following issues: Each time the underlying data models change, all of the JSON fixtures need to change as well to stay consistent with the new data models. Since fixtures deal with JSON and not Python, they are limited to a few simple primitives to represent many types of data. For example, primary and foreign keys must always be hard-coded integers. As soon as the fixture require more than a few objects it becomes difficult to maintain these complicated nests of foreign keys. I’ve found that when working with ORMs and foreign keys, it’s dangerous to assume the initial state of the database. Fixtures rely on a clean and consistent starting state of the database. If there is stale data lingering in the database (e.g., from a unit test that didn’t clean up), the fixtures will probably fail to load. A Better Solution Helper Libraries to the Rescue After further research, I discovered that the problem can be solved 100% programmatically with the help of two great libraries: factory_boy and faker . – factory_boy is for fixture replacement/generation – faker is for generating fake, human-readable data. factory_boy depends on faker already, so technically only one package is needed. This package and all its dependencies can be installed via the following command: $ pip install factory_boy These are some of the benefits of this approach: Since we’re populating the database programmatically (i.e. using Python), the data can be sourced from anywhere. Future iterations could make network calls or read files to gather data. The factories can be organized in a framework-agnostic manner so they can be used elsewhere while still being able to integrate into Django easily using the django-admin as demonstrated below. Implementation Let’s see some code already. We’ll use a simple data model borrowed from the Django Book to make it easier to explain our approach: from django.db import models\n\nclass Publisher(models.Model):\n    name = models.TextField()\n\nclass Author(models.Model):\n    first_name = models.TextField()\n    last_name = models.TextField()\n\nclass Book(models.Model):\n    title = models.TextField()\n    authors = models.ManyToManyField(Author)\n    publisher = models.ForeignKey(Publisher, on_delete=models.PROTECT)\n    publication_date = models.DateField()\n    isbn = models.TextField() Note that the code above is using Django’s built-in ORM to represent its data models. If you’re using a different ORM like SQLAlchemy your models will differ slightly. Working with factory_boy The first step is to re-create the models above as factories using the factory_boy library. It’s important to differentiate the concept of a factory as it pertains to factory_boy from the Python pattern of encapsulating object creation which also references the term ‘factory.’ Create a new file called factories.py in the same directory as your models ( mysite/books/factories.py in this example) and add the following code: import factory\nfrom books.models import Author, Book, Publisher\n\nclass PublisherFactory(factory.django.DjangoModelFactory):\n    class Meta:\n        model = Publisher\n\n    name = factory.Faker('company')\n\nclass AuthorFactory(factory.django.DjangoModelFactory):\n    class Meta:\n        model = Author\n\n    first_name = factory.Faker('first_name_female')\n    last_name = factory.Faker('last_name_female')\n\nclass BookFactory(factory.django.DjangoModelFactory):\n    class Meta:\n        model = Book\n\n    title = factory.Faker('sentence', nb_words=4)\n    publisher = factory.SubFactory(PublisherFactory)\n    publication_date = factory.Faker('date_time')\n    isbn = factory.Faker('isbn13', separator=\"-\")\n\n    @factory.post_generation\n    def authors(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for author in extracted:\n                self.authors.add(author) Each factory above defines the Django model it is based off of, as well as the fields in that model that need to be populated. The calls to factory.Faker tell factory_boy to use the Faker library to generate fake data for that field (discussed more below). The most difficult piece of code above to understand is the authors function in the BookFactory class. Since authors is a ManyToManyField on the Book model, it requires special functionality in order to be represented inside a factory. You can find more details on how that works here . Generating Fake Data with faker The factories above make use of a module in the factory_boy library called Faker . Faker is conceptually broken up into a number of “providers”. A provider is essentially the “type” of data that is being faked. Example providers include addresses, credit cards, phone numbers, etc. (full list of faker providers here ). The factories in the code above use a number of different providers to match the type of data the model would contain. For example, the isbn field in the BookFactory above uses the isbn provider because it makes the most sense. You can play around with these factories in the django shell: chranj@~/django-fixture-example/mysite$ ./manage.py shell\n>>> from books.factories import BookFactory\n>>> BookFactory.create()\n< Book: Book object (1)> As seen above, factories can be instantiated via the create() method. Now we can use the ORM and query for books: >>> from books.models import Book\n>>> Book.objects.all()\n< QuerySet [< Book: Book object (1)>]> Notice that a Book was returned even though we never explicitly created any Book objects. Faker is actually creating and saving this object to the database under the hood. How cool is that!? If you inspect this object you’ll see Faker also created a Publisher and populated all of the fields on both models with random data: >>> book = Book.objects.first()\n>>> book.publisher.name\n'Lowe-Curtis'\n>>> book.isbn\n'978-0-273-85745-7' Since authors is a ManyToManyField we need to manually create an Author first using the AuthorFactory and pass it into the BookFactory. Create another Book to test this: >>> from books.factories import AuthorFactory\n>>> book = BookFactory.create(authors=[AuthorFactory.create()])\n>>> book.authors.first().first_name\n'Cynthia' (More details on Faker can be found here ). Custom django-admin Commands Now that the necessary models and factories exist, it’s time to automate the use of them in a reusable fashion. The best way to do this is with a django-admin command. We’ll start by integrating factory_boy into django-admin. Implementing django-admin Commands django-admin looks for custom commands in the /management/commands/ directory inside a Django app. Commands should each go in their own module and the module name should be one word in all lowercase. For example, if your app’s name is books and your command is populatebooks , you would create a new file in books/management/commands/populatebooks.py. Create this file (change the naming to fit your needs) and insert the following: from django.core.management.base import BaseCommand\n\nclass Command(BaseCommand):\n    help = \"Populate mysite with sample data. Generates books, authors, and publishers.\"\n\n    def handle(self, *args, **options):\n        self.stdout.write(\"Inside populatebooks!\") Custom django-admin commands must inherit the BaseCommand class and must override the handle function, as seen above. Running Custom Commands Before building out populatebooks , let’s take a second to test that the django-admin command runs. If Django is running locally, it can be tested using the following: $ ./manage.py populatebooks In a Docker-ized environment, the command would look more like the following: $ docker exec -it CONTAINER_NAME sh -c './manage.py populatebooks' If everything is configured successfully, \"Inside populatebooks!\" should be printed to the terminal. Building A Robust populatebooks Now let’s integrate the factories we created earlier into this new populatebooks command. Start by defining a _load_fixtures function on the Command class that looks as such: from books.factories import BookFactory, AuthorFactory\n\ndef _load_fixtures(self):\n    author1 = AuthorFactory.create()\n    BookFactory.create(authors=[author1]) Then update the existing handle function to call _load_fixtures instead of just printing to the terminal. from django.db import transaction\n\ndef handle(self, *args, **options):\n    try:\n        with transaction.atomic():\n            self._load_fixtures()\n\n    except Exception as e:\n        raise CommandError(f\"{e}\\n\\nTransaction was not committed due to the above exception.\") We are wrapping the call to _load_fixtures in a try/except block as well as an atomic transaction so if anything goes wrong during its execution, we can rollback and won’t be left with a database in a partially populated state. Save these changes and run the django-admin command just as you did above as many times as you want. Once it finishes you can check the database and marvel at the new, randomly generated books in your database: chranj@~/django-fixture-example/mysite$ ./manage.py populatebooks\nchranj@~/django-fixture-example/mysite$ ./manage.py shell\n>>> from books.models import Book\n>>> Book.objects.all()\n< QuerySet [< Book: Book object (7)>, < Book: Book object (8)>, < Book: Book object (9)>]>\n>>> Book.objects.get(id=7).title\n'Blood value minute.' Congratulations! You’ve just populated your database with randomly generated data using a custom django-admin command! Additional Steps The following sections are optional but recommended, and shouldn’t take too long to implement. Wiping the Database Before Insertion It’s more than likely that you’ll want to wipe the database before running a command like populatebooks , but for safety reasons this functionality should be optional. Note that django-admin actually has a flush command that does this, but using it can possibly have unintended consequences as it will wipe everything . Implementing a custom wipe command will give you complete control over what data gets dropped. def add_arguments(self, parser):\n    parser.add_argument(\n        '--clean',\n        help='Wipe existing data from the database before loading fixtures.',\n        action='store_true',\n        default=False,\n    ) Next add a _clean_db function to take care of deleting objects from the desired models. In this case we want to delete all Author , Book, and Publisher objects: from books.models import Author, Book, Publisher\n\ndef _clean_db(self):\n    for model in [Author, Book, Publisher]:\n        model.objects.all().delete() This approach can get a little hairy depending on the complexity of relationships between models and the database backend you are using. For the example data model used throughout this post, simply importing the models and deleting them in a loop was sufficient, but you might need to experiment. Then update the handle function to check for the --clean argument and call the _clean_db function if passed: try:\n    with transaction.atomic(): if options['clean']:\n            self._clean_db() self._load_fixtures() Once all the code above is added, it can be tested by running populatebooks with the --clean flag. Keep in mind this will wipe any existing books in your database: $ ./manage.py populatebooks --clean If you jump into the database you should now see only the data created during the most recent execution of populatebooks . Seeding Fake Data One of the original requirements of this work was to make Bouncer more portable and its data replicable. Due to the nature of Python’s random module which is used by Faker, our populatebooks command is not idempotent, meaning if it runs multiple times in a row (with the --clean ) flag, the resulting data will always look different. We can fix this issue by explicitly setting the random module’s seed. Add another argument just as we did above that looks as such: parser.add_argument(\n    '--seed',\n    help='The initial seed to use when generating random data.',\n    default='mysite',\n    type=str,\n) Next create a _set_seed function which will seed the random engine used by both factory_boy and faker (read more about this here ): import factory.random\n\ndef _set_seed(self, seed):\n    self.stdout.write(f\"Using seed \\'{seed}\\' for randomization.\")\n    factory.random.reseed_random(seed) Then update handle again to grab the --seed option and pass it into _set_seed as such: try:\n    with transaction.atomic():\n        if options['clean']:\n            self._clean_db() seed = options.get('seed')\n        self._set_seed(seed) self._load_fixtures() Now populatebooks can be called in such a manner: $ ./manage.py populatebooks --clean --seed \"this is a seed\" If you call populatebooks multiple times or even from a completely different machine with the same seed (and same dependency versions), you should end up with the same exact data in the database. The only thing to keep in mind is that if you pass a seed but don’t also pass the --clean you may run into database errors depending on unique constraints on the data model. Other Ideas This blog post could go on forever with ideas on how to expand the populatebooks command. Some other features that I’ve implemented personally since starting this post include: – automating the creation of an admin user so there’s no need to run Django’s createsuperuser – checking the current environment that populatebooks is running in so it never runs in production – dumping the contents of the database to a file when the --clean command is used in case the command needs to be reverted See what you can come up with! Concluding Thoughts After adding factories and a custom django-admin command to Bouncer, it has become a more portable service and its on-boarding time has decreased significantly. One could probably argue that this solution was heavily over-engineered (and I would probably agree with you), but this was a great exercise for learning about custom django-admin commands, working with the ORM, and using some awesome third-party Python libraries. Hopefully the information in this post will be useful in helping you build out your own data generation utilities. If you have feedback or run into errors, typos, etc in the code feel free to reach out to me via email ( chranj@giphy.com ) or on Twitter . Thanks for reading! – Chris “Brodan” Hranj, Ad Products Engineer Resources Thanks to the following blog posts, StackOverflow answers, and documentation which I learned from and referenced while writing this post: – Factory Boy as an Alternative to Django Testing Fixtures – Populate Django database on StackOverflow – Random.seed(): What does it do? on StackOverflow – The Django Book Previous Post Next Post", "date": "2019-07-02"},
{"website": "Giphy", "title": "Introducing the New and Improved GIPHY SDK", "author": ["GIPHY"], "link": "https://engineering.giphy.com/introducing-the-new-and-improved-giphy-sdk/", "abstract": "Introducing the New and Improved GIPHY SDK June 26, 2019 by GIPHY Today, we’re announcing our new and improved GIPHY SDK, the fastest and easiest way to integrate the world’s most-loved GIF library — and the full GIPHY experience — into your app for free. Built with developers and product designers in mind, our SDK is the best way to get direct access to GIFs, stickers, and new content like GIPHY Emoji and Text . GIPHY SDK is packed with powerful features Packed with powerful features used by the biggest apps in the world, the GIPHY SDK brings the magic of GIPHY to your app with just a few lines of code. To make integrating the GIPHY SDK even more of a no-brainer, our favorite features include… Content on Content : Access to our infinite library of GIFs and stickers     featuring the latest in entertainment, sports, and more from GIPHY’s tens of thousands of official content partners and unique artists. Customization: Pre-built, customizable template options — available in both dark and light UI formats — so you can find the perfect look for your app. Dark and light mode on GIPHY SDK Exclusive Features : Unlock GIPHY’s latest content releases, such as animated GIPHY Emoji and Text, as well as new formats and content-drops, which will only be available to developers through the GIPHY SDK. Powerful Search: Our search algorithm lets you deliver the right GIF, at the right time, every time. In addition, we believe sharing content and expressing yourself should be safe and easy. That’s why we moderate our content so you can rest assured that everything served by GIPHY in your app is safe for distribution. With the latest GIPHY SDK, our best-in-class tools are readily available to help power your creativity. Now, any app can have a GIPHY button. To get started, head over to our brand new DevPortal at developers.giphy.com and click ‘create an app’ to receive your API key and start building today. GIPHY SDK is available for both iOS and Android. We’re excited to see what you build! To stay up-to-date on all things GIPHY Engineering, follow us on Twitter and Instagram ! Previous Post Next Post", "date": "2019-06-26"},
{"website": "Giphy", "title": "TKU Hackathon", "author": ["GIPHY"], "link": "https://engineering.giphy.com/tku-hackathon/", "abstract": "TKU Hackathon March 23, 2019 by GIPHY March 30, 2019 2 Metro Center – Brooklyn, NY GIPHY Engineering is proud to sponsor “Level Up: A Hackathon for Gaming with a Mission” by Tech Kids Unlimited (TKU), a tech-based organization for children and teens with special needs. TKU was founded by NYU Tandon Adjunct Digital Media Professor Beth Rosenberg and her son. The non-profit gives children and teens with learning and emotional disabilities valuable tech skills through weekly, summer, and evening programs like the annual hackathon. For more information, head to the event’s offical press release . Previous Post Next Post", "date": "2019-03-23"},
{"website": "Giphy", "title": "Who is GIPHY Engineering?", "author": ["GIPHY"], "link": "https://engineering.giphy.com/who-is-giphy-engineering/", "abstract": "Who is GIPHY Engineering? May 21, 2019 by GIPHY As GIPHY Engineering continues to grow, we thought it was important to take a step back and have a conversation about what it means to be a GIPHY Engineer. The goal wasn’t to create a spreadsheet with all the prototypical attributes of the perfect engineer, but instead to understand a bit more about ourselves as a team: what brought us here, why we’re proud of our work and what makes us happy. So, we all came together one afternoon and attempted to put the feelings and experiences that define our team into words. As with any good thing, GIPHY Engineering is a little bit different for everyone — but we think this is a strong place to start understanding what it means to be a GIPHY Engineer. Openness in Communication “GIPHY is an extremely transparent company. At the end of every year, we have a company brainstorm for new features which are used to create our company roadmap over the next year. We also do company-wide demos every Friday to show the cool stuff we’re working on, both work-related projects and personal projects.” – Jonny Mclaughlin, Staff Product Engineer Fun and Balanced Company Environment “I love coming in to work (yes, even on Mondays!) and seeing all my coworkers. Their sense of humor, adventure and fun is infectious and it’s hard not to enjoy yourself here. In addition to official events that GIPHY sponsors, everyone I work with here genuinely enjoys learning, being silly, and hanging out together every day, whether at the lunch table, in the halls or just around the office.” – Bjorn Roche, Engineering Manager Supportive + Talented Colleagues and Leaders “The Engineers at GIPHY are some of the brightest people I’ve had a chance to work with and have the amazing ability to explain their work in a non technical way. Our leaders are able to share their vision effectively and in such a way that inspires all of us to be our very best.” – Michelle Johnson, Senior Software Engineer, Front End Strong Brand Awareness with Customers “One of the coolest things about working at GIPHY is getting to TELL people I work at GIPHY and seeing their reactions. “WHAT?!? I use GIPHY everyday!”  The amount of friends, family, and random people who have told me how much GIPHY makes them smile, or make their jobs or day-to-day life that much more enjoyable and fun, has been fulfilling in a way I can’t really describe. Some startups want to “change the world,” but do so in ways that are antithetical to that very mission (or at the very least not in a positive way).  But at GIPHY, we just help people smile and laugh, keep the internet weird, and somehow get to make a job out of it.  I think that’s just about the coolest thing you could say about anywhere.” – Bryant Rockoff, Site Reliability Engineer The Opportunity to Learn New Things “Engineers at GIPHY have the unique opportunity of being involved in designing product roadmaps and understanding the strategic reasons behind our work, which I think prepares us for the next steps in our careers.” – Sixuan Liu, Tech Lead, Partner Growth Previous Post Next Post", "date": "2019-05-21"},
{"website": "Giphy", "title": "Luigi the 10x Plumber: Containerizing & Scaling Luigi in Kubernetes", "author": ["Walter Menendez"], "link": "https://engineering.giphy.com/luigi-the-10x-plumber-containerizing-scaling-luigi-in-kubernetes/", "abstract": "Luigi the 10x Plumber: Containerizing & Scaling Luigi in Kubernetes May 14, 2019 by Walter Menendez Here at GIPHY, we love data. We love it because it lets us make data-driven decisions that improve the quality of results our users get from our search engine. The more search traffic on GIPHY, the more insight we have into what’s popular and trending. We use these insights/signals to fine-tune search results we serve across desktop, mobile, or one of our integrations. In this way, we deliver the most relevant content and customize search experiences! As you can imagine, with the volume of search traffic we get (over 500M daily active users), computing things like click through rate or A/B test performance in efficient ways quickly becomes unwieldy. At GIPHY we rely heavily on Amazon Elastic MapReduce (EMR) jobs to help us crunch the numbers. We run a lot of EMR jobs to send our search data through different computations and data transformations, often taking the output of one calculation and making it the input of another. To help us with these complex chains of work: Luigi ! Luigi is Spotify’s open source framework for task pipeline management. We like Luigi for a number of reasons. Primarily, we love how it’s written in Python and how easy and simple it is to use. When our data ecosystem first started, we leveraged Luigi’s simple interface and established a series of crons to kick off entire data pipelines, as recommended in its documentation . These crons were responsible for running our tasks based on how often we’d like them to run: hourly, daily, and every 15 minutes. In the beginning, there were only a handful of tasks that ran in each time frame, so the naive cron-based solution worked really well! However, as we’ve added more and more task pipelines to handle more and more data, this cron-based approach started showing growing pains. In particular: – All our tasks were often grouped together in what is known as a sink task; a simple wrapper task that simply executes a list of tasks. A sink task would consider itself done only when all its listed dependent tasks were done, executing as many as it could. Through these sink tasks, any failures wouldn’t get retried until the next run, leading to delays in pipelines if shared dependencies were failing. – Any runtime errors that occured in a sink task prevented the rest of the sink task’s dependencies from running. Given that the only commonality a lot of these tasks shared was frequency of execution, this vulnerability meant unrelated tasks prevented each other from running. – The first thing that Luigi does before executing any task is check which tasks   – and their dependency chains – in the given list of required tasks are done. As that list of tasks has grown, Luigi spends upwards of 15 minutes just checking task history! On top of these limitations, our Luigi infrastructure existed on non-containerized provisioned AWS EC2 instances. Legacy infrastructure creates additional issues, especially in terms of inconsistent environment configuration and individual server resource contention. Not to mention, since we were writing Python for Luigi, it was difficult maintaining clean installs of all our Python packages across all users with access to those boxes. With these problems in mind, we resolved to do two things: first, separate the logic of deciding what tasks to run from executing the task itself; then containerize our Luigi infrastructure. To handle the problem of managing current task state, we implemented an AWS SQS-based task queue. To handle the actual task execution, we implemented a message-triggered task worker. The task queue continuously checks the current state of all our tasks,  then enqueues the needed tasks and their corresponding parameters to SQS.  In turn, the task worker is listening for messages from SQS and, upon receiving a message, will deserialize it to know which pipeline needs to be run with the specified parameters. Sitting between both of these components is a separate container that is running the core Luigi scheduler daemon. The task queue pings the scheduler’s HTTP task API to get the current state of running and pending tasks, which gets updated as task workers register their current workload with the scheduler daemon. As a result, we’ve seen a number of improvements: – We’ve been able to run more tasks more frequently, going from a previous theoretical and somewhat arbitrary cap of 32 tasks to an order of magnitude more. – We no longer have to worry about any lingering artifacts from previous run throughs because the Python runtime environment is created fresh for each run. – Tasks are now more testable and inspectable. This improvement comes from moving to a Kubernetes container infrastructure, which may restart and interrupt pods that Luigi is running on at any time. – Tasks can fail in isolation without interrupting the runs of others. – We can very easily scale the number of workers to handle changes in workload, eg catching up after an outage. There are still a number of things we can improve on: – When we rolled out this initial implementation, the team had not yet developed enough K8s experience. Now that we know more about the platform, we’d like to look into using K8s jobs, inspired by the Luigi 3rd party source code for running tasks inside of K8s. – In the case of a backlog, it is possible that one entire pipeline can take over the workers. We can solve this problem by dedicating workers to certain task types. Overall, we love Luigi for being a powerful, simple-to-use Python-based task orchestration framework. We can’t wait to scale it even further to give you even more GIFs! -Walter Menendez, Software Engineer, Data Engineering Previous Post Next Post", "date": "2019-05-14"},
{"website": "Giphy", "title": "ny-scala @ GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/ny-scala-giphy/", "abstract": "ny-scala @ GIPHY March 12, 2019 by GIPHY March 12, 2019 GIPHY HQ – New York, NY GIPHY Engineering is excited to host ny-scala! For this event, we’ll have a talk by Aesa, “A Tale of Two Type Systems” where he’ll address the questions below and more! “What are typeclasses and why do so many functional libraries use them? How do they fit into our standard understanding of inheritance and interfaces? What systems in the language exist that are designed to help us think about and model programs?” For more information and to register for the event, head here. Previous Post Next Post", "date": "2019-03-12"},
{"website": "Giphy", "title": "GIPHY Engineering Celebrates Women’s History Month!", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-engineering-celebrates-womens-history-month/", "abstract": "GIPHY Engineering Celebrates Women’s History Month! March 7, 2019 by GIPHY March is Women’s History Month! For this year’s special month, GIPHY Engineering is celebrating with a whole new sticker pack dedicated to Women Pioneers in STEM. All stickers were illustrated and brought to life by the amazing Denyse Mitterhofer , and you can find them on GIPHY.com, the GIPHY mobile app and anywhere you get your favorite GIPHY content! Keep reading to see the 15 women pioneers in STEM we’re honoring, in no particular order. Historical Figures: Grace Hopper Grace Hopper (1906-1992), AKA Amazing Grace, was an American computer scientist and United States Navy rear admiral. “A pioneer in the field, she was one of the first programmers of the Harvard Mark I computer” writes Scientific Women , “and invented the first compiler for a computer programming language.” Ada Lovelace Ada Lovelace (1815-1852), sometimes referred to as the  “Prophet of Computer Age,” was an “English mathematician and writer, chiefly known for her work on Charles Babbage’s proposed mechanical general-purpose computer, the Analytical Engine” according to Scientific Women , “she was the first to recognize that the machine had applications beyond pure calculation, and created the first algorithm intended to be carried out by such a machine” Sister Mary Kenneth Keller Sister Mary Kenneth Keller (1913-1985) “was the first woman in the US to be awarded a PhD in Computer Science in 1965, having previously studied Mathematics and Physics. Earlier she had taken her vows as a Roman Catholic religious sister in Ohio and in 1958 had started work at Dartford College in the male-only (at the time) computer centre” writes Computing History . Hedy Lamarr Hedy Lamarr (1914-2000) was an Austrian-American actress and inventor who pioneered the technology that would one day form the basis for today’s WiFi, GPS, and Bluetooth communication systems” writes Women’s History . “As a natural beauty seen widely on the big screen in films like Samson and Delilah and White Cargo, society has long ignored her inventive genius.” Lisa Meitner Lise Meitner (1878-1968) was an Austrian physicist who “was part of the team that discovered and explained nuclear fission and foresaw its explosive potential” writes the Atomic Heritage Foundation . “She refused to work on the Manhattan Project at Los Alamos, declaring,  “I will have nothing to do with a bomb!” Her epitaph on her gravestone, written by her nephew Otto Frisch, reads, “Lise Meitner: a physicist who never lost her humanity.”” Rukhmabai Raut Rukhmabai Raut (1864-1955) was one of the first women doctors to practice medicine in British India and staunchly opposed child marriage” writes Indian Express . “Raut went on to become India’s first qualified physician and was the major cause behind the enactment of Age of Consent Act in 1891.” Martha Euphemia Lofton Haynes Martha Euphemia Lofton Haynes (1890-1980) earned degrees in both mathematics and education, and in 1943 became the first African-American woman to receive a Ph.D. in mathematics. “She then took the educational system by storm, teaching in a wide variety of settings and pushing continually to change the face of education, which, at the time, often found black students falling into a system of de facto segregation” writes biography.com . Mary Golda Ross Mary Golda Ross (1908 – 2008) was the first known Native American female engineer, and the first female engineer in the history of Lockheed. “She is a pioneer in the research, development and application of the theories and concepts of ballistics, orbital mechanics and astrophysics, and is the first woman engineer in the history of Lockheed” reads her biography on the Silicon Valley Engineering Council Hall of Fame. Wang Zhenyi Wang Zhenyi (1768–1797) was a famous scientist from the Qing dynasty. “She breached the feudal customs of the time”, writes Scientific Women “which hindered women’s rights and arduously worked to educate herself in subjects such as astronomy, mathematics, geography, and medicine. She was a very strong and intelligent woman well known for her contributions in astronomy, mathematics, and poetry.” Dorothy Vaughan Dorothy Vaughan (1910-2008) “was an African American mathematician and human computer who worked for the National Advisory Committee for Aeronautics (NACA), and NASA, at Langley Research Center in Hampton, Virginia. In 1949, she became acting supervisor of the West Area Computers, the first African-American woman to supervise a group of staff at the center” according to Scientific Women . Barbara McClintock Barbara McClintock (1902-1992) was an American scientist who revolutionized the field of cytogenetics with her studies of chromosomes in corn. According to Biography.com , “she discovered the role of “controlling elements” in genetic regulation and transposition. Her work was considered too radical (or simply ignored) until it was replicated in the late 1960s. McClintock received the Nobel Prize in 1983.” Dorothy Hodgkin Dorothy Hodgkin (1910-1994) was a British chemist who developed protein crystallography, which lead to her winning the Nobel Prize in Chemistry in 1964. According to Scientific Women , “among her most influential discoveries are the confirmation of the structure of penicillin as previously surmised by Edward Abraham and Ernst Boris Chain, and the structure of vitamin B12, for which she became the third woman to win the Nobel Prize in Chemistry.” Elizabeth Garrett Anderson Elizabeth Garrett Anderson (1836-1917) was a pioneering physician and political campaigner, known as the first Englishwoman to qualify as a doctor. According to BBC , after being refused admission to study at medical schools in London, due to her gender, Anderson taught herself French and went to the University of Paris, where she successfully earned her degree. Later in her career, she “founded the New Hospital for Women in London (later renamed after its founder), staffed entirely by women.” Current Trailblazers: Carol Shaw Carol Shaw , is one of the first professional female video game designers, and is the programmer behind one of the Atari’s best-known shooter games, River Raid. River Raid was revolutionary because, for first time, it let gamers experience an inordinate amount of non-random, repeating terrain despite constrictive memory limits. River Raid was the first game that allowed the shooter to accelerate and slow down all over the screen. You can learn more about her from Computing History . Radia Perlman Radia Perlman often described as the ‘Mother of the Internet,’ insists that, “the Internet was not invented by any individual.” Perlman did, however, create the algorithm behind the Spanning Tree Protocol (STP), which is an essential part of the Internet’s underlying foundation. You can learn more about her story and accomplishments from BBC . – GIPHY Engineering A word from the artist: “It’s always exciting to work with GIPHY on any project. Especially when it comes to producing really awesome GIF stickers about incredibly smart and influential women who undoubtedly changed the world. As a creative, I feel so lucky that I get to create fun, colorful animations for others to share and talk about, while also learning. I hope I’ve done these courageous women justice with their tiny animated portraits. They were all surely created with lots of respect, admiration and love and I hope it translates!” – Denyse Mitterhofer, Art Director by day, compulsive GIF creator by night – GIPHY page: dmitterhofer – IG: @dmitterhofer Previous Post Next Post", "date": "2019-03-07"},
{"website": "Giphy", "title": "GIPHY’s AI Can Identify Lil’ Yachty, Can Yours?", "author": ["Nick Hasty"], "link": "https://engineering.giphy.com/giphys-ai-can-identify-lil-yachty-can-yours/", "abstract": "GIPHY’s AI Can Identify Lil’ Yachty, Can Yours? March 5, 2019 by Nick Hasty GIPHY is excited to announce the public availability of our custom machine learning model, The GIPHY Celebrity Detector, which is able to discern over 2,300 celebrity faces with 98% accuracy. The model was trained to identify the most popular celebs on GIPHY, and can identify and make predictions for multiple faces across a sequence of images, like GIFs and videos. This project was developed by the GIPHY R&D team with the goal to build a deep learning model able to annotate our most popular content at a level equal to (and ideally better than) similar fee-based models and services offered by major tech companies. We’re extremely proud of our results, and have released our model and supporting code with the hope others will build off our work, integrate the model into their own projects, and perhaps even learn from our approach. The model and code is available for download via GIPHY’s Github page . Motivation Entertainment and popular culture are at the heart of GIPHY. Half of all the search queries we get are entertainment related. Aside from cute animals, GIFs featuring celebrities drive more traffic across our API integrations, website, and mobile apps than any other type of content. Take a look at our top GIFs from 2018 and you’ll see a number of famous faces represented, like Cardi B, whose “okurrrrr” GIF has accumulated nearly 400 million views. Likewise, GIPHY Studios regularly collaborates with celebrities to create original content, like stickers , reactions , and even ads, as with our work creating gifs to promote the film “ Sorry to Bother You .” (Promotional gif for “Sorry to Bother You”) We needed a tool that could find and annotate this content within our ever-growing library of GIFs, so that this content could then be found in our search engine. Between our existing GIF library and the celebrity-based content we generate internally, we knew we’d have abundant training data from which to pull. Having total control of the model lets us update it as needed to maintain GIPHY’s unique position at the cutting-edge of popular culture. (GIPHY’s AI can identify Lil’ Yachty, can yours?) Model Training To generate our training data, we extracted all the celebrity names from the top 50,000 searches across all our platforms, including our website, mobile apps and integrations likes Facebook, Twitter, and Slack. This yielded a data set comprised of over 2,300 celebrity names (you can see the complete list here ). While we had a good amount of labeled training data already, we also needed to scour the internet for supplemental images for names which had a smaller representation in our catalog. The resulting dataset gave us lots of positive, representative images for the most-popular celebs, but the image sets we had for less-famous celebrities had more false-positives; as such, they were noisier. In order to refine these noisier sets, we used a separate model able to group images by similarity of facial features to evaluate the overall uniformity of each dataset. Clean datasets had smooth distributions, whereas noisy datasets were more unevenly distributed and clumpy. Those clumps in our noisy datasets that contained the most images tended to have similar distributions as our clean datasets, indicating those groupings were positive images of the celeb and could be safely used in training. This process helped us to de-noise and improve the accuracy of our model considerably. Celebrity Detection The celeb-detection process itself consists of two parts: face detection and face recognition. When a GIF or image is submitted to the classifier, it attempts to detect all faces across all frames using a popular pre-trained model called MTCNN . Each face is then sent through a deep convolutional neural network, based on Resnet-50 and trained on the dataset mentioned above, for recognition. The network itself is a facial features extractor which constructs a vector space of faces grouped together using center loss . Each face processed by the network is given a celebrity prediction along with a unique feature vector. Once all faces have received a prediction and vector set, as a post-processing step we use a GMM algorithm in a supervised fashion to cluster each face by its vector representation. For each cluster, an aggregate prediction is computed for all the faces within the cluster yielding one or more celebrity names, each with a confidence score. The model’s final output is the combination of these predictions across all clusters. Model Validation We took two approaches to model validation. First, we used the Labeled Faces in the Wild test. Our classifier scored a 96.8% accuracy. Then, we crowdsourced a labeled and verified a validation dataset consisting of almost 1000 popular GIPHY celebs. The model achieved 98% precision for 75% coverage on this validation dataset. Here is a coverage-precision curve for the results: Over the next few months we’ll be providing more details on this project here on the GIPHY Engineering blog, including a technical deep-dive and an overview of how we tested the model for different types of bias. Until then, we encourage you to download and play with the model and let us know if you come up with any cool use cases or extend the model’s capabilities for your own needs. – Nick Hasty, Director of R&D GIPHY R&D Team – Nick Hasty – Ihor Kroosh – Dmitry Voitekh – Dmytro Korduban Previous Post Next Post", "date": "2019-03-05"},
{"website": "Giphy", "title": "genEquality Hackathon Showcase + Pitch Competition", "author": ["GIPHY"], "link": "https://engineering.giphy.com/genequality-hackathon-showcase-pitch-competition/", "abstract": "genEquality Hackathon Showcase + Pitch Competition January 26, 2019 by GIPHY January 26, 2019 IDEAS42 – New York, NY Over the last four months, the genEquality Hackathon has created space and opportunities for teams to form and develop innovative solutions to alleviate gender inequality. The Hackathon has engaged participants across the globe – from the United States, Germany, India, Mexico, Japan, Myanmar, and more – all in service of building tech + tools that advance gender equality at work, at home, in culture, and in public. On January 26th, GIPHY’s CTO Anthony Johnson will take part as a judge at the pitch competition, where the winning team will have a chance to win seed funding and turn their product idea into a market reality! Join us by registering here! Previous Post Next Post", "date": "2019-01-26"},
{"website": "Giphy", "title": "Search Algorithms with PyLadies @GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/search-algorithms-with-pyladies-giphy/", "abstract": "Search Algorithms with PyLadies @GIPHY January 28, 2019 by GIPHY January 30, 2019 GIPHY HQ – New York, NY GIPHY started as a way to catalogue animated GIFS and since then has evolved as a platform of creation and discovery. Fundamentally, surfacing content has been a core challenge. We have a team dedicated to providing the best Search experience for animated GIFs. During this workshop we’ll take you through the history of search algorithms, from pre-digital days to web 2.0. This talk will be more of an overview of basic Search concepts which are applicable to any team looking for needles in haystacks. Register for free here! Previous Post Next Post", "date": "2019-01-28"},
{"website": "Giphy", "title": "DevOps & Drinks @ GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-devops-drinks/", "abstract": "DevOps & Drinks @ GIPHY June 18, 2018 by GIPHY June 21, 2018 GIPHY HQ – New York, NY GIPHY will host ‘DevOps & Drinks’ a meetup of DevOps Engineers in NYC who get together to discuss real world DevOps strategies with some of the top experts in their field, as well as conversations about the current state of the DevOps job market, employer hiring trends, how to balance multiple job offers, and much more. At this iteration of the meetup, Yuval Dovrat, Director of Solutions Architecture, Spotinst, will talk about “Playing Tetris with Containers – How to best utilize your infrastructure using k8s 2-levels Auto-scaler.” While GIPHY’s own CTO Anthony Johnson, will present on “Infrastructure At GIPHY Scale – The Ins and Outs of managing GIPHY’s Systems on A Lean Scale” Previous Post Next Post", "date": "2018-06-18"},
{"website": "Giphy", "title": "GIPHY @ Femgineering Lunch", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-femgineering-lunch/", "abstract": "GIPHY @ Femgineering Lunch May 21, 2018 by GIPHY May 24, 2018 Foursquare HQ – New York, NY Foursquare is hosting its first-ever “Femgineering” lunch in partnership with Union Square Ventures to facilitate impactful conversations around women in tech. The discussion will be led by Shani Offen, a talented Machine Learning Chapter Lead at Spotify. Natalie Weyerhaeuser, a Technical Lead who has worked on our Places by Foursquare team, will also join her on stage to help narrate the thought-provoking conversation. GIPHY engineers will be part of the 100 attendees from nearly 70 different companies across the industry. Previous Post Next Post", "date": "2018-05-21"},
{"website": "Giphy", "title": "GIPHY @ DataEngConf", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-dataengconf-2/", "abstract": "GIPHY @ DataEngConf April 9, 2018 by GIPHY April 17 – 18, 2018 Mission Bay Conference Center – San Francisco, CA DataEngConf is the first technical conference that bridges the gap between data scientists, data engineers and data analysts. Conference talks focus on examples of real-world architectures of data pipelines and platforms, and applied, practical examples of data science. This year in San Francisco, GIPHY’s own Platform Engineer, Sean Quigley returns to DataEngConf to speak about GIPHY’s cloud-based data architecture, Bayesian AB testing framework, and insights from using it in production. Previous Post Next Post", "date": "2018-04-09"},
{"website": "Giphy", "title": "GIPHY @ LDV Vision Summit 2018", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-ldv-vision-summit-2018/", "abstract": "GIPHY @ LDV Vision Summit 2018 May 21, 2018 by GIPHY May 23 – 24, 2018 SVA Theater – New York, NY LDV Vision Summit, the premier global gathering in visual tech where experts gather to explore how visual tech leveraging computer vision, machine learning & AI are empowering people, businesses and disrupting others. GIPHY is proud to be a part of this year’s LDV Vision Summit, where CTO Anthony Johnson will present his keynote, “Scaling GIPHY – The Legos of Visual Communication.” Previous Post Next Post", "date": "2018-05-21"},
{"website": "Giphy", "title": "GIPHY @ Machines + Media ’18", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-machines-media-18/", "abstract": "GIPHY @ Machines + Media ’18 May 12, 2018 by GIPHY May 15, 2018 Bloomberg – New York, NY NYC Media Lab’s Machines + Media, 2nd annual conference sponsored and hosted by Bloomberg – will focus on new applications of data science and technology in media and journalism. This year’s program will give special consideration to topics such as fake news and disinformation, changing business models, and the impact of automation on the media. GIPHY will be participating on a panel focusing on Automating Video & Images! Previous Post Next Post", "date": "2018-05-12"},
{"website": "Giphy", "title": "GIPHY @ Figure Eight’s TRAIN AI", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-figure-eights-train-ai/", "abstract": "GIPHY @ Figure Eight’s TRAIN AI May 9, 2018 by GIPHY May 9 – 10, 2018 PIER 27 – San Francisco, CA Train AI, presented by Figure Eight, is a two-day event for machine learning experts, forward-thinking executives, and product and engineering innovators to learn how AI is being applied to real business problems. Train AI features keynotes from industry leaders, real world case studies on practical machine learning applications, an exclusive Executive Briefing Center, and an AI solutions showcase with market-leading technologies and solutions. GIPHY was happy to be attend and have our CTO, Anthony Johnson, present ‘A GIF is worth a 1,000 words: Animated thoughts about the future of Enterprise Intelligence’. Previous Post Next Post", "date": "2018-05-09"},
{"website": "Giphy", "title": "GIPHY + Tech Kids Unlimited", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-tech-kids-unlimited/", "abstract": "GIPHY + Tech Kids Unlimited April 16, 2018 by GIPHY March 25, 2018 NYU Tandon School of Engineering – New York, NY GIPHY Engineering is a proud sponsor of ‘Stand Up for the Internet’ – a hackathon on digital citizenship, net neutrality, and privacy hosted by Tech Kids Unlimited (TKU). Students participating in the TKU Hackathon will work to explore issues related to internet health, namely net neutrality, online privacy, and digital citizenship. All students will participate in activities that involve thinking critically about what they share and how they protect themselves online, learning about practices like web tracking and password strength. Tech Kids Unlimited is a not-for-profit technology-based educational organization for kids ages 7 to 19 with special needs, which empowers and inspires the next generation of digital natives to learn, create, develop and share the tools of technology. Previous Post Next Post", "date": "2018-04-16"},
{"website": "Giphy", "title": "Dataiku + GIPHY + Meetup", "author": ["GIPHY"], "link": "https://engineering.giphy.com/dataiku-giphy-meetup/", "abstract": "Dataiku + GIPHY + Meetup March 13, 2018 by GIPHY March 20, 2018 NYC Data Science Academy – New York, NY GIPHY Engineering will be participating in an event hosted by Dataiku to present examples in which search engines can make or break a product. Our Lead Data Scientist, Yael Elmatad, will be speaking about how we develop our search engine to remain at the top of the social media frenzy. To hear about ‘Search at GIPHY: Staying on Top of the Zeitgeist’ – join us at the Meetup and RSVP here. Previous Post Next Post", "date": "2018-03-13"},
{"website": "Giphy", "title": "How to make GIFs with FFMPEG", "author": ["Collin Burger"], "link": "https://engineering.giphy.com/how-to-make-gifs-with-ffmpeg/", "abstract": "How to make GIFs with FFMPEG March 29, 2018 by Collin Burger INTRODUCTION To follow along, download media files here: https://github.com/cyburgee/ffmpeg-guide If you’ve worked with media encoding in the past decade it’s likely that you’ve come across FFmpeg. For those of you who are unfamiliar, in their words: “FFmpeg is the leading multimedia framework, able to decode, encode, transcode, mux, demux, stream, filter and play pretty much anything that humans and machines have created. It supports the most obscure ancient formats up to the cutting edge.” It’s packed with an enormous amount of functionality. It can be daunting for beginners. It can be baffling for experts. Maybe you can read the documentation and make some sense of it, or maybe you feel the same way about reading words as me. *JK – the FFmpeg docs are great* In that light, I wrote this post to share and explain some of its functionality, especially as it relates to GIF transcoding. To follow along you’ll need FFmpeg installed. The easiest way to do that is to go here and find a static build for whatever platform you’re working on. GIF TO VIDEO Let’s start with a simple example. Say you have a gif of my ancient, computer illiterate pug, Benji, and you want to convert it to a video format. Here’s one example: $ ffmpeg -i benji.gif -f mp4 -pix_fmt yuv420p benji.mp4 Let’s break down this command and its constituent arguments. ffmpeg This launches the FFmpeg executable. I’m assuming here that your shell knows the complete path to it. -i benji.gif This flag indicates our input. I assume that the i stands for “input”. I’m also assuming that benji.gif is a gif of my stubbornly smelly pug and that it is in your shell’s current working directory. -f mp4 This is optional in most circumstances. This tells FFmpeg that we want to output to an mp4 media container. FFmpeg will typically infer that from the extension supplied in the output file pattern, but when it comes to working with FFmpeg it doesn’t hurt to be specific. -pix_fmt yuv420p This is another optional flag, but I use it because it makes the file play nicely in the QuickTime Player, and subsequently Finder, on my Mac. benji.mp4 This last argument is our output. It’ll create the file if it doesn’t exist and if it already exists, you will be prompted on the command line to confirm if you’d like to overwrite the existing file. If you think you know better and are immune to pain and regret then you can add the -y flag somewhere in your command and it will automatically overwrite the file if it exists. *uses -y in ffmpeg indiscriminately* While the above generally works with most content, you may be interested to learn that GIFs support widely varying frame rates or frame delays that when converted to a video format without transformation, can trip up certain video players. If I were to use the previous command to convert my contrived and not very useful as a countdown example, 321.gif , the VLC player on my Mac has trouble playing that mp4. Don’t worry, I still love you VLC. If your particular application might be sensitive to such issues, you can try the following command: bash\r\n$ ffmpeg -i 321.gif -filter_complex \"[0:v] fps=15\" -vsync 0 -f mp4 -pix_fmt yuv420p 321.mp4 Some of the above arguments look pretty familiar, right? Let’s go over what’s changed. -filter_complex \"[0:v] fps=15\" This here is how you specify a filter graph. Filters process raw video or audio based on the name of the filter and the arguments specified. You can read more here. This is a very simple example of a filter, but if you read on you can see things get a bit more complex. Let’s decompose this one for now: [0:v] This specifies that we will be using the first video stream fed to FFmpeg as input to this section of the filter. If you had a file with multiple video streams, or you put two inputs in the command you could imagine selecting the second video stream with [1:v] , or the third audio stream as [2:a] , etc, etc. fps=15 This is the fps or “frames per second” or “framerate” filter. If you don’t want to lose any visual content, I suggest you set the value to the nominal framerate of the input, so the inverse of the minimum frame delay. -vsync 0 This vsync flag tells ffmpeg to not modify the timing of the frames as they come from the decoder. This should keep your weird gif timing intact. That about does it for GIFs to video formats. Here’s a handy guide for encoding H.264 video should you be interested in the ins and outs of that particular format. VIDEO TO GIF Note that these methods don’t play nicely with sources with transparency/alpha *yet* – hopefully more on that in a future blog post. Seeing the simplicity of the above examples, you might be tempted to create a gif with FFmpeg like so: bash\r\n$ ffmpeg -ss 61.0 -t 2.5 -i StickAround.mp4 -f gif StickAround.gif But I wouldn’t recommend it. File size issues aside, the quality probably just isn’t where you want it to be. The issue seems to be that FFmpeg’s native gif encoder doesn’t take into account the colors in the source image, and as an indexed color format, that just won’t do for a GIF. Let’s look at those new parameters though. -ss 61.0 The -ss option tells FFmpeg to seek to 61.0 seconds into the input file. Two “s”s indicate a fast seek but with recent FFmpeg it’s both fast AND accurate. -t 2.5 The -t option signifies that we only want to read in 2.5 seconds of the input video and then stop. So with these two arguments, FFmpeg will seek to second 61.0 and read in the next 2.5 seconds. The placement of these arguments are significant. Since they are in the command before the input -i StickAround.mp4 they apply to the reading of the input file. The values of the above two argument are totally dependent on what part of the video you want. Feel free to play around with them. Thanks to the work of some generous FFmpeg developers, namely ubitux , you can now generate a color palette to be used to generate a much higher quality GIF like so: bash\r\n$ ffmpeg -ss 61.0 -t 2.5 -i StickAround.mp4 -filter_complex \"[0:v] palettegen\" palette.png\r\n$ ffmpeg -ss 61.0 -t 2.5 -i StickAround.mp4 -i palette.png -filter_complex \"[0:v][1:v] paletteuse\" prettyStickAround.gif Let’s look at these exciting new arguments. -filter_complex \"[0:v] palettegen\" palettegen is a filter that generates a 256 color palette to be used in GIF encoding in the next step. Like the filter above, it uses the first video stream of the input, indicated by [0:v] . palette.png This is our output file. Easy! Now what about the following command? Let’s look at the input arguments. We now have: -i StickAround.mp4 -i palette.png We have two input files this time. The original video, and the palette, palette.png we created in the command just before. You can see how we use these in the filter arguments that come next: -filter_complex \"[0:v][1:v] paletteuse\" So, this is new. The paletteuse filter takes in two arguments, the first being the video content and the second being the color palette that gets applied to output a nice looking GIF. Sweet! You can see how two inputs are specified in a FFmpeg filter graph: [0:v][1:v] which corresponds to -i StickAround.mp4 -i palette.png Say you don’t feel like managing an intermediate palette file. It’s critical that you remember the exact timing of where you cut, you have to remember an extra filename, you have to not mess up the spelling of ffmpeg twice – it’s all too much! So, let’s get weird with that filter graph. Try this out: bash\r\n$ ffmpeg -ss 61.0 -t 2.5 -i StickAround.mp4 -filter_complex \"[0:v] split [a][b];[a] palettegen [p];[b][p] paletteuse\" FancyStickAround.gif About that filtergraph argument – this is the first one that lives up to the “complex” label. -filter_complex \"[0:v] split [a][b];[a] palettegen [p];[b][p] paletteuse\" First we’ve got the [split](https://ffmpeg.org/ffmpeg-filters.html#split_002c-asplit) filter. [0:v] split [a][b] split takes the first video as input and creates two outputs from the one input, as you might have suspected. I’ve labeled them [a] and [b] but you can call them [dog] and [cat] for all I care. Next up: ;[a] palettegen [p] The semicolon indicates that we’re specifying a new filter, and it’s the palettegen filter we all know and love. As you can see the input to this one is [a] that we defined as the output of the split filter before this one. The output of this filter is [p] which stands for “pug” or “palette” – whichever one you want to believe. *Approximately 80% of my thoughts are pug-related* Next our filtergraph has: ;[b][p] paletteuse\" We know this one too! The only twist here is that it uses the second output of our split filter, [b] as the first input and output of our palettegen filter [p] . Order is important here folks! There’s a drawback to doing it this way is that it can use more memory all at once, but overall you will probably save on memory usage, especially if you adhere to the suggestions that follow in the upcoming command. So that was nice, one-liner FFmpeg command to convert video to high quality GIF clip. But there’s still a problem. Our GIF is still too big. Unfortunately GIFs aren’t well suited to being 1080p even if they are 2.5 seconds long. Let’s fix that with this next command: bash\r\n$ ffmpeg -ss 61.0 -t 2.5 -i StickAround.mp4 -filter_complex \"[0:v] fps=12,scale=480:-1,split [a][b];[a] palettegen [p];[b][p] paletteuse\" SmallerStickAround.gif Most of the above is pretty familiar. We just have a couple of new additions to the filter graph. -filter_complex \"[0:v] fps=12,scale=w=480:h=-1,split [a][b];[a] palettegen [p];[b][p] paletteuse\" Let’s look at that first stage. [0:v] fps=12,scale=480:-1,split [a][b]; Our split filter from before has a couple of new friends – fps and scale . I’ve separated them with commas, which you can do with filters that accept a single input and output.  We’ve seen fps before. It changes the frame rate. In this case it took a 24 fps input video and halved the framerate to 12 as you can see. I think 12 fps is high enough for this content and gets our file size down, but decide for yourself.  The next filter in line here is scale , which resizes its input video. The scale filter needs two parameters, width and height, and they can look a little odd. To complicate matters they can take several forms. Here’s how I’ve written it: scale=w=480:h=-1 I could also be more explicit: scale=width=480:height=-1 How about a third option if that wasn’t confusing enough: scale=480:-1 Any way you write it, I’ve specified that I want to scale the input video to width 480 pixels and I want the height to be -1 pixels. Nah – that ain’t right. -1 has a special significance to the scale filter. It means I want the filter to maintain the aspect ratio or proportions of the video while resizing the other dimension I specified. “Pro” Tip 1: -2 has a similar meaning to -1 in the scale filter. It means to stay as close to the aspect ratio but round to an even number. This is important for many video codecs which require even dimensions, such as H.264. It doesn’t matter for GIFs. “Pro” Tip 2: If you’re changing the frame rate and resizing/scaling your video, resize before increasing the framerate or resize after decreasing the framerate. Your machine won’t have to resize as many frames – saving you time and processing power. Now we should have a lovely GIF that is a manageable size. But what if for some reason you didn’t think it was pretty enough? After all, that whole GIF only has 256 colors across all of its frames. Do you know where I’m going with this? *I have no idea what I’m doing* What if we could create a GIF with FFmpeg that has an insane 256 colors per frame? This is a pretty unique application that’s only really necessary if you’ve got some extremely varied colors, a long gif with multiple scenes, or both. Imma show you how to do it anyways. bash\r\n$ ffmpeg -ss 61.0 -t 2.5 -i StickAround.mp4 -filter_complex \"[0:v] fps=12,scale=w=480:h=-1,split [a][b];[a] palettegen=stats_mode=single [p];[b][p] paletteuse=new=1\" StickAroundPerFrame.gif Not very different right? This just leverages some of the options of the palettegen and paletteuse filters. Let’s take a look. [a] palettegen=stats_mode=single [p] We’re now specifying the stats_mode parameter of the filter. The argument single tells the filter to generate a new palette for every input frame. Next we specify a corresponding parameter for paletteuse [b][p] paletteuse=new=1 The new parameter for paletteuse indicates to the filter that it should grab a new palette for each frame, working in beautiful harmony with the palettegen=stats_mode=single filter stage. The drawback of doing a per frame palette is that it can increase the file size. But it could also reduce the file size. GIF optimization is weird. But that would have to be the subject of another blog post. Let us know if you want that one! I hope this has been a useful breakdown of FFmpeg and some GIF-related commands. Happy GIFfing folks! Github link – https://github.com/cyburgee/ffmpeg-guide — Collin Burger, Director of Content Engineering Previous Post Next Post", "date": "2018-03-29"},
{"website": "Giphy", "title": "Getting Started With Artificial Intelligence", "author": ["Nick Hasty"], "link": "https://engineering.giphy.com/getting-started-with-artificial-intelligence/", "abstract": "Getting Started With Artificial Intelligence February 27, 2018 by Nick Hasty Artificial Intelligence, also known as AI, is a hot topic these days and at GIPHY this is no foreign term. We work with Google Cloud ’s machine learning tools on multiple levels, but ultimately to help you search. To dive deeper into this topic, we sat down with James Maguire from Datamation to provide insight on the impact this emerging technology can have on businesses. The topics covered by Nick Hasty, Director of R&D, include: – How we leverage the Google Cloud AI platform – The challenges faced in getting started and progressing with the tool – The results seen from the AI deployment – Future plans for using AI For more great info on how AI is intertwining with the business world, head over to Calendar.com’s Guide to AI! Previous Post Next Post", "date": "2018-02-27"},
{"website": "Giphy", "title": "GIPHY Dancing Stickers are all the Hype(r)", "author": ["Jonny McLaughlin"], "link": "https://engineering.giphy.com/dancing-stickers-is-all-the-hype/", "abstract": "GIPHY Dancing Stickers are all the Hype(r) April 20, 2018 by Jonny McLaughlin Mission Make this jazz corn dance across my terminal window whenever I deploy code via Github. Introduction I’ve been using Hyper, a terminal emulator built with Javascript, for a while now and I love it. It’s built with React, which is the same technology that powers the front-end of GIPHY.com. It’s very customizable and, with a little React/JS knowledge, you can make it do whatever you’d like. I chose to harness this power to make stickers dance around my screen whenever I would perform certain actions. The plugin was hacked together in a few hours for pure enjoyment purposes but more features are totally welcome in the form of pull requests! I may also be adding things here and there. Installation Go to your preferences and add “hyper-giphy-stickers” to your plugins list. https://github.com/jonnymclaughlin/hyper-giphy-stickers Below are the currently supported keywords: – pull – push – start – deploy – publish PS – there also may or may not be a special treat when 4:20pm rolls around, but that all depends on who’s asking… via GIPHY — Johnny McLaughlin, Director of Engineering Previous Post Next Post", "date": "2018-04-20"},
{"website": "Giphy", "title": "Scaling Redshift without Scaling Costs", "author": ["Niger Little-Poole"], "link": "https://engineering.giphy.com/scaling-redshift-without-scaling-costs/", "abstract": "Scaling Redshift without Scaling Costs March 6, 2018 by Niger Little-Poole Drowning in Data Interactions with GIPHY via our apps and API network are generating 3+ billion events a day. Over the years we’ve built and maintained infrastructure to process and analyze all this data to improve our search algorithm , power our view counts , identify cultural trends , and much more. Today we rely on a combination of tools like Kinesis and Spark to move and transform data. Like many organizations, we use Amazon’s Redshift to handle much of our data warehousing (we use BigQuery too, but that’s another story). Redshift is a distributed columnar database. Similar to EC2 instances, Redshift is billed on a per instance, per hour basis. Amazon offers two instance types, Compute (DC2) and Storage (DS2). The former is optimized for query performance, and the latter for data storage. The size, and therefore cost, of a Redshift cluster depends on how much data we have and whether or not we are willing to sacrifice a bit of performance to achieve extra storage. via GIPHY With the amount of data we are consuming, we realized that the costs of analyzing it was increasing every month. As we add more data at an accelerating rate, we scale our cluster up to keep pace with storage. Many organizations circumvent this by not storing historical data in Redshift. However, the nature of our business necessitates easy, interactive access to historical data. Understanding what is going to be the best Valentine’s Day GIF requires looking at past Valentine’s Days and identifying patterns. The ease of data access allows our lean team to service the needs of our hundreds of millions of users at scale, while not increasing our infrastructure costs at the same rate. While GIPHY is by no means un-resourced, this efficiency is important to our overall structure. The ease of data access allows our lean team to service the needs of our hundreds of millions of users at scale, while not increasing our infrastructure costs at the same rate. This efficiency is critical to our ability to iterate. Endless Tuning Beyond costs, our team invests a lot of manpower in running Redshift optimally. Tons of blog posts, Stack Exchange answers, and Quora posts break down all the ways to tune and optimize Redshift. Still, it seems like there are a billion settings to tune,  and always a new one to learn.  For example, should I use DIST_ALL for my dist key? When is it better to use a COMPOUND sort key vs INTERLEAVED ? Don’t forget to vacuum your tables, but the right cadence and schedule is needed because you can only vacuum one table at a time, and the entire cluster takes a performance hit! I’d be doing you a disservice if I didn’t remind you to analyze your tables, too. What happens when you eventually need a bigger cluster? Resizing a Redshift cluster makes it Read-Only, and in some cases can take hours. Scheduling and coordinating a time to potentially not have fresh data for the entire company is far from ideal at GIPHY’s scale. via GIPHY Redshift Spectrum As with all our architecture, be it K8s or Redshift, we’d like to stop scaling our costs linearly, and make our data warehouse less complex to manage. In this case, we’ve found a solution in Redshift Spectrum. Announced in April 2017, Redshift Spectrum is a feature that allows Redshift to query data that does not live in the cluster, but rather on S3. Spectrum is seemingly a sibling to Amazon’s Athena product. Both are fully managed solutions based on Presto allowing for distributed SQL run against S3. They both cost around $5 per terabyte scanned,a similar cost and model to  Google’s BigQuery. With Spectrum, our largest event tables can live on S3 rather than being distributed on the disks of the cluster itself. S3 becomes our data lake and Redshift remains our data warehouse. When we initiate a query requiring data on S3, Amazon elastically scales out resources for the query and we pay only for the amount of data accessed. Given the vast majority of our queries happen during work hours, and few are full table scans, we end up saving more money by paying for these on-demand queries than we would running multiple instances 24/7. Building Things is Hard The key to minimizing our costs with Spectrum is reducing the amount of data we need to read in order to perform a query. Our first step to achieving this was denormalizing our events to include the values we commonly group against.. Spectrum can also use the folder structure of S3 as an index. If wanted to make querying by day easy, we could sort all our data into folders in our bucket with names like date=2018-01-01 and Spectrum could use the folder metadata to pick only the folders needed based on the SQL call. By nesting folders, we can essentially mimic a B+ tree index, similar to what is found in most row-oriented RDBMS. Each folder we set up contains a very small amount of data, minimizing the amount of data Spectrum actually read at query time.We came up with a complex folder structure, with millions of nested folders. via GIPHY While an interesting idea in theory, we discovered three problems. First, we ran into rate limiting issues trying to quickly create so many files and buckets in S3 from Spark. We thought about contacting AWS to get around this, but ultimately the overhead of writing so many files was making our Spark jobs too slow for feasibility. We also discovered Spectrum’s overhead for analyzing the tree of folders in a bucket; we lost performance from having too many. Lastly we discovered that there was a decent bit of latency from Spectrum in the following areas: 1. Parsing our data from text formats on S3 2. Filtering & aggregating the data 3. Transfering to our Redshift cluster Parquet We were able to solve all of these problems by storing all our data in Parquet format. Parquet is a binary, column oriented, data storage format made with distributed data processing in mind. We have been using Parquet  for a long time in other parts of our data infrastructure due to great interoperability with Spark. Parquet can be slower to write than CSV or JSON but it is substantially faster to read thanks to a reduced serialization cost and column-oriented efficiencies. To find the sum of all clicks in a day, a system only has to read the parts of the Parquet file that has that column, not the rest of the data in the file. This is both faster and saves us money on Spectrum costs. Additionally, each Parquet file has metadata detailing the range of values that can be found inside. We can take advantage of this by writing spark like the following: scala\r\ndf.repartition($\"date\")\r\n  .sortWithinPartitions($\"country\", $\"content_id\")\r\n  .write.partionBy(\"date\")\r\n  .parquet(\"s3:// \") I won’t get into the weeds of the above, but essentially we can tell Spark to write Parquet files to the bucket where we sort all the files into folders based on the day (partitionBy). Within each of these folders, we have a number of Parquet part files. Each part file has its columns sorted by country and content_id . If we run the following query in Spectrum: sql\r\nSELECT sum(clicks) from click_events\r\nWHERE date = '2018-01-01' and country='US' First, Spectrum can quickly navigate through the directory structure in our bucket to find the date=2018-01-01 folder. Then it can read the metadata in each Parquet file, noting which files have a max min country range that US would fall into. By using sortingWithinPartitions in Spark, we ensured this metadata will be present in each part file. Because these files are independent, Spectrum can parallelize the metadata inspection, yielding even better performance. The sums can also be parallelized. Each Spectrum process/worker only needs to sum all the clicks found in the files it has identified as valid. All the click numbers will be on disk together, because of columnar storage, so the worker can just sequentially scan on the disk and sum what it reads. Spectrum can sum all the intermediate sums from each worker and send that back to Redshift for any further processing in the query plan. Conclusions Overall the combination of Parquet and Redshift Spectrum has given us a very robust and affordable data warehouse. Pros – No Vacuuming and Analyzing S3 based Spectrum tables – Diminishing Marginal Costs – We can still write to S3, even during a cluster resize – Can utilize features like S3 glacier and versioning for more robust data backup and restore Cons – There is additionally latency added by Spectrum queries – We have to be careful to avoid full table scans – When doing joins the Query planner will try to move all the S3 data to the redshift cluster and do the join there, this eliminates most of the performance gain of this setup – We can’t change the schema. Spectrum doesn’t have the mergeSchema ability that spark does so creating a new schema means creating a new table and using a view to union them. — Niger Little-Poole, Data Scientist Previous Post Next Post", "date": "2018-03-06"},
{"website": "Giphy", "title": "How Video Formats Work", "author": ["Bjorn Roche"], "link": "https://engineering.giphy.com/how-video-formats-work/", "abstract": "How Video Formats Work January 31, 2018 by Bjorn Roche Introduction In my presentation on GIFs , I focused on how GIFs work and how GIFs compare to modern image formats. Let’s look a bit at modern video formats and see how they compress data. Here at GIPHY, we obviously work a lot with the GIF format, but you might be surprised to learn that we also work a lot with video. Much of our incoming content is video, and we often display and deliver video rather than GIFs because modern video formats both look and compress better than GIFs. But modern video formats are complex beasts: more complex than GIFs. To understand them, we need to think about color formats, chroma subsampling, resolutions, container formats and codecs. I’m assuming you are already familiar with basics like resolution and frame rates, but let’s break down the rest. Color Format and Chroma Subsampling Many developers are used to thinking about color in terms of red, green and blue, or RGB. These are the colors we use for display, when defining web colors, and often when working with raw pixel data. You might also be familiar with HSB, CMYK, or other “color spaces” used in programs like Photoshop, but you are probably less familiar with the colorspace used natively in most video formats: YUV (Footnote: For the purposes of this post, we are making some generalizations about color spaces . You might hear about other color spaces in the same family as YUV that have other names like Y’UV, YCbCr and so on. These are different, but since they belong to the same family, we’ll treat them as one). Historically, YUV’s advantage was the ability to add color to  black and white television broadcasts without interfering with existing signals or adding unnecessary information. While this kind of compatibility is not needed in the digital age, YUV still has a major advantage over other color spaces: this color space separates luminance (which can be thought of as brightness) from chrominance (which can be thought of as color), so we can apply different levels of compression to brightness and color. This separation is convenient because color is less significant to our perception of image quality than brightness.. By reducing the resolution of the chroma components relative to the luminance components, we can significantly reduce the bandwidth requirements of the video with almost no visual impact. This is called “ chroma subsampling ”, and is indicated using 4:X:Y notation . For example, 4:2:0 video has half the chroma resolution in both the horizontal and vertical direction, and is extremely common in consumer formats, such as Blu-ray. 4:4:4 video, on the other hand, uses no chroma subsampling, and is usually considered overkill, even for professional video. Codecs Once the color is converted to YUV and subsampled as required, the video can be further compressed using any number of available codecs, such as H.264, VP8, Sorensen and Cinepak. Despite the wide variety, the codecs have much in common and it is possible to make some broad generalizations. Like image compression formats, modern video formats can compress spatially, meaning pixels can use information from their neighbors to reduce storage requirements. However, they can also compress temporally, which means that frames can use information from nearby frames to reduce storage requirements. To enable temporal compression, individual frames can be divided into three categories: – Intra-Frames , or I-Frames , contain a complete image. Because of this, they are not dependent on adjacent frames for display, and do not enable temporal compression. I-Frames are sometimes called “key frames.” – Predicted-Frames , or P-Frames , are dependent only on previous frames for display. – Bidirectional-Frames , or B-Frames , are are dependent on both prior and subsequent frames for display. Complicating the matter is the fact that some modern codecs allow frames to be broken up into “macroblocks” or “slices”, which can be individually treated as intra, predicted, or bidirectional. Any given frame can be divided into rectangular regions, and those regions can be treated as separate types of frames. Obviously B-Frames allow for the most compression, but they can be complex to encode and decode. On the other hand, I-Frames allow for the least compression, but are easy to encode and decode. Because I-Frames can be rendered without reference to other frames, they represent points in playback that don’t require any buffering. As a result, I-Frames play an important role in “scrubbing,” skipping, video editing, streaming, and ensuring consistent picture quality. A set of images bounded by I-Frames is sometimes called a “Group of Pictures” or GOP. A GOP can be thought of as an atomic unit: it can be operated on and transmitted independently, without reference to other video content. GOPs therefore play an important role in streaming, where videos need to be broken into pieces for efficient delivery, and encoding, where videos need to be broken into pieces for parallel processing. Container Formats Once compressed, video must be stored in a “container”, such as OGG, AVI, FLV, MP4, MPEG, QuickTime, WebM , etc. Containers can be thought of as the box that the encoded data goes in. A physical box might have a shipping label with information about what’s inside, and might contain one or more items within. Similarly, video containers store metadata, ranging from information about the codec itself, to copyright and subtitle information. They also allow multiple video and audio streams to be packaged in one file. Most container formats are organized into “blocks” or “chunks.” These blocks allow readers to skip over sections they are unable to read or not interested in. Of course, readers that are unable to read certain chunks may render the video incorrectly, but in principle this design allows for formats to be extensible, and for readers of different types to gather the minimum amount of information they need as easily as possible. Of course, not every container format works like this. Some formats, especially old audio formats, are simply divided into metadata and data. But as requirements for things like interleaving data and extensibility have increased, newer formats are usually more complex. Some formats, like the MPEG Transport stream, include error correction and synchronization, which might be useful over unreliable transports, but usually just add overhead when sent over TCP/IP. Putting it All Back Together So far, we’ve discussed the particulars of video formats in the order you might need to think about them for encoding. To decode a file, you need to think about things in the opposite order: first you need to read the container format, then read the codec, and finally convert the (usually) YUV data to RGB for display, taking chroma subsampling into account. Because some of the steps involved are lossy, you might not get back exactly what you started with, but for most modern codecs, the goal is for the result to be as close to the original as possible. -Bjorn Roche, Sr. Media Pipeline Engineer Previous Post Next Post", "date": "2018-01-31"},
{"website": "Giphy", "title": "Scaling GIF Data Storage to Infinity and Beyond!", "author": ["Nima Khoshini"], "link": "https://engineering.giphy.com/scaling-gif-data-storage-to-infinity-and-beyond/", "abstract": "Scaling GIF Data Storage to Infinity and Beyond! January 23, 2018 by Nima Khoshini Introduction Everyday here at GIPHY, we have GIFs on GIFs on GIFs uploaded to our platform. In fact, we serve over three billion GIFs a day (that’s a lot of dancing cats!) to over 300 million daily active users. Every upload, however, brings new information and an increasingly large amount of data—this can become a lot to manage. Using database services like MySQL and DynamoDB, we’re able to organize this heavy amount of data in a high-performing way. Scaling our database infrastructure To understand how much data we’re talking about, let’s first walk through exactly what information is behind every GIF. For starters, size matters. Each GIF uploaded generates a good amount of data that we need to manage. GIFs are transcoded into different sizes (called renditions) which are optimized for different screen types: tablets, desktops and phones, etc. In addition, our machine learning models analyze and annotate each GIF with a lot of data, such as what celebrity might be in the GIF, what its MPAA content rating is, and how it relates to other GIFs in our catalog. Along with all this metadata come associated performance costs that could hinder our ability to scale and grow. To address this challenge we turned to a datastore solution by AWS called DynamoDB and found it to be a perfect companion to our existing data store. With DynamoDB, we had the flexibility to continuously grow the size of our data store without incurring these performance costs. Combined with DynamoDB Accelerator (DAX), we achieved better performance than with a traditional relational database. Success with DynamoDB Over a period of several weeks, we migrated all of the metadata accumulated from different GIF sizes from MySQL to DynamoDB. At each step, we measured the performance impact on the system and witnessed an overall improvement in read functionality of around 25%. As a result of moving high frequency data from MySQL to DynamoDB, overall system performance was improved. The below diagram illustrates the role of DynamoDB in our stack: Conclusion For any company experiencing rapid user growth, scaling data can be a hard and daunting task. Here at GIPHY we were pleased to learn that AWS offers a variety of services that made sure our systems could adapt for more user growth. In our case, RDS with a Dynamo/DAX solution was the perfect combination to alleviate our growing pains. — Alex Hoang, Services Engineer — Nima Khoshini, Services Team Lead Previous Post Next Post", "date": "2018-01-23"},
{"website": "Giphy", "title": "Introducing a new look for GIPHY Engineering", "author": ["GIPHY"], "link": "https://engineering.giphy.com/fresh-new-look-for-giphyeng/", "abstract": "Introducing a new look for GIPHY Engineering January 17, 2018 by GIPHY Technology is changing the way we communicate and the way we communicate is driving innovation in technology. GIPHY Engineering is revolutionizing both – when words don’t cut it, say it with GIPHY. Follow us on Twitter @GIPHYENG for all the exciting GIPHY Engineering updates! Previous Post Next Post", "date": "2018-01-17"},
{"website": "Giphy", "title": "Debugging Memory Leaks in Swift’s Enumerations Data Type", "author": ["Cem Kozinoglu"], "link": "https://engineering.giphy.com/debugging-memory-leaks-in-swifts-enum-type/", "abstract": "Debugging Memory Leaks in Swift’s Enumerations Data Type January 11, 2018 by Cem Kozinoglu Background We have been working on developing the GIPHY Core SDK for the open source community & 3rd party developers. So, if they’re developing an iOS or macOS app, they can easily integrate GIPHY search, trending topics, trending GIFs, Stickers, etc. directly into their apps without needing to re-invent the wheel. We decided to use Swift 3 as our main programming language. The Problem To make sure we became a consumer of our own product and battle tested it, we dogfooded the GIPHY Core SDK into our own flagship product, GIPHY Search ( https://itunes.apple.com/us/app/giphy-the-gif-search-engine-for-all-the-gifs/id974748812?mt=8 ) Just like with any other development lifecycle, we started to profile and see how the GIPHY Core SDK was performing inside the GIPHY Search app, and to see if there were any memory leaks. We were caught off-guard as things were not working the way they were supposed to. We ran into thousands of memory leaks! Debugging time! So we started debugging, trying to figure out what we potentially did wrong in the GIPHY Core SDK to see if it was caused by our code. It turns out, we couldn’t fix it. We exhausted all our options, rewriting code, involving our peers, changing data models. Gene even did a work around to convert Enums into Objects which stopped the leaking. However it was still clear that there were something going on with the compiler and we wanted to get to the bottom of it. AHA Moment! While we were keen on trying to figure out what was going on, I ran into a question in the StackOverflow community which had similarities to what we were experiencing: https://stackoverflow.com/questions/42602301/swift-3-enums-leak-memory-when-the-class-contains-an-array We created an public open-sourced GitHub repo ( https://github.com/GIPHY/ios-memory-leak-sample ) and with a few modifications to the original StackOverflow problem, voila! We were able to consistently reproduce the memory leak, and even managed to find a workaround. How to Reproduce the Leak The issue was, when a Swift class contains an optional Enum property that is marked with @objc, and this same class also contains a mutable (var) array holding at least one element, there is an unexpected memory leak. Here is an example: Gist Embed // Without @objc this enum won't leak\r\n// however when this enum is included in a class\r\n// which contains an array, it will leak\r\n@objcenumleakingObjCMarkedEnum: Int{\r\n    // Just some random cases.\r\n    caseapple, orange\r\n}\r\n// Wrapper class which contains an enum and Array\r\n// The class needs to contain the the Array in order for\r\n// the Enum to leak.\r\nclassWrapperClass { \r\n  // Optional enums marked with @objc will leak.\r\n  var leakyOptionalEnum: leakingObjCMarkedEnum?\r\n  \r\n  // Include an array to trigger this behaviour.\r\n  // Empty arrays won't cause the leak, so lets add an arbitrary Int\r\n  var myArray: [Int] = [80]\r\n}\r\n\r\nclassViewController: UIViewController {\r\n  // Hang on to a reference to our Wrapper Class instance.\r\n  var wc: WrapperClass?\r\n\r\n  overridefuncviewDidLoad() {\r\n    super.viewDidLoad()\r\n    \r\n    // Allocate an instance of our class\r\n    // and things will start leaking at this point.\r\n    wc =WrapperClass()\r\n  }\r\n} How to Prevent the Leak If we convert the ‘leakOptionalEnum’ optional var to a non-optional var the leak disappears. // Let's convert the optional property to a non-optional\r\nvar leakOptionalEnum: leakingObjCMarkedEnum = .apple Command + I -> Leaks Instrument -> Record Conclusion We filed a Radar to the Apple/Swift community ( https://bugs.swift.org/browse/SR-5625 ) and in less than 10 hours, a member of the community took care of the bug and merged it into the Swift repo ( https://github.com/apple/swift/pull/11341 ) & Xcode Version 9.0 beta 6 (9M214v). The bug fix, as implemented by community member @slavapestov: since objective c enumerations “lower as their raw type”, they should go through the same code path as imported enumerations.  This change applies to how the xcode compiler handles metadata related to these types of Enumerations when compiling projects that are hybrids of Swift and Objective C. In conclusion, it was an awesome experience to see how quickly the Swift Open Source community is and we are excited to be a part of this journey with them. — Cem Kozinoglu via GIPHY Previous Post Next Post", "date": "2018-01-11"},
{"website": "Giphy", "title": "Enhancing GIPHY Search with Google Cloud ML Tools", "author": ["Nick Hasty"], "link": "https://engineering.giphy.com/enhancing-giphy-search-with-google-cloud-ml-tools/", "abstract": "Enhancing GIPHY Search with Google Cloud ML Tools December 8, 2017 by Nick Hasty Editor’s Note: Earlier this week our Director of Engineering, Nick Hasty, was a guest author on the Google Cloud Big Data and ML blog. Nick touched on the use of Google Cloud Machine Learning to analyze and tag our GIFs, ultimately making it easier to find the perfect GIF on GIPHY. The following blog post is a condensed and modified version of the post published on Google Cloud Platform blog. Introduction Here at GIPHY, we serve over three billion GIFs a day to over 300 million daily active users, and are constantly looking for ways to improve the results of their GIF searches. Recently, we’ve integrated Google’s Cloud Vision and Cloud NLP APIs into our GIF processing pipeline, which has helped us collect more metadata about our GIFs and enhance the core GIPHY search engine. Specifically, using Web Entities data from Cloud Vision helped us boost our search performance and using syntax data from Cloud Natural Language and the Knowledge Graph API let us create rich titles for our GIFs. With entertainment and culture at the heart of our content, we need to be able to identify specific instances of celebrities, animals, movies, sports, video games, and more. The incredible pop-culture wizards in our in-house editorial team crawl through our catalog identifying and annotating our content, but at our current size, they need help. While we wanted to leverage machine learning to supplement our editors, our need for specificity presented a challenge. The depth of this problem grew with every GIF crawled or uploaded to our site. We knew it was possible to train custom ML models in-house, and we do have a large amount of labeled data, but we needed a solution fast. After some research, we believed Google offered the most mature service and the widest array of quality models. After a few exploratory tests with positive results, we decided to go big and processed over 10 million GIFs across using the full array of Google Vision services. Here’s an overview of what we did. In search of subtitles (and other text) Our first integration of the metadata generated from Cloud Vision into our search engine was to use its optical character recognition (OCR) endpoint, which evaluates images for the presence of text, notably subtitles. OCR detects text that is integrated into the actual pixel data of an image, and because this text can change over frames, like with subtitles, we wanted to make sure we captured as much text data as possible. To begin evaluating a GIF for text, we sent off an initial frame to see if the API detected any text. If it did, we sent another frame and computed the difference in text values to see if the the text found in the GIF was static or dynamic across frames. If the text differed enough across frames, like in the case of dialog subtitles, then we repeated this process across a percentage of total frames until we had sufficient coverage to perform textual analysis. You can check out  our sample Python code and try it for yourself-all you need is a captioned GIF with which to test and your Google Cloud credentials. Contextual labels with Web Entities After our success with OCR, we focused on integrating Google’s label data, specifically the Web Entities, which were great for discovering the additional specifics about the image. Cloud Vision provides two types of image labels: “Label Detection”, which provides labels for objects that Google’s machine learning models are trained to detect, and “Web Entities”, which yield labels derived from the context in which the image was crawled and indexed. Since Web Entity labels take into account the data embedded around an image, like the surrounding text or captions, they tend to be very specific and can even provide proper nouns. For example, while Cloud Vision doesn’t have a model specifically trained to identify a specific celebrity, if it discovers an person’s image across multiple websites and in each case that image is displayed with a caption containing a proper noun, then the Web Entities endpoint labels that image with that proper noun. Toward better GIF titles A parallel project to using Web Entities was to improve the algorithm for generating titles for our GIFs. GIF titles are most noticeable on our GIF detail pages-right above the GIF itself. Initially, creating these titles was fairly simple and involved choosing the most popular tag for that GIF. This worked for a while, but with an ever-growing catalog we were creating many duplicate titles and lacking specificity. For our new title schema, we wanted to be as descriptive as possible, with special emphasis on including the names of famous people, fictional characters, or TV shows, as well as actions, emotions, or reactions. Our schemas required metadata about, say, a tag’s part of speech and whether or not the tag was a proper noun. There are many solid open-source libraries for determining syntax, but we ended up using Google Cloud Natural Language asit provide very precise syntax detection and its Entity Recognition also provides a great way to identify proper nouns. Entity Recognition identifies known objects in a block of text, and provides Google Knowledge Graph IDs when available. This tight coupling between APIs helped us identify tags that refer to specific people, places, and things and highly specific metadata about those objects. After processing our tag catalog, we began putting the data to use and generating new GIF titles. So far we’ve been thrilled with the results. Check out some of our before and after GIF titles: In Conclusion Historically, taking advantage of the latest developments in machine learning and computer vision required in-house specialists and lots of time-two things that growth-stage startups like GIPHY don’t always have. Although these technologies are becoming more accessible, easier to use, and faster to implement, their ultimate success depends heavily on both the quality and quantity of data you have available for training. Google Cloud’s Machine Learning APIs provides tremendously powerful, cutting-edge models that are just a request away. To learn more about the technical details visit the Google Cloud Platform blog. –Nick Hasty, Director of Engineering Original featured GIF by Leroy Patterson, GIPHY Studios Artist Previous Post Next Post", "date": "2017-12-08"},
{"website": "Giphy", "title": "The Making of a GIF Music Video", "author": ["Bjorn Roche"], "link": "https://engineering.giphy.com/the-making-of-a-gif-music-video/", "abstract": "The Making of a GIF Music Video December 1, 2017 by Bjorn Roche When singer/songwriter Cassandra Kubinski recently visited the GIPHY office, she got inspired and thought of the idea to create a music video made entirely of GIFs. We weren’t sure if this had ever been done before, but it didn’t really matter – it sounded too fun not to try it ourselves. Cassandra has a great newly released Christmas EP called Holiday Magic and we both agreed that “It Doesn’t feel Like December” would be a perfect track for GIFs. While all the songs on the album are fantastic, this track’s silliness (it even has a Kazoo!) really sets the mood for fun GIFs. I suggested we could do it automatically using the song’s lyrics and GIPHY’s search and showed Cassandra my first pass. While it was no masterpiece, it was fun enough to keep it going. It took a little more work, but much less than I expected, and the results were so good, and so positively silly, that Cassandra decided to release the video on December 1st, in honor of the song’s title. The tools I used to build the video were nothing more than the GIPHY API, some open source tools including the fantastic Aubio and FFmpeg libraries, and a few simple shell scripts. I thought the shell scripts wouldn’t be useful for anything past proof of concept, but it turned out that they were good enough and I didn’t need anything fancier. The general approach was to: Split the song into beats using Aubio Assign a term or phrase to each beat (using lyrics if there were any, and words that fit the theme of the song if not) Search the GIPHY API for GIF that matched each term Edit the GIFs together Watch the finished video and tweak When I started, I wondered how much of the process could be automated. For the most part, I found that the only manual editing I needed was choosing the right thematic words, editing lyrics to remove words that resulted in incongruous searches, and then simply deleting GIFs I didn’t like or didn’t think fit. (Some of this process was entertaining in the way that only GIFs are: for example, I originally included the word “freeze” in my thematic words, forgetting that it means not just “very cold” but also “stop moving.”) Splitting the song and assigning search terms Splitting the into beats was easily accomplished by the “aubiocut” script that comes with Aubio, and results were saved in a CSV file: aubiocut -i in.mp3 -b | sed  '/[^[:digit:].+-]/d' | sed 's/$/,/' > times.csv That CSV file was edited manually by adding search terms to each line. Searching the Giphy API Making API calls and parsing JSON in shell requires a little help from curl and python. I used the translate endpoint , though search and random could also be used. You’ll need to get yourself an API key if you want to do this yourself, but the url is going to be something like this: api.giphy.com/v1/gifs/translate?api_key=${apikey}&rating=g&s=$tag\" You’ll then want to parse out the URL, to select the “rendition” you want. I decided to take the “original_mp4” rendition, which is usually smaller than a GIF, and higher quality in some cases. You can do that with something like this: python -c 'import sys, json, pprint; pp = pprint.PrettyPrinter(indent=4); print( json.load(sys.stdin)[\"data\"][\"images\"][\"original_mp4\"][\"mp4\"] ); ' In my code, I saved the results to another CSV. Editing the GIFs together After looping through the new CSV and fetching the GIFs, they have to be edited together. This is a bit tricky because the GIFs may be too short or too long, and may have different frame rates and sizes. I decided to standardize on 30 FPS and letterbox all GIFs to 1280×720. The code to do this, and trim to required length () is a bit of an eyesore, but it can all be done in one line with FFmpeg: ffmpeg -i gifs/$i.mp4 -ss '00:00:00.0' -t 00:00:\"$d\" -r 30 -vf \"scale=(iw*sar)*min(1280/(iw*sar)\\,720/ih):ih*min(1280/(iw*sar)\\,720/ih), pad=1280:720:(1280-iw*min(1280/iw\\,720/ih))/2:(720-ih*min(1280/iw\\,720/ih))/2\" -y gifs/$i-trimmed.mp4 Once all the files are conformed to the same format, they can be easily combined using a concat filter: ffmpeg -f concat -i concat-list.txt -i in.mp3 -c:v libx264 -c:a aac -movflags faststart -y output.mp4 This approach isn’t perfect because rounding errors due to differences between the requested length and the nearest frame will accumulate, but I found it worked extremely well for “It Doesn’t feel Like December” which is less than 4 minutes long, and had less than 300 edits in total. Watching and Tweaking From here, the video is finished and can be played in any video player that can play MP4s. I found it useful go back to the first step a few times and tweak the search terms until things looked pretty good, and then go back to looking at individual edit decisions and remove and occasionally move around ones to perfection. As a final tweak, I did a few manual searches on Giphy.com and found the perfect GIFs and pasted them in. Sometimes there’s just no substitute for that perfect find. Of course, for credits and a few custom GIFs of Cassandra, we used GIPHY’s Web GIF Maker . Future Directions It would be interesting to see if tweaking the search words could be automated using Natural Language Processing (NLP) to improve the semantic context of the lyrics and automatically determine thematic words. However, I would love to first expand these scripts with a simple User Interface so anyone could use it. — Bjorn Roche, Sr. Media Pipeline Engineer Previous Post Next Post", "date": "2017-12-01"},
{"website": "Giphy", "title": "GIPHY @ FullStack Engineering Meetup", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-fullstack-engineering-meetup/", "abstract": "GIPHY @ FullStack Engineering Meetup November 9, 2017 by GIPHY November 15, 2017 FullStack Engineering Meetup – New York, NY “Stacksgiving”, hosted by FullStack Engineering Meetup, will be a night focused on working with time series data. Our very own GIPHY Search Engineer Fiona Candon will be giving a talk titled ‘Time Series on a Time Crunch’. Using GIPHY’s user analytics launch as a case study, Fiona will cover some best principles for engineering low-risk time series indexes in Elasticsearch for uncertain load, and detail how GIPHY planned for foolproof backfills to adapt to changing requirements. She’ll also share some learnings from our effective short-term cross-team collaboration. For more info visit: https://www.meetup.com/Full-Stack-Engineering-Meetup/events/244719717 Previous Post Next Post", "date": "2017-11-09"},
{"website": "Giphy", "title": "Tech@NYU + GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/technyu-giphy/", "abstract": "Tech@NYU + GIPHY November 9, 2017 by GIPHY November 14, 2017 GIPHY HQ – New York, NY GIPHY is happily hosting Tech@NYU for their event on ‘Non-Traditional: Unique Paths Into Tech’. During this event attendees will hear from speakers who stumbled into the tech industry and provide insight around how they can do the same. If you’re changing your mind about your major but worried it’s a little too late or have always been interested in technology but never given it the time of day, Tech@NYU encourages you to join! RSVP at rsvp.techatnyu.org All Tech@NYU events are covered by a strict anti-harassment policy: http://techatnyu.org/anti-harassment Previous Post Next Post", "date": "2017-11-09"},
{"website": "Giphy", "title": "GIPHY <3 Analytics", "author": ["Niger Little-Poole"], "link": "https://engineering.giphy.com/giphy-loves-analytics/", "abstract": "GIPHY <3 Analytics October 10, 2017 by Niger Little-Poole Data Science and Analytics are an important part of what makes GIPHY the best place to find GIFs. We recently teamed up with Interana to help them spread the message of good data analytics practices during their “Summer Data Love Roadshow”. I answered questions from New York area Data Scientists, Product Managers, CEOs, etc along with experts from Comcast and Facebook. The following are some of the key insights we shared. Never Delete Your Data We encountered a question from a CEO wondering how his location based startup could manage their data given the volume was too expensive to store. He was considering just putting rollups in a data warehouse. If you find yourself in this situation, worst comes to worse sample your data. You never want to give up the power of the raw data. The rollups and views you create now, might be sufficient, but you never know when you’ll need to look at the data on a different set of dimensions. In some cases, like during an audit or for compliance, this could make or break your business. At GIPHY, every GIF, Sticker, API call, etc we’ve ever served has a corresponding log entry backed up to S3 in Parquet format. While we use aggregations to query things more quickly, we always have the raw data accessible if we need to build new views. Data is for Everybody Everyone today wants to be “data driven” but many organizations don’t know what that means and aren’t structurally ready for such a shift. For data to truly be for everyone, it needs to be presented in a way that complements existing workflows, like Calendar Analytics does, instead of operating orthogonally. At GIPHY we ensure that our tools either integrate with the Admin version of our platform, or have the capability to render relevant content directly so that our editorial team doesn’t have to add steps to connect insights to content. Rendering GIFs may seem like a small feature for any tool but it makes a substantial difference in the adoption of tools within our organization. We’re working on some cool projects at GIPHY that utilize computer vision, NLP, and machine learning. We have models that can detect if a GIF is viral, if it’s animated, if it has a caption (and the caption text), and even if it has certain celebrities. However the key to “Data is for Everyone” isn’t models. It’s really about learning lessons from UX and Human Factors to present information in the most effective way. We try to avoid spreadsheets and dashboards and focus more on automated or interactive tools that reduce cognitive burden. For example, we’ve built Slack bots that update relevant teams in concise, easy to understand English, avoiding jargon. Keep it Simple Stupid When organizations attempt to introduce more Data to their process, they sometimes go a bit overboard. Organizations will chase a “golden goose insight” that will magically improve their business. It doesn’t help that media reporting focuses on these kinds of data stories. For most organizations this insight doesn’t materialize and “data burnout” can occur. Its much better to focus on one metric at a time and slowly introduce more data. At GIPHY, our content team uses click through rate as their golden metric. The impact of new content partnerships, artist collaborations, and even work from our Studio are all judged on a high level by this one simple to understand metric. Having a simple high level metric like this, offers context and makes it easier to ask more detailed follow up questions like “how” or “why”. Automated Insight Lastly a big focus for us at GIPHY is the concept of “Automated Insight”. We believe it’s unrealistic for everyone to be remembering to check a dashboard every single day and also notice changes in that data. Tons of research in the field of Human Factors have demonstrated the limits of human memory and perception. People become numb to the data over time if it’s constantly the same. The key is to only present data to people when new information is available. For example, our content team is updated whenever we see a high volume of searches but a low volume of clicks. This indicates a lot of people are searching for something but not finding it. This automation allows us to react quickly to real time events. Conclusion In conclusion, we had a really great conversation on data. Interana was a great host for this event as their Platform really encapsulates the takeaways we discussed at the event. They are helping organizations to harness the power of the raw data, make it easily presentable to users, and provide an API to build further tooling. Stay tuned for more talk on data on the GIPHY engineering blog. – Niger Little-Poole, Data Scientist Previous Post Next Post", "date": "2017-10-10"},
{"website": "Giphy", "title": "Visioncamp: betaworks + GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/visioncamp-betaworks-giphy/", "abstract": "Visioncamp: betaworks + GIPHY October 4, 2017 by GIPHY January 2018 betaworks HQ – New York, NY Betaworks is rolling out visioncamp, a thematically-driven, frontier tech accelerator focused on the visual computing space, and GIPHY is thrilled to be a part of it. Visioncamp, beginning in January 2018,  is the third camp program out of betaworks and will focus on AR, computer vision, and ‘camera-first’ products/services. As co-hosts, and co-believers in the power of the camera (GIPHY Cam, GIPHY Says and our new AR app GIPHY World), we’re excited for participants to dig into the world of Augmented Reality and build products that extend beyond the basics behind the camera. If this excites you, make the opportunity yours and visit betaworks.com/visioncamp to learn more. Online applications are open through October 10, 2017. Previous Post Next Post", "date": "2017-10-04"},
{"website": "Giphy", "title": "New Stickers API Available", "author": ["Nick Santaniello"], "link": "https://engineering.giphy.com/new-stickers-api-available/", "abstract": "New Stickers API Available September 28, 2017 by Nick Santaniello Okay hotshot, you may know what a GIF is, but how about a Sticker? Give up? A Sticker is an animated GIF with a transparent background, making them the perfect choice for use in presentations, to be dropped into your favorite messaging app via the official GIPHY mobile app , or even to spruce up your favorite website with GIPHY Sticker Embed ! Today, GIPHY is encouraging you to say it with Stickers by focusing on new Sticker-related features and content across our web site and mobile apps. As always, GIPHY has you covered for your Sticker-related needs with a massive collection of Stickers available in all the same versatile sizes and renditions as the content from our GIF library. To make finding that perfect Sticker even easier, you can now even toggle searches on giphy.com for GIFs to show only Stickers instead. GIPHY Engineering is proud to announce that developers can also get in on the action by leveraging our recently upgraded Stickers API ! Read more about our new Sticker Pack endpoints , perfect for use in developing messaging apps, image-editing apps, or anywhere else curated collections of awesome Sticker content fit the bill. Be sure to let us know when your latest project featuring GIPHY Stickers is ready to be unleashed — we’re looking forward to seeing what you create! – Nick Santaniello, Special Projects Engineer Previous Post Next Post", "date": "2017-09-28"},
{"website": "Giphy", "title": "The GIPHY SDK is now Open Source", "author": ["GIPHY"], "link": "https://engineering.giphy.com/announcing-the-open-source-giphy-sdk/", "abstract": "The GIPHY SDK is now Open Source September 27, 2017 by GIPHY This week at GIPHY we are excited to announce the open-sourcing of the GIPHY SDK. This change takes the same code that powers some of our own products and creates an even simpler process for integrating the GIPHY API. Available for iOS, Android and various other programming languages, the code can be found on Github under the Mozilla Public License, v. 2.0. Where applicable, binaries are published to public repositories. iOS https://github.com/Giphy/giphy-ios-sdk-core Android https://github.com/Giphy/giphy-android-sdk-core Javascript https://github.com/Giphy/giphy-js-sdk-core Python https://github.com/Giphy/giphy-python-client Ruby https://github.com/Giphy/giphy-ruby-client PHP https://github.com/Giphy/giphy-php-client The SDK supports a variety of features available in our API , to name a few: GIPHY Gif Search GIPHY Translate GIPHY Sticker Search GIPHY Sticker Packs No need to re-invent the wheel: If you use GIPHY in your application, the SDK provides you with a library that will have you programming your application logic faster. If you think we have missed a feature, or have a idea for something new, provide a PR and after review perhaps we will merge your work into the SDK! We’re excited to welcome your pull requests and can’t wait to see what you build. Happy coding! – Bogdan Tirca, Cem Kozinoglu, Cosmo Cochrane, David Hargat, Gene Goykhman, Jillian Fisher and Zack Kantor Previous Post Next Post", "date": "2017-09-27"},
{"website": "Giphy", "title": "GIPHY + NYCML ’17 Workshop", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-nycml17-workshop/", "abstract": "GIPHY + NYCML ’17 Workshop September 29, 2017 by GIPHY September 29, 2017 GIPHY HQ – New York, NY NYC Media Lab is dedicated to connecting digital media and technology companies with New York City’s universities to drive innovation, entrepreneurship and talent development. This year they are hosting NYCML’17, a summit bringing the best thinking, projects and talent in digital media from universities in NYC and beyond and GIPHY is excited to be a part of it. GIPHY will be hosting a workshop at our headquarters on SEARCH: PAST, PRESENT, FUTURE. During this workshop we’ll take you through the history of search algorithms, from pre-digital days to web 2.0. Previous Post Next Post", "date": "2017-09-29"},
{"website": "Giphy", "title": "GIPHY Search Gets Glasses", "author": ["Bethany Davis"], "link": "https://engineering.giphy.com/giphy-search-gets-glasses-with-google/", "abstract": "GIPHY Search Gets Glasses September 26, 2017 by Bethany Davis Hey! My name is Bethany and I was a software engineering intern on the Search team this summer. I was also a 2017 hackNY fellow. I study computer science at the University of Pennsylvania. In this post, I discuss my internship project: leveraging optical character recognition (OCR) to help you find the perfect GIF. At the beginning of the summer, my friend was starting her first full-time job. I wanted to ~GIF~ her a pep talk before her first day, and I had the perfect movie reference in mind: Becca from Bridesmaids saying, “You are more beautiful than Cinderella! You smell like pine needles and have a face like sunshine!” The GIF I was envisioning: via GIPHY I searched GIPHY for “ you are more beautiful than Cinderella ” to no avail, then searched for “ bridesmaids ” and scrolled through several dozen results before giving up. Searching for Bridesmaids or the direct quote did not yield any useful results: It was easy to search for GIF with popular tags, but it was unrealistic to expect that someone would have tagged this GIF with the full line from the movie. And yet, I knew this GIF was out there. I wished there was a way to find the exact GIF that was pulled from the line in that movie / scene from that TV show / lyric from that song. Luckily, I was about to start my internship at GIPHY, with the opportunity to tackle this problem head on! GIF Me the Tools and I’ll Finish the Job When I started my internship, GIPHY engineers had already generated metadata about our collection of GIFs using Google Cloud Vision, an out-of-the box image recognition tool that is powered by machine learning. Specifically, Cloud Vision had performed optical character recognition (OCR) on our GIF corpus to detect text or captions within the image. The OCR results we got back from Google Cloud Vision were so good that my team felt confident about incorporating the data directly into our search engine. I was tasked with parsing the data and indexing each GIF, then updating our search query to leverage the new, bolstered metadata. The Search team uses several tools that helped me along the way: Luigi : I used this Python framework to write a batch job that processed the JSON data generated from Google Cloud Vision AWS Simple Queue Service : I used this message queueing service to coordinate data transfer from Google Cloud Vision to documents in our search index Elasticsearch : GIPHY search is built on top of this open source search engine. GIF documents are stored here, and the search query returns results based on the data in our Elasticsearch index. Bringing all these components together looks something like this: Technical GIFficulties The biggest technical challenge I faced during my internship was writing code that would scale. Most of my projects in school ran on such small datasets that my failure to optimize runtime was never an impediment. At GIPHY, that didn’t fly. I started testing my first worker, schedulable PHP code that prepares GIF updates for Elasticsearch, and realized that it would take 80+ hours (that’s 4 days) to process data for millions of GIFs. Clearly, scale and speed weren’t things I could ignore. via GIPHY I spent a week and a half focusing on ways to speed up my Luigi task and my PHP worker. First, I investigated expensive operations involved in each process and how I might make fewer calls to them. For example, instead of making a new network connection for each individual piece of data, I re-wrote the method so that it would hit each service in batches. Additionally, I sought out ways to run the work in parallel. For example, I refactored my first worker to operate over specific date parameters, so that I could distribute the work across multiple workers, each focusing on one week of data. Ultimately, this sped up the processing time from 80 hours to only 8 hours, and in the process I acquired some new strategies and perspectives on writing scalable code. The Home “Stretch”: Querying Elasticsearch Finally, all the data was indexed and it was time to incorporate text/caption metadata into our query. To perform a text-based search against Elasticsearch, I would be utilizing some form of a match query. There were a few variables to consider: first, I had to decide whether to compare the caption data against the search input using a match query with an “and” operator or using a match phrase query. The “and” match makes sense because it searches for captions that contain all of the words in the search input, which is what I wanted when I was searching for a line from a movie. However, I ultimately chose to use a match phrase query, which goes a step further: the words in the caption must appear in the same order as the words in the search input. This would guarantee that a substring of my movie quote is intact in the results that Elasticsearch returns. Another variable to consider was how much to trust data from Google Cloud Vision relative to other sources of data we have about a GIF. For example, how meaningful is a GIF’s caption compared to its tags, the frequency with which users click on it, or its description on the page it came from? In most cases, the relevance of a caption inside an image will be greater than or equal to the relevance of the image’s description from its source, so I decided to incorporate the data from Cloud Vision directly above this level in the relevance model hierarchy. Internal Testing: See the GIFference? Updated query in hand, it was time to find out whether my change actually made any difference. I used GIPHY’s suite of internal tools to visualize how a change to the search query would affect search results. The first tool, Search UX, demonstrates the impact on the scale of a single search. It shows the Elasticsearch score for each GIF in the result set, and it can be expanded to view the raw JSON response that explains why the GIF was included in the results. Search UX shows me a very dramatic before-and-after when I search for “where are the turtles,” a quote from The Office: The second tool, Side-by-Side, examines the query change on a larger scale by running the old and new queries against a random set of search terms and aggregating the affected metrics in an interactive dashboard. It’s useful to investigate the results and ensure that the change will not disrupt popular searches, like “cat” or “happy birthday,” that already deliver high-quality content. Zooming in on any of the queries from the side-by-side test yields more specific insights, like the set of GIFs that would be added and removed by the proposed update. Here’s what that looks like when we search for “spirit fingers”: So far so good! Time to GIF It a Go The internal tools indicate a positive change, but it’s time to let our users decide. I launched the updated query as an A/B experiment, and the results look promising: across all search traffic for the duration of the experiment, the lift in click-through rate was 0.5%. However, my change affects a very specific type of search, especially longer phrases, so the impact of the change is even more noticeable for queries in this category. For example, click-through rate when searching for the phrase “ never give up never surrender ” increased 32%, and click-through rate for the phrase “ gotta be quicker than that ” increased 31%. In addition to famous quotes from movies and TV shows, we saw improvements for general phrases like “ everything will be ok ” and “there you go”. The final click-through rate for these queries is almost 100%. To put my project to the ultimate test, however, I went to giphy.com to revisit my search query from the beginning of the summer: Success! The search results are much improved. Now, the next time you use GIPHY to search for a specific scene or a direct quote, the results will show you exactly what you were looking for. Learn more about my project? Click here . Original “GIPHY Search Gets Glasses” GIF by Leroy Patterson, GIPHY Studios Artist – Bethany Davis, Engineering Intern Previous Post Next Post", "date": "2017-09-26"},
{"website": "Giphy", "title": "Beyond Content: Extracting Image Property Data from GIFs", "author": ["Ruben Stern"], "link": "https://engineering.giphy.com/extracting-image-property-data/", "abstract": "Beyond Content: Extracting Image Property Data from GIFs October 24, 2017 by Ruben Stern For my summer internship on the GIPHY engineering team, I was tasked with extracting image property data from GIFs. Image property data is metadata about the GIF files themselves, particularly attributes that affect human perception and image “quality”, as opposed to content-related metadata. GIPHY can then use this data to do things like build predictive models that can help categorize and label GIFs, find correlations with other data points like popularity, optimize file generation and generally learn more about the overall makeup of their massive GIF catalog, Here is a list of some of the properties we settled on : Transparency : GIF pixels are encoded with the RGBA color space format where RGBA stands for Red, Green, Blue, and Alpha. The color is encoded in the first 3 values (with integers between 0 and 255) and the last value corresponds to a transparency element and is either 0 (transparent) or 255 (not-transparent). We use transparency percentage to distinguish between Stickers and regular GIFs, and though we already compute overall transparency, we supplemented this existing data by also computing the ratio of pixels that are transparent for every frame and then computing the mean, the standard deviation and the skewness of this ratio. Brightness :  Brightness (aka Value as in HSV) measures the degree to which something appears to be radiating or reflecting light. We calculate the brightness of each pixel of each frame by converting RGB to HSV, and these values are then averaged in order to get one single value for each frame. We compute the mean, standard deviation and skewness of the brightness over all a GIF’s frames. A GIF with high brightness: A GIF with low brightness: Luminosity : Luminosity (aka Lightness) measures how we perceive a color’s brightness, or the amount of white in the color, and is also known as tone. We calculate the lightness of each pixel of each frame by converting RGB to HSL, and these values are then averaged in order to get one single value for each frame. We compute the mean, standard deviation and skewness of the lightness over all a GIF’s frames. Both of these GIFs have “bright” color tones and thus high luminosity: Contrast : Contrast is the difference in luminance or color that makes an object distinguishable. It can be computed by taking the square root of the distance between an individual frame’s lightness and the GIF’s overall lightness.  We compute the mean, standard deviation and skewness of the contrast over all a GIF’s frames. A high contrast gif, frame colors and brightness change dramatically for the duration of the gif: A very low contrast gif, it’s hetergenous in color and brightness for the duration of the gif: Sharpness : Perceived sharpness is a measure of image resolution and “acutance”, and describes how we perceive the contrasts in edges and boundaries between items in an image. Sharp images have clearly delineated details, while in blurry image it’s difficult to distinguish objects and boundaries. To get sharpness, we compute the average gradient magnitude for each frame. We then compute the mean, standard deviation and skewness over all frames. We also compute an histogram of the values. This GIF has high sharpness due to high resolution textures and patterns: This GIF has low sharpness due to lack of color variation and the blurriness between objects: Entropy : Image entropy describes the amount of information in an image; it’s a measurement of the number of “states” contained within the image, or how “busy” it is. We compute the entropy of each frame of a GIF by calculating the probability of difference between adjacent pixels. A high probability means high entropy, and low probability means low entropy. We then compute the mean, standard deviation and skewness over all frames. This GIF from the Simpsons intro has high entropy due to the wide variety of characters and objects it: This GIF has low entropy because the minimal amount of movement and color results in a small difference in pixel values: Noise : Noise is unintentional distortion, like random fluctuations in brightness and color, that reduces the visibility or quality of an image. We compute noise in two ways. First apply a median filter on a frame, and then compute the MSE distance between the original frame and the filtered one. Second, we use total variation denoising to create a denoised version of the original and compute SSIM and PSNR distance between the frames. For both, we compute the mean, standard deviation and skewness over all frames. Example of a noisy GIF: For this GIF, the average SSIM difference across frames of the original noisy frames (example frame left) and the total variation denoised frames (example right)  is .33/1.00, indicating the original is a very noisy gif. Motion : This property represents the overall motion energy in a GIF, and is calculated from a GIF’s Motion History Image (MHI). The MHI is a grayscale transformed image that uses black, white and shades of gray to capture motion history. In the example below, the rightmost image is the MHI of a GIF. The black area corresponds to the part where there is no movement, the white area to the recent movement and the gray one to the old movement. We compute the histogram over the different grayscale colors of the MHI in order to get the area of each of these colors in the MHI. The motion histogram created from the above GIF: Color Histogram : We wanted to find the 10 dominant colors in a GIF and the percentage of pixels for these displaying colors. After experimenting with K-means clustering, we found a faster method with equal if not better results. The idea behind it is to get a binary tree of colors that are in the image and to do the separation between two “close” colors based on the two colors that have the biggest eigenvalues in it covariance matrix. Furthermore, as each frame could use a different palette of colors, and as I wanted to get 10 colors for the whole GIF, I had to first concatenate all the frames inside one image and then apply this method. The results of this method are very good as it enables to get very different colors and their pixel proportion with a decent running time. Loopiness : A metric specifically for GIFs, we wanted some way to measure a GIF’s “loopiness”, that is how perfectly the ending of GIF syncs up with it’s beginning. After a number of experiments, we ended up doing distance comparisons between a percentage of frames in the beginning and ending of the GIF, eg the 1st frame and the last frame, the 2nd frame and the penultimate frame, and so on.  We  also added a “continuous loopiness” algorithm that performs the same operation but against all adjacent frames. For these algorithms, we ended up using 4 distance metrics: MSE (Mean Square Error), NRMSE (Normalized Root Mean Square Error), PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity), and we estimate “loopiness” based on the mean of those values per frame. As documented, the SSIM comparison seem to be the most indicative of similarity. GIFs with high “loopiness” values: GIPHY is now in the process of computing these properties and other metrics against tens of millions of GIFs. A number of investigations and experiments are planned, so stay tuned to the GIPHY Engineering blog for updates on their progress. – Ruben Stern, Engineering Intern Previous Post Next Post", "date": "2017-10-24"},
{"website": "Giphy", "title": "GIPHY @ PennApps", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-pennapps/", "abstract": "GIPHY @ PennApps September 7, 2017 by GIPHY September 8 – 10, 2017 University of Pennsylvania – Philadelphia, PA Founded in the fall of 2009, PennApps was the nation’s first student-run college hackathon and GIPHY is a proud sponsor this year! Over a thousand students from the U.S. and other countries like Switzerland, Canada, England, and Singapore will be in Philadelphia for a weekend of creation and discovery. GIPHY is excited to be part of the fun. Previous Post Next Post", "date": "2017-09-07"},
{"website": "Giphy", "title": "GIPHY @ MHACKS X", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-mhacks-x/", "abstract": "GIPHY @ MHACKS X September 9, 2017 by GIPHY September 22 – 24, 2017 University of Michigan – Ann Arbor, MI GIPHY is a proud sponsor of University of Michigan’s 2017 MHACKS X. A 36 hour hackathon where teams work together to build, code or design projects, aka hacks. GIPHY representatives will participate in the fun and help pick winners for the best use of the GIPHY API – exciting stuff! Previous Post Next Post", "date": "2017-09-09"},
{"website": "Giphy", "title": "GIFS: Why do They Look Like That?", "author": ["Bjorn Roche"], "link": "https://engineering.giphy.com/gifs-why-do-they-look-like-that/", "abstract": "GIFS: Why do They Look Like That? August 30, 2017 by Bjorn Roche Here at GIPHY we work with GIFs every day. Sometimes, however, it pays to stop and ask “What exactly is a GIF?”. Since the format is over thirty years old, it has some real limitations, so it is worth asking: “What are they? What does it do well? And what can’t it do? The GIF format has a distinctive look, one that we at GIPHY are familiar with. Watch this talk to learn more about GIFs and why they look the way they do… Previous Post Next Post", "date": "2017-08-30"},
{"website": "Giphy", "title": "GIPHY @ BTNY BK", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-btny-bk/", "abstract": "GIPHY @ BTNY BK September 8, 2017 by GIPHY July 26, 2017 Bishop Loughlin Memorial High School – New York, NY GIPHY partnered with BREAKTHROUGH NY and visited a local high school to talk to them about opportunities in tech, share some info on what GIPHY does and ultimately, get them excited about programming and tech. By participating, GIPHY helps BREAKTHROUGH NY, a non-profit organization, achieve their mission to transform the lives of motivated, low-income students by prepping them for college graduation and succeed in the world. Previous Post Next Post", "date": "2017-09-08"},
{"website": "Giphy", "title": "HackUC 24-hour Hackathon", "author": ["GIPHY"], "link": "https://engineering.giphy.com/hackuc-24-hour-hackathon/", "abstract": "HackUC 24-hour Hackathon June 5, 2017 by GIPHY June 22 – 23, 2017 UC Tech – Scotch Plains, NJ GIPHY is a proud sponsor of HackUC’s 24 hour high school hackathon hosted by the Union County Vocational-Technical Schools, a district composed of five schools spanning a variety of subjects from engineering and computer science to health and performing arts. The first HackUC hackathon was twelve hours and held on June 22nd, 2015. It all started with a huge plan to encourage interest in technology in high schoolers and was a huge success thanks to both the mentors and the students. You can check out some of the projects at hackuc.devpost.com . To learn more, visit: http://hackuc.com/ Previous Post Next Post", "date": "2017-06-05"},
{"website": "Giphy", "title": "Introducing GIPHY Developers", "author": ["Nick Santaniello"], "link": "https://engineering.giphy.com/introducing-giphy-developers/", "abstract": "Introducing GIPHY Developers October 4, 2017 by Nick Santaniello We’re excited to announce the launch of GIPHY Developers : a one-stop shop and interactive resource for developers interested in getting started with GIPHY’s API offerings! By visiting our new portal, aspiring GIPHY developers can read top-level guides as well as detailed technical documentation regarding the GIPHY API’s many endpoints. It’s easier than ever to get started; developers can even play around with the API via an interactive API Explorer without writing a single line of code! For those of you who don’t know, the GIPHY API is the ultimate way to integrate GIPHY’s search engine and vast GIF library into your project. The GIPHY API helps you and your users find relevant GIFs based on search terms, tags, ratings, our unique translate algorithm, or just randomly. The GIPHY API also delivers GIFs in a multitude of renditions and file types sorted by max dimensions, filesize, framecount, and several other parameters to fit your needs. Whether you need a static thumbnail or a high-res .mp4, the GIPHY API has you covered. In the future you’ll be able to upload and create GIFs programmatically, too! What’s new? GIPHY has been offering a robust public-facing API for some time; however, we’ve never had a place devoted entirely to supporting developers with detailed technical documentation and API Key management tools. In the past we’ve offered a single public beta key to developers looking to experiment with GIPHY’s various APIs, but with the launch of GIPHY Developers, developers can now obtain unique API Keys for each project/app assigned directly to their GIPHY account. This will help us provide better individual support to our developers. What’s more, developers can also use these API Keys with our new interactive API Explorer to experiment with different API calls and view the live responses they’ll receive. These individual per-project keys also streamline the process of applying for a production-level key without rate limits or restrictions, something developers can also do directly from their GIPHY Developer Dashboard . What are you waiting for? Check it out at developers.giphy.com ! If you have any feedback or questions about GIPHY Developers or the GIPHY API in general, drop us a line –we’d love to hear from you. – Nick Santaniello, Special Projects Engineer Previous Post Next Post", "date": "2017-10-04"},
{"website": "Giphy", "title": "GIPHY Presents: TIME FRAME – Celebrating 30 Years of the GIF", "author": ["GIPHY"], "link": "https://engineering.giphy.com/giphy-presents-time-frame-celebrating-30-years-of-the-gif/", "abstract": "GIPHY Presents: TIME FRAME – Celebrating 30 Years of the GIF June 7, 2017 by GIPHY June 17 – 22, 2017 Gallery 151 @ 245 W 14th St – New York, NY Happy 30th Birthday to the GIF! GIPHY is marking the occasion with TIME_FRAME: A week-long gallery exhibition exploring the history of art online, and demonstrating the power of the GIF as a format that will guide us into the future of visual culture. Featuring works by over 30 artists, the exhibition includes interactive installations, VR experiments, artist talks, and workshops. RSVP to get the latest updates on all free/open to the public events, as we’ll be adding more soon! Admission is on a first-come, first-served basis. https://timeframe.splashthat.com/ Previous Post Next Post", "date": "2017-06-07"},
{"website": "Giphy", "title": "Building Pex with Pants in Production", "author": ["Fiona Condon"], "link": "https://engineering.giphy.com/building-pex-with-pants-in-production/", "abstract": "Building Pex with Pants in Production August 9, 2017 by Fiona Condon GIPHY’s search analytics team has one of the coolest jobs in the world—getting data about the way the world uses GIFs and using that data to make it easier to find the best cat fail . To make this happen, we write ETLs in Scala with Spark and manage them with a Python-based task management tool called Luigi that helps us schedule our code to run across multiple workers. Robustly deploying Python code is not one of the coolest jobs in the world. Our initial solution involved managing remote repositories and virtualenvs on our workers, and doing a git pull and rebuild per-host when we wanted to update our code. This system was especially painful given that the slowest step was compiling our Scala code, which could easily be built into portable JARs locally if we didn’t have the Python component to worry about. Around the time it became a real pain point for us, we migrated to Pants as a unified build tool for our Scala & Python code. Pants supports building PEX targets—”Python EXecutables,” which self-contain their own dependencies and can be shipped to remote hosts as easily as JARs. In this post, I’ll explain the broad strokes of how PEX works and share some of our learnings from using it in production with Pants. How? Pants makes it possible to build a PEX for a “hello world”-style application with a few lines of boilerplate. Imagine this is my application: > cat hello/hello.py print \"hello this is dog\" The directory containing my code might look like this: > ls hello/ BUILD    hello.py That BUILD file tells Pants how to build the code contained in this directory. In this case, I want it to express that I’d like to build a PEX target containing the functionality of hello.py. > cat hello/BUILD python_binary( name='hello', source='hello.py', ) Once I build that target, I have an executable .pex file that I can invoke directly with python. > pants binary hello/ ... > python ./dist/hello.pex hello this is dog But how ? Behind the scenes, that ./dist/hello.pex file is just a directory containing my code. In fact, it’s a zipped directory with a #!/usr/bin/env python that enables it to be invoked like you see above. All of this is actually standard Python functionality since version 2.6—the PEX format simply exploits the convenience of those features and adds a __main__.py file that handles loading in packaged dependencies and setting up the environment for portable execution. If I went ahead and unzipped ./dist/hello.pex , I’d see something like this: > ls unzipped/ PEX-INFO     __main__.py  __main__.pyc hello.py     hello.pyc Brian Wickman gave an excellent talk with more examples if you’re interested in learning more about the format. In the rest of my post, I’ll explain some of the things we’ve learned about using PEX at GIPHY. Debugging Since PEX files are just zipped directories containing my application (and its dependencies), they’re relatively easy to introspect on and debug. To return to my previous example, the unzipped version of my PEX file contains my unaltered application code. > cat unzipped/hello.py print \"hello this is dog\" This is handy for debugging the state of the deployed code in a pinch, but has also proved useful for investigating subtler issues. During our migration, we ran into an issue with a Python package with an odd structure that didn’t play nicely with the PEX format, and were able to figure out what was going on by unzipping the executable and introspecting on the dependency structure. Building for Portability PEXes store all of their dependencies as wheels in a hidden directory. If I were to add a couple of Python dependencies to my project, I could see their corresponding wheel files in my unzipped PEX directory like this: > ls unzipped/.deps/ platform-agnostic-dep-none-any.whl platform-specific-dep-macosx_10_11_intel.whl platform-specific-dep-none-linux_x86_64.whl This approach enables the PEX to run on both my local environment and our Linux workers, but requires that wheels for every relevant platform be available at build time. It wasn’t immediately obvious to us what the best way to manage these files was—we didn’t want to explode the size of our git repository with large wheels and weren’t eager to stand up our own web-based solution. In the end, we opted to track our wheels with git lfs and register their containing directory with Pants, which has been a pretty lightweight solution. Deploying The payoff for introducing this format has indeed been speedier, less fussy deploys! We use Fabric to scp our Scala and Python executables to each worker, storing them under a directory named for the commit hash of the deployed version and updating a symlink to track the latest deploy. > ls -l /mnt/deploys/ 48b10732b5d022e13e9a5ec85e0c00f4ac2f93f1 78ca5ad8b6585715a4abdfbf51b83ac25ae8c177 c6299f8df863d80130e015923d22c6d9130e39dc latest -> 78ca5ad8b6585715a4abdfbf51b83ac25ae8c177 We keep the last several deployed directories available on the remote workers, which enables us to roll back to a previously-deployed version of our code with the touch of a symlink. Drawbacks The major challenge we ran into was some poorly-defined Pants build caching behavior around our platform-specific Python dependencies. We sometimes had to manually invalidate the Pants cache to build our Python executables by rm ing the relevant cache directories. Conclusion There are many other ways we could have chosen to improve on our initial system, including containerization or even debian packages , but building and deploying with the PEX format has been a straightforward solution that’s served us well. It’s vastly simplified our deploys while minimizing the number of new technologies in our stack and keeping our build process very consistent—in fact, we can compile our Fabric-based deploy scripts into PEXes as well! – Fiona Condon, Search Engineer Previous Post Next Post", "date": "2017-08-09"},
{"website": "Giphy", "title": "GIF Paint: Drawing GIFs on Canvas", "author": ["Jonny McLaughlin"], "link": "https://engineering.giphy.com/drawing-on-canvas/", "abstract": "GIF Paint: Drawing GIFs on Canvas August 2, 2017 by Jonny McLaughlin What was once intended to simply be a fun internal hack day project for GIPHY Maker’s Fair eventually turned into an actualized tool, better known as GIF Paint. We had a hack day here where we (GIPHY employees) were split into pods and spent time building GIF related tools which were then presented to GIPHY at large. Think an old school science fair turned GIF fair. My GIF related tool was GIF Paint – a mobile website where you upload a photo and use your finger to paint on it with GIF’s, choosing your GIFs from the select toolbar provided. This presentation, brought to you by Bob Ross, digs into the technical limitations it took to build the GIF Paint tool and the interesting problems encountered along the way. What’s more is that I was able to not only leverage some existing tools but improve and build upon them throughout the process, continuously enhancing the tool overall. – Jonny McLaughlin, Director of Engineering Previous Post Next Post", "date": "2017-08-02"},
{"website": "Giphy", "title": "#BBG @ GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/bbg-giphy-hq/", "abstract": "#BBG @ GIPHY May 23, 2017 by GIPHY May 11, 2017 GIPHY HQ – New York, NY GIPHY hosted a some of the girls who are involved in the #builtbygirls after school program to come to the office and learn more about GIPHY. The girls also participated in a small focus group with our mobile team. Our Director of Mobile Products did an overview of the GIPHY Mobile App and then had girls participate in a brainstorm session and develop fun features that they think added value to the app – it was a great time! Previous Post Next Post", "date": "2017-05-23"},
{"website": "Giphy", "title": "All Star Code @ GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/all-star-code-giphy-hq/", "abstract": "All Star Code @ GIPHY May 30, 2017 by GIPHY July 12, 2017 GIPHY HQ – New York, NY GIPHY is hosting an All Star Code site visit for their 2017 summer initiative. All Star Code is an organization dedicated to empowering young men with the skills, networks, and mindset they need to create new futures through technology – focusing primarily on Black and Latino high school males. GIPHY will be opening their office doors to All Star Code and inviting  them to learn more about GIPHY, interact with the team and gain insight first-hand into the tech industry. Previous Post Next Post", "date": "2017-05-30"},
{"website": "Giphy", "title": "Out In Tech Spring Social", "author": ["GIPHY"], "link": "https://engineering.giphy.com/out-in-tech-spring-social/", "abstract": "Out In Tech Spring Social May 23, 2017 by GIPHY May 11, 2017 Hotel Americano @ 518 W 27th St – New York, NY Out in Tech unites the LGBTQ+ tech community, they empower aspiring tech leaders to improve our world by showcasing accomplished speakers, producing timely and thought-provoking events, and connecting their members to new opportunities and each other. They provide resources and mentorship to ensure career access for the next generation of LGBTQ youth. GIPHY joined hundreds of NYC’s top LGBTQ + tech talent to catch up, grab a drink, network, and celebrate spring! We donated drink tickets on the night of the event and hosted our GIPHY Frame Gif booth. It was a fun night of socializing and mingling and it was a pleasure to be a part of the Out In Tech spring social at Hotel Americano. Previous Post Next Post", "date": "2017-05-23"},
{"website": "Giphy", "title": "GIPHY + Wharton NY Startup Trek", "author": ["GIPHY"], "link": "https://engineering.giphy.com/wharton-ny-startup-trek-giphy/", "abstract": "GIPHY + Wharton NY Startup Trek May 22, 2017 by GIPHY March 24, 2017 416 West 13th Street – New York, NY GIPHY hosted students from The Wharton School of UPenn interested in emerging tech startups at the GIPHY office. We gave them an office tour, followed by a Q&A with a few GIPHY employees. This included some folks from all departments including Ops, Partnerships, Finance, Business Development, & Editorial. Next Post", "date": "2017-05-22"},
{"website": "Giphy", "title": "Introducing GIPHY Engineering", "author": ["Randy Shepherd"], "link": "https://engineering.giphy.com/introducing-giphy-engineering/", "abstract": "Introducing GIPHY Engineering July 21, 2017 by Randy Shepherd Welcome to our new GIPHY Engineering blog! We’ll be using this platform to share stories from GIPHY’s technical teams – the folks who build, run and enhance all of our products, tools and infrastructure. First up, lets talk through what we do. We power GIFs everywhere. Whenever you see a GIF (whether on a social network, chat app or your favorite website/app), it is highly likely that it’s coming to you via GIPHY. In fact, most developers first become aware of GIPHY this way – most notably, through our Slack integration. Almost every interview candidate we’ve talked to sites this example, and at a recent conference when I asked the audience if this was the case, the majority raised their hands. I like to refer to this as “the tip of the GIPHY iceberg”. While this is a great example, we do so much more than just make your work day a bit more fun, and we’re excited to share these projects with you. Some of the GIPHY Engineering topics you’ll hear about are: – Engineering our API and website to operate at scale – Evolving our search engine to ensure relevance and allow for discovery of new content – Delivering performant (and fun!) user experiences in the browser – Leveraging machine and deep learning to better understand our media assets and search performance – Transcoding, processing and generating GIFs and other short form media at scale – Developing our suite of mobile applications, while pushing the boundaries of what mobile devices can do I’ll stop there, but there is so much more to showcase and we’ll be using the blog to do just that. Our posts will come directly from our Engineers, giving you first-hand insight into how we accomplish the above, and much more. In the process of doing so I hope you will get a better sense of who our team is, what GIPHY Engineering is all about, and why we are so excited to share the behind-the-scenes view into the work that we do. – Randy Shepherd, VP of Engineering Previous Post Next Post", "date": "2017-07-21"},
{"website": "Giphy", "title": "Facebooks’s F8 Dev Conference @ GIPHY", "author": ["GIPHY"], "link": "https://engineering.giphy.com/facebookss-f8-developer-conference-giphy-hq/", "abstract": "Facebooks’s F8 Dev Conference @ GIPHY May 26, 2017 by GIPHY May 17, 2017 GIPHY HQ – New York, NY GIPHY hosted the NYC live stream of Facebook’s annual 2-day event, where developers and businesses come together to explore the future of technology. This year’s event was bigger than ever, it included interactive demos, announcements, and best practices that will keep us all looking ahead. GIPHY welcomed local New Yorkers to tune in and discover new features and ways to make the world more open and connected. With the help of GIPHY, there was an F8 for everyone. Previous Post Next Post", "date": "2017-05-26"},
{"website": "Giphy", "title": "TechCrunch Disrupt Hackathon", "author": ["GIPHY"], "link": "https://engineering.giphy.com/tech-crunch-disrupt-hackathon/", "abstract": "TechCrunch Disrupt Hackathon May 25, 2017 by GIPHY May 13 – 14, 2017 Pier 36 @ 299 South Street – New York, NY GIPHY is a proud sponsor of the 2017 TechCrunch Disrupt Hackathon. Preceding the Disrupt Conference is Hackathon weekend on May 13-14, where developers and engineers descend from all over the world to take part in a 24-hour hacking endurance test. Teams join forces to build a new product, present it on the Disrupt stage to a panel of expert judges for a chance to win a variety of prizes. Previous Post Next Post", "date": "2017-05-25"},
{"website": "Giphy", "title": "GIPHY Capture: Lessons Learned From Flux", "author": ["Mike Nolan"], "link": "https://engineering.giphy.com/giphy-capture-flux/", "abstract": "GIPHY Capture: Lessons Learned From Flux September 20, 2017 by Mike Nolan Jumping into native app development from a web development background can be a monumental task for many. The web development community provides an incredible and seemingly endless supply of resources, free and open source tooling, packages and frameworks to help even intermediate developers build scalable and maintainable interfaces. Given the ubiquitous nature of the web we have ended up seeing lots of thought go into the tooling and creation of paradigms for maintaining increasingly complex user interfaces. Tools such as React and Redux along with design paradigms such as Flux have helped make managing complex UIs significantly more accessible. Most importantly active web development communities have created mounds of documentation and blog posts explaining concepts, benefits, and drawbacks to these paradigms. GIPHY being a predominantly being a web focused company sometimes has our full stack devs dip their toes into some native app development when the need arises. When our application logic becomes more and more complex, as with any application, refactoring and rearchitecting becomes a necessity. GIPHY Capture as with most video editing programs deals with a lot of state represented in a temporal manner. It must keep track of all of the edits the user has performed, find where the user is currently within the video preview and then render the correct preview and UI elements based upon all of that. As we began to add more tools to edit our increasingly large state, we began to realize that we were seeing an increasing amount of cascading effects. We would have different views call out to parents and sibling views to notify a change, and they would take that data, mutate it and hand it off to other views which needed to know of the information. It became increasingly difficult to understand what the state of the project was and bugs started cropping up. Coming from recently working on a large React/Redux project, I began to see the benefits of the one way data flow that architectures such a Flux push. Instead of fragmenting our state across many components we could share them in one big store. When the UI triggers changes to that state we could create actions and send them to dispatchers that will update our store, and therefore update the relevant components. This ended up becoming incredibly helpful because at any point in time you can export your entire project to a serialized edit decision list and inspect your state. As we create new features for editing your GIFs, it minimally increases the complexity of our code base. MVC allows cascading updates between components which can make maintaining state a nightmare Flux’s one way data flow architecture can often times simplify UI and prevent the cascading effects that you can possibly see crop up in large MVC applications. Perhaps most importantly, Flux isn’t constrained to React based javascript applications. It helps compose our UI into more hierarchical components, describe a semantic API of how we modify state, and perhaps most importantly for myself, give a good jumping off point to dive into the scary world of native app development. We can use a dispatcher and store to figure out what components need to update based on what information has changed Previous Post Next Post", "date": "2017-09-20"}
]