[
{"website": "500px", "title": "automated test optimization at 500px", "author": ["Michael Zou"], "link": "https://developers.500px.com/automated-test-optimization-at-500px-636a82b570a7", "abstract": "The primary automated test suite at 500px contains over 11000 automated examples written in Rspec with Capybara. These tests run on development branches, as well as each time a commit is merged into master. They take an average of 20 minutes to run across six threads on Semaphore . We have dedicated test engineers (and co-ops!) responsible for helping maintain this test infrastructure as part of the 500px quality process . Reducing the run time of the test suite can have a significant impact on developer productivity. Let’s deep dive into two fixes we implemented to keep our test suite running smoothly. Override the default max wait time for Capybara. Capybara’s “have_selector” method is used to detect whether a given select matches an element on the page. It takes an optional “wait time” parameter. When “wait time” is not specified, it uses the configured global maximum wait time, which for us was 90 seconds. Often, tests want to verify that an element doesn’t exist on the page, which looks like: expect(page).not_to have_selector(…). In these cases, it’s important to consider wait times. If you don’t specify a wait time, the negated matcher will wait the default wait time before checking, so that any code responsible for removing the element has time to complete. But if the code being tested isn’t time-sensitive, this can cause large, unnecessary delays in the completion of the test. To avoid this problem, specify the wait time: expect(page).not_to have_selector(“#element”, wait: 3). This makes Capybara wait 3 seconds before checking the page for the existence of that element, rather than the default 90. Instances of this issue can add up quickly. An audit for this issue found 6 instances of this pattern in our test suite, which was adding 9 minutes to the total run time. Don’t waste time waiting As mentioned before, we use Semaphore for our CI which runs our tests across 6 jobs. In order to optimize total run time, we need to put some thought into how we split up the tests. Previously, we balanced tests across threads based on the number of lines in the test file, but this wasn’t very effective, because the number of lines in a test file doesn’t necessarily reflect the test’s run time. Now we’re using a gem called Knapsack . This gem uses a JSON manifest that keeps track of every test’s run time and uses that information to balance which jobs get which tests, allowing them to all finish around the same time. This change reduced our test suite’s average run time from 35+ minutes to 20 minutes. However, this solution had a problem: over time, the JSON manifest that Knapsack depends on could get out of date, because we’re constantly adding and removing new tests. Ideally, the JSON manifest would be automatically updated. To automate this, we needed to automatically update the JSON manifest based on the result of each test run. Since these tests run in parallel across 6 threads, we could have had each thread just write their results to the manifest. However, this proved to be impractical as it caused a lot of threadlock. Threads would finish around the same time and wait to write to the JSON manifest one by one. Instead, we decided to have all 6 threads upload their results after each run, and read those files and combine them into the complete manifest at the start of each run. Our automation now load balances itself, and updates the run times of each spec in preparation for the next run. The tests are less flaky, and timeout failures are less costly. Adding specs is effortless, and test run times are minimized. Faster tests = happy devs! Welcome to the 500px Engineering Blog! 61 1 Thanks to Gavin Sharp . Testing Automation Continuous Integration Rspec Capybara 61 claps 61 1 Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-08-24"},
{"website": "500px", "title": "understanding rendering in react redux", "author": ["Michael Tighe"], "link": "https://developers.500px.com/understanding-rendering-in-react-redux-7044c6402a75", "abstract": "In the process of rethinking and modernizing our front-end stack here at 500px, I’ve spent the last month or so deep diving into React, Redux, and the dizzying array of tech surrounding them. As a relative newcomer to React and Redux (I know, I’m a bit late to the party), I found it a bit challenging to find a good resource clearly laying out the details of how, and more importantly, when , components are rendered. Understanding and controlling rendering is essential to writing good, bug free and performant React + Redux code. It is the core of the framework. Let’s get started. React renders a component whenever its props or state change. This is fairly easy to understand, but it’s actually a bit misleading; it is not the only cause of a render. Components are also rendered when their parent is rendered. In fact, the entire component tree below the updated component is blindly re-rendered, whether or not their own state or props changed at all. It’s important to keep in mind that in React, rendering does not necessarily mean updating the DOM. There are two stages in React rendering: Call the render() function to render to the virtual DOM Compare the real DOM with the virtual DOM, and update it accordingly Since re-rendering an unchanged component does not result in any real DOM updates, it’s not as costly as you might think, especially for simple components. For the most part, it’s not a problem and you can leave well enough alone. But what can you do when it is a problem? Sometimes a large number of unnecessary render calls can start to weigh down your application. React provides a hook to take control of rendering in the form of the shouldComponentUpdate lifecycle method. React checks this method to determine whether or not to render a component, and by default it simply returns true . If you implement shouldComponentUpdate yourself, however, you can use it to check whether or not props which affect the rendered component have changed and only render if necessary. Be careful, though. In simple components, this process can actually be more time consuming than just rendering, especially when you start doing deep object comparisons. It also makes understanding and maintaining the component more difficult, and leaves you more vulnerable to introducing subtle bugs in the future. As with any performance optimization, you should only be doing it if you need to, and not a moment earlier. In many cases, you may want to do a simple check to determine if any props have changed before deciding to render. Rather than writing a shouldComponentUpdate method to handle this for every component, you can extend React.PureComponent , which will take care of it for you. You can also use a helper function like pure() from Recompose , which has the same effect as extending PureComponent with the added benefit of working with functional components as well. Keep in mind, though, that both methods only perform a shallow comparison of your new and old props. That is, if you change an attribute of an existing object in your props or state, a pure component will not notice. It will see the same object referenced in both new and old props, and conclude that nothing has changed. With some care and good practices, though, this shouldn’t be an issue — but you need to be careful. When you connect your component to a Redux store using the connect function, which maps Redux state to component props , Redux will ensure that the props are updated and a render is triggered whenever the relevant Redux state has changed. It does this with a shallow compare, though, so Redux reducers must always return new state objects instead of simply mutating the existing state . Essentially, Redux is taking care of converting your component into a pure component, without the need of manually implementing shouldComponentUpdate yourself. Making sure you always return new state in your Redux reducers can be tricky at times. It can also be costly, involving expensive deep copying of state objects. As such, it can be helpful to use a library which enforces immutability, such as Immutable.js . This library provides immutable implementations of common collections, making updating Redux state generally both cleaner and less error prone. It also implements some fancy data structures behind the scenes which make updating state a lot more efficient than deep copying large objects or lists. It is not without its drawbacks, though, as you’ll now have to access properties through get() and set() accessor methods, and you’ll lose some nice syntax like destructuring . As immutable objects start to spread themselves through your code, it also makes it very easy to create pure components, since your props can be reliably compared with a simple shallow equality check. React rendering is smart, but not as smart as you think it is. If a component’s state changes and triggers a re-render, it will render all of its child components, all the way down the tree. Generally, this is fine; remember that premature optimization is the root of all evil (or thereabouts). Also remember that a render does not necessarily result in any change to the DOM, as the virtual DOM diffing will still take place. But should it become a problem, you can tame it by taking control of the decision to render with shouldComponentUpdate and PureComponent . Redux is a bit smarter and helps ensure that connected components are only rendered when the state they care about changes, but you need to be careful when implementing your reducers to ensure they always return new values and don’t simply mutate objects, as Redux won’t notice the update. Immutable.js can help you out with this, and provide some other benefits, but again, be careful to not over-engineer your solution and add complexity and dependencies which you might not ever need. Understanding how React and Redux decide to render components is key to writing and debugging your React projects. Knowing how to take more control over this process is equally important — just be sure to wield your new power responsibly! Welcome to the 500px Engineering Blog! 165 2 Thanks to Ian Van Den Heuvel and Zimu Liu . JavaScript React Immutablejs Redux Front End Development 165 claps 165 2 Written by Software Developer, photographer, musician, climber. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Software Developer, photographer, musician, climber. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-13"},
{"website": "500px", "title": "how do we maintain 500px", "author": ["Shreya Khasnis"], "link": "https://developers.500px.com/how-do-we-maintain-500px-1012a6fcfabd", "abstract": "500px users may notice new features on our site occasionally, but we put new code into production multiple times a day! How do we ensure that everything works after every change? Good question! I’m a member of the quality assurance (QA) team at 500px — we play a big part in this. Here are some of the development techniques we use to ensure a smooth sail. First of all, code changes are reviewed by other developers before they even reach the QA team. This often catches bugs and corner cases early in the process. Code review also helps reduce redundant code and encourages syntactic consistency, both of which keep our codebase clean. Pull requests are reviewed by developers, but also reviewed by machines! We have a large suite of automated tests, which run when new pull requests are opened. These tests are a great way to ensure that new features work as expected and verify that these new changes do not break existing functionality. Given the variety of features on our site, it would be time-consuming to test all aspects by hand on every code change. Currently, we have about 4,000 automated tests that are separated into threads which run simultaneously. We use a continuous integration framework called Semaphore CI that runs these tests on every proposed change. The tests are randomly executed, which encourages the development of independent tests to ensure the order of execution does not impact the expected result. This helps us parallelize the test suite into different threads. Semaphore can also be integrated with Slack to inform developers about tests that have passed or failed. From this, developers are able to triage through and fix the code that broke things. For a more in-depth overview of how we integrate and use Semaphore check out this link. From my perspective as a QA developer, this automated testing allows us to focus on capturing edge cases of new product requirements and spend less time reviewing previously-tested functionality. The automated test cases are reusable: they eliminate the need to manually retest all features. We also create new tests and update older ones to keep the test specifications up-to-date and make sure they accurately reflect the behaviour of 500px. But test automation does not cover everything! Each change is also manually tested to catch any remaining bugs. This includes positive testing (confirming that everything works when it should) and negative testing (confirming that errors are handled gracefully). Since 500px caters to many users, testing with a diversity of user accounts (e.g. both free users and paid members, or different language preferences) is crucial! For the QA team, testing with multiple user accounts helps us verify that features are accessible to the correct group of users and catch discrepancies between translations. Generally, QA has the final say about when changes are ready to be deployed. Our Slack bot, BMO, can deploy changes to all our different testing and production environments. BMO encourages a process known as conversation-driven development and promotes greater transparency between team members. It’s also very easy for new team members to deploy changes! All they do is type a command such as “bmo deploy 500px” into Slack and BMO begins the deployment. BMO also notifies the user of any deployment errors as well as successful deployments and includes a log to help any debugging. The QA team uses BMO’s ability to deploy to different environments to view code changes before they go live. Since we have access to multiple pre-production environments, BMO also tracks which ones are in use and notifies team members when someone is using an environment. This prevents people from inadvertently using the same environment and affecting their tests.To use an environment, we type “bmo lock environment” and “bmo release environment” in Slack. To know more about the mechanics of BMO and how it was implemented, check out the ChatOps segment earlier on in the 500px developer blog. Overall, that’s a quick summary of how we move fast and introduce new features without breaking things. If you have any questions, leave a comment below and check out our other blog posts for more details on how we work. And if this is interesting to you and have suggestions on how we can make our setup better: we’re hiring! Welcome to the 500px Engineering Blog! 32 Continuous Integration Software Development Test Automation 500px Chatops 32 claps 32 Written by Computer Science Student at University of Waterloo Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Computer Science Student at University of Waterloo Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-28"},
{"website": "500px", "title": "tips and tools from the 500px web team", "author": ["Javier Ruiz"], "link": "https://developers.500px.com/tips-and-tools-from-the-500px-web-team-f63ac9ae39cd", "abstract": "The 500px web team meets weekly for a broad-ranging discussion we call the “Web Tech Deep Dive”. Sometimes we explore a dark corner of our codebase; sometimes someone presents a new technology; and sometimes it’s a round-table discussion. We recently shared tools and tricks we use to be more productive, and thought it would be a great idea to share some of these with you! Let’s begin. Ag — The Silver Searcher Ag is a code searching tool for your command line, just like Ack or Grep, but better. It has the speed of Grep, and the ability to ignore files like Ack. It automatically excludes files that you’ve added to .gitignore, and groups matches by file. Price: Free. Who doesn’t like free stuff? Link : https://github.com/ggreer/the_silver_searcher Dash Dash (iOS, macOS) lets you have your own personal documentation library. It’s most commonly used to download documentation for languages and APIs to have quick access to them under a singular application. However, it also has online support allowing you to search Google and Stack Overflow for answers if you’re unable to find what they’re looking for in the docs. Some other features are snippets, annotations inside docs, and integration into most editors allowing you to quickly look up anything under your cursor. Dash is used by most members of our team. Note : Velocity is a good alternative for Windows users. Price : Free with the occasional prompt to purchase for $24.99. Link : https://kapeli.com/dash FZF — Fuzzy-Finder Command Line Tool FZF is a fuzzy finder for your terminal that’s written in Go. It lets you search for files inside the current directory or past commands in your history via blazing fast autocomplete. Bob’s most common uses for FZF are picking files to commit (`git add <filename>`) and edit ( `vim <filename>`). Ctrl+R will initialize FZF to look through your history and Ctrl+T will initialize FZF to look through all of the folders and files under the current directory. Price: Free. Link: https://github.com/junegunn/fzf Synced-Sidebar Plugin for Atom/Sublime Text Synced-sidebar was originally a plugin for Sublime Text but now works on Atom as well. These can be installed through the built-in package managers. It automatically syncs your sidebar to always show the currently opened file, helping you keep track of your position in your project & the context for your edits. It’s a small plugin, but it affects your workflow more than you’d expect! Price : Free Link : https://atom.io/packages/synced-sidebar , https://github.com/TheSpyder/SyncedSideBar Rubocop Rubocop helps you follow your Ruby style guide throughout your code base. One of the best features of Rubocop is that it allows you to customize the types of issues you want it to catch. Once configured, Rubocop detects syntax and styling issues in your current file and highlights the issues in yellow (for warnings) or red (for errors). This lets you correct style issues as you go, rather than having to resolve them all in a pull request.. Here’s an example of our Rubocop settings: https://gist.github.com/JavierR14/c5ac03df9ffd92206244ac3c697be376 Price: Free Link: https://github.com/bbatsov/rubocop , or download through your editor’s package manager by looking for ‘linter-rubocop’ Spectacle Spectacle is a general purpose tool that isn’t specific to developers, but it’s incredibly useful for Mac users. macOS doesn’t have any built-in shortcuts to resize windows or snap them to the borders of your screen. Spectacle makes it easy to define shortcuts that move, resize, and snap windows in position. Personally, I use the default shortcuts for snapping windows to different sides of the screen as I find them convenient. However, every shortcut is customizable. Price: Free. They have a donation link if you’d like to support them. Link: https://www.spectacleapp.com/ Gas Mask Several developers and QA team members use this macOS hosts manager to easily switch between /etc/hosts configurations without manual edits. Gas Mask lets you create as many hosts files as you want and easily switch between them by double clicking (or using the menu bar icon). It’s extremely useful when you have multiple environments such as development, staging, and production where each one needs its own set of IP mappings. Price: Free Link: https://github.com/2ndalpha/gasmask Conclusion All of the tips and tools mentioned above help make your coding life more pleasant and efficient, so we recommend adding a few to your arsenal! At 500px, we’re always looking to grow and expand our knowledge. What do you do personally to be more productive as a developer? Leave a comment below and let us know about your favourite tools or tips! Welcome to the 500px Engineering Blog! 18 1 Coding Web Development Software Development Developer Tools Developer 18 claps 18 1 Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-21"},
{"website": "500px", "title": "incremental dom with backbone marionette on rails", "author": ["Michael Tighe"], "link": "https://developers.500px.com/incremental-dom-with-backbone-marionette-on-rails-6f372d38c0f9", "abstract": "At 500px, the majority of our client-side code is written using Backbone and Marionette , with handlebars templates. Although it has served us well so far, we’re always on the lookout for ways to improve our stack, architecture and best practices. As a rule, we avoid directly manipulating the DOM as much as possible, preferring to update state on our view component and re-render. This makes our code a lot easier to maintain, reason about, and debug. It does, however, have some downsides; it can result in re-rendering much more of the DOM than is necessary, lose scroll positions, and “jump” or “flicker” when re-rendering (depending on the elements being rendered and how they fit into the overall page). Furthermore, our approach favours moving more logic into our templates, which starts to push the limits (and intent) of handlebars. One solution is to apply only the changes between renders to the DOM rather than completely replacing it, using something like DOM diffing or a virtual DOM. To this end, we had a look at making use of the Incremental DOM library during one of our hack days. Incremental DOM is a library for building templates and updating the DOM in-place as required, instead of simply replacing large chunks of it. Templates are defined in JavaScript, using a handful of methods to open and close elements and create other DOM nodes. Clearly, this is not something you’d want to write directly, and it’s certainly not intended to be used that way. Rather, it’s designed to be a compilation target for a templating language. Once you have your template, you can update the DOM with a call to the patch method. That’s all there is to it. There are two steps we need to figure out to get it up and running in our project: Update Marionette’s render method to use the Incremental DOM patch method Compile our templates into Incremental DOM The first step is relatively straightforward, while the second is a bit more involved, especially given our particular setup. I’ll elaborate further on that shortly. It’s possible to override Marionette’s default render method to implement custom rendering. We were already doing this to plug in our handlebars templating, so adding Incremental DOM didn’t require much work. (Note, we’re using Marionette 2.4, so if you’re on version 3, things may look a bit different). We need to override Marionette.Renderer.render with our new render method, and we also need to update the view’s attachElContent method to get Marionette to use the Incremental DOM patch method instead of simply replacing the DOM node. Let’s quickly walk through what’s going on there. First, we define a function which we’ll use to override our view’s standard method of attaching rendered html to the DOM. Typically, this function takes a chunk of html generated by the render call as its parameter, but IncrementalDom.patch is expecting a function. Next, we set up our custom render function. In order to allow us to have a mix of views using our standard handlebars templates as well as using Incremental DOM, we use a flag on the view named renderer to determine which method to use. If the view is using Incremental DOM, we override its attachElContent method and return a function which runs our Incremental DOM template with the current data . As mentioned above, this function is what ends up being passed into patch . For now, our template functions are attached to window with the prefix tmpl_ . More on this later. All of our client-side templates are rendered in Rails views, on each request. This was less of a conscious decision and more a result of the evolution of our codebase over the years, and has both advantages and serious drawbacks, which I won’t get into here. We define our handlebars templates with a helper which wraps the template in a <script> tag. There are a few libraries and templating languages out there for Incremental DOM, but none quite fit into our exact use case. As such, we opted to write a quick parser to take care of it for us, and a new helper to define our Incremental DOM templates. We write the templates in what is essentially “embedded JavaScript”. Let’s jump right to an example. Anything in our template surrounded by <% ... %> is treated as JavaScript, and everything else is treated as html. We can also use <%= ... %> to insert the result of the enclosed JavaScript in a string or as a text node. Converting this to an Incremental DOM template, which is just a JavaScript function, is just a matter of converting html elements into Incremental DOM calls and leaving the rest, which proved to be quite straightforward. To wrap everything up, our incdom helper passes its block into the parser, assigns the resulting JavaScript to an appropriately named function on window (in this case, window.tmpl_todo_item ), and wraps it in a script tag. That’s it! At this point, we can write templates in our “embedded JavaScript” form and make full use of Incremental DOM in our Marionette views. In order to write clean, maintainable code in Marionette , we avoid manipulating the DOM anywhere outside of the render method, and prefer instead to modify view state and re-render. In order to get around some of the downfalls of this approach, we have swapped our handlebars templates and standard Marionette rendering with Incremental DOM, which updates the DOM with new changes rather than simply replacing chunks of it. We also replaced handlebars with our own form of “embedded JavaScript” templates, which allow us to use JavaScript to write much more powerful templates. While we have yet to put this technique into production, it shows promise in solving some of the issues we have with our current methodology. In the meantime, we’re continuing to explore a number of possibilities to improve our tech stack and, in turn, our codebase. Welcome to the 500px Engineering Blog! 62 Thanks to Ian Van Den Heuvel . JavaScript Marionettejs Backbonejs Incremental Dom Web Development 62 claps 62 Written by Software Developer, photographer, musician, climber. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Software Developer, photographer, musician, climber. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-08"},
{"website": "500px", "title": "aws elastic beanstalk and private docker hub repos", "author": ["Kevin Martin"], "link": "https://developers.500px.com/aws-elastic-beanstalk-and-private-docker-hub-repos-8ba823813642", "abstract": "Elastic Beanstalk makes it simple to deploy an application on AWS infrastructure, including automatic scaling. When this works it’s good. When it doesn’t it can be frustrating to debug but we typically have all the tools necessary to find the cause. I’ve deployed an application on Elastic Beanstalk where the Docker images were hosted on an Elastic Container Service repostory. This went well but was only for testing purposes as I’d made Beanstalk-specific changes to the Docker config file. Once this was deployed and tests were successful I needed to consolidate changes to the Docker config file and have Beanstalk pull from our Docker Hub repository. This proved difficult due to reasons. Originally, to use an ECS repo, I just specified the full repo name, along the lines of “123456789.dkr.ecr.us-east-1.amazonaws.com/service:latest”. This was enough to get the cluster to pull the image. Later, switching to using a private Docker Hub repo, things changed. To do this AWS says you must push a Docker credentials config file to S3 in an older format then reference this in your Dockerrun file. Getting the credentials into the correct format is simple, just strip an intermediate object (the “auths” key on the root object) and leave its keys as keys of the root. See the docs at AWS for details: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.container.console.html#docker-images-private I tried configuring things as described by AWS. It didn’t work. Deploying a new version meant to pull from a private Docker Hub repo resulted in errors in the logs like: Welp. That didn’t work. I tried re-doing things a few times because you never know… and that didn’t help. Now to actually sorting things out. We’ve got an instance ID. We can go from there to an EC2 instance we can SSH into (note that we’ll connect as the ec2-user). And have a look at the Docker config file: Everything looks well, proper user is there. Yet the `docker pull` fails. Well, those are the old credentials from testing against a Docker repo on AWS. Let’s just try that pull ourselves again: Failing, as expected. What if we remove the old creds… Oh. So that worked. So it turns out that this Elastic Beanstalk application was started with the AWS sample application. It then was changed to pull a Docker image from an AWS Docker repository. This apparently created a Docker config file with the correct credentials in the location that newer versions of Docker expect, at ~/.docker/config.json. Later, trying to switch to use the image from Docker Hub, requires specifying a key at S3 containing the Docker Hub credentials: Beanstalk then apparently pulls the credentials object at S3 and stores it on the EC2 instance at the old Docker config location, ~/.dockercfg. When the Beanstalk scripts (/opt/elasticbeanstalk/hooks/appdeploy/) tried to pull Docker preferred the new config location and, when that failed, it did not try to fall back to the old config location. This all makes sense but definitely led to a bit of pain. Ideally the EC2 instance would be cleaned up after each deploy, removing any credentials (or other temporary data) to avoid such an issue. So, I suppose the point here is that even with a system like Elastic Beanstalk providing a great deal of automation to ease your deployments there may still be issues. But, with a little investigation, nothing is hidden from you and simple debugging practices will still serve you well. Welcome to the 500px Engineering Blog! 11 2 AWS Elastic Beanstalk Docker 11 claps 11 2 Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-10-13"},
{"website": "500px", "title": "consistency managers communication channels rocketdata as a messaging bus", "author": ["Kevin Truong"], "link": "https://developers.500px.com/consistency-managers-communication-channels-rocketdata-as-a-messaging-bus-78ff2ba31622", "abstract": "I’m a Computer Engineering student at the University of Waterloo, currently working at 500px as a mobile developer for my co-op term. This term I’ve primarily been working on the 500px iOS app, and specifically tackled the problem of consistency management. Consistency management is a common problem for mobile apps. When a user interacts with the app, they expect that their actions will be reflected consistently throughout all areas of the interface. For example, if a user deletes their photo, they expect to see this photo removed from other views of the app, such as their profile or the home feed. Achieving consistency can be problematic for the views retained in memory, where the data can become outdated. Consistency is easy to take for granted — but when it’s not there, users notice! In 500px for Android, we use Jackie to ensure consistency . We wanted a similar solution for iOS, so we started off looking into what was already out there. After some research, we found RocketData from LinkedIn. RocketData provides similar functionality to Jackie, and both solutions were inspired by the same Facebook talk . RocketData has some nice benefits over Jackie: it handles concurrency slightly better, is explicit about what thread a function will run on, and it nicely handles most threading issues automatically. RocketData also supports disk caching, whereas Jackie only supports an in-memory cache. Overall, they both satisfy the core functionality required to solve the consistency problem. After playing around with RocketData, we realized that it could be used as a message bus to solve communication issues between various parts of the view hierarchy. Let’s dig into the details of both approaches, and describe why we ended up with the RocketData message bus approach. A key part of consistency management is communicating state changes throughout different components of our app. When there is a change in a data model, this change needs to be propagated to other parts of the app, so that other view controllers can update their UI to reflect the change. In some older parts of our app, we use NSNotificationCenter as a message bus to call different view controllers to perform some action, and we use the delegate pattern to handle communication between UIViews and their view controllers. The delegate pattern is a good way of handling communication between UI views and their view controllers. However, it can create long delegate chains when you have deeply nested view hierarchies. We encountered this issue when we started using IGListKit to simplify our massive collection/table view controllers. IGListKit allowed us to decouple our UI into different components: view controller, section controller and UIViews. The view controller managed all the section controllers, and the section controllers managed the different type of cell views that we presented. Here’s a simple example of the long delegate chain problem: the user clicks a button in the app. The button is in our UIView, which will have to call a delegate function that’s implemented in the section controller. However, the operation that we want to perform is in our view controller, i.e. dismiss the cell. Therefore, we need to make another delegate call. The section controller ends up just being a messenger between our UIView and the view controller. Although using IGListKit made our code more modular, cleaner and easier to debug, it introduced these long delegate chains which added a lot of duplicate code and were annoying to debug. It’s also wasteful because a section controller might have to store a delegate property that it may never use, but simply passes to one of its child views. First, we considered using NSNotificationCenter as an alternative to using delegates. With the NSNotificationCenter approach, we register observers for particular events, and send NSNotification events to trigger those observers. When notified, the observer calls its selector function to handle the event. One advantage to using NSNotificationCenter is that there is no coupling , so it makes it easy to swap out code. However, the NSNotificationCenter approach has several downsides: First, you need to make sure that you unregister all your observers or you risk having bad pointer crashes. This is no longer a problem with iOS 9 and up , but you still need to unregister observers for older iOS versions, as well as block based observers. Second, there is the risk of deadlock: if two observers broadcast notifications, and are listening to each other’s notifications, we will deadlock. The usual fix for this is to ensure that observers aren’t broadcasting any messages, but that is error-prone and often leads to messy workarounds. Third, NSNotificationCenter makes debugging difficult. When debugging there is no line-of-sight access between the observer and the notification sender . So if we were to place a breakpoint in the function that our observer calls, we can’t navigate the call stack to where the notification was sent to check the state of the app at that point. Those are just a few of the common issues with using NSNotificationCenter. We’ve experienced these problems first-hand in our app in the past, so we wanted to find a better alternative. To use Rocket Data as a message bus, we used RocketData’s DataProvider and DataModelManager classes. From LinkedIn’s post : “Any models that are added to a data provider become “managed” models. Rocket data will ensure that these models are kept in sync with the cache and any other data providers”. DataModelManager and DataProvider ensure that when one part of the app updates a model, the change is automatically reflected in other parts of the app. We decided to try using this functionality as a message bus by storing event objects in the DataProvider. I’ll use one of our collection view controllers as an example. First, we create a DataProvider in our ActivityViewController that holds a refresh event model (ActivityViewControllerEvent). In our viewDidLoad function, we instantiate a refresh event model and pass it to the DataProvider. We also set the DataProvider’s delegate to be the ActivityViewController: Next, we implement the DataProviderDelegate protocol to handle changes to the model: When we use DataModelManager to make a change to the refresh event model that is stored in the consistency manager, it will call the DataProviderDelegate in our ActivityViewController to handle this change. In order for our DataModelManager to detect changes between the model in the consistency manager and the model that we’re passing into DataModelManager, we need to implement the == function. In our case we give the ActivityViewControllerEvent a timestamp property, set to the current date when the model is created. This timestamp property is used by DataModelManager to compare the model to other models. Since the timestamp is set at creation, every model will be different. So when we use DataModelManager to update our refresh event model, it will always see that the objects are different and notify our DataProviders that there has been a change. In our case, the DataProvider in ActivityViewController will be notified and will call its DataProviderDelegate to handle the change. RocketData directly addresses the pain points from the NSNotificationCenter approach: there’s no risk of memory leaks, it’s easier to debug, and we don’t need to worry about unregistering our DataProviders. When we call updateModel from DataModelManager, the response from the DataProviderDelegates responds on the same thread, which simplifies things, and it automatically guarantees consistency. We’d love to hear about how others have solved these problems or tried out similar solutions, so drop us a note and let us know what you think! Welcome to the 500px Engineering Blog! 38 1 Thanks to Akbar Nurlybayev . iOS Swift Consistency Mobile App Development 38 claps 38 1 Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-28"},
{"website": "500px", "title": "a basic principle for writing sane backbone marionette code", "author": ["Michael Tighe"], "link": "https://developers.500px.com/a-basic-principle-for-writing-sane-backbone-marionette-code-a62d2d52bc0f", "abstract": "At 500px, we use Backbone Marionette for the majority of our web application. Marionette presents a fairly straightforward framework which is easy to jump into, but while it’s more well defined than Backbone on its own, it’s not very opinionated on how exactly you should structure your app. Over time, we’ve developed best practices which help us write sane, maintainable Marionette code. There is one basic principle that should result in mostly reasonable Marionette components: You should always be able to re-render. That is, calling render() should result in no change to the DOM. The DOM should always be a reflection of the state of the view instance. To put it another way, if we were to clone the view instance in its current state and then render it on another part of the page, both instances should render identical markup. We will identify a few rules and best practices that follow naturally from this principle and which will help you achieve it. This rule could also be called “ don’t touch the DOM” . Directly manipulating the DOM (adding/removing classes, changing styles, creating elements, etc.) in your JavaScript leads to a couple of undesirable results: Your view becomes more difficult to maintain and debug as it’s not clear where DOM state changes might be taking place. It becomes very easy to end up with a mismatch between what the DOM is showing and the stored state of your view. When this happens, some of the state of your component is actually stored only in the DOM, and re-rendering the component will cause this information to be lost. How can we avoid this? Do as much as you possibly can in the template, and if absolutely necessary, manipulate the DOM only in the onRender function (which is called by Marionette after rendering). You can make use of templateHelpers to make this a bit easier. Any time you need to change the state of the DOM, you should modify a view instance variable and re-render the view by calling render() . You should avoid storing state in the DOM at all costs. The DOM should reflect the state of the view instance, and you should never need to query the DOM for information. If some state is stored exclusively in the DOM (in the form of “selected” elements, checkbox states, user re-ordered items, etc.), then calling render() will cause us to lose that state, breaking our basic principle. There are a lot of obvious and oft-mentioned benefits to breaking code up into smaller chunks. It’s easier to read and maintain, promotes code reuse, is easier to test, helps parallelize development… the list goes on. As it relates to our rule of always rendering instead of directly updating the DOM, it helps us keep the scope of renders under control and render only what is required. If your app has distinct pages, don’t overcomplicate things by listening for events and updating the page piece at a time — just use the router. Even if you have some common elements, like navigation, you can pull those into their own view components and include them in each page. You can think of this as applying the Just render rule to the entire page. It’s much easier to follow and understand your code if there is one, clean path to building each page or screen. Of course, there are always exceptions to the rule. It is not always possible to avoid updating the DOM outside of render() . For example, sometimes re-rendering will result in a “jump” in the UI, or perhaps the loss of a scroll position. Rendering for each change also means you can’t use any transitions. Try to keep these situations limited to small sub-views in order to contain the damage, and it doesn’t hurt to leave a comment explaining why you’re breaking the general rule of thumb. To recap: Change the DOM by updating instance variables and calling render() Don’t update the DOM piecemeal throughout your view Do as much as possible in the template, use onRender if necessary Don’t save state in the DOM Break down views into small sub-views Use the router If you follow theses basic guidelines, and always ask yourself “if I re-rendered this view right now, would anything change?”, you’ll be on your way to writing some reasonably maintainable Backbone Marionette views. They seem straightforward, but even when you know them, they’re easy to ignore when deadlines are looming and crunch time is upon you. Your future self (and coworkers) will thank you if you resist that urge. Welcome to the 500px Engineering Blog! 19 Thanks to Alex P . Web Development JavaScript Backbonejs Marionettejs 19 claps 19 Written by Software Developer, photographer, musician, climber. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Software Developer, photographer, musician, climber. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-08-10"},
{"website": "500px", "title": "udp load balancing with keepalived", "author": ["Matt Stobo"], "link": "https://developers.500px.com/udp-load-balancing-with-keepalived-167382d7ad08", "abstract": "About halfway into my four month internship as a platform developer at 500px, I was faced with the problem of load balancing UDP packets. The rate limiting service that I had been writing was ready to ship: the code was finished, the Chef cookbooks were written, and the servers were provisioned. The only thing left on my plate was to come up with a load balancing solution. I needed to make sure that the rate limiter didn’t get overwhelmed with packets. There was no precedent in our codebase; this was our first service serving a large amount of UDP traffic. I had a clean slate to choose whatever implementation I wanted. A couple of options quickly surfaced. The first was Keepalived, a load balancer built on top of the virtual router redundancy protocol (VRRP) and the Linux Virtual Server (LVS). The other, more modern choice was everyone’s favourite proxy server, nginx . Initially, nginx was the more attractive choice simply because of better (i.e. existent) documentation. In fact, if you don’t need robust health checking, Nginx is the simpler solution. However, if you need something deeper than “ can I ping this server ”, you’ll have to look elsewhere. Since our rate limiter implemented an HTTP status endpoint to do more sophisticated health checks, we chose to move forward with Keepalived. A quick read though our Chef cookbooks showed that we already had keepalived.conf file ready to go. Two HAProxy load balancers were using Keepalived as a failover mechanism (as described here ). Basically, a VRRP-controlled IP address floated between the two machines, starting at the master and moving to the backup in case of a failure. Five different microservices were being managed under this virtual IP; it made sense to use this IP address to refer to the rate limiter as well. Only a few extra lines of configuration were needed. The outcome looked a little something like this: A reference for the Keepalived configuration can be found here . These settings define one virtual IP 10.1.1.20 as well as a set of two real servers load balanced under that virtual IP. Notice how easy it is to configure an HTTP health check! I tested out the changes on a couple virtual machines and was pleased to find that everything worked perfectly. Packets alternated between VMs just like I had told them to. It was time to ship. Unfortunately, nothing is ever that easy. Almost immediately after running Chef Client on the production load balancers, the site went down. All of the services behind those two load balancers were unreachable using the virtual IP. Needless to say we rolled back quickly, but we were confused. The old configuration hadn’t been changed. Why would introducing new, independent functionality break our load balancers? As it turns out, it didn’t. We had some deeper problems. After a day of digging through logs and documentation with my team, we came upon the problem. Upon examining a few of our servers, we found that their ARP entries for the virtual IP address pointed to the backup load balancer as opposed to the master. Packets arrived at the wrong doorstep and the secondary load balancer didn’t know what to do with them. It simply looked at their addresses, said “this isn’t for me”, and sent them away into the abyss. Armed with this knowledge, we dug a little deeper into the inner workings of Keepalived. Whenever a server transitions to the master state, it sends a gratuitous ARP message declaring that it is now the owner of the virtual IP address. When we made our configuration changes, we caused a failover situation, causing the backup to temporarily take control. When the master came back up, it should have sent its gratuitous ARP message to reclaim control of the virtual IP. But, it turns out we had pinned Keepalived to a version with this very specific bug . Ashamed at our ops faux-pas, we upgraded to a new version. We could have shipped at this point, but we were troubled. Other dark networking problems might have been waiting, ready to take our site down once again. It turns out that caution was to our benefit. After some more research, we came across something called “ the ARP problem ”. The ARP problem occurs when using LVS with direct routing or IP tunnelling. Since all of the machines in the LVS (ie. the load balancers and real servers) believe that they have ownership of the virtual IP address, it’s possible for clients to receive the MAC address of one of the real servers instead of the load balancer when making an ARP request. Put simply, there was a significant chance that the clients of the rate limiter were going to bypass the load balancer and access one of the rate limiters directly, rendering all of our work useless. Luckily, there is a simple solution to the ARP problem. Linux allows dummy networking interfaces to be added to machines. A dummy interface mocks a real IP address and does not respond to ARP requests. All we had to do was add dummy interfaces with the virtual IP address to the rate limiter servers and the ARP problem was avoided. Here’s how we did it: Our setup was starting to look pretty solid. There was one more thing we wanted to fix before we launched. In the event of a failover, we wanted to ensure connections to the load balancer wouldn’t get dropped. Rate limiting is an important piece in our infrastructure and accuracy is a key trait. Keepalived allows you to enable the LVS sync daemon through a configuration option . The sync daemon watches the master’s connection state, and continuously sends it to the backup in case a failover occurs. Our final configuration looked like this: This is the final solution that is running in production today. It took some research and some understanding of lower level networking, but our setup has been very solid. We are now using this configuration for two different microservices with no issues. While there are many pitfalls that need to be avoided, Keepalived turned out to be a solid choice to load balance UDP traffic. Welcome to the 500px Engineering Blog! 16 500px Keepalived Load Balancing Networking DevOps 16 claps 16 Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-21"},
{"website": "500px", "title": "app crash on long passwords or yet another uikit bug", "author": ["Akbar Nurlybayev"], "link": "https://developers.500px.com/app-crash-on-long-passwords-or-yet-another-uikit-bug-6f4fdc8314a6", "abstract": "So last week we release our brand new app RAW by 500px . The app was built around the idea of a streamlined process of publishing your photos to 500px. Of course in order to publish to 500px you need to be authenticated. So we started getting few reports from people complaining about a crash when they try to enter relatively long password. So the investigation began… Turns out it is likely a UIKit bug on iOS 10 (we couldn’t reproduce it on iOS 9) that causes an infinite loop that leads to an Out-Of-Memory crash. We narrowed it down to two values of UITextField properties — Adjust to Fit (scale down the font to a min size as input grows) and Secure Text Entry (to not show the password). So if we turn off the Adjust to Fit , since we don’t really need that for password fields everything works fine. Here is what happens after about 30 seconds once you’ve input about 20 or so characters on a iPhone 6S. No wonder iOS kills the app. Inspecting the Instruments tab reveals that following two functions are getting called non-stop by UIKit: Big kudos to 0xOS for figuring out the cause and filing a bug report with Apple. Welcome to the 500px Engineering Blog! 4 1 Swift Uikit Bugs iOS 10 500px 4 claps 4 1 Written by Father. Husband. Software Engineering Manager at TradeRev. We are hiring! Toronto, 🇨🇦 Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Father. Husband. Software Engineering Manager at TradeRev. We are hiring! Toronto, 🇨🇦 Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-10-17"},
{"website": "500px", "title": "synchronize associations through nested attributes", "author": ["Zimu Liu"], "link": "https://developers.500px.com/synchronize-associations-through-nested-attributes-d95391cf1d99", "abstract": "ActiveRecord provides a handy mechanism called Nested Attributes to help you build a complex form associated with multiple models. It allows you to create several models at once, using a code pattern similar to the one for a single update_attributes or attribute assignment call as you would usually do for a simple form associated with a single model. However, it also has some limitations for subsequent record updates. In this article, we briefly review the capabilities of nested attributes and discuss how to build an ActiveRecord extension to easily synchronize associated records through nested attributes. Let’s start with a multi-step form we built on 500px.com. This form allows photographers to register themselves for our Photography on Demand service. In this form, photographers can indicate their own company names as well as their specialties, such as Wedding photography, Automobile photography, Maternity photography, and so on. To store this data internally, we built a few models as shown below: Note the line accepts_nested_attributes_for in User model: it enables nested attributes support for specialties association and makes it possible to create specialty records while creating user's information. For example: Though the support of nested attributes has made our lives much easier for form creation, using it to update multiple models is not as straightforward as expected. As described in the documentation , if you wish to update existing associated records, you have to explicitly set the record’s id . For record deletion, you have to specify _destroy: true together with id . To correctly assemble a hash for an idempotent update, all these special treatments need to be taken care of in the API. More specifically, in our example of User - Speciality relation, we must do the following steps in the backend: Query existing Specialty records associated with the given user Use the predefined unique index (in this case [user_id, name] in specialties table) to cross-check if there are any records that already exist but should be updated or removed. If an existing record is not included in the update hash, an entry with id and _destroy will be added for record removal. If an existing record is included in the update hash, an id field will be added for record update. For example, an update request, from a user with the two specialties wedding and maternity , will be translated to the second hash illustrated below: Though the logic above is not hard to implement, it may become a bit of a nightmare if you have multiple associations — with different unique indexes — to update ( 5 associations in our case of the Photographer on Demand registration form). For us, copy-and-pasting the code with some revisions of association names and index names is definitely a NO . We’d prefer creating some generic helper to handle arbitrary associations, as simple as the declaration for nested attributes: By carefully reviewing the steps for proper translation, you may find the key is to use the unique index to cross-check the existing records. We couldn’t help but ask: could we pass the unique indexes into the setup for nested attributes, so that the helper could use them to automatically add id and/or _destroy ? Imagine how a statement like: could make our code much cleaner. After investigating the implementation of accepts_nested_attributes_for , we decided we could make this work with a bit of ActiveRecord and Ruby meta programming. We start by creating a NestedAttributesSync concern, which provides a class method to mimic what accepts_nested_attributes_for does to set up nested attributes. We name it accepts_nested_attributes_sync_for , where sync indicates that nested records can be automatically synchronized with the update hash passed in. This method accepts parameters to indicate indexes to use for each individual associations, programmatically analyzes/validates the underlying reflections (i.e., the ActiveRecord information about how two models are associated), enables nested attributes support with allow_destroy: true , and stores verified configurations into a class variable nested_attributes_sync_options . The code above basically reflects what we just described for the setup, but with one more step at the end of the loop: generate_association_sync_chain(association_name) . This generator is our secret sauce; it makes magic happen for nested attribute synchronization. The generator uses meta programming to synthesize an alias method chain to intercept #{association_name}_attributes= calls and translate the update parameters into the expected form for the nested attribute assignment. Internally, the translation is done in attributes_for_association_sync , which injects id and/or _destroy fields whenever necessary; interested readers may refer to our complete implementation to check out details. For tables which use composite primary keys (instead of surrogate keys) the composite primary key actually implies a unique index, which can be used by accepts_nested_attributes_sync_for . In our implementation, we build a default_unique_column_names_from_reflection helper to derive unique index info if a composite primary key is found. As you may have known , alias_method_chain is deprecated in Rails 5, leaving Module#prepend — a Ruby 2 feature — as the recommended mechanism for method interception. We are considering rewriting this code to make it Rails 5-compatible. Welcome to the 500px Engineering Blog! 40 1 Ruby on Rails Nested Attributes Metaprogramming 500px Active Record 40 claps 40 1 Written by Engineering Manager @ 500px Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Engineering Manager @ 500px Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-04"},
{"website": "500px", "title": "querying aws athena from python", "author": ["Kevin Martin"], "link": "https://developers.500px.com/querying-aws-athena-from-python-4bc0fbd611ca", "abstract": "Amazon recently released AWS Athena to allow querying large amounts of data stored at S3. This is built on top of Presto DB . Amazon releasing this service has greatly simplified a use of Presto I’ve been wanting to try for months: providing simple access to our CDN logs from Fastly to all metrics consumers at 500px. Previously we investigated using Presto on an Elastic MapReduce (EMR) cluster to execute queries during our daily ETLs. We’d then load those queries’ outputs to Redshift for further analysis throughout the day. This was meant to avoid the cost of having an EMR cluster running all the time, or the latency of bringing up a cluster just for a single query. Athena allows us to avoid this additional cluster management as AWS is providing the always-on Presto cluster. Some minor difficulties were encountered. Query execution time at Athena can vary wildly. In my evening (UTC 0500) I found query times scanning around 15 GB of data of anywhere from 60 seconds to 2500 seconds (~40 minutes). During my morning tests I’ve seen the same queries timing out after only having scanned around 500 MB in 1800 seconds (~30 minutes). We’ll have to see if these become more stable over time. Experimenting with table design brought up a couple issues as well. One of my tables with a RegexSerde took a little coaxing to nicely handle some minor log format changes we’ve done. This took awhile as the row format of a Presto table can’t be updated. Dropping the old table and creating a new one fails as it appears the table’s schema is cached for some time. This left me with creating the table with the new schema with a new name, generally just appending a number to it. Once I resolved issues with the schema I intended to simply rename the table to its final name. Again I ran into a minor difficulty: table renaming isn’t currently supported despite being documented. Oh well, I let the cached schema expire then created the table once again with its final name. Schema resolved, I moved on to integrating with our metrics pipeline that we’ve built in Python using Luigi . This… posed some minor challenges of its own. Athena currently only has two interfaces: the AWS web console and a JDBC driver. Making use of the JDBC driver from Python is possible with Baztian’s JayDeBeApi module, with a few minor tweaks. These were required as Athena’s JDBC driver doesn’t support passing some options as part of the URL but instead require they be passed as properties which were previously unsupported in JayDeBeApi. Along with this came a simple implementation of non-prepared statements — Athena does not support prepared statements yet. These tweaks are available at https://github.com/Melraidin/jaydebeapi One further issue encountered while initially testing the JDBC connection was the seeming inability to query any tables outside the default DB. For now I’ve simply put our tables in the default DB and asked on the AWS dev forums ; I’m sure this will be resolved soon (or I’m doing it wrong). I received a response to the above issue in the linked thread. It seems that through the JDBC driver (possibly an artifact of how I’m using it from Python though) the DB name must be quoted, e.g.: \"db\".\"table\" Querying then failed with an unexpected Java error seemingly related to type conversions. Trawling the dev forums again found a thread mentioning an issue with the decimal data type . Changing our decimal columns to doubles resolved the issue but isn’t ideal. I’m sure this will likely be fixed soon. This brought us the ability to query Athena from Python. The last piece remaining were the base classes for Athena interactions with Luigi. These simply factor out boilerplate that would otherwise be required for Athena tasks. We have one to query Athena and store the results at S3: One note: after querying Athena we remove any key at the result path at S3 ending with “.csv.metadata”. This is an object that is created by Athena that might be required for the Athena web console to properly display the results. It also causes problems when loading to Redshift as it will match our S3 path in the Redshift “copy” command causing a failure to load. See this discussion at the AWS forums. After getting our results to S3 we only have to load them to Redshift. This base class makes use of an internal class to load data at S3 to Redshift but should give you an idea of the process: What this all leads to is a task pipeline to load data such as logs from Fastly to Redshift that looks like this: This pulls data from our partitioned Fastly table through Athena. We’re able to partition the Fastly table in a form suitable for use from Athena using some undocumented options for the Fastly log location. It supports at least some of the format characters from strftime() giving us hourly partitions to optimize our queries. Thanks to Aaron Peckham for this tip. And so now we’re able to pull data from Fastly logs to Redshift for further analysis. This will let us take advantage of some of the more interesting data Fastly makes available in their logs, like geographic location. Welcome to the 500px Engineering Blog! 62 13 Thanks to Gavin Sharp . Big Data Analytics Athena Presto Fastly 62 claps 62 13 Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-21"},
{"website": "500px", "title": "prototyping a new photo recommendation engine", "author": ["David Rusu"], "link": "https://developers.500px.com/prototyping-a-new-photo-recommendation-engine-d57a4b1dae89", "abstract": "500px is, first and foremost, a community of photographers — millions of amazing photographers — who upload thousands of photos every day. Users can follow photographers whose work they enjoy and then see these photographers’ photos in their feed. Users can also show their appreciation by liking photos, either from photographers in their feed or from our ‘Discover’ section. Given how many photos we have — over 80 million — it can be hard to find those great photos that match your taste. We want to make this easier. One way we help users discover new content is with personalized recommendations. On mobile, we thought the home feed was a great place to start. Our mobile apps already included image recommendations, but the algorithm behind these recommendations is simplistic. In essence, we ranked photos by number of likes from photographers you follow. That is, if 3 of the photographers you follow liked the same photo, that photo will have a score of 3, do this for every photo and return randomly chosen photos weighted by the photo scores as recommendations. The problem with this approach is that instead of looking at the photos you have liked directly, we are looking at photos liked by people you follow. This means your true preference is muddled by one level of indirection. Recommendations given by this algorithm tend to replicate our Popular page. We felt that we could improve on this approach by using collaborative filtering. The idea behind collaborative filtering is that if Alice has liked similar photos to Bob, Bob & Alice probably share a common taste in photos. We can now use this correlation to show Alice photos she will (probably!) like based on photos liked by Bob. This would allow us to use the data that is generated by the user directly which, in turn, give us more confidence in our recommendations. To do this, we worked with researchers at the University of Laval to implement and evaluate a collaborative filter. We knew we wanted collaborative filtering, but there are many collaborative filters. We were not sure which one would perform best with the type of data that we had. We pruned the choice of models down to ALS, WALS and Factorization Machines and decided to evaluate models using the p@k metric . The data we are using to train these models is the photo likes by users. Imagine a very large matrix where rows represent users, and columns represent photos. Entries in this matrix are either ‘one’ or ‘zero’, where ‘one’ means the user represented by that row has liked the photo represented by that column. A ‘zero’ means we don’t have data for this interaction. Note that “no data” is not the same thing as “dislike” — more about that later. For now, our models were only receiving information on what users like, not on what users don’t like. To evaluate these algorithms, we took three months of our “user X liked photo Y” data and fed it into the recommenders. At first, it essentially shows photos at random because it has no information. But sometimes, by chance, the recommender suggests a photo the user has liked. We mark that as a win, and retrain the model with this new data after each virtual day. This method of evaluation lets us see how quickly the recommender improves when it’s given new information, as well as telling us which recommenders perform best at the end of the 3 months. The research team at the University of Laval evaluated these recommendation algorithms and found that WALS — Weighted Alternating Least Squares — worked best with the current dataset we had. We now had a promising recommendation engine, but evaluating based on replayed data can be error prone. Liking behaviour has an implicit bias caused by the old photo recommender — that is, users can only like photos they can see, this means the existing photo like data is going to have a bias towards photos that the old recommender likes to recommend. The only way to know for sure if WALS gives useful recommendations is to test it in production. Our algorithm was still experimental, so we wanted the ability to iterate quickly without disrupting the rest of our system. Wrapping our WALS recommender in a microservice allowed us to do this, and gave us more options for scaling it later. This recommendations microservice is a simple Flask app that queries the WALS model for photo recommendations and stores the viewed photo IDs in Redis. We didn’t want to have users receive the same recommendation multiple times, so tracking viewed photo IDs lets us exclude viewed photos from the recommendation results. Next, we designed an A/B test: a controlled experiment where we have two versions of a user interface and we want to know which of these two versions performs better according to a chosen metric. A successful experiment is one where we get a statistically significant difference in the metric we chose between the two versions. Given that WALS needs user activity before it can return useful results, we limited our sample to users who have liked at least 10 photos in the past 90 days. From this pool, we randomly split 100k users between the legacy recommender and the new WALS recommender. Finally, we updated our apps to support the new recommendation engine. We use a lot of microservices at 500px — each of our frontends might need data from a few services. This is why we have a BFF (Backend for Frontend) service. This service acts as an intermediary between our frontend apps and the microservices, abstracting away the details of which service to call for which type of data. The WALS-based image recommender is a great example of this. The BFF can check whether a user is part of the WALS-based testing pool, and use a different back-end service if so. The experiment showed good results. This gave us the confidence to deploy personalised recommendations to all our mobile users. If you’d like to try them for yourself, download our iOS or Android apps and make sure you’ve liked at least 10 images (our Popular page is a great place to get started). You’ll start getting personalised image recommendations within 24 hours. We recognized we were throwing away a lot of useful information by not giving the model a negative signal. A negative signal in our case is when a users sees a photo, but does not press the like button. Zeros in our previous model meant one of two things: the user didn’t see the photo (no information), or the user saw it but didn’t like it. To remove that ambiguity, we introduced a -1 value to represent “user doesn’t like this photo”, leaving zero to cover the “no information” case. While we were busy building out the infrastructure to test WALS in production, the researchers at Laval were hard at work expanding the models to support negative signals. Their analysis showed that this time, ALS performed significantly better. We are currently in the middle of an A/B test between WALS and ALS, both using a negative signal. Expect even better recommendations soon! Welcome to the 500px Engineering Blog! 64 1 Thanks to Julian Villella and Gavin Sharp . Machine Learning Data Science Recommendation System Photography Prototyping 64 claps 64 1 Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-09-07"},
{"website": "500px", "title": "7 years of 500px then and now", "author": ["Raymond Ji"], "link": "https://developers.500px.com/7-years-of-500px-then-and-now-27e287ace67e", "abstract": "500px turned 7 this year! Our community has experienced tremendous growth since it was founded in 2009, and to cap off our 7th year I thought it would be interesting to explore how 500px photographers and their photos have changed. Liking a photographer’s photo is one of the most fundamental user actions on 500px. The “like” has existed since the site began, and it remains the most common way photographers engage with one another’s photos. I thought that comparing our first ever likes with our latest would bring up some interesting patterns and differences. I started by querying our Redash for the very first 100,000 likes our users ever performed, and a batch of 100,000 likes from early 2016. I then wrote some code to analyze both datasets and used Sigma.js to render the visualizations. This isn’t a rigorous analysis of the data but rather an approximate, visual look into our community. Every node in this graph (visualized as a black dot) represents a user. The edges (pink lines) indicate one of them has liked the other user’s photo. Each graph has exactly 100,000 edges (likes), but it’s immediately visible that the 500px community has grown significantly since 2009 — the number of users (black dots) represented in the 2016 graph is far denser. In 2009, it took our community over 1 year to generate our first 100,000 likes. In 2016, it took our community less than 10 days to generate the same number! Each horizontal bar represents one hour, and the density of the bar represents how many likes occurred during that hour of the day (Toronto time). In 2009, user activity fell off significantly from 12–5am, a time when much of the 500px community was likely asleep. 12–5am is still the least active time period in 2016, but by a far smaller margin. 7 years later, the 500px community is much more international, keeping the site active around the clock. In 2009, our most active period was 8–9pm with 7% of all the likes, and the least active was 2–3am, generating only 1% of all the likes. In 2016, the most active period was 6–7pm, generating 6% of the total like activity, and the least active was 5–6am, generating 3%. The gap is closing. I define the ‘Popularity Lifecycle’ of a photo as how the number of likes per day that a photo receives changes over time, beginning from when the photo was first uploaded to 500px. To measure this, I calculated the time difference between when a like occurred and when the liked photo was uploaded. Likes close to the upload time are plotted at the top of the graph, and “delayed” likes are plotted at the bottom. In both years, our community gave the lion’s share of its likes to new photos on the site; but even very old photos continued to receive some affection. In 2009, photos posted by the 500px community experienced a slightly more gentle falloff in likes over time; the graph is more dense at the 1-week and 1-month marks than the 2016 graph. Numerically, photos in 2009 received 29% of their likes in the first day after being uploaded, dropping to 2.5% about a week later. In 2016, photos received 55% of their likes within the first day of being uploaded, and around 1% a week later. As our community has grown and the rate of photos being uploaded has sped up, there is significantly more ‘new’ content available to view at any moment, so the distribution has shifted towards the short-end. The density and height of each bar reflect the number of likes that Photo category received. Overall, the popularity ranking of categories has remained relatively stable in the 500px community, with a few notable changes. Several new categories (including Nature and Street) were introduced partway through the 2009 dataset’s time period, so they are underrepresented in the 2009 graph. In 2016, the community has wholeheartedly embraced many of these new categories — Nature in particular has become one of the most popular categories on 500px, with a 7% share of the total likes. On the flip side, Journalism effectively switched places with Nature in popularity, dropping from 7% of the total likes in 2009 to only 0.2% in 2016. Even accounting for the addition of new categories, the 500px community has become much better at categorizing their photos since 2009. Uncategorized went from 32% of the total share of likes in 2009 to just 4.3% in 2016. At the top end, the three most popular categories remained stable between both time periods — People, Landscapes and Nude, in descending order. In 2016, Landscape has nearly caught up to People in popularity, rising from 7% to 18% of the likes. I hope this quick look at how our 500px community has changed was interesting. It’s amazing to look back on where we came from, and where our community is today. Our company mission is still the same — to enable and reward visual creativity — and I’m excited to see how our 500px community continues to change and grow! If you want to learn more about 500px and the company history, take a look at the company timeline Welcome to the 500px Engineering Blog! 14 Photography Data Visualization Website 500px Community 14 claps 14 Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-13"},
{"website": "500px", "title": "splash search how to do image search on 250k images in 100ms", "author": ["erik"], "link": "https://developers.500px.com/splash-search-how-to-do-image-search-on-250k-images-in-100ms-bf87e56546f2", "abstract": "H ey! My name is Erik, I’m a Systems Design Engineering student at the University of Waterloo and I’m a coop on 500px’s innovation team. One of the things I got to work on this semester was a colour search tool we called Splash. E very quarter, 500px holds a 2 to 3 day hackathon for all employees. This July, myself and David, a coop on our web team, decided to build what would eventually become Splash search . We quickly realized we would need support from our platform team, so we also enlisted Madigan , another coop from that team, to help us out. With our team ready, we started hacking. (Splash was originally called “Sam Search” after our coworkers adorable Husky-German Sheppard who walked in while we were trying to pick a name. Turns out we can’t send out a press release like that so the name had to go.) Splash works by allowing you to sketch a terrible drawing of what you want on a canvas and sending it to our backend which searches through tens of thousands of photographs from our marketplace finding the most visually similar images, sorts them based on similarity, and returns the images. For example, if you wanted a very dark portrait of a person in the center of the frame, normally you’d have to search for “dark portrait” and scroll until you find the framing you want, but we wanted to enable you to search by painting a canvas black, adding a dash of colour in the center, restricting the search to people, and seeing what you get. The math we use for finding similar images is incredibly simple: resize all images and the users reference sketch to small squares, reshape the image and sketch matrices into 1D vectors, and take a weighted euclidean distance (sort of) between each image and the sketch. For comparing a sketch to a single image, the process looks like this: Resize both the sketch and picture to 16 by 16. For each channel of each pixel, we square the difference between the value for the sketch and the image. We then weigh the value by the alpha of the sketch, so if the user didn’t draw anything in part of the canvas they don’t care what’s there. We do this for every pixel in our 16 by 16 grid and add up all those values. Now normally, to get a euclidean distance you’d have to take the square root of that sum but we’re not interested in the actual distance, we only want a relative measure against other images and not square rooting maintains the relative ordering of distances. We do this to get a distance between the sketch and every single image, sort based on which images are “closest” to the sketch, and return the top 100. That’s it! We chose to use golang for this hack (I wanted Python but I was outvoted…). Like any hackathon project, there were a number of things wrong with it and it was amazing it worked at all. We were initially using 70 by 70 images that had to be loaded from disk every time the service was started, our threading model was a little broken, we hit our own rate limiter before we finished pulling the data we needed so we only had a partial dataset… Somehow we got it working and won the prize ($200 to spend on photography gear) for the most innovative hack! The team decided that we should pursue this project further and release Splash to our community. This meant we had to fix all of the small things wrong with it, get the UI done by one of our designers , get it to start up quickly, and generally not be a messy collection of supporting python and SQL scripts that no one else understands how to use. After a couple of weeks of work including an all-nighter by Madigan, it was finally ready — starting up in seconds not minutes, codebase readable by humans, not a nightmare to deploy, looking great — and we released it to the world. At this point, we had changed the image comparison to using 16 by 16 vectors instead of 70 by 70, fixed our threading, and made a bunch of micro-optimizations that got our search time of 500ms for 15k images to about 70ms for 50k images, a speedup of about ~23x. After a bit of experimentation, we found out you can reduce your image vector size to about 8 by 8 without any significant loss in result quality but we kept it at 16. At that point the bottleneck was RAM and shrinking the vector size did not speed anything up. The hack just searched through about 15k images, but in production we can get through 50k images quickly with 5 categories to choose from. Shortly after releasing Splash the articles started coming out, nothing we couldn’t handle. But then it was posted to reddit . Within a couple of hours, it had made it to the front page peaking at the second link from the top. At that point we were serving about 7000 concurrent connections and had to ramp up the number of servers from 2 to 18 to survive the hug of death, but we made it without anything going down and I finally get to say I did a thing that got reddit front paged. Working on this hack and bringing it to production was one of the most rewarding things I got to do this semester, the support we got from both our team and our community was fantastic, and seeing the product make a Splash — I’ll show myself out. If you’re interested in joining a startup that has the team and the community to make this kind of hackathon project possible, apply here ! Welcome to the 500px Engineering Blog! 16 Design Image Search Hackathons Web Development 16 claps 16 Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-09-01"},
{"website": "500px", "title": "managing consistency on android", "author": ["Priyesh Patel"], "link": "https://developers.500px.com/managing-consistency-on-android-8767817d057a", "abstract": "Users of apps expect that their interactions are reflected throughout all areas of the interface. For example, after a user deletes their photo, it should no longer be visible in other areas such as their profile or feed. Achieving this can be problematic for views retained in memory, where the data the views present can become stale or outdated. Consistency is something users take for granted. If you don’t have it, it will be noticeable and will reduce the confidence users have in your app. In this post, we’ll discuss our approach to managing consistency on Android. To go back to the example above, when a user deletes one of their photos, we want to remove it from the feed and their profile. FeedFragment and ProfileFragment are retained in memory while the app is running using a standard ViewPager embedded within the single instance of our main Activity . The important thing to note here is that these fragments are instantiated only once for the duration of the app’s lifetime. As a result, the underlying data that they each present can be become stale in the event that the data is modified elsewhere. In order to keep the fragments up to date with the newest data models available, some further action is required. There are a few approaches to achieving this: Define interfaces, allowing views to communicate with each other Global notification mechanism (centralized event bus) Publish-subscribe system The first solution that may come to mind is to simply use an interface to allow FeedFragment and ProfileFragment to listen for changes made in PhotoFragment . In this solution, PhotoFragment holds a reference to each of the listeners and notifies them of the deletion. Each listening fragment is then responsible for removing the item from its internal state and updating its interface accordingly. However, there are a few issues with this approach. For one, PhotoFragment now has responsibility beyond its initial scope of simply presenting a photo. It should not be concerned with coordinating updates in surrounding fragments, let alone know of their existence. Another drawback to this solution is the lack of elegant scaling. Whenever a new section of the app needs to respond to photo deletions, we must add more extraneous code to PhotoFragment . For common actions in a large app, this can quickly lead to tightly coupled code where there is no clear separation of concerns. Another solution is to use a global notification mechanism, or an event bus. In this approach, changes are represented as event objects and are broadcasted to a receiver. The receiver then determines the event type, dissects any necessary information, and handles it accordingly. This was the route taken previously on our iOS app, using NSNotificationCenter . Although this loosens the coupling between views, we found it had some drawbacks. The number of event types quickly became difficult to manage, especially when trying to relay specific information about each event such as failure, success, and completion. Another annoyance we encountered with NSNotificationCenter was that notifications (events) are identified by strings. This led to the app being polluted with string constants, in an attempt to workaround the loss of type safety. The last approach we’ll discuss is the publish-subscribe model. The idea here is that publishers send messages without knowing specifically where they will be received. Subscribers then subscribe to the types of messages that they’re interested in receiving. This system allows data producers to be almost entirely decoupled from data consumers. Scalability is another benefit of this model. Adding support for additional data types simply requires writing new publishers and subscribers, and doesn’t interfere with the existing ones. For these reasons we decided to use the pub-sub model at the core of our solution. Jackie is a consistency cache for Android that we built at 500px. At a high level, Observers subscribe to DataItems (model objects) and are notified of changes. Jackie maintains a centralized cache of your data and lets you easily circulate updates of the data throughout your application. DataItem is a simple interface that requires classes to implement the getId() method as a unique identifier of the item’s instance. Unique identifiers let us pass around immutable copies of objects, rather than modify a single object instance, so that we don’t have to worry about multiple threads mutating the same object. In order to conveniently update an object in an immutable manner, we make use of Lombok ’s Wither annotation. This lets us clone an object while changing a single field. This can be thought of as an immutable setter. Here’s a basic example to illustrate this: Now let’s have a look at how we would handle the photo deletion example discussed above using Jackie: First we define observers in each of the fragments where we want to listen for changes to photos. ListObserver provides a few other callbacks for more granular updates, but for the sake of simplicity, we’ve only implemented onItemsRemoved() . Inside of this method, we can update our fragment to reflect the removal of the given photos. Typically this would be notifying a list adapter of the changes; however, the implementation is entirely up to you. After the observers have been set up, all that’s left is to subscribe them to an identifier with some initially bound items. There are a few things to note here. In order for all of this to work, Photo must have implemented the DataItem interface. As do most resources coming from a data store, Photos on 500px each have a unique identifier, so implementing getId() is as simple as returning it. Another thing to remember is to unsubscribe observers when necessary. In the realm of Android, this should be done when the fragment or activity is destroyed, to avoid leaking the observer: Finally, in PhotoFragment we simply inform Jackie of the photo deletion when it occurs. Jackie takes care of notifying all subscribed observers. The beauty of this solution is the fact that PhotoFragment does not need to manually update other areas of the application. Leveraging Jackie to manage data consistency allows for concise and decoupled UI code and eliminates the need for communication between unrelated pieces of the interface. We noticed that most of our fragments and activities had essentially the same behaviour. They all fetched data from an API, bound the data to views, and sometimes handled pagination. These tasks, while not very difficult, required lots of boilerplate code which was duplicated across many areas of our app. We took this as an opportunity to build an abstraction we call RestBinder to reduce the amount of setup and glue code needed in our views. RestBinder lets us bind fragments and activities to an API endpoint we’re interested in accessing and uses Jackie under the hood to cache the incoming data. It also handles common use cases, such as pagination and refreshing, and accounts for the Android lifecycle by providing methods to easily subscribe and unsubscribe listeners, as well as save and restore state during configuration changes. From the listeners defined in our fragments and activities, we delegate presentation of the received data models to custom views. This is done via a method exposed on our views, <A> void bind(A a) , where A is the data type which the view displays. This pattern lets us tuck away the often uninteresting presentation code and provides an explicit way of interacting with the view from the outside. With these abstractions in place, our fragments and activities become extremely lean and their intentions are made clear. The responsibility of networking and presenting data is neatly contained, while the fragment or activity simply serves as a thin layer to handle delegation. Currently Jackie is used internally by the 500px Android team, allowing us to easily tackle consistency issues and make our code easier to maintain. Plans are in the works to polish the API and add features including RxJava bindings. We hope to eventually open source the project so that other Android developers can benefit from the power of Jackie! Welcome to the 500px Engineering Blog! 48 3 Thanks to Akbar Nurlybayev , Julian Villella , Gavin Sharp , and Kaiva . Android Android App Development 48 claps 48 3 Written by Computer Engineering, University of Waterloo Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Computer Engineering, University of Waterloo Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-16"},
{"website": "500px", "title": "improving 500px api performance presenter model caching", "author": ["Madigan Kim"], "link": "https://developers.500px.com/improving-500px-api-performance-presenter-model-caching-99720550ecda", "abstract": "Hello there! I’m Madigan, a Computer Science student at the University of Waterloo. This is my second co-op term working with the platform team at 500px. It began with a Rails upgrade. At the beginning of this year, 500px completed a major Rails upgrade, from version 3.2 to 4.2. After several weeks tackling various incompatibilities between the old version and the new, we steadily rolled out the upgrade to different parts of our system. The result? A massive performance hit to our API servers. We dug into the Rails source code to see if there were any changes that could explain the performance hit, and we presented our discovery at Ruby Lightning Talks T.O. . One member of the Toronto Rails community found that a commonly used Ruby method was significantly slower on a Rails 4.2 console than in a Rails 3.2 console. This discovery led us to do more profiling, and while it turns out that benchmark didn’t explain the performance hit we were seeing, we did discover that a large part of our API server response time was spent on JSON serialization in our presenter models. Presenter models are Ruby modules used by our API controllers to render JSON responses. Each presenter model corresponds to a Rails model (Users, Photos, Galleries, etc.) and specifies which attributes to return to the client. Over time, and through no fault of any individual, the presenter model code had become plagued with conditionally rendered parts, ad hoc database queries, and little to no caching. We realized we couldn’t fix this with incremental improvements — we needed to break down this overly complex behemoth. Our goals were as follows: Improve performance Maintain identical functionality Keep things maintainable We decided the obvious win would be to introduce caching of presenter models. This would reduce the number of database queries, and remove the need to execute the complex business logic in the code for every request. We decided to use MessagePack for serialization because of the outstanding results from performance tests conducted by several different people . As an example, let’s take a look at how we can cache one of the Gallery presenter formats. A full Gallery presenter has the option of including or omitting fields for the gallery owner (in a short User presenter format). We considered storing the Gallery presenter and the User presenter separately, and then stitching them together at the end. However, this design would require that Gallery presenter query for multiple users when responding to bulk gallery requests. We didn’t want to make database queries in the presenter model, so this idea was put aside. The much simpler solution we settled on would be to store separate cache entries for full Gallery presenters, one with and one without the user, and putting the option in the cache key. So the code for caching a gallery model with user data would look something like: But wait! We realized that this caching logic lends itself well to Redis Hashes. Unlike Memcached, Redis can store several different data types, each with their own set of advantages over plain old strings. With Hashes (which are just maps between string fields and string values), we can associate each object with each presenter format by setting the options as the field and the serialized presenter as the value: This makes it much easier to delete all cache entries for a single object. Under the first strategy, we would have had to iterate through all keys prefixed with “model_123” . With Redis Hashes, we can just delete entries using the main key. How do you prove your idea is working? Back it up with numbers! Graphs! Timelines! We needed to demonstrate that our Redis-backed Presenter Model Cache improved performance, but we also wanted to ensure that it maintained identical behavior. This exact problem can be elegantly solved by GitHub’s Scientist . With Scientist, you can run experiments that execute both your old code (the control) and new code (the candidate) in some random order with the same input data. The results of the experiment contain the duration and return value of the control and candidate, which can be published to a monitoring system like Datadog for comparison over time. To use Scientist, we first defined a custom Experiment class that publishes results to Datadog: We then wrap the control in a use block and the candidate in a try block, and run the experiment. Upon completing its run, the experiment returns the result of the control, which can be used to render our response as if nothing happened. To run this in production, we wanted to only measure a small sample of requests, so we modified the Nginx config file on one of our many API servers to conditionally append a use_scientist=true parameter based on the request URI. Note that if statements are strongly discouraged by Nginx and should be used with caution. Here is a snippet of our ever-cautious config file: Our experiment was conducted on our galleries endpoint, which returns up to 100 galleries with user data in full presenter format. Initially, and every so often when cache entries expire, the cached version of the code took a few milliseconds longer to populate Redis with presenter models. However, the graph below shows that in aggregate the Presenter Model Cache took a significantly shorter time to assemble the gallery presenter models with their respective user presenter models. It’s worth noting — although I am a bad scientist and do not have a record of this — that when we used Memcached to store the presenter models, there was hardly any improvement to the request duration. We suspect that the improvement from using Redis came from their “special” encoding scheme for Hashes with fewer than 100 fields. The Presenter Model Cache is a simple and effective solution to our performance issues that can easily extend to other API endpoints. Although it provides the same functionality and better performance as the old uncached presenter models, it introduced more conditional logic, more parameters, and more code into our Rails monolith. We wondered: could we get these caching benefits without neglecting our maintainability goal, by somehow simplifying the presenter model or speeding up (or parallelizing) database queries? Perhaps in a different language? Perhaps as a microservice? Our next step will be building a hydration service that acts as a proxy between Nginx and our API servers. For certain endpoints, the hydration service will request partial responses from the API, and then concurrently makes database queries to fill in the missing parts. We’re writing it in Go, and we hope to extract the presenter model logic from our Rails monolith, and parallelizing as much of the code as we can. Our hope is that the hydration service will reduce complexity in the monolith, making our Rails code more maintainable. My work term is coming to an end, but there are still several experiments left to run, and many other strategies worth exploring. The quest for better API performance continues at 500px! Welcome to the 500px Engineering Blog! 15 1 Thanks to Gavin Sharp and Margaret Leibovic . Web Development JavaScript Platform Web Performance Caching 15 claps 15 1 Written by Madigan is an intern at your company. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Madigan is an intern at your company. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-25"},
{"website": "500px", "title": "how to get 74 million followers", "author": ["Nick Frasser"], "link": "https://developers.500px.com/how-to-get-74-million-followers-c38be8867bd4", "abstract": "In April 2016, a user wrote in wondering why their profile said they were following 6 people but it was not possible to see who they were. Thus began my month-long journey that ended in triple distilled, single-malt scotch whisky. 500px uses a MySQL-based master/slave database to store the most frequently accessed user data. I quickly narrowed the cause down to how we store who follows whom. That data lives in one database table, “ followings ”, that’s structured like this: And here’s how followings is related to users : For example, if I follow Alex , I am the follower and he is the followee. An entry is created with my user ID in the user_id field and Alex’s ID in the followee_id field. There’s another column called user_active which tracks the follower’s account state (e.g. active, banned, account deactivated). It has the same value as the active column from the users table. When retrieving the followers count on a user’s profile, we can write a query like this: This way we don’t have to join the users table to get the active state! The database table tells us whether the follower in the following relation is active, but not the followee. This is why the user wasn’t seeing any of the 6 people they were following — they were following 6 inactive people! Inactive accounts aren’t displayed in the list of people following the user, but there’s no way to retrieve a count from the followings table that excludes inactive followees. Realizing this, I thought “Great, an easy fix! All we need is an extra followee_active column to track the followee’s state!” So I wrote a migration, ensured that all the counting code uses the new column, submitted a pull-request and called it a day. If only it had been so easy. A little while later I got this message from Chris, our director of platform at the time. Those pull requests had been opened in early 2014. That’s from back when we used Jira for project management, and the ticket number was 42 . No one has been able to fix this bug for over two years. And by “a very long time”, Chris meant “hours”: When running an ALTER TABLE SQL query, MySQL has to rewrite every row in that table. It locks reads/writes on the table for the duration of the migration . In other words, it’s very very difficult to add an extra column to that table without taking 500px down for half a day — a site with active users in nearly every country in the world. Obviously, this is not feasible. So I sat down with Chris and the web team to devise a better solution. I should note at this point that I am a co-op student and this was my second week at 500px. What had I gotten myself into? The pull requests Chris referred to in his comment attempted to solve the issue by repurposing that user_active column to track the state of both users. The column had type tinyint , capable of storing numbers from 0 to 255, so we could use different values to track all combinations of states. We began with a mapping that tracked all the different states: But then I noticed that this could be simplified. A following is invalidated when either user becomes inactive — we don’t care whether it’s the follower or the followee. So now our mapping boils down to this: This solution made the code clearer and had good performance, so it’s the one we went with. Changing the meaning of this database field meant all the application code that touched it had to be rewritten. I wrote methods to manually update the user_active field for all relevant followings when a user’s account was deactivated or reactivated. This was not as straightforward as it sounds. Some of our users are followed by over 100,000 people .Trying to update 100,000 records in one go would put significant stress on the database. It would also take a while; a long-running web request like this would quickly be killed off by the Unicorn instance. We used a Sidekiq worker to call the update methods instead, which avoids these problems. The workers update a batch of a few thousand records, pause for a few seconds, then move on to the next batch. Not pretty, but it works. Once deployed, we had to update all following records in the database where at least one of the users was inactive. Again, this had to be done in batches of a thousand records each. Here’s the code for it: The selection code is very similar to the aforementioned deactivation/reactivation code. The migration took about 8 hours and updated about 5.5 million rows. I was honestly surprised at how smoothly it went. And voilà The issue is resolved. My reward? This beauty: From the 500px Platform team, for squashing a hairy, two-and-half year old bug. Shout out to Chris, Colin, Jacob, Junji, Keith, Kevin, Madigan, Paul and Vlad for the liquor, as well as for running a very tight ship. In an ideal world, web developers don’t need to worry about the servers catching fire. The database should abstract away much of the complexity and performance concerns we faced with this bug. A different brand of SQL database or data store may indeed have taken care of some of these for us. But reimplementing an existing system is not a trivial task. Many readers of this post will be all too familiar with technical debt and the obstacles to moving away from technology that’s a headache to maintain. As a community for photographers with millions of active users, followings are heavily ingrained into our product. Making significant changes to them is a always huge undertaking. Reimplementing them could take weeks (or months) of effort across the entire development team, without many user-facing benefits. Right now we’re focused on building a healthy, focused and self-sustaining community. While we work on that, we try to find opportunities like these to pay back some technical debt. That means we can provide an even more refined experience for both our users and our developers. If you’d like to help us get there faster, we’re hiring ! Welcome to the 500px Engineering Blog! 11 1 Ruby on Rails Programming 11 claps 11 1 Written by Software Developer at Structura Bio. Aspiring photographer. Alleged hipster. http://nfrasser.com Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Software Developer at Structura Bio. Aspiring photographer. Alleged hipster. http://nfrasser.com Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-16"},
{"website": "500px", "title": "image classification with hadoop streaming", "author": ["Renat"], "link": "https://developers.500px.com/image-classification-with-hadoop-streaming-1aa18b81e22b", "abstract": "Note: this article was originally posted on a previous version of the 500px engineering blog. A lot has changed since it was originally posted on Feb 1, 2015. In the future posts, we will be covering how our image classification solution has evolved and what other interesting machine learning projects we have. TLDR : This post provides an overview of how to perform large scale image classification using Hadoop Streaming. First, we look at each component individually and identify things that need to be taken into account for the final integration. Then we go over how to ‘glue’ all pieces together to produce the desired results. Till recently image search and discovery at 500px was based mainly on meta information provided by users which included tags , title and description . It’s obvious that the quality of such search and discovery directly depends on how well users describe their photos. Since this is such a tedious process, most photos are simply left untagged or have inconsistent keywords. The recent update from Facebook illustrates the scale of the problem: ~2 Billion photos are shared on Facebook per day! Luckily, due to the recent advancements in Deep Neural Networks (NN) and their efficient implementations using GPUs, it is now possible to perform automatic image classification (tagging) with nearly human performance . Getting close to human capabilities or even beating them does not mean we’ve achieved real Artificial Intelligence (AI). The deep NN algorithms do not ‘understand’ content of images nor the concepts that describe them. Instead, they learn invariant representation of patterns (features) found in images and create a mapping function that maps pixels of an image to a combination of patterns found in the image . This mapping function is essentially a specially crafted mathematical function that takes a set of numbers (pixels) and outputs another set of numbers (features). Despite not being real AI, deep NN algorithms applied to image classification tasks have been significantly improved over the last few years. This is best illustrated by looking at the results of the ImageNet competition . Additional advantages of using NN algorithms include: calculating keyword weights in order to better control search results calculating similarity between images reverse image search extracting aesthetic qualities of images There are many open source projects like Theano , Torch , Caffe , DL4J , Toronto DeepNet that provide tools for implementing NNs. For our approach we chose Caffe because it was one of the easiest to start with and also satisfied our requirements: availability of pre-trained models which is important, considering it takes weeks to train a model and requires large training set like ImageNet. Python wrapper so we can use a lot of existing machine learning libraries like scikit-learn in the same environment active community, decent documentation and examples So we have the library and models for automatic image recognition. Let’s look at the steps we need to perform to classify one image: download an image resize and crop to fit the model’s input dimensions convert RGB pixels into an array of real numbers call Caffe’s APIs extract predicted labels and features save results When operating with hundreds of millions of images, just the download step may take weeks to complete on a single machine. A faster approach would include performing these steps in parallel. We achieved this by parallelizing execution both within one machine using GPU and multiprocessing, and across many machines relying on Hadoop. Hadoop is a framework for distributed processing and storage. Normally, it requires to write Java code to run your jobs. Conveniently, it also offers Hadoop Streaming that allows to use any scripting language to create custom jobs. Hadoop Streaming is essentially a job that will run your script in a child process. stdin and stdout are used to pass data to and from your code. stderr is used for logging and debugging. Hadoop Streaming assumes that your input and output data is textual and has one item per line. This means you cannot simply point Hadoop to a folder containing images. It will try to read each image as a text file and will eventually fail. The big challenge with Hadoop is installing and configuring the cluster. It could take a full-time devops position just to do that. Our shortcut to the problem was to use Elastic MapReduce (EMR) which is Amazon’s web service that provides APIs to configure a Hadoop cluster and run jobs. Another advantage of EMR is seamless integration with Amazon S3. Essentially, when running an EMR job you specify where to read data from and save results to on S3. The only limitation is the input param needs to point to a file or a folder with files on S3, it will not traverse subfolders. So the input to EMR should be either a text file or a folder with text files on S3. Since we are dealing with a very large collection of photos, we generated a list of files, where each file contained around 1000 records (lines) in the following format: By default, EMR outputs data as one large list split among many part-xxxxx files. These split files could be very large and are not aligned with new line boundaries which make parsing them hard. For our classification pipeline we saved predictions for each image as a separate JSON file. This is why we explicitly specified destination location in the input for Hadoop so each worker can save results for each image separately. We performed image classification on GPU-based EC2 instances. To our surprise we found out that these g2.2xlarge instances are highly unreliable. The most common problem we observed was the failures of the CUDA driver to find the GPU card. From Hadoop side it looked like a stuck worker whose containers were blocked by the GPU driver. The workaround was to kill the failed instance and let EMR to bring a new node and restore HDFS cluster. There were also problems resulting in failed jobs that Hadoop/EMR could not handle. Those included exceeding memory configuration limits, running out of disk space, S3 connectivity issues, bugs in the code and so on. This is why we needed ability to resume long running jobs that failed. Our approach was to write photo_id into Hadoop’s output for successfully processed images. In case of a failed job, we would parse part-xxxxx files and exclude successfully processed photo ids from the input files. Before we can schedule a job on EMR we first need to bootstrap the cluster and then configure Hadoop. Amazon provides command line tools to perform both actions. However, we took an easier path and used mrjob which is a Python library that provides abstractions over EMR APIs and makes developing and testing of MapReduce jobs very easy. It’s worth mentioning that mrjob is not the slowest Python framework for Hadoop as opposed to the blog post by Cloudera. Instead, it is as fast as any Hadoop Streaming job can get. It just does more than other frameworks by offering protocols which serialize and deserialize raw text into native Python objects. Of course, this feature can be disabled in the code. To bootstrap and configure EMR via mrjob you create a YAML file. This is what our classifier_job.conf looked like: It’s important to highlight the memory specific settings both for YARN (Hadoop’s resource manager) and MapReduce. One map operation required around 2.7GB of RAM: 1.7GB to keep Caffe model in memory + 1GB for downloading and preparing 5 images in parallel. This is why we allocated 3GB per one YARN container. Each g2.2xlarge worker had 15GB RAM out of which 12GB was allocated to MapReduce and 3GB was left to the OS. This gave us 4 containers per node or 80 containers for 20 nodes cluster. The bash script classifier_job.sh that installed Numpy, Caffe and other libraries would run on the master and worker nodes. To make sure it did not fail on the master node without GPU we had the following switch: The image classification job consisted of a single map operation that included downloading images, preparing batch data, performing classification on GPU and saving results. To justify overhead of loading data in/from GPU, images needed to be processed in batches. Unfortunately, mrjob does not support batch processing out of the box. The workaround was to override run_mapper method and manually control reading lines from stdin . The batch size was set to 50 images based on the available GPU memory of g2.2xlarge instance. Below are the two main methods that handled batch operation: run_mapper for preparing batch data and process_batch for batch classification task. Among all the major steps of process_batch , downloading images was the slowest step: download_preprocess_for_caffe_batch was parallelized to perform downloading using 5 processes. Normally this download step would be a good candidate for a separate map operation. However, the limitation here is to store all resized images in HDFS (Hadoop Distributed File System), which would require to have a very large EMR cluster. Alternatively, it is possible to increase the number of parallel processes from 5 to 10, for example. But doing so would require to increase the map task’s memory footprint and as the result to reduce number of containers per node (e.g. from 4 containers to 3). Here are the stats for the final run: 20 nodes EMR cluster 80 containers (4 containers per node) ~600 images/s (30 images/s per node) ~80% average CPU load across the cluster several days to complete Just a compilation of random tips we found useful while working on this project. Use EMR job flow to start a persistent cluster so you can avoid provisioning and boostraping every time you want to run a job. An example of possible workflow using mrjob: Use EC2 spot instances to save EMR costs by specifying the bid price in the config file: Use Hadoop’s counters and stderr to output debug and profiling info. For example: ssh to EMR cluster and run: Specify S3 location where to store EMR logs in the mrjob’s config file. These logs contain when and where each map/reduce attempt is started and whether it succeeded or failed. For the failed tasks you can look at the corresponding container logs that contain your code’s stderr output and other possible exceptions. We showed how to perform a large scale image classification task using caffe , Elastic MapReduce (EMR) and mrjob . In our approach we optimized for simplicity and speed of development. The combination of Hadoop Streaming and machine learning libraries available in Python opens up interesting opportunities for large scale data processing and analysis. Welcome to the 500px Engineering Blog! 11 2 Hadoop Machine Learning AWS 11 claps 11 2 Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-03-28"},
{"website": "500px", "title": "greedo layout for ios", "author": ["David Jeremy"], "link": "https://developers.500px.com/greedo-layout-for-ios-4a216a4abbe1", "abstract": "About a year ago, we at 500px decided it was time for a rewrite and a redesign of our mobile apps. One of the first decisions was to let go of the cropped square thumbnails. Solutions to display images in a grid were limited: 1. full aspect ratio images that fit in a square 2. fixed width images (layout made popular by Pinterest) 3. fixed height images Solution 1 doesn’t show the image big enough and has a lot of white space. It was quickly discarded. Solution 2 presents portrait oriented images in a much bigger way that landscape images. At 500px , the most common aspect ration is 3:2 (which is the standard 35mm film, or DSLR aspect ratio). Furthermore, we wanted to keep an idea of a timeline. Multiple columns of fixed width images doesn’t have a clear representation of time. So, solution 3 it is. Full aspect ratio, fixed height images, variable row height. This class was originally written in Swift back when it was first available. It was actually my first Swift project. Given how Apple updated Swift with breaking changes, at a much faster rate than we expected, we grew tired of updating the code and Jerome ( http://www.jeromescheer.com ) ended up rewriting it in Objective-C. We considered rewriting back in Swift 2.0 now that the language is more stable, but we’d rather get this code out in the open now, in its current, tested, production state. Pretty much everywhere :) User profiles, Popular photos, Search, our custom UI for picking photos from the device library. The home feed is the only view that doesn’t display images that way (single column fixed width). Short answer: it is not. Greedo has to be used in conjunction to the default UICollectionViewFlowLayout. A full aspect ratio grid of photos is technically a “Flow layout” where all cells are next to each other, and cells wrap to a new row when there’s not enough space. Greedo ’s job is to simplify how the size of each cell is computed so that you don’t have to do the math. I recommend using Cocoapods to install Greedo . To integrate GreedoLayout into your Xcode project using CocoaPods, specify it in your Podfile: Then, run the following command: Setup your UICollectionView with a UICollectionViewFlowLayout . When the collectionViewFlowLayout dataSource inquires about the size of your cells, ask Greedo and return the value :) Done! If you support multiple orientation, or your collection view changes attributes, reset the size cache of Greedo. Here is a quick explanation of this simple algorithm: Get the original pixel size of the image Scale it so that it fits on the collection view width while keeping its original aspect ratio Is it too tall? (determined with a maximum height property) If it’s not, then the job’s done. That’s your cell size. If it is too tall, ask for the next image size. Put those two together on one row, at their original aspect ratio. Are they still too tall? etc… You get the idea. Greedo asks its delegate for the image sizes as it needs them and keeps a cache of all sizes computed for performance purposes. Setting up Greedo should be easier if it was an actual custom collection view layout, so that’s the obvious improvement we can make. Swift is now definitely mature enough, and all new iOS code written at 500px is in Swift. Greedo should probably be rewritten in Swift as well at some point. I’d also like to add options to keep the layout when the collection view bounds change (and just make the images bigger or smaller instead or recomputing the layout). Greedo lays out the image in the order they are provided, which sometimes is not the most pleasing layout (panoramas are sometimes really small for example). It could be interesting to explore alternatives. To make the layout less monotonous, having an option to make a few images “featured” (bigger) would be interesting as well. Most of our internal libraries, in the 500px mobile team, have a “human name”. Our message banner library is called Bruce (Yes, like the Hulk ), our Android Data Consistency library is called Jackie . It made sense to give our “PhotoSizeCalculator” a friendlier name :) With the release of Star Wars, our “Grid” layout could not be named anything different than Greedo. (Well, it could, but that wouldn’t be as fun, would it?) Check out Greedo on GitHub . Submit your pull requests, or just come and work for us, we’re hiring ! Julian Villella , a fellow 500px mobile developer, wrote a similar library for Android. I’m sure it’s nice too ;) Check it out here . Welcome to the 500px Engineering Blog! 9 2 iOS Github Design 9 claps 9 2 Written by Photographer and iOS Developer. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Photographer and iOS Developer. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-03-01"},
{"website": "500px", "title": "consolidating string resources across android ios", "author": ["Jessica Wu"], "link": "https://developers.500px.com/consolidating-string-resources-across-android-ios-468932b84ccf", "abstract": "Hey there! My name is Jessica and I’m a Software Engineering student from the University of Waterloo. I’m currently on a coop term at 500px as a part of their mobile team. I contribute to the 500px Android application ( try it out! ). 500px builds mobile applications on two platforms: Android and iOS. Both of these platforms have different style guides and general flows, but the strings they use overlap significantly. We wanted to simplify and unify these to reduce duplication of effort, make app localization easier, and reduce translation costs. Consolidating string resources across the two platforms isn’t as straightforward as it may seem. Looking deeper, Android and iOS have very different string resource formats. Consider typical string definitions: Android strings.xml: iOS Localizable.strings: Beyond superficial differences, there are also more complicated differences, such as formatting strings with variables (used primarily for pluralization and gender rules), HTML styling, ways of handling pluralization, etc. As you can see, there are going to be some complications already. “This must be a common problem, right?” we thought, and so we scoured the internet for help…and we found Mobiata’s Twine . Twine is a set of Ruby scripts that, through the command line, help manage strings and their translations. This is done through storing them all in a human-readable master text file, which is then used to import and export localization files. It supports a whopping 6 different localized string file formats, this was too good to be true! This was 4 too many for our purposes, but it had the two platforms we were interested in: Android and iOS. However, digging deeper, we found out that Twine did not support plural string resource conversions ( for various reasons ). Which is totally understandable once you look at how differently they are done: Android, included in strings.xml: iOS Localizable.stringsdict (brace yourselves): Another issue arose: iOS string keys don’t follow the snake_case naming convention like Android does [1], which means if we did unify our strings into some sort of huge master file, the copy that overlapped would not be squashed together when using Twine. Refactor all of our iOS strings to abide by Android’s convention of having a snake_case key and maintain this standard from here on out (with the help of Lin autocomplete, this wouldn’t be too painful for the future [2]) Patch Twine to consume plural string resources and spit them out in their respective formats Run all of our localization files into Twine and generate a master string file, which can then be used to create future localization files. This could also be sent to translation services to integrate new languages into our applications! Generate all our string files from this master file And…we did it! Our fork of Twine with all the plural string goodness can be found on GitHub . Running Twine at the root of the project will setup your Ruby library path properly. Now that you have Twine, there are 2 major commands: This command will create your localized string files in the directory that you specify. This command will fill your master file from the directory that you specify. If you’re curious about trying it out there are more explicit instructions, details about the format of the master file, and commands that can be found in the README . So, now that we have this slick master file, how do we add new strings on each of the platforms? Adding to that, how do we modify existing strings and differentiate these changes from string additions? One possible solution: Create a git hook that will run every time the developer commits a change to strings.xml or Localizable.strings, which will in turn, run that file through Twine and send an update to our master file The master file could be stored in another git repo so we can keep track of changes This is our first iteration on trying to streamline this tedious process. With this thorn in every multi-platform application company’s side, we are welcome to (and asking for!) suggestions, feedback, and more insight on your workflow. Feel free to comment below on your processes! [1] Localizable.strings is generated using the genstrings script on the codebase, which sifts through all the uses of NSLocalizedString and produces a human-readable text file and the default behaviour is having the key match the value of the string. [2] Unfortunately, Lin doesn’t work in XCode 8 because Apple deprecated the XCode plugin APIs it uses. Welcome to the 500px Engineering Blog! 117 8 iOS Localization Android iOS App Development Android App Development 117 claps 117 8 Written by Dev, photog, plastic disc thrower. instagram.com/wwwjess Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Dev, photog, plastic disc thrower. instagram.com/wwwjess Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-02"},
{"website": "500px", "title": "missing autocompletion for swift in xcode", "author": ["Akbar Nurlybayev"], "link": "https://developers.500px.com/missing-autocompletion-for-swift-in-xcode-97656a5d95fb", "abstract": "In short, check your Bridging-Header.h. In our case all we had to do was change to Welcome to the 500px Engineering Blog! 4 iOS Xcode Swift 4 claps 4 Written by Father. Husband. Software Engineering Manager at TradeRev. We are hiring! Toronto, 🇨🇦 Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Father. Husband. Software Engineering Manager at TradeRev. We are hiring! Toronto, 🇨🇦 Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-02-26"},
{"website": "500px", "title": "chatops", "author": "Unknown", "link": "https://developers.500px.com/chatops-f07c15d7749c", "abstract": "At 500px, we have a complex architecture composed of a number of microservices, plus a central monolithic Rails application. We use several tools to deploy our apps and services, depending on what technology each is based on. Trying to remember how to interact with N different apps and microservices across X number of different languages and Y number of different frameworks sucks. Remembering one common set of commands that can interact with any part of our infrastructure, regardless of how that infrastructure was built, is much easier. Over the last year or so we’ve been moving away from a single, gigantic 500px Rails app to a much smaller 500px Rails app supported by a variety of microservices (if you’re interesting in learning a little more about our stack, you can check out this blog post ). Back when we only had to worry about one Rails app, deploying was pretty simple: Easy. But once we started spliting out pieces of the monolith into services, introducing new technologies, new frameworks, and new deployment tools, suddenly we had to start remembering a lot more stuff. It was no longer quite as easy for new developers to join the team, write some code, and deploy it to our users, and new developers often weren’t deploying anything themselves at all . Deployment into production is serious business. We want anyone to be able to deploy, but also we need to make sure that when somebody pushes out some changes, the rest of the team knows what’s up. Nobody should be surprised that a pull request went out. It seemed like we had few problems that could be solved at the same time. Why not have one set of simple commands to deploy any part of our infrastructure while also keeping track of what everybody’s been up to? Enter ChatOps . The idea, for the uninitiated, is that you issue commands right in your company’s shared chat, where everyone can see what you’re doing, and a bot of some kind interprets those commands and does something real with them. In our case, we’re using Slack and Hubot . One of Hubot’s many lovely features is the ability to personalize your robot a little bit by giving it a name. We named ours after BMO , the friendly robot/videogame console from Adventure Time. Apart from responding to silly commands with sass and Adventure Time references, BMO provides us with a simple set of commands. These include things like: bmo deploy someapp : Deploy some application bmo restart someapp : Restart some application bmo status someapp : Retrieve the status of an application bmo update yourself : BMO deploys itself bmo pager ack : Acknowledges an automated alert from PagerDuty bmo pug bomb n : Posts n number of pictures of pugs into Slack, great for spamming your coworkers My favourite command is where BMO deploys itself. Some of these come from Hubot’s rich ecosystem of user-made plugins, and some of these we’ve built ourselves. For example, pug bomb comes with the base Hubot code, but deploy is something we slapped together in-house. This solves part of our problem: it provides us with a consistent user interface that is easy to use and which has a very low barrier to entry. It also allows us to delegate sensitive deployment credentials to BMO. Users don’t need to ask for permission to access all the different systems they want to deploy to. We can give that access to BMO instead, which removes a gatekeeper from the critical path of the deployment workflow, and also simplifies access control in our organization. Using Chatops means that developers can deploy without having direct access to production. The other part of the problem is what sits behind the user interface. Many of our developers are polyglots and we try our best to deploy microservices using the most appropriate language or framework for the job. Since we deploy many languages, BMO needs to be able to use the tooling or framework that is most appropriate for that ecosystem. We might use Bash, Ansible, Capistrano, Fabric, or something else entirely. We needed a way for Hubot to be able to execute command line tools like a human would. As an answer to these questions, we built BMO a little companion library to help her out. We dubbed this library Gunter , after the mischievous penguin from Adventure Time. Gunter’s a pretty simple penguin: rather than building a new deployment tool to replace all of our existing deployment tools, we decided to just wrap them all in an abstraction layer. At its core, Gunter is a simple bash task runner; he runs arbitrary shell commands on arbitrary servers. You define tasks in Gunter, giving it a shortname you can later use to run the task. Task definitions generally look something like: These tasks include a list of bash commands to run, a host to run these commands on (either localhost or some server), and a working directory to run the commands in. You can include variables in the command list that’ll be filled in at run time to make the tasks a little more flexible. You can also include defaults for these variables in your task definition, and authentication credentials for the server you want to run the task on, such as a username, password, path to a private key file, and a port number to connect to. Once we’ve defined tasks, and loaded them into memory with gunter.load() , we can execute them with gunter.exec() . When we take these two pieces, BMO and Gunter, and combine them together, we get a pretty powerful combination. We have BMO connected to chat and listening for someone to issue a command to it, and Gunter interprets these commands as an actual set of shell commands. Together, these two tools give us the ability to do basically anything we could in terminal, in chat instead. ChatOps is still very young at 500px, and appropriately rough around the edges. Gunter is an immature library, and still unstable, but its come a long way. But our hope (or at least my hope) is that through ChatOps, we can build an organization where everyone is easily empowered, everything that goes on in the company is transparent, and everyone’s dreams come true. Originally published at developers.500px.com on June 17, 2015. Welcome to the 500px Engineering Blog! 7 Chatops Deployment DevOps 7 claps 7 Written by Developer and mercenary for hire. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Developer and mercenary for hire. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-03-04"},
{"website": "500px", "title": "how 500px serves up over 500tb of high res photos", "author": ["Paul Liu"], "link": "https://developers.500px.com/how-500px-serves-up-over-500tb-of-high-res-photos-fa81a376c594", "abstract": "Note: this article was originally posted on stackshare.io . 500px is an online community for premium photography. Millions of users all over the world share, discover, sell and buy the most beautiful images. We value design, simplicity in the code, and getting stuff done. I’m a DevOps Developer at 500px working on the Platform Team. I work on stuff like backend systems, monitoring, configuration management, and deployment and automation. Prior to joining 500px, I spent many years as a sysadmin working in the insurance industry. The 500px Engineering team is split into four groups: the Web team, the Mobile team, the QA/Release Engineering team and my team, the Platform team, which is a combined team that handles building our API and backend services as well as technical operations and infrastructure management. Our teams are highly cross functional and boundaries are fairly loose, so engineers wind up moving around a lot between teams to work on specific projects. This helps us spread knowledge and prevent siloing. There is also a very tight communications loop between engineering, product, design and the customer excellence teams, which helps to keep us honest, agile, and focused on delivering the right things. The architecture of 500px can be thought of as a large Ruby on Rails monolith surrounded by a constellation of microservices. The Rails monolith serves the main 500px web application and the 500px API, which powers the web app, mobile apps and all our third party API users. The various microservices provide specific functionality of the platform to the monolith, and also serve some API endpoints directly. The Rails monolith is a fairly typical stack: the App and API servers serve requests with Unicorn, fronted by Nginx. We have clusters of these Rails servers running behind either HAProxy or LVS load balancers, and the monolith’s primary datastores are MySQL, MongoDB, Redis and Memcached. We also have a bunch of Sidekiq servers for background task processing. We currently host all of this on bare metal servers in a datacenter. The microservices are a bit more interesting. We have about ten of them at the moment, each centered around providing an isolated and distinct business capability to the platform. Some of our microservices are: Search related services, built on Elasticsearch Content ingestion services, in front of S3 User feeds and activity streams, built on Roshi and AWS Kinesis A dynamic image resizing and watermarking service Specialized API frontends for our web and mobile applications We run our microservices in Amazon EC2 and in our datacenter environment. They are mostly written in Go, though there are a couple outliers which use NodeJS or Sinatra. However, regardless of the language in use, we try to make all of our microservices good 12-factor apps, which helps to reduce the complexity of deployments and configuration management. All of our services are behind an HAProxy or an ELB. The microservices pattern is great because it allows us to abstract away complex behaviour and domain-specific knowledge behind APIs and move it out of the monolith. Front-end product teams consuming these services only have to know about the service’s API, and service maintainers are free to change anything about their service (as long as they maintain that API). For example, you can query the search service without having to know a thing about Elasticsearch. This flexibility has proven to be extremely powerful for us as we evolve our platform because it lets us try out new technologies and new techniques in a safe and isolated way. If you are curious about implementing microservices yourself, former 500pxer and overall rad dude Paul Osman gave a great talk at QConSF last year about how and why we did it. My face is on one of the slides, but that is only one of the many reasons why this talk is awesome. Probably the most interesting set of microservices we run at 500px are the ones having to do with serving images and image processing. Every month we ingest millions of high resolution photos from our community and we serve hundreds of terabytes of image traffic from our primary CDN, Edgecast. Last month we did about 569 TB of total data transfer, with 95th percentile bandwidth of about 2308 Mbps. People really like looking at cool pictures of stuff! To ingest and serve all these images we run a set of three microservices in EC2, all built around S3, which is where we store all of our images. All three of these services are written in Go. We really like using Go for these cases because it allows us to write small, fast, and concurrent services, which means we can host them on fewer machines and keep our hosting costs under control. The first microservice users encounter when they upload a photo is one we call the Media Service. The Media Service is fairly simple: it accepts the user’s upload, does some housekeeping stuff, persists to S3, and then finally enqueues a task into RabbitMQ for further processing. Next, consuming those tasks off of RabbitMQ is another service called (creatively) the Converter Service. The Converter Service downloads the original image from S3, does a bunch of image processing to generate various-sized thumbnails, and then saves these static conversions back to S3. We then use these conversions in lots of places around our site and for our mobile apps. Probably so far this isn’t very surprising for a photo-sharing website, and, for awhile, these two services did everything we needed — we simply set the S3 bucket containing the resulting thumbnails as the origin for our CDN. However, as the site continued to grow, we found this solution was pretty costly and space inefficient, as well as not very flexible when new products required new sizes. To solve this problem, we recently built what we creatively call our Resizer Service (yes, we tend to choose descriptive names for these things). This new service now acts as the CDN origin and dynamically generates any size or format of image we need using the S3 original. It can also watermark the image with a logo and apply photographer attribution, which is reassuring to our community. The Resizer Service is fairly high throughput, with the cluster handling about 1000 requests per second during peak times. Doing all this resizing and watermarking is pretty compute-intensive, so it’s a bit of a challenge to keep response times reasonable when the load is high. We’ve worked really hard on this problem, and at peak traffic we’re able to maintain a 95th percentile response time that is below 180 ms. We do this through the use of a really cool, really fast image processing library called VIPS , aggressive caching, and by optimizing like crazy. Outside of peak hours, we can usually get below 150 ms. And we’re not done with this problem yet! There are almost certainly more optimizations to be found, and we hope to keep pushing those response times down further and further in the future. We use Github and practice continuous integration for all of our primary codebases. For the Rails monolith, we use Semaphore and Code Climate . We use a standard rspec setup for unit testing, and a smaller set of Capybara/Selenium tests for integration testing. Fellow 500pxer and professional cool guy Devon Noel de Tilly has written at length about how we use those tools, so I won’t try to out do him — just go check it out. For our Go microservices, we use Travis CI to run tests and to create Debian packages as build artifacts. Travis uploads these packages to S3, and then another system pulls them down, signs them, and imports them into our private Apt repository. We use FPM to create packages, and Aptly to manage our repos. Lately, though, I’ve been trying out packagecloud.io and I really like it so far, so we may be changing how we do this in the near future. For deployments, we use a combination of tools. At the lowest level we use Ansible and Capistrano for deploys and Chef for configuration management. At a higher level, we’ve really embraced chatops at 500px, so we’ve scripted the use of those tools into our beloved and loyal Hubot friend, BMO . Anyone at 500px can easily deploy the site or a microservice with a simple chat message like bmo deploy <this thing>. BMO goes out, deploys the thing, and then posts a log back into the chat. It’s a simple, easy mechanism that has done wonders to increase visibility and reduce complexity around deploys. We use Slack, which is where you interact with BMO, and it makes everything really nicely searchable. If you want to find a log or if you’ve forgotten how to do something, all you have to do is search the chat. Magical. We monitor everything with New Relic , Datadog , ELK stack ( Elasticsearch, Logstash and Kibana ), and good old Nagios. We send all our emails with Mandrill and Mailchimp and we process payments with Stripe and Paypal . To help us make decisions, we use Amazon’s Elastic MapReduce and Redshift , as well as Periscope.io . We use Slack , Asana , Everhour , and Google Apps to keep everyone in sync. And when things go wrong, we’ve got Pagerduty and Statuspage.io to help us out and to communicate with our users. Right now I’m working on experimenting with running our microservice constellation in Docker containers for local dev (docker-compose up), with an eye to run them in production in the future. We’ve got a CI pipeline working with Travis and Docker Hub , and I’m really excited by the potential of cloud container services like Joyent Triton and Amazon ECS . As we build more and more microservices and expand the stack, we’re also looking at service discovery tools like Consul and task frameworks like Mesos to make our system scale harder better and faster. We’re expanding quickly and hiring all kinds of positions. We’re looking for DevOps types, Backend and Frontend developers, Mobile developers (both Android and IOS), UX designers, and salespeople. We build cool stuff, we’re passionate, and we’re flying by the seat of our pants at breakneck speed, building the best thing we know how to make. If you like doing awesome cool stuff and you aren’t afraid to get your hands dirty, come join us . Welcome to the 500px Engineering Blog! 109 DevOps Microservices Startups 109 claps 109 Written by Awful. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Awful. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-03-02"},
{"website": "500px", "title": "developing with docker at 500px part one", "author": ["Paul Liu"], "link": "https://developers.500px.com/developing-with-docker-at-500px-part-one-1380df7ccb4", "abstract": "This article is part one of two. Note: this article was originally posted on a previous version of the 500px engineering blog. The Docker ecosystem has matured greatly since this was written and a lot of our local dev problems have been resolved or alleviated through better tooling. This and other topics will be discussed in the next installment of this series, which will be published here. 500px ❤ Docker. Computers are hard. Distributed systems are harder. That’s a popular thing people say, right? Well, it’s true. The 500px system architecture can be thought of as a large Rails monolith surrounded by a constellation of Go and Ruby microservices . The monolith houses our main web app and the 500px API, and the microservices provide ancillary functionality like image processing, search services, user feeds and activity streams, push notifications and other background processing. While we love the microservices pattern because it offers us lots of technical freedom and flexibility, it can make development pretty painful sometimes. We’re in the process of trying to make local development a bit easier with Docker, so we thought we’d blog a bit about our journey so far. This article is the first in a series about how we are planning to address the problem, the challenges involved, and the (hopefully successful) results of our experiment. Please note, this article assumes a passing familiarity with Docker , Docker Compose and continuous delivery concepts. At the time of this writing, all development of the 500px monolith is done locally. To get the monolith fully up and running, developers need to run an instance of every microservice we have and all of their related backend datastores. This involves installing lots of libraries, running and compiling tons of Go programs, and setting up a bunch of configuration heavy things like MySQL, MongoDB, RabbitMQ, Elasticsearch and Roshi (a Redis backed CRDT database for time series event storage). All of this complexity is the cause of a lot of cognitive overhead and headaches for the team, and it makes onboarding new developers sometimes more difficult than it should be. Things move pretty fast around here and we deploy frequently, which means debugging a local dev stack can get pretty hairy. Depending on the context, a simple question like “why doesn’t this upload work” may have a simple answer, but it also runs the chance of having an incredibly complicated answer that requires deep knowledge about the stack, 45 minutes, a whiteboard, Charles , pprof , tcpdump, and maybe a couple beers. The situation is generally manageable at the moment, but as we scale our team in every direction and across multiple offices, things are starting to get out of hand. It’s time to step up our local dev game. The best solution to this problem is, we think, Docker. Instead of asking developers to compile and manage their own versions of microservices, our plan is to containerize everything we can and then ship the whole stack as images to our dev teams. To automate setting up, updating and tearing down these images, we’re using Docker Compose (and, probably, a Bash script or two). If we can successfully containerize the backend stack and make it easy to deploy, working on the monolith should become much easier. The ideal implementation is such that a brand new developer can show up at 500px, unwrap their shiny new computer and, with no context and minimal instruction, simply install Docker and get the entire stack up and running within an hour or so. Ideally, they will also push a change and deploy the site that same day, but we’ve already got that part covered . And when some part of the system changes the next day, that same developer should be able to simply pull down an updated container and keep on truckin’. It’s a simple idea, but a powerful one. The first step of this project is one we have already taken and are pretty happy with: building containers and integrating Docker into our CI pipeline. Our Go apps are all pretty straightforward, so containerizing them was relatively easy. Since we already package everything as Debian packages, most of our dockerfiles just look something like this: The resultant container is totally stateless, and service configuration is done though command line arguments or shell environment variables. Since we are going to use Docker Compose, we can set up that stuff in docker-compose.yml or the user’s shell environment. We host all our Debian packages in private repositories which require API keys to access. Unfortunately, since Docker doesn’t support build-time argument passing, we’ve had to commit repo access keys to our dockerfiles and thus, to version control. This is a huge bummer and it makes us very sad, but there wasn’t any way around it. Secrets in Docker are, in general, pretty tough to deal with at the moment, but we have hope that this will get easier in future. After writing some dockerfiles, next up was integrating Docker image building into our continuous integration pipeline. Our pipeline consists of four services: Github for hosting code, Travis CI for CI, Packagecloud.io for hosting packages, and Docker Hub for hosting Docker images. We like all of these tools and think they are rad. From keyboard to container, the pipeline looks like this: A service maintainer writes some code, pushes to Github, and merges their branch to master Travis runs tests, builds master, and creates a .deb Travis uploads the .deb to Packagecloud Travis then does a POST to activate a Docker Hub build webhook Docker Hub builds the image, using the .deb that we just uploaded A service consumer runs docker pull and downloads the new image Easy as 123… 456. Curious readers might be wondering why we didn’t simply use Docker Hub’s Automated Build feature. The reason we set up our pipeline this way was to avoid building our projects from source, inside the Docker image. Building the project in the image is simpler, but would require installing Go and other build time dependencies, which results in a slow build and a fat gross image. Our Debian packages on the other hand contain only a compiled Go binary and whatever libs we need to deploy. By not installing build time cruft, we can create minimal images that build fast and are super lean. And since we install the same Debian packages in production, creating our images this way also ensures that developers get to work with something that is relatively close to reality. Building our images and setting up CI was a pretty straightforward process, but we hit a couple snags while figuring out the user side. To make getting the stack up and running as simple as possible, we wanted to use Docker Compose with only a single large docker-compose.yml file that starts and stops everything. Since “everything” consists of more than a dozen containers, this wound up being a little bit tricky to implement. The issue we ran into is that although Compose is smart enough to start linked containers in the right order, it doesn’t have any concept of health checking to determine when the applications within those containers are ready to accept connections. This can lead to issues like this: Suppose we have 3 containers: Service A, Service B, and Database Service A requires Database to be up and running in order to initialize and start Service B requires Service A to be running in order to initialize and start Now suppose that Database has to run a migration before starting up. Uh oh. When Compose starts the Database container, the process will start and begin its migration. It will take a minute before the DB is ready to accept connections. However, since Compose doesn’t know that, it just moves on to start Service A immediately after the Database instance starts. This results in Service A failing to start, because it can’t connect to the DB, which is still running its migration. Service B then tries to come up and fails because Service A hasn’t started. Add in a dozen more containers and pretty soon what you’ve got is a hot mess that is tough to debug . Luckily, it’s easy enough to work around this problem with startup scripts or by using a linked container that blocks until things are ready. This is the path we took (along with changing the init behaviour of some of our services), but it’s easy to see things getting complicated if more than just simple health checks are required. Another solution we are thinking about is ditching Compose completely in favour of Ansible . While using Ansible would provide more robust controls, we prefer to stay within the Docker ecosystem if we can. Another issue we ran into was connectivity from containers to services running on localhost. At 500px we mostly use OSX, which means people are running Docker in a VirtualBox VM via Boot2Docker (we haven’t tried out Docker Toolbox yet, but definitely plan to). Although the Boot2Docker OSX installer makes it easy to install VirtualBox and set up inbound port forwarding (that is, connecting from localhost to a container), it is somewhat less obvious how to get outbound connections working from a container to localhost (for example, connecting from inside a container to an API server running locally). It turns out that it’s actually really easy to configure this using PF port forwarding rules, like this: These rules forward traffic from vboxnet0, the VirtualBox adaptor, to 127.0.0.1. Containers can then access localhost via the IP of the vboxnet0 interface on the OSX side and packets will be routed appropriately (if you are following along, you can find this IP by running “boot2docker config | grep -i hostip”). To persist PF rule changes across reboots, set up a pf.rules file and a pf.plist file as described here and that’ll do it. But this is only really necessary if you actually need to route packets to localhost. We struggled with this for a little bit, but in the end chose an even simpler solution than the one above: we just changed our applications to listen on the Virtualbox adapter instead of localhost. Sometimes, less is more. So that’s where we are so far. We’ve got containers, CI and a working docker-compose.yml that runs our stack. All we’ve got to do now is rollout and see what happens. As we start using Docker more and more in our daily workflows, we’ll probably run into a few more issues, but we’re confident in our technology choices and our ability to work around whatever may come up. We hope this article is interesting and that it is helpful as you plan your own Docker deployments. Next time around, we’ll blog about the things that went right, the things that went wrong, and what we did to make stuff work. If you have questions, or if you think what we’re doing is cool, or even (especially) if you think what we are doing is dumb and batshit insane, drop me a line and let’s talk. As always, we are hiring . Welcome to the 500px Engineering Blog! 5 DevOps Docker Development 5 claps 5 Written by Awful. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Written by Awful. Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-03-02"}
]