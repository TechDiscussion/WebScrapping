[
{"website": "Codemancers", "title": "rails reloading in dev mode", "author": ["Yuva"], "link": "https://crypt.codemancers.com/posts/2013-10-03-rails-reloading-in-dev-mode/", "abstract": "How rails reloads your source code in development mode? Written by Yuva on October 3, 2013; tagged under rails , code-walkthrough We all know Rails has this feature, which reloads source code in development mode\neverytime a request hits server. Starting from version 3.2.0, it introduced faster\ndev mode where it reloads application code when the code changes, and not on every request. This blog will talk about important parts of Rails source code which helps in\nachieving faster dev mode. ActiveSupport::FileUpdateChecker : Checks whether any of the application code\nfiles are changed. Activesupport::Dependencies : Actual module responsible for reloading classes. ActionDispatch::Reloader : The middleware which helps in reloading classes. reload_classes_only_on_change : As name states, this configuration option tell\nRails to enable/disable code for re-loading classes only if the code changes. Lets go through them one by one. ActiveSupport::FileUpdateChecker This class helps in checking whether application code is updated or not. It exposes\n2 methods: updated? and execute . The former tells whether files are updated or\nnot, while latter executes a block given by updating timestamp of latest changed\nfile. The code can be found here .\nHow file checker checks whether a file is updated or not depends on the modified\ntime of the file. There is a small function max_time which returns timestamp for\nrecently modified path (most recent one) class ActiveSupport :: FileUpdateChecker # NOTE: Removed some code to reflect the logic def updated? current_updated_at = updated_at ( current_watched ) if @last_update_at < current_updated_at @updated_at = current_updated_at true else false end end # Executes the given block and updates the timestamp. def execute @last_update_at = updated_at ( @last_watched ) @block . call end def updated_at ( paths ) @updated_at || max_mtime ( paths ) || Time . at ( 0 ) end # This method returns the maximum mtime of the files in +paths+ def max_mtime ( paths ) paths . map { | path | File . mtime ( path )} . max end end ActiveSupport::Dependencies This module consists of the core mechanism to load classes, by following Rails\nnaming conventions. It uses const_missing to catch missing classes, and then\nsearches in autoload_paths to load those missing classes. The code can be\nfound here . module Dependencies def const_missing ( const_name ) from_mod = anonymous? ? :: Object : self Dependencies . load_missing_constant ( from_mod , const_name ) end def load_missing_constant ( from_mod , const_name ) qualified_name = qualified_name_for ( from_mod , const_name ) path_suffix = qualified_name . underscore file_path = search_for_file ( path_suffix ) if file_path expanded = File . expand_path ( file_path ) require_or_load ( expanded ) end raise NameError , \"uninitialized constant #{ qualified_name } \" end end ActionDispatch::Reloader This is a middleware which provides hooks that can be run while code reloading. It\nhas 2 callback hooks, :prepare and :cleanup . Rails code will make use of these\nhooks to install code which determine whether to reload code or not. :prepare callbacks will run before request is processed, and :cleanup callbacks will run\nafter request is processed. You can see call(env) of reloader here class ActionDispatch :: Reloader def call ( env ) @validated = @condition . call prepare! response = @app . call ( env ) response [ 2 ] = :: Rack :: BodyProxy . new ( response [ 2 ] ) { cleanup! } response rescue Exception cleanup! raise end end reload_classes_only_on_change This configuration option is defined in railties .\nBy default, it is set to true, so Rails reloads classes only if code changes. Set it\nto false, and Rails will reload on each request. Digging into the place where this\nboolean is defined, we find that there is an initializer set_clear_dependencies_hook .\nThis initializer is defined here . initializer :set_clear_dependencies_hook , group : :all do callback = lambda do ActiveSupport :: DescendantsTracker . clear ActiveSupport :: Dependencies . clear end if config . reload_classes_only_on_change reloader = config . file_watcher . new ( * watchable_args , & callback ) self . reloaders << reloader ActionDispatch :: Reloader . to_prepare ( prepend : true ) do reloader . execute end else ActionDispatch :: Reloader . to_cleanup ( & callback ) end end The above code installs a file watcher if config var is true. watchable_args consists of autoload_paths along with other files like schema.rb . So, file_watcher is configured to watch these paths. If config var is false, it\njust installs callback as :cleanup hook, which means all the code will be\nunloaded after each request is processed. How do these components fall in place? By joining all the dots, the sequence is: Request hits the server. The middleware ActionDispatch::Reloader kicks in,\nand executes callbacks inserted One of the callback inserted is to check whether any application code files\nhave been changed and execute the lambda {shown above which clears dependencies} The callback (lambda) will clear all the dependencies, i.e it unloads all the class\nconstants {by using builtin remove_const }. The middleware passes the request down the middleware stack for proper handling.\nMost probably routes will process the request. Once the request handling starts, since all the constants are unloaded, the main\nmodule ActiveSupport::Dependencies kicks in, uses const_missing and loads all\nclasses once again, thus loading the modified code. Bonus If you want to know how routes reloading happens, check this file If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-10-03"},
{"website": "Codemancers", "title": "image rendering on hd screens", "author": ["Vijay"], "link": "https://crypt.codemancers.com/posts/2013-11-03-image-rendering-on-hd-screens/", "abstract": "Rendering images for Retina or any high DPI screens. Written by Vijay on November 3, 2013; tagged under media-query , dpi , dips , retina , dppx Authoring HTML and CSS has come a long way. As opposed to the regime built in past, today we focus on lots of thing while authoring a page. We follow the standards and best practices and create semantic HTML with visual properties separated out in CSS.\nHere I’ll not talk about writing semantic HTML and CSS but will focus on rendering images for latest devices having device pixel ratio of 2(Retina Display) , 3(Galaxy S4) and even more in near future. Does it really need to be taken into account? Yes, if you are building a responsive site.\nBecause the images created for normal display will look pixelated and blurred in these devices. To understand better, lets look into some basic terms associated with responsive coding. dpi , Device Pixel Ratio , dip , dppx , are few common units that we should familiarise with. dpi - Dots per inch, is way to measure hardware resolution of any device. Which means number of dots per inch. A 21” screen can hev a resolution of 1680 X 1050 and even a 27” can have the same resolution. Larger screen will have less DPI than smaller screen. dip - Device independent pixels is actually equivalent to one pixel on 160 dpi screen. Also referred as dp . dppx - dots per ‘px’ unit. This unit uses more than 1 pixel to represent 1 css pixel. For example 2dppx means that it uses 2 pixels to represent 1px in css. Device Pixel Ratio (dpr) - This ratio is more of interest as we can switch CSS properties based on this value. This value also determines whether the device display is retina or that of S4. Device Pixel Ratio = window.devicePixelRatio . IE even 10 doesn’t support this. Most of the display have DPR 1 like all non-retinas and windows desktop displays. Retina display has a dpr of 2 which means it has twice more pixel density than a normal display. Find more on units Example: An image of 64px X 64px on dpr 1 display will use 64px wide and 64px vertical screen pixels. whereas on retina it will be rendered by 128px wide and 128px vertical screen pixels. This will make that image look blurry. Will be more disaster on display with dpr 3 (like Galaxy S4) 10px by 10px image rendered on different screens. Coming back, how do we then render images without blurring or pixelating it. Use svg images Scalable vector graphics are supported by most browsers. We can create most of the graphics as .svg and use it. Can be even used as sprites.\nThis solves the problem partially as we may not be able to create svg for every graphics especially photographs. In that case we can use media queries. Above icons are in svg format and it will look perfect in all devices. Check by zooming the page. Note: svg is not supported below IE9. SVG Web polyfill is available for that. CSS media queries For example we have a breakpoint for responsive design at 768px (tablet) and we are displaying a background image for both normal and retina display. As this image cannot be converted to svg we will use media query. Image size is say 250px X 400px and file name is image1.jpg .For retina display we will create the image at 500px X 800px and name is as image1@2x.jpg ans similarly, for device with dpr 3 create the same image at 750px x 1200px and name is as image1@3x.jpg Prefixing with @2x or @3x is just to denote that this image is scaled 2 times or 3 times. Any other consistent prefix works well as long as you decode it correctly. @ media only screen and ( min-width : 768px ) { /* non-retina */ . image1 { background-image : url ( 'image1.jpg' ); } } @ media only screen and ( -webkit-min-device-pixel-ratio : 2 ) and ( min-width : 768px ), only screen and ( min--moz-device-pixel-ratio : 2 ) and ( min-width : 768px ), only screen and ( -o-min-device-pixel-ratio : 2 / 1 ) and ( min-width : 768px ), only screen and ( min-device-pixel-ratio : 2 ) and ( min-width : 768px ), only screen and ( min-resolution : 2dppx ) { /* retina display. dpr 2 */ . image1 { background-image : url ( 'image1@2x.jpg' ); background-size : 250 px 400 px ; /* same size as normal image would be or on web it will scale up 2 times */ } } @ media only screen and ( -webkit-min-device-pixel-ratio : 3 ) and ( min-width : 768px ), only screen and ( min--moz-device-pixel-ratio : 3 ) and ( min-width : 768px ), only screen and ( -o-min-device-pixel-ratio : 3 / 1 ) and ( min-width : 768px ), only screen and ( min-device-pixel-ratio : 3 ) and ( min-width : 768px ), only screen and ( min-resolution : 3dppx ) { /* dpr 3 */ . image1 { background-image : url ( 'image1@3x.jpg' ); background-size : 250 px 400 px ; /* same size as normal image would be or on web it will scale up 3 times */ } } If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-11-03"},
{"website": "Codemancers", "title": "box model behaviour", "author": ["Vijay Sharma"], "link": "https://crypt.codemancers.com/posts/2013-11-17-box-model-behaviour/", "abstract": "CSS3 Box Model behaviour Written by Vijay Sharma on November 17, 2013; tagged under box-model , css , css3 , html5 What is CSS Box Model? W3C defines “The CSS box model describes the rectangular boxes that are generated for elements in the document tree and laid out according to the visual formatting model” It means that any html element on a web page can be represented using a box. Yes, a web page is full of boxes.\nThese boxes are made up of basically four components which affects their representation on a page.\nThis four components are Content , Padding , Border and Margin . HTML elements represented by a box can have all these four components or at least one component which is Content . Normally a box model can be represented by the following illustration. Content - this defines the content area of the box where the actual content like text, images or maybe other elements reside. Padding - this clears the main content from its containing box. Border - this surrounds both content and padding. Margin - this area defines a transparent space that separates it from other elements. A bit of history Prior to IE6, internet explorer had its own box model called Internet Explorer box model which was considered a buggy model. As per CSS 1 specifications, any width and height applied to an element via css will apply only to content area. Any padding , border and margin applied are added to the content area. This is how width and height are applied to element boxes. And this still holds true for all browsers. Even CSS3 spec makes this a default way of handling box models. As I mentioned earlier that internet explorer followed its own box model which was considered buggy. Any width and height applied to an element in internet explorer did include only content area, rather it applied to content area including padding and border. So an element normally appeared narrower in IE compared to other browsers. For backwards compatibility IE supported this model in quirks mode. You can trigger quirks mode in IE by using HTML 3 or earlier DTD or by completely removing DTD. Strangely, over the years we realized that the model considered buggy is actually what web authors need? CSS3 brought back this so called buggy model and now we have two box models which web authors can choose as per their requirement. These two different box models can be triggered via css with property box-sizing with values content-box | border-box | inherit . Below image shows how two different models behave. We are happy with what is there by default. Is it useful? In my opinion, having the flexibility of choosing box-model of your preference has made life much easier when it comes to coding HTML. Major browsers are already supporting it with vendor prefixes like below. div { -moz- box-sizing : border-box ; -webkit- box-sizing : border-box ; box-sizing : border-box ; } Example: Say we need a two column layout. Left column with 40% width and right column with 60% width. With default box model or box-sizing:content-box , we will create two floated divs with 40% and 60% with respectively with out adding any padding or margin, or else it will add to the content width and break the layout. These two divs will act as wrappers for left and right content. Any padding intended will have to applied to inner container div inside each wrapper. . div40 , . div60 { float : left ; } . div40 { width : 40 % ; } . div60 { width : 60 % ; } /* For Padding */ . div40 > div , . div60 > div { padding : 20 px ; } Using border-box model there’s no need for using extra div for padding which will surely take one step ahead in semantic coding. Lets see the code. . div40 , . div60 { -moz- box-sizing : border-box ; -webkit- box-sizing : border-box ; box-sizing : border-box ; float : left ; padding : 20 px ; //Padding to the same div will be adjusted from the specific width of each div. Same will be true for borders as well. } . div40 { width : 40 % ; } . div60 { width : 60 % ; } This example is just an illustration of CSS Box Model . You are the best judge for the kind of box-model you are going to use in your project. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-11-17"},
{"website": "Codemancers", "title": "isolate namespace in rails engines", "author": ["Yuva"], "link": "https://crypt.codemancers.com/posts/2013-09-22-isolate-namespace-in-rails-engines/", "abstract": "Isolate Namespace in Rails Engines - A hack in itself Written by Yuva on September 22, 2013; tagged under rails , engines , code-walkthrough , namespaces isolate_namespace is that one feature which Rails boasts about to use while\ncreating gems, but doesn’t work when one wants to extend models, controllers and\nviews which are provided by that gem. We have used it while developing one of our\ngems and found that its really hard to extend models, controllers and views.\nFor instance, if you copy views from gem to main application in order to customize\nthem, and try to use routes defined in the main application, you will be slapped\nwith an error saying those routes are not defined! It takes some time to understand\nhow it works under the hood, and all it boils down to is: isolate_namespace is\nnothing but a bunch of hacks. This blog post will do a code walk-through and will\ntry to explain how it works: The isolate_namespace method This method can be found in railties-engine file. Its reads as: def isolate_namespace ( mod ) engine_name ( generate_railtie_name ( mod )) self . routes . default_scope = { module : ActiveSupport :: Inflector . underscore ( mod . name ) } self . isolated = true unless mod . respond_to? ( :railtie_namespace ) name , railtie = engine_name , self mod . singleton_class . instance_eval do define_method ( :railtie_namespace ) { railtie } unless mod . respond_to? ( :table_name_prefix ) define_method ( :table_name_prefix ) { \" #{ name } _\" } end # removed code for :use_relative_model_naming? # removed code for :railtie_helpers_paths unless mod . respond_to? ( :railtie_routes_url_helpers ) define_method ( :railtie_routes_url_helpers ) { railtie . routes . url_helpers } end end end end mod is module, which is has to be isolated. In case of blorgh gem, its Blorgh itself. Hence forth, we will use blorgh gem as an example given in rails guides routes.default_scope : It defines default scope for routes. This scope will\nbe used while generating routes for Rails engine. It says module to be used is blorgh , and all the controllers will be searched under gem. This can be\neasily understood. Just put a binding.pry inside routes.rb of gem, and you\ncan see this: [ 4 ] pry ( #<Mapper>)> self.instance_variable_get(:@scope) => { :path_names => { :new => \"new\" , :edit => \"edit\" }, :module => \"blorgh\" , :constraints => {}, :defaults => {}, :options => {}} It says default module that should be used is blorgh . All the controllers will\nbe prepended by blorgh/ If module doesn’t respond to railtie_namespace {generally modules dont!}, it goes\nahead and adds bunch of methods to module {i.e Blorgh for example}, and not to\nengine. Thats why its a hack! There is nothing done on engine! Everything is added\nto Blorgh module. So, what it adds exactly? table_name_prefix : This can be easily guessed. It will be used by active record.\nNow searching through activerecord source, we find this: def full_table_name_prefix #:nodoc: ( parents . detect { | p | p . respond_to? ( :table_name_prefix ) } || self ) . table_name_prefix end It looks tricky, but this is how it works. It searches through all the parents of\nthe AR class, and checks whether any of them responds to table_name_prefix and\nreturns it. {Default value is empty string}. Well, parents are not the parents of\nthe class, but the hierarchy of the modules. activesupport defines this method: # Returns all the parents of this module according to its name, # ordered from nested outwards. The receiver is not contained # within the result. # #   module M #     module N #     end #   end #   X = M::N # #   M.parents    # => [Object] #   M::N.parents # => [M, Object] #   X.parents    # => [M, Object] def parents parents = [] if parent_name parts = parent_name . split ( '::' ) until parts . empty? parents << ActiveSupport :: Inflector . constantize ( parts * '::' ) parts . pop end end parents << Object unless parents . include? Object parents end Now, here comes the funny part: Create a folder called blorgh in your\nmain application under app/models folder, and create a model called Test module Blorgh class Test < ActiveRecord :: Base end end Now, fire up console, and execute the following: [ 1 ] pry ( main ) > Blorgh :: Test . table_name => \"blorgh_tests\" See, it automatically prepends tests table name with blorgh_ . One will be\nwondering how did that happen? Well it happened because of the module called Blorgh . So anything one puts under Blorgh module will get special\ntreatment. Do it with any other namespace, {i.e module}, it will just return tests . The only way you can get rid of this behavior is to specify the table\nname explicity on model using self.table_name = \"tests\" . If someone in some\ngem magically says to isolate a namespace which you are using in your application,\nhell breaks loose! You will be wondering why your application code is behaving\nstrangely. You can find other hacks by searching through the Rails code. We will cover\nanother hack here: railtie_routes_url_helpers : This method is used to define route helpers which\ncan be accessible to generate paths. Digging through the code, you can find it in\nactionpack. module AbstractController module Railties module RoutesHelpers def self . with ( routes ) Module . new do define_method ( :inherited ) do | klass | super ( klass ) namespace = klass . parents . detect { | m | m . respond_to? ( :railtie_routes_url_helpers ) } if namespace klass . send ( :include , namespace . railtie_routes_url_helpers ) else klass . send ( :include , routes . url_helpers ) end end end end end end end This inherited method will be called in the context of your controllers. Again\nif your controller is under Blorgh module, it magically includes only\nthe routes defined by gem, otherwise it includes the application route helpers.\nThats why even though you copy views from gem to your main app, they still cannot\naccess helpers defined by main application. Generally we all know how to fix this:\nCall the url helpers by prepending with either main_app or gem mount point,\ni.e blorgh here. This way all the route helpers will be available in all\nthe views. = link_to \"Surveys\" , blorgh . question_groups_path = link_to \"Login\" , main_app . new_user_session_path Other issues include extending models and controllers. Rails guides gives two\noptions here .\nOne to use class_eval , and other to use concerns introduced in Rails 4. Both\nare kind of hacky. Hope there is a better solution. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-09-22"},
{"website": "Codemancers", "title": "random ruby tips from trenches number 1", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2014-01-07-random-ruby-tips-from-trenches-number-1/", "abstract": "Random Ruby tips from trenches #1 Written by Hemant on January 7, 2014; tagged under ruby , rails , tips Rubocop and Flycheck : Flycheck is a emacs mode which helps\nus with IDE like warnings in Emacs. I am already using enh-ruby-mode which helps\nwith some of the syntax errors and stuff, but what is nice about flycheck is\nit integrates with rubocop and shows rubocop errors in-place in the editor. A picture is worth thousand words so:\n\n![rubocop with flycheck](https://f.cl.ly/items/1T173n0g351N381D2C2z/Screenshot%202013-12-19%2012.09.50.png \"Rubocop stuff\") pry ––gem: pry --gem opens a pry session with ./lib added to $LOAD_PATH and ‘require’s the gem.\nA good shortcut while working on gems and you want a quick console with the gem loaded. ruby –S : This can be used for running binaries which are present in local directory. For example,\nif you are working on bundler and want to run bundle command with local version of\nbundler rather than one installed globally you can use: ruby -S ./bin/bundle -I ./lib/ install\n\n**The advantages are:**\n\n* `ruby -S` allows you to ignore `#!/usr/bin/env ruby` line and even if current version of ruby is X using `ruby -S` you can run the command with another version of Ruby. Especially useful for running scripts with JRuby, Rbx etc.\n\n* Based on `RUBYPATH` environment variable running a script via `ruby -S` (or `jruby -S`) allows\n`PATH` environment to be modified. For example running `jruby -S gem` does not run `gem` which\nis in current path, but it runs `gem` command provided by JRuby because JRuby defines different\n`RUBYPATH`. Faster rbenv load : If you are using rbenv and there is a lag while opening a new shell, consider\nupdating the rbenv initializing line in your shell profile to: eval \"$(rbenv init - --no-rehash)\"\n\nThe `--no-rehash` flag tells `rbenv` to skip automatic rehash when opening a new\nshell and speeds up the process. This also speeds up VIM if you are using\n[vim-rails](https://github.com/tpope/vim-rails) or\n[vim-ruby](https://github.com/vim-ruby/vim-ruby). If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2014-01-07"},
{"website": "Codemancers", "title": "form objects validations", "author": ["Yuva"], "link": "https://crypt.codemancers.com/posts/2013-12-18-form-objects-validations/", "abstract": "Form object validations in Rails 4 Written by Yuva on March 22, 2014; tagged under rails , rspec Of late, at Codemancers, we are using form objects to decouple forms in\nviews. This also helps in cleaning up how the data filled by end user is\nconsumed and persisted in the backend. So, far the results have been\ngood. What are form objects This blog post assumes that you are already familiar with form objects. Railscasts\nhas a nice screencast about form objects .\nDo check it out if you haven’t already. Use case Let’s say that there is an organization and it has several employees.\nWe’re tasked to build a Rails app that provides an interface where an\nadmin can select one or more employees and send them emails. A typical\ninterface implementation might look like this: After selecting employees, filling-in the subject and body, and upon\nclicking “Send”, the backend should send emails to the selected\nemployees. This is done by passing the array of the ids of employees,\nthe subject and body to the backend. The POST parameters for that request\nlook like this: { \"utf8\" => \"✓\" , \"email_form\" => { \"employee_ids\" =>[ \"\" ] , \"subject\" => \"\" , \"body\" => \"\" }, \"commit\" => \"Send emails to employees\" } Mass mailer form We will create a EmployeesMassMailerForm form to encapsulate the\nvalidations and performing the actual action of sending email. This\nform should accept the params sent by the form, perform validations\nlike checking whether all the employee ids belong to organization etc.,\nand then send the emails. class Organization < ActiveRecord :: Base def get_employees ( ids ) employees . where ( id : ids ) end end class EmployeeMassMailerForm include ActiveModel :: Model attr_accessor :organization , :employee_ids , :subject , :body validates :organization , :employee_ids , :subject , :body , presence : true validate :employee_ids_should_belong_to_organization def perform return false unless valid? @employees = organization . get_employees ( xemployee_ids ) @employees . each { | e | schedule_email_for ( e ) } true end private def employee_ids_should_belong_to_organization if organization . get_employees ( employee_ids ) . length != employee_ids . length errors . add ( :employee_ids , :invalid ) end end def schedule_email_for ( e ) Mailer . send_email ( e , subject , body ) end end With Rails 4, ActiveModel ships with Model module which helps in assigning\nattributes, just like how you can do with ActiveRecord class, along with\nhelpers for validations. It is no longer necessary to use other libraries for\nform objects. Just include ActiveModel in a PORO class and you are good to\ngo. Testing using rspec and shoulda All the form objects can be broken down into 2 main sections: Validations Performing actions Testing validations Adding validations on forms and models is pretty straight forward.\nExcept for database-related validations like uniqueness , all the\nActiveRecord validations can be used on form objects. These validations\nalso make it easy to display validation errors in the view. At Codemancers, we mostly use rspec and shoulda for testing. Validations\non forms can be tested like this: describe EmployeeMassMailerForm do describe 'Validations' do it { should validate_presence_of ( :organization ) } it { should validate_presence_of ( :employee_ids ) } it { should validate_presence_of ( :subject ) } it { should validate_presence_of ( :body ) } context 'when employee ids belong to organization' do it 'validates form successfully' do employee_ids = [ 1 , 2 ] organization = mock_model ( Organization , get_employees : employees_ids ) form = described_class . new ( organization : organization , subject : 'Test' , employee_ids : employee_ids , body : 'Test' ) expect ( form ) . to be_valid end end context 'when one or more employee ids donot belong organization' do it 'fails to validate the form' do organization = mock_model ( Organization , get_employees : [] ) form = described_class . new ( organization : organization , subject : 'Test' , employee_ids : [ 1 , 2 , 3 ] , body : 'Test' ) expect ( form ) . to be_invalid end end end end You can notice here that while validating employee ids, we use stubs and\nmock models so that tests never hit database. Testing a form that has\nvalidations is a bit hard, because one has to heavily stub and mock\nmodels until form becomes valid. But testing an invalid form is easy and\nsometimes easy to maintain. Notice that we do not care what get_employees returns and that we hard coded it with an empty array\nwhose length is 0. Always try to put as many validations as possible on\nform object, so that very less exceptions are raised while performing\nactions. Testing actions performed by form Once all the validations pass, the form object will go ahead and perform\nthe action it is supposed to do. It can be anything from sending emails\nto persisting objects to database. Lets see how we can test the action perform from above form. describe EmployeeMassMailerForm do describe '#perform' do let ( :organization ) do employees = [ stub ( email : 'a@b.com' ), stub ( email : 'b@c.com' ) ] mock_model ( Organization , get_employees : employees ) end let ( :form ) do described_class . new ( organization : organization , subject : 'Test' , employee_ids : [ 1 , 2 ] , body : 'Test' ) end before ( :each ) do described_class . any_instance . should_receive ( :valid? ) . and_return ( true ) InvitesMailer . deliveries . clear end it 'sends emails to all employees' do form . perform expect ( InvitesMailer . deliveries . length ) . to eq 2 end it 'returns true' do expect ( form . perform ) . to be_true end end end The trick here is to hard-code valid? to be true in before block. Since we\nhave already tested validations, we can hard code the return value of valid? to\nbe true . This saves a bunch of db calls and mocks. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2014-03-22"},
{"website": "Codemancers", "title": "design and development process", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2014-01-29-design-and-development-process/", "abstract": "Design and Development Process Written by Hemant on January 29, 2014; tagged under outsourcing , startup , india Remote engagements are hard by their very nature. In a set up where people involved in the project are separated by time and distance, having a well worked out process is the only way to ensure smooth engagement and steady progress. However, process implementation requires a fine balance - There should be enough of it to drive frictionless communication but not so much that adhering to it is in itself hindering progress. In this post, we will describe the barest minimum of a process you need to put in place to be able to effectively carry out long term engagements with offshore teams. Identifying Roles As a product owner, you play an important role in the development effort. Since you are the best person to present the customer perspective for your product, your continuous engagement is highly recommended. The communication process can be streamlined by identifying a single point of contact for you in the development team. Deciding roles, expectations, frequency of communication and points of contact right at the beginning of the project will help in making sure there are no ugly surprises later on. Process For Design Once you have had calls to discuss about product and features, the next important step is to mock up those ideas. It is not a good practice to start development right after discussions. Though this point is stressed enough in several blogs and books, programmers sometimes dive straight into development in their enthusiasm to get their hands dirty. Translating feature discussions to product development by skipping the intermediate mocking up stage may look like a time saver initially. But if expectations are not met, iterating on the actual product will end up being much more expensive than iterating on a mockup. There is a definite advantage in adhering to the processes and best practices we are outlining here even if your interaction is limited to an Elance developer and not a full-fledged development firm. Mockups and Discussions Since whiteboard discussions are not an option in a remote setup, you will have to necessarily employ tools that aid your design discussions. There are several online mockup tools which you and the team can use to sketch your ideas. As for feature implementations, either you can provide the development team with the necessary mockups or get the UX person from the development team to come up with the mockups to be validated from you. Once a few key pages and features are mocked up, the team can get on with the development efforts while you and the UX person from the team continue with the discussions for the next set of features. Iterative Model Following an iterative and an incremental approach to development substantially reduces the risk of ending up with an unusable product. In this model, you will implement a small set of meaningful features which is finetuned iteratively with actual usage. Each set of features is then added incrementally to the existing product and finetuned further after usage. This approach even offers the ability to change the feature set based on your feel for the product from initial usage. Development cycles & Product demos Your development discussion with the team should focus on deciding the duration of development cycles and what can be achieved in those cycles. As a product owner, your contribution is valuable in deciding the right set of features for implementation in each development cycle and also in providing feedback after usage. Most companies offer a weekly cycle with a demo at the end of it to help you measure progress. Continuous Integration & Continuous Deployment Continuous integration and deployment are software development best practices which, if employed, will considerably reduce the amount of time spent in testing and fixing bugs. A company with this set up will be able to discover bugs as and when they are introduced which inturn will help in easy isolation and fixing. Compare this to a scenario where you encounter a whole set of bugs and broken features after deploying your product. The time spent on fixing it this way is so huge that it is smarter to set up an environment for continuous deployment and integration. We recommend this irrespective of how complex or simple your product is. click here for a detailed reading on this subject. Project Management tools Project management tools play a key role in helping you set expectations and deliverables; track progress; and reasonably estimate development time for features. Thus it is important to choose a tool which you are comfortable with. The process should be managed in such a way that at any point during the project, you should be able to assess progress without having to actually communicate with any of the team members. Code Quality Metrics If you are not a developer, you may not be fully equipped to check the quality of code the team produces. You can solve this to some extent by using tools like Codeclimate which will give you a broad idea about the health of the code.If there are any red flags, you can take it up with the team for a discussion. Rinse Repeat Once you have figured out the process and communication for the first set of features, it is all about going through various cycles of the same procedure. Rinse repeat till you get to the launchable version of your product. The idea of this post was to bring out the importance of having a process and the role of management tools in achieving effective communication. In the next post, we will discuss some of the specific tools that we use internally for our design and development process; and our interaction with clients. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2014-01-29"},
{"website": "Codemancers", "title": "making india work for startup", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2014-01-21-making-india-work-for-startup/", "abstract": "Making an Indian agency work for your startup Written by Hemant on January 21, 2014; tagged under outsourcing , startup , india India has come a long way in the past decade with respect to software talent\nand contribution. The number and the quality of conferences; technical meetups;\nand API hackathons, especially in cities like Bangalore, indicate the positive\nchange the industry has seen in the last 10 years. Such events have led to\nmore open source contributions and a much more passionate generation of\ndevelopers who love programming for the fun of it. This shift in the industry definitely renders India to be a reliable and\ncost-effective outsourcing destination. But, here’s the caveat: India has a\nlarge number of programmers and by law of percentages it also has a\nproportionately big pool of the bad ones. This means that even though there\nis an attractive talent pool and a compelling cost advantage in outsourcing,\nthere is the potential risk of ending up with a bad firm and burning your\nfingers in the process. If you are a company stationed outside India but looking to outsource,\nhow will you discern the quality of a firm or its developers sitting so\nfar away? This post is intended to help you with a basic framework to make the right\nchoice when it comes to finding your development partner. Defining the scope for your project This is the first and the most important step in your agency finding exercise.\nWhile many might argue that a good firm should be able to help you with defining\nthe scope and details, it is necessary for you to do it yourself as a first step\nto be able to get to the right firm. How you go about finding a firm will depend on the nature of your project. To\ngive an example, if you are looking to build a basic first version of your\nproduct, you will have to shortlist somebody who is willing to work closely\nwith you to understand your domain and market needs and has the necessary\ntechnical depth to make the right early decisions. If you already have a\nworking product and you are looking for a firm to optimize the existing code\nor build particular modules, you may want to look at firms with deep backend\nexpertise. Further still, if you are looking for a firm to just develop the\nmobile app version of your product, you may have to choose somebody with that\nspecific experience. Channels for finding right agency References Mining your personal/professional network for references is undeniably the best\nway to start your search. References generally come through only when somebody\nhas had a great experience and this filter makes your selection process easier.\nEven if you don’t have immediate connections, with networks like Linkedin, it\nshould be possible to take advantage of 2nd or even 3rd level connections. Mailing lists If you have decided on the technology to use, you can reach out to developers\nthrough mailing lists for that technology in the region that you are considering.\nDevelopers from a good agency will have an active participation in discussions\nand it should be a good place to get in touch with the ones relevant to you. Conferences and networking events If you have the time and budget, it is best if you can make a trip to India to\nattend local conferences and networking events. A good development agency usually\nhas speakers at key events in the city. Attending such events will help you make\nyour selection after face to face meetings with the prospects. Evaluation Once you have a shortlist of companies, the next crucial step is to see who will\nbest suit your needs and working style. The following points will give you a\nframework for that decision making. Team Depending on the scope of your project, you may want to see how many developers\nneed to be engaged full time on your project and what their profiles are. Even\nif the founders and some team members have impressive profiles, the quality of\nthe work delivered depends very much on who is working on your project. Be sure\nto evaluate the profiles of members who will be working directly with you. Project Portfolio A confident firm will discuss about projects done, share their project portfolio\nand will also willingly extend customer references. Not all details are published\nin a company website because sometimes client projects are confidential. In such\ncases you can ask the firm to mail you the project portfolio that talks about all\ntheir projects and processes. Also, it will definitely help in your decision making process to talk to their\nclients about what they liked/disliked about their engagement with that firm. Customer testimonials Customer testimonials offer credibility and provide insight into the company.\nEventhough a company will only publish the positive feedback from clients as\ntestimonials, a quick look at those testimonials should help you understand what\nthe firm is doing right. Open Source contribution If your choice of technology is open source, you are better off choosing firms\nwho contribute to Open Source and more importantly firms that encourage\ncontributing to Open Source. An active participation and contribution in the\ndevelopment community generally indicates a love for their own craft. It goes\nwithout saying that the ones that love their craft produce quality work. You can can check a company’s contributions by asking for their Github profile and\nchecking the projects the company has created or contributed to. The advantage of this method is that you can even engage somebody independently\nto check the quality of the code contributed before you make the decision to hire\nthe firm. Process Development firms everywhere have the tendency to use technology buzzwords to\nappeal to their clients. If a team claims they follow agile processes, make sure\nyou get them to explain what it means for you in layman terms. Does Agile mean\nweekly updates? Does ‘pair programming’ mean higher costs? Make sure you get\nthem to explain their processes. This will help you in understanding how your\neveryday interaction will fit into their processes. Communication Most offshore engagements fail due to poor communication. Even if the firm has an\nimpressive background, if you are not convinced about their communication skills,\nyou are better off letting go. Communication problems only worsen over time\nespecially with the time zone differences. By communication, I refer to the\nability of the team members to listen, understand, ask the right questions and\nconvey their points of view to arrive at an agreement over discussions. Budget ‘If you pay peanuts you will get monkeys’ holds true when it comes to software\noutsourcing in India. If you are looking to hire someone at a rate of USD 10/per\nhour you are likely losing your entire budget without getting any work done.\nIt takes anything between $2000 to $7000 per month per developer or more\n(depending on complexity) to engage a firm that will deliver on its promises.\nEven at this price point, the costs are lower than hiring a full\ntime developer in the US or Europe. More often than not, mending bad code ends up being more expensive than building\nfrom scratch. Cost is a decent indicator of quality and hence you should have\nyour guards up if you come across ridiculously low bids. The evaluation process plays a crucial role in your success because making a wrong\nchoice can cost you dearly. Our advice here stems from years of experience of working with startups around the world.\nWhile this post will help\nyou in finding the right firm, it is only the first step. In our successive posts,\nwe will have discussions on design and development processes that will help you\nengage with your offshore team effectively. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2014-01-21"},
{"website": "Codemancers", "title": "what you need to know about software process and tools as a non tech product owner part 1", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2014-04-02-what-you-need-to-know-about-software-process-and-tools-as-a-non-tech-product-owner-part-1/", "abstract": "What you need to know about software process and tools as a non-tech product owner - Part 1 Written by Hemant on April 2, 2014; tagged under outsourcing , startup , india Software development process can be considered a machine into which you feed ideas to generate finished products. When it comes to building products, it is not just raw programming skill that contributes to successful engineering. The development process employed and the tools used to put such a process in place contribute as much to product success. (Read more on this here ). This is especially the case in remote engagements. Your choices determine the shape and health of the process. Using the right tools is absolutely necessary to have smooth workflows and minimal overheads. Here, we will discuss briefly about the process and tools used internally in Codemancers. This information is a good starting point for you to be able to participate in conversations with your offshore team about their process and tools. Mock up & Design The ability to visualize and express a product in all its detail even before its implementation is an essential pre step to product building. The ability to express and have discussions about how the user will interact with the product and how the different modules of the product will interact with each other plays an important role in building usable products. While most people are comfortable using pencil & paper for quick initial sketches, there are significant advantages in using the online tools available today. You will have a record of all your initial sketches and ideas You will be able to collaborate over the sketches and discuss even if members of a team are geographically distributed It gets easy to integrate the design step with rest of the process to render a smooth work flow Balsamiq We chose to use Balsamiq because, it is easy; it is pretty; and it integrates with our Project management tool. Every UI screen that we mock up and agree upon is exported to our project management tool where it is discussed in terms of features and implementation. If we were to skip this step to discuss implementation details straight away, the product will end up being a crude hack instead of being a usable piece of software. A screenshot of the initial wireframing done with Balsamiq Project Management A good way to approach web application development today is by employing a methodology that supports Agile process (incremental and iterative development). Instead of designing and implementing a product in its entirety, in this method, you start with a minimum set of features that will add value to an end user. Once it gets implemented, you can start working on the next set of features. These features themselves are fine-tuned over several iterations. The advantage of the Agile methodology is that, it is easy to change or modify the direction of the product roadmap based on feedback from actual usage of the implemented features. Pivotal Tracker Pivotal tracker is our choice of project management tool because it supports iterative development and is easy to use. Also, it is the perfect tool for a non-technical product owner to get a quick understanding of the agile process. Once you understand how to be agile the Pivotal way, progress gets smooth and predictable. Here, we first break the product idea into features (also called stories). Each story has a description which explains the feature in detail. Once a feature is detailed and discussed, it is translated into specific actionable tasks for developers. At Codemancers, we prefer to document all stories and descriptions before translating them into tasks in Pivotal. Any of the following tools will work for this purpose - Basecamp , Google Docs , JIRA or Github issues . The only thing to be taken care of is that, if a story requires collaboration of several team members, the tool should have features that support commenting and attaching of mockups or any other required file to aid such a discussion. A screenshot of the stories done the Agile way in Pivotal #####Github Issues Some of our technically savvier clients who do not want to go the Pivotal way, insist on using Github Issues. In such cases, we use Waffle with Github issues to have the necessary storyboard like visualization. A screenshot of the storyboard visualization of Github Issues with Waffle As somebody who oversees the creation of the product, you should be able to evaluate a team not just based on their technology skills but also based on their ability to put a framework in place. In the next post, we will discuss the rest of the process - about Code hosting, Continuous integration and tools that can be used for internal communication. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2014-04-02"},
{"website": "Codemancers", "title": "using git for knowledge sharing", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2014-05-01-using-git-for-knowledge-sharing/", "abstract": "Using Git for Knowledge sharing Written by Hemant on May 1, 2014; tagged under knowledge-sharing , startup-culture , git As imperative as it is for efficiency, knowledge sharing is not a behavior that is easily assimilated into the company culture. There is no immediate incentive for a team member to go out of the way to share resources and hacks that might prove useful for somebody else in the future. Even well intentioned employees might not contribute if the process is too much trouble. There are several threads on Quora discussing the various available ways - Wiki, Google docs, knowledge management software&mldr;the list goes on. There is no dearth of options. Even so, we chose to do it our own way. Here we share our 2 cents of wisdom on how we do it and why it works. Why not a knowledge management software? As programmers, we generally are either too lethargic or too busy to step out of the comfort zone of a text editor or terminal. In such a case, it would be quite a hassle to open or login to a separate system to share tips and tricks learnt through the day. The only way we can hope to make knowledge sharing possible is by hacking a process which will not interfere with a programmer’s flow. Our hack is very simple but quite efficient - we have been doing this for over a year now. What we do is - anything a developer thinks worth sharing is written down in a Markdown file and pushed into the collective-knowledge repository which we call ‘Picks’. Others in our team get notified via Hipchat, they can have a look and comment on commit if needed. Content is easily searchable via\nsimple tools like grep or ag . There is a good reason why developers would prefer a Jekyll to Wordpress. The philosophy of Jekyll (as below), quite reflects our reason to go ‘Picks’. Jekyll does what you tell it to do — no more, no less. It doesn’t try to outsmart users by making bold assumptions, nor does it burden them with needless complexity and configuration. Put simply, Jekyll gets out of your way and allows you to concentrate on what truly matters: your content. The toothpick gem We have even created a command line utility that automates this workflow. Using Toothpick , saving the file in your editor automatically pushes the pick to git repository. Knowledge sharing works quite well this way at our scale; we are a team of 10 devs. We are not too sure about how this will scale as the team scales. But, that is a problem to be solved in the future. We would love to learn what works for you and how you manage it in your company. Looking forward to your comments. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2014-05-01"},
{"website": "Codemancers", "title": "setting up emacs as development environment on osx", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2013-09-26-setting-up-emacs-as-development-environment-on-osx/", "abstract": "Setting up Emacs as Ruby development environment on OSX Written by Hemant on October 17, 2013; tagged under emacs , osx , ruby , rails Learning and setting up Emacs can be intimidating and while some folks\nfind much use in using a pre-configured setup like - Emacs Prelude,\nI personally think it is better that someone new to Emacs installs\neach and every module himself and learns from it. I am assuming\nOSX as development environment here but the instructions should\nwork well enough on Linux as well. Install Emacs : Install Emacs using Homebrew. I personally do not recommend\nusing Emacs for OSX distribution.\nYou can find out why here . brew install emacs --use-git-head --cocoa --srgb There is a bug in Emacs HEAD that prevents it to be working with Cask and thus\ntry not to use Emacs HEAD. Install a Package Manager : I use pallet for managing various packages. Think of pallet as Bundler/Maven for Emacs.\nFollow instructions available in section For newly installed Emacs . Once you have pallet installed each\npackage you install via package-install command will automatically update the Cask and you can checkin the file in version control system and carry it around. Sometimes you have to install a package directly from git. I personally use Git Subtrees for adding packages from git. Feel free to use submodules or directly copying files as well. Structuring your Customization : Over last couple of years I personally have been using following style\nof structuring my emacs setup. .emacs.d/\n   init.el -> The real .emacs file.\n   custom/\n     01ruby.el\n     02org.el\n     03auto-complete.el\n     04web-mode.el The .emacs file : Contents of init.el looks something like this: (require 'cask \"~/.cask/cask.el\")\n (cask-initialize)\n (require 'pallet)\n (add-to-list 'load-path \"~/.emacs.d/custom\")\n (add-to-list 'load-path \"~/.emacs.d/other_paths)\n ..\n ..\n (load \"00common-setup.el\")\n (load \"01ruby.el\")\n (load \"02org.el\") Global Config : I use a lot of global configuration stuff which isn’t mode specific, some important ones are: ;; Navigate between windows using Alt-1, Alt-2, Shift-left, shift-up, shift-right\n (windmove-default-keybindings)\n\n ;; Enable copy and pasting from clipboard\n (setq x-select-enable-clipboard t) You can see complete file on, 00common-setup.el . If you are completely\nnew to Emacs you should use C-h t (That is press Control-h and then t) to launch inbuilt Emacs tutorial from within Emacs. Packages You need For complete list of package see my Caskfile . Also\nsome packages I use straight from git and hence may not be listed in Caskfile. With that\nout of the way, I will try to cover most important packages I love and use everyday: auto-complete : You can install it via package-install and below is my\nconfiguration of auto-complete mode: (require 'auto-complete-config)\n (add-to-list 'ac-dictionary-directories\n     \"~/.emacs.d/.cask/24.3.50.1/elpa/auto-complete-20130724.1750/dict\")\n (ac-config-default)\n (setq ac-ignore-case nil)\n (add-to-list 'ac-modes 'enh-ruby-mode)\n (add-to-list 'ac-modes 'web-mode) It gives me access to all powerful auto-completion system for Emacs. Below\nis a screenshot. ag : You should use silver searcher and corresponding Emacs mode ag . They beat likes of ack or grep hands down. enh-ruby-mode : Personally I am big fan of Enhanced Ruby Mode . It gives you better syntax highligting,\nbetter indentation schemes and code intelligence without running flymake kind of thing. You can see rest of my enh-ruby-mode specific configuration in file 01ruby.el . You will notice that I have included\na mini rspec-mode there , which allows me to run specs of a complete file or\njust spec under the cursor. smartparens : smartparens mode is too much of a moving target and hence I always use it from git. My smartparens configuration is: (require 'smartparens-config)\n (require 'smartparens-ruby)\n (smartparens-global-mode)\n (show-smartparens-global-mode t)\n (sp-with-modes '(rhtml-mode)\n   (sp-local-pair \"<\" \">\")\n   (sp-local-pair \"<%\" \"%>\")) SmartParens mode is many things. It automatically inserts closing parethesis, tags, end’s\ndepending on major-mode. Highlights them, allows you to move between them. Allows you to\nwrap existing texts. However you may not even have to go through their documentation, it\nmostly just works. Projectile Mode : Projectile is one among\nmany ways of handling projects in Emacs. My projectile configuration looks like: (require 'grizzl)\n (projectile-global-mode)\n (setq projectile-enable-caching t)\n (setq projectile-completion-system 'grizzl)\n ;; Press Command-p for fuzzy find in project\n (global-set-key (kbd \"s-p\") 'projectile-find-file)\n ;; Press Command-b for fuzzy switch buffer\n (global-set-key (kbd \"s-b\") 'projectile-switch-to-buffer)\n\n ![It allows me to do:](\n\n  \n    https://crypt.codemancers.com/images/emacs/emacs4.2be08a609329db017ab61dafd234d821fe408e5fffa0f0002957e8ca52fc0a65.gif\n) Rainbow Mode : Add color to your css/scss. Check screenshot below: Robe Mode : Robe mode is what makes Emacs full featured IDE even comparable to likes of RubyMine etc. Robe can also integrate with `auto-complete` mode to provide more intelligent auto-completion.\nI personally do not use that feature because it makes things slower. Highlight indentation for Emacs : Highlight Indentation mode allows\nyou to see indentation guides. My setup for the mode is: (require 'highlight-indentation)\n (add-hook 'enh-ruby-mode-hook\n           (lambda () (highlight-indentation-current-column-mode)))\n\n (add-hook 'coffee-mode-hook\n           (lambda () (highlight-indentation-current-column-mode))) Yasnippet : Yasnippet is Emacs snippet\nlibrary. Out of box it won’t work with enh-ruby-mode because it searches for ruby-mode .\nYou can make it work by copying ruby-mode snippets to enh-ruby-mode folder. Flyspell : If you are like me, you make lot of typos in code comments, strings etc.\nUsing Flyspell mode you need not worry about it - interfering with programming language keywords\netc. My setup for Flyspell is : (require 'flyspell)\n(setq flyspell-issue-message-flg nil)\n(add-hook 'enh-ruby-mode-hook\n          (lambda () (flyspell-prog-mode)))\n\n(add-hook 'web-mode-hook\n          (lambda () (flyspell-prog-mode)))\n;; flyspell mode breaks auto-complete mode without this.\n(ac-flyspell-workaround) If you are on OSX you may have to run brew install ispell to make it work. Rinari : With Projectile mode, I am finding less and less use of this mode. But this\nis your canonical rails mode for Emacs. Get it via package-install or Rinari Github repo . dash-at-point : If you are on OSX you should totally use Dash .\nUsing dash-at-point you can view documentation of methods, classes etc right from Emacs. multiple-cursors : Multiple Cursors is Emacs answer to multi-line editing feature of Sublime etc. Anything I can add here is\nprobably better covered in this Emacs Rocks screencast. textmate.el : textmate.el allows me to quickly\nmove between methods in a file, independent of programming language mode (and without ctags). It has many other utilities. WebMode : For a long time Emacs users were mostly stuck with half-assed multi-language\nmode when editing html templates. WebMode is like breath of fresh air\nfor those who mix - ruby/js/css/coffee in their HTML! Apart from aforementioned modes, I still use too many other modes - sometimes even unknowingly! Some modes that\ndeserve honourable mention but I don’t use them is - Magit , Expand Region . If you do lot of Javascript then have a look at - Swank-js . If you are coming from Vim and looking for\ndecent nerdtree replacement, you can try Dirtree or ECB or Project explorer . Learning Emacs in a way is - as deep as you are willing go. For resources I recommend: EmacsWiki Emacs Reddit Mastering Emacs Emacs Movies If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-10-17"},
{"website": "Codemancers", "title": "an introduction to json schema", "author": ["Kashyap"], "link": "https://crypt.codemancers.com/posts/2014-02-11-an-introduction-to-json-schema/", "abstract": "An Introduction to JSON Schema Written by Kashyap on April 5, 2014; tagged under json , api , ruby JSON, or JavaScript Object Notation has become the most widely used\nserialization and transport mechanism for information across various\nweb-services. From it’s initial conception, the format garnered swift\nand wide appreciation for being really simple and non-verbose. Lets say you want to consume the following JSON object via an API: { id : 3232 , name : \"Kashyap\" , email : \"kashyap@example.com\" contact : { id : 123 , address1 : \"Shire\" , zipcode : LSQ424 } } Now, let’s assume that you want to ensure that before consuming this data, email and contact.zipcode must be present in the JSON. If that\ndata is not present, you shouldn’t be using it. The typical way is to\ncheck for presence of those fields but this whack-a-mole quickly gets\ntiresome. Similarly, lets say you are an API provider and you want to let your API\nusers know the basic structure to which data is going to conform\nto, so that your API users can automatically test validity of data. If you ever had to deal with above two problems, you should be using\nJSON schemas. What’s a Schema? A schema is defined in Wikipedia as a way to define the\nstructure, content, and to some extent, the semantics of XML documents;\nwhich probably is the simplest way one could explain it. For every\nelement — or node — in a document, a rule is given to which it needs to\nconform. Having constraints defined at this level will make it\nunnecessary to handle the edge cases in the application logic. This is a\npretty powerful tool. This was missing from the original JSON\nspecification but efforts were made to design one later on. Why do we need a Schema? If you’re familiar with HTML, the doctype declaration on the first line\nis a schema declaration. (Specific to HTML 4 and below.) HTML 4 Transitional DOCTYPE declaration: <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"> This line declares that the rest of the document conforms to the\ndirectives specified at the url http://www.w3.org/TR/html4/loose.dtd .\nThat means, if you declare the document as strict, then the usage of any\nnew elements like <sp></sp> will cause the page to display nothing. In\nother words, if you make a typo or forget to close a tag somewhere, then\nthe page will not get rendered and your users will end up with a blank\npage. At first glance, this looks like a pain — and it is, actually. That’s\npart of the reason why this was abandoned altogether in the newer\nversion of HTML. However, HTML is not really a good use case for a\nschema. Having a well-defined schema upfront helps in validating user\ninput at the language/protocol level than at the application’s\nimplementation level. Let’s see how defining a schema makes it easy to\nhandle user input errors. JSON Schema The JSON Schema specification is divided into three parts: JSON Schema Core :\nThe JSON Schema Core specification is where the terminology for a\nschema is defined. Technically, this is simply the JSON spec with the\nonly addition being definition of a new media type of application/schema+json . Oh! a more important contribution of this\ndocument is the $schema keyword which is used to identify the\nversion of the schema and the location of a resource that defines a\nschema. This is analogous to the DOCTYPE declaration in the HTML 4.01\nand other older HTML versions. The versions of the schema separate changes in the keywords and the\ngeneral structure of a schema document. The resource of a schema is\nusually a webpage which provides a JSON object that defines a\nspecification. Confused? Go open up the url http://www.w3.org/TR/html4/loose.dtd which I’m linking to here in a browser and go through the contents.\nThis is the specification of HTML 4.01 Loose API. Tags like ENTITY,\nELEMENT, ATTLIST are used to define the accepted elements, entities\nand attributes for a valid HTML document. Similarly, the JSON Schema Core resource URL (downloads the schema document) defines a superset of constraints. JSON Schema Validation :\nThe JSON Schema Validation specification is the document that defines\nthe valid ways to define validation constraints. This document also\ndefines a set of keywords that can be used to specify validations for\na JSON API. For example, keywords like multipleOf , maxLength , minLength etc. are defined in this specification. In the examples\nthat follow, we will be using some of these keywords. JSON Hyper-Schema :\nThis is another extension of the JSON Schema spec, where-in, the\nhyperlink and hypermedia-related keywords are defined. For example,\nconsider the case of a globally available avatar (or, Gravatar). Every\nGravatar is composed of three different components: A Picture ID, A Link to the picture, Details of the User (name and email ID). When we query the API provided by Gravatar, we get a reponse typically\nhaving this data encoded as JSON. This JSON response will not download\nthe entire image but will have a link to the image. Let’s look at a\nJSON representation of a fake profile I’ve setup on Gravatar: { \"entry\" :[{ \"id\" : \"61443191\" , \"hash\" : \"756b5a91c931f6177e2ca3f3687298db\" , \"requestHash\" : \"756b5a91c931f6177e2ca3f3687298db\" , \"profileUrl\" : \"http:\\/\\/gravatar.com\\/jsonguerilla\" , \"preferredUsername\" : \"jsonguerilla\" , \"thumbnailUrl\" : \"http:\\/\\/1.gravatar.com\\/avatar\\/756b5a91c931f6177e2ca3f3687298db\" , \"photos\" :[{ \"value\" : \"http:\\/\\/1.gravatar.com\\/avatar\\/756b5a91c931f6177e2ca3f3687298db\" , \"type\" : \"thumbnail\" }], \"name\" :{ \"givenName\" : \"JSON\" , \"familyName\" : \"Schema\" , \"formatted\" : \"JSON Schema Blogpost\" }, \"displayName\" : \"jsonguerilla\" , \"urls\" :[] }] } In this JSON response, the images are represented by hyperlinks but they\nare encoded as strings. Although this example is for a JSON object\nreturned from a server, this is how traditional APIs handle input as\nwell. This is due to the fact that JSON natively does not provide a way\nto handle hyperlinks; they are only Strings. JSON hyperschema attempts to specify a way to have a more semantic\nway of representing hyperlinks and images. It does this by defining\nkeywords (as JSON properties) such as links , rel , href . Note\nthat this specification does not try to re-define these words in\ngeneral (as they are defined in HTTP protocol already) but it tries to\nnormalize the way those keywords are used in JSON. Drafts The schema is still under development and the progress can be\ntracked by comparing the versions known as “drafts”. Currently, the\nschema is in the 4th version. The validation keywords can be dropped or\nadded between versions. This article — and many more over the interwebs\n— refer to the 4th version of the draft. Usage Let’s build a basic JSON API that accepts the following data with some\nconstraints: A post ID. This is a number and is a required parameter. Some free-form text with an attribute of body . This is a required\nparameter. A list of tags with an attribute of ‘tags’. Our paranoid API cannot\naccept more than 6 tags though. This is a required parameter. An optional list of hyperlinks with an attribute of ‘references’ Let’s face it, almost every app you might’ve ever written must’ve had\nsome or the other constraints. We end up repeating the same verification\nlogic everytime . Let’s see how we can simplify that. We will be using Sinatra for building the API. This is the basic\nstructure of our app.rb : require 'sinatra' require 'sinatra/json' require 'json-schema' post '/' do end The Gemfile : gem 'sinatra' gem 'sinatra-contrib' gem 'json-schema' We will be using the JSON-Schema gem for the app.\nLet’s look at the schema that we will define in a schema.json file: { \"$schema\" : \"http://json-schema.org/draft-04/schema#\" , \"type\" : \"object\" , \"required\" : [ \"id\" , \"body\" , \"tags\" ], \"properties\" : { \"id\" : { \"type\" : \"integer\" }, \"body\" : { \"type\" : \"string\" }, \"tags\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"maxItems\" : 6 }, \"references\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" , \"format\" : \"uri\" } } } } The properties attribute holds the main chunk of the schema\ndefinition. This is the attribute under which each of the individual\nAPI attribute is explained in the form of a schema of it’s own. The required attribute takes in a list of strings that mention\nwhich of the API parameters are required. If any of these parameters\nis missing from the JSON input to our app, an error will be logged\nand the input won’t get validated. The type keyword specifies the schema type for that particular\nblock. So, at the first level, we say it’s an object (analogous to\na Ruby Hash). For the body , tags and references , the types are string , array and array respectively. In case an API parameter can accept an array, the items inside that\narray can be explained by a schema definition of their own. This is\ndone by using an items attribute and defining how each of the item\nin the array should be validated. The format attribute is a built-in format for validation in the\nJSON Schema specification. This alleviates the pain of adding regex\nfor validating common items like uri , ip4 , ip6 , email , date-time and hostname . That’s right, no more copy-pasting URI\nvalidation regexes from StackOverflow. The $schema attribute is a non-mandatory attribute that specifies\nthe type of the schema being used. For our example, we will be using\nthe draft#4 of the JSON Schema spec. To use this schema in our app, we will create a helper method that uses\nvalidates the input with the schema we just defined. The json-schema gem provides three methods for validation — a validate method that\nreturns either true or false , a validate! that raises an\nexception when validation of an attribute fails and a fully_validate method that builds up an array of errors similar to what Rails’ ActiveRecord#save method provides. We will be using the JSON::Validator.fully_validate method in our app\nand return a nicely formatted JSON response to the user if the\nvalidation fails. helpers do def validate ( json_string_or_hash ) JSON :: Validator . fully_validate ( 'schema.json' , json_string_or_hash ) end end Now, we can use this helper inside routes to check the validity of the\ninput JSON like so: post '/' do input = JSON . load ( request . body . read ) errors = validate ( input ) if errors . empty? json ({ message : \"The blog post has been saved!\" }) else status 400 json ({ errors : a }) end end If the input is valid, the errors object will be empty. Otherwise, it\nwill hold a list of errors. This object will be returned as a JSON\nresponse with the appropriate HTTP status code. For example, if we run\nthis app and send in a request with a missing id parameter, the\nresponse will be something similar to the following: [ \"The property '#/' did not contain a required property of 'id' in schema schema.json#\" ] Let’s say if we send in a request with id having a string parameter.\nThe errors object will hold the following: [ \"The property '#/id' of type String did not match the following type: integer in schema schema.json#\" ] Last example. Let’s try sending a references parameter with a\nmalformed URI. We will send the following request: { \"id\" : 1 , \"body\" : \"Hello, Universe\" , \"tags\" : [ \"start\" , \"first\" ], \"references\" : [ \"data:image/svg+xml;base64 C==\" ] } (This input is in the file not_working_wrong_uri.txt ) curl \\ -d @not_working_wrong_uri.txt\n  -H 'Content-Type: application/json' \\ http://localhost:4567 The output of this would be: [ \"The property '#/references/0' must be a valid URI in schema schema.json#\" ] Thus, with a really simple validation library and a standard that\nlibrary implementers in different languages use, we can achieve input\nvalidation with a really simple setup. One really great advantage of\nfollowing a schema standard is that we can be sure about the basic\nimplementation no matter what the language which might implment the\nschema. For example, we can use the same schema.json description with a JavaScript library for validating the user input — for\nexample, in the front-end of the API we’ve just built. Summary The full app, some sample input files are present in this\nrepo . The json-schema gem is not yet official\nand might have some unfinished components — For example, the format validations of hostname and email for a string type have not been\nimplemented yet — and the JSON Schema specification itself is under\nconstant revisions. But that doesn’t mean it’s not ready for usage. Few\nof our developers use the gem in one of our projects and are pretty\nhappy with it. Try out the gem and go through the specfication to gain\nan idea of why this would be beneficial yourself. More Reading Understanding JSON Schema JSON Schema Documentation This excellent article by David Walsh JSON Schema Example : This example uses more\nkeywords that weren’t discussed in this post. For example, title and description . If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2014-04-05"},
{"website": "Codemancers", "title": "what you need to know about software process and tools as a non tech product owner part 2", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2014-06-11-what-you-need-to-know-about-software-process-and-tools-as-a-non-tech-product-owner-part-2/", "abstract": "What you need to know about software process and tools as a non-tech product owner - Part 2 Written by Hemant on June 11, 2014; tagged under outsourcing , startup , india In the first part, we discussed the design and project management aspects of the software development process. Here is the link to the complete post . If you have not already read it, please consider reading it first for better continuity. Code Hosting & Version Control The code that programmers write is generally stored in text files. Over time, as they add features, the code in these files is modified and every modification renders a new version of your product. At some point in the future if you decide that some of the modifications were a bad choice, and that you want to go back to a previous version, it cannot be easily done without what is called a ‘Version control software’. In short, it is the time machine for your product which lets you go back to past versions without much trouble. At Codemancers we use Git, a popular opensource version control software. For a further understanding of the basics of version control and the options available, read here . When code is written, the files are created and stored locally. But it is a good practice to use a code hosting service like Github (or Bitbucket or SourceForge ) to store code because it provides a reliable backup, makes it easy to review changes, and have discussions about the code itself. Github Github has become an important part of the development cycle not just because it is a great place to host code, but also because of the way it integrates with the rest of the tools in the development process. We develop features or modify existing ones as guided by the stories in the project’s Pivotal Tracker. These additions/changes in code are then uploaded to the code repository on Github. These code submissions (in the context of version control) are called commits. There is an option to add a message to every commit to make it easier for other developers to know what has been changed. We also add the Pivotal Tracker story ids in our commits. This helps in pulling out code commits associated with a specific story — another name for a new feature for the product in Pivotal Tracker parlance. The advantage of doing it is that if, some months from now, someone looks for answers on why a particular commit was introduced, he/she will be able to extract the Pivotal Tracker id (or Jira id as in the screenshot) from the commit message and find out complete details about the bug or feature for which the commit was introduced. This can also be done if you are using Github issues instead of Pivotal Tracker for project management. A screenshot of the initial wireframing of a project done using Balsamiq Continuous Integration Continuous integration is the process of integrating incremental code changes to the product on an ongoing basis instead of integrating everything as one big change. This process is particularly critical if your product is already live. We have outlined the need for continuous integration and continuous deployment in a post about design and development process . Jenkins There are quite a few tools available for automating and managing the process of continuous integration. We use Jenkins because we can host it ourselves and it has a large ecosystem of plugins which allows us to configure build-pipelines, auto-deployments and quite a lot of things which are simply not possible with other proprietary solutions. Since we also use Github pull requests for code reviews, we absolutely love the Jenkins integration with Github. Code Review Code climate & Coveralls Tools like Codeclimate and Coveralls offer feedback on code quality and information on test coverage respectively. Even if you don’t understand everything about the code written, the feedback from these tools should help you check if the team is at-least sticking to some of the software development best practices. As for the feedback itself, if you find something amiss, we would encourage you to have a discussion with your team than jumping to conclusions about the quality of the code. The boilerplate reviews are not always right but they offer a decent guideline for a healthy discussion. Github integrates with these review tools, thus making it possible to run a review automatically every time there is a pull request in Github. These tools help us check if the code has proper coverage, if the code metrics look good and if we have a green build in general. This feedback on every pull request is valuable in fixing things as we make progress instead of struggling with the software in its entirety at a later stage. General tools for communication When team members are geographically distributed, having private chat rooms go a long way in bringing a semblance of proximity to discussions. Having a system for internal communication also ensures that every conversation gets documented for future reference. HipChat There are several chat tools available — Hipchat , Flowdock , Campfire , Slack are some of the popular ones. We use Hipchat. It is free and it integrates with Github and the other tools that we use. We have notifications set up in Hipchat for all activities — from code commits to comments on pivotal stories. The basic idea behind the tight integration is to get every member on the team up to date on the progress without the need for verbal communication through meetings and conference calls. While it would be absolutely necessary to sync up via phone calls every now and then, majority of the communication should be done using asynchronous communication tools. This ensures that everything is documented for reference and nothing is lost in translation. This post and the previous one are intended to give you a taste of the entire development process. We have explained the ‘What’s and ‘Why’s of the process than the ‘How’s. If you have questions or need elaboration on anything in particular, please feel free to mention that in your comments. We will be happy to address those in our forthcoming posts. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2014-06-11"},
{"website": "Codemancers", "title": "bower with rails", "author": ["Yuva"], "link": "https://crypt.codemancers.com/posts/2014-12-10-bower-with-rails/", "abstract": "Cake walk: Using bower with rails Written by Yuva on December 10, 2014; tagged under rails , assets , bower , sprockets Traditionally, in order to use any javascript library (js lib), Rails users\nwill do either of these two things: Directly copy paste js lib source into vendor/assets/javascript folder Create a gem, add js lib source to gem, and reuse that gem. First option is problematic because users have to keep track of changes to\njs lib, and update them whenever required. Second option delegates this\nresponsibility to gem author, and users will just bump up gem version\nand most of the times assume that latest gem will have latest js lib. Both\nthese approaches have problems because everytime js lib author improves lib\neither users have to copy sources or gem authors have to make a new release. Of late, creating js libs and distributing them through bower has\ngained lot of traction. There are different ways to use bower with Rails.\nA popular way is to use bower rails gem. This blog will not\nuse this gem, but explores sprockets inbuilt support for bower. Sprockets - Bower support Sprockets has support for bower. It doesn’t do package management on its\nown, but it can understand bower package structure, and pick up js, and\ncss files from bower package. Lets go through this simple example: Create a simple rails app. Just use rails new , nothing new here. Install Faker js lib and use it in rails app. Setting up bower json file Packages installed from bower need to be specified in a bower.json file.\nRun bower init command at the root of rails app to generate this file.\nThis file will be checked into version control, so that other devs can\nalso know about dependencies. > bower init\n? name: rails app\n? version: 0.0.0\n? description:\n? main file:\n? what types of modules does this package expose?:\n? keywords:\n? authors: Yuva <yuva@codemancers.com>\n? license: MIT\n? homepage: rails-app.dev\n? set currently installed components as dependencies?: No\n? add commonly ignored files to ignore list?: No\n? would you like to mark this package as private which prevents\n  it from being accidentally published to the registry?: Yes { name: 'rails app' ,\n  version: '0.0.0' ,\n  homepage: 'rails-app.dev' ,\n  authors: [ 'Yuva <yuva@codemancers.com>' ] ,\n  license: 'MIT' ,\n  private: true } ? Looks good?: Yes One of the important things to note here is to mark this package as private\nso that its not published by mistake. The generated bower.json file can be\nfurther edited, and un-necessary fields like homepage, author can be removed. { \"name\" : \"rails app\" , \"version\" : \"0.0.0\" , \"private\" : true } Note: This is a one time process. Setting up .bowerrc file Since rails automatically picks up assets from fixed locations, bower can be\ninstructed to install packages to one of the pre defined locations. Create a .bowerrc file like this: { \"directory\" : \"vendor/assets/javascripts\" } Since bower brings in third party js libs, its recommended to put them under vendor folder. Note: This is a one time process Installing Faker js lib and using it Use bower install to install above said lib. Since .bowerrc defaults the\ndirectory to vendor/assets/javascript , Faker will be installed under this\ndirectory. Use --save option with bower to update bower.json . > bower install Faker --save Thats it! Faker lib is installed. Add an entry in application.js and use\nthe lib. //= require Faker Note: Make sure that its just Faker and not Faker.js . More details on\nwhy no extension will be explained later in the blog post. What just happened? How did it work? The vendor/assets/javascript folder has a folder called Faker , and\nthat folder does not have any file called Faker.js . Inspecting any page\nfrom the rails app, script tab looks like this: Looking at source code of Faker , there is a file called faker.js , and is under build/build folder. How did rails app know\nthe location of this file, even though application.js doesnot have explicit\npath? This is where sprockets support for bower kicks in: Sprockets will search for Faker in asset paths. If found, it checks whether its a file or a directory. In the rails app,\nbower installed this lib under directory Faker . If directory, it checks if the directory contains bower.json file.\nIn the rails app, Faker folder does have bower.json file. It decodes json file, and parses main field containing files and\nreturns the desired assets. Now, bower.json for Faker has explicit path for faker.js , which will\nbe returned by sprockets { \"name\" : \"faker\" , \"main\" : \"./build/build/faker.js\" , \"version\" : \"2.0.0\" , # ... } Bonus: Digging into sprockets source code First, sprockets will populate asset search paths. When sprockets sees require Faker in application.js , it checks for extension, and since\nthere is no extension, ie .js , asset search paths will be populated\nwith 3 paths: Faker/.bower.json Faker/bower.json Faker/components.json Gist of the source: def search_paths paths = [ pathname . to_s ] # optimization: bower.json can only be nested one level deep if ! path_without_extension . to_s . index ( '/' ) paths << path_without_extension . join ( \".bower.json\" ) . to_s paths << path_without_extension . join ( \"bower.json\" ) . to_s # DEPRECATED bower configuration file paths << path_without_extension . join ( \"component.json\" ) . to_s end end Source code is here: populating asset paths Second, while resolving require Faker , if bower.json file is found,\nsprockets will parse the json file, and fetches main entry. Gist of\nthe source: def resolve ( logical_path , options = {}) args = attributes_for ( logical_path ) . search_paths + [ options ] pathname = Pathname . new ( path ) if %w( .bower.json bower.json component.json ) . include? ( pathname . basename . to_s ) bower = json_decode ( pathname . read ) yield pathname . dirname . join ( bower [ 'main' ] ) end end Source code is here: resolving bower json If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2014-12-10"},
{"website": "Codemancers", "title": "diy ruby cpu profiling part ii", "author": ["Emil Soman"], "link": "https://crypt.codemancers.com/posts/2015-03-12-diy-ruby-cpu-profiling-part-ii/", "abstract": "DIY Ruby CPU profiling - Part II Written by Emil Soman on March 12, 2015; tagged under ruby , cpu , profiler In Part I we learned what\nCPU profiling means and also about the two modes of CPU profiling. In this\npart we’re going to explore CPU time and Wall time, the units\nused to measure execution cost. We’ll also write some code to get these\nmeasurements as the first step towards building our CPU profiler. Part II. CPU time and Wall time Wall time Wall time is just the regular real world time that elapses between a method call\nand return. So if you were to measure the “Wall clock time” taken for a method\nto run, it would be theoretically possible to do so with a stopwatch. Just\nstart the stopwatch when the method starts, and stop when the method returns.\nThis is also called real time. One important point about wall time is that it’s unpredictable and you may\nget different results every time you try to measure the same piece of code. This is because\nwall time is affected by the number of processes running in the background. When\nthe CPU has to work on a bunch of processes at the same time, the operating system\ndoes a \" scheduling \" of the processes which are running at the time and tries to give a fair share\nof CPU to each of them. This means the total time spent by the CPU is divided\ninto many slices and our method gets only some of these slices and not all of them.\nSo while the wall clock ticks away, our process may be sitting idle and giving\nway for other processes which are running in parallel. This means the time spent\non other processes will add to our Wall time too! CPU time CPU time is the time for which the CPU is dedicated to run the method.\nCPU time is measured in terms of CPU cycles(or ticks) which are used to\nexecute the method. We can convert this into time if we know the frequency of the\nCPU in units of cycles per second aka Hertz. So if the CPU took x ticks to execute\na method, and the frequency of the CPU is y hertz, the time taken by the CPU\nto execute the method = x/y seconds. Sometimes the OS does this conversion for\nus so we don’t have to do this calculation by ourselves. CPU time will not be equal to Wall time. The difference depends on the type of\ninstructions in our method. We can categorize the instructions\ninto broadly 2 types: CPU bound and I/O bound . When I/O instructions are\nbeing executed, the CPU becomes idle and can move on to process other CPU\nbound instructions. So if our method has a time consuming I/O instruction,\nthe CPU stops spending time on our method and moves on to something else until\nthe I/O operation is completed. During this time the Wall time keeps ticking\nbut the CPU time stops and lags behind Wall time. Let’s say a very slow running method took 5 minutes on your clock to finish\nrunning. If you were to ask how much time was spent on the method, your\nwall clock would say “It took 5 minutes to run this method”, but the CPU would\nsay “I spent 3 minutes of my time on this method”. So who are you going to listen\nto? Which time more accurately measures the cost of executing the method? The answer is, it depends™ . It depends on the kind of method you want to\nmeasure. If the method spends most of its time doing I/O operations, or it\ndoesn’t deal with CPU bound instructions directly, the cost of execution depicted\nby CPU time is going to be grossly inaccurate. For these types of methods, it\nmakes more sense to use Wall time as the measurement. For all other cases, it’s\nsafe to stick with CPU time. Measuring CPU time and Wall time Since we’re going to write a CPU profiler, we’ll need a way to measure CPU time\nand wall time. Let’s take a look at the code in Ruby’s Benchmark module which already measures CPU time and Wall time. def measure ( label = \"\" ) # :yield: t0 , r0 = Process . times , Process . clock_gettime ( BENCHMARK_CLOCK ) yield t1 , r1 = Process . times , Process . clock_gettime ( BENCHMARK_CLOCK ) Benchmark :: Tms . new ( t1 . utime - t0 . utime , t1 . stime - t0 . stime , t1 . cutime - t0 . cutime , t1 . cstime - t0 . cstime , r1 - r0 , label ) end So Ruby uses 2 methods from the Process class to measure time : times to measure CPU time clock_gettime to measure real time aka Wall time But unfortunately the resolution of time returned by the times method is 1 second which means if we use times to measure CPU time in our profiler, we’ll only be able to profile\nmethods that take a few seconds to complete. clock_gettime looks interesting,\nthough. clock_gettime Process::clock_gettime is a method added in Ruby 2.1 and\nit uses POSIX clock_gettime() function and falls back\nto OS specific emulations to get the value of time in case clock_gettime is\nnot available in the OS or when the type of clock we are measuring with clock_gettime is not implemented in the OS.\nThis function accepts a clock_id and the time resolution as arguments. There are a bunch of clock_ids\nyou could pass in to pick the kind of clock to use, but the ones we’re interested in are: CLOCK_MONOTONIC : This clock measures the elapsed wall clock time since an\narbitrary point in the past and is not affected by changes in the system clock.\nPerfect for measuring Wall time. CLOCK_PROCESS_CPUTIME_ID : This clock measures per-process CPU time, ie the time\nconsumed by all threads in the process. We can use this to measure CPU time. Let’s make use of this and write some code: module DiyProf # These methods make use of `clock_gettime` method introduced in Ruby 2.1 # to measure CPU time and Wall clock time. def self . cpu_time Process . clock_gettime ( Process :: CLOCK_PROCESS_CPUTIME_ID , :microsecond ) end def self . wall_time Process . clock_gettime ( Process :: CLOCK_MONOTONIC , :microsecond ) end end We could use these methods to benchmark code: puts \"****CPU Bound****\" c1 , w1 = DiyProf :: cpu_time , DiyProf :: wall_time 10000 . times do | i | Math . sqrt ( i ) end c2 , w2 = DiyProf :: cpu_time , DiyProf :: wall_time puts \"CPU time \\t = \\t #{ c2 - c1 } \\n Wall time \\t = \\t #{ w2 - w1 } \" puts \" \\n ****IO Bound****\" require 'tempfile' c1 , w1 = DiyProf :: cpu_time , DiyProf :: wall_time 1000 . times do | i | Tempfile . create ( 'file' ) do | f | f . puts ( i ) end end c2 , w2 = DiyProf :: cpu_time , DiyProf :: wall_time puts \"CPU time \\t = \\t #{ c2 - c1 } \\n Wall time \\t = \\t #{ w2 - w1 } \" Running this code would give an output similar to this: ****CPU Bound****\nCPU time\t=\t5038\nWall time\t=\t5142\n\n****IO Bound****\nCPU time\t=\t337898\nWall time\t=\t475864 This clearly shows that on a single CPU core, CPU time and Wall time are nearly\nequal when running purely CPU bound instructions whereas CPU time is always\nless that Wall time when running I/O bound instructions. Recap We’ve learned what CPU time and Wall time mean, their differences, and when to use\nwhich. We also wrote some Ruby code to measure CPU time and Wall time which would\nhelp us measure time in the CPU profiler we’re building. In part 3 we’ll\ntake a look at Ruby’s tracepoint API and use it to build an instrumentation\nprofiler. Thanks for reading! If you would like to get updates about subsequent\nblog posts in this DIY CPU profiler series, do follows us on twitter @codemancershq . If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2015-03-12"},
{"website": "Codemancers", "title": "diy ruby cpu profiling part i", "author": ["Emil Soman"], "link": "https://crypt.codemancers.com/posts/2015-03-06-diy-ruby-cpu-profiling-part-i/", "abstract": "DIY Ruby CPU profiling - Part I Written by Emil Soman on March 6, 2015; tagged under ruby , cpu , profiler At Codemancers, we’re building Rbkit ,\na fresh code profiler for the Ruby language with tonnes of cool features.\nI’m currently working on implementing a CPU profiler inside rbkit gem which would\nhelp rbkit UI to reconstruct\nthe call graph of the profiled ruby process and draw useful visualizations\non the screen. I learned a bunch of new things along the way and I’d love\nto share them with you in this series of blog posts. We’re going to start from the fundamentals and step by step, we’re going to\nwrite a rudimentary CPU profiler for Ruby ourselves! By the time we finish\nwe would learn : What’s CPU profiling Profiling modes - Instrumentation and Sampling CPU time and Wall time - what they mean and how to measure them Writing a simple C extension and using it in Ruby land Ruby tracepoints - call and return Signal handling in C Pausing Ruby process using a signal and peeking at the call stack Some useful and silly experiments using profiling data Part I. An Introduction to CPU Profiling By doing a CPU profile of your program, you can find out how expensive\nyour program is with respect to CPU usage. In order to profile your program,\nyou’ll need to use a profiling tool and follow the following steps : Start CPU profiling Execute the code you want to profile Stop CPU profiling and get profiling result Analyze result By analyzing the profiling result, you can find the bottlenecks which slow\ndown your whole program. Profiling modes CPU profiling is done in broadly 2 ways: 1. Instrumentation In this mode, the profiling tool makes use of some hooks, either provided by the\ninterpreter or inserted into the program, to understand the call graph and measure\nthe execution time of each method in the call graph. For example, consider the following piece of Ruby code : def main 3 . times do find_many_square_roots find_many_squares end end def find_many_square_roots 5000 . times { | i | Math . sqrt ( i )} end def find_many_squares 5000 . times { | i | i ** 2 } end main I’ve inserted some comments to help understand how the hooks would get executed\nif the Ruby interpreter gave us method call and return hooks: def main # method call hook gets executed 3 . times do find_many_square_roots find_many_squares end # method end hook gets executed end def find_many_square_roots # method call hook gets executed 5000 . times { | i | Math . sqrt ( i )} # method end hook gets executed end def find_many_squares # method call hook gets executed 5000 . times { | i | i ** 2 } # method end hook gets executed end main Now if we could print the current time and the name of the current method inside\nthese hooks, we’d get an output which looks somewhat like this : sec:00 usec:201007\tcalled  \tmain\nsec:00 usec:201108\tcalled  \tfind_many_square_roots\nsec:00 usec:692123\treturned\tfind_many_square_roots\nsec:00 usec:692178\tcalled  \tfind_many_squares\nsec:00 usec:846540\treturned\tfind_many_squares\nsec:00 usec:846594\tcalled  \tfind_many_square_roots\nsec:01 usec:336166\treturned\tfind_many_square_roots\nsec:01 usec:336215\tcalled  \tfind_many_squares\nsec:01 usec:484880\treturned\tfind_many_squares\nsec:01 usec:484945\tcalled  \tfind_many_square_roots\nsec:01 usec:959254\treturned\tfind_many_square_roots\nsec:01 usec:959315\tcalled  \tfind_many_squares\nsec:02 usec:106474\treturned\tfind_many_squares\nsec:02 usec:106526\treturned\tmain As you can see, this output can tell us how much time was spent inside each\nmethod. It also tells us how many times each method was called. This is roughly\nhow instrumentation profiling works. Pros : High accuracy We get method invocation count Easy to implement Cons : Extra overhead of executing hooks for each and every method that’s profiled 2. Sampling In this mode of profiling, the profiler interrupts the program execution once\nin every x unit of time, and takes a peek into the call stack and records what\nit sees(called a “sample”). Once the program finishes running, the profiler\ncollects all the samples and finds out the number of times each method appears\nacross all the samples. Hard to visualize? Let’s look at the same example code and see how different\nthe output would be, if we used a sampling profiler. Output from a sampling profiler would look like this : Call stack at 0.5sec: main/find_many_square_roots\nCall stack at 1.0sec: main/find_many_square_roots\nCall stack at 1.5sec: main/find_many_square_roots\nCall stack at 2.0sec: main/find_many_squares In this example, the process was interrupted every 0.5 second and the call stack\nwas recorded. Thus we got 4 samples over the lifetime of the program and out of\nthose 4 samples, find_many_square_roots is present in 3 and find_many_squares is present in only one sample. From this sampling, we say that find_many_square_roots took 75% of the CPU where as find_many_squares took only 25%. That’s roughly\nhow sampling profilers work. Pros : Negligible overhead compared to instrumentation profiling Easy to find slow/long running methods Cons : Not good at measuring short running methods We don’t get method invocation count Not easy to write a sampling profiler on your own Recap We just looked into what CPU profiling means and the 2 common strategies of\nCPU profiling. In part 2 , we’ll explore the 2 units of measuring CPU\nusage - CPU time and Wall time. We’ll also get our hands dirty and write\nsome code to get these measurements. Thanks for reading! If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2015-03-06"},
{"website": "Codemancers", "title": "dawn of the kitchen sink", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2012-05-08-dawn-of-the-kitchen-sink/", "abstract": "The kitchen sink libraries Written by Hemant on May 8, 2012; tagged under ruby , rails , gems There was a time when Ruby libraries where like small Unix utilities, they did one thing and\nthey did it well. If you go read code of some of the older plugins, it will be quite simple\nand straightforward affair. Today however trend is, to write kitchen sink libraries that do a lot. You want file upload,\nthey work at Rack level and support not only Rails but all the rack complaint frameworks under\nthe sun, not only relational databases but Mongo as well, not just local file storage\nbut S3/Cloudfiles as well, not just file uploads but some kind of caching via Rack::Cache as well. Now at its core, handling file uploads is very simple and straightforward affair. Uploading to S3 or\nCloudfiles is even simpler. I daresay, getting started with the kitchen sink library will\nbe easy and simple as well. But if one thing Law of leaky Abstractions has taught us that is - eventually you will have look under the hood of these kitchen sink\nlibraries and suddently MysqlAdapter which subclasses AbstractPersistenceAdapter won’t\nappear very attractive, because your application doesn’t gives two hoots about Mongo support.\nJumping through hooves to grab that uploaded file via Rack will appear waste of time as well\nbecause you are probably never going to need this working on framework other than Rails.\nAnd while support for Rack::Cache was nice, you can’t use it in production anyways. I don’t know about you, but I prefer gems and plugins that I include in my code easily grokkable.\nBelow is a time bomb taken from a popular file upload gem and good luck - if you will have\nto grok this code and fix something yourself. autoload_files_in_dir = proc do | path , namespace | # Define the module eval ( \"module #{ namespace } ; end\" ) # Autoload modules/classes in that module Dir . glob ( \" #{ path } /*.rb\" ) . each do | file | file = File . expand_path ( file ) sub_const_name = camelize [ File . basename ( file , '.rb' ) ] eval ( \" #{ namespace } .autoload(' #{ sub_const_name } ', ' #{ file } ')\" ) end # Recurse on subdirectories Dir . glob ( \" #{ path } /*/\" ) . each do | dir | sub_namespace = camelize [ File . basename ( dir ) ] autoload_files_in_dir [ dir , \" #{ namespace } :: #{ sub_namespace } \" ] end end If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . Next Post →", "date": "2012-05-08"},
{"website": "Codemancers", "title": "remove branches merged into master", "author": ["Yuva"], "link": "https://crypt.codemancers.com/posts/2013-03-17-remove-branches-merged-into-master/", "abstract": "Remove branches merged into master Written by Yuva on March 17, 2013; tagged under git Run the following command to remove all tracking branches that are merged to master git branch --merged master | sed /master/d | xargs -n 1 git branch -d If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-03-17"},
{"website": "Codemancers", "title": "git checkout previous branch", "author": ["Amitava"], "link": "https://crypt.codemancers.com/posts/2013-03-17-git-checkout-previous-branch/", "abstract": "Git checkout previous branch Written by Amitava on March 17, 2013; tagged under git Execute this command to go back to the previous branch git checkout - If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-03-17"},
{"website": "Codemancers", "title": "mountain lion", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2012-07-27-mountain-lion/", "abstract": "Upgrading to Mountain Lion and your development environment Written by Hemant on July 27, 2012; tagged under osx , rvm , ruby , mountain_lion Prem of thoughtbot already had some brief instructions about upgrading to mountain lion and then thanks\nto Kenny for adding much to those instructions . What follows is my own upgrade path: Take backup via Time Machine or superduper!. Upgrade using installer from App store and reboot. Install XCode 4.4 from App store. Looks like, if you don’t care much about developing iOS or OSX applications, you can skip this step. But I didn’t. Install command line tools of XCode as instructed in Prem’s blog post. Change permissions of Homebrew directory. sudo chown -R `whoami` /usr/local\n  brew update If you are like me, you still have few applications running on 1.8.7. Ruby versions before 1.9.3 can’t be easily compiled using llvm-gcc compiler. Install gcc-4.2 using homebrew: brew tap homebrew/dupes\n  brew install apple-gcc42 Configure apple-gcc42 as default compiler. Update your ~/.zshrc or ~/.bashrc and add following lines: export CC=/usr/local/bin/gcc-4.2 Above line should take care of compiling ruby itself and many native gems which use proper\ncompiler detection. Unfortunately this won’t work for few odd gems and hence, you also need\nto do: sudo ln -s /usr/local/bin/gcc-4.2 /usr/bin/gcc-4.2 Install XQuartz . This step may require you to logout and login again. Even if you already had ruby 1.8.7/1.9.3 compiled, I will urge you to recompile new one - just to ensure everything works perfectly.\nFor compiling Ruby 1.8.7/REE: export CPPFLAGS=\"-I/opt/X11/include\"\n  export CFLAGS=\"-I/opt/X11/include\" For compiling ruby: rvm reinstall ree\n  rvm reinstall 1.9.3 You should be able to compile ruby 1.9.3 without requiring any of above flags. Some recommend reinstalling key packages that you have installed using homebrew before ML upgrade.I did not bother to reinstall all the packages, but I did reinstall imagemagick . brew unlink imagemagick\n  brew install imagemagick At this point you should have a working ruby and rails setup. I faced a minor problem where\nI was running mysql using packages downloaded from Mysql website and they were installed in /usr/local and after I changed permissions to restore Homebrew setup, my Mysql installation\nbroke. To restore the sanity: sudo chown -R _mysql:_mysql /usr/local/mysql-5.xx In some of Rails applications, Nokogiri was throwing warning about being compiled against\ndifferent version of libxml2 . To get rid of those, I simply nuked Rails.root/vendor/ruby and reinstalled the gems my app was using via bundle install . Mark has suggested these instructions for getting pow running with Safari 6. I found that automatic reconnecting to wi-fi after coming out of sleep was broken. To fix this, I removed all the\nremembered locations and then in Configure Network Prefrences -> TCP/IP changed Configure IPV6 to Link-local only . I also noticed that external monitor when plugged in, won’t be detected at all.To fix this I had to reset my PRAM.\nYou can follow this article for more information on resetting PRAM. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2012-07-27"},
{"website": "Codemancers", "title": "bundle clean", "author": ["Yuva"], "link": "https://crypt.codemancers.com/posts/2013-03-27-bundle-clean/", "abstract": "Bundler: Cleanup old unused gems Written by Yuva on March 27, 2013; tagged under bundler , rails , ruby Recently I resumed an old project {after 2 months}. My peers are still working on\nit and they have upgraded lots of gems. After running bundle install , many gems\ngot upgraded and old version were still hanging around in vendor folder. This\nlittle handy command helped in removing them: bundle clean -V NOTE: Btw, I always install gems in ./vendor/ruby folder, and don’t maintain any\nrvm gemsets for projects. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-03-27"},
{"website": "Codemancers", "title": "poor mans rspec mode for emacs", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2013-03-28-poor-mans-rspec-mode-for-emacs/", "abstract": "Poor man's rspec mode for emacs Written by Hemant on March 28, 2013; tagged under emacs , rspec , rails For whatever reason rspec-mode didn’t work for me. I couldn’t either get it to use current version of Ruby or it used to crap out on Bundler detection. Annoyed, I decided to write a very simple rspec-mode that works almost always. More importantly\nit runs rspec from root of current project directory using your current shell and hence you\ndon’t need to configure RVM or rbenv from Emacs. Whatever version of Ruby your shell has\nit will run using that. As a bonus it runs rspec in a comint buffer that means if you are using pry or a debugger you can interact with it from Emacs itself. ( require 'compile ) ;; Find root directory by searching for Gemfile ( defun* get-closest-gemfile-root ( & optional ( file \"Gemfile\" )) ( let (( root ( expand-file-name \"/\" ))) ( loop for d = default-directory then ( expand-file-name \"..\" d ) if ( file-exists-p ( expand-file-name file d )) return d if ( equal d root ) return nil ))) ( defun rspec-compile-file () ( interactive ) ( compile ( format \"cd %s;bundle exec rspec %s\" ( get-closest-gemfile-root ) ( file-relative-name ( buffer-file-name ) ( get-closest-gemfile-root )) ) t )) ( defun rspec-compile-on-line () ( interactive ) ( compile ( format \"cd %s;bundle exec rspec %s -l %s\" ( get-closest-gemfile-root ) ( file-relative-name ( buffer-file-name ) ( get-closest-gemfile-root )) ( line-number-at-pos ) ) t )) ( add-hook 'enh-ruby-mode-hook ( lambda () ( local-set-key ( kbd \"C-c l\" ) 'rspec-compile-on-line ) ( local-set-key ( kbd \"C-c k\" ) 'rspec-compile-file ) )) An astute reader will notice that I am not using default ruby-mode . You can change the\nhook to ruby-mode-hook if you are using default ruby-mode . So last few lines will become: ( add-hook 'ruby-mode-hook ( lambda () ( local-set-key ( kbd \"C-c l\" ) 'rspec-compile-on-line ) ( local-set-key ( kbd \"C-c k\" ) 'rspec-compile-file ) )) If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-03-28"},
{"website": "Codemancers", "title": "profile ruby apps dtrace part1", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2013-04-16-profile-ruby-apps-dtrace-part1/", "abstract": "Profile Ruby 2.0 apps using DTrace - Part 1 Written by Hemant on April 16, 2013; tagged under ruby , dtrace , profiling , rails This is a series of blog posts we are going to do in tutorial format, this is Part 1 of our tutorial on Dtrace. DTrace is awesome. If your operating system supports it, using DTrace can give incredible insight into processes\nthat are running on your system. If you are still not convinced about its usefullness you should checkout - How Dtrace helped twitter identify bottlenecks , Profiling Rails startup with Dtrace . So what exactly is DTrace? According to Wikipedia - “DTrace is a dynamic tracing framework that allows users to tune and troubleshoot applications and the OS itself.” It is available on Solaris, Mac OSX, FreeBSD and recent versions of Oracle Linux . If you are running any of the above\noperating systems, there is nothing to install - just hop in. Dtrace allows you to instrument a live process by defining trace points called probes, which can be turned on or off dynamically.\nThe probes fire when a particular condition is met and it requires no change in application code, no special flags and there is\nvery little penalty of running a production app with probes turned on. What is a probe? At its core a probe is like a light bulb that switches on when a condition is met. The condition could be entering a C function\ndefined within a module, exiting a C function, entering a system call provided by Kernel etc. A probe is made of 5 parts, ID - Internal ID of the probe. Provider - Name of the provider. Module - The name of the UNIX module or Application. Function - The name of function in which probe exists. Name - Name of the probe (as chosen by author of probe). All the processes currently running on operating system (including the kernel itself) have some probes defined. As an example, lets try and list all the probes defined by Ruby 2.0 ~> sudo dtrace -l | grep ruby Running Dtrace Dtrace probes can be set and run via the command line itself or can be run via Dtrace scripts\nwritten in a programming language called D. Some simple command line examples: ~> sudo dtrace -f write Above probe will start firing each time any application makes write system call. Dtrace scripting capabilities The real power of Dtrace lies in ability to script - tracing of running applications, gathering\nresults and then printing or plotting them. A DTrace script is essentially made of 3 parts: probe description\n/predicate/\n{\n    action;\n} D language has some similarity with C but at its core it is a scripting language, probably closer to awk etc. For example, lets write a D script for printing all system calls being made by currently running applications. ~> cat > dtrace_scripts/syscall.d\nsyscall:::entry\n{\n  printf(\"%s(%d) called %s\\n\", execname, pid, probefunc)\n} Above script when ran via sudo dtrace -qs syscall.d will start printing name,pid of processes that are making system calls and what system calls they are making. The script does not have\npredicate, but only probe description. Probe description A probe description can be written as: provider:module:function:probename Few simple examples: # A probe that will fire each time a system call called write is entered\nsyscall::write:entry\n# This probe will fire each time someone creates a new ruby array\nruby*::ary_new:: As you can see, you can skip parts of the probe and can use wildcards such as * or ? . That about sums up content for part 1 of DTrace tutorial. In the next part we are going to cover predicates, aggregates and we will do profiling of real Ruby applications.I will recommend you to play with\nvarious probes and stay tuned for Part 2 which is coming up very soon. You should also follow us on twitter @codemancershq if you don’t want to miss out Part 2. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-04-16"},
{"website": "Codemancers", "title": "tmux zoom pane", "author": ["Amitava"], "link": "https://crypt.codemancers.com/posts/2013-03-31-tmux-zoom-pane/", "abstract": "Zoom pane in tmux Written by Amitava on April 3, 2013; tagged under tmux , screen With tmux 1.8 it’s now easy to zoom a pane and use the full size of the window. Just press <prefix> z to zoom the pane and the same keys again to restore it. Here is a screenshot of the new feature which shows a three pane layout and how the first pane running vim is maximized and restored. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-04-03"},
{"website": "Codemancers", "title": "rails autoload path", "author": ["Yuva"], "link": "https://crypt.codemancers.com/posts/2013-05-01-rails-autoload-path/", "abstract": "Rails Autoload Path Written by Yuva on May 1, 2013; tagged under rails Typically autoload paths configured in application.rb will be like this: config . autoload_paths += %W(lib) Recently we faced a problem where we were adding another root level folder app to autoload_paths , and using one of utility classes inside that folder. Everthing\nworked fine in development, but not in production. We started seeing strange\nerrors while running migrations: cannot load such file -- app/subdir1/utility_class Inspecting $LOAD_PATH revealed that there is no absolute path for app folder.\nAppending autoload_path with absolute path solved this issue. config . autoload_paths += [ Rails . root . join ( \"app\" ) ] If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-05-01"},
{"website": "Codemancers", "title": "backbone", "author": ["Amitava"], "link": "https://crypt.codemancers.com/posts/2013-04-06-backbone/", "abstract": "Using Backbone JS in multipage application Written by Amitava on April 6, 2013; tagged under rails , backbone Recently we worked on a cloud resouces management application written on rails\nfor one of our clients. Think about this app as GUI for Amazon AWS. All the\nresources will be displayed in tabular form and ajax requests for management\nlike creation, updation and deletion of resources. Soon application became more\ncomplex because of lots of server side javascript code directly interacting\nwith DOM. As a result testing became more complex, DOM changes used to break\njavascript code, and tests used to break often. This encouraged us to find a\nbetter alternative where we could use javascript in more modular fashion and\nalso test it. Hence we decided to use backbone.js Since we were already using coffeescript we added the rails-backbone gem\nas it has generators for creating scaffolds in coffeescript . Initially we\nwere using the generated code but as we became more confident we stopped using\nthe generator. Also we didn’t use the router as this was not a single page app\nwe were building. We were using backbonejs only for pages where we needed to\nimplement client-side filtering of tabular data. This worked really well than\nwe expected. Not only the filtering code was easier to maintain, it became quite\neasy to test as well. Did we mention we used jasmine for testing all our javascript\ncode :). Later we started to use backbone for forms and other pages as well. It was all working fine in terms of performance, but then we found a few challenges\nas the code-base was growing and becoming complex. Most views and models were\nusing similar code and logic so we decided to abstract common behaviour and logic\nin base classes and subclass our views and models to share the same code. For the entire\napplication we used the following flow - When the page is loaded it initializes the code to create a ListView . ListView then fetches the resources through collection and renders in a tabular\nformat. Each row in the table itself is rendered with a ModelView . Create and update forms are managed using a FormView . To begin with we created a ListView class with common features to fetch and dispaly\ndata in a table. class App . Views . ListView extends Backbone . View initialize: (options) -> # Here we bind collection events @collection . listenTo 'add' , 'onAdded' @collection . listenTo 'remove' , 'onRemoved' addAll: () => # Iterate the collection and call `@addOne` addOne: (model) => # Render each model inside the table renderEmpty: => # Render message if the collection is empty fetchCollection: (options)-> # This is where the data is fetched through a collection render: (options) => # Call `@addAll` to start the rendering process this Then we built the ModelView to extract all common features to display a single row\nand handle events. class App . Views . ModelView extends Backbone . View events: -> \"click  .edit\" : \"onEdit\" \"click  .destroy\" : \"onDestroy\" initialize: -> # Here we bind or initialize model events @model . listenTo 'change' , 'onChanged' onEdit: (e)=> # Handle displaying the edit form onDestroy: (e)=> # Handle deletion of a single model And finally there is the FormView for which is responsible for rendering the forms for\ncreating and updating resources. class CloudGui . Views . ModalFormView extends Backbone . View # Since we were using twitter-bootstrap modal, we set the dom attributes here. attributes: class : \"modal hide fade\" tabindex: \"-1\" \"data-backdrop\" : \"static\" \"data-keyboard\" : \"false\" events: -> \"submit       form\" : \"onFormSubmit\" \"ajax:success form\" : \"onFormSuccess\" \"ajax:error   form\" : \"onFormError\" onFormSubmit: => # It does client-side validation if required and then submits the # form using ajax. onFormSuccess: (event, data)=> # Here we add the newly created resource to the collection. # For update, it updates the model attributes. # In both the cases the `ModelView` automatically renders the changes. onFormError: (event, xhr, status, error) => # Handles server-side errors We did not use JSON format for submitting the data since we had to support file uploads. And also\nthe format of input attributes were different than the models. So we decided to use regular ajax\nform submission where the data is sent using form-url-encoded (or multipart/form-data for file uploads)\ncontent-type and the server returning JSON. The rest is then handled by onFormSuccess or onFormError . One important lesson we learnt is to be careful with handling events in Backbone. It can create memory\nleaks if the event binding is not done in a correct manner. We were using on to listen to collection\nor model events but then we found that it does not release the event handlers when the remove function\nis called on a view object. We used listenTo instead of on and all the events were unbound when\nthe view was removed. Using backbone.js outside single page application is easy and worked pretty well for us. It enabled us\nto have proper frontend architecture (rather than jquery callback spaghetti), it made unit testing frontend\nfeatures easier without resorting to a integration test framework (like Cucumber), we totally recommend\nthis approach if you are consuming bunch of JSON and don’t want to go whole hog single page. More to follow on other interesting aspects of backbone pagination and real time event handling. Watch out. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-04-06"},
{"website": "Codemancers", "title": "vim tabcompletion for rubymotion", "author": ["Amitava"], "link": "https://crypt.codemancers.com/posts/2013-05-01-vim-tabcompletion-for-rubymotion/", "abstract": "Vim tab completion for RubyMotion Written by Amitava on May 1, 2013; tagged under vim , rubymotion Here is a quick rundown on setting up tab completion in vim for RubyMotion. Configure ctags support Using ctags with vim can save us from\ntyping really long class or method name by hand. RubyMotion already includes\na rake task for generating ctags for a project, but first we need to ensure\nthe proper ctags executable is installed in the system. ctags can be installed\nusing a package manager. OSX users can inatall it using homebrew . ~> brew install ctags Generate RubyMotion ctags file After ctags is installed run the following rake task from inside the root of the\nRubyMotion project directory. ~> rake ctags This will generate the ctags file that we will use in vim to setup tab\ncompletion. Setup vim for tab completion supertab is a vim plugin which\nallows you to use Tab for all your insert completion needs. Once installed\nthere is no need to type those long class or method names anymore. Just type two\nor three characters and press Tab and it will use the ctags file to display a\nlist of matching names. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-05-01"},
{"website": "Codemancers", "title": "how to write a gem", "author": ["Yuva"], "link": "https://crypt.codemancers.com/posts/2013-05-30-how-to-write-a-gem/", "abstract": "All you need to know about writing a mountable Rails Gem with rspec Written by Yuva on May 30, 2013; tagged under gem , mountable , rspec , rails This article briefly talks about how to write a mountable rails gem and what are\nthe practices that I followed First things first Rails generates nice boilerplate for your gem which is sometimes hard to get it\nright in the first go. Use this simple command and you are good to go $ rails plugin new <gem-name> --mountable --skip-test-unit --dummy-path = spec/dummy The --mountable option helps in creating a mountable gem. The --skip-test-unit skips generation of test templates in test folder The --dummy-path specifies where the dummy test application {which we will be\nusing for testing} should be generated. It is usually created in test folder\nbut you want that to be created in spec folder so that you can use rspec Importance of dummy app Some of the gems that you develop which depend on rails need to run specs in rails\nenvironment. Since gem doesn’t have a rails env, it uses dummy app’s rails env to\nrun such tests. This is useful when you write specs for controllers, helpers etc. When you set up rspec by adding rspec-rails to your Gemfile , you need to add\nthese lines at the top of spec_helper.rb so that rspec loads rails env of dummy\napp. ENV [ \"RAILS_ENV\" ] ||= 'test' require File . expand_path ( \"../dummy/config/environment\" , __FILE__ ) General flow of development Never checkin Gemfile.lock in your git repository {or whatever SCM that you\nare using} First, add all the dependent gems that your gem needs in Gemfile and develop\nyour gem. Once you have finalized your dependent gems, move those gem declarations to gemspec file. Add all dependent gems with add_dependency method and add\nall development dependent gems with add_development_dependency method Gem :: Specification . new do | s | # your information s . add_dependency \"strong_parameters\" , \"~> 0.2.1\" s . add_development_dependency \"factory_girl_rails\" end Its important to note that you should require your dependent gems explicitly in the root file of your gem. Say if your gem is named my_cool_gem , then you\nshould have my_cool_gem.rb created inside lib folder. If your gem is\ndependent on strong_parameters , then you need to add these lines: require \"my_cool_gem/engine\" require \"strong_parameters\" module MyCoolGem end If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-05-30"},
{"website": "Codemancers", "title": "profile ruby 2 dot 0 apps using dtrace part 2", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2013-05-04-profile-ruby-2-dot-0-apps-using-dtrace-part-2/", "abstract": "Profile Ruby 2.0 apps using DTrace - part 2 Written by Hemant on May 27, 2013; tagged under ruby , dtrace , profiling , rails This is Part - 2 in series of articles we are writing on Dtrace. If you haven’t already - do read Part 1 . Running Dtrace scripts Before we delve deeper into guts of Dtrace, one of the questions which I often get asked is,\nHow can I profile one off programs (i.e not a application running in background) with Dtrace. There are basically couple of ways you can fire Dtrace. One of them is obviously straight up sudo dtrace -[fmp] [name] which we kind of covered in\nlast tutorial. Needless to say, you can’t run dtrace scripts this way and hence is not very useful. Second one is, you can supply Dtrace script as argument. This is useful for profiling long running applications (such as unicorn, thin) ~> sudo dtrace -qs <dtrace_script>.d Third one is, which I am very fond is: ~> sudo dtrace -s count.d -c '/Users/gnufied/.rbenv/versions/20/bin/ruby leaky_app.rb' What is a predicate? In last part of tutorial we described what a probe description looks like. For a refresh: probe description\n/predicate/\n{\n    action;\n} Predicate is a way to filter the number of probes fired based on certain critirea. For example, in last tutorial we wrote a Dtrace script to keep\ntrack of all system calls being made by all currently running applications. Lets say, now we want to restrict those probes only to system calls that are being made by Ruby applications. We can do this\nby introducing a predicate in our Dtrace script. syscall:::entry\n/execname==\"ruby\"/\n{\n\tprintf(\"%s(%d) called %s\\n\", execname, pid, probefunc);\n} Or you can use pid predicate: /*\n * Print system calls made by pid 61465\n *\n */\nsyscall:::entry\n/pid == 61465/\n{\n\tprintf(\"%s(%d) called %s\\n\", execname, pid, probefunc);\n} Now when I was learning Dtrace, I was mightly confused from where these things called execname , pid , probefunc are coming from? Turns out, Dtrace automatically provides built-in variable\ndefinitions for inspecting processes. They make Dtrace scripts quite powerful. Built-in Variables Here is a list of important built-in variables available in your DTrace scripts: execname - Name of current process. probeprov - Name of the probe provider (Read previous post about probes) probemod - Name of the module in which probe is defined. probefunc - Name of the function probename - Name of the probe arg0..arg9 - The function you are trying to probe likely takes arguments. For example, if you are profiling a Ruby program, you may be interesting in knowing about what kind of objects we are creating. When probes fire arguments accepted by the function (for which probe fired) are automatiaclly available as variables arg0 to arg9 in your DTrace script. cpu - CPU identifier tid - Thread identifier cwd - Name of current working directory uid - User ID gid - Groupd ID pid - Process ID timestamp - Current Timestamp errno - Set by system call errors Now, I believe previous scripts would have started making sense. Above list does not include all built-in variables though. Lets do an exercise and try to print number of bytes being read (from network or file system) by process with pid 61465. The script will look like: syscall::read:entry\n/pid == 61465/\n{\n    printf(\"%s read %d\\n\", execname, arg2);\n} If you crack open man 2 read you will find out that read system call’s third argument contains\nnumber of bytes being read. Having just ability to print numbers as probes fire is useful but you can easily imagine it would be nice, if we can somehow aggregate the data. For example, how much of your program’s memory is taken by string allocation or how much time is being spent in total in the function foo . Ability to aggregate such data would be really useful. Aggregates and Collections Agrregates, Collections and variables are most important tools DTrace has to offer for\ndata gathering, analyzing and printing. We will start with different types of\nvariables available to us. Variable types D scripting language has 3 types of variables which you can use to keep track of things. Block local You can define a block local variable with : syscall::read:entry\n{\n  this->currentTime = timestamp;\n} Scope of a block local variable is limited to the block in which it is defined. Global scalar variables You can define a global variable like: BEGIN {\n  startTime = timestamp;\n} Variables defined like this are global and they are available in all the probe descriptions. Also, if you haven’t\nguessed it - BEGIN and END [2] are two special probes that fire at the beginning of a DTrace script and at the end of it. They can be very\nuseful for initializing things or printing summary results at the end of execution. Thread local variables If you are trying to profile a multi-threaded application you will want to keep DTrace variables\nseparate for different threads. You can do that with thread local variables: syscall::read:entry\n{\n  self->currentTime = timestamp;\n} Variables declared like this won’t be shared between threads and hence can be used reliably. Associative Arrays If you are coming from Ruby, think of associative array as a Hash where that keys can be\na single value or a tuple of values. In either case once declared type of key shouldn’t change. For example,\nfollowing script will calcuate time spent by each process in a system call. syscall:::entry\n{\n  callTime[execname, probefunc] = timestamp;\n}\n\nsyscall:::return {\n    this->timeElapsed = timestamp - callTime[execname,probefun];\n} You can learn more about these different kind of variables at [1] . Aggregates Armed with knowledge of variables and collections, we can finally dip our toes in\naggregates. Aggregates is a way of gathering data from individual probes without\nyou the programmer doing actual work of summing things up,calculating the median, frequency of\noccurence etc. For example, lets write a DTrace script that prints number of top 20 objects Rails creates when it\nboots up. You are obviously going to need Ruby 2.0 for following script to work. /* rails_boot.d\n * object-create is the name of the probe that fires\n * when any new ruby object is created.\n * Corresponding function called is - rb_obj_alloc\n */\nruby*:::object-create\n{\n   /* If you look up code of rb_obj_alloc, first argument to\n    * function contains class which is being instantiated\n    */\n   @objects[copyinstr(arg0)] = count();\n}\n\nEND {\n  /* Truncate the aggregate to top 20 results */\n  trunc(@objects,20);\n  /* Print the aggregate */\n  printa(@objects);\n} The output should look something like: Mime::Type                                                       22\n  Psych::Nodes::Scalar                                             27\n  Bundler::LazySpecification                                       39\n  Set                                                              54\n  Sass::Script::Functions::Signature                               55\n  Rails::Paths::Path                                               71\n  Class                                                            73\n  Pathname                                                         74\n  Bundler::DepProxy                                                77\n  Rails::Initializable::Collection                                 93\n  Regexp                                                           95\n  Rails::Initializable::Initializer                               143\n  Range                                                           332\n  Gem::Specification                                              410\n  Hash                                                            579\n  Gem::Dependency                                                1660\n  Gem::Version                                                   2416\n  Gem::Requirement                                               3061\n  String                                                         4723\n  Array                                                          4948 Keep in mind, running above script isn’t as simple as running sudo dtrace -qs rails_boot.d -c 'rails s' . Depending on whether\nyou are using RVM or rbenv all those shell scripts shims create a problem when running dtrace scripts. What I ended up doing is,\ncreated a file called load_env.rb that had just one line require_relative config/environment and then ran the Dtrace script with: sudo dtrace -qs rails_boot.d -c \"/Users/gnufied/.rbenv/versions/20/bin/ruby load_env.rb\" Now there are some new keywords in above script and we will to get them in a bit, but first let us talk about aggregates. DTrace allows\nyou to store result of aggregate functions in objects called aggregates, they are defined like: @name[ keys ] = aggfunc ( args ); Aggregate object name starts with @ and behave very similar to associative arrays and hence can use tuples as keys. Using aggregate functions\nfrees you up from keeping counter yourself. In above script we are using count function which keeps track of number of times that particular probe fired.\nThere are other aggergate functions available as well: count - The number of times called. sum - The total value of the specified expressions. avg - The arithmetic average of the specified expressions. min - The smallest value among the specified expressions. max - The largest value among the specified expressions. lquantize - A linear frequency distribution, sized by the specified range, of the values of the specified expressions. Increments the value in the highest bucket that is less than the specified expression. quantize - A power-of-two frequency distribution of the values of the specified expressions. Increments the value in the highest power-of-two bucket that is less than the specified expression. You can find out more about aggregates from [3] . That about sums up content for part 2 of DTrace tutorial. In the next part we are going to cover in built functions, printing and profiling of real world Ruby applications.\nYou should also follow us on twitter @codemancershq if you don’t want to miss out Part 3. References [1] - http://docs.oracle.com/cd/E19253-01/817-6223/chp-variables/index.html [2] - http://stackoverflow.com/questions/11166477/dtrace-end-probe-never-fires [3] - http://docs.oracle.com/cd/E19253-01/817-6223/chp-aggs/index.html If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-05-27"},
{"website": "Codemancers", "title": "introducing invoker", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2013-06-10-introducing-invoker/", "abstract": "Introducing Invoker - Manage processes in development envionment Written by Hemant on June 8, 2013; tagged under ruby , productivity , gems If you are working on a large application with several dependencies such as - more than one application server, background job, it can get tedious to start and stop and check their logs in bunch of different places. Invoker is a small utility we wrote to manage all such processes from one place. Invoker owes much to Foreman for inspiration, however unlike Foreman it has been\nexclusively written to be used in development environment. Some major differences are: If one process managed by invoker crashes, it doesn’t crash entire process manager. Error reporting is much better. Crashed processes do not just exit with a cryptic error, but they will print their full error. To start/stop/restart individual process in the group, you no longer need to restart entire process manager. To get started we need to install invoker gem which will give us command line utility called invoker , we can do that via: gem install invoker Currently it only works with Ruby 1.9.3 and 2.0. After that we need to start by creating a ini file which will define processes we want to manage using invoker. For example: [rails]\ndirectory = /home/gnufied/god_particle\ncommand = zsh -c 'bundle exec rails s -p 5000'\n\n[dj]\ndirectory = /home/gnufied/god_particle\ncommand = zsh -c 'bundle exec ruby script/delayed_job'\n\n[events]\ndirectory = /home/gnufied/god_particle\ncommand = zsh -c 'bundle exec ruby script/event_server' After that we can start process manager via: ~> invoker start invoker.ini Above command will start all your processes in one terminal with their stdout/stderr merged and labelled. Now additionally we can control individual process by, # Will try to stop running delayed job by sending SIGINT to the process\n~> invoker remove dj\n\n# If Process can't be killed by SIGINT send a custom signal\n~> invoker remove dj -s 9\n\n# add and start running\n~> invoker add dj One can also enable OSX notifications for crashed processes by installing terminal-notification gem. It is not a dependency, but can be useful if something crashed and you weren’t paying attention. Using with rbenv or rvm The way rbenv and rvm work sometimes creates problems when you are trying to use a process supervisor like invoker . There are couple of things to keep in mind,\nIf you are running invoker with Ruby version x, but your application requires Ruby version Y: When using rbenv , you can define the command with environment variable RBENV_VERSION=Y and then start your application. In other words: command = RBENV_VERSION=2.0.0-p0 zsh -c \"bundle exec rails s\" Unless version of Ruby using which you are running invoker command and version of Ruby you are using in the application is same, you almost always will want to use zsh -c or bash -c . RVM in particular requires a login shell and hence sometimes you may have to use bash -lc . For example: command = bash -lc \"rvm use 2.0.0-p0 && bundle exec rails s\" I hope you find this gem useful and do not forget to follow us on twitter @codemancershq if you want to keep up with latest utility we are making to make our lives wee bit easier. Source Code https://github.com/code-mancers/invoker Report bug or feature request If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-06-08"},
{"website": "Codemancers", "title": "non native fullscreen for osx on emacs 24 dot 3", "author": ["Hemant"], "link": "https://crypt.codemancers.com/posts/2013-07-05-non-native-fullscreen-for-osx-on-emacs-24-dot-3/", "abstract": "Non native fullscreen for OSX on emacs 24.3 Written by Hemant on July 5, 2013; tagged under emacs , osx , fullscreen Since upgrading to Emacs 24.3 I have been missing ns-toggle-fullscreen . This feauture\nhas been replaced in newer version of Emacs with native OSX fullscreen. Needless to say native OSX fullscreen can be painful when you are on multiple monitors\nand even on single monitor setup I find it unusable. After searching high and low, I found the magic variable which can make Emacs 24.3\nbehave like old fashioned full screen. Emacs no longer moves to a new desktop and everything\nworks as expected: Download nightly version of Emacs from Emacs for OSX and\nadd following to .emacs (setq ns-use-native-fullscreen nil) After that, when you use toggle-frame-fullscreen Emacs should be on fullscren but\nyou should still have access to other monitor and things should be fine and dandy! If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-07-05"},
{"website": "Codemancers", "title": "redefine rake routes to add your own custom tag in rails", "author": ["Deepak"], "link": "https://crypt.codemancers.com/posts/2013-07-12-redefine-rake-routes-to-add-your-own-custom-tag-in-rails/", "abstract": "Redefine rake routes to add your own custom tag in Rails Written by Deepak on July 12, 2013; tagged under rake , rails , tips The only valid measurement of code quality is: WTFs/minute Now Rails has this nifty rake task called rake notes . You can always see notes for a custom tag by: ~> rake notes : custom ANNOTATION = WTF But rake notes by default only shows notes for OPTIMIZE, FIXME and TODO\nThe Rakefile for changing those defaults is # for re-defining the Rake task # otherwise the previous Rake task is still called task ( :notes ) . clear desc \"Enumerate all annotations (use notes:optimize, :fixme, :todo, :wtf for focus)\" task :notes do SourceAnnotationExtractor . enumerate \"OPTIMIZE|FIXME|TODO|WTF\" , :tag => true end namespace :notes do task ( :wtf ) . clear task :wtf do SourceAnnotationExtractor . enumerate \"WTF\" , :tag => true end end Here we have redefined rake notes . The only gotcha is to remember\nto clear the old rake task by calling task(:notes).clear For other customizations eg. search the spec folder also\nyou will have to monkey-patch the SourceAnnotationExtractor class PS: also check this Stackoverflow thread for some fun picks If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-07-12"},
{"website": "Codemancers", "title": "setting up source maps for coffeescript in rails", "author": ["Amitava"], "link": "https://crypt.codemancers.com/posts/2013-09-05-setting-up-source-maps-for-coffeescript-in-rails/", "abstract": "Setting up Source Maps for CoffeeScript in Rails Written by Amitava on September 5, 2013; tagged under coffeescript , rails [IMPORTANT] Only works in development environment. Source maps are a language agnostic way to map minified, combined or pre-processed code to\nthe original source file. This becomes highly useful as it helps to debug or\ninspect compressed client-side code in browser (depends on browser support). When using CoffeeScript we often find it difficult to locate the exact\nlocation of an error, since the line numbers in generated javascript\ncode does not match with the original coffee file. Using source maps we can instruct the browser to load the original coffee file\nand show us where the error has occurred. Also it is possible to debug a\ncoffee file in a browser by setting breakpoints. Let’s find out how to add source maps support for CoffeeScript. Requires\nCoffeeScript 1.6.1 or above. Install coffee-rails-source-maps . Enable source maps in Google Chrome by going to Settings page in the\ndevtools panel and checking ‘Enable source maps’. Requires the latest\nversion of Chrome. Next time reloading the browser will generate source maps inside public/assets/source-maps directory and Chorme DevTools will display any js\nerror right inside the coffee file instead of generated js code. Source maps greatly simplifies the process of debugging CoffeeScript right\ninside the browser. However as mentioned at the beginning of this post, it only\nworks in development at the moment. Adding support for production is quite\nchallenging since it would require to map a single combined and minified file to\nthe original source files. uglifier already seems to have support with this pull request and sprockets is also getting ready to\nadd it to the asset pipeline. Hopefully Rails will pretty soon have support for\nsource maps in production environment. What about Firefox? At the time of this writing Firefox 24 also has source maps support, however it\ndidn’t work as expected as it did seemlessly in Chrome.\nHere is a good example of how to enable source maps it http://www.codeproject.com/Articles/649271/How-to-Enable-Source-Maps-in-Firefox . If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2013-09-05"},
{"website": "Codemancers", "title": "diy ruby cpu profiling part iii", "author": ["Emil Soman"], "link": "https://crypt.codemancers.com/posts/2015-04-15-diy-ruby-cpu-profiling-part-iii/", "abstract": "DIY Ruby CPU profiling - Part III Written by Emil Soman on April 15, 2015; tagged under ruby , cpu , profiler In Part I we saw how\ninstrumentation profilers work, and\nin Part II we learned how\nto measure execution time of programs using CPU time and Wall time.\nIt’s recommended to read those first before you continue.\nIn this part we’ll apply what we have learned so far and build a very basic\ninstrumentation CPU profiler. Part III. DIY Instrumentation CPU Profiler Instrumentation in Ruby In Part I , we learned\nthat instrumentation profiling tools make use of hooks which can either be\ninstalled by the profiling tools or provided by the language itself to do their job.\nLuckily, Ruby already has these hooks baked into it and gives us the nifty TracePoint class to make use of these hooks. TracePoint API Ruby VM emits a bunch of events at various points while executing your code.\nRuby allows you to enable callbacks for these events using\nthe TracePoint class. The API documentation of TracePoint has a list of all the events you could listen to, but we’re interested in only\ntwo of those for now: :call , which is triggered when a ruby method gets called :return , which is triggered when a ruby method returns. We can use the TracePoint.new method to create a tracepoint object and\npass it an event type to listen to and a block of code which should\nget executed when the event gets triggered.\nHere’s the code to create a tracepoint for :call and :return events: trace = TracePoint . new ( :call , :return ) do | trace | puts \"Method: #{ trace . method_id } , event: #{ trace . event } \" end The argument trace which is passed into the callback code block gives you\naccess to a bunch of tracepoint attributes which can tell you more about\nthe event that got triggered. In our example, we’re going to need the method_id attribute which gives us the method name and the event attribute which gives us the name\nof the event which gets triggered. The callback code block above will print\nthe method name and the type of the triggered event (call/return),\nevery time a Ruby method gets called or returned. After you create the tracepoint, you need to enable it: trace . enable Once the tracepoint is enabled, we can create a method and call it, and see\nif the callback that we registered gets executed: def hello end hello #=> Method: hello, event: call #=> Method: hello, event: return Great, our callback got executed twice as expected. diy_prof gem It’s finally time to start putting together all that we’ve learned and build\na RubyGem. We’re going to call this gem diy_prof . The source for this gem\nis on GitHub and you can find it here . Now let’s build the gem from scratch using bundler: bundle gem diy_prof This creates a skeleton project which we’ll build upon. Next, we’ll create a TimeHelpers module which will contain the methods cpu_time and wall_time which we wrote in Part II : # lib/diy_prof/time_helpers.rb module DiyProf::TimeHelpers # These methods make use of `clock_gettime` method introduced in Ruby 2.1 # to measure CPU time and Wall clock time. def cpu_time Process . clock_gettime ( Process :: CLOCK_PROCESS_CPUTIME_ID , :microsecond ) end def wall_time Process . clock_gettime ( Process :: CLOCK_MONOTONIC , :microsecond ) end end We also need to require this file in our gem’s main file lib/diy_prof.rb : # lib/diy_prof.rb require 'diy_prof/version' require 'diy_prof/time_helpers' # Rest of the original file .. Next, we’ll use the TracePoint API in our gem to print the time (using our TimeHelpers)\nwhen a :call or :return event occurs. # lib/diy_prof/tracer.rb module DiyProf class Tracer include TimeHelpers def initialize ( clock_type : :cpu ) @tracepoint = TracePoint . new ( :call , :return ) do | trace | time = clock_type == :wall ? wall_time : cpu_time printf ( \"%-20s:%-20s%-20s \\n \" , time , trace . event , trace . method_id ) end end def enable @tracepoint . enable end def disable @tracepoint . disable end end end Also, we’ll require this file from our gem’s main file too: # lib/diy_prof.rb require 'diy_prof/version' require 'diy_prof/time_helpers' require 'diy_prof/tracer' # Rest of the original file .. Now let’s write an example script to test what we’ve built so far. We’ll\nuse the same example we saw in Part I : # examples/demo.rb $: << File . join ( File . dirname ( __FILE__ ), \"../lib\" ) require 'diy_prof' ### Begin sample program ### def main 3 . times do find_many_square_roots find_many_squares end end def find_many_square_roots 5000 . times { | i | Math . sqrt ( i )} end def find_many_squares 5000 . times { | i | i ** 2 } end ### End sample program ### tracer = DiyProf :: Tracer . new ( clock_type : :cpu ) tracer . enable main tracer . disable If we run the above Ruby program, we should get the following output: 70508               :return              enable\n70587               :call                main\n70604               :call                find_many_square_roots\n73225               :return              find_many_square_roots\n73256               :call                find_many_squares\n74527               :return              find_many_squares\n74562               :call                find_many_square_roots\n77042               :return              find_many_square_roots\n77064               :call                find_many_squares\n77653               :return              find_many_squares\n77690               :call                find_many_square_roots\n79681               :return              find_many_square_roots\n79698               :call                find_many_squares\n80288               :return              find_many_squares\n80306               :return              main\n80326               :call                disable The first column of the output is the CPU time of the triggered events,\nthe second column is the name of the event that got triggered(call/return) and\nthe third column is the name of the method which got called or returned.\nThis is similar to the output we saw in Part I where we learned how instrumentation profilers work. As you can see, we can easily\nrecreate a call stack using this output because we know which method called which\nother methods. We also have the time of all call and return events, so we can also tell\nhow much time was spent in each method. It wouldn’t be too difficult to write\na script that parses this output and prints a list of methods ordered by\ntheir execution them. But that’s too boring, we can build something way more\ninteresting than that. Watch out for the next part to learn what else we can do with this data. Recap We’ve learned about Ruby’s TracePoint API and how to use it to listen to\nmethod calls and returns. We’ve also taken the code we’ve written so far and\nput everything together to build a gem which allows you to print the execution\ntrace of your Ruby program. In Part 4 we’ll learn how to make sense of the\ndata that our gem collects and build some cool call graphs out of it.\nThanks for reading! If you would like to get updates about subsequent\nblog posts in this DIY CPU profiler series, do follows us on twitter @codemancershq . If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2015-04-15"},
{"website": "Codemancers", "title": "diy ruby cpu profiling part iv", "author": ["Emil Soman"], "link": "https://crypt.codemancers.com/posts/2015-07-18-diy-ruby-cpu-profiling-part-iv/", "abstract": "DIY Ruby CPU profiling - Part IV Written by Emil Soman on July 18, 2015; tagged under ruby , cpu , profiler In Part I we saw how\nCPU profilers work,\nin Part II we learned how\nto measure execution time of programs using CPU time and Wall time and\nin Part III we learned how\nto use Ruby’s TracePoint API to trace method calls and returns in Ruby VM.\nWe also started building our DiyProf gem.\nIt’s recommended to read those first before you continue.\nIn this part we’re going to create a\nsimple call graph visualization using the data we collected using the TracePoint API. Part IV. Call Graphs In Part III , we used\nRuby’s TracePoint API to print method call and return traces. It’d be more\nuseful if we can parse these traces and create a call graph out of it.\nA call graph is basically a visual representation of the method calls, in\nthe form of a tree. It looks like this : This graph tells us that outer_method calls 3 methods from inside it: inner_method{1,2,3} and inner_method_3 calls another method named inner_most_method . Graphviz and the DOT language Graphviz is a graph visualization tool which can\ntake a graph description in text format and generate diagrams as PDF and other\nformats. For describing graphs in text format, Graphviz uses a language called DOT ,\nwhich is a simple declarative way of describing nodes and connections between them. Before proceeding, make sure you’ve installed graphviz and the dot command\nis available in your $PATH.\nFollow their official documentation for\ninstallation steps. Now let’s take an example. digraph graph_name {\n  a -> b -> c;\n  b -> d;\n} Save the above lines as sample.dot and run the following command: dot -Tpdf sample.dot -o graph.pdf The above command will generate a directed graph that looks like this: There’s another way to represent graphs, by separating nodes\nand connections like so: digraph graph_name {\n  <nodes>\n  <connections>\n} So the following DOT notation represents the same graph: digraph graph_name {\n  a;\n  b;\n  c;\n  d;\n  a -> b;\n  b -> c;\n  b -> d;\n} It’s also possible to add additional labels to nodes and connections. digraph graph_name {\n  a [label=\"a \\n foo: bar\"];\n  b [label=\"b \\n foo: baz\"]\n  a -> b [label=\"xyz\"];\n} Above DOT code represents the following graph: Call Graph with Graphviz We’re going to create a reporter class for our DiyProf gem which will keep collecting\nall method calls and create a DOT file that represents the call graph. We’ll add a record method under the DotReporter class which pushes method infos into a stack\nwhen calls happen and pop them out of the stack when the methods return.\nWhen the method info is popped from the stack, the time it spent in the stack\nis recorded(which is the execution time) and it adds the method’s call info(method name and time)\nto a call tree. See the code below: module DiyProf # Struct that will hold a method name and a timestamp CallInfo = Struct . new ( :name , :time ) class DotReporter def initialize # A stack for pushing/popping methods when methods get called/returned @call_stack = [] # Nodes for all methods @methods = {} # Connections between the nodes @calls = {} end def record ( event , method_name , time ) case event when :call @call_stack << CallInfo . new ( method_name , time ) when :return # Return cannot be the first event in the call stack return if @call_stack . empty? method = @call_stack . pop # Set execution time of method in call info method . time = time - method . time add_method_to_call_tree ( method ) end end end end add_method_to_call_tree adds nodes and connections for the added method.\nHere’s the code that does that: # Struct that holds the time spent in a method # and the total number of times a method was called MethodInfo = Struct . new ( :count , :total_time , :self_time ) def add_method_to_call_tree ( method ) # Add method as a node to the call graph @methods [ method . name ] ||= MethodInfo . new ( 0 , 0 , 0 ) # Update total time(spent inside the method and methods called inside this method) @methods [ method . name ]. total_time += method . time # Update self time(spent inside the method and not methods called inside this method) # This will be subtracted when children are added to the graph @methods [ method . name ]. self_time += method . time # Update total no of times the method was called @methods [ method . name ]. count += 1 # If the method has a parent in the call stack # Add a connection from the parent node to this method if parent = @call_stack . last @calls [ parent . name ] ||= {} @calls [ parent . name ][ method . name ] ||= 0 @calls [ parent . name ][ method . name ] += 1 # Take away self time of parent @methods [ parent . name ] ||= MethodInfo . new ( 0 , 0 , 0 ) @methods [ parent . name ]. self_time -= method . time end end Now that we have the nodes and connections in @methods and @calls ,\nit’ll be trivial to generate a DOT file that looks like this: digraph G {\n  <graph_nodes>\n  <graph_links>\n} Here’s the code that will walk through @methods and @calls and prepare\nsuch a DOT file: def dot_notation dot = %Q( digraph G { #{ graph_nodes } #{ graph_links } } ) end def graph_nodes nodes = \"\" @methods . each do | name , method_info | nodes << \" #{ name } [label= \\\" #{ name } \\\\ ncalls: #{ method_info . count } \\\\ ntotal time: #{ method_info . total_time } \\\\ nself time: #{ method_info . self_time } \\\" ]; \\n \" end nodes end def graph_links links = \"\" @calls . each do | parent , children | children . each do | child , count | links << \" #{ parent } -> #{ child } [label= \\\" #{ count } \\\" ]; \\n \" end end links end Check out the lib/diy_prof/dot_reporter.rb file on the github repo to see how all of this will fit together. We also need some changes in the\nTracer class we wrote in Part III to make use of a custom reporter class: module DiyProf class Tracer # We'll need to initialize the Tracer with DotReporter.new def initialize ( reporter ) @reporter = reporter @tracepoints = [ :call , :return ]. collect do | event | TracePoint . new ( event ) do | trace | # Use the reporter instead of just printing the traces reporter . record ( event , trace . method_id , cpu_time ) end end end ... # New method added to collect the report after profiling def result @reporter . result end end end Now that our tracer is able to generate a DOT formatted report, we can use it\non our sample program we wrote in the previous parts: def main 3 . times do find_many_square_roots find_many_squares end end def find_many_square_roots 5000 . times { | i | Math . sqrt ( i )} end def find_many_squares 5000 . times { | i | i ** 2 } end DiyProf . start_profiling main result = DiyProf . stop_profiling require 'tempfile' f = Tempfile . open ( 'dot' ) f . write result f . close system ( \"dot -Tpdf #{ f . path } -o call_graph.pdf\" ) We should get a call graph that looks like this: Congratulations if you got this far!\nCheck out the github repo which has the complete code if you got stuck somewhere. Bonus! Wouldn’t it be cool if we could vary the size of the nodes based on self or total\ntime of the method?\nThe DOT syntax accepts a bunch of attributes for nodes and edges .\nLooks like we can use size and fontsize attributes to vary the size of the\nnodes and the text inside the nodes. I’ll leave it to you to think of how to use\nthese to make the most expensive methods stand out in the call graph. Recap We’ve learned about Graphviz and the DOT language and how to use them\nto create call graph diagrams out of the data collected by Ruby Tracepoint API.\nIn Part V, we’ll start building a sampling profiler for Ruby, in particular\nwe’ll learn some basics about interrupts and signal handling in C.\nThanks for reading! If you would like to get updates about subsequent\nblog posts in this DIY CPU profiler series, do follows us on twitter @codemancershq . If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2015-07-18"},
{"website": "Codemancers", "title": "say goodbye to vendor prefixes with autoprefixer", "author": ["Girish"], "link": "https://crypt.codemancers.com/posts/2015-09-25-say-goodbye-to-vendor-prefixes-with-autoprefixer/", "abstract": "Say goodbye to vendor prefixes with Autoprefixer Written by Girish on September 25, 2015; tagged under autoprefixer , SCSS , CSS , vendor-prefixes , cross-browser CSS vendor prefixes are used in browsers to add support for new CSS features in their testing and experimentation period, that may not be part of a formal specification. The CSS browser prefixes are: -webkit- /* Chrome, Safari, newer versions of Opera, Android and iOS */ -moz- /* Firefox */ -ms- /* Internet Explorer */ -o- /* Old versions of Opera */ We face the challenge of supporting different kinds and versions of browsers when developing websites. In most cases, to use a more advanced CSS3 style property, you take the standard CSS property and add the prefix above for each browser. For example, if you want to add a CSS3 transition, you would use the transition property with the prefixes: -webkit-transition : ANIMATION-NAME 5s ease-in-out infinite ; -moz-transition : ANIMATION-NAME 5s ease-in-out infinite ; -ms-transition : ANIMATION-NAME 5s ease-in-out infinite ; -o-transition : ANIMATION-NAME 5s ease-in-out infinite ; transition : ANIMATION-NAME 5s ease-in-out infinite ; Vendor prefixes are annoying Adding these prefixes to CSS is a routine task for all web designers and frontend developers. It’s also hard to keep track of which prefixes are needed and where, as per browsers and their versions. Please keep in mind that the web browsers will continue changing and evolving over a period of time. It is inevitable that you’ll require some type of hack unless you’re keen on building web applications or webpages which are years behind than the ones created with modern methods. When you use browser prefixes with the standard prefix listed last, you are setting your web pages up to be future proofed. Browsers that use the prefixes will use them and ignore the properties they don’t understand. Browsers that can support the standards based properties will apply them because they are listed last, in your CSS class. It feels annoying and repetitive to write the properties 2–5 times to get it to work in all browsers, but it’s temporary. As browsers improve, they add support for the standards based version of the property, and you can remove the prefixed versions. For example, just a few years ago, to set a rounded corner on a box you had to write: -moz-border-radius : 10px 5px ; -webkit-border-top-left-radius : 10px ; -webkit-border-top-right-radius : 5px ; -webkit-border-bottom-right-radius : 10px ; -webkit-border-bottom-left-radius : 5px ; border-radius : 10px 5px ; But now you only need the standards version: border-radius : 10px 5px ; Chrome has supported the CSS3 property since version 5.0, Firefox added it in version 4.0, Safari added it in 5.0, Opera in 10.5, iOS in 4.0, and Android in 2.1. Even Internet Explorer 9 supports it without a prefix (and IE 8 and lower didn’t support it with or without prefixes). We have used mixin libraries that get shipped with pre-processors like Compass for SASS. They solve a lot of problems, but create other problems instead. For instance, they force us to use a new syntax, which often changes between major releases. For example, to add a linear-gradient in SCSS: . linear-gradient { @include background-image(linear-gradient(#ffffff, #dddddd)) ; } It compiles to CSS as shown below: . linear-gradient { background-image : url ( 'data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4gPHN2ZyB2ZXJzaW9uPSIxLjEiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PGRlZnM+PGxpbmVhckdyYWRpZW50IGlkPSJncmFkIiBncmFkaWVudFVuaXRzPSJvYmplY3RCb3VuZGluZ0JveCIgeDE9IjAuMCIgeTE9IjAuMCIgeDI9IjEuMCIgeTI9IjEuMCI+PHN0b3Agb2Zmc2V0PSIwJSIgc3RvcC1jb2xvcj0iI2ZmZmZmZiIvPjxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI2RkZGRkZCIvPjwvbGluZWFyR3JhZGllbnQ+PC9kZWZzPjxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSIxMDAlIiBoZWlnaHQ9IjEwMCUiIGZpbGw9InVybCgjZ3JhZCkiIC8+PC9zdmc+IA==' ); background-size : 100 % ; background-image : -webkit- gradient ( linear , 0 % 0 % , 100 % 100 % , color-stop ( 0 % , #ffffff ), color-stop ( 100 % , #dddddd )); background-image : -moz- linear-gradient ( top , #ffffff , #dddddd ); background-image : -webkit- linear-gradient ( top , #ffffff , #dddddd ); background-image : linear-gradient ( to bottom right , #ffffff , #dddddd ); } These pre-processors iterate much slower than modern browsers. A stable release can have a lot of unnecessary prefixes and sometimes we need to create our own mixins. ####So what’s the solution? Autoprefixer is a postprocessor. It doesn’t use any specific syntax and works with common CSS. It parses your CSS files and adds vendor prefix properties and values as per caniuse database. It will only add the necessary prefixes and not bloat your stylesheet. It even lets you specify what browsers you want to target(by default the last 2 versions). It will also remove existing prefixes which are no longer needed. It can be Integrated with SASS, since it runs after CSS is already compiled. All you have to do is add it to your asset building tool and you can completely forget about CSS vendor prefixes. Just write regular CSS according to the latest W3C specifications without any prefixes. Like this: For example: input :: placeholder { color : #999 ; background : #f5f5cd ; } It compiles to: input :: -webkit-input-placeholder { color : #999 ; background : #f5f5cd ; } input :: -moz-placeholder { color : #999 ; background : #f5f5cd ; } input : -ms-input-placeholder { color : #999 ; background : #f5f5cd ; } input :: placeholder { color : #999 ; background : #f5f5cd ; } Incorporate Autoprefixer into your project build system using autoprefixer-rails gem, gulp or grunt . There’s also an online compiler , if you want to try compiling CSS snippets with autoprefixer. If you found this article useful, don’t forget to follow us on twitter\nto get updates on our posts. We’re looking for UI/UX experts to join\nus, so get in touch! . If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2015-09-25"},
{"website": "Codemancers", "title": "new in batches method in active record", "author": ["Kashyap"], "link": "https://crypt.codemancers.com/posts/2015-12-23-new-in-batches-method-in-active-record/", "abstract": "New in ActiveRecord: #in_batches Written by Kashyap on December 23, 2015; tagged under rails Rails recently introduced a new method #in_batches similar to the existing #find_in_batches method, the\ndifference being what class the yielded object belongs to. In the case\nof the latter the yielded collection is an Array whereas in the new\nmethod, the yielded collection is an ActiveRecord::Relation . The\nfollowing post is a usecase that we found for this new method. In one of our client projects, there is code that looks like: def fetch_payments_for_merchants ( merchants ) PaymentFetcher . where ( merchant_id : merchants . map ( & :id )) end Mapping over a collection of ActiveRecord objects results in an N+1\nquery syndrome, which means that for each iteration of the #map method, we make an SQL call. A detailed description about N+1 queries\nand how to avoid them is provided in this Rails guides\narticle . To summarize that section, we should prefer\nusing the #pluck that works on any ActiveRecord collection object to\nusing #map . I was looking to refactor the codebase to use the #pluck method, but it was not possible here as the argument is not always an\nActiveRecord collection object. This method is also used for report\ngeneration method, which in turn uses the #find_in_batches method to\nfetch a batch of merchant objects and operate upon them, inorder to\nreduce memory usage. The #find_in_batches method takes a block and\nyields an Array containing the batch of merchants. Something like\nthis: def generate_csv Merchant . find_in_batches do | merchants | payments = fetch_payments_for ( merchants ) # do some report generation end end # elsewhere in the controller def show @merchants = Merchant . where ( id : params [ :id ] ) @payments = PaymentFetcher . fetch_payments_for ( @merchants ) end So using #pluck would fail with a NoMethodError when the app has to\ngenerate a report—at least, as of Rails 4.1.x. To make method work in\nboth cases, the original author used the .map method, since it works\non both Arrays and ActiveRecord collection objects. But this penalizes\nthe case where the user loads the page in the browser because there is a\npossibilty of the show method resulting in an N+1 query. And this is\nprecisely where the new #in_batches method helps. So the generate_csv action can be rewritten as: def generate_csv Merchant . in_batches do | merchants | payments = fetch_payments_for ( merchants ) # do some report generation end end private def fetch_payments_for ( merchants ) PaymentFetcher . where ( merchant_id : merchants . pluck ( :id )) end Now for the bad news: the #in_batches method is only available in\nRails master (that would mean version 5, as of today). As per Rails’\nmaintenance policy, new features are not backported to older version.\nThis leaves us with some options: Polymorphic style conditional In the fetch_payments_for method, we check the class of the merchants object, and switch between #map and #pluck accordingly. def fetch_payments_for ( merchants ) merchant_ids = if merchants . is_a? ( Array ) merchants . map ( & :id ) else merchants . pluck ( :id ) end PaymentFetcher . where ( merchant_id : merchant_ids ) end This works, but pretty ugly. Monkeypatching Array The other, equally ugly/hacky, way would be to monkeypatch the Array class and define a #pluck method: def fetch_payments_for ( merchants ) PaymentFetcher . where ( merchant_id : merchants . pluck ( :id )) end class Array # The argument is named columns because this method is expected to be # used only on an Array of ActiveRecord items. def pluck ( * columns ) map do | item | columns . map do | column | item . send ( column ) end end end end IMO, this implementation is incredibly hacky because of how the\ndefinition of a method in a general purpose type (Array) is now coupled\nwith ActiveRecord object structure. Although Duck typing suggests that\nthis is the way, I tend to think twice before polluting a language-level\nmodule. That said, as of Rails 5.x, #pluck is defined for Enumerable\nmodule via ActiveSupport core extensions, and so,\ntechnically, we could use this. Monkeypatching ActiveRecord::Batches The other option would be to backport the #in_batches method for the\nRails version we use. This would be a bit more complicated than\nmonkey-patching Array class, but it’s restricted only to the ActiveRecord::Batches module. This seemed like a nicer change, so we\nwent with this for our case. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2015-12-23"},
{"website": "Codemancers", "title": "understanding exit signals in erlang slash elixir", "author": ["Emil Soman"], "link": "https://crypt.codemancers.com/posts/2016-01-24-understanding-exit-signals-in-erlang-slash-elixir/", "abstract": "Understanding Exit Signals in Erlang/Elixir Written by Emil Soman on February 29, 2016; tagged under elixir “Process linking and how processes send and handle exit signals” - a very\nimportant topic to understand in order to build robust apps in Erlang or Elixir, but\nalso the source of a lot of confusions for beginners. In this post, we’ll cover\nthis topic and understand it well, once and for all. Note: Processes are the same in both Erlang and Elixir, so everything\nbelow is equally applicable to both languages. Processes Processes in Erlang are like threads that don’t share any data. These processes\nare VM level, so don’t confuse them with OS processes.\nThese VM level processes are scheduled for execution by the Erlang VM,\nlike how the OS schedules the OS level processes for execution.\nSince these Erlang processes own their own data, they are scheduled freely on all\navailable CPUs and this is how Erlang makes concurrency easy for developers. But a bunch of processes that run in isolation are rarely of any use. To build\nanything useful, processes need to work together by communicating with each\nother. Messages In Erlang processes communicate among themselves by message passing.\nEvery process has one mailbox. A process can read messages that\nappear in its mailbox and can also send messages to mailboxes that belong\nto other processes. This way, processes can communicate with each other\nwithout having to share any data. This frees developers from writing locks\naround data access when writing code that may run concurrently. Apart from these regular messages, processes also communicate\nusing “exit signals”, a special type of message. Exit Signals Processes have a signalling mechanism by which they can let other processes\nknow that they are exiting. These “exit signals” also contain an exit reason,\nwhich help other processes to decide how to react to the signal. A process can terminate for 3 reasons: A normal exit - This happens when a process is done with its job and ends execution.\nSince these exits are normal, usually nothing needs to be done\nwhen they happen. So these signals are usually ignored, but they are\nemitted anyway for the sake of interested processes. The exit reason for this\nkind of exit is the atom :normal . Because of unhandled errors - This happens when an exception is raised inside the process\nand not caught. A pattern matching error is an example - a technique used\nby Erlang programmers to “fail fast”. The exit reason for this kind of exit\nis the exception details - name of the exception and some stack trace. Forcefully killed - This happens when another process sends an exit signal with\nthe reason :kill , which forces the receiving process to terminate. A process can subscribe to another process’s exit signal by establishing a\n“link” with that process. When a process terminates, all the linked processes\nreceive the exit signal from the terminating process. The force-kill signals, the ones with exit reason :kill , will terminate the\nreceiving process no matter what. But the other kinds of exit signals - those\nwith reasons :normal or any other reason - can cause different effects on the\nreceiving process depending on whether the receiving process is trapping exits\nor not. Let’s see how trapping of exit signals works. Trapping exits When a process traps exit signals, the exit signals that are received from the links\nwill be converted into messages which are then put inside the mailbox that\nbelongs to the process. Here’s how a process can trap exits in Elixir: Process . flag ( :trap_exit , true ) receive do { :EXIT , from , reason } -> # Handle exit as needed When this process receives an exit signal other than :kill signal, it\nwill be converted into a message that will be received inside the receive block. In Erlang/Elixir, this is what makes supervisor trees possible. A supervisor is a process whose responsibility is to start child\nprocesses and keep them alive by restarting them if necessary.\nLet’s see how a supervisor does that. If you look at the source code of supervisor in Erlang, you can see that the first thing that happens\nin the init function is trapping exit signals: init (...) -> process_flag ( trap_exit , true ), ... This means that exit signals from child processes will be converted into\nmessages. The supervisor then handles these messages by restarting the child\nprocesses, based on the restart strategy of the supervisor. This is how\nyou write fault tolerant apps in Elixir or Erlang - you let your processes fail\nfast, and the supervisor that spawned these processes will make sure they\nare restarted. So if processes can trap exit signals, how is it possible to kill\nthem? Using the :kill exit signal, of course. The exit reason :kill is\nused to forcefully terminate the receiving process even if it’s trapping exits. In Elixir, this is how you kill a process using its pid: Process . exit ( pid , :kill ) A process can also send an exit signal to itself using: Process . exit ( self (), < reason > ) The process responds to this signal from self in a similar manner it would respond to an\nexit signal it receives from another process, but with one exception.\nIf a process sends itself an exit signal with the reason :normal , the process\nterminates and when it does, it sends a :normal exit signal to all linked\nprocesses. Recap :normal exit signals are harmless. These are ignored by the receiving process unless trapping exits,\nin which case, these will be received as messages. If it’s sent by self, it\nwill cause the process to terminate with a :normal exit reason. :kill exit signals always result in the termination of the receiving process. Exit signals with other reasons will terminate the receiving process unless trapping exits,\nin which case, these will be received as messages. Here’s a cheatsheet for your reference (click to enlarge) : Thanks for reading! If you would like to get updates about subsequent\nblog posts about Elixir from Codemancers, do follows us on twitter: @codemancershq . If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2016-02-29"},
{"website": "Codemancers", "title": "visualizing parallel requests in elixir", "author": ["Emil Soman"], "link": "https://crypt.codemancers.com/posts/2016-01-15-visualizing-parallel-requests-in-elixir/", "abstract": "Visualizing Parallel Requests in Elixir Written by Emil Soman on January 15, 2016; tagged under elixir We have been evaluating Elixir at Codemancers and today I was learning how to\nspin up a minimal HTTP API endpoint using Elixir. Like Rack in Ruby land,\nElixir comes with Plug, a swiss army knife for dealing with HTTP connections. Using Plug to build an HTTP endpoint First, let’s create a new Elixir project: $ mix new http_api --sup This creates a new Elixir OTP app. Let’s add :cowboy and :plug as\nhex and application dependencies: # Change the following parts in mix.exs def application do [ applications : [ :logger , :cowboy , :plug ], mod : { HttpApi , []}] end defp deps do [ { :cowboy , \"~>1.0.4\" }, { :plug , \"~>1.1.0\" } ] end Plug comes with a router which we can use to build HTTP endpoints with ease.\nLet’s create a module to encapsulate the router: # lib/http_api/router.ex defmodule HttpApi.Router do use Plug.Router plug :match plug :dispatch get \"/\" do send_resp ( conn , 200 , \"Hello Plug!\" ) end match _ do send_resp ( conn , 404 , \"Nothing here\" ) end end If you have worked with sinatra-like frameworks, this should look familiar to\nyou. You can read the router docs to\nunderstand what everything does if you are curious. To start the server, we’ll tell the application supervisor to\nstart the Plug’s Cowboy adapter: # lib/http_api.ex defmodule HttpApi do use Application def start ( _type , _args ) do import Supervisor.Spec , warn : false children = [ # `start_server` function is used to spawn the worker process worker ( __MODULE__ , [], function : :start_server ) ] opts = [ strategy : :one_for_one , name : HttpApi.Supervisor ] Supervisor . start_link ( children , opts ) end # Start Cowboy server and use our router def start_server do { :ok , _ } = Plug.Adapters.Cowboy . http HttpApi.Router , [] end end The complete code for the above example can be found here .\nYou can run the server using: $ iex -S mix This starts the interactive Elixir shell and runs your application on the\nErlang VM. Now comes the fun part. Visualizing processes using :observer In the iex prompt, start the Erlang :observer tool using this command: iex> :observer.start This opens a GUI tool that looks like this: On the left hand side of the Applications panel, you can see a list of\nall the applications currently running on the Erlang VM - this includes our\napp (http_api) and all its dependencies, the important ones being cowboy and\nranch. Cowboy and Ranch Cowboy is a popular HTTP server in the Erlang world and it uses Ranch , another Erlang library, to handle TCP connections behind the scenes.\nWhen we start the Plug router, we pass on the router module to Plug’s Cowboy\nadapter. Now when Cowboy receives a connection, it passes it over\nto Plug, and Plug runs it through it’s plug pipeline and sends back the request. Concurrent Requests Plug by default asks cowboy to start 100 TCP connection acceptor processes in\nRanch. You can see the 100 acceptor processes for yourself if you see the\napplication graph of ranch using :observer. Does this mean that we can only have 100 concurrent connections? Let’s find out.\nWe’ll change the number of acceptors to 2 by passing it as a parameter to Plug’s\nCowboy adapter: Plug.Adapters.Cowboy.http HttpApi.Router, [], [acceptors: 2] Let’s see the how the processes look like now: Okay, so we’ve got only 2 TCP connection acceptor processes running. Let’s\ntry making 5 long running concurrent requests and see what happens. # lib/http_api/router.ex # Modify router to add some sleep defmodule HttpApi.Router do use Plug.Router plug :match plug :dispatch # Sleep for 100 seconds before sending the reponse get \"/\" do :timer . sleep ( 100000 ) send_resp ( conn , 200 , \"Hello Plug!\" ) end match _ do send_resp ( conn , 404 , \"Nothing here\" ) end end Let’s make 5 requests now by running this in the iex prompt: for n <- 1..5, do: spawn(fn -> :httpc.request('http://localhost:4000') end) Start :observer from iex using :observer.start and see the process graph: We can see that there are only 2 acceptor processes still, but 5 other\nprocesses were spawned somewhere else. These are connection processes which\nhold accepted connections. What this means is that, acceptor processes do not\ndictate how many processes we can run at a time, it just restricts how many\nnew processes can be accepted at a time. Even if you want to serve\n1000 concurrent requests, it’s safe to leave the number of acceptor processes at\nthe default value of 100. Summary You can build simple HTTP endpoints using Plug router Ranch can handle multiple TCP connections at a time by spawning processes Erlang :observer is a great way to visualize concurrency in your apps Acceptor processes only accept connections. You only need 100 of them. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2016-01-15"},
{"website": "Codemancers", "title": "rails webpack heroku", "author": ["Kashyap"], "link": "https://crypt.codemancers.com/posts/2016-03-18-rails-webpack-heroku/", "abstract": "Working with Rails, Webpack and Heroku Written by Kashyap on April 8, 2016; tagged under rails Note: This Rails version targeted in the post is one before webpacker got\nadded. The folder structure and procedure might differ if you’re using a\nnewer version of Rails. But the context does help. Deployment of a Rails app that uses Webpack to Heroku is tricky because\nit requires both Node.js and Ruby. After various unsuccessful attempts,\nI had a solution that worked without customizing Heroku buildpacks. If\nyou know how Heroku buildpacks work, skip to the summary for the TL;DR\nversion . A little bit of background first. The application I’m working on uses React for\nwriting interactive components on the front-end. Rails handles routing and\ndatabase-related functions. The React components are written in ES6\nsyntax that gets transpiled to the more widely supported ES5 API via the\nBabel transpiler. ESLint is used for code linting, and Redux for\nhandling complex state transitions of React components. All the modular\nJavaScript components are bundled into a single file using Webpack. It’s worth pointing out that there’s already a well supported React gem\nfor use with Rails react-rails , but we chose to use npm\ninstead. We wanted to leverage the latest version(s) of React, Redux,\nESLint, and we felt that using the npm-based package management workflow\nworks best in this case. This is the application directory structure we decided to go with: app/\nlib/\nconfig/\n... ( usual rails directories ) webpack/ | -- package.json | -- components | -- reducers Initially the package.json required by npm was placed inside the webpack/ directory, but this made deployment to Heroku a bit\ncomplicated. The Webpack asset compilation step bundles all the\nJavaScript files into a single file called react_bundle.js and places\nit under the app/assets/javascripts directory. This file is in turn\nloaded into Rails’ asset pipeline. So the prerequisite for the asset\npipeline to work properly is that a react_bundle.js file must\nbe present in app/assets/javascripts by the time assets:precompile step is run during deployment. Heroku buildpacks Heroku uses setup scripts they call buildpacks for\nfiguring out the type of application, and to run the necessary build\nsteps. For example, Heroku runs build steps for a Rack-based application\nif it finds a Gemfile in the root directory. If it finds a Railties\ngem in the Gemfile specs, it will trigger the Rails related build steps.\nAt a simplified level, these are the steps that are run for a Rack\napplication: Check for Gemfile and Gemfile.lock in the root directory. If it’s\npresent, assume this is a Ruby application and start Ruby related\nbuild steps. Check to see if specs in Gemfile.lock has Rack gem as a dependency. Install necessary Ruby version Install bundler and install all dependencies Run a web process based on a Procfile or the default web\nprocess In the same way, Heroku figures out the build steps for a Node.js\napplication based on the presence of a package.json file in the root\ndirectory. So what if we need both npm related steps and Ruby related\nsteps? We need to keep both the Gemfile and package.json in the root\ndirectory. That’s step 1 . Multiple Heroku Buildpacks Heroku supports having multiple buildpacks for a\nsingle application. If we use both Node.js and Ruby buildpacks, we need\na way to tell Heroku to run the npm-related steps first, and only then\nto run the Ruby related steps. This is to ensure that the react_bundle.js file is present under app/assets/javascripts when\nRails’ asset pipeline commands are run. The Heroku command line provides\na way configure the build packs and specify an order to them: heroku buildpacks:clear\nheroku buildpacks:set heroku/nodejs\nheroku buildpacks:add heroku/ruby --index 2 The --index argument means the heroku/ruby buildpack is inserted after the heroku/nodejs buildpack. This completes step 2 . Post build hook on Heroku Once the buildpacks are setup, we need a way to run a Webpack command\nright after the Node.js setup is completed, and just before the\nRuby/Rails related setup starts. Heroku’s Node.js buildpack provides a\nway for users to run a custom script right after the normal\nNode.js build steps are completed. For this, a command must be specified\nin package.json under the scripts section with a heroku-postbuild key. Here is the relevant section from package.json : \"scripts\" : { \"webpack:deploy\" : \"webpack --config=webpack/webpack.config.js -p\" , \"heroku-postbuild\" : \"npm run webpack:deploy\" } , This completes step 3 . With these three steps, I was able to setup a\nworkable Webpack + Rails scaffold on Heroku. Summary Put the package.json in the root folder of the Rails application. Setup multiple buildpacks on Heroku via Heroku’s command line utility,\nwith heroku/nodejs in the first position and heroku/ruby second. Add the webpack bundling command in package.json 's scripts section with heroku-postbuild as the key. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2016-04-08"},
{"website": "Codemancers", "title": "continuous deployment using docker cloud", "author": ["Emil Soman"], "link": "https://crypt.codemancers.com/posts/2016-07-25-continuous-deployment-using-docker-cloud/", "abstract": "Continuous Deployment using Docker Cloud Written by Emil Soman on July 25, 2016; tagged under docker One good thing about dockerizing all your services is the ability to choose\nfrom a plethora of orchestration tools to deploy those docker images in\ndifferent ways for different needs, like: Bootstrapping your multi-tiered app on developer boxes Creating ad-hoc disposable staging environments for testing pull requests Deploying, scaling and monitoring services in production There are so many tools out there which help you run docker containers on the cloud Kubernetes, Rancher, Flynn and many more. If you’re just getting started with docker based\ndeployments and wondering how to set up your first continuous deployment pipeline\nfor your dockerized app, this blog post is for you. PS: There are no illustrations or specific instructions on how to execute a\nspecific step. That’s intentional because UIs change often, but the concepts\nshould remain the same. Anyways, if you get stuck, drop a comment. What you need: An app with a Dockerfile like this one hosted on Github (or BitBucket) A Docker Hub account Knowledge of how to write a docker-compose.yml (or readiness to learn :)) Step 1. Automated builds on Docker Hub Login to Docker Hub, create an auto-building docker repository by clicking on\n“Create” > “Create Automated Build”. Finish the steps by following instructions\nor follow this official documentation if you get stuck. What you need to ensure is that you have set up the automated\ndeployment of your production ready branch and given it a tag like latest in Docker Hub. We’ll use this tag name later. Step 2. Set up your node Now log in to Docker Cloud , a tool for running\ndocker containers on nodes - either on a cloud provider or your own servers.\nFollow your gut or this guide until you are ready with a node running Docker Cloud agent. Step 3. Create a stack Once you have set up your node, you can proceed to create a “stack”. A stack\nis a collection of services which run together to make your application work.\nClick on “Stacks” and click “Create”. This will open a text editor where you\ncan write something called a “Stackfile”. Step 4. Write a Stackfile for your application A Stackfile is a YAML file that specifies the configurations for the\ndocker containers that are part of your stack. If you have ever written a docker-compose.yml file, you should feel right at home. If you have no idea\nwhat I’m talking about, read this official documentation .\nHere’s an example Stackfile: db : image : 'postgres:9.4' environment : - POSTGRES_PASSWORD=secret - POSTGRES_USER=user mywebapp : image : 'codemancers/mywebapp:latest' autoredeploy : true environment : - POSTGRES_HOST=db - POSTGRES_PASSWORD=secret - POSTGRES_USER=user - WEB_HOST=example.com links : - db ports : - '3000:3000' volumes : - '/home/user/cache:/workspace/cache' The above Stackfile will start two containers: a “db” container and a “mywebapp”\ncontainer. A quick walkthrough: image - This is the docker image that will be run as containers. For the service\nthat requires continuous deployment, this is the namespaced repository name\nalong with the tag that you configured on Docker Hub in step one. environment - Everything under environment will be passed as environment variables\nwhen starting the containers. Use these to configure your containers. autoredeploy - This is the secret sauce for continuous deployment. Setting this\nto true will cause the containers of this service to be redeployed everytime\nthe docker image is updated on Docker Hub. links - Using links you can create references to other containers inside\nthe containers of this service. In the above example, POSTGRES_HOST is set as db instead of a static IP address. That’s because db hostname inside mywebapp will\nresolve to the IP address of the db container. ports - Use this if you want to expose any TCP/UDP port of your container on\nyour host node. In the above example, port 3000 of the container can be accessed\nfrom the host node’s port 3000. volumes - As the name implies, this allows you to mount directories on the host\nas volumes on the container when it runs. You can use this for persisting data\nacross container redeployments. Step 5. Deploy! Once you have a Stackfile, paste it into the text editor you saw in step 3\nand click the button to save and deploy. You can access the deployment logs\nfor the stack by clicking on the event name under “Timeline”. Recap To recap, this is how the continuous deployment pipeline works: You push code to Github Github triggers docker image build on Docker Hub Docker Hub triggers redeployment of containers started from the docker image on Docker Cloud PS: For those of you who reached this far, thanks for reading. I’ll make you an\noffer you can’t refuse. Get in touch with us\nand I’ll help you set up a kickass dev-to-prod workflow using chatbots and docker\nthat allows you to test feature branches on disposable staging environments.\nThis is free for the first 10 requests I get (sorry, I have only so much time\nfor doing things for free :)). I hope that helped. Now, dockerize all the things! If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2016-07-25"},
{"website": "Codemancers", "title": "api transformations in rails", "author": ["Yuva"], "link": "https://crypt.codemancers.com/posts/2016-03-25-api-transformations-in-rails/", "abstract": "APIs supporting snake_case and camelCase with Rails Written by Yuva on August 25, 2016; tagged under rails , api Recently, for one of our clients, we encountered one interesting problem. Their\nstack is Rails + React, where Rails is for providing APIs, and React is for\nconsuming those APIs. Frontend team prefers API payloads to be in camelCase.\nThere are existing npm modules which can automatically convert snakecase to\ncamelCase, but (un)fortunately front-end team is not using those. We use AMS (active model serializers) to generate json responses. We are\ndealing with Rails 3, and app cannot be upgraded :). Latest version of AMS has\nout of box support for transformation. Say if we have to expose attributes of\nmodel Post , we have this code: # ignore attributes (or schema), they are for demo'ing the usecase class Post < ActiveModel :: Serializer attributes :id , :title , :authorName , :totalComments end Using camelCase in ruby code looks ugly, but legacy code responds\nwith camelCase for apis. One (obvious) solution is to request frontend team to\nuse npm packages to convert snakecase to camelCase for GET requests,\nand camelCase to snakecase for all POST requests. Making that change would\ninvolve touching lots of components, which we didn’t want to do right away. We\nlooked for alternatives and figured out this approach. Let me show you how the tests for the Post object described above look, so that\nyou get undestand what the problem is: describe PostsController do let ( :post ) { FactoryGirl . create ( :post , title : \"hello world\" , author_name : 'Jon' ) } describe 'GET show' do it \"returns all details of a post\" do get :show , id : post . id expect ( response ) . to be_success expected_json = { \"id\" => post . id , \"title\" => \"hello world\" , \"authorName\" => 'Jon' , # <-------- camelCase here! \"totalComments\" => 0 # <-------- camelCase here! } post_json = JSON . parse ( response . body ) expect ( expected_json ) . to eq post_json end end describe 'PUT update' do it \"updates post fields successfully\" do #                                  params has camelCase here! put :update , id : post . id , title : \"new title\" , authorName : \"Ben\" expect ( response ) . to be_success end end end If you look at specs, generally we don’t use camelCase in ruby to write specs.\nThe code tries to map authorName to author_name , lots of copying going\naround. Let’s go step-by-step in improving this situation: Step 1: Make snakecase/camelCase configurable via urls We modified all APIs to support a param called snakecase . If this query\nparam is set, APIs are served in snakecase, otherwise they are served in\ncamelCase. So, modified specs look like this: describe PostsController do let ( :post ) { FactoryGirl . create ( :post , title : \"hello world\" , author_name : 'Jon' ) } describe 'GET show' do it \"returns all details of a post\" do get :show , id : post . id , snakecase : 1 # see, snakecase=1 here expect ( response ) . to be_success expected_json = { \"id\" => post . id , \"title\" => \"hello world\" , \"author_name\" => 'Jon' , # note we have snakecase here! \"total_comments\" => 0 # note we have snakecase here! } post_json = JSON . parse ( response . body ) expect ( expected_json ) . to eq post_json end end describe 'PUT update' do it \"updates post fields successfully\" do #                          note: authorName is author_name, we have snakecase! put :update , id : post . id , title : \"new title\" , author_name : \"Ben\" , snakecase : 1 expect ( response ) . to be_success end end end And AMS also looks sane, ie it looks like this: # ignore attributes (or schema), they are for demo'ing the usecase # note: we have snakecase here for author_name, and total_comments! class Post < ActiveModel :: Serializer attributes :id , :title , :author_name , :total_comments end Bit of faith restored for ruby developers. APIs typically look like this now: for a GET request: GET /posts/10?snakecase=1 for a PUT request: PUT /posts/10?snakecase=1 (body contains payload) But frontend still expects payload to be in camelCase. It’s simple, avoid the\nquery param snakecase=1 and we are all good. Step 2: Implement snakecase/camelCase intelligence in controller params Some problems can be solved with another level of indirection. We are going\nto play with controller level params , and provide one nice wrapper around it.\nWe are not going to use params directly in our controllers, we are going\nto use a wrapper called api_params . Before, we jump into code, we have to\nsupport usecases like these for frontend: filtering posts: GET /posts?authorName=Jon&page=2&perPage=10 updating a post: PUT /posts/10 , body has: { title: 10, authorName: 'Ben' } Note those camelCases in urls, and POST/PUT body. Now, onto code: class ApisController < ApplicationController # yet to implement snakecase_params method. def api_params params [ :snakecase ]. present? ? params : snakecase_params end end class PostsController < ApisController def index search_params = api_params . slice ( :author_name , :page , :per_page ) @posts = SearchPostsService . new ( search_params ) . fetch render json : @posts , serializer : PostSerializer end def update @post = Post . find ( api_params [ :id ] ) update_params = api_params . slice ( :title , :author_name ) UpdatePostService . new ( @post , update_params ) . save end end Ignoring specifics of code, instead of using params in controller, we are\nusing api_params . What this does is: If snakecase=1 , it means all the params are already in snakecase, and\ncan be directly consumed by Ruby/Rails code. If snakecase is not set (our frontend case), we assume that all the\nparams are in camelCase, and they have to be converted to snakecase\nbefore Ruby/Rails code consumes it. snakecase_params method is interesting, and simple. All it has to do is to\nperform a deep key transformation. Basecamp has already written some code to\ndeep transform hashes. Code can be found in deep hash transform repo.\nWe are going to re-use that code. Code looks like this: module ApiConventionsHelper extend ActiveSupport :: Concern class HashTransformer # Returns a new hash with all keys converted by the block operation. #  hash = { person: { name: 'Rob', age: '28' } } #  hash.deep_transform_keys { |key| key.to_s.upcase } #  # => {\"PERSON\"=>{\"NAME\"=>\"Rob\", \"AGE\"=>\"28\"}} def deep_transform_keys ( hash , & block ) result = {} hash . each do | k , v | result [ yield ( k ) ] = value . is_a? ( Hash ) ? deep_transform_keys ( v , & block ) : v end result end def snakecase_keys ( hash ) deep_transform_keys ( hash ) { | k | k . to_s . underscore . to_sym } end end def snakecase_params HashTransformer . new . snakecase_keys ( params ) end end Now, we can inject this concern into our ApisController , and boom, we have snakecase_params helper. Now, we have backend developers happy with using\nsnakecase, and frontend developers are happy working with camelCase. What\nelse is left? Yes, automatically transforming payload for GET requests. Step 3: Teaching AMS to be aware of snakecase/camelCase Remember, our AMS for Post is still using snakecase. Now based on url params,\nwe have to transform attributes. Let’s take a look at AMS for Post again: class Post < ActiveModel :: Serializer attributes :id , :title , :author_name , :total_comments end AMS makes it easy to play with attributes , again with another level of\nindirection. First, we will use serializer options at controller level to tell AMS to\ntransform payload to camelCase or snakecase. class ApisController < ApplicationController def default_serializer_options { root : false , snakecase : params [ :snakecase ]. present? } end def api_params params [ :snakecase ]. present? ? params : snakecase_params end end All AMSes will now pickup default_serializer_options from controller. In\nthese default options, we are appending snakecase option into AMS world.\nNow, how do we tell AMS to transform payload to camelCase or snakecase?\nIts simple: Use a base serializer, and derive all serializers from it. class BaseSerializer < ActiveModel :: Serializer # override attributes method. def attributes @options [ :snakecase ]. present? ? super : camelize ( super ) end end We have to implement camelize now. We will extend ApiConventionsHelper and\nimplement camelize module ApiConventionsHelper extend ActiveSupport :: Concern class HashTransformer def deep_transform_keys ( hash , & block ) result = {} hash . each do | k , v | result [ yield ( k ) ] = value . is_a? ( Hash ) ? deep_transform_keys ( v , & block ) : v end result end def snakecase_keys ( hash ) deep_transform_keys ( hash ) { | k | k . to_s . underscore . to_sym } end def camelize_keys ( hash ) deep_transform_keys ( hash ) { | k | k . to_s . camelize ( :lower ) } end end def snakecase_params HashTransformer . new . snakecase_keys ( params ) end def camelize ( hash ) HashTransformer . new . camelize_keys ( hash ) end end # use ApiConventionsHelper and derive PostSerializer from this class class BaseSerializer < ActiveModel :: Serializer include ApiConventionsHelper def attributes @options [ :snakecase ]. present? ? super : camelize ( super ) end end class Post < BaseSerializer attributes :id , :title , :author_name , :total_comments end Finally With bit of conventions in place, and playing around controllers, and AMS,\nwe are able to keep both backend developers and frontend developers happy.\nInteresting point to note here is, there is hardly any meta programming. Thanks for reading! If you would like to get updates about subsequent blog\nposts Codemancers, do follows us on twitter: @codemancershq . If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2016-08-25"},
{"website": "Codemancers", "title": "elixir phoenix distillery", "author": ["Yuva"], "link": "https://crypt.codemancers.com/posts/2016-10-06-elixir-phoenix-distillery/", "abstract": "Elixir/Phoenix deployments using Distillery Written by Yuva on November 26, 2016; tagged under elixir , phoenix Of late, we have been porting one of our internal apps from Rails to Phoenix.\nWe are using Capistrano for deploying Rails apps. We have Jenkins CI which\nlistens to features merged into master, and uses Capistrano to deploy to\nproduction. For Elixir/Phoenix, we are looking for something similar. Merge features into\nmaster, let Jenkins CI run, and package Phoenix app which can be run on production.\nIn Elixir world, there are bunch of package managers Exrm: Legacy one, works mostly. Does not support latest Elixir features like\numbrella apps Edeliver: built on top of Exrm, has it’s own learning curve. Distillery: Rewrite of Exrm aimed to be part of Mix itself with goodies. You can read more about Exrm and Distillery here Since Distillery is the latest one and also fits our use case nicely, let’s dig\nthrough that tool more. Documentation of Distillery is quite nice. In this blog\npost, we are going to explore: How to initialize a simple Phoenix app. Configure Jenkins CI script to deploy Phoenix app Doing hot upgrades to deployed Phoenix app without bringing app down Managing versions of app, and Distillery plugins. Before getting started, you need these: Dev machine: Mostly Linux or OSX, your choice. We have tested this on Linux. Build server: System which has the same platform as that of production, mostly\nLinux. You can also use CI server as your build system. Production server: Well, you know this :) Since Elixir has a compilation step where it compiles Elixir code to BEAM, we\nneed to set up a work flow for compiling and deploying our Elixir application.\nA typical work flow would look like: Write code on your development machine, and push the same to any source\ncontrol of your choice, say Github. Make CI server listen to commits, or PR merges, pull code from Github. Install dependencies like Erlang, Elixir, development tools like GCC\non CI server. Validate changes (means running specs, and all). Compile code to BEAM. Package compiled code, along with Erlang runtime. Ship the package to production server, unpack, and start the app. Interesting thing to note here is, CI server needs source code of all dependencies\nof the Elixir app, but production server uses only the compiled BEAM code just\nto make it clear. So there is no bloat on production servers, it’s easy to spin up\nnew servers, unpack the package, and start the app. Let’s explore Distillery, and how it helps in deploying Phoenix apps.\nThe steps to create and run the distillery based app are taken from\nDistillery documentation . Create Phonenix app and initialize Distillery Let’s create a simple Phoenix app which we will use throughout our discussion. > mix phoenix.new --no-ecto --no-brunch phoenix_app\n* creating phoenix_app/config/config.exs\n* creating phoenix_app/config/prod.secret.exs\n* creating phoenix_app/config/test.exs\n* ...\n\nFetch and install dependencies? [ Yn ] Y\n* running mix deps.get We are all set! Run your Phoenix application: > cd phoenix_app\n> mix phoenix.server If it’s all good, you should be able to see the Phoenix app at localhost:4000\nNow, open mix.exs and append distillery defp deps do [{ :phoenix , \"~> 1.2.1\" }, { :phoenix_pubsub , \"~> 1.0\" }, { :phoenix_html , \"~> 2.6\" }, { :phoenix_live_reload , \"~> 1.0\" , only : :dev }, { :gettext , \"~> 0.11\" }, { :cowboy , \"~> 1.0\" }, { :distillery , \"~> 0.10.1\" }] end Now, fetch dependencies using mix, and initialize repository with Distillery > mix deps.get\n> mix release.init This should create a rel folder with config.exs file in it. Please take a\nmoment to go through this file which is well commented to understand what it\ndoes. Configuring CI and generating release pack CI does all the heavy lifting in order to cut a release. You can run these\ncommands locally and replicate the same on CI server. Make sure CI server\nhas these packages installed: Erlang (Erlang solutions) Elixir Cutting a release is very easy, just run this command: > MIX_ENV = prod mix release --env = prod == > gettext\nCompiling 1 file ( .erl ) ... == > Assembling release.. == > Building release phoenix_app:0.0.1 using environment prod == > Including ERTS 8.1 from /usr/local/Cellar/erlang/19.1/lib/erlang/erts-8.1 == > Packaging release.. == > Release successfully built!\n    You can run it in one of the following ways:\n      Interactive: rel/phoenix_app/bin/phoenix_app console\n      Foreground: rel/phoenix_app/bin/phoenix_app foreground\n      Daemon: rel/phoenix_app/bin/phoenix_app start If you look at the output of this command, you’ll notice that mix is packaging everything along with the app. Interpreting mix release output: phoenix_app : Compiled files of all source code in deps folder along with\nsource code of the Phoenix app. ERTS: Erlang runtime which is required to run the server. Note that since\nmix is packaging runtime also, there is no need to install Erlang on\nproduction machine. Helpful commands to run Phoenix app on server. Optionally you can set up Travis/Jenkins to observe features merged into\ngit, automatically pulling latest source code, and packaging app Now, take a look at rel/phoenix_app folder: rel\n└── phoenix_app\n    ├── bin\n    │   ├── nodetool\n    │   ├── phoenix_app\n    │   ├── release_utils.escript\n    │   └── start_clean.boot\n    ├── erts-8.1\n    │   ├── bin\n    │   │   ├── beam\n    │   │   ├── beam.smp\n    ....\n    │   ├── include\n    │   │   ├── driver_int.h\n    │   │   ├── erl_nif.h\n    │   ├── lib\n    │   │   ├── internal\n    │   │   ├── liberts.a\n    │   │   └── liberts_r.a\n    ├── lib\n    │   ├── compiler-7.0.2\n    │   ├── cowboy-1.0.4\n    │   ├── crypto-3.7.1\n    ...\n    │   ├── phoenix-1.2.1\n    │   ├── phoenix_app-0.0.1\n    │   ├── ranch-1.2.1\n    └── releases\n        ├── 0.0.1\n        │   ├── commands\n        │   ├── hooks\n        │   ├── phoenix_app.boot\n        │   ├── phoenix_app.rel\n        │   ├── phoenix_app.script\n        │   ├── phoenix_app.sh\n        │   ├── phoenix_app.tar.gz\n        │   ├── start_clean.boot\n        │   ├── sys.config\n        │   └── vm.args\n        ├── RELEASES\n        └── start_erl.data Going through the top level folders: erts : Erlang runtime, contains beam files, library files to run Erlang lib : all compiled dependencies of phoenix_app , this folder resembles\nvery similar to deps folder at root level releases : Folder of our interest. By default, app version will be 0.0.1 . All the files required to run phoenix_app . Inside a release\nversion folder, all files are packaged into a single .tar.gz file.\nThis file can be copied to production server for running the app. Making successive deployments Erlang has a rich heritage, and Erlang programs are designed to run for\nyears without bringing servers down, which guarantees nearly 100% up time.\nRolling out bug fixes, new features, improvements are done using hot\nupdates to servers. Erlang provides ways to patch existing running code\non production servers so that there is no need to stop and start the\napp. Let’s look at ways to deploy phoenix_app Hot upgrades distillery provides support to create releases which can be applied\nas hot upgrades. The process of generating the tar file is same, but the\ncommand is different. For the sake of brevity, change version number in mix.exs from 0.0.1 to 0.0.2 before proceeding > MIX_ENV = prod mix release --env = prod --upgrade == > Assembling release.. == > Building release phoenix_app:0.0.2 using environment prod == > Including ERTS 8.1 from /usr/local/Cellar/erlang/19.1/lib/erlang/erts-8.1 == > Generated .appup for phoenix_app 0.0.1 -> 0.0.2 == > Relup successfully created == > Packaging release.. == > Release successfully built!\n    You can run it in one of the following ways:\n      Interactive: rel/phoenix_app/bin/phoenix_app console\n      Foreground: rel/phoenix_app/bin/phoenix_app foreground\n      Daemon: rel/phoenix_app/bin/phoenix_app start The command to create a new release is same as that of first run, but\nthere is a new argument, i.e --upgrade . Also, output of command is also\nslightly different. Generated .appup for phoenix_app 0.0.1 -> 0.0.2 ,\nwhere .appup means hot upgrade. There will be new folder 0.0.2 under\nreleases. There will be another file called relup which contains\ninstructions about how to upgrade. It looks like this: {\"0.0.2\",\n [{\"0.0.1\",[],\n   [{load_object_code,{phoenix_app,\"0.0.2\",\n                                   ['Elixir.PhoenixApp.Endpoint',\n                                    'Elixir.PhoenixApp.Gettext']}},\n    point_of_no_return,\n    {load,{'Elixir.PhoenixApp.Endpoint',brutal_purge,brutal_purge}},\n    {load,{'Elixir.PhoenixApp.Gettext',brutal_purge,brutal_purge}}]}],\n [{\"0.0.1\",[],\n   [{load_object_code,{phoenix_app,\"0.0.1\",\n                                   ['Elixir.PhoenixApp.Endpoint',\n                                    'Elixir.PhoenixApp.Gettext']}},\n    point_of_no_return,\n    {load,{'Elixir.PhoenixApp.Endpoint',brutal_purge,brutal_purge}},\n    {load,{'Elixir.PhoenixApp.Gettext',brutal_purge,brutal_purge}}]}]}. This file contains instructions about how to switch between 0.0.1 and 0.0.2 . If the upgrade needs to be rolled back, this file helps in\ndowngrading from 0.0.2 to 0.0.1 . Note about versions Mix knows only semantic versioning. If one has to use SHA-IDs as the version,\nmix will throw errors. Say, you change version from 0.0.2 to 48dcbccd ,\nand try to generate new package, mix throws this error: > MIX_ENV = prod mix release --env = prod --upgrade\nCompiling 11 files ( .ex ) ** ( Mix ) Expected :version to be a SemVer version, got: \"48dcbccd\" If you are coming from Rails world, and used to continuous deployments using\nCapistrano, editing version every time for deployments is a pain. Let’s see\nhow we can get Capistrano kind of continuous deployments. Generating versions on the fly One way to generate versions on the fly is to read version from environment variable.\nAlso, since mix enforces semantic versioning, generating incremental versions\nmakes sense. Capistrano generates folders with YYYYMMDDHHMMSS format, i.e.\nyear, month, date. Change mix.exs file to have version like this: def project do [ app : :phoenix_app , version : ( if Mix . env == :prod , do : System . get_env ( \"APP_VERSION\" ), else : \"0.0.1\" ), elixir : \"~> 1.2\" , elixirc_paths : elixirc_paths ( Mix . env ), compilers : [ :phoenix , :gettext ] ++ Mix . compilers , build_embedded : Mix . env == :prod , start_permanent : Mix . env == :prod , deps : deps ()] end Now when mix is building package for production, the app read the version from\nthe environment variable, otherwise it’s hard coded to 0.0.1 . Since version\ncan be dynamic now, we can let CI specify what version needs to be generated.\nFollowing how Capistrano generates versions, we can do export APP_VERSION = ` date +%Y.%m.%d%H%M ` npm install # for brunch mix deps.get MIX_ENV = prod mix release --env = prod --upgrade So, this code generates the app version with year as major version, month as\nminor version, and as patch version. Note that in\norder for --upgrade to work on CI, you should have previous versions in\nreleases folder, otherwise upgrade will fail because there is no reference\nrelease. If you are using Travis/Circle-CI, make sure that releases folder\nis cached. Recovering from failures, doing proper patch updates Say code which is pushed in is buggy, and CI has made a release. It\ncan happen that hot upgrade itself fails. In such cases, CI will have a\nstale releases like this: rel/\n  phoenix_app/\n    releases/\n      0.0.1 # deploy succeeded 2016.09.250826 # deploy succeeded 2016.09.250829 # deploy failed 2016.09.251007 # new package based on failed deploy package When CI runs again, it will generate a new package assuming previous package\nis a good one. Hot upgrades will start failing from now on! It’s very important to keep a note of which deploy succeeded, or which one\nfailed. There are two ways this can be done: Maintain revisions.log just like how Capistrano keeps track of deploys. Read which version is currently deployed directly from the production server\nitself. If you follow 2nd approach, you can read current running version from the\nfile start_erl.data . It contains runtime version and app version. CI\ncan read that version from production server, and create a hot upgrade\nfrom that version. In addition to --upgrade option, Distillery supports --upfrom option also. This takes in app version from which upgrade\npackage has to be generated. Typically CI code looks like this: export PREV_APP_VERSION = ` ssh mix@10.0.0.5 cat /srv/app/releases/start_erl.data | cut -d ' ' -f2 ` export APP_VERSION = ` date +%Y.%m.%d%H%M ` touch mix.exs # so that new app_version is picked npm install # for brunch mix deps.get MIX_ENV = prod mix release --env = prod --upgrade --upfrom = $PREV_APP_VERSION Now hot upgrades always work! Immutable infrastructure Immutable infra means - services in production have to be treated as immutable\nentities and upgrades should not change anything in production, but instead\nshould be redeployed each time. This means no hot-upgrades are allowed in\nproduction. If you are running stateless services like web servers written in\nPhoenix, sticking with immutable infrastructure is recommended. Just create a\npackage, spin up a new node behind load balancer, run the app, and nuke old\nnodes. Compiling assets using plugins NOTE: We haven’t initialized our app to use brunch, so the following section\nwon’t work. Try creating a new app without --no-brunch option, and\nfollow this section. Mostly people use Phoenix for APIs, and use Reactjs or other JS framework\nfor front-end. If you are reading through Distillery docs, it suggests to push\nassets compilation to shell script itself. Let’s take a small detour and see\nhow we can do that using plugins. A Distillery plugin can be used to hook\ninto the package generation process. It has 5 hooks: before_assembly after_assembly before_package after_package after_cleanup Names are pretty self-explanatory. If you want to know more about these, you can\ntake a look at them here . You can hook into before_assembly block, and compile assets. defmodule PhoenixApp.PhoenixDigestTask do use Mix.Releases.Plugin def before_assembly (% Release {} = _release ) do info \"before assembly!\" case System . cmd ( \"npm\" , [ \"run\" , \"deploy\" ]) do { output , 0 } -> info output Mix.Task . run ( \"phoenix.digest\" ) nil { output , error_code } -> { :error , output , error_code } end end def after_assembly (% Release {} = _release ) do info \"after assembly!\" nil end def before_package (% Release {} = _release ) do info \"before package!\" nil end def after_package (% Release {} = _release ) do info \"after package!\" nil end def after_cleanup (% Release {} = _release ) do info \"after cleanup!\" nil end end environment :prod do set plugins : [ PhoenixApp.PhoenixDigestTask ] set include_erts : true set include_src : false end So, more Elixir code and less shell scripting. When you run release command\nagain, output looks like this: == > Assembling release.. == > Building release phoenix_app:2016.10.251007 using environment prod == > before assembly! ## <- Plugin print here == >\n> brunch build --production 25 Oct 10:07:47 - info: compiled 3 files into 2 files, copied 3 in 3.1 sec\n\nCheck your digested files at \"priv/static\" == > Including ERTS 8.0.2 from /usr/lib/erlang/erts-8.0.2 == > Generated .appup for phoenix_app 2016.09.261241 -> 2016.10.251007 == > Relup successfully created == > after assembly! ## <- Plugin print here == > Packaging release.. == > before package! ## <- Plugin print here == > after package! ## <- Plugin print here == > Release successfully built!\n    You can run it in one of the following ways:\n      Interactive: rel/phoenix_app/bin/phoenix_app console\n      Foreground: rel/phoenix_app/bin/phoenix_app foreground\n      Daemon: rel/phoenix_app/bin/phoenix_app start You can see logs from the plugin throughout the deploy process. Plugins is\nan interesting concept, please take a look at that. You can find whole source code here Thanks for reading! If you would like to get updates about subsequent blog\nposts from Codemancers, do follows us on twitter: @codemancershq .\nFeel free to get in touch if you’d like Codemancers to help you build your\nElixir/Phoenix app. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2016-11-26"},
{"website": "Codemancers", "title": "autodeploy from github using aws codedeploy", "author": ["Revath S Kumar"], "link": "https://crypt.codemancers.com/posts/2016-12-26-autodeploy-from-github-using-aws-codedeploy/", "abstract": "Autodeploy from github using AWS CodeDeploy Written by Revath S Kumar on December 26, 2016; tagged under deployment AWS CodeDeploy is part of Amazon deployment services which can be used to deploy your application across EC2 instances.\nThis post will walk you through how to setup aws codedeploy for you application on github. Create AWS IAM roles The first step towards setting up codedeploy is to setup two IAM roles. One for codedeploy to talk to EC2 instances\nand other for EC2 instance to access s3. A IAM role is a set of permission policies that can be used to grant access to various\nAWS resources by associating with either by an IAM user in same account or different. Let’s create the first role for codedeploy Go to IAM -> roles -> create new Role Give a name for the role “code-deploy” and Goto Next Step In Role Type -> select -> Amazon EC2 below AWS service Roles In Attach Policy select -> AWSCodeDeployRole Create Role Edit the Trust relationship and update the content to the following one { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"codedeploy.amazonaws.com\" }, \"Action\" : \"sts:AssumeRole\" } ] } Now we can create the second role to give EC2 instance to access s3. Go to IAM -> roles -> create new Role Give a name for the role “CodeDeploy-EC2” and Goto Next Step In Role Type -> select -> Amazon EC2 below AWS service Roles In Attach Policy select -> AmazonS3ReadOnlyAccess Create Role Create AWS instance Next step is to Goto EC2 Instances and launch a new instance.\nWhile creating an instance you can choose any instance type but make sure to choose CodeDeploy-EC2 as IAM role in Configure instance. In Add tags section add a tag with Name as key and Value as codedeploy-demo (You can name the instance as per your need) Install code deploy Once the instance is booted up we can install the code deploy agent that instance.\nSince I used ubuntu AMI to create the EC2 instance, we can install the codedeploy agent using apt-get . sudo apt-get install python-pip ruby wget cd /home/ubuntu Now you need to download the agent as per the region of you instance. Here is the list of all regions .\nSince we booted the EC2 instance in Asia Pacific (Mumbai) region we can use the below commands to download and install the codedeploy agent. wget https://aws-codedeploy-ap-south-1.s3.amazonaws.com/latest/install\nchmod +x ./install\nsudo ./install auto Once it is installed you can verify whether the codedeploy agent is running or not by using the command sudo service codedeploy-agent status If the service is inactive, you can start the service using the command: sudo service codedeploy-agent start Prepare the application Next is to add the appspec.yml file to the application, appspec.yml file will have information\non what to install on to instances and what lifecycle events to run. The format for appspec.yml file is version : 0.0 os : linux files : - source : /index.html destination : /var/www/html/ hooks : BeforeInstall : - location : deploy/before_install timeout : 300 runas : ubuntu AfterInstall : - location : deploy/restart_server timeout : 300 runas : ubuntu The beforeInstall hook will will be # deploy/before_install #!/bin/bash sudo rm -f /var/www/html/index.html and AfterInstall hook # deploy/after_install/ #!/bin/bash sudo service apache2 restart You can find more detailed options for appspec.yml on AWS CodeDeploy AppSpec File Reference Setup AWS CodeDeploy Now its time to create a deployment. On aws navigate to AWS CodeDeploy and create new application. Fill in the name of application and instances uing the tag Name and value codedeploy-demo . Now choose the deployment configuration, for the demo purpose I set it as CodeDeployDefault.AllAtOnce . More info on aws doc Now add the the IAM role code-deploy , which we create before as the service role Once the application is created, we can deploy new revision. For the first time, the codedeploy app will ask to connect to Github . Once the github connection is setup, You can provide the repo name along with github username,\nEg: revathskumar/aws-github-auto-deploy , commit id. Now we can click on F Deploy Now , which will deploy to all the instance configured for the codedeployment application. Setup Autodeploy from Github Now we are able to create the deployments, but manually creating the deployments eveytime is tedious.\nSo lets automate the deployments using Github Integrations . For that first we will\ncreate a new IAM policy and user for github. Create IAM policy First we will create a IAM policy which give access to register and create a new deployment,\nalso to create new revision for a deployment group. Choose Create Your own policy from Create Policy and give some name codedeploy-github-access and for policy document use the below template. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"codedeploy:GetDeploymentConfig\" , \"Resource\" : \"arn:aws:codedeploy:ACCOUNT_REGION:ACCOUNT_ID:deploymentconfig:*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"codedeploy:RegisterApplicationRevision\" , \"Resource\" : \"arn:aws:codedeploy:ACCOUNT_REGION:ACCOUNT_ID:application:APPLICATION_NAME\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"codedeploy:GetApplicationRevision\" , \"Resource\" : \"arn:aws:codedeploy:ACCOUNT_REGION:ACCOUNT_ID:application:APPLICATION_NAME\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"codedeploy:CreateDeployment\" , \"Resource\" : \"arn:aws:codedeploy:ACCOUNT_REGION:ACCOUNT_ID:deploymentgroup:APPLICATION_NAME/DEPLOYMENT_GROUP\" } ] } Please make sure you replace the ACCOUNT_REGION, ACCOUNT_ID, APPLICATION_NAME and DEPLOYMENT_GROUP according to your application. Create IAM user Now lets create a new user. In the next screen attach the policy we created before Once the user is created, it will show the Access ID and Secret Access Token Copy those and keep it. Will come handy later. Github Integration To invoke AWS codedeploy from github, we need to configure two integrations on Github.\nBefore we configure we need to generate new token with access to\nrepo status and repo_deployments. Once the token is generated, Copy the token and keep it. We need this token while enabling the integrations. 1) AWS CodeDeploy We can add AWS CodeDeploy integration by navigating to Project Settings -> Integrations and services .\nThen from the Add service dropdown choose AWS CodeDeploy Fill the Application name, Deployment group, AWS region, Access Key, Secret Token and Gihub Token we generated.\nOnce we save it, We can move to next integration. 2) GitHub Auto-Deployment From the same Project Settings -> Integrations and services , this time we can choose GitHub Auto-Deployment from the Add service dropdown. If you don’t have a CI server, You don’t need to check the Deploy on status checkbox. Now when we edit file and commit on master branch or merge any Pull request a new deployment\nwill be created on AWS CodeDeploy. Thanks for reading! If you would like to get updates about subsequent blog\nposts from Codemancers, do follows us on twitter: @codemancershq .\nFeel free to get in touch if you’d like Codemancers to help you build your\nawesome app. References Code Deploy Setup (IAM, EC2) AWS CodeDeploy What Is IAM? IAM Roles If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2016-12-26"},
{"website": "Codemancers", "title": "react server side rendering", "author": ["Revath S Kumar"], "link": "https://crypt.codemancers.com/posts/2016-09-16-react-server-side-rendering/", "abstract": "React.js : Server side rendering Written by Revath S Kumar on September 16, 2016; tagged under reactjs These days server side rendering has become an important feature for heavy client side applications and now most of\nthe client side frameworks support it. Yesterday I tried a bit of react server\nrendering along with express.js and react-router. Setup Express js server and dependencies. We can start with scaffolding a new express.js app using expressjs-generator and installing all the dependencies. We use webpack for client side\nbundling. We can install all the dependencies by running npm i --save react react-dom react-router babel-register and development dependencies by running npm i --save-dev babel-cli babel-core babel-preset-es2015 babel-preset-react babel-preset-stage-0 webpack babel-loader Setup webpack & babel for client. We will keep all our client JavaScript and React components in a new folder named client and put all the compiled js in public/javascripts . Also we shall add a .babelrc to load babel presets and configs. { \"presets\" : [ \"es2015\" , \"react\" , \"stage-0\" ] } Now in webpack.config.js , we will configure the entry point, output directory and babel\nloader. // webpack.config.js var path = require ( 'path' ); module . exports = { entry : './client/app.jsx' , output : { filename : 'app.js' , path : path . join ( 'public/javascripts/' ) }, module : { loaders : [ { test : /\\.jsx?$/ , loader : 'babel-loader' , query : { presets : [ 'react' , 'es2015' ] } } ] } }; Now we can run the following to build client JavaScript files in development mode with debug flag turned on, and watch for changes. webpack --debug --watch For easier use, we can add the command to npm scripts with name webpack:server . Now we just need to run npm run webpack:server . Setup react router Now the basic scaffolding and development setup is finished and time to start\nbuilding our app. We can start with configuring the router. We are planning mainly for\ntwo routes /home to show the rendering of static component and /list to show\nthe server side rendering with some data. First we have to define the entry point which will mount our react-router component to DOM. // client/app.jsx import React from 'react' ; import { render } from 'react-dom' ; import AppRouter from './router.jsx' ; render ( < AppRouter /> , document . querySelector ( '#app' )); Next, define routes in client/router.jsx // client/router.jsx import React from 'react' ; import { Router , browserHistory , Route } from 'react-router' ; import AppRoot from './app-root.jsx' ; import Home from './home.jsx' ; import List from './list.jsx' ; const AppRouter = () => { return ( < Router history = { browserHistory } > < Route path = \"/\" component = { AppRoot } > < Route path = \"/home\" component = { Home } /> < Route path = \"/list\" component = { List } /> < /Route> < /Router> ); }; export default AppRouter ; AppRoot is nothing but a simple layout for our app. // client/app-root.jsx import React , { Component } from 'react' ; import { Link } from 'react-router' ; class AppRoot extends Component { render () { return ( < div > < h2 > React Universal App < /h2> < Link to = \"/home\" > Home < /Link> < Link to = \"/list\" > List < /Link> { this . props . children } < /div> ); } } export default AppRoot ; Setup express js for server side rendering Since we are using React + ES6 for components, we have to use the babel-register on server\nside so that we can write express js routes also in ES6 and import the react routes we\nalready wrote. Please note that, we have to require/import the babel-register at the beginning of\nexpress js entry point app.js . // app.js require ( 'babel-register' ); var express = require ( 'express' ); var path = require ( 'path' ); var favicon = require ( 'serve-favicon' ); // rest of the express js boilerplate Then we rename the routes/index.js to routes/index.jsx , after this we can use the\nreact routes and react components on server side. For server side rendering we use renderToString method from react-dom/sever package and methods like match , createRoutes and RouterContext from react-router . match function in react-router module will match a set of routes to a location and calls a callback, without rendering.\nWe use createRoutes method from react-router to create a set of routes from our client/router.jsx ( appRouter ) component and provide it to match . // routes/index.jsx // express and react imports import appRouter from '../client/router.jsx' ; const routes = createRoutes ( appRouter ()); Once we have a match RouterContext will render the component tree for the given router state and return the component markup as a string with the help of renderToString method. // Express.js route router . get ( '*' , ( req , res ) => { match ({ routes , location : req . url }, ( error , redirectLocation , renderProps ) => { // check for error and redirection const content = renderToString ( < RouterContext {... renderProps } /> ); // pass content to jade view (we'll see it in a while) }) }) Now we have the react components rendered as string and we need to pass this to our pug.js (Previously known as jade) view.\nThe jade view will accept the string in content variable and substitute inside the react app mount point. //- views/index.jade\nextends layout\n\nblock content\n  script(type='text/javascript').\n    window.__INITIAL_STATE__ = !{JSON.stringify(data)}\n  div.container#app!= content Rendering a static component on server /home points to a static component called Home which we are going to render from server. import React from 'react' ; const Home = () => { return ( < div > < h1 > Home < /h1> < /div> ); }; export default Home ; and now when we join the dots the routes/index.jsx will look like this import express from 'express' ; import React from 'react' ; import { renderToString } from 'react-dom/server' ; import { RouterContext , match , createRoutes } from 'react-router' ; import appRouter from '../client/router.jsx' ; const routes = createRoutes ( appRouter ()); const router = express . Router (); router . get ( '/home' , ( req , res ) => { match ({ routes , location : req . url }, ( error , redirectLocation , renderProps ) => { if ( error ) { res . status ( 500 ). send ( error . message ); } else if ( redirectLocation ) { res . redirect ( 302 , redirectLocation . pathname + redirectLocation . search ); } else if ( renderProps ) { const content = renderToString ( < RouterContext {... renderProps } /> ); res . render ( 'index' , { title : 'Express' , data : false , content }); } else { res . status ( 404 ). send ( 'Not Found' ); } }); }); Rendering component on server with data In this section, we are trying to render a list of users, the data source is not DB but an API for demo purpose.\nIn order to render data, we need to fetch data from the server, pass it to a component via context . For this we need to write a Higher Order Component to set the data to context. import express from 'express' ; import request from 'request' ; import React , { Component } from 'react' ; import { renderToString } from 'react-dom/server' ; import { RouterContext , match , createRoutes } from 'react-router' ; import appRouter from '../client/router.jsx' ; const routes = createRoutes ( appRouter ()); class DataProvider extends Component { getChildContext () { return { data : this . props . data }; } render () { return < RouterContext {... this . props } /> ; } } DataProvider . propTypes = { data : React . PropTypes . object }; DataProvider . childContextTypes = { data : React . PropTypes . object }; The above DataProvider will set data to context if we pass it via props named data . The List component will look like, import React , { Component } from 'react' ; class List extends Component { constructor ( props , context ) { super ( props , context ); this . state = this . context . data || window . __INITIAL_STATE__ || { items : []}; } componentDidMount () { this . fetchList (); } fetchList () { fetch ( 'http://jsonplaceholder.typicode.com/users' ) . then ( res => { return res . json (); }) . then ( data => { this . setState ({ items : data }); }) . catch ( err => { console . log ( err ); }); } render () { return ( < ul > { this . state . items . map ( item => { return < li key = { item . id } > { item . name } < /li>; })} < /ul> ); } } List . contextTypes = { data : React . PropTypes . object }; export default List ; The above list component will look for data in context first, then in global state and later in component level state.\nWhile we render it from server, the data will be available in context and component use the data in context to render the initial HTML.\nLater after loading it in browser the component can fetch again and update the data. Now we can setup the route to fetch the data and render the component. router . get ( '/list' , ( req , res ) => { match ({ routes , location : req . url }, ( error , redirectLocation , renderProps ) => { if ( error ) { res . status ( 500 ). send ( error . message ); } else if ( redirectLocation ) { res . redirect ( 302 , redirectLocation . pathname + redirectLocation . search ); } else if ( renderProps ) { request ( 'http://jsonplaceholder.typicode.com/users' , ( error , response , body ) => { const data = { items : JSON . parse ( body )}; const content = renderToString ( < DataProvider {... renderProps } data = { data } /> ); res . render ( 'index' , { title : 'Express' , data , content }); }); } else { res . status ( 404 ). send ( 'Not Found' ); } }); }); Thats it. We successfully rendered our react components from server side with and without data, so that user don’t have to wait\nfor another ajax request after loading the page to see the data. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2016-09-16"},
{"website": "Codemancers", "title": "elixir remote debugging", "author": ["Emil Soman"], "link": "https://crypt.codemancers.com/posts/2017-11-22-elixir-remote-debugging/", "abstract": "Elixir Remote Debugging Written by Emil Soman on November 22, 2017; tagged under elixir , docker , distribution In Elixir, it’s possible to connect nodes together to form a cluster in which\nall the nodes are visible to each other. This feature\ncan be used to inspect and debug production apps running on remote nodes.\nLet’s see how we can connect to an Elixir node running inside a docker container\nremotely from a local iex shell and use Erlang’s debugger tool to set\nbreakpoints and debug the code running in the remote node. Remote node Let’s say we have a host machine running on IP 1.2.3.4 . Our docker container\nwill be running on this host and the command that runs inside the container is: iex --cookie secret --name remote@1.2.3.4 \\ --erl '-kernel inet_dist_listen_min 9000' \\ --erl '-kernel inet_dist_listen_max 9000' \\ -S mix This will start the app inside iex shell and also does the following things: Sets secret as the cookie which is used when connecting nodes together. Sets the node name as remote@1.2.3.4 where 1.2.3.4 is the IP address used to connect to the node. Makes the node listen on port 9000 . This also starts the epmd process if it’s not running already. epmd epmd (Erlang Port Mapper Daemon) is Erlang’s name server which runs by default on the port 4369 . If a local\nnode wants to connect to the remote node remote@1.2.3.4 , it first talks to\nthe epmd process running on 1.2.3.4 and gets the port where the node remote is running which is 9000 . Now the local node can directly talk to the remote\nnode using this port. Docker port mappings The local node needs to access ports 4369 and 9000 on the remote IP\nthat it tries to connect to, which is 1.2.3.4 in our example. The following command starts the docker container and also maps the ports\non the host machine: docker run -p 4369:4369 -p 9000:9000 my_elixir_image Local node This is the command we’ll use to connect to the remote node from our local iex: iex --cookie secret --name local@127.0.0.1 \\ -e \"Node.connect(:'remote@1.2.3.4')\" \\ -S mix run --no-start This will start the iex shell and compile our app and all dependencies,\nbut won’t start our app. This is done because we only want the application\nrunning on the remote node to activate the breakpoints. If we have the same\napp that uses the same modules running locally, the breakpoints may get triggered\nfrom the local app. So we disable the app from running locally using mix run --no-start . When the iex shell starts, it will connect to the remote node by executing the\nexpression Node.connect(:'remote@1.2.3.4') . You can see that the cookie that’s\nused here is same as the cookie that was used when we started the remote node. Once the iex shell is up and running locally, you can use Node.list() to\nverify that the remote node is visible inside our local shell. iex ( local @ 127.0 . 0.1 ) 1 > Node . list () [ :\"remote@1.2.3.4\" ] Debugger Now that our nodes are connected, we can start the Erlang debugger GUI locally\nand start debugging. iex ( local @ 127.0 . 0.1 ) 2 > :debugger . start () You should be able to see the debugger UI which looks like this: Now, we’ll run the following in the local shell to interpret the module that\nwe want to debug: iex ( local @ 127.0 . 0.1 ) 3 > :int . ni ( RemoteDebuggerTest ) { :module , RemoteDebuggerTest } Interpreting a module makes the debugger aware of the source code for the module\nand allows the debugger to add breakpoints in the module\nand attach to processes running the code inside this module. Once the module\nis interpreted, the debugger will list all the processes executing the code\nin the interpreted module: The example program that is currently being debugged is in idle state most of\nthe time because it doesn’t do much other than sending a message to itself and\nsleeping. Now, let’s open our module by double clicking on the module name on the left\npanel of the monitor window which is already open and add a breakpoint by\ndouble-clicking an executable line. Now when this line of code gets executed, we can see in the monitor window\nthat the status of the process changes to break . Now we can attach to this process by double-clicking on it and then we can\ninteract with the debugged process: Read the user manual here to learn how to use the many features in the debugger tool. Hope you found this useful, and do follow us on twitter\n@codemancershq\nto get updates with more Elixir content. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2017-11-22"},
{"website": "Codemancers", "title": "reactjs server side rendering with router v4 and redux", "author": ["Revath S Kumar"], "link": "https://crypt.codemancers.com/posts/2017-06-03-reactjs-server-side-rendering-with-router-v4-and-redux/", "abstract": "ReactJS : Server side rendering with router v4 & redux Written by Revath S Kumar on June 3, 2017; tagged under reactjs When I wrote React.js: Server side rendering a few months back, I used react router v3.0.2. But ever since react router released v4, which is a total rewrite into a declarative format, the old blog post won’t work with react router v4. So I decided to write a new blog as 2nd part of it which uses react router v4 along with redux . Since we already have a blog post explaining the initial setup, I will be skipping the repeated steps needed here but will add the new updates need to use the new router. Major Router Updates Major changes in React Router v4 are Declarative routing No more central routes config Separate packages for web & native No onEnter or onChange hooks/callbacks, instead use component lifecycle hooks Adding React Router v4 to our application Since react router has separate packages for web and native, let go with installing the package needed for the web. npm i --save react-router-dom react-router-config react-router-config package will have configuration helpers to use with StaticRouter for server side rendering.\nIf your application already has react-router package, I recommend you to remove it and use only the above ones. Adding Redux to our application Let add the redux packages need for our application. Since this demo contains async actions we will add redux-thunk package as well. npm i --save redux react-redux redux-thunk If you are not familiar with the redux setup you can follow my previous blog on Getting started with redux . Also, let’s install isomorphic-fetch so we can use fetch on both server and client. npm i --save isomorphic-fetch Setup React Router Setting up the Router will start with defining the routes. // client/routes.js import AppRoot from './app-root' ; import Home from './home' ; import List from './list' ; const routes = [ { component : AppRoot , routes : [ { path : '/' , exact : true , component : Home }, { path : '/home' , component : Home }, { path : '/list' , component : List } ] } ]; export default routes ; And load the routes on client side like // client/app.jsx import React from 'react' ; import { render } from 'react-dom' ; import BrowserRouter from 'react-router-dom/BrowserRouter' ; import { renderRoutes } from 'react-router-config' ; import { createStore , applyMiddleware } from 'redux' ; import { Provider } from 'react-redux' ; import thunk from 'redux-thunk' ; import routes from './routes' ; import reducers from './modules' ; const store = createStore ( reducers , window . __INITIAL_STATE__ , applyMiddleware ( thunk ) ); const AppRouter = () => { return ( < Provider store = { store } > < BrowserRouter > { renderRoutes ( routes )} < /BrowserRouter> < /Provider> ) } render ( < AppRouter /> , document . querySelector ( '#app' )); Here <BrowserRouter> is a new component provided by react router which uses HTML5 history API. The above setup is used only on client side.\nFor server side rendering we will be using <StaticRouter> component. Render static component on Server As same as PART 1, we have a /home route which will render some HTML. No dynamic content or data from API.\nEven though our Home component is same, we have a different setup on routes/index.jsx . // routes/index.jsx import express from 'express' ; import request from 'request' ; import React from 'react' ; import { renderToString } from 'react-dom/server' ; import StaticRouter from 'react-router-dom/StaticRouter' ; import { renderRoutes } from 'react-router-config' ; import routes from '../client/routes' ; const router = express . Router (); router . get ( '*' , ( req , res ) => { let context = {}; const content = renderToString ( < StaticRouter location = { req . url } context = { context } > { renderRoutes ( routes )} < /StaticRouter> ); res . render ( 'index' , { title : 'Express' , data : false , content }); }); module . exports = router ; Render component with data Now when it comes to rendering component with data, we need to make some changes to the express route, setup redux store and\nadd static method on a component to fetch the data and update the store. Since we are using redux we need to setup reducer & action to fetch the user details from API. Here I will be using erikras/ducks-modular-redux pattern, so the constants, reducer & actions will be available in a single file. // client/modules/users.js import 'isomorphic-fetch' ; export const USERS_LOADED = '@ssr/users/loaded' ; const initialState = { items : [] }; export default function reducer ( state = initialState , action ) { switch ( action . type ) { case USERS_LOADED : return Object . assign ({}, state , { items : action . items }); default : return state ; } } export const fetchUsers = () => ( dispatch ) => { return fetch ( '//jsonplaceholder.typicode.com/users' ) . then ( res => { return res . json (); }) . then ( users => { dispatch ({ type : USERS_LOADED , items : users }); }) } Now let’s modify the List component to use the fetchUsers action and also add a fetchData static method which can be used on the server. // client/list.jsx import React , { Component } from 'react' ; import PropTypes from 'prop-types' ; import { bindActionCreators } from 'redux' ; import { connect } from 'react-redux' ; import { fetchUsers } from './modules/users' ; class List extends Component { static fetchData ( store ) { return store . dispatch ( fetchUsers ()); } componentDidMount () { this . props . fetchUsers (); } render () { return ( < div > { this . props . items . map ( item => { return ( < div key = { item . id } > < span > { item . name } < /span> < /div> ) }) } < /div> ); } } const mapStateToProps = ( state ) => ({ items : state . users . items }); const mapDispatchToProps = ( dispatch ) => bindActionCreators ({ fetchUsers }, dispatch ); export default connect ( mapStateToProps , mapDispatchToProps )( List ); Now the updates for the express route. // routes/index.jsx import express from 'express' ; import request from 'request' ; import React , { Component } from 'react' ; import { renderToString } from 'react-dom/server' ; import StaticRouter from 'react-router-dom/StaticRouter' ; import { matchRoutes , renderRoutes } from 'react-router-config' ; import { createStore , applyMiddleware } from 'redux' import { Provider } from 'react-redux' ; import thunk from 'redux-thunk' ; import routes from '../client/routes' ; import reducers from '../client/modules' ; /*eslint-disable*/ const router = express . Router (); /*eslint-enable*/ const store = createStore ( reducers , applyMiddleware ( thunk )); router . get ( '*' , ( req , res ) => { const branch = matchRoutes ( routes , req . url ); const promises = branch . map (({ route }) => { let fetchData = route . component . fetchData ; return fetchData instanceof Function ? fetchData ( store ) : Promise . resolve ( null ) }); return Promise . all ( promises ). then (( data ) => { let context = {}; const content = renderToString ( < Provider store = { store } > < StaticRouter location = { req . url } context = { context } > { renderRoutes ( routes )} < /StaticRouter> < /Provider> ); res . render ( 'index' , { title : 'Express' , data : store . getState (), content }); }); }); module . exports = router ; In the above snippet, matchRoutes will filter the routes and components needed to render the given URL.\nOnce we have the list of routes for the given URL, we can map through each and check whether it has a static method named fetchData . If the component has the fetchData method, then execute those else return a null promise. Once we collect all the promises, executed and updated the store, we can render the component using <StaticRouter> component and return the data and compiled HTML to the client. Now when we navigate to /list , the route we can see the list of users rendered from the server. Handling 404 Next, let’s see how to handle the 404. In this case just rendering the NotFound component is not enough, we have to return back appropriate status code to the client as well. Let’s start with adding NotFound component // client/notfound.jsx import React from 'react' ; import { Route } from 'react-router-dom' ; const NotFound = () => { return ( < Route render = {({ staticContext }) => { if ( staticContext ) { staticContext . status = 404 ; } return ( < div > < h1 > 404 : Not Found < /h1> < /div> ) }} /> ); }; export default NotFound ; In NotFound component, rendering some 404 message is not enough. We should be setting the status on staticContext so that when rendering on the server we can access the status on the context object we passed. Remember staticContext will be available only on the server, so make sure we guard the setting of status with if condition. next, we add the route to handle 404. // client/routes.js import AppRoot from './app-root' ; import Home from './home' ; import List from './list' ; + import NotFound from './notfound' ; const routes = [ { component : AppRoot , routes : [ { path : '/' , exact : true , component : Home }, { path : '/home' , component : Home }, { path : '/list' , component : List } + { + path : '*' , + component : NotFound + } ] } ]; export default routes ; Now we need to update the express routes to set the response status as 404. // routes/index.jsx import express from 'express' ; import request from 'request' ; import React , { Component } from 'react' ; import { renderToString } from 'react-dom/server' ; import StaticRouter from 'react-router-dom/StaticRouter' ; import { matchRoutes , renderRoutes } from 'react-router-config' ; import { createStore , applyMiddleware } from 'redux' import { Provider } from 'react-redux' ; import thunk from 'redux-thunk' ; import routes from '../client/routes' ; import reducers from '../client/modules' ; const router = express . Router (); const store = createStore ( reducers , applyMiddleware ( thunk )); router . get ( '*' , ( req , res ) => { const branch = matchRoutes ( routes , req . url ); const promises = branch . map (({ route }) => { let fetchData = route . component . fetchData ; return fetchData instanceof Function ? fetchData ( store ) : Promise . resolve ( null ) }); return Promise . all ( promises ). then (( data ) => { let context = {}; const content = renderToString ( < Provider store = { store } > < StaticRouter location = { req . url } context = { context } > { renderRoutes ( routes )} < /StaticRouter> < /Provider> ); + if ( context . status === 404 ) { + res . status ( 404 ); + } res . render ( 'index' , { title : 'Express' , data : store . getState (), content }); }); }); module . exports = router ; Handling Redirects After handling 404, now we can handle redirects in a similar way. For redirects, we will be using <Redirect> component from react router. To show the redirection we will be redirecting /list route to a new route /users where we will list the users from API. For this, we will define a new component ListToUsers which utilises <Redirect> . // client/listtousers.jsx import React from 'react' ; import { Route , Redirect } from 'react-router-dom' ; const ListToUsers = () => { return ( < Route render = {({ staticContext }) => { if ( staticContext ) { staticContext . status = 302 ; } return < Redirect from = \"/list\" to = \"/users\" /> }} /> ); }; export default ListToUsers ; As we did in handling 404 , here as well we need to set the status on staticContext to 302 or 301 as per your need. Here I am using 302 . Now let’s update the routes . // client/routes.js import AppRoot from './app-root' ; import Home from './home' ; import List from './list' ; import NotFound from './notfound' ; + import ListToUsers from './listtousers' ; const routes = [ { component : AppRoot , routes : [ { path : '/' , exact : true , component : Home }, { path : '/home' , component : Home }, + { path : '/list' , + component : ListToUsers + } + { path : '/users' , + component : List + } { path : '*' , component : NotFound } ] } ]; export default routes ; Next, make necessary changes for express routes so it will perform redirect // routes/index.jsx // All neeeded imports router . get ( '*' , ( req , res ) => { const branch = matchRoutes ( routes , req . url ); const promises = branch . map (({ route }) => { let fetchData = route . component . fetchData ; return fetchData instanceof Function ? fetchData ( store ) : Promise . resolve ( null ) }); return Promise . all ( promises ). then (( data ) => { // render component to string + if ( context . status === 302 ) { + return res . redirect ( 302 , context . url ); + } res . render ( 'index' , { title : 'Express' , data : store . getState (), content }); }); }); module . exports = router ; Now we have a fully functional server rendered react application. The demo app is available on github and working demo can be found in now.sh If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2017-06-03"},
{"website": "Codemancers", "title": "intro to interceptor", "author": ["Amit Bhavikatti"], "link": "https://crypt.codemancers.com/posts/2018-04-24-intro-to-interceptor/", "abstract": "Interceptor : Your Own Mock Server inside the browser Written by Amit Bhavikatti on April 24, 2018; tagged under chrome , firefox , browser-extension During development, often times front-end devs have to wait for the back-end devs to offer an API to work on. Even after the API is given, if the front-end team wants a different set of data to work on, they again need to wait for the changes to be made and hosted. We, front-end developers resort to techniques like storing the data in a variable or using mock API’s. To overcome this, we are working on a browser extension Interceptor , which lets you define your own response, and everytime the browser requests data from a particular URL, returns you with mock data instead of, from the server. Interceptor in action Let’s say we are developing a simple web-app which tries to autocomplete the name of the countries that we are typing - We would code it as such . An 'onkeyup' eventListener is added to the input element and everytime we type a character, an XHR is fired to https://restcountries.eu to query for all the country names that contain the typed characters. The server returns an array of objects that contain the names of the countries. We further loop through this array and create an HTML string that is appended to the parent ul . Listening to requests If you go to the full-screen mode for the above jsbin , open up Interceptor , start listening to requests by clicking the START LISTENING button, and type any letter, then Interceptor would list all the requests that we sent to restcountries.eu API. Since we are making a request on 'onkeyup' event, the number of requests fired would be equal to the number of characters typed. The screenshot below shows the list of requests made when IN is typed in. The number of requests made are displayed in a small badge as seen. The blue coloured badge-icon signifies that the interception mode is ON which means that Interceptor would return mock data for all the intercepted requests. On toggling the intercept-mode to OFF state, the bagde-icon turns red and all requests will query the backend server instead of being intercepted by the extension. Intercepting requests Suppose one wants to display just one name Afghanistan when the character a is typed as opposed to many countries whose name starts with the letter a . A gif below demonstrates how one can intercept the corresponding request and change its response for the above use case. This README describes all the available features of the extension. We currently support Chrome and Firefox browsers. Download Interceptor Chrome Firefox Development Interceptor is an open-source project. The github repo can be found here . You can find all the releases here . Please feel free to contribute to the project - Create/review PR’s. Do report issues if you find any. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2018-04-24"},
{"website": "Codemancers", "title": "rubyspec for tracepoint", "author": ["Atul Bhosale"], "link": "https://crypt.codemancers.com/posts/2018-02-26-rubyspec-for-tracepoint/", "abstract": "RubySpec for Tracepoint Written by Atul Bhosale on February 26, 2018; tagged under ruby I like to contribute to opensource projects and learn from them. This post is about what I learned while working on my pull request to Ruby Spec which got merged recently. In Ruby github repository I found that it has a spec folder which has a Readme file and that’s how I & learned about Ruby Spec Suite project. Ruby Spec Suite is a test suite which has specs for Ruby methods. It is used to check if any Ruby version passes the specs or not. While going through the issues in Ruby Spec Suite Github repository I came across an issue of TracePoint Specs i.e. to add specs for TracePoint class which were missing since it was introduced in Ruby 2.0. I decided to work on this issue myself and started learning about TracePoint. TracePoint class TracePoint is a Ruby class that lets you listen to events that happen at the Ruby virtual machine level and lets you register callbacks for these events. It provides methods for getting more information about the event. Let’s take an example to trace a method call to find the class where the method is defined. class A\n  def bar ; end\nend last_class_name = nil trace = TracePoint.new ( :call ) do | tp | last_class_name = tp.defined_class\nend\n\ntrace.enable do A.new.bar\n  puts last_class_name # => A end We can provide event names to the new method as a parameter. After tracepoint object is enabled it starts listening to the events and hence we get the value of the last_class_name as A. The following are some other tracepoint events which you can try - * class\n* end\n* call\n* return\n* raise TracePoint Examples method_name = nil\ndef test ; end trace = TracePoint.new ( :call ) do | tp | method_name = tp.method_id\nend\n\ntrace.enable do test puts method_name # => test end trace_value = nil\ndef test ; 'test' end\n\nTracePoint.new ( :return ) { | tp | trace_value = tp.return_value } .enable do test puts trace_value # => test end Running Ruby Specs using mspec tool The mspec gem is used as RSpec-like test runner for the Ruby Spec Suite. The mspec gem can be installed using - gem install mspec If specs are missing for a Ruby class we can contribute by first running a generator to generate spec files for methods of the class in the Ruby Spec folder using the mkspec command - mkspec -c TracePoint After adding specs for a Ruby class we can run specs using mspec - mspec core/tracepoint/ The documentation for mspec is available here . Bugs in TracePoint class For TracePoint#enable & TracePoint#disable I added a spec - TracePoint . new ( :line ) do | tp | event_name = tp . event end . enable { event_name . should equal ( :line ) } I thought of checking what arguments get passed to the block using *args .\nIt contains nil as the value in the *args array. I added assertion for that – TracePoint . new ( :line ) do | tp | event_name = tp . event end . enable do |* args | event_name . should equal ( :line ) args . should == [ nil ] end I asked myself should args be nil? There is no reason for enable to yield nil here. In fact, it should not yield anything. I created an issue for this in the Ruby issue tracker and added a spec for the expected behavior as shown below. Notice how issues are tagged using ruby_bug : ruby_bug \"#14057\" , \"2.0\" ... \"2.5\" do it 'can accept arguments within a block but it should not yield arguments' do event_name = nil trace = TracePoint . new ( :line ) { | tp | event_name = tp . event } trace . enable do |* args | event_name . should equal ( :line ) args . should == [] end trace . enabled? . should be_false end end For TracePoint#new I initilized an object without a block and it raised a ThreadError - >> TracePoint . new ( :line ) ThreadError : must be called with a block from ( irb ): 1 :in `new' from (irb):1 from /Users/atul/.rvm/rubies/ruby-2.4.0/bin/irb:11:in ` < main > ' Why did ThreadError get raised? We are not dealing with threads in this code. This looks like a bug, there is no need for a ThreadError to be raised if block is not provided for TracePoint#new. We can write a spec for this bug – ruby_bug \"#140740\" , \"2.0\" ... \"2.5\" do it 'expects to be called with a block' do -> { TracePoint . new ( :line ) } . should raise_error ( ArgumentError ) end end The bugs have been reported on Ruby issue tracker: TracePoint#enable and TracePoint#disable should not yield arguments. TracePoint#new without a block should not raise ThreadError. Adding specs for bugs We need to add specs for bugs so as to reflect the correct behavior of the method. mspec provides a guard ruby_bug that wraps the spec showing what is considered to be the correct behavior. ruby_bug \"#140740\" , \"2.0\" ... \"2.5\" The ruby_bug method takes 3 arguments i.e. 1. bug id - from Ruby Issue Tracking website\n2. version - which version of Ruby is affected by this bug.\n3. block - the spec block Contributing to Ruby Specs helps to learn and improve our Ruby skills. I hope you find this useful to learn more about Ruby & contributing to the Ruby Spec Suite. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2018-02-26"},
{"website": "Codemancers", "title": "leveraging pundit", "author": ["Akshay Sasidharan"], "link": "https://crypt.codemancers.com/posts/2018-07-29-leveraging-pundit/", "abstract": "Using Pundit for authorization in Rails - recipes and best practices Written by Akshay Sasidharan on July 29, 2018; tagged under rails Web applications involving user management has two parts to it, which is authentication and authorization. And you don’t get to authorization without authentication, as we can’t determine what you can do unless we know who you are in the first place. Hand rolling out user authentication is a tedious task and majority of the Rails community has delegated out authentication to cool gems such as Devise . So in this post, we will be talking about another awesome gem which you can leverage to delegate out authorization. And that is Pundit . So what is Pundit? When there arises need for restricting access to your application for certain users, role based authorization comes into play. This is where you can make leverage of Pundit. Pundit helps us to define policies which are PORC - Plain Old Ruby Classes - which means that the class does not inherit from other classes nor include in other modules from the framework. Thus makes it very easy to understand the code. We would still need to define roles for our Users. But now the advantage is that we get to keep our controllers and models skinny. Policies that you define takes away code complexity from the model/controller which otherwise would have been used to determine access to a particular page. Makes our life easy, don’t you think? Setting up Pundit It’s very easy to set it up into your application. The documentation for the gem is well explained. Nonetheless, let me put it down here: Add gem 'pundit' to your Gemfile . Within your application controller include Pundit . Run command bundle install . Optionally run rails g pundit:install which will set up an application policy with some useful defaults. The Policies will be defined in app/policies/ directory. And don’t forget to restart the Rails server so that Rails can pick up new classes that you define there. Understanding Policies Like mentioned earlier, policies are PORC, which houses the authorization for a particular page. Let’s look at a policy class example taken out from the documentation. class PostPolicy attr_reader :user , :post def initialize ( user , post ) @user = user @post = post end # CRUD actions def update? user . admin? or not post . published? end end This is a policy defined to impose restriction for updating a post if the user is an admin, or if the post is unpublished. Characteristics of Policy class The policy name should begin with the name of the model it corresponds to and should always be suffixed with Policy . So in the above example - PostPolicy would be the policy for Post model. The initialize method of the policy would need the instance variable user and the model to be authorized. On a sidenote, we can also get by if the model is simply some other object we want to authorize. For example, say a service or form object which has conditions to be checked on it so as to perform the controller action. The method names should correspond to controller actions suffixed with a ? . So for controller actions such as new , create , edit etc, the policy methods new? , create? , edit? etc are to be defined NOTE: Incase the controller does not have access to current_user method we can define a pundit_user method which will then be used instead. def pundit_user User . find_by_other_means end We can further abstract this Policy if we run the generator rails g pundit:install , which creates an Application policy with defaults for controller actions and also takes care of the initialization part. This can be inherited by other policies. class ApplicationPolicy attr_reader :user , :record def initialize ( user , record ) @user = user @record = record end def index? false end def show? false end def create? false end def new? create? end def update? false end def edit? update? end def destroy? false end class Scope attr_reader :user , :scope def initialize ( user , scope ) @user = user @scope = scope end def resolve scope end end end But hold on sec, what is a class Scope doing in the generated ApplicationPolicy? .\nAnd that is what makes Pundit even more awesome, which we will be getting into soon. With this generated base policy can simpify our PostPolicy as class PostPolicy < ApplicationPolicy # Here we are overriding :update? inherited from ApplicationPolicy def update? user . admin? or not record . published? end end With this setup in place, let’s see what changes at the controller level: class PostController < ApplicationController def update post = current_user . posts . find ( params [ :id ] ) authorize post if post . update ( post_params ) redirect_to post else render :edit end end # other controller actions end With this code in plate, update action of the controller when invoked is authorized and the authorize method that we invoke here will retrieve the policy for the given record, initialize it with the record and current user and finally throw an error if the user is not authorized to perform the given action. Understanding Scopes Scopes are just like using the scopes you define for a model. But in our case, these scopes are done within the policy in context of the user’s role for a particular controller action. Scopes are used to retrieve a subset of the records that we have. For example, in a blog app, a non admin user should be restricted to see only posts which has been published but not in draft state.\nI see you already imagining the controllers and models becoming thinner. Let’s rework our Post policy: class PostPolicy < ApplicationPolicy # Inheriting from the application policy scope generated by the generator class Scope < Scope def resolve if user . admin? scope . all else scope . where ( published : true ) end end end def update? user . admin? or not record . published? end end Here we have created a class which will scope the posts based on the user’s role. And in order to use it in our controller, we just need to make use of the method policy_scope . Characteristics of Scope class They are too PORC, which are to be nested within the policy class. It needs to initialize with a user and a scope which can either a be ActiveRecord class or ActiveRecord::Relation. It needs to define a resolve method which scopes based on the user role. So now, we revise our Post controller’s index to be like: class PostController < ApplicationController def new # code to render new view end def create # code to create end def edit # code to render edit end def update post = current_user . posts . find ( params [ :id ] ) authorize post if post . update ( post_params ) redirect_to post else render :edit end end def show # code to render show end def index policies = policy_scope ( Post ) # code to render index end end The index action will show only published posts unless the user is an admin. Good Practices that can be leveraged using pundit Keeping authorization explicit Rather than making authorization or scoping implicit we rather be explict about it. We can add in checks at the ApplicationController level so that exception is raised if we forget to add in authorize or policy_scope in our controller. class ApplicationController < ActionController :: Base include Pundit after_action :verify_authorized , except : :index after_action :verify_policy_scoped , only : :index end But still, we can make use of skip_authorization or skip_policy_scope in circumstances where you don’t want to disable verification for the entire action. Keeping a closed system If we are making use of a Base policy such as ApplicationPolicy . We can fail gracefully if at all an unauthenticated user makes through. class ApplicationPolicy def initialize ( user , record ) raise Pundit :: NotAuthorizedError , \"must be logged in\" unless user @user = user @record = record end end Handling errors on authorization Since Pundit::NotAuthorizedError will be raised if not authorized, we’d need to handle it gracefully.\nThis can be done by making use of rescue_from directive for Pundit::NotAuthorizedError and then pass in a method to handle the exception. We can also go a step further and customize error messages based on which policy’s action was not authorized. class ApplicationController < ActionController :: Base protect_from_forgery include Pundit rescue_from Pundit :: NotAuthorizedError , with : :user_not_authorized private def user_not_authorized policy_name = exception . policy . class . to_s . underscore flash [ :error ] = t \" #{ policy_name } . #{ exception . query } \" , scope : \"pundit\" , default : :default redirect_to root_path end end And you can have your locale file to be like this: en : pundit : default : 'You cannot perform this action.' post_policy : update? : 'You cannot edit this post!' create? : 'You cannot create posts!' This is a way to setup error messages for authorization as here we make use of the information NotAuthorizedError provide ie. what query (e.g. :create?), what record (e.g. an instance of Post), and what policy (e.g. an instance of PostPolicy) caused the error to be raised. Ultimately, it’s up to you on how you organize your locale files.\nAlternatively, we can also serve them with 403 error page by configuring in application.rb config . action_dispatch . rescue_responses [ \"Pundit::NotAuthorizedError\" ] = :forbidden Extending policy with multiple roles Often there comes in requirement that a particular CRUD action’s authorization varies on multiple roles.\nIn context of our example, say, there also comes in a role ‘premium’. And now there exists posts which can be viewed by premium users and the admin only. No worries, just create a new ‘premium’ role and update our PostPolicy as below: class PostPolicy < ApplicationPolicy # Inheriting from the application policy scope generated by the generator class Scope < Scope def resolve if user . admin? scope . all elsif user . premium? scope . where ( published : true ) else scope . where ( published : true , premium : false ) end end end def update? user . admin? || ! record . published? end def show? return user . premium? || user . admin? if record . premium? true end end With the above changes now a normal user can’t view premium posts in the index view listings as we are scoping it out and also we are authorizing the show page as to not allow non-premium users to see premium post content. Pretty neat isn’t it? We no longer need to delegate the app execution flow to model or controller and let Pundit do all the heavy lifting. This gives us fine granularity in controlling role based access and now that we understand how Pundit is structured and what conventions we need to follow, writing authorization code becomes intuitive. Skinny controllers and skinny models FTW! If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2018-07-29"},
{"website": "Codemancers", "title": "ruby 2 6 adds key error receiver key", "author": ["Atul Bhosale"], "link": "https://crypt.codemancers.com/posts/2018-08-06-ruby-2-6-adds-key-error-receiver-key/", "abstract": "Ruby 2.5 adds KeyError#receiver and KeyError#key Written by Atul Bhosale on August 6, 2018; tagged under ruby Ruby 2.5.0 was recently released . Earlier to Ruby 1.9 Hash#fetch method would raise IndexError for an invalid key.\nAn IndexError is raised when the given index value is invalid. irb ( main ): 001 : 0 > hash = { 'foo' => 'bar' } irb ( main ): 002 : 0 > hash . fetch ( 'foo' ) IndexError : key not found from ( irb ): 2 :in `fetch' from (irb):2 from :0 KeyError class was introduced in Ruby 1.9 and it inherits from IndexError class. It is raised when the specified key is not found. >> hash = { foo : :bar } => { :foo => :bar } >> hash . fetch ( :baz ) KeyError : key not found : :baz from ( irb ): 2 :in `fetch' from (irb):2 from /home/atul/.rvm/rubies/ruby-2.4.4/bin/irb:11:in ` < main > ' It was proposed that having KeyError#receiver and KeyError#name (or KeyError#key ) like NameError, will help to debug and find typo on did_you_mean gem. begin h = { foo : 1 , bar : 2 } h . fetch ( :bax ) rescue KeyError => e p e . name #=> :bax p e . receiver #=> {foo: 1, bar: 2} equal `h` end There was a discussion about what method name is best between name and key .\nBenoit Daloze suggested that name is more consistent with other error\nclasses such as NameError#name and NoMethodError#name . Having key as\nmethod name was approved. Ruby 2.5.0 Ruby 2.5.0 provides KeyError#receiver and KeyError#key as methods to return details of receiver object and key which was not found. begin h = { foo : 1 , bar : 2 } h . fetch ( :bax ) rescue KeyError => e p e . key #=> :bax p e . receiver #=> {foo: 1, bar: 2} end I hope these methods are useful in your future debugging attempts. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2018-08-06"},
{"website": "Codemancers", "title": "ruby 2 6 added options exception full message", "author": ["Atul Bhosale"], "link": "https://crypt.codemancers.com/posts/2018-07-30-ruby-2-6-added-options-exception-full-message/", "abstract": "Ruby 2.6 added options to Exception#full_message Written by Atul Bhosale on July 30, 2018; tagged under ruby Ruby 2.6.0-preview2 was recently released . Ruby 2.5.0 The Exception#full_message method returns a formatted string of the exception. >> e = StandardError . new ( 'foo' ) => #<StandardError: foo> >> e . full_message => \" \\e [1mTraceback \\e [m (most recent call last): \\n (irb):14:in `full_message': \\e [1mfoo ( \\e [1;4mStandardError \\e [m \\e [1m) \\e [m \\n \" The string contains escape sequences too. It was proposed that escape sequences should be excluded from the error message. Nobuyoshi Nakada said that since Exception#full_message is expected to return\nthe message printed to stderr, escape sequences are intentional. Benoit Daloze suggested that we can provide an option to disable escape sequences and it was approved. Ruby 2.6.0 Ruby 2.6.0 provides highlight option to the Exception#full_message method to exclude escape sequences. >> e = StandardError . new ( 'foo' ) => #<StandardError: foo> >> e . full_message => \" \\e [1mTraceback \\e [m (most recent call last): \\n (irb):11:in `full_message': \\e [1mfoo ( \\e [1;4mStandardError \\e [m \\e [1m) \\e [m \\n \" >> e . full_message ( highlight : false ) => \"Traceback (most recent call last): \\n (irb):12:in `full_message': foo (StandardError) \\n \" The order argument provides options to place the error message and the\ninnermost backtrace come at the top or the bottom of the result returned by Exception#full_message.\nThe order value must be either :top or :bottom . >> e = StandardError . new ( 'foo' ) => #<StandardError: foo> >> e . full_message => \" \\e [1mTraceback \\e [m (most recent call last): \\n (irb):2:in `full_message': \\e [1mfoo ( \\e [1;4mStandardError \\e [m \\e [1m) \\e [m \\n \" >> e . full_message ( highlight : false ) => \"Traceback (most recent call last): \\n (irb):3:in `full_message': foo (StandardError) \\n \" >> e . full_message ( highlight : false , order : :top ) => \"(irb):4:in `full_message': foo (StandardError) \\n \" >> e . full_message ( highlight : false , order : :bottom ) => \"Traceback (most recent call last): \\n (irb):5:in `full_message': foo (StandardError) \\n \" Let’s set a backtrace for an error object and try order option for Exception#full_message method. >> e = StandardError . new ( 'foo' ) => #<StandardError: foo> >> e . set_backtrace ( [ \"a.rb:1\" , \"b.rb:2\" ] ) => [ \"a.rb:1\" , \"b.rb:2\" ] >> e . full_message ( highlight : false , order : :top ) => \"a.rb:1: foo (StandardError) \\n\\t from b.rb:2 \\n \" >> e . full_message ( highlight : false , order : :bottom ) => \"Traceback (most recent call last): \\n\\t 1: from b.rb:2 \\n a.rb:1: foo (StandardError) \\n \" Hope you use these options with Exception#full_message to debug ruby applications. Here is the relevant commit and discussion . If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2018-07-30"},
{"website": "Codemancers", "title": "frozen middleware with rack freeze", "author": ["Atul Bhosale"], "link": "https://crypt.codemancers.com/posts/2018-06-07-frozen-middleware-with-rack-freeze/", "abstract": "Frozen middleware with Rack freeze Written by Atul Bhosale on June 7, 2018; tagged under ruby One of my favourite pastimes is to go through GitHub issues for\nlibraries I like. One of those is the Rack gem, where I found an issue\ntitled “ Middleware should be frozen by default \". A couple\nof questions I had were: What exactly is a frozen middleware? and why\nshould that be done? Example: Web request count As a simple first example, let’s consider a Rack middleware which counts\nthe number of requests received by the server. A very simple (and\nbroken) implementation might look like this: class Counter def initialize @counter = 0 end def call ( _env ) counter = @counter sleep 1 counter += 1 @counter = counter [ 200 , { 'Content-Type' => 'text/html' }, [ \" #{ @counter } \" ]] end end The @counter instance variable gets incremented each time call method gets called, which happens for every request. If you’re not\nfamiliar with what a middleware is, or how they get used, these\nresources might be useful: Rack Middlewares on Railscasts Understanding Rack apps and Middleware Introduction to Rack Middleware Middleware recipes on Sinatra Recipes Running this application in a single threaded environment results in the\nfollowing output: You can run this is in single threaded mode as - Rack :: Server . start :app => Counter . new , server : :puma , max_threads : 1 , min_threads : 1 Running this in a multi-threaded environment, however, results in the\nfollowing output: In the multi-threaded environment, the counter doesn’t increment. This is called a race condition, and occurs when two or more threads can access shared data and they try to change it at the same time. Because the thread scheduling algorithm can swap between threads at any time, you don’t know the order in which the threads will attempt to access the shared data. Hence, the result of the change in data is dependant on the thread scheduling algorithm i.e. both threads are racing to access/change the data. Achieving thread-safety When we want to avoid thread safety issues in multi-threaded environments we have some options: Not mutate the state in the middleware Freeze middleware instances to catch the thread-safety issues in the middleware that you didn’t write yourself. Use data structures from the concurrent-ruby gem. How do we do this? Example: Web request thread-safe count class Counter def initialize @atomic = Concurrent :: AtomicReference . new ( 0 ) end def call ( _env ) @atomic . update { | v | v + 1 } [ 200 , { 'Content-Type' => 'text/html' }, [ \"{@atomic}\" ]] end end The word atomic it means that the contents of the block are executed to completion without other threads being able to read/modify the value (note that this is not same as a mutex). Multiple threads attempting to change the same AtomicReference object will not make it end up in an inconsistent state. Freezing middleware instances Rack middleware is initialized only on the first request of the process. So any instance variables acts like class variables, and modifying them in call() isn’t thread-safe. It’s necessary to dup the middleware to be thread-safe. A middleware should be frozen to avoid potential issues with handling concurrent requests. Rack recently introduced a freeze_app method to freeze middleware instances. An example usage of that would be: use ( Class . new do def call ( env ) @a = 1 if env [ 'PATH_INFO' ] == '/a' @app . call ( env ) end freeze_app end ) In this example, we are initializing an instance variable to 1 when we hit the /a url. We call freeze_app method in the middleware. When we run this program, and hit /a multiple times the freeze_app method will notify us that there is a problem by raising an exception, which you wouldn’t otherwise know: FrozenError : can ' t modify frozen #<Class:0x00007f9b0d1e95b0> The server will respond with 200 for all other URLs because we are not\nmodifying the instance variable in those. Internally freeze_app method calls a .freeze on the middleware instances. Can I unfreeze a frozen object? No, it’s not possible in MRI and JRuby . Hope you found this useful. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2018-06-07"},
{"website": "Codemancers", "title": "phonenix webpack", "author": ["Atul Bhosale"], "link": "https://crypt.codemancers.com/posts/2018-10-03-phonenix-webpack/", "abstract": "Switching an existing Phoenix app from Brunch to Webpack Written by Atul Bhosale on October 3, 2018; tagged under webpack , phoenix Phoenix version 1.4 onwards will be using webpack as a module bundler for javascript. This blog post is about the steps which I followed to switch from brunch to webpack in an existing Phoenix project. Clone Phoenix Since Phoenix 1.4 is unreleased as of when this blog is published, we will generate a new project using Phoenix source installer. Clone Phoenix and run - cd phoenix/installer\nmix phx.new dev_app --dev Run npm install in dev_app/assets folder. Copy this assets folder to the existing Phoenix project folder which we want to switch from brunch to webpack. Delete brunch config Delete brunch-config.js , package-lock.json & node_modules folder. Copy required dependencies from existing package.json to package.json file in assets folder except for brunch related dependencies like brunch , babel-brunch etc. Delete package.json from the root folder. Run npm install after adding the dependencies to the new package.json file. Copy js & css files from static folder Copy js & css files from web/static folder to assets/js & assets/css folder & delete web/static folder. Delete default styles used for starter applications Delete assets/css/phoenix.css & its entry from assets/css/app.css Update npm watch script Update npm watch script in assets/package.json to – \"watch\" : \"webpack --mode development --watch-stdin\" Using --watch-stdin option exits the nodejs process when stdin(standard input) is closed. Update webpack watcher config Update webpack watcher config in config/dev.exs to – watchers : [ node : [ \"node_modules/webpack/bin/webpack.js\" , \"--mode\" , \"development\" , \"--watch-stdin\" , cd : Path . expand ( \"../assets\" , __DIR__ )]] Add to .gitignore Add node_modules under assets to .gitignore /assets/node_modules Update babel presets Update babel presets which are required in the project. {\n  \"presets\": [\n    \"env\",\n  ]\n} Start Phoenix and check webpack Start Phoenix with - iex -S mix phx.server and check if webpack has built the js bundle. Here is the reference PR and discussion . If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2018-10-03"},
{"website": "Codemancers", "title": "decluttering translations in rails apps", "author": ["Manu Raj"], "link": "https://crypt.codemancers.com/posts/2019-11-12-decluttering-translations-in-rails-apps/", "abstract": "Decluttering Translations in Rails Apps Written by Manu Raj on November 12, 2019; tagged under rails Internationalization(i18n) or translations are an integral part of any application\nthat caters to a global audience. Rails supports this out the box. To add a set of translations to your rails app, it is fairly easy. By default,\nrails will provide an en.yml inside config/locales folder. en:\n  hello: \"Hello world\" What this provides is that the key hello inside locale :en will have the\nvalue Hello world . In this post we are going to focus on how to grow the translation files as\nthe application gets bigger and avoid clutters and duplications as much as we\ncan. Consider an application with namespaces and a set of different contexts\nunder it. It’s common to end up having a structure like below in translation files. en:\n  admin:\n    attendance \n      policies:\n        index:\n          blank:\n            title: Some title We have some namespaces to cross until we reach the key we require which is title . Consider the directory app/views/admin/attendance/policies for all the\nviews for attendance namespace under admin.\nIt contains several files index.html.slim , _table.html.slim , blank.html.slim Now, to use that title for the file blank.html.slim we would be using it as t('admin.attendance.policies.index.blank.title') .\nThis is not good, and this can only get worse if our keywords get bigger\nor if we have more namespaces. Use rails lazy-lookup Rails provides us with a lazy lookup functionality where in which we don’t have\nto provide the whole path to the translation if it is directly in sync with the\npath to the view. Let me refactor the above and show you. I updated the view file app/views/admin/attendance/policies/_blank.html.slim to have the translation as: t(.title) Now I update the translation file as below (please note the hierarchy) en:\n  admin:\n   attendance: \n      policies:\n        blank:\n          title: Some title Here I removed the index and made the blank key come directly under the policies key. What this resembles is the same directory structure as we\nhave in our app (check the path shared above). If this structure matches, rails will automatically pick up t('.title') . Long chained paths are not required. This would require us to re-arrange the translation file a bit but the extra effort would be worth it. Grouping common terms We have a lot of forms and components which make use of the same words as Submit , Cancel etc that are being used in many places. We should group them\nunder minimal hierarchy i.e without a long chain. For example, form related\nthings can be grouped under en:\n  form:\n    submit: Submit\n    cancel: Cancel Then to be used as t('form.submit') or t(:submit, scope: :form) FYI: You can also chain scopes like this t(:key, scopes: [:scope_one, :scope_two]),\nlet’s stick to minimal as possible It may not be form but could be any other entity of the app that you could come\nacross, end goal being to prevent this t('admin.some_module.some_resource.some_action.some_form') == \"Submit\" and t('admin.another_module.another_resource.another_action.another_form') == \"Submit\" NOTE: I have seen the usage of human_attribute_name , it’s also good in\nabstracting out the complexity - wasn’t mentioned here because the intent of\nthis write up was to primarily focus on views and partials that may not be\ndealing with model objects all the time, even the ones that are having a model object may not be displaying the exact name. Splitting up the file Since we have lots of modules we could go ahead and split up the translation\nfile as well. Considering the above, any module-specific translations can be\nmade to go under locales/admin/<module_name>/*.yml The current example we are looking at have all the translations at this\nlocation: config/locales/en.yml . Following the split-up approach, we will end up with the path like config/locales/admin/attendance/en.yml Rails will lazy lookup the translation even in this manner. Although splitting up would not help reduce redundancy, it would help in better\narrangement. Approach Combining all three methods would be an ideal approach. Start with making\nlazy lookup work and then re-arrange the commonly used ones and finally\nsplitting the files up. There is no hard and fast rule as in it should be done\nin the same order but eventually, we should be able to do all these. Deciding a word to be moved globally or under a namespace is a preference,\nexample if something like ‘Step 1’ is used in all modules then it could very\nwell be moved out of the modules. Only the step definitions could change. If a\nword is only used in two modules or so it’s okay to still have it in the modules\nitself, but the ideal use of translations is to not repeat any words. i.e if we\nneed to change something we have to change it only in one place. TIP: While doing a refactor take a look at this gem i18n-tasks .\nIt can help you find and manage missing and unused translations. Reference Check out all the nifty extras that you can do with the i18n api here Rails doc If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2019-11-12"},
{"website": "Codemancers", "title": "sorbet type checker for ruby", "author": ["Akshay Sasidharan"], "link": "https://crypt.codemancers.com/posts/2019-08-12-sorbet-type-checker-for-ruby/", "abstract": "Sorbet - A static type checker for Ruby Written by Akshay Sasidharan on August 12, 2019; tagged under ruby Sorbet is a type checker which is now joining the flock on the duck typed Ruby Land. In this post, we shall explore why we would need static type checking and what are the implications of incorporating it. What is a Type system? Types are how a programmer can impart meaning to the raw sequence of bits such as a value, constant or some object within a program and also helps the language’s compiler/interpreter to allocate memory accordingly. Type safety is used to enforce constraints on the various types used in a programming language. This helps to catch hold of operations between in-compatible types. These checks are done either at compile-time - static type checking or at run-time - dynamic type checking . To enforce type safety, the language needs a Type system which is defined as a part of the programming language’s compiler/interpreter. Based on how strictly the compiler/interpreter enforces type safety check determines whether a language is strongly or loosely typed. Where does Ruby fit in all this? Ruby is a dynamic and strongly typed language. And it is what that enables us to do duck typing or metaprogramming capabilities which we love Ruby for. Strong typed nature of ruby helps us to enforce type safety. But these type of errors can only be found only during run-time. And for this reason, we keep relying on tests to help us out to catch them. Certain pain points come in Ruby when having a huge codebase and a large collaborating team. Hidden bugs: Bugs in code like - method not being found, invalid arguments being provided, uninitialized constant errors, etc can only be found during run-time. Whereas with a static type checker, we can find it even before the execution of the program and thus save time. Explicit documentation: When dealing with undocumented code, programmers usually have confusions like - what arguments does this method take and what could it return? What type does variable hold? etc. Even with documented code, there is a possibility of it falling out of sync. Static checker enforces type within code such that this confusion can easily be avoided. Code refactoring: To catch errors post refactoring cycle, one has to depend upon tests to ensure nothing has been broken. In case there is something broken, we can only know it during runtime. But with a static checker, refactoring cycle becomes easier as a programmer can be confident when changing interfaces such that the type checker would catch hold of any part of the programs which is inconsistent to the updated interface. Furthermore, the interpreter can be made to leverage the explicit types specified to produce better-optimized machine code and IDEs to implement auto-completion features. In order to address these pain points and missing productivity for not having static type checker, one must either depend on an elaborate test suite which could not still guarantee 100% type safety or consider to rewrite in a language which addresses them. A rewrite may not be practical in most cases because it will not be inclined towards the actual business goals. Enter Sorbet A gradual type system that can be adopted incrementally in order to introduce static type checking to your code. You can start adding type checking to existing parts of the codebase along with the development of other features. Adopting Sorbet Add these two gems for Sorbet command-line interface and runtime into your Gemfile: gem 'sorbet' , :group => :development gem 'sorbet-runtime' > bundle install Now initialize sorbet for the project by running the command: > srb init This creates the following directory and needs to be version controlled. sorbet/\n│ # Default options to passed to sorbet on every run ├── config\n└── rbi/\n    │ # Community-written type definition files for your gems ├── sorbet-typed/\n    │ # Autogenerated type definitions for your gems ├── gems/\n    │ # Things defined when run, but hidden statically ├── hidden-definitions/\n    │ # Constants which were still missing └── todo.rbi Config files contain simply the options and arguments to be passed onto the command srb tc (which statically type checks the code). RBI files are “Ruby Interface” files. Sorbet uses RBI files to learn about constants, ancestors, and methods defined in ways it doesn’t understand natively. These files are autogenerated but can also be handwritten. You can learn more about RBI files from the official docs . sorbet-typed is a folder which contains RBI files for the gems pulled out from a community-driven central repositry . And voila! You are all set to start type-checking your code. Type checking your code It all starts with a magical comment # typed: which the sorbet team calls as sigils . This is to be added onto the file which is to be typed checked. There are various strictness level based on which srb decides what to report and what to silence. We can start with # typed: true At # typed: true , things that would normally be called “type errors” are reported. This includes calling a non-existent method, calling a method with mismatched argument counts, using variables inconsistently with their types, etc. An example Let us consider a silly example to showcase what sorbet is capable of: # typed: true class Farm def initialize ( animals = [] ) @animals = animals end def all_speak make_animals_speak ( @animals ) end def animal_count @animals . length end def insert_animals ( animals ) animals . each { || animal | @animals << animal } end private def make_animals_speak ( animals ) if animals animals . each { | animal | p animal . speak } else p 'awkward silence.. 😪' end end end class Duck def initialize @speech = 'quack' end def speak @speech end end class Cow def initialize @speech = 'moo' end def speak @speech end end class Dog def initialize @speech = 'woof woof' end def speak @speech end end farm = Farm . new farm . speak # undefined method 'speak' ducks = Array . new ( 3 ) { Duck . new } cows = Array . new ( 2 ) { Cow . new } dogs = Array . new ( 1 ) { Doggo . new } # uninitialized constant Doggo farm . insert_animals ( ducks , cows , dogs ) # wrong number of arguments (given 3, expected 1) farm . speak p \"count: \" + farm . animal_count # no implicit conversion of Integer into String (TypeError) If we were to run this program only then we would find the error undefined method 'speak' for Farm object. Once we correct that, then comes the next error uninitialized constant Doggo . Oops, how silly to miss that. We fix it only to find the next error when we run the program wiz. wrong number of arguments (given 2, expected 1) . ‘But hey, how tedious is it to find these errors? We’ve been doing this kind of debugging from long since and we keep tests to ensure the correctness of our program’s intentions.' - one could argue. And then on the command line, I’d type in: > srb tc\n\nsorbet-example.rb:56: Unable to resolve constant Doggo https://srb.help/5002 56 | dogs = Array.new ( 2 ) { Doggo.new } # uninitialized constant Doggo ^^^^^\n    sorbet-example.rb:41: Did you mean: Dog? 41 | class Dog\n        ^^^^^^^^^\n\nsorbet-example.rb:52: Method speak does not exist on Farm https://srb.help/7003 52 | farm.speak # undefined method 'speak' ^^^^^^^^^^\n\nsorbet-example.rb:58: Too many arguments provided for method Farm#insert_animals. Expected: 1, got: 3 https://srb.help/7004 58 | farm.insert_animals ( ducks, cows, dogs ) # wrong number of arguments (given 3, expected 1) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    sorbet-example.rb:16: insert_animals defined here 16 | def insert_animals ( animals ) ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nsorbet-example.rb:59: Method speak does not exist on Farm https://srb.help/7003 59 | farm.speak\n\nErrors: 4 All the errors have been well listed out before having to run the program. We can truly benefit from adopting this on our codebases and have our tests focused more on the program behaviour. But wait these are simply syntax and constant resolution errors. To get the real juice out of this gem, we need Signatures . Runtime checks with Signatures Signatures are simply Ruby code that is added above a method as a contract. In order to make use of signature, we’d need to add extend T::Sig onto our respective class or module. Signature is composed of optional parameters and a required return types to be specified. sig { params ( x : SomeType , y : SomeOtherType ) . returns ( MyReturnType )} These kinds of annotations (at the cost of increased verbosity of the program) helps us to catch type errors and add enforced documentation for method. This can be further utilized for autocompletion & instant type-checked feedback on IDEs and leveraged by the interpreter to produce better-optimized machine code. Let’s consider the program from earlier and add signatures to the class Farm . class Farm extend T :: Sig sig { params ( animals : Array ) . void } def initialize ( animals = [] ) @animals = animals end sig { void } def all_speak make_animals_speak ( @animals ) end sig { returns ( Integer ) } def animal_count @animals . length end sig { params ( animals : Array ) . void } def insert_animals ( animals ) animals . each { | animal | @animals << animal } end private sig { params ( animals : Array ) . void } def make_animals_speak ( animals ) if animals animals . each { | animal | p animal . speak } else p 'awkward silence.. 😪' end end Now that we have specified type information, let’s see how the type check goes. > srb tc\n\nsorbet-example.rb:78: Expected String but found Integer for argument arg0 https://srb.help/7002 78 | p \"count: \" + farm.animal_count\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    https://github.com/sorbet/sorbet/tree/80e1b24dadafc4ead575cb5e3166a691d2eb73e7/rbi/core/string.rbi#L24: Method String#+ has specified arg0 as String 24 | arg0: String,\n                ^^^^\n  Got Integer originating from:\n    sorbet-example.rb:78: 78 | p \"count: \" + farm.animal_count\n                      ^^^^^^^^^^^^^^^^^\n\nsorbet-example.rb:34: This code is unreachable https://srb.help/7006 34 | p 'awkward silence.. 😪' ^^^^^^^^^^^^^^^^^^^\nErrors: 2 We just found a type error and an unreachable part of code. Nice! Let’s fix that. class Farm ... sig { params ( animals : Array ) . void } def make_animals_speak ( animals ) if ! animals . empty? animals . each { | animal | p animal . speak } else p 'awkward silence.. 😪' end end ... end ... ... p \"count: \" + farm . animal_count . to_s I’ll just take my liberty to point out the obvious just in case you are not thinking about it - we have zero tests written until now. In this way, we can incrementally add type checking at our preferred pace and granularity. And when dealing with parts of the codebase which does not have any types given, it is considered to be of type - T.untyped T.untyped has two special properties: Every value can be asserted to have type T.untyped . Every value of type T.untyped can be asserted to be any other type! If you want to understand why it so, check out the docs . Initially when we are starting most of our code will be of T.untyped and incrementally by adding statically typed code we should be reducing T.untyped types. How will testing be affected? Ruby being a dynamic language, tests are integral when building large programs. We will still be relying on an automated test but also with added confidence on type safety. These automated tests implicitly become the tests of these added signature contracts. Moreover, we can add type checking to be a part of the CI/CD pipeline as well. What now? Sorbet is written in C++, it is pretty fast as it is multithreaded and scales across CPU cores. There is support coming out for IDEs such that type-checked feedback is instantaneous. You can try out the editor support online `. It will help resolve the pain points and increase productivity when we make it as a part of our toolchain. Given the popularity and adoption trend of static type checking with Typescript or Flow in Js, mypy in Python and Hack in PHP - it is great that static type checkers are making its way onto our Ruby community as well.\nMatz has put forward 3 goals for Ruby 3 at a keynote given RubyKaigi 2019 wiz. performance optimizations, concurrency support, and static type checking. So types are inevitably coming to Ruby (It is yet to be seen if type annotations or separately kept out RBI files will prevail). Sorbet was initially developed for internal tooling at Stripe. This was later open-sourced. It has been tested by about 30 companies on their codebase and this includes Shopify, Coinbase, Sourcegraph, Kickstarter, etc. So if you are at a point wherein you are slowly drowning in technical debts as we had mentioned earlier or maybe want to prevent them. Do try out Sorbet and see if that floats your boat. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2019-08-12"},
{"website": "Codemancers", "title": "overlap an image over a qr code using mogrify", "author": ["Atul Bhosale"], "link": "https://crypt.codemancers.com/posts/2020-01-02-overlap-an-image-over-a-qr-code-using-mogrify/", "abstract": "Overlap an image over a QR code using Mogrify Written by Atul Bhosale on January 2, 2020; tagged under elixir Recently, I was working on writing a QR code generator using Elixir for one of our\nclients. There was a requirement to have a logo above the QRCode\nand it should be in the center, to identify which company’s QR code is being\nscanned. This post is about what I learned about using Mogrify Elixir\nlibrary to achieve this. First we need to generate a QR code and store it in a image file. svgfilepath = Temp.path!(%{suffix: \".svg\"})\n\tPoison.encode!(%{data: \"Hello world\"})\n\t|> QRCode.create(:high)\n\t|> Result.and_then(&QRCode.Svg.save_as(&1, svgfilepath)) I have used the temp library to create a temporary file & qr_code library to generate the QR code. Since I need to\ngenerate the QR code out of an elixir map I have used the poison library to encode it to json string. Using Result.and_then() method, provided by qr_code library we can store the QR code in a .svg file. To overlap an image over another image we need another elixir library. I came\nacross Mogrify for this. Mogrify uses ImageMagick for\nimage manipulation. You can install ImageMagick by following the instructions from the official page . After ImageMagick is installed, we can use Mogrify to overlap the logo\nover the QR code image. Going through ImageMagick docs I found that I need to use the composite command provided by the library to overlap an image over another. After going through the Mogrify readme I didn’t find any\nmethod which uses the composite command of Imagemagick. I checked the open issues\non the library . I came across this issue which points to another issue mentioned to use image_operator method since\nthe library doesn’t support the composite command. I followed this comment and ImageMagick composite command docs to overlap an image over another. logo_png = File.cwd!() <> \"/logo.png\"\n\tpngfilepath = Temp.path!(%{suffix: \".png\"})\n\n\tMogrify.open(svgfilepath)\n\t|> Mogrify.format(\"png\")\n\t|> Mogrify.save(path: pngfilepath)\n\n\tFile.rm!(svgfilepath)\n\n\tconversion_command_string = \"convert #{pngfilepath} #{logo_png}\\\n\t  -gravity center -composite -size 500x500\"\n\n\tMogrify.open(svgfilepath)\n\t|> Mogrify.image_operator(conversion_command_string)\n\t|> Mogrify.create(inplace: true) After the svg image is converted to png format, we can delete the file\nusing File.rm!() . I was able to overlap the logo image on top of the QR code by converting the svg QR code image to png and then overlapping it with the logo image.\nFollowing is the screenshot when the QR code is successfully scanned. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-01-02"},
{"website": "Codemancers", "title": "extract data from qr code in elixir", "author": ["Atul Bhosale"], "link": "https://crypt.codemancers.com/posts/2020-02-27-extract-data-from-qr-code-in-elixir/", "abstract": "Extract data from a QR code in Elixir Written by Atul Bhosale on February 27, 2020; tagged under elixir After I had finished working on writing a QR code generator using Elixir for one\nof our clients, there was a requirement to extract the data from a QR code and\nstoring it. This post is about how I learned to extract the data from a QR\ncode in Elixir. I came across zbar while searching for a tool to read a QR\ncode. Since I will be using this tool to read the QR code image I will have to\nuse System.cmd() in my code to run the zbar binary. defmodule Sample do\n\t  def qr_string(image_path) do\n\t    System.cmd(\"zbarimg\", [image_path], stderr_to_stdout: true)\n\t  end\n\tend When the stderr_to_stdout option is set to true, the error details will be\nincluded in the tuple returned by Sysmtem.cmd . I generated a QR code with Hello world as the data. When I run the above code I get the following: I just need the QR data i.e. Hello world in this case, but it returns a tuple\nwith first value as string & second as 0. I was curious to know why 0 is\nreturned. I tried scanning an invalid QR code image like the Codemancers logo. Notice that now it returns 4. For invalid or non-QR code images, the zbarimg library will return 4. what about an invalid file type? We can extract the data for 0, return “No QR code present” for 4 and return\n“Invalid file” for anything else. defmodule Sample\n \n      require Logger\n\n      def qr_string(image_path) do\n        case System.cmd(\"zbarimg\", [image_path], stderr_to_stdout: true) do\n          {data, 0} ->\n            data\n\n          {data, 4} ->\n            raise Logger.info(\"No QR code present. msg: #{data}\")\n\n          {data, _} ->\n            raise Logger.info(\"Invalid file. msg: #{data}\")\n        end\n      end\n    end We still need to extract the QR code data from the string. We can split the string & find the data. I came across <> operator which can be used for string pattern matching,\nusing which I could find the data in the string. The existing code can be updated to: case System.cmd(\"zbarimg\", [image_path], stderr_to_stdout: true) do\n\n      {data, 0} ->\n        \"QR-Code:\" <> result =\n          data\n          |> String.split(\"\\n\")\n          |> List.first()\n\n        result\n\n      {data, 4} ->\n        raise Logger.info(\"No QR code present. msg: #{data}\")\n\n      {data, _} ->\n        raise Logger.info(\"Invalid file. msg: #{data}\")\n    end I was able to extract the QR code data from a QR code image. Following is the\nscreenshot of the data extracted from a QR code image. Update: I received a recommendation about parsing QR code data from Antonin\nKral, which is as follows: In case the data stored in QR code consists of multiple lines, its recommended\nto use --xml option while using zbarimg The Elixir code will be: args = [\"--xml\", image_path]\n    System.cmd(\"zbarimg\", args, stderr_to_stdout: true) Note: In our client’s requirement, the QR code data didn’t span multiple lines\nhence I haven’t used --xml option in my examples. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-02-27"},
{"website": "Codemancers", "title": "a full stack guide to graphql node_server", "author": ["Jawakar Durai"], "link": "https://crypt.codemancers.com/posts/2020-03-04-a-full-stack-guide-to-graphql-node_server/", "abstract": "The Full-Stack guide to GraphQL: NodeJs Server Written by Jawakar Durai on March 6, 2020; tagged under javascript , graphql , nodejs GraphQL has been adopted in many products after Facebook open-sourced it in 2015. We, at Dockup lately changed our APIs to GraphQL service, and that was my job. I thought it must be way more complex to implement a full GraphQL service for our app, but surprisingly it was easy and fun. Thinking about how to convey what I learned with the same interest and fun, I concluded, learning by building an app will be great.\nSo, Yeah! we’ll build a small app for restaurants to add, view and delete menu items,\nallowing us to deep dive into different GraphQL concepts. I couldn’t do all of these in one post, so there will be a series of them: Intro: you can go through my another post for some intro (optional) Implementation with NodeJs as server Implementation in Elixir with Absinthe GraphQL and Phoenix framework React front end app connected to GraphQL Setup Let’s start by setting up the project directory layout: $ nvm install 12.16.1\n$ nvm use 12.16.1 # make a project dir tree and cd into server dir $ mkdir menucard && cd menucard && mkdir nodeserver elixirserver client && cd nodeserver # init the project $ yarn init Add the packages we need: $ yarn add graphql apollo-server nodemon\n$ touch index.js Add this to your package.json for nodemon to sync with the changes \"scripts\" : { \"start\" : \"node index.js\"\" \" start : dev \" : \" nodemon\" } And run yarn start:dev in your terminal. We can use the apollo-server package, which is an abstract wrapper over the graphql package. We’ll use an Array as a data source for now and connect the database later. Query In GraphQL, you can consider Query as a set of user-defined functions that can do what GET does in REST. Let’s assume we want to get this data: const menuItems = [ { id : 1 , name : \"Pizza\" , category : \"Meal\" , price : \"4.5\" }, { id : 2 , name : \"Burger\" , category : \"Meal\" , price : \"3.2\" }, ] Let me write a basic index.js setup for a query and explain it: // index.js const { ApolloServer , gql } = require ( \"apollo-server\" ); const menuItems = [ { id : 1 , name : \"Pizza\" , category : \"Meal\" , price : \"4.5\" }, { id : 2 , name : \"Burger\" , category : \"Meal\" , price : \"3.2\" }, ] const typeDefs = gql ` \"\"\"\" Menu item represents a single menu item with a set of data \"\"\" type MenuItem { id: ID! name: String! category: String! price: Float! } type Query { \"Get menu Items\" menuItems: [MenuItem] } ` const resolvers = { Query : { menuItems : () => menuItems } } const server = new ApolloServer ({ typeDefs , resolvers , }); server . listen (). then (({ url }) => { console . log ( `🚀  Server ready at ${ url } ` ); }); Okay, what’s going on? First, we are getting the services we need, ApolloServer and gql . We instantiate ApolloServer to get the server running and interact with it gql parses the GraphQL string which JavaScript doesn’t support internally. TypeDefs const typeDefs = gql ` \"\"\"\" Menu item represents a single menu item with a set of data \"\"\" type MenuItem { id: ID! name: String! category: String! price: Float! } type Query { \"Get menu Items\" menuItems: [MenuItem] } ` typeDefs is a GraphQL schema. A GraphQL schema is written in its own typed language: The Schema Definition Language (SDL) We define the queries a client can do in Query type ( here menuItems ) and the\nmutations a client can do in Mutation type (we’ll see later) . A type is nothing but an object containing fields, where keys are the Field names and\nvalues are the data-type of the Field. In GraphQL, every field should be typed. And the strings you see above the fields are documentation strings. You can see the docs\non the graphiql interface which we’ll talk about later. In GraphQL, data types are: Scalar types: Int String Float Boolean ID : Represents unique number, like id in db. Object types type : In the above example we are defining MenuItem as a type\nand use that as the data-type for menuItems field in Query type Types can be represented as: [typeName] : Array of data with the type typeName should be returned typeName! : the ! in the typeName represents nonNullable i.e, the returning field should not be null You can also combine these representations like [typeName!]! i.e, you should return a nonNull\narray with nonNull elements which matches typeName type Resolvers const resolvers = { Query : { menuItems : () => menuItems } } Resolver map relates the schema fields and types to a function it has to call to\nturn the GraphQL operations into data by fetching from any source you have. For now, it’s just an array for us. Let us add another field to menuItem : reviews with its type: const typeDefs = gql ` type Review { id: ID! comment: String! authorId: ID! } \"\"\"\" Menu item represents a single menu item with a set of data \"\"\" type MenuItem { id: ID! name: String! category: String! price: Float! reviews: [Review] } type Query { \"Get menu Items\" menuItems: [MenuItem] } ` Let’s assume there’s some array of reviews which have the menuItem Id as\na Foreign key, then the resolvers will be: const resolvers = { Query : { menuItems : () => menuItem , }, MenuItem : { reviews : ( parent , args , context ) => { return reviews . filter ( item => item . menuItemId == parent . id ) } } } The query query { menuItems { reviews { comment } } } will call Query.menuItems first and then pass it’s returned value as parent to MenuItem.reviews . The result will be: { data : { menuItems : [{ reviews : [{ comment : \"Some comment\" }] }] } } A resolver can return an Object or Promise or scalar value, this should also match the data-type defined in the schema\nfor that field. Resolved data will be sent if a promise returned. Every resolver function takes four arguments when it’s called: parent : Object contains the returned value from the parent’s resolver. Every GraphQL query is a tree of function calls in the server. So, every field’s resolver gets the\nresult of parent resolver, in this case: query aka rootQuery is one of the top-level Parents. Parent in Query.menuItem will be whatever the server configuration passed for rootQuery . Parent in MainItem.reviews will be the returned value from the resolver Query.MenuItems . Parent in Review.id , Review.comment and Review.authorId will be the resolved value of MenuItem.reviews . params : Object contain the arguments we passed in the query like query { menuItem(id: 12) { name } } the params will be { id: 12 } context : You can pass an object when instantiating the server and access it on every resolver. Example: const server = new ApolloServer ({ typeDefs , resolvers , context : { menuRefInContext : MenuItem } }); Query : { menuItems : ( parent , __ , { menuRefInContext }) => menuTableInContext . findAll (), }, info : This argument mostly contains the info about your schema and current execution state. Default resolver Every type need not have a resolver, ApolloServer provides a default resolver ,\nthat looks for relevant field name in the parent object, or call the function if we have defined one for the field\nexplicitly. For the following schema, comment field of the Review would not need a resolver if the result\nof reviews resolver returns a list of objects which are already containing a comment field. type Review { comment : String ! authorId : ID ! } type MenuItem { reviews : [ Review ] } Start the server Instantiate ApolloServer with typeDefs and resolvers to listen for queries. const server = new ApolloServer ({ typeDefs , resolvers , }); server . listen (). then (({ url }) => { console . log ( `🚀  Server ready at ${ url } ` ); }); Make the query Go to localhost:4000 , enter the query on the left side of the graphiql : An interface that is provided by apollo-server to test,\non the right, you can access docs of different types you have and field infos. # This is the root operation: graphql provides three operations # # query # mutation # subscription query { # endpoint with what are the values we need, here we are asking # for \"name, price and, comment and authorId of reviews of all the menuItems\" menuItems { name price reviews { comment authorId } } } the result will closely match the query by returning only what we asked for: // result { \"data\" : { \"menuItems\" : [ { \"name\" : \"Pizza\" , \"price\" : 4.5 , \"reviews\" : [ { comment : \"Not bad\" , authorId : 12 , } ] }, { \"name\" : \"Burger\" , \"price\" : 3.2 , \"reviews\" : [ { comment : \"Good\" , authorId : 90 , } ] } ] } } A query is like a tree of functions which fetches data.\nFor example: If we take the above query, You can imagine fields as pipes of functions, which could return a data or call another function.\nFields with data-types Int , String , Boolean , Float and ID returns a data, but type fields\nwill call a function, depends on its fields’ data-type, returning a data or function call will happen. | - rootQuery () | - menuItems () | - return name | - return price | - reviews () | - return comment | - return authurId Database setup We’ll use a package called sequelize to use Postgres database with MongoDB\nlike functions. It will also work with any other DB. Set up a DB and get the URL to pass it as an argument to the Sequelize constructor. Stop the app and run the below command in your terminal: $ yarn add pg pg-hstore sequelize && yarn start:dev We’ll use sequelize to connect to database , Model a table , then interact with the table ” GraphQL is not bound to any database, doesn’t interact with\nthe database on its own, we have to do all the querying and return the data which\na GraphQL query expects. “ Our Postgres database hosted on Heroku. We’ll change the existing code to use sequelize: // index.js const { ApolloServer , gql } = require ( \"apollo-server\" ); const { Sequelize , DataTypes } = require ( \"sequelize\" ); // connect to database const sequelize = new Sequelize ( \"PASTE YOUR POSTGRES URL HERE\" ); // Expecting a table name \"menuItems\" with fields name, price and category, // You'll use \"MenuItem\" to interact with the table. id, createdAt and // updatedAt fields will be added automatically const MenuItem = sequelize . define ( \"menuItems\" , { name : { type : DataTypes . STRING }, price : { type : DataTypes . FLOAT }, category : { type : DataTypes . STRING } }); const Review = sequelize . define ( \"reviews\" , { comment : { type : DataTypes . String }, authorId : { type : DataTypes . INTEGER } }); MenuItem . hasMany ( Review , { foreignKey : \"menuItemId\" , constraints : false }) // `sync()` method will create/modify the table if needed, comment it when not // needed, uncomment whenever you change the model definition. // For production you might consider Migration (https://sequelize.org/v5/manual/migrations.html) // instead of calling sync() in your code. // MenuItem.sync(); const typeDefs = gql ` type Review { id: ID! comment: String! authorId: ID! } \"\"\"\" Menu item represents a single menu item with a set of data \"\"\" type MenuItem { id: ID! name: String! category: String! price: Float! reviews: [Review] } type Query { \"Get menu Items\" menuItems: [MenuItem] } ` // Note: We removed the separate resolver for reviews because // menuItems itself returned reviews for each MenuItem const resolvers = { Query : { menuItems : ( parent , __ , { menuItem }) => { return menuItem . findAll ({ include : [{ model : Review }] }) }, } } const server = new ApolloServer ({ typeDefs , resolvers , }); server . listen (). then (({ url }) => { console . log ( `🚀  Server ready at ${ url } ` ); }); Run the query in graphiql query { menuItems { name price reviews { comment } } } You should get an empty array because we haven’t created any menu item in the\ndatabase yet, we will do that using mutation Mutations You can think of Mutations as user-defined functions which can do what POST , PUT , PATCH and DELETE does in REST Change the TypeDefs to: // index.js const typeDefs = gql ` type Review { id: ID! comment: String! authorId: ID! } \"\"\"\" Menu item represents a single menu item with a set of data \"\"\" type MenuItem { id: ID! name: String! category: String! price: Float! reviews: [Review] } input MenuItemInput { name: String! category: String price: Float } input ReviewInput { comment: String! authorId: ID! menuItemId: ID! } type Query { \"Get menu Items\" menuItems: [MenuItem] } type Mutation { addMenuItem(menuItem: MenuItemInput): MenuItem addReview(review: ReviewInput): Review } ` Here, we introduced two new fields: Mutation and input Mutation field Like Query , Mutation is also a special field where we will define all mutations\n(the fields inside Mutation type), we will also write resolver functions\nfor our mutations. Input Objects Input fields are also like type fields, but this defines types for the\narguments to be passed by the client query if it is an object. For example, the query to create a menuItem: # Operation we are doing mutation { addMenuItem ( menuItem : { name : \"Pizza\" , category : \"Meal\" , price : 10 .3 }) { name price category } } See, we are passing an object menuItem as an argument to the addMenuItem mutation, like a function.\nThis menuItem should match the input MenuItemInput type we defined. Resolver function // index.js const resolvers = { Query : { ... }, Mutation : { addMenuItem : ( _ , { menuItem : { name , price , category } }, __ ) => { return MenuItem . create ({ name , price , category }); }, addReview : ( _ , { review : { comment , authorId , menuItemId } }) => { return Review . create ({ comment , menuItemId , authorId }); }, } } Sequelize functions will always return a promise, so we are returning promises returned by\nthe functions. Run this query: mutation { addMenuItem ( params :{ name : \"asdasd\" , price : 21 , rating : 33 }){ id name price } } The menu item will be created in the table using the resolver function and the value returned for the query will be: { \"data\" : { \"addMenuItem\" : { \"id\" : \"1\" , \"name\" : \"Toast\" , \"price\" : 3 } } } Then run this query with the returned menuItemId: mutation { addReview ( review : { comment : \"not bad\" , authorId : 12 , menuItemId : 1 }){ id comment authorId } } The result would be: { \"data\" : { \"addReview\" : { \"id\" : \"1\" , \"comment\" : \"not bad\" , \"authorId\" : \"12\" } } } Did you notice that even though we didn’t define id in the models we got the result with id ? It is because sequelize automatically added it to the table when creating the record. We can also pass arguments as individual values, mutation { addMenuItem ( name : \"Pizza\" , category : \"Meal\" , price : 10 .3 ) { ... } It’s all about how we get the values from parameters in the resolver function. That’s it from my side. You have exercises on creating a query getSingleMenuItem and a mutation deleteMenuItem . See you in the next post: How to implement these with Elixir and Phoenix. Good Luck! 😇 If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-03-06"},
{"website": "Codemancers", "title": "concurrency_in_javascript_browser", "author": ["Jawakar Durai"], "link": "https://crypt.codemancers.com/posts/2020-02-17-concurrency_in_javascript_browser/", "abstract": "A quick introduction to parallelism in JavaScript Written by Jawakar Durai on February 17, 2020; tagged under chrome , javascript Parallelism in JavaScript? you might be thinking that we are already doing parallel programming in JavaScript\nusing setTimeout() , setInterval() , XMLHttpRequest , async/await and event handlers. But that’s just not true.\nAs developers, we have been mimicking parallel programming because of JavaScript’s single threading nature using event loop . Yes, all of the above techniques are asynchronous and non-blocking. But, that doesn’t necessarily mean parallel programming.\nJavaScript’s asynchronous events are processed after the currently executing script has yielded. Meet “Web Workers” Web Workers introduce real parallel programming to JavaScript. A Web Worker allows you to fire up long-running computationally expensive task(s) which helps the already-congested main thread to spend all of it’s time focusing on layout and paint 😃 Understand web worker in renderer process Where Web Workers goes in a renderer process (a process in chrome, which is responsible\nfor everything that happens inside a tab): Every chrome tab has its own renderer process, chrome will also try to have a single renderer process for tabs with the same domain,\nif you want to know about how the same renderer process shares one v8 instance , Good luck! Renderer process creates multiple threads: Main thread : Handles most of the JavaScript sent to the browser Worker threads : Handles Web workers Compositor and Raster thread : Responsible for rendering the page smooth and efficient Just for fun, you can check processes (not only renderer processes) running in chrome by: Menu(3 verticle dots on top right) > More tools > Task Manager How to use? So, whenever we do const worker = new Worker ( \"worker.js\" ) new worker thread will be created and code we have in worker.js will run in there. Communicating with worker You can’t directly call functions that are in the worker thread.\nCommunication between the main thread and the worker is done via sending and listening to message event . Example: Finding length of a string: In index.js // message event helps to communicate between threads. // to listen for incoming messages from the worker. worker . addEventListener ( 'message' , e => { console . log ( `Message from worker ${ e . data } ` ) }) // to send message to the worker worker . postMessage ( \"Hello from main\" ) AND in worker.js // to listen for incoming messages from the main thread or other worker. // Yes, a worker can also create subworkers. self . addEventListener ( 'message' , e => { console . log ( `Message from main thread ${ e . data } ` ) // send a message back to the main thread self . postMessage ( e . data . length ) }) Here, postMessage takes one argument message , that will be available in the delivered event’s data key,\nvalue of the message should be supported by structured cloning algorithm. You can also use second argument to send array of Transferable objects . worker . postMessage ( message , [ transfer ]); Worker scope Both self and this of the worker, references the global scope of the worker. From previous example self.postMessages(e.data.length) can also be written as postMessages(e.data.length) Limitations Because of the multi-threading behavior and thread-safety of worker threads, it doesn’t have some features,\nwhich the main thread has: DOM window object document object parent object See other available functions and classes in MDN . Real-world example Enough talk, let’s code. We’ll write a small application that does some intense task using web workers: accept an image and apply filter. Run in terminal $ mkdir filter\n\n$ touch index.html script.js worker.js Html setup: <!--index.html --> <!DOCTYPE html> < html > < head > < meta charset = \"UTF-8\" /> < meta name = \"viewport\" content = \"width=device-width\" /> < title > Worker </ title > </ head > < body > <!-- Input to select filter --> < p > < label > Choose a filter to apply < select id = \"filter\" > < option value = \"none\" > none </ option > < option value = \"grayscale\" > grayscale </ option > < option value = \"brighten\" > brighten by 20% </ option > < option value = \"threshold\" > threshold </ option > </ select > </ label > </ p > <!-- Get image --> < form accept-charset = \"utf-8\" action = \"asd\" > < label > Image to filter </ label > < input type = \"file\" name = \"image\" id = \"image-to-filter\" alt = \"Enter image to filter\" > </ form > <!-- show output here --> < canvas id = \"output\" ></ canvas > < script src = \"script.js\" ></ script > </ body > </ html > Script setup: //script.js document . addEventListener ( 'DOMContentLoaded' , () => { const inputImage = document . getElementById ( 'image-to-filter' ); const output = document . getElementById ( 'output' ); const filter = document . querySelector ( '#filter' ); // Context let you draw on the canvas const outputContext = output . getContext ( '2d' ); // This will return HTMLImageElement (<img>) const img = new Image (); let imageData ; const drawImg = () => { output . height = img . height ; output . width = img . width ; outputContext . drawImage ( img , 0 , 0 ); // returns ImageData object, we can get pixel data from ImageData.data // https://developer.mozilla.org/en-US/docs/Web/API/ImageData/data imageData = outputContext . getImageData ( 0 , 0 , img . height , img . width ); } // Getting image from the user inputImage . addEventListener ( 'change' , e => { // API to read files from the user const reader = new FileReader (); // will be called once we read some file using any of the reader // function available for FileReader reader . onload = e => { img . src = e . target . result ; img . style . display = 'none' ; // will be called once the src is downloaded img . onload = () => { document . body . appendChild ( img ); drawImg (); }; }; // reads the file as a dataURL reader . readAsDataURL ( e . target . files [ 0 ]); }); }) Okay, what’s going on?\nThis is mostly implementation details of how we are getting the image from the user and storing the values in imageData . This will create a worker to run the code in worker.js : // script.js const worker = new Worker ( 'worker.js' ); Setting up a worker in worker.js : If you are like me, first intuition you would get is, let’s just pass a function to call back to the worker\nand get it over with but, you can’t pass a function to the web worker, because functions are not supported by structured cloning algorithm. // worker.js const Filters = {}; Filters . none = function none () {}; Filters . grayscale = ({ data : d }) => { for ( let i = 0 ; i < d . length ; i += 4 ) { const [ r , g , b ] = [ d [ i ], d [ i + 1 ], d [ i + 2 ]]; // CIE luminance for the RGB // The human eye is bad at seeing red and blue, so we de-emphasize them. d [ i ] = d [ i + 1 ] = d [ i + 2 ] = 0.2126 * r + 0.7152 * g + 0.0722 * b ; } }; Filters . brighten = ({ data : d }) => { for ( let i = 0 ; i < d . length ; ++ i ) { d [ i ] *= 1.2 ; } }; Filters . threshold = ({ data : d }) => { for ( var i = 0 ; i < d . length ; i += 4 ) { var r = d [ i ]; var g = d [ i + 1 ]; var b = d [ i + 2 ]; var v = 0.2126 * r + 0.7152 * g + 0.0722 * b >= 90 ? 255 : 0 ; d [ i ] = d [ i + 1 ] = d [ i + 2 ] = v ; } }; onmessage = e => { const { imageData , filter } = e . data ; Filters [ filter ]( imageData ); postMessage ( imageData ); }; We are listening for a message event Getting the imageData Filtering the imageData Sending imageData back to the main thread. Send a message to the worker: // script.js const filter = document . querySelector ( '#filter' ); let imageData ; filter . addEventListener ( 'change' , e => sendImageDataToWorker ()) const sendImageDataToWorker = () => { worker . postMessage ({ imageData , filter : filter . value }) } Here, we are listening for a change in the filter, then sending imageData and which filter to use to the worker. Listen for the message from worker //script.js worker . onmessage = () => e => outputContext . putImageData ( e . data , 0 , 0 ); We are listening for a message from the worker, then changing the image data in the context. Finally script.js will be // script.js document . addEventListener ( 'DOMContentLoaded' , () => { const worker = new Worker ( 'filter_worker.js' ); const inputImage = document . getElementById ( 'image-to-filter' ); const outputC = document . getElementById ( 'output' ); const filter = document . querySelector ( '#filter' ); const oCtx = outputC . getContext ( '2d' ); const img = new Image (); let imageData ; const sendDataToWorker = () => worker . postMessage ({ imageData , filter : filter . value }); const receiveFromWorker = e => oCtx . putImageData ( e . data , 0 , 0 ); worker . onmessage = receiveFromWorker ; const drawImg = () => { outputC . height = img . height ; outputC . width = img . width ; console . log ( img . height ); oCtx . drawImage ( img , 0 , 0 ); imageData = oCtx . getImageData ( 0 , 0 , img . height , img . width ); sendDataToWorker (); }; inputImage . addEventListener ( 'change' , e => { const file = e . target . files [ 0 ]; const reader = new FileReader (); reader . onload = e => { img . src = e . target . result ; img . style . display = 'none' ; img . onload = () => { document . body . appendChild ( img ); drawImg (); }; }; reader . readAsDataURL ( file ); }); filter . addEventListener ( 'change' , e => sendDataToWorker ()); }); Try live example . Code in GitHub . Terminating a worker You can terminate the worker from the main thread by calling worker.terminate() worker . terminate () If you want to terminate from the worker itself you can call the worker’s close() function close () Upon calling close() , any queued tasks present in the event loop are discarded and the web worker scope is closed.\nThe web worker is also given no time to clean up, so abruptly terminating a worker may cause memory leaks. Other web workers In reality, there are two types of web worker: Dedicated and Shared , for the scope of this post\nwe only used dedicated workers. Use cases Interacting with WebAssembly Syntax highlighting Image filtering Audio/Video analyzing Processing large number of arrays (For Arrays you can use binary Arrays which are faster to copy) Useful links how fast are web workers? . Stackoverflow questions If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-02-17"},
{"website": "Codemancers", "title": "a full stack guide to graphql elixir_phoenix_server", "author": ["Jawakar Durai"], "link": "https://crypt.codemancers.com/posts/2020-03-16-a-full-stack-guide-to-graphql-elixir_phoenix_server/", "abstract": "A Full Stack Guide to Graphql: Elixir Phoenix Server Written by Jawakar Durai on March 20, 2020; tagged under graphql , elixir , phoenix This is the second post in the series of GraphQL posts where we started with intro to GraphQl , and then implemented a\nsimple NodeJs GraphQl service to understand the concepts like Resolvers , Schemas (typeDefs) , Query and Mutation . As soon as GraphQl was open-sourced, the community had started to implement the specs in their favorite server-side languages . With that known, we are going to implement the same GraphQL service in Elixir using Phoenix Framework. Phoenix Framework Phoenix is a web framework that implements the popular server-side Model View Controller pattern and\nit is written in elixir. You should have basic experience in working with Elixir and Phoenix to continue reading. Absinthe GraphQL Absinthe GraphQL is a GraphQL implementation in Elixir we are going to use. Phoenix setup As Phoenix is written in Elixir, the first step is to install Elixir, then Phoenix. Elixir Installation guide . Phoenix installation guide . If you’re lazy, like me 🥱 and don’t have these installed, just run these commands: $ brew install asdf\n$ brew install postgresql\n\n$ KERL_CONFIGURE_OPTIONS = \"--without-javac --with-ssl= $( brew --prefix openssl ) \" $ asdf plugin-add erlang https://github.com/asdf-vm/asdf-erlang.git\n$ asdf install erlang 22.1\n$ asdf global erlang 22.1\n\n$ asdf plugin-add elixir https://github.com/asdf-vm/asdf-elixir.git\n$ asdf install elixir 1.9\n$ asdf global elixir 1.9 # To check that we are on Elixir 1.5 and Erlang 18 or later, run: $ elixir -v\n\n$ mix local.hex\n$ mix archive.install hex phx_new 1.4.16 Assuming the installation is complete, let’s go to our project setup. Project setup Create a new Phoenix project: $ mix phx.new menucard --no-webpack Create the database and start the server: $ cd menucard\n$ mix ecto.migrate\n$ mix phx.server [ info ] Running MenuCardWeb.Endpoint with cowboy 2.7.0 at 0.0.0.0:4000 ( http ) [ info ] Access MenuCardWeb.Endpoint at http://localhost:4000 Phoenix app template ships with postgres as default database adapter with postgres user and postgres password. Now the app is running inside iex session but not interactive(use iex -S mix phx.server to start the interactive session).\nStop the app and continue (hit ctrl+c twice). Absinthe If you’re new to GraphQL, I’d suggest you read the previous article or go through official GraphQL documentation or guide\nfor better knowledge of GraphQL specs. Absinthe mostly supports all the specs GraphQL supports, we are now going to create the app in Phoenix. Let’s setup Absinthe. Add absinthe and absinthe_plug as a dependency, absinthe_plug is to use absinthe with phoenix and GraphiQL interface. # mix.exe defp deps do [ .. , { :absinthe , \"~> 1.4\" }, { :absinthe_plug , \"~> 1.4\" } ] end And run: $ mix deps.get In Absinthe also we define Schema , Resolvers , and Types . We can define them as\ndifferent modules or in a single file. Create schema in lib/menu_card_web/schema.ex as MenuCardWeb.Schema A simple schema for our app can be: # lib/menu_card_web/schema.ex defmodule MenuCardWeb.Schema do use Absinthe.Schema @desc \"An item\" object :item do field :id , :id field :name , :string end # Example data @menu_items %{ \"foo\" => %{ id : 1 , name : \"Pizza\" }, \"bar\" => %{ id : 2 , name : \"Burger\" }, \"foobar\" => %{ id : 3 , name : \"PizzaBurger\" } } query do field :menu_item , :item do arg :id , non_null ( :id ) resolve fn %{ id : item_id }, _ -> { :ok , @menu_items [ item_id ]} end end end end This is the simple schema where we can query for a specific item. We are using some macros and functions which are written in Absinthe.Schema . query: rootQuery object macro where we define different queries as fields. There’s also\nequal mutation macro for mutations field: A field in the enclosing object, here it’s query and object . arg: An argument for the enclosing field. resolve: Resolve function for the enclosing field. We are also defining an object type :item using another two built-in scalar types :id represents\na unique number and :string is obvious. And we are using the type :item for the query :menu_item to return a map with this type. Add this in MenuCardWeb.Router to access GraphiQL interface provided by Absinthe. # lib/menu_card_web/router.ex defmodule MenuCardWeb.Router do ... forward \"/graphiql\" , Absinthe.Plug.GraphiQL , schema : MenuCardWeb.Schema end Go to localhost:4000/graphiql and run a query: { menuItem ( id : \"bar\" ) { name } } result: { \"data\" : { \"menuItem\" : { \"name\" : \"Burger\" } } } With Ecto: Let’s use mix tasks to generate contexts, schemas, and migrations for items and their reviews. $ mix phx.gen.context Menu Item items name:string price:integer \n\n$ mix phx.gen.context Menu Review reviews comment:string author_id:integer item_id:references:items These will create our required migrations and columns. This should have added lib/menu_card/menu/item.ex , lib/menu_card/menu/review.ex , lib/menu_card/menu.ex and migrations inside prev/repo/migrations . Edit review.ex to let us add item_id when creating a review. # lib/menu_card/menu/review.ex defmodule MenuCard.Menu.Review do use Ecto.Schema import Ecto.Changeset schema \"reviews\" do field ( :comment , :string ) field ( :author_id , :integer ) belongs_to ( :item , MenuCard.Menu.Item ) timestamps () end @doc false def changeset ( review , attrs ) do review |> cast ( attrs , [ :comment , :author_id , :item_id ]) |> validate_required ([ :comment , :author_id , :item_id ]) end end Run the migrations: $ mix ecto.migrate Create and Get an item Let’s write a mutation, and types we need to create an item in the schema.\nDelete old code in schema and start with empty file: # lib/menu_card_web/schema.ex defmodule MenuCardWeb.Schema do use Absinthe.Schema @desc \"An item\" object :item do field ( :id , :id ) field ( :name , :string ) field ( :price , :integer ) field ( :reviews , list_of ( :review )) end @desc \"Review for an item\" object :review do field ( :id , :id ) field ( :comment , :string ) field ( :author_id , :integer ) end mutation do field :create_item , :item do arg ( :name , non_null ( :string )) arg ( :price , non_null ( :integer )) resolve ( fn args , _ -> { :ok , MenuCard.Menu . create_item ( args )} end ) end end end Here, the only difference is the language(Elixir), and the rest of GraphQL spec remains unchanged from the previous blog . Few points I’d like to add are: how the resolve functions and constraints on field type differs in absinthe then\nthe NodeJS version. Resolver functions can be a 3 or 2 arity function. 3 arity resolver : item ( id : 1 ){ name } The first argument will be the parent, i.e, the resolved values of item(id: 1) will be\nthe parent of the field name . The second argument will be args passed for the field, so, for the field item(id: 1) the args will be %{id: 1} The third argument will be the global context that we can set as a plug. 2 arity resolver: Here, the first argument will be args and second will be context . list_of(object_type/sclar-type): Returned value or arg passed should be a list. Equalent to [TypeName] non_null(object_type/scalar-type): Returned value or arg should be passed i.e, not null. Equalent to TypeName! Resolver function should return a tuple with first element as :ok or :error and second\nthe element should be a map. Now, if you run $ iex -S mix phx.server You will see an error saying there should be a query object, which is the root of all\nobjects we define. A mutation is also a rootMutation object but mutation object is allowed to be null. Let’s add a query and try: Add this before mutation object: query do field :item , :item do arg ( :id , non_null ( :id )) resolve ( fn args , _ -> { :ok , MenuCard.Menu . get_item ( args )} end ) end end Note:\nThere is also another type of object which is subscriptions . We’ll see it\nextensively in another chapter. Now run iex -S mix phx.server and open localhost:4000/graphiql to use GraphIQL interface. In the left text area,\nwrite the query and hit the play button or ctrl + enter to run the query. Query: mutation { createItem ( name : \"Rice pudding\" , price : 30 ){ id name price } } Result: { \"data\" : { \"createItem\" : { \"id\" : \"1\" , \"name\" : \"Rice pudding\" } } } Hurray 🎉! We finished the basic query and mutation. Loading associations You can see that we have reviews for each item @desc \"An item\" object :item do field ( :id , :id ) field ( :name , :string ) field ( :price , :integer ) field ( :reviews , list_of ( review )) end We can load review association in three ways Write a separate resolver for it object :item do field ( :id , :id ) field ( :name , :string ) field ( :price , :integer ) field ( :reviews , list_of ( :review )) do resolve ( fn parent , _ , _ -> { :ok , MenuCard.Menu . get_reviews_by_item ( parent . id )} end ) end end Return reviews in the :item resolver itself. # lib/menu_card/menu.ex defmodule MenuCard.Menu do ... def get_item (%{ id : id }) do Repo . get! ( Item , id ) |> Repo . preload ( :reviews ) end end Absinthe recommends batching using dataloader to load association. Even through we’ll stick with using preload . This is not a best practice and comes with cons, try to use\ndataloader for prod. Add the function which uses preload to your code. To get reviews with items, first, we need a way to create them: mutations Add a mutation in schema : # lib/menu_card_web/schema.ex defmodule MenuCardWeb.Schema do ... mutation do ... field :do_review , :review do arg ( :comment , non_null ( :string )) arg ( :author_id , non_null ( :id )) arg ( :item_id , non_null ( :id )) resolve ( fn args , _ -> { :ok , MenuCard.Menu . create_review ( args )} end ) end end end Reset and start with fresh DB: $ mix ecto.reset\n$ iex -S mix phx.server We will create a Menu Item and then add a Review for it mutation { createItem ( name : \"Rice pudding\" , price : 40 ) { name price } } Result: { \"data\" : { \"createItem\" : { \"name\" : \"Rice pudding\" , \"price\" : 40 } } } With that returned id, create a review: mutation { doReview ( itemId : 1 , authurId : 1 , comment : \"Yummmmy!\" ) { comment } } Result: { \"data\" : { \"doReview\" : { \"comment\" : \"asdad\" } } } That’s it, that’s how you would create an HTTP GraphQL API with phoenix framework. Here is the code . If you want to do more with this, create a mutation to delete and edit both items and review. See you in the next post: How to utilize these API in the front-end using apollo-client with ReactJS Good luck learning! 😇 If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-03-20"},
{"website": "Codemancers", "title": "ruby 2 6 key error accepts message receiver key as argument", "author": ["Atul Bhosale"], "link": "https://crypt.codemancers.com/posts/2018-08-14-ruby-2-6-key-error-accepts-message-receiver-key-as-argument/", "abstract": "Ruby 2.6 KeyError#initialize accepts message, receiver, and key as arguments Written by Atul Bhosale on August 14, 2018; tagged under ruby Ruby 2.6.0-preview2 was recently released . Ruby 2.5.0 introduced public instance methods KeyError#key and KeyError#receiver .\nYou can read my earlier blog about KeyError#key & KeyError#receiver here . Ruby csv library raises a KeyError on CSV::Row#fetch method when the field is not found. def fetch ( header , * varargs ) # ... raise KeyError , \"key not found: #{ header } \" # ... end Instead of raising a KeyError shown above, how about if this is possible? - raise KeyError.new(\"key not found\", key: header) with this we can check the KeyError object for the error message and key using KeyError#message and KeyError#key methods which were introduced in Ruby 2.5.0. begin raise KeyError . new ( 'key not found' , key : :unknown ) rescue StandardError => error p error . message #=> \"key not found\" p error . key #=> :unknown end It was proposed that KeyError#initialize method should be\nintroduced which accepts message , receiver and key as arguments to set them on a KeyError object and it was approved. Ruby 2.6.0 In Ruby 2.6.0 KeyError#initialize accepts message , receiver , and key as\narguments, where the message is a default argument with default value as nil . KeyError#initialize >> error = KeyError . new => #<KeyError: KeyError> KeyError#message >> error . message => \"KeyError\" >> error = KeyError . new ( 'Message' ) => #<KeyError: Message> >> error . message => \"Message\" When message is not passed to KeyError it sets the class name as the message. KeyError#initialize with message, :receiver and :key >> error = KeyError . new ( 'Message' , receiver : Object . new , key : :unknown ) => #<KeyError: Message> >> error . message => \"Message\" >> error . receiver => #<Object:0x0000561132683d48> >> error . key => :unknown I hope these method arguments for KeyError class are useful in your future debugging attempts. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2018-08-14"},
{"website": "Codemancers", "title": "auto deploy to heroku using gitlab ci", "author": ["Revath S Kumar"], "link": "https://crypt.codemancers.com/posts/2020-06-26-auto-deploy-to-heroku-using-gitlab-ci/", "abstract": "Auto Deploy to Heroku Using Gitlab CI Written by Revath S Kumar on June 26, 2020; tagged under devops As of now, Heroku doesn’t support auto-deploy from GitLab. So we have to use Gitlab CI to deploy to Heroku. This post will help you to set up auto deploy to Heroku. For this blog post, we will take a rails app with a Postgres database. We need Gitlab to run the unit tests and deploy to staging and then later to production. For staging and production, we will keep different branches to make things simpler. It will also make it easier for us to push a hotfix. Setting up Gitlab CI First, we have to set up our project to run the tests using Gitlab CI.\nYou can add new .gitlab-ci.yml file using the templates available in Gitlab web UI. If you prefer to add it manually you can add the following section to set up. # .gitlab-ci.yml image : 'ruby:2.7.1' Then add the services section to support postgres db for the rails app. services : - postgres : 11.7 variables : RAILS_ENV : test POSTGRES_DB : app_test POSTGRES_USER : postgres POSTGRES_PASSWORD : postgres POSTGRES_HOST_AUTH_METHOD : trust DATABASE_URL : 'postgres://postgres:postgres@postgres:5432/app_test' Next, let’s add cache to make the build faster. We can cache the ruby gems, node modules etc # Cache gems in between builds cache : paths : - node_modules - .yarn - vendor/ruby Once the basic config is done, let us configure the gitlab ci to run the test cases. Tests To run the test cases, we have to first install the missing dependencies like nodejs , postgresql-client (to run migrations), yarn etc. All these installations can be done as part of the before_script . You can give a name for the stage using stage config in the test block. test : stage : tests before_script : - ruby -v # Print out ruby version for debugging - apt-get update -q && apt-get install nodejs postgresql-client -yqq # Install yarn as outlined in (https://yarnpkg.com/lang/en/docs/install/#alternatives-stable) - curl -o- -L https : //yarnpkg.com/install.sh | bash # Make available in the current terminal - export PATH= \"$HOME/.yarn/bin:$HOME/.config/yarn/global/node_modules/.bin:$PATH\" - yarn install - gem install bundler - bundle config set path 'vendor' - bundle install -j $(nproc) - bundle exec rake db : migrate --quiet - bundle exec rake db : test : prepare --quiet script : - bundle exec rails test Add Environment variables Before we configure the deployment, we need to add the ENV variables to Gitlab’s project configuration.\nTo add these navigate to Variables section in CI / CD Settings of the project. We require the following ENV variables. $HEROKU_PRODUCTION_KEY $HEROKU_APP_NAME $HEROKU_STAGING_APP_NAME We can mark the variables Protected & Masked depending on our setup. The protected variables will be exported only in protected branches. We can configure different environments for the project in Operations -> Environments . Deploy Once the tests are passing, we can configure the deployments in the next stage.\nThe deployments to run only from particular branches, ie., from develop branch, deploy to staging from master branch, deploy to production The deployments will be processed with the help of dpl gem.\nSince we are deploying to Heroku that requires us to run migration manually after each deployment, we should install Heroku CLI before the deploy stage. .before_script_deploy : before_script : - curl https : //cli-assets.heroku.com/install-ubuntu.sh | sh - gem install dpl # This deploy job uses a simple deploy flow to Heroku, other providers, e.g. AWS Elastic Beanstalk # are supported too: https://github.com/travis-ci/dpl production : type : deploy extends : .before_script_deploy environment : production variables : HEROKU_API_KEY : $HEROKU_PRODUCTION_KEY script : - dpl --provider=heroku --app=$HEROKU_APP_NAME --api-key=$HEROKU_PRODUCTION_KEY - heroku run rails db : migrate --exit-code --app $HEROKU_APP_NAME only : - master staging : type : deploy extends : .before_script_deploy environment : staging variables : HEROKU_API_KEY : $HEROKU_PRODUCTION_KEY script : - dpl --provider=heroku --app=$HEROKU_STAGING_APP_NAME --api-key=$HEROKU_PRODUCTION_KEY - heroku run rails db : migrate --exit-code --app $HEROKU_STAGING_APP_NAME only : - develop The .before_script_deploy template will help us to share the before_script between the production & staging.\nWhile writing this blog post the version of stable dpl gem was 1.10.15 & the master branch was pointing to new work in progress 2.0 version.\nWhen you are reading the documentation of dpl please pay attention to the version. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-06-26"},
{"website": "Codemancers", "title": "ssh access management part 1", "author": ["Ananth Kamath"], "link": "https://crypt.codemancers.com/posts/2020-04-28-ssh-access-management-part-1/", "abstract": "SSH Access Management - Control SSH Access With Vault OTP Engine (PART 1) Written by Ananth Kamath on April 28, 2020; tagged under vault , bastion , devops , aws , ssh , hashicorp In this blog series you will go through the problem involved with SSH Access Management and how to fix the same without compromising your organisations security and compliance using Hashicorp’s Vault and Bastion server. SSH access management is a known problem in DevOps world today. It is a tedious job to create access for your team on demand and then ensure that the access is revoked post their task. To simplify this task you end up creating your application workloads(servers) in the public subnets, sharing pem files or copying users public key into the workloads themselves. Thereby risking exposure of your critical application or user data to the world. Let Us Address The Elephant In The Room Generally, you would provide your users access to the application workloads via Bastion/jump server and keep all our workloads in a private subnet hidden from the rest of the world. By doing this you can restrict access for the external users and the attackers to your workloads, but you would still need to take care of access and audit-ability of your organisation’s internal users. Controlling access to the bastion server and logging the access requests for auditing are critical for your organisation’s security and compliance goals. There are multiple authorities with packages to help you achieve this, one such authority is Hashicorp’s Vault. Vault helps you control access to the bastion server and also provides audit logs for the access requests. Implementation We will be using two EC2 instances, one for setting up the Vault server and the other as the bastion server in the public subnet. Users will interact with the Vault server via Vault CLI to fetch the OTP(One Time Password) and then SSH to the bastion server using the OTP as the password. The implementation includes 4 steps Setup Vault server Enable SSH OTP engine on Vault server Setup Bastion Server with SSH helper Setup Vault client for user access Setup Vault Server Let us start by setting up the Vault server on an EC2 instance. We will be using the Ubuntu AMI to spin up the EC2 instance. Download & Install Vault Server # Download Vault binary based on the version, replace ${VAULT_VERSION} with required version below\n$ wget https://releases.hashicorp.com/vault/${VAULT_VERSION}/vault_${VAULT_VERSION}_linux_amd64.zip -O vault.zip\n\n# Unzip the Vault zip file\n$ unzip vault.zip\n\n# Make Vault executable and accessible\n$ chmod +x vault\n$ sudo mv vault /usr/local/bin/vault\n\n# Set Linux capability flag on the binary, this will let the binary do memory locking without granting unnecessary #privileges\n$ sudo setcap cap_ipc_lock=+ep /usr/local/bin/vault\n\n# To check if vault is installed\n$ vault --version\nvault v1.3.1\n\n# To install command line completion\n$ vault -autocomplete-install\n\n# Create a vault system user\nsudo useradd --system --home /etc/vault.d --shell /bin/false vault\n\n# Create vault service file\nsudo touch /etc/systemd/system/vault.service\n\n# Update /etc/systemd/system/vault.service with below content\n[Unit]\nDescription=\"HashiCorp Vault - A tool for managing secrets\"\nDocumentation=https://www.vaultproject.io/docs/\nRequires=network-online.target\nAfter=network-online.target\nConditionFileNotEmpty=/etc/vault.d/vault.hcl\nStartLimitIntervalSec=60\nStartLimitBurst=3\n\n[Service]\nUser=vault\nGroup=vault\nProtectSystem=full\nProtectHome=read-only\nPrivateTmp=yes\nPrivateDevices=yes\nSecureBits=keep-caps\nAmbientCapabilities=CAP_IPC_LOCK\nCapabilities=CAP_IPC_LOCK+ep\nCapabilityBoundingSet=CAP_SYSLOG CAP_IPC_LOCK\nNoNewPrivileges=yes\nExecStart=/usr/local/bin/vault server -config=/etc/vault.d/vault.hcl\nExecReload=/bin/kill --signal HUP $MAINPID\nKillMode=process\nKillSignal=SIGINT\nRestart=on-failure\nRestartSec=5\nTimeoutStopSec=30\nStartLimitInterval=60\nStartLimitIntervalSec=60\nStartLimitBurst=3\nLimitNOFILE=65536\nLimitMEMLOCK=infinity\n\n[Install]\nWantedBy=multi-user.target Setup Vault Server Provide vault user access to /etc/vault.d sudo mkdir /etc/vault.d\nsudo touch /etc/vault.d/vault.hcl\nsudo chown --recursive vault:vault /etc/vault.d\nsudo chmod 640 /etc/vault.d/vault.hcl Vault stores the data in its storage in an encrypted format. There are multiple types of storage supported by Vault. We will be using the Filesystem as storage for the demonstration. On a production server though, it is recommended to use a highly available key value store like etcd, Consul, etc. Let’s update the file /etc/vault.d/vault.hcl with the values below: storage \"file\" {\n  path = \"/mnt/vault/data\"\n}\n\nlistener \"tcp\" {\n address     = \"0.0.0.0:8200\"\n tls_disable = 1\n}\n\nui = true\nlog_level = \"Info\" Provide vault user permission to /mnt/vault directory # Create /mnt/vault\n$ sudo mkdir /mnt/vault\n\n# Give vault permission to the directory\n$ sudo chown -R vault:vault /mnt/vault Initialise Vault Server Vault server always boots up in sealed state, data in its storage is stored in an encrypted format. Vault is configured to know how to access the physical storage and its location, but doesn’t know how to decrypt any of it. Unsealing is the process of constructing the master key necessary to read the decryption key to decrypt the data. When you initialise the Vault for the first time you will be given a set of 5 keys. To unseal Vault, you will need at least 3 of these. You will also be provided a root token which will be used later to login to the vault and create required users, groups and policies. The number of keys to create master key and threshold of unseal keys are configurable, they are set to 5 and 3 by default. Every time the vault is restarted or if the underlying EC2 instance gets restarted the vault goes back to sealed state and will have to be unlocked by using at least 3 of the 5 keys provided during the initialisation step. To avoid this manual effort you can use AWS KMS to Auto Unseal Vault. (We will be writing a detailed blog on this soon) Let’s initialise the vault server now, these keys and token are displayed only once on initialisation so keep them somewhere safe. You would not be able to unseal vault later if you lose the keys. # Now start the vault server as a service\n$ sudo systemctl enable vault # Enable vault to start automatically when the system boots up\n$ sudo systemctl start vault\n$ sudo systemctl status vault\n\n# Initialise the Vault server this will return root token and 5 keys used to unseal\n$ export VAULT_ADDR=\"http://127.0.0.1:8200\"\n$ vault operator init >> ~/vault.keys\n\nInitial Root Token: s.######\n\nUnseal Key 1: key1###\nUnseal Key 2: key2###\nUnseal Key 3: key3###\nUnseal Key 4: key4###\nUnseal Key 5: key5### Now the vault server setup is complete and is ready for use. Enable SSH OTP Engine On Vault Now that the vault server is initialised, we can access it over http://{{VaultEC2PublicIP}}:8200 . Let’s go ahead and enable the SSH OTP engine on Vault. Follow below steps to accomplish the same. Log in to the vault using the root token obtained on initialisation Click on enable engine, to enable SSH engine Choose SSH engine, proceed further Create a role with any name, but select OTP as the type Once this step is complete, you now have the Vault server enabled with SSH OTP engine ready to use. What Next? As discussed earlier the implementation includes 4 steps, we have covered just two of those four steps in this blog. Part 2 of this blog series will cover: How to setup the bastion host to communicate with the SSH OTP engine enabled on the Vault server? How can users request for the OTP from the Vault server? Appendix Vault Bastion Server Consul If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-04-28"},
{"website": "Codemancers", "title": "ssh access management part 2", "author": ["Ananth Kamath"], "link": "https://crypt.codemancers.com/posts/2020-05-04-ssh-access-management-part-2/", "abstract": "SSH Access Management - Control SSH Access With Vault OTP Engine (PART 2) Written by Ananth Kamath on May 4, 2020; tagged under vault , bastion , devops , aws , ssh , hashicorp This is part 2 of SSH Access Management blog series. In part 1 of this series we covered SSH access management problem, solution design, how to setup the Vault server and setting up SSH OTP engine on it . In this post, we will cover How to setup the bastion host with SSH helper? How to setup the Vault client for users to request access? Setup Bastion Server with SSH helper Let us start by setting up an EC2 instance as the bastion host. We will be using Hashicorp’s Vault SSH Helper to authenticate the users with the Vault server and provide access to users post confirmation from the Vault. Vault SSH Helper is an agent used to communicate with the HashiCorp Vault’s Server. It allows a machine to consume the One-Time-Password (OTP) created by our Vault server and then use it as client authentication credential for every SSH connection request. This helper agent must be installed on the target host that you want to SSH into, in this case, the bastion. Install Vault SSH helper # Download the vault-ssh-helper\n$ wget https://releases.hashicorp.com/vault-ssh-helper/0.1.4/vault-ssh-helper_0.1.4_linux_amd64.zip\n\n# Unzip the vault-ssh-helper in /usr/local/bin\n$ sudo unzip -q vault-ssh-helper_0.1.4_linux_amd64.zip -d /usr/local/bin\n\n# Make sure that vault-ssh-helper is executable\n$ sudo chmod 0755 /usr/local/bin/vault-ssh-helper\n\n# Set the usr and group of vault-ssh-helper to root\n$ sudo chown root:root /usr/local/bin/vault-ssh-helper\n\n# Setup vault ssh-helper config /etc/vault-ssh-helper.d/config.hcl\nsudo mkdir /etc/vault-ssh-helper.d\nsudo touch /etc/vault-ssh-helper.d/config.hcl\n\n#Contents of config.hcl has to be as below\nvault_addr = \"${VAULT_ADDRESS}\"\nssh_mount_point = \"ssh\"\nca_cert = \"-dev\"\ntls_skip_verify = true\nallowed_roles = \"*\" Replace ${VAULT_ADDRESS} with something like an ip address( http://ip.ip.ip.ip/ ) or domain( https://vault.example.com ) Value of ‘ca_cert’ can be set to ‘-dev’ and ‘tls_skip_verify’ to ‘true’ if you want to use Vault over HTTP or are using it to test the setup before production( It is recommended to use Vault over HTTPS in production ). This file should contain the certificate deployed for the Vault server and tls_skip_verify should be set to false. You can use the Amazon Certificate Manager to get certificate from AWS or get a Let’s Encrypt certificate. Setting up the pam.d config # Create a backup of pam.d file\n$ sudo cp /etc/pam.d/sshd /etc/pam.d/sshd.orig Modify the file /etc/pam.d/sshd to comment the standard UNIX authentication( @include common-auth ) module and add vault ssh-helper modules as below #@include common-auth\nauth requisite pam_exec.so quiet expose_authtok log=/tmp/vaultssh.log /usr/local/bin/vault-ssh-helper -dev -config=/etc/vault-ssh-helper.d/config.hcl\nauth optional pam_unix.so not_set_pass use_first_pass nodelay Update the sshd service to pick the latest pam.d config # Make a backup of the original\n$ sudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.orig Add or modify the file /etc/ssh/ssh_config to include below changes: ChallengeResponseAuthentication yes\nPasswordAuthentication no\nUsePAM yes Restart the sshd service sudo systemctl restart sshd Create a Unix user which will be used to access the bastion server with OTP as password # Creates linux user unix-user-name\n$ useradd -ms /bin/bash unix-user-name\n$ usermod -aG sudo unix-user-name Logs for the vault-ssh-helper can be found below for debugging purposes tail -f 100 /tmp/vaultssh.log Setup Vault Client For User Access This journey has been really long and we are now heading to the last stop. Every client or user who has to be authenticated by Vault and wants to SSH into the bastion server needs to install Vault client. Install Client Download vault client from link Copy the executable to /usr/local/bin Check if Vault is installed by checking the version $ vault --version\nVault v1.3.1 Connect To The Vault Server # Setup your Vault server address\n$ export VAULT_ADDR=\"http://vault.example.com\"\n\n# You can create users through Vault UI and the login to vault with username and password to get the token\n$ export VAULT_TOKEN=`vault login -token-only -method=userpass username=johndoe password=unthinkable`\n\n# Or to get a quick preview you can export the vault root token generated on vault initialisation (Not recommended for production)\n$ export VAULT_TOKEN=\"s.######\"\n\n# Check if the values have been set in the env\n$ env | grep 'VAULT'\nVAULT_ADDR=http://vault.example.com\nVAULT_TOKEN=s.yourvaulttokenfornow Get OTP From Vault Server # username in the below result is the unix user we had created inside the bastion server. We are using the Vault OTP engine we created in part 1 of the blog series.\n$ vault write ssh/creds/otp-engine ip=#{bastion-ip}\nKey                Value\n---                -----\nlease_id           ssh/creds/otp-engine/DFjbgD5cN6dmwPuJ01SKzn9z\nlease_duration     1h\nlease_renewable    false\nip                 {bastion-ip}\nkey                99375f15-53d8-3388-7d62-0379a7e2034b\nkey_type           otp\nport               22\nusername           unix-user-name\n\n# Once you have obtained the OTP(key) you can now SSH into the bastion with unix-user and the OTP as password.\n$ ssh unix-user-name@#{bastion-ip}\npassword:99375f15-53d8-3388-7d62-0379a7e2034b The expiry time for the OTP can be customised by the administrator, it is recommended to set the expiry time under 30 minutes. Also the key or OTP obtained can only be used once. New request has to be made to obtain a new OTP in order to login again. Conclusion In this two part blog series, we learned how to handle SSH Access Management problem with a system designed using Hashicorp’s Vault and the Bastion server. Vault is known to be a Swiss Army Knife packaged with multiple functionalities, if you are interested then you can read more about the Vault on Hashicorp’s Site . If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-05-04"},
{"website": "Codemancers", "title": "a full stack guide to graphql react client", "author": ["Jawakar Durai"], "link": "https://crypt.codemancers.com/posts/2020-07-15-a-full-stack-guide-to-graphql-react-client/", "abstract": "A Full Stack Guide to Graphql: React Client Written by Jawakar Durai on July 15, 2020; tagged under graphql , reactjs , javascript , apollo This is the final post on the series of GraphQL posts. We started with intro to GraphQl , implemented a simple NodeJs GraphQl service to understand the\nconcepts like Resolvers , Schemas (typeDefs) , Query and Mutation , and then we built the same GraphQL service in elixir with Absinthe . In this post, we’re going to build the UI using ReactJs and Apollo graphql client for the menu card service we wrote. Let’s get started. Like every React example starts: $ npx create-react-app menu_card_client\n$ cd menu_card_client \n$ yarn start Let us add GraphQL dependencies we need: $ yarn add @apollo/client graphql @apollo/client is a GraphQL client library to fetch data via GraphQL graphql is a core GraphQL library apollo uses We’re ready to code! 👨‍💻 Edit index.js : import React from \"react\" ; import ReactDOM from \"react-dom\" ; import \"./index.css\" ; import App from \"./App\" ; import * as serviceWorker from \"./serviceWorker\" ; import { ApolloProvider , ApolloClient , InMemoryCache } from \"@apollo/client\" ; const client = new ApolloClient ({ url : process . env . REACT_APP_API_URL , cache : new InMemoryCache (), }); ReactDOM . render ( < ApolloProvider client = { client } > < React . StrictMode > < App /> < /React.StrictMode> < /ApolloProvider>, document . getElementById ( \"root\" ) ); // If you want your app to work offline and load faster, you can change // unregister() to register() below. Note this comes with some pitfalls. // Learn more about service workers: https://bit.ly/CRA-PWA serviceWorker . unregister (); Update your .env to REACT_APP_API_URL=https://localhost:4000 set REACT_APP_GRAPHQL_URL to the graphql service endpoint you’re running. Lines we added: const client = new ApolloClient ({ url : process . env . REACT_APP_GRAPHQL_URL , cache : new InMemoryCache (), }); The client is the interface between our React app and the GraphQL service, we are telling it to use the service from the provided\nurl and to use configurable cache . If you already didn’t have the GraphQL node server running, start the service and continue or clone the repo : $ git clone https://github.com/jawakarD/node-graphql-menu.git && cd node-graphql-menu && yarn && yarn start. ReactDOM . render ( < ApolloProvider client = { client } > < React . StrictMode > < App /> < /React.StrictMode> < /ApolloProvider>, document . getElementById ( \"root\" ) ); For the client to be consumed by the hooks we use in our Application, pass the App component as a child to ApolloProvider which acts as a context provider and passes the client as a context. That’s all in index.js . Let us move to functionalities: Don’t worry about css, delete the content on App.css , copy and paste this gist . Get menu items - useQuery useQuery hook allows us to query the graphql service as we did in the server-side playground, but here we are going to\nuse react to call those queries. basic usage: const { loading , error , data } = useQuery ( QUERY , options ); useQuery takes two parameters GraphQL query string, for the query we are going to make options will have the variables and other options we can pass Edit your App.js to have two components MenuItem and App : // App.js const MenuItem = ({ name , id , price , rating }) => { //just for fun const COLORS = [ \"#ff00f6\" , \"#00ff50\" , \"#fff900\" , \"#ff8300\" ]; const color = useMemo ( () => COLORS [ Math . floor ( Math . random () * COLORS . length )], [] ); return ( < div className = \"menu-item\" > < div className = \"info\" > < p className = \"name\" style = {{ color }} > { ` ${ name } * ${ rating } ` } < /p> < p className = \"item-price\" style = {{ color }} > { price } < /p> < /div> < div className = \"action\" > < button type = \"button\" style = {{ color , borderColor : color }} > Delete < /button> < /div> < /div> ); }; const GET_MENU_ITEMS = gql ` query { menuItems { id name price rating } } ` ; const App = () => { const { loading , error , data } = useQuery ( GET_MENU_ITEMS ); if ( loading ) { return < p > Loading ... < /p>; } if ( error ) { return < p > error < /p>; } const { menuItems } = data ; return ( < div className = \"App\" > < div className = \"App-body\" > < div className = \"add-form\" > < button > Add Item < /button> < /div> < div className = \"menu-box\" > { menuItems . map ( item => ( < MenuItem key = { item . id } {... item } /> ))} < /div> < /div> < /div> ); }; In the App component, we call useQuery hook with GET_MENU_ITEMS query to fetch the list of menu items. The hook returns different states of the data that is being fetched: loading, error (if any in the query, actual data for the query, and other values . The query will also be cached. If same is called again in another component, apollo will fetch the data from the cache that will make the subsequent queries much faster.\nAnother interesting thing is, you can also interact with the stored cache. But it’s not recommended to use apollo cache as a state management like\nredux, because redux will give us more control over updating data than apollo cache. useQuery also allows other features: Polling (execute query periodically at a specified interval) and Refetching (Refetch the query).\nYou might want to opt useLazyQuery hook for cases where the actual execution of the graphql query needs to happen on some other user action or event,\nas opposed to the default behaviour of execution. Read more on this here. Now, if you turn into the browser, you can see the list of currently available menus. If it’s empty, you can wait to add some when we go to the useMutation section. Create menu item - useMutation useMutation allows us to run mutations with the graphql server. basic usage: const [ mutateFunction , { called , loading , data , error }] = useMutation ( MUTATION , options ); Like useQuery , useMutation takes two arguments: gql query string and mutation options . But like useLazyQuery it doesn’t do any API call on its own\nbut returns a tuple containig: a mutate function is to execute mutation at any time an object with state of the mutation: called , loading , data and error To add a new menu item using the UI, add a Form component, and incorporate that in the App component. Edit your App.js to have Form component and use that in the App component: const CREATE_MENU_ITEM = gql ` mutation addMenuItem($name: String!, $price: Int!, $rating: Int) { addMenuItem(params: { name: $name, price: $price, rating: $rating }) { id name price rating } } ` ; const Form = () => { const [ name , setName ] = useState ( \"\" ); const [ price , setPrice ] = useState (); const [ rating , setRating ] = useState (); const [ addMenuItem ] = useMutation ( CREATE_MENU_ITEM , { update ( cache , { data : { addMenuItem } }) { const { menuItems } = cache . readQuery ({ query : GET_MENU_ITEMS }); cache . writeQuery ({ query : GET_MENU_ITEMS , data : { menuItems : [ addMenuItem , ... menuItems ] }, }); }, }); const onSubmit = ( e ) => { e . preventDefault (); addMenuItem ({ variables : { name , price : Number ( price ), rating : Number ( rating ), }, }); }; return ( <> < form className = \"form\" > < div className = \"form-group\" > < label htmlFor = \"name\" > Name < /label> < input type = \"text\" onChange = {( e ) => setName ( e . target . value )} id = \"name\" value = { name } /> < /div> < div className = \"form-group\" > < label htmlFor = \"price\" > Price < /label> < input type = \"number\" onChange = {( e ) => setPrice ( e . target . value )} id = \"price\" value = { price } /> < /div> < div className = \"form-group\" > < label htmlFor = \"rating\" > Rating < /label> < input type = \"number\" onChange = {( e ) => setRating ( e . target . value )} id = \"rating\" value = { rating } /> < /div> < /form> < button type = \"button\" onClick = { onSubmit } > Submit < /button> < /> ); }; // Edit App component to: const App = () => { > const [ formOpen , setFormOpen ] = useState ( false ); const { loading , error , data } = useQuery ( GET_MENU_ITEMS ); if ( loading ) { return < p > Loading ... < /p>; } if ( error ) { return < p > error < /p>; } const { menuItems } = data ; return ( < div className = \"App\" > < div className = \"App-body\" > < div className = \"add-form\" > > < button onClick = {() => setFormOpen (( o ) => ! o )} > Add Item < /button> > { formOpen && < Form /> } < /div> < div className = \"menu-box\" > { menuItems . map (( item ) => ( < MenuItem key = { item . id } {... item } /> ))} < /div> < /div> < /div> ); }; update function: const [ addMenuItem ] = useMutation ( CREATE_MENU_ITEM , { update ( cache , { data : { addMenuItem } }) { const { menuItems } = cache . readQuery ({ query : GET_MENU_ITEMS }); cache . writeQuery ({ query : GET_MENU_ITEMS , data : { menuItems : [ addMenuItem , ... menuItems ] }, }); }, }); Here we are passing options to the useMutation , But we can also pass options to the mutate function when we’re calling the addMutation function. The options we pass to the mutate function will override the options in useMutation . update function is a callback, that will be called after the a successful mutation, with arguments data that mutation returns\nand the current cache interface. Through mutation, you’re only updating the server-side data. If a mutation updates a single existing entity, apollo-graphql will automatically update that entity’s value in its cache when the mutation\nreturns with the updated entity’s id . Apart from updating, if we are doing other updates(addition and deletion) using mutation, we have to update the cache to reflect the changes manually. So, understanding update function we are using in above useMutation : Get the cache of the query GET_MENU_ITEMS : const { menuItems } = cache . readQuery ({ query : GET_MENU_ITEMS }); cache.writeQuery used to write data to a specific query, here are updating data of GET_MENU_ITEMS query with the newly created data using spread operator: cache . writeQuery ({ query : GET_MENU_ITEMS , data : { menuItems : [ addMenuItem , ... menuItems ] }, }); On submit: const onSubmit = ( e ) => { e . preventDefault (); addMenuItem ({ variables : { name , price : Number ( price ), rating : Number ( rating ), }, }); }; Here, we are passing variables in the options that will be used in the mutation mutation addMenuItem($name: String!, $price: Int!, $rating: Int) to create a menu item. Save the file and go to the browser to add a menu item. After you submit the form, you will see the submitted menu item will be added to the list because of the update function. You can take an exercise to implement deleting a menu item and updating the cache for the delete function. query and mutation are the main things to know in graphql. There’s also a topic called subscriptions , which uses WebSocket to push changes to the client\ninstead of us querying or mutating. Github Repo . Thanks. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-07-15"},
{"website": "Codemancers", "title": "a real world use case with elixir recursion", "author": ["Atul Bhosale"], "link": "https://crypt.codemancers.com/posts/2020-07-29-a-real-world-use-case-with-elixir-recursion/", "abstract": "A real world use case with Elixir Recursion Written by Atul Bhosale on July 29, 2020; tagged under elixir , ecto , recursion Recently, I was working on writing a background worker using Elixir for one of\nour clients. There was a requirement to update records in a database table &\nalso update those many records in another table. This post is about how I used\nRecursion to solve this use case. I am working on an application where there are orders for each account. The\nbackground worker will search all orders which are pending & mark them completed\n& update those related accounts. I searched for something similar to find_in_batches from\nRails in Elixir and found this discussion . I came across Ecto.Repo.stream/2 to iterate through each Order,\nupdate its status & then update the accounts related to those orders. It can be\ndone as follows: defmodule BalanceUpdateWorker do alias Bank.Order alias Bank.Account alias Bank.Repo import Ecto.Query @pending 0 @completed 1 def perform do Repo . transaction ( fn -> Repo . stream ( orders_query ()) |> Enum . each ( fn order -> { :ok , updated_order } = update_order ( order ) { :ok , _ } = update_account ( updated_order ) end ) end ) end defp orders_query () do from ( order in Order , where : order . status == ^ @pending ) end defp update_order ( order ) do order |> Order . changeset (%{ status : @completed }) |> Repo . update () end defp update_account ( order ) do account = Repo . get_by ( Account , user_id : order . user_id ) account |> Account . changeset (%{ amount : Decimal . add ( account . amount , order . amount )}) |> Repo . update () end end However, there is a problem with the above approach. When the time taken to\nupdate the records exceeds the timeout, Ecto will raise a timeout error as\nfollows: Also, all the record updates are rolled back. In Repo.stream/2 the SQL adapters can only enumerate a stream inside a\ntransaction. I realized that instead of having all records wrapped inside one database transaction , it’s better to have a database\ntransaction for each record update which will solve the database timeout\nproblem. In the new approach without Repo.stream/2 , I will have to handle\nfind_in_batches by myself i.e. by iterating over a batch of records & then\nproceeding to the next batch. This is a use-case of recursion . Using recursion, I can have a method that will call itself multiple times as\nlong as the number of records yet to be processed is more than the batch size. First, we need to know how many records are pending, then pass that count to a\nmethod that will process that batch of records & then calls itself again. @batch_size 500 def perform do remaining_records_count () |> iterate_multiple_times () end defp remaining_records_count do orders_query () |> Repo . aggregate ( :count ) end defp iterate_multiple_times ( count ) when count <= @batch_size , do : make_account_balance_available () defp iterate_multiple_times ( _count ) do make_account_balance_available () remaining_records_count () |> iterate_multiple_times () end defp orders_query_with_limit do from ( order in Order , where : order . status == ^ @pending , limit : ^ @batch_size ) end defp make_account_balance_available do orders_query_with_limit () |> Repo . all () |> Enum . each ( fn order -> { :ok , updated_order } = update_order ( order ) { :ok , _ } = update_account ( updated_order ) end ) end In the above refactoring, the perform/0 , finds out the count of records to be\nupdated & passes it to the iterate_multiple_times/1 recursion function.\nIf the count is above 500 it will process that batch of records & then count\nremaining records & call itself and so on. The updated code runs successfully. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-07-29"},
{"website": "Codemancers", "title": "nuances of elixir releases for phoenix applications", "author": ["Sreenadh TC"], "link": "https://crypt.codemancers.com/posts/2020-08-18-nuances-of-elixir-releases-for-phoenix-applications/", "abstract": "Nuances of Elixir Releases for Phoenix Applications Written by Sreenadh TC on August 18, 2020; tagged under elixir , phoenix , ecto Deploying a phoenix application using Releases is very straightforward in most cases. However for people who are just starting off, certain steps of configuration can be a bit confusing. I’ll try to address some of those which I have encountered in my earlier days of developing and deploying Phoenix applications. This post assumes that you already have atleast a basic Phoenix application that connects to a PostgreSQL database, and is ready to be deployed. For the sake of the post, I’ll call my application as Salmon . Compile-time vs Run-time configurations - System environment variables Configurations based on Environment variables is part and parcel of any 12Factor app . In addition to that, not all of them are available when we compile our application on our CI servers. Some of these environment variable values change based on Production and Staging environments. The elixir config/*.exs files are all compile time configurations. If we add something like this in one of those files, we will get an error instead of a successful release: # config/prod.exs use Mix.Config database_url = System . get_env ( \"DATABASE_ECTO_URL\" ) || raise \"\"\" environment variable DATABASE_ECTO_URL is missing. \"\"\" config :salmon , Salmon.Repo , url : database_url , pool_size : String . to_integer ( System . get_env ( \"POOL_SIZE\" ) || \"10\" ) If we run a mix release now, we get a Runtime error. $ MIX_ENV = prod mix release\n** ( RuntimeError ) environment variable DATABASE_ECTO_URL is missing. While this is an expected outcome of our setup, we want this to happen when we start the server on the Production/Staging environment. We may not want to set the value of DATABASE_ECTO_URL when assembling the release. Simply by moving these lines from config/prod.exs to config/releases.exs , Releases can configure the application to use runtime configurations. $ MIX_ENV = prod mix release\n* assembling salmon-1.0.0 on MIX_ENV = prod\n* using config/releases.exs to configure the release at runtime\n\nRelease created at _build/prod/rel/salmon! Compile-time vs Run-time configurations - Elixir Module attributes Another scenario where the compile-time and run-time configuration can be confusing is in the use of Application.get_env/3 and Elixir module attrs. Module attributes in Elixir are configured during compile-time. One needs to be careful when using module attributes to store values configured via Environment Variables during run-time. Those values will not be reflected inside your app! In other words, module attrs in Elixir should only be used to store constants which are available during compile-time. Everything else that happens in run-time should use functions. This includes the Application.get_env/3 app’s environment lookup. # Don't defmodule Salmon @base_url \"https://world-fishes.com/api/v1\" @api_access_token Application . get_env ( :salmon , :api_access_token ) def fetch_fishes () do ... . headers = [ { \"token\" , @api_access_token }, { \"content-type\" , \"application/json\" } ] ... . end end Even though we might set the salmon: :api_access_token based on the system env variable value when our OTP application starts, this still sets the value of @api_access_token to nil when compiling the application in environments like test and image builds. Instead we should use functions to fetch the application configuration value in run-time as: # Do defmodule Salmon @base_url \"https://world-fishes.com/api/v1\" def fetch_fishes () do ... . headers = [ { \"token\" , api_access_token ()}, { \"content-type\" , \"application/json\" } ] ... . end defp api_access_token , do : Application . get_env ( :salmon , :api_access_token ) end Integrating database and running migrations with Release artifacts When we develop a phoenix application that has database dependency, we often come across the following mix commands: $ mix ecto.create\n$ mix ecto.migrate These are nothing but the two commands used to create and migrate our database tables as per the schemas we have defined. However, when we use Releases to deploy the same application to a production environment, we might hit a roadblock. While using docker for maintaining images for deployment, we often start from alpine to keep the image size to a bare minimum. On top of that, Elixir works smoothly with containerisation. This helps us build thin docker images by just using the release artifacts from _build directory. The Mix build tool is however not available in our release artifacts. Running migrations as part of deployment is crucial, and luckily we have a neat little workaround for the same. Our release binary supports bin/salmon eval <expression> command that can be used to run Elixir expressions. All we have to do is to create a module within our Salmon app, which can run migrations with the help of Ecto ! # lib/salmon/release.ex defmodule Salmon.Release do @app :salmon def migrate do load_app () maybe_create_db () for repo <- all_repos () do { :ok , _ , _ } = Ecto.Migrator . with_repo ( repo , & Ecto.Migrator . run ( &1 , :up , all : true )) end end def rollback ( repo , version ) do load_app () { :ok , _ , _ } = Ecto.Migrator . with_repo ( repo , & Ecto.Migrator . run ( &1 , :down , to : version )) end defp all_repos do Application . fetch_env! ( @app , :ecto_all_repos ) end defp load_app do Application . load ( @app ) end defp maybe_create_db () do for repo <- all_repos () do :ok = ensure_repo_created ( repo ) end end defp ensure_repo_created ( repo ) do IO . puts ( \"==> Create #{ inspect ( repo ) } database if it doesn't exist\" ) case repo . __adapter__ . storage_up ( repo . config ) do :ok -> IO . puts ( \"*** Database created! ***\" ) :ok { :error , :already_up } -> IO . puts ( \"==> Database already exist <(^_^)>\" ) :ok { :error , term } -> { :error , term } end end end Now we can build the docker image and then deploy the same by running the below commands as part of docker run : $ _build/prod/rel/salmon/bin/salmon eval \"Salmon.Release.migrate\"\n\n$ _build/prod/rel/salmon/bin/salmon start If you are deploying this to a Kubernetes cluster, you can use Jobs to run the eval migrate command. The official Phoenix Documentation has helped me a long way in tackling the above nuances of releasing a Phoenix/Elixir application using Releases. Hope you found this post helpful. Stay tuned to our blog, if you are interested in knowing how to deploy a similar application to a Kubernetes cluster. I’ll be writing about it in a future post! If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-08-18"},
{"website": "Codemancers", "title": "what i learned while working on an elixir app", "author": ["Atul Bhosale"], "link": "https://crypt.codemancers.com/posts/2020-08-28-what-i-learned-while-working-on-an-elixir-app/", "abstract": "What I Learned while working on an Elixir App Written by Atul Bhosale on August 28, 2020; tagged under elixir , ecto , recursion , transaction Recently, I was working on writing a background worker using Elixir for one of\nour clients. This post is about what I learned while writing the worker. Adding decimal numbers I tried addition for Decimal type, the same way as we do Integers like: It raises an Arithmetic error since Decimal has an underlying struct: %Decimal{coef: coefficient(), exp: exponent(), sign: sign()} I checked the decimal library & I found that it has an add\nfunction to add Decimal or integer values. Hence the addition can be done as follows: Comparing decimal values Integer comparison can be done with == operator. How about Decimal? It returns false since the value on RHS(Right-hand-side) of == operator is not a decimal. I tried using Decimal.new as follows: Based on the warning message I tried using Decimal.from_float & Decimal.cast as\nfollows: but that too returns false. I then tried to pass a string to Decimal.new as follows: and it worked. Approach using find_in_batches While working on a business use case to fetch the pending orders & to process\nthem. The code would be as follows: defp orders_query () do from ( order in Order , where : order . status == ^ @pending ) end def perform do orders_query () |> Repo . all () |> Enum . each ( fn order -> update_order ( order ) end ) end The above code will load all the records at once in the memory. I decided to\nsearch for something which will be similar to find_in_batches from Rails in Elixir and I found this discussion . The updated code will be as follows: def perform do Repo . transaction ( fn -> orders_query () |> Repo . stream () |> Enum . each ( fn order -> update_order ( order ) end ) end ) end I have used Repo.stream which by default fetches the records in\nbatches of 500 & it needs to be wrapped in a transaction. Recursion In the above approach, I faced an issue. When the time taken to update the\nrecords exceeds the timeout, Ecto would raise a timeout error which I have\ndescribed here . I solved it by using recursion as follows: @batch_size 500 def perform do remaining_records_count () |> iterate_multiple_times () end defp remaining_records_count do orders_query () |> Repo . aggregate ( :count ) end defp iterate_multiple_times ( count ) when count <= @batch_size , do : make_account_balance_available () defp iterate_multiple_times ( _count ) do make_account_balance_available () remaining_records_count () |> iterate_multiple_times () end In the above code, the iterate_multiple_times/1 is a recursive function which calls\nitself until there aren’t any remaining records. I hope you will find these learnings helpful while building any app/library using Elixir. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-08-28"},
{"website": "Codemancers", "title": "understanding gem packaging", "author": ["George Thomas"], "link": "https://crypt.codemancers.com/posts/2020-08-17-understanding-gem-packaging/", "abstract": "Diving into the internals of Gem packaging Written by George Thomas on August 17, 2020; tagged under ruby , gems , package management Package managers are indispensible to most modern languages. Package managers\nhave allowed developers to create and distribute building blocks that can be\nused to bootstrap complex web applications. In this post we will dive into the\ninternals of Rubygems, the Ruby programming language’s package manager. Any package manager has a simple mission: take code separated into a module and\nconvert it into a format that can be easily distributed and installed in a\nnumber of different environments Anatomy of a gem A gem at the very minimum consists of a gemspec file and a Ruby file with the\nsame name as the name of the package. The convention is to place this Ruby file\nin a lib folder. -- hola\n   |\n    -- hola.gemspec\n   |\n    -- lib\n       |\n        -- hola.rb The gemspec file is used to list out the specifications for the gem. The\ngemspec must specify the authors, files, name and summary and version for the\ngem. Optionally, you can also specify runtime and development dependencies.\nThese specifications are then used when building as well as installing the gem. $LOAD_PATH.unshift File.expand_path(\"../lib\", __FILE__)\nrequire \"rspec/version\"\n\nGem::Specification.new do |s|\n  s.name        = \"rspec\"\n  s.version     = RSpec::Version::STRING\n  s.platform    = Gem::Platform::RUBY\n  s.license     = \"MIT\"\n  s.authors     = [\"Steven Baker\", \"David Chelimsky\", \"Myron Marston\"]\n  s.email       = \"rspec@googlegroups.com\"\n  s.homepage    = \"http://github.com/rspec\"\n  s.summary     = \"rspec-#{RSpec::Version::STRING}\"\n  s.description = \"BDD for Ruby\"\n\n  s.metadata = {\n    'bug_tracker_uri'   => 'https://github.com/rspec/rspec/issues',\n    'documentation_uri' => 'https://rspec.info/documentation/',\n    'mailing_list_uri'  => 'https://groups.google.com/forum/#!forum/rspec',\n    'source_code_uri'   => 'https://github.com/rspec/rspec',\n  }\n\n  s.files            = `git ls-files -- lib/*`.split(\"\\n\")\n  s.files           += [\"LICENSE.md\"]\n  s.test_files       = `git ls-files -- {spec,features}/*`.split(\"\\n\")\n  s.executables      = `git ls-files -- bin/*`.split(\"\\n\").map{ |f| File.basename(f) }\n  s.extra_rdoc_files = [ \"README.md\" ]\n  s.rdoc_options     = [\"--charset=UTF-8\"]\n  s.require_path     = \"lib\"\nend gem build To convert the gem codebase into a distributable package, Rubygems provides a build command. The build command requires a gemspec file as a parameter and\nuses the provided specifications to create .gem file which can then be\ndistributed. This .gem file is simply a tarball file that in turn consists of three .gz files: a data.tar.gz file, a checksum.yaml.gz file and a metatdata.gz file.\nA cryptographically signed gem will also contain corresponding .sig files that\ncontain signatures. -- hola.gem\n   |\n    -- checksum.yaml.gz\n   |\n    -- metadata.gz\n   |\n    -- data.tar.gz Rubygems does this using a Gem::Package::TarWriter class that creates tarballs\nfrom the information and data provided in the gemspec. The metadata file is a\ncompressed YAML file that lists the information provided in the gemspec. --- !ruby/object:Gem::Specification\nname: rspec\nversion: !ruby/object:Gem::Version\n  version: 3.9.0\nplatform: ruby\nauthors:\n- Steven Baker\n- David Chelimsky\n- Myron Marston\n    ~~~~~~~~~~~~~~~ removed for brevity ~~~~~~~~~~~~~~~~~~~~~~~~\ndependencies:\n- !ruby/object:Gem::Dependency\n  name: rspec-core\n  requirement: !ruby/object:Gem::Requirement\n    requirements:\n    - - \"~>\"\n      - !ruby/object:Gem::Version\n        version: 3.9.0\n  type: :runtime\n  prerelease: false\n  version_requirements: !ruby/object:Gem::Requirement\n    requirements:\n    - - \"~>\"\n      - !ruby/object:Gem::Version\n        version: 3.9.0\n    ~~~~~~~~~~~~~~~ removed for brevity ~~~~~~~~~~~~~~~~~~~~~~~~\nexecutables: []\nextensions: []\nextra_rdoc_files:\n- README.md\nfiles:\n- LICENSE.md\n- README.md\n- lib/rspec.rb\n- lib/rspec/version.rb\nhomepage: http://github.com/rspec\nlicenses:\n- MIT\n    ~~~~~~~~~~~~~~~ removed for brevity ~~~~~~~~~~~~~~~~~~~~~~~~\nrequired_ruby_version: !ruby/object:Gem::Requirement\n  requirements:\n  - - \">=\"\n    - !ruby/object:Gem::Version\n      version: '0'\nrequired_rubygems_version: !ruby/object:Gem::Requirement\n  requirements:\n  - - \">=\"\n    - !ruby/object:Gem::Version\n      version: '0'\nrequirements: []\nrubygems_version: 3.0.6\nsigning_key:\nspecification_version: 4\nsummary: rspec-3.9.0\ntest_files: [] The data.tar.gz file is a tarball that consists of the actual gem code as\nlisted in the files attribute in the gemspec. The checksum.yaml.gz file is a compressed YAML file containing checksums for\nthe data and metadata files. ---\nSHA256:\n  metadata.gz: 717820f4463baa76607e57e500d14c680608fe6aac01405c7cfe6fd2dcd990db\n  data.tar.gz: 919fc9aedde011882f1814d4d16cf92fdfedc728a979f0f814c819211787627f\nSHA512:\n  metadata.gz: c39a368fbab5da77ca12870485b0d7663fce1deb90b9528f57f695a8543525d61494ac55ffb4ffc7fc6a6c80c2ca2e5492499965bb26485f5c76a49916b699b7\n  data.tar.gz: 90ee39bf3cb841049201bdec98d2d45dcdfd0d7c927566621ca7be5529a6c89c8b6b85cab37166e8005aeb44279d3fcf20001aa80cdf4f1d62cd92be391bea82 gem install When gem install is run, Rubygems fetches this .gem file from the configured\ngem repository( https://rubygems.org by default) and untars the files and copies\nthem into their proper location using the Gem::Package::TarReader class. This\nlocation would depend on the OS and the Ruby version management tool\nused(rbenv, rvm). Once a gem is installed, the gem can be loaded in any Ruby file with a require statement. The require statement changes the $LOAD_PATH global variable\nwhich contains a list of paths from where code should be loaded. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-08-17"},
{"website": "Codemancers", "title": "real world usecase for genserver agent in elixir", "author": ["Sreenadh TC"], "link": "https://crypt.codemancers.com/posts/2020-08-28-real-world-usecase-for-genserver-agent-in-elixir/", "abstract": "Real world usecase for GenServer and Agent in Elixir Written by Sreenadh TC on August 28, 2020; tagged under elixir , otp , agent , genserver Concurrency is the bread and butter of an Elixir OTP application. We are gonna look at one of many ways how Elixir helps in achieving concurrency for serving several function calls parallely. Concepts discussed in this blog post requires a very basic understanding of GenServer in Elixir/Erlang. Any application that talks to an external API requiring OAuth2 credentials, would have to manage the access token and the rudementary step of refreshing the token when it expires. Although there are two common ways of making sure the tokens are refreshed, I tend to incline on the side of making sure the token is refreshed just before it expires. This way, I don’t have to wait for the token failure in order to refresh it. The other way would be to fire a refresh when we receive a 401 . But wait! ⚠️ 🚨 In most cases, distinguishing between an expired token and a wrong token is really hard, as both cases would have an HTTP-401 Unauthorized response from the server Our goal We want a module inside our Fancy application that gives us an OAuth2 access token to call https://funny-server.foo/videos/charlie-chaplin API. An ideal way of keeping the token refreshed and accessible by every other module in our Fancy application is, to cache it somewhere. Caching could be as simple as an in-memory object, in Redis or even in a file. But for storing a small string like the access token, Redis and Files are way too fancy 🤷🏽‍♂️ Best option is to use an In-Memory object storage. Elixir/Erlang in general let’s us store and share data in about 4 different ways. 1. GenServer GenServer (or General Server) is the most common and obvious OTP library you will utilize in Elixir application. We can use GenServer to implement behaviours as BEAM processes which holds a state (our data). We can also query the same state and make changes to the current state through calls and casts . 2. Agent Agent is built on top of GenServer . It requires lesser lines of code. When it comes to the actual use, one could argue that it is similar to GenServer. As for the actual usecase of storing/sharing state (our data), Agent does exactly the same thing which a bare-bone GenServer can, but in a more developer friendly manner by abstracting away the GenServer implementation. Agent exposes 4 main functions, cast/2 , get/3 , update/3 and get_and_update/3 . We’ll see them in detail later on in the post. 3. ETS (Erlang Term Storage) tables Unless you have the need and a strong reasoning behind it, I would recommend not to use ETS for usecases like our’s. The number of tables in an Erlang node used to be limited, but this is no longer the case now. Even then, one thing to keep in mind is about the inability of automatic garbage collection of the tables. This can be an issue as Erlang will only remove the tables when it’s owner process terminates. Long story short, let’s not use this option for now. 4. Dedicated Storages Modern cloud services offer high speed cache storage, and disks which can be an option for some usecases. However our’s is not one. Files and network can also be options, but those aren’t viable for us. Alright, so let’s look at GenServer and Agent in detail. Firstly I will show how a GenServer version of Fancy.Auth looks like. We’ll then see how Agent help us unclutter some of our code into a concise easy to read set of lines. defmodule Fancy.Auth do use GenServer @endpoint \"https://funny-server.foo/oauth2/token\" # Client APIs def start_link ( _arg ) do GenServer . start ( __MODULE__ , %{}, name : __MODULE__ ) end def token do GenServer . call ( __MODULE__ , :token ) end # Server APIs def init ( state ) do # fetch token and save it as state in the GenServer process { :ok , refresh_token ()} end def handle_call ( :token , _from , state ) do { token , new_state } = get_token ( state ) { :reply , token , new_state } end defp refresh_token () do # ... lines skipped for brevity ... %{ status_code : 200 , body : body } = resp = HTTPoison . post! ( @endpoint , payload , headers , options ) # Successfully fetched access token body = Jason . decode! ( body , keys : :atoms ) Map . put ( body , :expires_in , :os . system_time ( :seconds ) + body . expires_in ) end defp get_token (%{ expires_in : expires_in , access_token : token } = state ) do now = :os . system_time ( :seconds ) # I am greedy ^_^ has_aged? = now + 1 > expires_in if has_aged? do # Refresh OAuth token as it has aged auth = refresh_token () { auth . access_token , auth } else # No need to refresh, send back the same token { token , state } end end end This is a very simple and straightforward GenServer which exposes a single API, the Fancy.Auth.token/1 that retrieves an access token from funny-server.foo and also refreshes if it is expiring in the next second.\nNow let us see how we can achieve same functionality using Agent with lesser but more readable lines of code. defmodule Fancy.Auth do use Agent @endpoint \"https://funny-server.foo/oauth2/token\" def start_link ( _arg ) do Agent . start_link ( fn -> refresh_token () end , name : __MODULE__ ) end def token do token = Agent . get_and_update ( __MODULE__ , fn state -> get_token ( state ) end ) token end # ... lines below are exactly the same as in the GenServer snippet ... defp refresh_token () do # ... lines skipped for brevity ... %{ status_code : 200 , body : body } = resp = HTTPoison . post! ( @endpoint , payload , headers , options ) # Successfully fetched access token body = Jason . decode! ( body , keys : :atoms ) Map . put ( body , :expires_in , :os . system_time ( :seconds ) + body . expires_in ) end defp get_token (%{ expires_in : expires_in , access_token : token } = state ) do now = :os . system_time ( :seconds ) # I am greedy ^_^ has_aged? = now + 1 > expires_in if has_aged? do # Refresh OAuth token as it has aged auth = refresh_token () { auth . access_token , auth } else # No need to refresh, send back the same token { token , state } end end I have used Agent.get_and_update/3 here, but we can also do it in two steps by using Agent.get/3 inside Auth.token/1 and Agent.update/3 inside Auth.refresh_token/0 . Agent.cast/2 can be used to make the Auth.refresh_token/0 asynchronous. Keep in mind that cast is Fire-and-Forget, meaning the actual execution of API call for refreshing the access token happens asynchronously. Depending on your use case, this may or may not be favorable. What exactly did we gain here by using Agent over GenServer? I would agree on the fact that GenServer makes our module look more complex. In reality, the only core functionality of this module is, fetching an access token. For such concise usecase Agent makes more sense. While implementing a GenServer behaviour, a common pattern is abstraction of Client -> Server functions away from the user code for packages or the business logic in applications. This introduces additional lines of code in our module. While this is great for a more complex server, our’s doesn’t need to be bloated with GenServer handle_call functions and other stuff. Agent looks more neat and thin! Having said that, there is no real performance gain in using GenServer over Agent for such usecases. I am not talking about the benchmarks where you might see an ever so slightly better performance from GenServer, which would be of the order of µs. However, a GenServer is much easier to understand in more complex cases. Separating callbacks from the public API would let me document the code better and also help me reuse most of the code. In the end its your code, take the call! This post does not intend to favor one OTP over the other. I believe the choice of Agent vs GenServer comes down to more of a personal preference for a particular usecase. Keep it simple and stupid with Agent when the module itself is a simple key-value access mechanism. Separate the concerns using GenServer when the functionality of the module is much larger and you require fine tuned handling of callbacks! I hope you found this post useful in implementing similar real world usecases. Stay tuned to our blog for more exciting topics! If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-08-28"},
{"website": "Codemancers", "title": "using index of an array as key can lead to bad consequences", "author": ["Sujay Prabhu"], "link": "https://crypt.codemancers.com/posts/2021-04-14-using-index-of-an-array-as-key-can-lead-to-bad-consequences/", "abstract": "Using Index of an array as KEY Can Lead to Bad Consequences Written by Sujay Prabhu on April 14, 2021; tagged under reactjs Every application makes use of Lists/arrays in one or the other way.\nWhen working with lists in React, one need to be more cautious, because of the key prop which is used with elements within array. key prop is an identity to an element in the DOM. So, it is expected to be unique and static in order to differentiate between the elements. If you fail to pass key prop, you will end up seeing this in your console . Warning: Each child in a list should have a unique key prop At this point, if you think, hey, it's just a warning... , you are taking a chance ( PLEASE DON’T ).\nBecause this may not seem like causing a serious issue, Believe me, you are not safe either. Ways of implementing Keys There are various ways of implementing keys with lists. 1. index of an array Index as a key is an anti-pattern.\nThis might be the easiest approach to get rid of the warning in the console, but this is a double edged sword.\nThis approach may go wrong, if implemented without proper knowledge. So, lets have a look at situations in which indexes cannot be used as keys If new elements are added to the beginning or somewhere in the middle If array is filtered const App = () => { const [ counter , setCounter ] = useState ( 1 ); const [ list , setList ] = useState ([ { id : counter , }, ]); const addToStart = () => { const newId = counter + 1 ; setCounter ( newId ); setList (( oldArray ) => [ { id : newId , }, ... oldArray , ]); }; const addToEnd = () => { const newId = counter + 1 ; setCounter ( newId ); setList (( oldArray ) => [ ... oldArray , { id : newId , }, ]); }; return ( < div className = \"App\" > < button onClick = { addToStart } > Add To Start < /button> < button onClick = { addToEnd } > Add To End < /button> < table > < tr > < th > Id < /th> < th > Item < /th> < /tr> { list . map (( item , index ) => ( < tr key = { index } > < td > < label > { item . id } < /label> < /td> < td > < input /> < /td> < /tr> ))} < /table> < /div> ); }; In the above code snippet, we have an array and we can add elements either to start/end of the array. If we add elements to end of the array, Item 1 will have key 0 and Item 2 will have key 1 .\nThen, if element is added to start of the array, Item 3 falls on top, but it ends up having key 0 instead of 2 . In order to avoid this unexpected behaviour, we should not use array index as key { list . map (( item ) => ( < tr key = { item . id } > < td > < label > { item . id } < /label> < /td> < td > < input /> < /td> < /tr> )); } Now, if the same steps are followed as above, if element is added to the start of the array, Item 3 falls on top and will have key 2 indexes can be used as keys If new elements are pushed to the end of the array (as pushing elements to the end of the array will not affect indexes of existing elements) If the array is static If the array is not filtered In the above snippet, we can have array index as key, if we are adding element only to the end of the array. { list . map (( item , index ) => ( < tr key = { index } > < td > < label > { item . id } < /label> < /td> < td > < input /> < /td> < /tr> )); } Here is the link of the repository , in which, index is used as key and the other way round for transitions. This demo shows how using index as key will lead to unexpected behaviour. 2. Unique id from dataset This is undoubtedly the best approach as you have unique id available from the data provided. const items = [ { id : 1 , value : 'Item 1' , }, { id : 2 , value : 'Item 2' , }, { id : 3 , value : 'Item 3' , }, ]; < div > { items . map (( item ) => { return < div key = { item . id } > { item . value } < /div>; })} < /div>; 3. Generate unique ids using packages In situations where you don’t have unique ids along with the list, it is better to generate unique keys with the help of some packages.\nA lot of packages are available in the internet for generating unique keys/ids like react-uuid , uuid . I am going with react-uuid for now. import uuid from 'react-uuid' ; const arrayWithoutIds = [ 'user 1' , 'user 2' , 'user 3' ]; const arrayWithIds = arrayWithoutIds . map (( element ) => { return { id : uuid (), value : element , }; }); const Component = ({ arrayWithIds }) => { return ( < div > { arrayWithIds . map (( item ) => { return < div key = { item . id } > { item . value } < /div>; })} < /div> ); }; One thing to remember here is, keys should be generated while creating the dataset or before the component mounts ( componentWillMount ), in order to prevent generation of unique id on every render. 4. Math.Random() Usage of Math.Random() is not recommended because of its unstability .\nBecause, one cannot deny the possibility of generating the same number twice. Conclusion: Try to implement keys using unique id from dataset / generate unique ids using packages approach. index as key can be used as a last resort. Do not use Math.Random() as keys. Happy Learning If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2021-04-14"},
{"website": "Codemancers", "title": "poor mans ha vault server with auto unseal using kms", "author": ["Ananth Kamath"], "link": "https://crypt.codemancers.com/posts/2020-08-24-poor-mans-ha-vault-server-with-auto-unseal-using-kms/", "abstract": "Poor Man's High Availability Vault Server With Auto Unseal Using KMS Written by Ananth Kamath on August 24, 2020; tagged under vault , devops , aws , kms , hashicorp , unseal Hashicorp’s Vault provides a solution to secure, store and control access to secrets and other sensitive data. There are multiple ways to achieve a highly available Vault setup, some might use a cluster of vault servers, a few others might go with a master slave deployment strategy. In this blog we will be choosing a poor man’s approach to achieve high availability for a non complex and small scale project with minimal configuration and resources. This post assumes that you have a basic understanding of the Vault and how to install it. To know more about installing the Vault server on an EC2 instance, please refer to our blog post on SSH Access Management with Vault Why Auto Unseal for HA? Before we move into the details of implementation, we need to answer the question why Auto Unseal is necessary for high availability?\nVault server always boots up in the sealed state. Data in the Vault storage is always stored in an encrypted format. Vault is configured to know how to access the physical storage and its location, but doesn’t know how to decrypt any of it. Unsealing is the process of constructing the master key necessary to read the decryption key to decrypt the data. If the Vault server restarts itself or is manually restarted by an user, it goes back to the sealed state and will have to be manually unsealed. This can cause major outages which would come up as an unnecessary surprise during off hours. Auto Unseal is a process where the keys required to unseal the Vault are stored in the key management system like AWS KMS and is made available to the Vault to unseal itself on accidental or forced reboot without any manual interruption required. Components used in this solution:(We will be using AWS as cloud provider) AWS KMS Key AWS IAM policies and roles required to access AWS KMS Key EC2 server with Auto Recovery (To minimize the outage caused due to hardware failure) Vault binary to be deployed on EC2 IAM and KMS Setup: Start the implementation by creating the IAM policy called VaultKMSUnsealPolicy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"kms:Encrypt\" , \"kms:Decrypt\" , \"kms:DescribeKey\" ], \"Resource\" : \"*\" } ] } Create an IAM role called VaultKMSUnsealRole and attach the VaultKMSUnsealPolicy to it. Create an EC2 instance enabled with Auto-Recovery. (To know more about Auto recovery go through this AWS Blog ) Now attach the VaultKMSUnsealRole role to the EC2 instance. (If you already have an IAM role attached to your instance, skip step 2 and attach the VaultKMSUnsealPolicy to the existing role directly) Go to AWS KMS section and create a new synchronous key called vault . Make note of the key id, as we will be using this in the next step. Vault Config Setup Download and install the Vault binary. Below step has to be performed before initialising the vault. Once initialised it would already have the 5 keys provided by default which has to be then used to unseal the vault manually. Caution: This step has to be completed before vault operator init is run. Modify or create the vault config /etc/vault.d/vault.hcl to include below module. seal \"awskms\" { region = \"${aws_region}\" kms_key_id = \"${kms_key}\" } awskms is the module used by Vault to communicate with the AWS KMS to fetch and update the keys aws_region refers to the AWS region where the AWS KMS key is setup kms_key_id is the key id of the AWS KMS key( vault ) created in IAM and KMS Setup section Once the config has been updated with above content, initialise the vault with below command. $ export VAULT_ADDR=\"http://127.0.0.1:8200\"\n$ vault operator init >> ~/vault.keys\nInitial Root Token: s.######\n\nUnseal Key 1: key1###\nUnseal Key 2: key2###\nUnseal Key 3: key3###\nUnseal Key 4: key4###\nUnseal Key 5: key5### Yes, we did it!!! We now have a Vault server which is setup with Auto Unseal feature, deployed over an auto recoverable EC2 instance. Now if your EC2 instance reboots due to a hardware failure or by an accident caused by a human error the Vault server will automatically boot up in an unsealed state and will be ready for use without any manual intervention. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-08-24"},
{"website": "Codemancers", "title": "working around rate limiting external apis with elixir", "author": ["Sreenadh TC"], "link": "https://crypt.codemancers.com/posts/2021-04-20-working-around-rate-limiting-external-apis-with-elixir/", "abstract": "Working around rate limiting external apis with Elixir Written by Sreenadh TC on April 20, 2021; tagged under elixir , redis Have you ever come across API services that restricts the number of calls you can make within a time period?\nThis in fact is very common these days, that developers would have to work around the limitation in order to not flood the service and also reduce the cost. Neither the limitation nor the solution is new to most of us, but every language has its own way of implementing it. We are going to look how we can solve this using caching with redis and pattern matching in elixir. Suppose we have an application “oscar”, that fetches movie ratings from 5 review portals using APIs. The problem does not sound complicated, so let’s add some more layers to it. Users can request a movie to be rated, but results are curated asynchronously by Oscar All of the 5 APIs have rate limits, calling frequently will produce 429 http status code Let’s start with a basic Elixir module defmodule Oscar do alias Oscar . { Imdb , RottenTomatoes , Sify , Netflix , AmazonPrime } def rate_movie ( name ) do Logger . info ( \"Fetching movie ratings for #{ name } \" ) ratings = %{ imdb : Imdb . get_rating! ( name ), rt : RottenTomatoes . get_rating! ( name ), sify : Sify . get_rating! ( name ), netflix : Netflix . get_rating! ( name ), amazon : AmazonPrime . get_rating! ( name ) } save ( name , ratings ) end end Nothing fancy here. We have 5 clients namely Imdb, RottenTomatoes, Sify, Netflix, and AmazonPrime all of which allows us to “get_rating” of a movie. defmodule Oscar.Imdb do def get_rating! ( name ) do case HTTPoison . get! ( \"https://imdb.com/movies/ #{ name } \" ) do % HTTPoison.Response { status_code : 200 , body : body } -> Logger . debug ( \"Got Imdb rating for #{ name } \" ) parse_rating ( body ) _ -> raise HTTPoison.Error , reason : \"Oops! we got a non 200 status code from Imdb\" end end # ... other functions omitted for brevity end As you can see, 200 status code is important for us to parse the rating. All other status code for the purpose of this blog is unacceptable for us. Assuming we have implemented the other 4 clients, we move onto calling these APIs within a background worker. Unlike ruby where you’d use Sidekiq, or Celery in python, most often elixir does not require such heavy background workers. But our assumption is that the problem requires us to use Redis caching methods towards the end, so let’s use ExQ as background worker. Case 1: All APIs succeed: iex> {:ok, job_id} = Exq.enqueue(Exq, \"default\", OscarWorker, [\"interstellar\"])\n[info] Fetching movie ratings for interstellar\n[debug] Got Imdb rating for interstellar\n[debug] Got RottenTomatoes rating for interstellar\n[debug] Got Sify rating for interstellar\n[debug] Got Netflix rating for interstellar\n[debug] Got AmazonPrime rating for interstellar\n[info] Saved rating for interstellar\n[info] Elixir.Oscar.OscarWorker[e90f871a-a2dd-4a86-8914-6e94dfde09b3] success: 1250ms Awesome! That worked like a charm. Case 2: AmazonPrime blocks with 429: iex> {:ok, job_id} = Exq.enqueue(Exq, \"default\", OscarWorker, [\"interstellar\"])\n[info] Fetching movie ratings for interstellar\n[debug] Got Imdb rating for interstellar\n[debug] Got RottenTomatoes rating for interstellar\n[debug] Got Sify rating for interstellar\n[debug] Got Netflix rating for interstellar\n** (HTTPoison.Error) \"Oops! we got a non 200 status code from AmazonPrime\"\n...\n[info] Elixir.Oscar.OscarWorker[e90f871a-a2dd-4a86-8914-6e94dfde09b3] fail: 1270ms\n[info] Queueing job e90f871a-a2dd-4a86-8914-6e94dfde09b3 to retry in 54.0 seconds\n.\n.\n[info] Fetching movie ratings for interstellar\n[debug] Got Imdb rating for interstellar\n[debug] Got RottenTomatoes rating for interstellar\n[debug] Got Sify rating for interstellar\n[debug] Got Netflix rating for interstellar\n[debug] Got AmazonPrime rating for interstellar\n[info] Saved rating for interstellar\n[info] Elixir.Oscar.OscarWorker[e90f871a-a2dd-4a86-8914-6e94dfde09b3] success: 2750ms Here the worker had to retry once to get all the ratings. When we have 5 services, its possible that many a times all of them might hit the rate limit within one job, delaying the results or even end up as a dead job. How can we reduce the number of calls we make to same service for the very same movie, to exactly once within a job? When we look at caching options, there are other ways to store the data we already queried, into our redis. However, we are going to consider the “per job basis” for this blog. Meaning, everytime we fetch a rating from one portal, we cache it. Caching based on movie name is also possible but it would mean the review can get outdated, and we have to manage re-fetch based on timestamp. Let’s not get into those in this blog. Simply caching it doesn’t really help us. We need to lookup for the cache before calling the API. Let’s make some changes to use cache: defmodule Oscar do alias Oscar . { Imdb , RottenTomatoes , Sify , Netflix , AmazonPrime } alias Oscar.RedisCache @default %{ imdb : nil , rotten_tomatoes : nil , sify : nil , netflix : nil , amazon_prime : nil , uid : nil } def rate_movie ( uid , name ) do Logger . info ( \"Fetching movie ratings for #{ name } \" , uid : uid ) metadata = get_cached_or_new_metadata ( uid ) metadata |> get_rating ( \"imdb\" , name ) |> cache () |> get_rating ( \"rt\" , name ) |> cache () |> get_rating ( \"sify\" , name ) |> cache () |> get_rating ( \"netflix\" , name ) |> cache () |> get_rating ( \"amazon_prime\" , name ) |> cache () |> save ( name ) end defp get_cached_or_new_metadata ( uid ) do case RedisCache . get ( uid ) do { :ok , nil } -> Map . put ( @default , :uid , uid ) { :ok , cached } -> Jason . decode! ( cached , keys : :atoms ) end end defp cache ( metadata ) do :ok = RedisCache . set ( metadata . uid , Jason . encode! ( metadata )) metadata end defp get_rating (%{ imdb : imdb , uid : uid } = metadata , \"imdb\" , name ) when is_float ( imdb ) do Logger . debug ( \"Imdb rating for #{ name } exists in cache\" , uid : uid ) metadata end defp get_rating ( metadata , \"imdb\" , name ) when is_float ( imdb ) do Map . put ( metadata , :imdb , Imdb . get_rating! ( name )) end defp get_rating (%{ rt : rt , uid : uid } = metadata , \"rt\" , name ) when is_float ( rt ) do Logger . debug ( \"RottenTomatoes rating for #{ name } exists in cache\" , uid : uid ) metadata end defp get_rating ( metadata , \"rt\" , name ) when is_float ( rt ) do Map . put ( metadata , :rt , RottenTomatoes . get_rating! ( name )) end # ...other similar functions omitted for brevity end Now we revisit our cases once again and see if these changes helped. Assuming we have already wrapped up the Oscar.rate_movie/2 call in an ExQ OscarWorker , we can queue the job like this: Case 1: All APIs succeed: iex> {:ok, job_id} = Exq.enqueue(Exq, \"default\", OscarWorker, [\"interstellar\"])\n[info] Fetching movie ratings for interstellar\n[debug] Got Imdb rating for interstellar\n[debug] Got RottenTomatoes rating for interstellar\n[debug] Got Sify rating for interstellar\n[debug] Got Netflix rating for interstellar\n[debug] Got AmazonPrime rating for interstellar\n[info] Saved rating for interstellar\n[info] Elixir.Oscar.OscarWorker[ab7f871a-c4ae-4a86-8914-6e94dfde09b3] success: 1250ms As expected there are no issues here, let’s move on to next case. Case 2: Sify and AmazonPrime blocks with 429: iex> {:ok, job_id} = Exq.enqueue(Exq, \"default\", OscarWorker, [\"interstellar\"])\n[info] Fetching movie ratings for interstellar\n[debug] Got Imdb rating for interstellar\n[debug] Got RottenTomatoes rating for interstellar\n** (HTTPoison.Error) \"Oops! we got a non 200 status code from Sify\" <---- first failure\n...\n[info] Elixir.Oscar.OscarWorker[ab7f871a-c4ae-4a86-8914-6e94dfde09b3] fail: 1270ms\n[info] Queueing job ab7f871a-c4ae-4a86-8914-6e94dfde09b3 to retry in 54.0 seconds\n.\n.\n[info] Fetching movie ratings for interstellar\n[debug] Imdb rating for interstellar exists in cache\n[debug] RottenTomatoes rating for interstellar exists in cache\n[debug] Got Sify rating for interstellar\n[debug] Got Netflix rating for interstellar\n** (HTTPoison.Error) \"Oops! we got a non 200 status code from AmazonPrime\" <---- second failure\n...\n[info] Elixir.Oscar.OscarWorker[ab7f871a-c4ae-4a86-8914-6e94dfde09b3] fail: 1570ms\n[info] Queueing job ab7f871a-c4ae-4a86-8914-6e94dfde09b3 to retry in 125.0 seconds\n.\n.\n[info] Fetching movie ratings for interstellar\n[debug] Imdb rating for interstellar exists in cache\n[debug] RottenTomatoes rating for interstellar exists in cache\n[debug] Sify rating for interstellar exists in cache\n[debug] Netflix rating for interstellar exists in cache\n[debug] Got AmazonPrime rating for interstellar\n[info] Saved rating for interstellar\n[info] Elixir.Oscar.OscarWorker[ab7f871a-c4ae-4a86-8914-6e94dfde09b3] success: 250ms iex > { :ok , cached } = Oscar.RedisCache . get ( uid ) iex > Jason . decode! ( cached , keys : :atoms ) %{ imdb : 4.90 , rt : 4.75 , sify : 4.8 , netflix : 4.85 , amazon_prime : 4.90 } Using caching was straighforward, and the solution works as intended. Oscar does not call Imdb/RottenTomatoes/Netflix more than once between retries. This helps us preserve the API calls for upcoming jobs instead of exhausting more calls for finishing this particular job. What excites me more is how easy this is to write in Elixir! The functions are super lean and no if-else blocks to check if we have already cached our progress! defp get_rating (%{ imdb : imdb , uid : uid } = metadata , \"imdb\" , name ) when is_float ( imdb ) do Logger . debug ( \"Imdb rating for #{ name } exists in cache\" , uid : uid ) metadata end defp get_rating ( metadata , \"imdb\" , name ) when is_float ( imdb ) do Map . put ( metadata , :imdb , Imdb . get_rating! ( name )) end These two patterns for the get_rating/3 function takes care of fetching data either from cache or API. Since we store the metadata (the ratings so far) as json string and decode it back to an Elixir map, it makes it very simple to pattern match against the map keys. If we have a floating point value in the key, this means we have already fetched the rating for that particular portal (in this case, imdb). We can simply return the current metadata as is without calling the Imdb API. Furthermore, if this pattern match fails, the default fallback pattern is to fetch the rating from API and we put the value into the same key we look for in the next iteration (if any). 🎉 Ain’t that cool! 💜 This is a minimal version of a bigger problem I helped a client solve recently. The app makes around 10-15 external service calls, most of which gets blocked if we start hitting them aggressively. Since the data we query for is the same across a particular job, we can always save the current progress and pick up from where we left off if we crash. But as I mentioned in the beginning, you don’t always need Redis or a dedicated background worker with Elixir. You can achieve this very same behaviour with a GenServer! As for me, the app was already using many of the good features of ExQ which needs Redis. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2021-04-20"},
{"website": "Codemancers", "title": "", "author": "Unknown", "link": "https://crypt.codemancers.com/posts/", "abstract": "", "date": "2021-05-14"},
{"website": "Codemancers", "title": "encrypting and storing kubernetes secrets in git", "author": ["Atul Bhosale"], "link": "https://crypt.codemancers.com/posts/2020-04-27-encrypting-and-storing-kubernetes-secrets-in-git/", "abstract": "Encrypting and storing Kubernetes secrets in Git Written by Atul Bhosale on April 27, 2020; tagged under devops , kubernetes , secrets , gitops , argocd Recently, I configured Kubernetes secrets for a Rails app to be stored in git\nfor deploying the app using the GitOps approach for one of our clients.\nThis blog post is about the approach I followed to store the Kubernetes secrets\nfor GitOps. Storing Kubernetes manifest on git We store the deployment, configmap, ingress & service YAML files in Gitlab.\nHence some part of GitOps is already done. For the secrets.yml I\nhad to first export secrets from the Kubernetes cluster to yaml since it’s not\nstored anywhere else using: kubectl get secrets my-app-secret -o yaml > secrets.yaml This is how secrets.yaml content looks like: apiVersion: v1\ndata:\n  user: cGFzcw==\nkind: Secret\nmetadata:\n  name: my-app-secret\ntype: Opaque Secrets format In Kubernetes, secrets are store encoded in base64 format, which can be\ndecoded easily. When you follow GitOps and secrets are to be stored\non git, they can’t be stored in base64 format, they need to be encrypted &\nstored on git. I came across kubeseal to solve this. What is Kubeseal? Its a tool used for encrypting Kubernetes secrets. Kubeseal will be used to\nmake SealedSecrets as templates for secrets. A SealedSecret will be a CRD(Custom Resource Definition) that can be decrypted only by\nthe kubeseal controller running on your Kubernetes cluster. The controller then\ncreates a Kubernetes secret on the cluster. Using kubeseal You can install kubseal by following the instructions from its README.md . The kubeseal consits of two parts. Client - Installed locally. Controller - Installed on the remote cluster. For the controller to decrypt the SealedSecret it needs a certificate. While\ncreating a SealedSecret locally we will use the certificate to encrypt it\nlocally. Following is the command to fetch the certificate. kubeseal --fetch-cert > staging.pem Now with this certificate we can create a SealedSecret . echo -n foo | kubeseal --raw --from-file secrets.yml --cert staging.pem -o yaml --name mysecret which returned: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq... but we need a SealedSecret resource as yaml instead of just a secret. We need\nto pass a Kubernetes secret.yaml file for kubeseal to create a SealedSecret for us. kubeseal --cert staging.pem --format=yaml < secrets.yml which returns: apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  creationTimestamp: null\n  name: my-app-secret\n  namespace: default\nspec:\n  encryptedData:\n    my-db-pass: AgChw/NSDxfhun7kWZZnXHR6zj...\n  template:\n    metadata:\n      creationTimestamp: null\n      name: my-app-secret\n      namespace: default\n    type: Opaque Now I can output it to a yaml file as: kubeseal --cert staging.pem --format=yaml < secrets.yml > sealedsecret.yml The above steps are helpful when we don’t have full access to the Kubernetes\ncluster for using the kubeseal controller directly. Other ways to create SealedSecret There is another way to generate a sealedsecret with kubeseal command,\nwe need to pass secret.yaml to kubeseal command as input. Note that we are not\npassing the certificate manually this time, the controller will fetch it\nautomatically. kubectl create secret generic app-secret --dry-run --from-literal=foo=bar -o yaml | \\\n kubeseal \\\n --controller-name=kubeseal-sealed-secrets \\\n --controller-namespace=kubeseal \\\n --format yaml > sealedsecret.yaml this will create a sealedsecret.yaml. The sealedsecret.yaml can be committed to\ngit along with other kubernetes manifest files for GitOps. Using these manifest\nfiles we can deploy our app using any CD tool. In our case, we deployed using\nArgo CD, an opensource GitOps continuous delivery tool for Kubernetes. When the sealedsecrets.yaml is deployed to the Kubernetes cluster the\ncontroller will unseal the SealedSecret automatically & create a secret. It\nwill also watch for any changes to the SealedSecret & will update the\ncorresponding secret. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post Next Post →", "date": "2020-04-27"},
{"website": "Codemancers", "title": "guide to setup gitlab ci for rails test", "author": ["Menuba Tenny"], "link": "https://crypt.codemancers.com/posts/2021-05-14-guide-to-setup-gitlab-ci-for-rails-test/", "abstract": "Guide to set up Gitlab CI for Rails tests Written by Menuba Tenny on May 14, 2021; tagged under rails , Gitlab CI In this article, I’ll walk you through the steps for setting up a rails application on Gitlab CI. We’ll create a new application and configure it to execute tests on Gitlab CI. Create a new ruby on rails application $ rails new sample_rails_ci_setup After creating the application you need to push it to Gitlab. Use the instructions on Gitlab to create new project and push the code. Verify whether the test command is running successfully in the CLI by giving the command rails test Create Configuration file for CI/CD pipeline Gitlab uses a .gitlab-ci.yml file for the instructions to be executed in the CI pipeline. Let us start by creating the file in the project’s root directory. Note: You can use YAML Linter to validate the .yml file for proper indentation. ( http://www.yamllint.com/ ) Verify Successful execution using sample script Let us check whether the Gitlab CI/CD pipeline is executing the commands in the .gitlab-ci.yml by giving a sample script. stages:\n    - build\n    - test\n\nbuild-job:\n    stage: build\n    script:\n        - echo \"Hello\"\ntest-job:\n    stage: test\n    script:\n        - echo \"This job tests something\" Here, stages are the steps that are involved in the execution. The commands under the build stage will be executed first and then the commands under the test stage will be executed. script is where we write the commands that needs to be executed. For better understanding, I have added two stages build & test . Since we are working on testing we will not use the build stage now. Let’s commit and push the changes to Gitlab and verify that both the build & test jobs execute successfully. Sample script successfully executes in the pipeline. Script for testing Now that the sample code for test has been executed successfully, let’s replace the script for testing the rails application by the command bundle exec rails test . stages:\n    - test\ntest-job:\n    stage: test\n    script:\n        - bundle exec rails test Commit the changes and push it to Gitlab. Next we get the following error bundler: command not found: rails Install bundler Let’s install the bundler gem. Here I am specifying the version that matches my local bundler version. Also, in the next step give a command for bundle install to install all gems from our Gemfile. stages:\n    - test\ntest-job:\n    stage: test\n    script:\n        - gem install bundler -v 2.1.4\n        - bundle install\n        - bundle exec rails test While running it shows: Your Ruby version is 2.5.9, but your Gemfile specified 2.7.2 Use the correct ruby version Seems like Gitlab currently uses ruby v2.5.9 by default. Since we are using ruby v2.7.2, let’s specify CI to use the same ruby version for testing using the image key image: ruby:2.7.2 Before specifying the version just confirm which version you are currently using.\nCommand to check the version of ruby is ruby -v stages:\n  - test\ntest-job:\n  stage: test\n  image: ruby:2.7.2\n  script:\n    - gem install bundler -v 2.1.4\n    - bundle install\n    - bundle exec rails test Now, let’s commit these changes to Gitlab and check whether the rails test command is running. The tests execute successfully! Set Postgresql as database for testing Since we are using postgres in production, let’s use the same in testing environment too. And also PostgreSQL is the most popular choice for Ruby on Rails projects. stages:\n    - test\ntest-job:\n    stage: test\n    image: ruby:2.7.2\n    services:\n        - postgres:12.0\n    variables:\n        POSTGRES_USER: depot_postgresql\n        POSTGRES_PASSWORD: depot_postgresql\n        DB_USERNAME: depot_postgresql\n        DB_PASSWORD: depot_postgresql\n        DB_HOST: postgres\n        RAILS_ENV: test\n        DISABLE_SPRING: 1\n        BUNDLE_PATH: vendor/bundle\n    script:\n        - gem install bundler -v 2.1.4\n        - bundle install\n        - bundle exec rails db:create db:schema:load --trace\n        - bundle exec rails test In this block of code we have included postgres service version 12.0 and ENV variables for setting up postgres user and database for testing.\nThe command bundle exec rails db:create db:schema:load --trace will create a database and it will also load the database from schema.rb file. Update the database.yml to use postgres instead of sqlite database In your database.yml file, test:\n  adapter: postgresql\n  encoding: unicode\n  pool: <%= ENV['DB_POOL'] %>\n  username: <%= ENV['DB_USERNAME'] %>\n  password: <%= ENV['DB_PASSWORD'] %>\n  host: <%= ENV['DB_HOST'] %>\n  port: 5432\n  database: depot_test Now, push the changes to Gitlab The above error occured because schema.rb file doesn’t exist in db/config, let’s create schema.rb by migrating the database. Migrating the database Now, we can give the command bundle exec rails db:migrate to migrate the database. stages:\n    - test\ntest-job:\n    stage: test\n    image: ruby:2.7.2\n    services:\n      - postgres:12.0\n    variables:\n      POSTGRES_USER: depot_postgresql\n      POSTGRES_PASSWORD: depot_postgresql\n      DB_USERNAME: depot_postgresql\n      DB_PASSWORD: depot_postgresql\n      DB_HOST: postgres\n      RAILS_ENV: test\n      DISABLE_SPRING: 1\n      BUNDLE_PATH: vendor/bundle\n    script:\n      - gem install bundler -v 2.1.4\n      - bundle install\n      - bundle exec rails db:migrate\n      - bundle exec rails db:create db:schema:load --trace\n      - bundle exec rails test The tests execute successfully! This was a basic TDD-like step by step setup guide to setting up rails tests on Gitlab CI. If you have any questions or feedback, feel free to drop us a mail at team@codemancers.com . ← Previous Post", "date": "2021-05-14"}
]