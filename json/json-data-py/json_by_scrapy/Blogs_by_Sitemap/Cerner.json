[
{"website": "Cerner", "title": "Version Databag - A Chef Release Process", "author": ["Mike Rzepka"], "link": "https://engineering.cerner.com/blog/version-databag-a-chef-release-process/", "abstract": "At Cerner, we use Chef . In fact, we’re heavily ingrained with Chef in our configuration management practices . We deploy services from Tomcat to Kafka with Chef. Even the first open source project we announced was a tool for Chef ! With all of this integration with Chef, we need a simple way to manage all of those versions. This is where the Version Databag cookbook comes in. At a high level, this cookbook allows us to define our versions in a centralized data bag (grouped under conceptual units) and populates the corresponding node attributes required with those versions at runtime. This significantly reduces the effort involved in finding, updating, and maintaining the various versions of things that we have across our Chef configuration. Because we found it so useful, we decided to open source it! We encourage you to try it and manage Chef versions with it for easier configuration management practices. If you feel that it could be further improved, let us know (or better yet, submit the improvement yourself!). We’re welcome to take enhancements and feedback to improve our processes.", "date": "2017-04-04"},
{"website": "Cerner", "title": "One Cerner Style Icons: The future of icons at Cerner", "author": ["Seth Claybrook", "Bradley Scott"], "link": "https://engineering.cerner.com/blog/one-cerner-style-icons/", "abstract": "One Cerner Style Icons (OCS Icons) are foundational UI elements required to produce a cohesive and consistent User Experience for Cerner Solutions. OCS Icons consist of a comprehensive icon library of platform agnostic assets which can be easily consumed by development teams. The primary goal of this effort to elevate every aspect of the UI to create the best possible user experience.  In order to do that, it is imperative to develop a system composed of strong design patterns and well-founded user research.  By creating a single repository of platform agnostic icons, we are able to better manage icons and create a framework that makes consuming icons easier and more efficient.  By reducing the number of separate libraries, styles and formats, we ease the burden on implementation for developers.  In addition, we dramatically reduce the amount of effort needed to maintain a host of different asset libraries while producing a higher quality product.  Going open source also facilitates a more seamless process when icons need updated or when styles change and evolve. OCS Icons are the core iconographic communication vehicle between the functional software and the UI.  The icons are designed on the foundation of a strict icon language consisting of core design principles and uniform style.   Proper implementation requires consumers to be on a code guide and obtain assets by referencing codified Scalable Vector Graphics.  To ensure consistent implementation, icon packages are versioned and carefully managed. The individuals responsible for One Cerner Style Icons are: James Bray, Seth Claybrook, Brett Jankord, Will Reynolds and Bradley Scott. https://github.com/cerner/one-cerner-style-icons", "date": "2017-04-06"},
{"website": "Cerner", "title": "Influyendo en una generación de jovencitas de habla hispana", "author": ["Lindsay Ullyot"], "link": "https://engineering.cerner.com/blog/influyendo-en-una-generacion-de-jovencitas-de-habla-hispana/", "abstract": "Cuando no están trabajando, Denisse Osorio de Large, Directora de Cerner en Desarrollo de Salud Poblacional y Steven Large, Director sénior de Desarrollo de IP, quienes están casados, tienen como pasión crear una generación joven motivada por la tecnología y mostrarle a la misma el amplio mundo de oportunidades existentes en el desarrollo de software. Además de dar su tiempo como voluntarios en la comunidad promotora del área de ciencias, tecnología, ingeniería y matemáticas (STEM), contribuyen a organizar la conferencia de desarrollo de Cerner (DevCon) y realizan presentaciones en conferencias, Denisse y Steven disfrutan trabajar con niños de la comunidad de habla hispana. El año pasado, Denisse, originaria de Colombia y cuya lengua materna es el español, y Steven, quién habla español, organizaron dos eventos de codificación ofrecidos específicamente a familias de habla hispana en colaboración con la agrupación Girl Scouts , niñas exploradoras. Los objetivos de estos eventos de “Pastelitos y Programación”, Coding and Cupcakes , consistieron en ofrecer un entorno seguro e incluyente para que las chicas llevaran a cabo actividades de programación, desarrollaran capacidades de liderazgo y un gusto por la resolución de problemas mediante la codificación. Las jóvenes disfrutaron la satisfacción que brinda resolver los desafíos de codificación de code.org y de juegos como Laberintos de Angry Birds . En el juego, usando conceptos de programación, las chicas se divirtieron al encontrar la forma de guiar a las aves que, atravesando un laberinto, debían llegar a los cerdos (ver video del evento a continuación). Como reconocimiento por su pasión por desarrollar a futuros innovadores en STEM, las Girl Scouts , honraron a Denisse y a Steven Large por conducir el evento Pastelitos y Programación, Coding and Cupcakes que se llevó a cabo en Cerner. El Premio de Innovador de las Girl Scouts , reconoce a los voluntarios que perciben un problema y desarrollan una solución innovadora para fomentar las actividades de escultismo para las chicas, Girl Scouting . Estos eventos familiares contribuyen a favorecer la comunicación entre padres e hijas al participar en eventos de Girl Scouts . Por su pasión y dedicación como voluntarios, Denisse y Steven alientan a cada vez más niñas a participar en las actividades de STEM. ¡Enhorabuena, Denisse y Steven!", "date": "2017-05-11"},
{"website": "Cerner", "title": "Influencing the Young, Spanish-speaking Female Generation", "author": ["Lindsay Ullyot"], "link": "https://engineering.cerner.com/blog/influencing-the-young-spanish-seaking-female-generation/", "abstract": "Outside of work, married couple Denisse Osorio de Large, a Cerner Director in Population Health Development, and Steven Large, a Cerner Senior Director in IP Development, are passionate about creating a younger generation that is passionate about technology and introducing them to the vast world of opportunities in software development. In addition to volunteering in the STEM community, helping organize Cerner’s development conference (DevCon), and speaking at conferences, Denisse and Steven enjoy working with children in the spanish-speaking community. Last year, Denisse (who is Colombian and a spanish-speaking native) and Steven (who is fluent in spanish) hosted two coding events specifically catered to spanish speaking families in partnership with Girl Scouts. The goals of these events, “Pastelitos y Programación” (“Coding and Cupcakes”), were to provide a safe and inclusive environment for girls to program, grow leadership abilities, and love solving problems with code. Girls enjoyed completing coding challenges from code.org and playing Labyrinths for Angry Birds. In the game, the girls had fun figuring out how to guide birds through a labyrinth to the pigs (see more about the event in the video below) using programming concepts. In recognition of their passion for developing future innovators in STEM, Girl Scouts honored Denisse and Steven Large for their work in hosting the Pastelitos y Programación (Coding and Cupcakes) at Cerner. The Girl Scouts Innovator Award recognizes volunteers who saw a problem and developed an innovative solution to further Girl Scouting. These family events help bridge the communication gap between parents and girls when it comes to participating in Girl Scout events. With their passion and dedication for volunteering in the community, Denisse and Steven are encouraging more girls to become involved with STEM activities. Congratulations, Denisse and Steven!", "date": "2017-05-11"},
{"website": "Cerner", "title": "ShipIt VIII Day: Spring 2017", "author": ["Isabella Kuzava", "Shahzad Zafar"], "link": "https://engineering.cerner.com/blog/shipit-day-viii-spring-2017/", "abstract": "ShipIt Day VIII was held on April 20 and 21st. Seventeen teams made up of internal associates had just 24 hours to create something innovative, usable, and value-adding. This quarter’s ShipIt Day was held at our new Innovations Campus in the Link Conference Center. We were excited to get to utilize the space for this event. Another awesome addition to this quarter’s ShipIt Day was our new game rooms. We hosted a Donkey Kong challenge and the winner ( @russ_starr ) received a five-pound gummy bear! The start of a long day for participants! Donkey Kong. Participants were given loads of snacks, the most important being Monster Energy Drinks! After a long day of hard work, everyone was ready to eat extra big pizzas from Pizza 51. Eleven big pizza’s and one hundred and ninety-six breadsticks were surprisingly enough to feed the hard working brains. After dinner, it was back to work for the teams. Later on in the evening, we played movies including Rogue One and MST3K. Participants also battled for the top Donkey Kong score. Watching movies while working on projects. Friday, April 21st rolled around and by 9:00am the teams were getting antsy to present their projects. Since this was our first ShipIt Day at the new Innovations Campus, we were able to utilize the Assembly space. Old school advertising for ShipIt Day Demo’s- chalk! Prize table ShipIt Day VIII brought to life some incredible projects. Team Freudian Ship, made up of Kate Young, Erin Dorpinghaus, Courtney Ground, Chris Farr and Sean Albery, took first place (and the golden keyboard!) with their project. The Intellectual Property (IP) Analytics team is tasked with managing a data warehouse. This includes ensuring that data updates take place in “jobs.” When a job fails it results in stale data and this can impact thousands of associates. This project took advantage of Amazon Web Services by connecting tasks associated with managing the database to a voice command Alexa skill which results in the ability to manage the database from anywhere the user has an internet connection. To put it in perspective, the database admin will now be able to ensure that the database is up to date without even getting out of bed in the morning. A simple voice command to an Amazon Echo Dot (which each of us won as a prize) will result in the restarting of failed jobs and even logging JIRA tickets to begin the troubleshooting process if needed. ShipIt Judge Dan Stocksick was impressed with the project, “Great use of consumer products into our ecosystem.” Team Freudian Ship with the coveted Gold Keyboard! Second place went to team Flowalva (Wuchen Wang and Dhruv Patel). Our ShipIt project was a traffic visualization system for web services supporting Millennium. In this project we used Splunk API to collect the data from services and visualize the traffic between nodes in using Netflix’s vizceral project. Judge Jason Heiting was impressed. “Very neat visualizations.  Intuitive UI for navigation.” Last but not least, was third place winners, Flying Mongoose (Cihan Kaynak, Matt Stramel, Mahesh Acharya, Taylor Clay). We created a cross platform mobile app which can be used by a pharmacy technician while picking up and scanning medications at the pharmacy to refill Cerner’s RxStation cabinets. Prior to this project, an RCP based desktop app was running in a PC mounted on a cart with an attached barcode scanner and label printer. This type of bulky setup was reducing the mobility of the technician. The Assembly was crowded with associates watching presentations. Part of the ShipIt Day fun is livestreaming the presentations and allowing our associates from all over to participate by voting for their favorite team name, favorite presentation and favorite project. The winners of the categories are as followed: Favorite Presentation : Sip It - Patrick Gross, Adam Tibbs, Mario Perez, Robert Cornett, Trey Peek Favorite Team Name : CHAPPiE - Karthik Nimmagadda, Manidee Gattamaneni, Venkatesh Sridharan, Prasanth Chakka, Wendell Wolfe Favorite Project : Freudian Ship - Kate Young, Erin Dorpinghaus, Courtney Ground, Chris Farr, Sean Albery People’s Choice Winners with the coveted cheeseballs! ShipIt continues to be a great way for associates to work on something they haven’t had a chance to work on, build relationships within different organizations and most importantly, enjoy themselves. Ian Kottman, an engineer on the Population Health Genomics team, has participated in multiple ShipIt Days. The reason: “I enjoy getting the time to create something that will make someone’s life easier.” Carlo Filipelli, system engineer, also agrees, adding on, “ShipIt is great because it allows associates to work on interesting projects that they might not be able to get to during normal work hours. It fosters innovation and new ideas that would typically get overlooked in our busy day to day jobs. It is also a conducive environment for learning, collaboration, and having some fun!” Thank you to all our participants and a special thank you to judges Jason Heiting, Justin Morrison, Dwight Sloan and Dan Stocksick. Mark your calendars: ShipIt Day IX will be held in July 2017. If you are interested in learning more about other ShipIt Days, see these earlier posts:", "date": "2017-05-17"},
{"website": "Cerner", "title": "You May Not Be a Polyglot, but Your Code Needs to Be", "author": ["Denisse Osorio de Large"], "link": "https://engineering.cerner.com/blog/you-may-not-be-a-polyglot-but-your-code-needs-to-be/", "abstract": "Last October, I had the privilege to speak at Grace Hopper , the world’s largest gathering of women technologists. It was impressive to see 15,000 fellow female technologists gather together to share their experiences and technical expertise. To look into the audience, and see a room full of other female engineers was a great experience! My talk was titled, You May Not Be a Polyglot, but Your Code Needs to Be ( previously presented at Midwest.io ).\nOne definition of the term polyglot is “composed in many languages.” During my talk, I discussed common misconceptions about writing code for global audiences, and I provided tips to help position engineers for success in these types of projects. Having worked in engineering, translation, and localization for more than over a decade, I have seen first-hand how projects struggle through the globalization process. A properly globalized solution is generally achieved by ensuring the appropriate internationalization, localization, and translation steps have taken place; however, as developers, we have misconceptions that make this process more costly and less effective than it needs to be. To help set the stage, I began my talk by speaking in Spanish. I love watching the audience react to my comments, as I usually say something silly during the introduction, so those who understand react one way, while others in the audience tend to look puzzled. One of the most common misconceptions when coding for a global audience is the infamous statement: “I’ll NEVER need to do this.” The most common rationalization for this misconception is that in the U.S., we only speak English, however, one in five U.S. residents speaks a foreign language at home. This is a record 61.8 million residents who speak languages other than English. Of these residents, 25.1 million (41 percent) told the Census Bureau that they speak English less than very well. Additionally, data from 2013 shows that there are seven languages with at least one million speakers in the US. These languages were Spanish (38.4 million), Chinese (three million), Tagalog (1.6 million), Vietnamese (1.4 million), French (1.3 million), and Korean and Arabic (1.1 million each). By only serving one audience, we are ignoring many customers and consumers of our solutions.  Even in my adopted hometown of Kansas City, 6% of the population speaks Spanish as their primary language. Another common misconception is that internationalization and  localization are the same. Here are some definitions according to the Word Wide Web Consortium (W3): I like to think of their relationship more like this: i18N + L10N == G11N While the W3 notes, “some people use other terms, such as globalization to refer to the same concept” as internationalization,\" I see the appropriate execution of internationalization and localization together as providing a globalized solution. Among examples of appropriately globalized solutions, TripAdvisor is one I like to highlight because it has a broad user base, and many of its users don’t realize it is used by others around the world in their own language. The following Trip Advisor elements help provide a positive user experience: Another common misconception when creating globalized applications is that once something has been internationalized and translated, it can be shipped. The best way to highlight the folly of this perception is to look at some marketing mishaps. Below are some of my favorite ones: Translation alone is not a solution. The translator must be proficient. In a previous position I was frequently asked how long it takes to learn a language. I was fighting the misconception that anyone who has taken two years of a language could translate. After failing to articulate why that was a misconception, I found a wonderful tool, thanks to the Foreign Language Institute and the Interagency Language Roundtable (ILR), who created a scale that is a set of descriptions of abilities to communicate in a language. One important aspect of this scale is that it is accurate when the evaluations are done by native speakers. It was originally developed by the United States Foreign Service Institute (FSI), and is still widely known as the FSI scale. It consists of descriptions of five levels of language proficiency. Then the scale estimates how long it takes to achieve that level of proficiency when a student is immersed learning the language at least six hours a day and at least five days per week. It then breaks down the languages by groups based on their difficulty: “Easy” languages are French, German, Indonesian, Italian, Portuguese, Romanian, Spanish, and Swahili: “Hard” languages are divided into the following groups. To learn these languages, you must spend the following time: Taking a language for two years (usually college or high-school) is simply not enough to achieve the appropriate level of proficiency needed to translate. Even native speakers are not effective at translation. Becoming proficient in a language is very challenging, and cultural aspects are important as well. Additionally, brain activation patterns identified by the University of Turku in Finland show extensive activation of the brain during non-native language use, specifically, the left dorsolateral frontal cortex, which is associated with lexical search, semantic processing and verbal working memory. Brain activation patterns were clearly modulated by direction of translation, with more extensive activation during translation into the non-native language, which is often considered to a be more demanding task. These are not the same patterns used when speaking a language, though, as different brain connections need to exist. Machine translation has become more effective in the last few years, but only language experts should leverage it as part of their toolset. Translating something into a language you don’t understand is a recipe for disaster. Simply go to an online translation tool, put in a phrase, convert it to another language, then convert it back.  Chances are, the original does not match the twice-translated phrase. If you are relying on professional translators, another common misconception is that translation can simply happen overnight. The truth is it CAN happen, just NOT well. A professional translator can usually translate about 2000 words per-day. If they are doing localization testing as well, you have to budget about twice the time for the regression test plans to be completed, as words will iteratively need to be changed as context is understood within the software. Now that I have covered the most common misconceptions with internationalization, localization, and translation, the next portion of this blog will focus on practical tips. As a developer, it is imperative that you understand your global strategy and what outcome the business is trying to accomplish. You don’t want to invest a lot of development time making sure you can handle Right-To-Left languages or those that are double-byte if your business isn’t planning on going to those markets. Creating appropriate partnerships with language providers is key to successfully globalizing software, as they can help handle many aspects of the globalization process from translation to localization. Speaking to companies who are already globalizing their software is a good way to find good providers. If you have to use free online tools to translate something internally (where the quality doesn’t need to be high), make sure you understand the licensing of the tool. Some free tools will own your content once you run it through their translation engine. Including internationalization as part of your development process is a key aspect to globalization success. This means it should be included in: Localization testing is a new step that should be included in your process, which requires language experts to be involved. From a development perspective, there are a lot of existing artifacts that can be leveraged in this process, such as regression and feature test plans. It is always helpful if those artifacts can be separated into those that have UI changes or not. Localization testing will likely focus on language changes and impacts to the UI. A byproduct of localization testing is that other types of defects might surface. Having a good process so that bugs can be reported and tracked is key. If you are just starting down the road of internationalization, be sure you don’t reinvent the wheel on how to do it. Programming languages usually have well-documented libraries that can be used. If you are already internationalizing and working with a localization team, make sure you are commenting your code to provide clarification to the translators. Below is an example of a popular localization tool: Once you have incorporated all the steps for localization into your process, it is important to understand that your text changes will now have financial impact as they will affect translations. Translation memories are commonly used by translators, and any change to the source text will trigger a new translation to be required. Translation companies typically charge per word. The average cost per word in 2012 Common Sense Advisory’s 2012 survey of more than 3,700 suppliers in 114 countries, using 220 language pairs, found that the average per-word rate for translation for the 30 most commonly used languages on the web is around 13.4 cents. The more impactful implication to your process is that text changes will trigger your localization testing cycle as it will be important to verify the new translations are correct in context. Let’s discuss some quick principles that you should be able to apply regardless of what programming language you use. Don’t concatenate strings. Language order matters with translation. Instead of doing this: Do something like this: Refraining from concatenating strings allows lthe localization professional to ensure that grammar is appropriately represented in the language. If you concatenate strings you will be forcing someone to say something like: “House Red” instead of “Red House.” Speaking of names, be mindful how you use them in an application. Build appropriate abstraction and flexibility. Names can cause headaches in localization because first names aren’t always first, people can have multiple last names, and middle names might not be there at all. I have a personal story about this. When I went to file my marriage license, we had a very interesting conversation with the clerk. He asked me what was my middle name. When I said that I don’t have a middle name, he said the system had it as a required field. I suggested that he enter a space…after about 15 tense seconds he confirmed it had worked. This was in early 2000, and I hope things have changed, but since government moves at the speed of snails, I doubt it has. My final tip to better handle localization is to always use the FULL locale, that is, both language and country code to allow for maximum flexibility and to budget more time than what you believe you will need for your project. This last tip sounds very lame, but experience has taught me that chances are you won’t do it. Engineers tend to be very optimistic on their estimates, and generally fail to allocate sufficient time.", "date": "2017-07-07"},
{"website": "Cerner", "title": "Managing Splunk's Knowledge", "author": ["Bryan Baugher"], "link": "https://engineering.cerner.com/blog/managing-splunk-knowledge/", "abstract": "When we first were given access to Splunk we were excited about all the functionality it could provide our team to help us monitor and debug our applications. We created alerts to email us if our applications are logging errors, dashboards to show health or metrics of our services, and field extractions as well as tags to make searching easier. As we created more and more of these Splunk knowledge objects we started to have issues. For regulatory and validation concerns, we have several Splunk instances that represent different environments. This required us to copy and paste or duplicate the Splunk knowledge object we created in one environment to all the others. This became very tedious and we saw all sorts of problems. Objects were only copied to some Splunk instances, updates were sometimes lost, and permissions weren’t consistent or someone would forget to make their object publically viewable. We set out to look for a solution. After we couldn’t find anything that could easily help us we created a tool called splunk-pickaxe . splunk-pickaxe is a Ruby gem that provides a command-line interface for synchronizing a directory or repository of Splunk knowledge objects written as files to a Splunk instance. So now when a developer creates a new Splunk object, they would create and test it in our Splunk dev instance and then open a pull request to our Splunk Github repository with the object’s configuration or contents . Other developers review the changes before it gets merged and automatically pushed out to all our Splunk instances using our splunk-pickaxe tool and Jenkins our continuous integration engine. This simplified things and provided consistency on these Splunk changes. We were also able to more effectively review the Splunk objects for efficient and proper Splunk usage. The tool grew in popularity at Cerner and currently supports many Splunk objects: Check out the Getting Started section to try it out and feel welcome to contribute any ideas or improvements.", "date": "2017-07-14"},
{"website": "Cerner", "title": "Structure Matters: How Cerner Handled Too Much Health Data in HBase", "author": ["Michelle Brush", "Vinay Koduri"], "link": "https://engineering.cerner.com/blog/structure-matters-how-cerner-handled-too-much-health-data-in-hbase/", "abstract": "In order to manage the health of populations, Cerner builds data-driven solutions built on top of its HealtheIntent ℠ platform. The platform transforms and standardizes data across disparate health systems. The data is then used in algorithms that aid healthcare providers and hospitals in managing and monitoring health both at a person and population level. The Reference Record is one of the core components of the HealtheIntent platform. It represents the data known about a person’s health from a particular source of healthcare information. Downstream components in the platform aggregate information across multiple reference records to understand the full picture of a person’s health. To create the record, the system performs both batch and stream processing. Some sources send the data to HealtheIntent once a day or once a month. That data is processed in batch, using Hadoop MapReduce jobs. For sources that send the data as streams of updates in near real time, we rely on a homegrown near real time framework that sits on top of Apache Storm. Both the pipelines persist their processed data in HBase tables. For the data processed in batch, we write all the data about the person from a given source to the HBase table as one HBase row. We refer to this as a wide table. The HBase Rowkey is the source identifier followed by the person identifier: /source:<id>/person:<id> . Each row could have multiple columns, one per each healthcare entity like allergy, medication, procedure, etc. Another thing to note is this table is salted for better distribution of data across regions. That means a given person’s data will be in a single region since it’s all in one row, but the data for a given source will be distributed across the HBase cluster. For streaming sources, we use a different table approach. We have a tall table, where each healthcare entity is written separately. The Rowkey in this approach would be built from the source identifier, person identifier, entity type, and then individual entity identifier. This allows the system to manage the information flowing from the stream more efficiently. To make sure that the consumers of the record don’t have to know whether the data is coming from a streaming or batch source system, we offer a Crunch-based extract API that is capable of reading from either table structure. The API provides a single, consistent representation of the data. The purpose of this post is to discuss what happened when a single person in our system received a little too much healthcare over the course of her fake life. We were bringing a new source of healthcare data onto the HealtheIntent platform using our batch processing pipeline. In the midst of this work, a small number of MapReduce jobs started failing. Our failure monitoring triggered and notified us that something wasn’t right. We turned to the logs to investigate the failures. We saw this (not very informative) message: Task attempt_xxxx failed to report status for 601 seconds. Killing! Our first assumption was that the issue was related to memory, but we profiled the job and found that the heap utilization looked fine. Querying across all the logs, we were able to see two key pieces of information. One, a job was more likely to fail if it was reading from a particular source of data, and two, failures were correlated with alerts on slow response times in HBase. Drilling deeper into our logs, we were able to find that sure enough, we were timing out trying to read from a particular region in HBase. The next obvious step was to look at region metrics. We found the region had grown in size to 115 GB on a cluster configured to have a max region size of 10 GB. More logs showed the table had stopped compacting and splits weren’t working due to a 30 GB row in the table. At this time, anything trying to read that row was failing, but some large writes were still succeeding. There would be attempts to bulk load new data to the region, but the loads would invariably fail due to the large row. When the job ran again, it would attempt to write the same data, over and over. Occasionally during these attempts, some of the data would get written to the region. This process piled more and more data onto the region, and the region wouldn’t split. As the region was growing, the situation got worse. As previously mentioned, we salt the row keys to distribute data across the cluster. Jobs interested in other source data were also reading from this region, so we started seeing failures in other jobs. Then in the midst of our investigation, we saw more HBase region servers start failing. Then HBase bulk loads in other MapReduce jobs were also failing. We knew we had to get rid of the row and split the region. As some team members worked on a plan to deal with the row, others moved on to answering the question, “How did this one row get so big that the bulk loads started failing?” With healthcare data, it’s possible to imagine diseases that might lead someone to have so many encounters and activity within a single healthcare system that we could get to a gigabyte of health data about a person. However, we had never seen anything near this large before. It was a pretty significant outlier. We decided to delete the person’s data. However, the person had grown so large, this proved difficult. (The naive approach to deleting a row in HBase failed spectacularly.) Splitting was also not possible. We needed a better plan. First, all reads and writes to the region had to stopped. This meant suspending all jobs that any potential to read or write. Since several teams depended on our data for their processing, we started by notifying all our downstream consumers. Then we stopped all reads and writes to the affected region. We copied the affected Hfiles to a new table. This allowed compaction (and as a result, deletion) to succeed. Cleaning up this one bad record brought the region size down to 2 GB from 200 GB. As part of this process, the table was disabled and the compacted Hfiles from the fixed region were copied back to the old table before it was re-enabled. When we stopped all the jobs loading into that table, we had a stale data period for downstream processing. Reports that needed the data showed old information during the length of the incident investigation and mitigation. The good news is that our near real-time algorithms such as our Sepsis alerts and readmissions risk assessments were built on top of the stream processing system which was not impacted by this incident. In our postmortem for the incident, we reviewed what we could have done better before, during, and immediately following the situation. A quick win for us was to add HBase region size monitoring & alerting. It is still reactive, but would help us react faster and minimize the duration of the event. Ultimately, the root of the problem is that we have an approach that encourages a lot of data to accumulate in a single row. We had a design flaw. While this patient was outside the realm of anything we’d seen before, it wasn’t outside the realm of possibility. To quote an oldie, but goodie from Frederick Brooks, “Our first response should be to reorganize the modules' data structures.” This event made us realize our batch and streaming representations of the data need to converge. That is the path we’re now taking which will introduce its own set of challenges, but we’re confident this will land us in a more stable, scalable place.", "date": "2017-08-28"},
{"website": "Cerner", "title": "Continuous Delivery for DC/OS with Spinnaker", "author": ["Will Gorman"], "link": "https://engineering.cerner.com/blog/continuous-delivery-for-dcos-with-spinnaker/", "abstract": "Last fall our team ( Mike Tweten , Trevin Teacutter , and Zameer Sura ) started working on the problem of automating DC/OS deployments in a way that wouldn’t require multiple teams to duplicate effort and tie themselves directly to DC/OS APIs.  While DC/OS certainly makes the act of deploying an application much easier than anything we’ve used previously, there are still many different ways you could choose to layer on deployment strategies and integrate with continuous delivery systems. Additionally, there may be lots of teams with very similar needs in this space and there are certainly more efficient uses of their time than making them all solve the same problems.  Furthermore, while DC/OS is a good choice today we need to make sure that we don’t become locked in and can change our mind in the future without a big impact to those teams. Being an engineering team, naturally our first approach was to try to write a service to manage a simple set of deployment strategies with an API that we could reimplement over other resource schedulers (like Kubernetes ) in the future.  However, after producing a prototype we really started to grasp that we were taking on a much bigger task than we should.  Our API could handle some basic pre-built deployment workflows, but if it wasn’t flexible enough for teams we’d need to continue to adapt it.  In addition, we would still have to work out how it would integrate with CI/CD tools like Jenkins , and we’d have to do a lot of that work again each time we wanted to support another resource scheduler as a deployment target. At this point we decided to take a deeper look at Spinnaker , a continuous delivery platform that was open-sourced by Netflix.  It had originally been developed to orchestrate AWS deployments and had expanded to other providers like Azure, OpenStack, and more recently Kubernetes was added as the first container based deployment target.\nSpinnaker allows for building deployment pipelines that can be triggered by things like Github commits or Jenkins builds and can then run stages to do things like start other Jenkins jobs, deploy new versions of an application, scale up or down applications, and disable old application versions.  With these capabilities we would be able to use Spinnaker to provide a consistent deployment abstraction while still providing the flexibility to handle deployment workflows that we didn’t build ourselves. The only minor problem was that it didn’t support DC/OS, our preferred deployment target. After taking some time to dive in and get familiar with the details it quickly became apparent that adding support for DC/OS would be much easier than continuing to build our own system, especially since we had already become familiar with the DC/OS APIs while developing our prototype.  Not only would we then gain the flexibility to allow more deployment strategies than we originally anticipated, but it would also buy us integration with Jenkins and the ability to convert pipelines over to Kubernetes without much effort.  We checked with the Spinnaker community to make sure that no one was already working on DC/OS support and also with Mesosphere (the company behind DC/OS) to make sure they weren’t planning to do it.  When both of those came back negative we began working on the project in earnest, with the goal of building something that would benefit not only Cerner but also the entire Spinnaker and DC/OS communities. This was the first time that any of us had tried to contribute significant changes to an established open-source project so there was some adjustment and learning along the way.  We initially created our fork of the necessary repositories to our internal Github with the plan to finish the project before making anything visible publicly. Instead, we found that we wanted to more easily be able to share things with Mesosphere since they had agreed to give us any guidance we might need.  It also became apparent that it was too easy to commit things like config that referenced internal resources, or comments that referred to our own JIRA issues.  If we kept going that way we were only going to make more work for ourselves cleaning those things up when it came time to submit our changes.  By moving our development to repos in Cerner’s Github organization we were able to solve both of these problems at once. In spite of moving our code into the public Github we still maintained an insular development approach, determined to get everything just right before submitting a perfect polished gem to the upstream project.  In hindsight, we should have been less concerned about trying to get everything right all at once and submitted more incremental changes.  We thought it would be easier for everyone not to have to deal with our messy work in progress, but after our first pull request for one of the services ended up with over 130 files and a history of 100+ commits we realized that it’s too much to expect the maintainers to review so much at once, especially when they may not be that familiar with DC/OS.  Ultimately, they wanted us to carve up that massive patch anyway so it would have been less work for us to do it from the start. After completing this effort we’ve found that we get even more benefit from Spinnaker than we initially expected.  Spinnaker also helps us manage deployments to multiple DC/OS clusters, use Chaos Monkey to test the resiliency of our applications, and even to deploy DC/OS itself on AWS. We’re pleased to announce that our contribution has been accepted by the Spinnaker maintainers and is now available for anyone to use .  The Spinnaker community has been great to work with and helped us by answering questions and inviting us to participate in the discussions with other companies that have contributed major functionality to Spinnaker.  This is an exciting opportunity to contribute something back to the open-source community and we can’t wait to see where it goes from here.", "date": "2017-09-08"},
{"website": "Cerner", "title": "ShipIt IX Day: Summer 2017", "author": ["Isabella Kuzava", "Ian Kottman"], "link": "https://engineering.cerner.com/blog/shipit-ix-day-summer-2017/", "abstract": "There is no better way to end summer than with ShipIt Day! We kicked off our ninth ShipIt Day on August 10th, with more than 75 Cerner associates signed up. Nineteen different teams worked for 24 hours straight to complete their projects. For those unfamiliar with the event, ShipIt Day is a 24 hour hackathon for internal associates to create something innovative, usable, and value-adding. This was the second ShipIt held at our new Innovations Campus . Associates love using our new campus for ShipIt because of the game rooms and collaboration spaces! One new addition to this quarter’s ShipIt Day was the scavenger hunt. Participants were given a list of crazy tasks to accomplish within the 24 hours. Some tasks included spoon feeding a fellow team member, completing the obstacle courses around the outdoor walking trail, and finding binary codes around campus and decoding them. Shoutout to the winners of the scavenger hunt: the Jumpy Woodchucks team! Teams were given lots of snacks and drinks to keep them fueled for the next 24 hours. Collaboration is key to a successful project. For dinner, we brought in some of the biggest pizzas Kansas City has to offer from Pizza 51. By dinner time the participants were ready to chow down and give their brains a break. Dinner entertainment included School of Rock and Futurama . Chaos ensued as everyone runs to grab some pizza! School of Rock was played during dinner. Associates stayed overnight and worked around the clock to ensure their projects would be complete by presentation time. However, sometimes you just need a nap. Presentation day is always an exciting one. After 24 hours of hard work, the teams have 4 minutes to show off their product to their peers and a panel of rock-star judges. A special thank you to Brian Wallace, Amon Woolridge, and Greg Whitsitt for judging this round. We always start presentation day off with bagels for breakfast from Panera. The Prize Table. ShipIt Day IX teams brought their A game. The projects get more intricate and more impressive each time around. ShipIt Day is a great way to show off the talent we have at Cerner. Team Bravo Avocado, made up of Thomas Clay, Jacob Zimmerman, Jan Monterrubio, Max Schroeder, and Elisha Boozer took first place! Judges were impressed by their live demo and humorous presentation style. Our project was an enhancement for Millennium Java services that removes certain required pieces of server implementation by adding custom annotations. Consumers will no longer have the same restrictions from traditional service generation (i.e. defining a generation file, method naming), because the annotation processor will generate that for them. Also, the framework can bind the structured request/reply objects to an annotated object of their choice. These annotations remove lots of redundant code and gives consumers more time to focus on what matters. Team Bravo Avocado with the Golden Keyboard. Second place went to team E=MC Hammer, made up of Kyle Lipke, Nimesh Subramanian, James Boyd, Mahesh Sampekatte, and Megan McConnell. For ShipIt IX, our team E=MC Hammer built a tool that looks up and publishes Millennium OAuth keys. This tool was created to be an easy-to-use, easy-to-search, and updateable source of truth for all those that request and use Millennium OAuth keys. We set up an api that hosts only the data we wish to present, in a read-only “MAP” so users understand the structure of the data. We had a blast on the project and would recommend attending a ShipIt day yourself! Judge Greg Whitsitt noted that their project was: …applicable across teams, solves a problem many face. A great idea and great execution. Team E=MC Hammer. Team Jumpy Woodchucks, made up of Hans Perera, Nathaniel Owen, Ann Dickey, James Thomas, and Pramoth Murali took third place. Their project was Chamberlain: Light notification system for all your notification needs that decorates your desk. This system currently supports alerts for Microsoft Outlook, Crucible, and Millennium domain health. Designed to be highly customizable and a project for teams to learn about electronics and use of Makerspace. Judges commented that this project was what the heart of ShipIt Day is all about. Team Jumpy Woodchucks. We had a packed house for ShipIt Day presentations. Associates could watch live in the Assembly or catch it on the livestream website.  One of the best parts of ShipIt Day is letting the audience join in on the fun and vote for their favorite team name, best presentation, and favorite project. Congratulations to the following teams… The Assembly during presentations. Each ShipIt Day, we see a handful of ShipIt “veterans.” It is always interesting to hear our associates take on the 24 hour event. Ian Kottman, a software engineer on the Population Health Genomics team, has a lot to say about his experience participating in ShipIt Day. I recently got to take part in one of my favorite events at Cerner: ShipIt Day. This was my fourth ShipIt and I was excited about the project my team had come up with. My team had named ourselves 🌝, which is one of the team member’s favorite emoji. We planned to add a UI to a tool we created last ShipIt to manage access to Cerner’s Makerspace. Our primary goal this time around was to have fun and win the coveted cheeseballs. In the short 24 hours we had our team went through all of the stages of a typical software project: design, implementation, realization we overestimated, shedding unneeded features, and finally getting something working. Partway through the day we ran into issues getting Chrome to run in a Docker container and there was a brief incident involving internationalization breaking our error page, but after many whiteboard sessions and late night sodas we completed a working product. We even managed to have time for a few hours of sleep before the presentations. When it was all said and done we had accomplished our goal: we won cheeseballs for best team name! All in all, we had an excellent time collaborating and developing a tool that will be used for a long time to come. I’m already thinking about what to work on next ShipIt… ShipIt continues to be a great way for associates to work on something they haven’t had a chance to work on, build relationships within different organizations, and most importantly, enjoy themselves. ShipIt Day IX was a great success. The projects were creative and have potential to be built upon and used in Cerner’s everyday work flow. We are already looking forward to our next ShipIt Day in December! If you are interested in learning more about other ShipIt Days, see these earlier posts:", "date": "2017-09-11"},
{"website": "Cerner", "title": "Engineers Overseas for SMART and FHIR (FHIR DevDays 2016)", "author": ["Dennis Patterson"], "link": "https://engineering.cerner.com/blog/engineers-overseas-for-smart-and-fhir/", "abstract": "In a previous post , we discussed the need to provide education around our ecosystem and implementation of the FHIRⓇ and SMARTⓇ standards for developers and how we did so at our annual Cerner Health Conference. That same week, we were also a part of FHIR DevDays , which included a track entitled “Apps in the EHR.” We worked alongside track participants who were seeking to integrate their apps in a variety of EHRs, including Cerner. FHIR DevDays is a conference put on by Furore in Amsterdam. This year, over 250 participants from all over the world gathered to attend tutorials given by FHIR community leaders in the mornings and participate in hands-on sessions in the afternoons. Cerner co-led the “Apps in the EHR” track. This track was comprised of roughly 20 people from across France, Germany, Ireland, Israel, Italy, Netherlands, Norway, Spain, Switzerland, the United Kingdom, and the United States. Josh Mandel kicked things off with a tutorial on SMART and how to get started building cross-vendor applications. Afterward, we met with track participants to show them how to get plugged into Cerner’s sandbox and ecosystem. We started out by covering our documentation launchpoint , registering applications in our code console , and going over our technical documentation . For those who didn’t come to the conference with an existing application to work on, we provided a tutorial . With those tools in hand, they were ready to get to work! Over the course of the three afternoons, we assisted people as they used SMART and FHIR for the first time, but also people with experience in FHIR who were looking to get their technical and business questions answered about integrating with Cerner.\nSeveral of the projects were plugged into multiple sandboxes. The developers shared that once their app was working in one sandbox, it was easy to plug into others, which is exactly how it should be! When we gave our closing presentation, we actually had some technical difficulties loading up the slides. Little did we know that Grahame Grieve , the original architect of FHIR, snapped a picture and would use it shortly in his closing keynote on FHIR’s future. With this image on screen , he stated: “[to us:] Well done. [to the rest of the audience:] Next time somebody in your acquaintance tells you that certain vendors aren’t invested in interoperability, I want you to think about this. That’s part of what the FHIR community is about, alright? Cerner and Epic on the stage together. It’s not special anymore, even. It’s great!” FHIR DevDays is Europe’s biggest FHIR conference and many attendees don’t typically make it to the tri-annual HL7 Working Group Meetings that focus on the formation and evolution of the specification. We were pleased to be a part of that gathering and to work alongside people interested in engaging with Cerner. But, we were also encouraged by the comments Grahame and others gave as they expressed appreciation that Cerner would be present and show our interest and dedication to the standard. We look forward to being back at DevDays 2017 this November to lead tracks on SMART & FHIR and CDS Hooks.", "date": "2017-09-21"},
{"website": "Cerner", "title": "Jwala Joins Cerner's Open Source", "author": ["Arvindo Kinny"], "link": "https://engineering.cerner.com/blog/jwala-joins-cerner-open-source/", "abstract": "Apache Tomcat and Apache httpd are hardened, scalable, and reliable open-source solutions. Widely adopted, these solutions are commonly used to host and load-balance Java applications. Configuring and managing a small set of elements is relatively easy, but once you start scaling out your application and platform, things get complicated. Questions such as “How do I update the heap or database pool-size across hundreds of Java Virtual Machines (JVMs)?” arise. Although commercial solutions exist to solve this and other similar problems, an open-source solution was not available - until now. Originally designed and developed to accommodate Cerner’s complex topologies, Jwala was developed to automate deploying our web applications here at Cerner. Now Cerner is donating Jwala as open source. Making contributions to development communities is an important part of Cerner’s engineering culture. We love it when we’re able to give back to the community, and we’re sure you’ll love Jwala! A Cerner platform based on Apache Tomcat and Apache httpd, Jwala enables the configuration, deployment, and management of logical groups of Tomcat JVMs load-balanced by Apache web servers. With Jwala you can configure, deploy, and manage your large-scale Tomcat and Apache httpd topology at an enterprise scale. You can create and persist definitions of Group instances that include web applications, JVMs, web servers, and other resources, and expose them through a RESTful interface, Jwala’s REST API, to perform management operations on the group. Jwala utilizes a defined file system structure and Secure Shell (SSH) agents on each machine to manage running Tomcat instances on remote servers. Jwala’s application deployment model knows how to request current status from each Tomcat and httpd instance. Jwala also is able to update each instance as changes are made to the configuration and allows maintenance operations to be executed from a central console. Last but not least, Jwala maintains an audit log of all the operations or changes performed on any of its managed resources. So, if you are someone in development, operations, or management looking for an easier way to configure and manage a large Tomcat topology, check out Jwala .", "date": "2017-09-27"},
{"website": "Cerner", "title": "Accessibility and Usability: On the Road to Great UX", "author": ["Rebecca Green", "Amber Wilks"], "link": "https://engineering.cerner.com/blog/accessibility-and-usability-on-the-road-to-great-ux/", "abstract": "In 2010, a group of designers from Columbus, Ohio set out to create a grassroots, cross-discipline, and creative conference. Since that first event, Midwest UX has been an ongoing tradition that attracts innovative ideas and provides an opportunity for regional professions to engage in discussion with other professionals. Last year, Amber Wilks and Dr. Becca Green were invited to take their talk from DevCon 2016 on the road to Midwest UX. On October 20, 2016, they presented Accessibility & Usability: The Equality of Experience in Louisville, KY. At Midwest UX, the talk covered all of the great content from DevCon enhanced by examples of how accessibility specifically affects our solutions used by our consumer market or clinical staff. Each point covered demonstrated how UX and development work together to achieve an accessible software solution and the importance of crafting equally awesome experiences for all of our users. When people talk about accessibility, there are some common myths that go along with the topic. So the talks starts off by covering three myths of accessibility that have impacted the health care community, and Cerner in particular. The first myth covered was that project teams look at accessibility as a minefield they must cross to achieve accessible software. The truth is that it is simply a blend of legislation and regulation that aims to eliminate barriers in technology for people with disabilities. In fact there are two main forms of standards used in the U.S. to define accessible content, Section 508 and W3’s Web Content Accessibility Guidelines. While they serve similar purposes, there are some very key differences. They different in terms of the technology they cover, the organization of the guidelines, and their application. However, they are just a small part of a landscape of legislation and regulation that make up accessibility. The second myth covered the assumption that accessibility is all about screen readers and alt tags. In reality, screen readers are just a small subset of assistive technologies. Low-vision or blind users only make up a small slice of all U.S. citizens with disabilities. There are a variety of assistive technologies to aid every type of disability. The third and final myth we covered is specific to the healthcare industry; that clinicians aren’t disabled so accessibility isn’t a priority for our users. The truth is that clinicians are not immune to disabilities. In fact, a study in the British Journal of General Practice that found that the proportion of male doctors with color vision deficiency was at 8%, in line with the percentage of the general population. As the guidelines for accessibility have evolved, so has the organization of these guidelines such that they can now be organized into 4 tenant principles: perceivable, operable, understandable, and robust. Information and UI components must be presented in ways users can perceive. In other words, information can’t be invisible. We do that by making information distinguishable and by providing alternative forms of content. To make information distinguishable, the foundations involve color math and text zoom. To create distinguishable information using color there are two mathematical calculations that can be used to ensure the content is both legible and discernible. To ensure the content is legible, Cerner follows WCAG’s (Web Content Accessibility Guidelines) color contrast guidelines include Guideline 1.4.3, which states that “the visual presentation of text and images of text has a contrast ration of at least 4.5:1.” To check if your information is legible, the WCAG recommends using the WebAIM Color Contrast Checker. This tool simply requires that you enter in your text, or foreground, color and background color and it will tell you to what degree you meet the contrast guidelines. The second calculation ensures users can perceive changes in the content. While this calculation is not part of accessibility guidelines, we leverage the calculation as a baseline metric for interaction indications (i.e., hovers, click states, icons, etc.). This numerical value starts at 0 and the higher the number, the greater the difference between two colors. We recommend a minimum value of 3.5, which represents an obvious difference. The other part of having distinguishable content is allowing for text zooming. In the U.S. alone, 8.1 million people have a vision disability. In March 2013, WebAIM conducted a survey of users with low vision, which found that the bulk of accessible technologies used by these individuals included some form of text resizing. Text zooming is a method used to scale text up and down while retaining the screen formatting and layout. A successful text zoom is when the content can be resized up to 200% and everything on the screen scales uniformly and scroll bars are provided if needed. The second part of providing perceivable content is to provide content that supports all the senses. A video may require a paired audio supplement. An audio file may require a written transcript. Just like the transcript we provided with our talk. While people with vision disabilities are a small slice of the population, those with hearing disabilities make up a larger portion and are more likely to be employed. So it’s far more likely that a user would need supplemental text than they would be using a screen reader. One of the most common ways to provide alternative content is through captioning. Captioning is the transcription of speech and important sound effects. Not to be confused with subtitling which is a written translation of dialogue. UI components and navigation must be operable. In other words, the UI cannot require an interaction that a user cannot perform. Most assistive technologies used by people with disabilities emulate how a keyboard functions. So a good rule of thumb is that if it can be navigated with a keyboard, then it is accessible. Operability is about giving the user control with controls to pause, stop, and hide moving, blinking, or scrolling content. Information and the operation of the user interface must be understandable. In other words, the content and operation cannot be beyond a user’s understanding. Users consume content in a variety of ways and we must be flexible to accommodate all of the different ways they might access information. Therefore, it’s important to ensure that content is housed in a structured framework to facilitate navigation. As part of the talk, an example of the experience a screen reader user might have while navigating a well known and popular website is presented. This set of videos demonstrates some of the common problems found with navigation such as duplicate links, ambiguous link names, and long navigation. The results of an automated accessibility tool are presented to highlight that not only are automated tools important for assessing accessibility but that it also requires a manual component. Content must be robust enough that it can be interpreted reliably by a wide range of user agents. In other words, user must be able to access the content as technologies advance. Since there are a lot of different types of assistive technology, as these technologies change, so does the landscape of accessibility. Being future friendly isn’t as hard as keeping up with technology. It’s actually as easy as sticking to “the basics.” The key to crafting equally compelling experiences for all is the concept of Universal Design. Universal design is the design and composition of buildings, products, and services so they can be accessed, understood, and used by all people. It’s an inclusive approach to design. When you design for the average person, you’re actually designing for no one. We are each unique and have different needs. We want to encourage you to not think of accessibility as a flat, one-size-fits-all solution but instead as a multifaceted, configurable space. It’s more than just what’s on the screen though. It’s about the environment and the users in that environment.", "date": "2017-10-09"},
{"website": "Cerner", "title": "2^5 Coding Competition 2017: 32 lines or less", "author": ["Isabella Kuzava", "Carl Chesser"], "link": "https://engineering.cerner.com/blog/2-to-the-5th-coding-competition-2017/", "abstract": "Part of Cerner’s engineering culture is to celebrate technology. This past fall, we had our annual celebration for Programmers’ Day . We celebrated the day with great food (taco bar and ice cream) and an evening of trivia. Topping off Programmers' Day with a taco bar and trivia night with @geekswhodrink pic.twitter.com/pSjqBVBHxK Happy Programmers' Day! Celebrating with an ice cream social. pic.twitter.com/bakBeOx20M With our celebrations, we also like to blend in challenges or activities that can mold desired behaviors. Therefore, as part of Programmers' Day, we also kicked off a programming competition (our third year of organizing the competition). For 32 days (September 13 - October 16), associates submitted small snippets of code (32 lines or less) in any language that represented the concept of engineering impact. After 32 days, our panel of judges sorted through the code and assigned a winner to each of the following categories: As with anywhere, engineers at Cerner are busy. Asking them to do something additional, like a coding competition, imposes challenges on participation based on their availability of time. However, we found that by organizing the competition for engineers to explore and share what they are working on, in small and consumable bites of code, helped in keeping it a simple time investment and gained participation. Furthermore, it helped reinforce a routine behavior to continually look and evaluate different technologies and languages. Just like tweets, by having small code examples being shared, it was easy for others to also learn what they were doing. We facilitated sharing these tweets by making a dashboard which served up crawled results from GitHub. This was achieved by having participants including a specific phase in their code (like a hashtag), which could then be pulled from GitHub’s search and built into a dashboard. This dashboard was based on how we originally achieved this in our earlier 30 Days of Code competition . Our winners from this year are as follows: Congratulations to the winners and thank you to everyone who submitted code. Until next year!", "date": "2017-12-19"},
{"website": "Cerner", "title": "Composable MapReduce with Hadoop and Crunch", "author": ["Ryan Brush"], "link": "https://engineering.cerner.com/blog/composable-mapreduce-with-hadoop-and-crunch/", "abstract": "Most developers know this pattern well: we design a set of schemas to represent our data, and then work with that data via a query language. This works great in most cases, but becomes a challenge as data sets grow to an arbitrary size and complexity. Data sets can become too large to query and update with conventional means. These challenges often arise with Hadoop, simply because Hadoop is a popular tool to tackle such data sets. It’s tempting to apply our familiar patterns: design a data model in Hadoop and query it. Unfortunately, this breaks down for a couple of reasons: So how do we approach this? Let’s look to an aphorism familiar to long-term users of Hadoop: Start with the questions to be answered, then model the data to answer them. Related sets of applications and services tend to ask related questions. Applications doing interactive search queries against a medical record can use one data model, but detecting candidates for health management programs may need another. Both cases must have completely faithful representations of the original data. Another challenge is leveraging common processing logic between these representations: there may be initial steps of data normalization and cleaning that are common to all needs, and other steps that are useful for some cases. One strategy is for each shared piece of logic to write output to its own data store, which can then be picked up by another job. Oversimplifying, it may look like this: Such a model can be coordinated with Hadoop-based tools like Oozie . But this model of persisting every processing stage and using them downstream has some drawbacks: So how do we solve this? Rather than making intermediate data stores as the point of reuse, let’s reason about the system at a higher level: make abstract, distributed data collections our point of reuse for processing. A data collection is a set of data that can be persisted to an arbitrary store when it makes sense, or streamed between processing steps when no persistence is needed. One data collection can be converted to another by applying functions to it. So the above diagram may now look like this, where arrows are functions used to transform data collections: This has several advantages: This model supports storing and launching processing from intermediate states, but it doesn’t require it. Processing downstream items from a raw data set will probably be a regular occurrence, but that need not be the case for other collections. Perhaps the biggest advantage of this approach is that it makes MapReduce pipelines composable. Logic expressed as functions can be reused and chained together as necessary to solve a problem at hand. Any intermediate state can optionally be persisted, either as a cache of items expensive to process or an artifact useful to applications. So, how does this work?  Here are the key pieces: Fortunately, a newer MapReduce framework supports this pattern well: Apache Crunch , based on Google’s FlumeJava paper , represents each data set as a sort of distributed collection, and allows them to be composed together with strongly-typed functions. The output of a Crunch pipeline may be a data model easily loaded into a RDBMs system, inverted index or queried via tools like Apache Hive. Crunch will also fuse steps in the processing pipeline whenever possible.  This means we can chain our functions together, and they’ll automatically be run in the same process. This optimization is significant for many classes of jobs.  And although persisting intermediate state must be done by hand today, it will likely be coming in a future version of Crunch itself. We may have reached the end of direct implementation of MapReduce jobs. Tools like Cascading and Apache Crunch offer excellent higher-level libraries, and Domain-Specific Languages like Hive and Pig allow the simple creation of queries or processing logic. Here at Cerner, we tend to use Crunch for pipeline processing and Hive for ad hoc queries of data in Hadoop. MapReduce is a powerful tool, but it may be best viewed as a building block. Composing functions across distributed collections that use MapReduce as its basis lets us reason and leverage our processing logic more effectively.", "date": "2013-02-03"},
{"website": "Cerner", "title": "Why Engineering Health?", "author": ["Charlie Huggard"], "link": "https://engineering.cerner.com/blog/why-engineering-health/", "abstract": "Hello and welcome to a public face for Cerner Engineering, by Cerner Engineering associates, to talk about engineering, technology, and all of the other awesome things we do. Cerner has been recognized as a visionary company, transforming the delivery of healthcare around the world. Improving the health of individuals and the delivery of care is an extremely large, complex, ever-changing problem. Along with the efforts of our strategists and consultant organizations, solving this problem takes a ton of smart folks in our Engineering and CernerWorks Hosting organizations, who are free to play with, adopt, and embrace new technologies and ways of working. Speaking of working differently, I’m currently at a coffee shop, writing this introduction post on my Cerner-issued Mac. I’ve completed a few code reviews for my team online using Crucible , and have been pushing minor tweaks to the code behind this website to our GitHub Enterprise instance. We collaborate on changes internally as well as directly with our clients using our Jive-powered uCern Connect site. But it’s more than just how cool is my day… Cerner understands the value in participating in a meaningful dialog about technology, sharing our discoveries, and learning from our peers in the industry. That’s why this summer our Engineering organization will be holding it’s 3rd annual DevCon, where all of our Engineering associates take two days away from their normal work to present and share their ideas and discoveries with each other. Cerner holds regular Hack Nights, where pizza and drinks are provided to get people together to try out new ideas and new technologies and solve things that may or may not apply to our usual work. Cerner also sends associates to attend and present not only at conferences around technologies we already use, such as ApacheCon , EclipseCon , PyCon , RailsConf and WWDC , but also at conferences around new technologies and ways of thinking like Compute Midwest , Strange Loop and SXSW . In a similar vein, by launching this Engineering Health site, we hope to provide transparency to our internal organizations, as well create another way to connect to the broader community involved in solving big problems with software and technology. You will see that in addition to being a leader in the healthcare industry, Cerner is also at the forefront of software technologies. With our inaugural blog post from Ryan Brush , you will get a taste for some cool ways we’re putting big data processing methods to work for some of our new initiatives. Future blog posts will get into other technologies used, lessons learned, and just fun things direct and unfiltered from our Engineering organization. I hope you enjoy this site, and feel free to connect with us on Twitter @CernerEng .", "date": "2013-02-04"},
{"website": "Cerner", "title": "Evangelizing User Experience", "author": ["Cerner Engineering"], "link": "https://engineering.cerner.com/blog/evangelizing-user-experience/", "abstract": "In the dark ages of development, great software meant packing in the functionality. People began doing more and more with their software. Updates meant newer and more exciting functionality. Sounds great, right? Of course it does, but something went horribly wrong. Slowly we became inundated with cluttered screens as software developers struggled to find a place to put their latest innovative functionality. Buttons began adding up and before we knew it, we were inventing user interface controls like ribbons to hold all the buttons. On the bright side, things can only get better from here. __ Somewhere along the straight and narrow path to UI nirvana, we strayed. Distracted by the allure of “doing more,” we forgot to question why more needed to be done in the first place. We overlooked the importance of designing how something should be done because we were busy discovering new things to do. And, most importantly, we failed to include the user in the development process. Instead, we used inaccurate perceptions and engineering constraints to dictate how users should interact with our solutions. Fortunately, software developers everywhere are beginning to see the light. We are entering a new golden age of software solutions—one where the greatness of software is not measured by the number of functions, but by its ease of use. Usability is in the spotlight more now than ever before. Perhaps you’ve caught wind of some of these buzz words lately: Usability, User Experience, Interaction Design, Emotional Designer, and User-Centered Design. Generally speaking, they all point to the same thing: making software easier, more elegant, more intuitive, and (dare I say) more enjoyable to use. Interaction designers may be the instigators of software development’s Great Awakening, but user experience experts cannot do the job alone. It takes a great team to bring everything together and produce an exceptional product. Without user-focused engineering, great ideas and concepts would never come to life, regardless of their theoretical merit. If leadership is not committed to doing whatever it takes to ensure the users have an intuitive, enjoyable experience with our solutions, entire projects and initiatives would never see the light of day. On the other hand, having a team of leaders, designers, and engineers working to produce software that users will love, can produce incredible results. The creators of Paper by FiftyThree, Apple’s iOS App of the Year, understand the importance of working together to produce an awesome experience. The results of their work speak for themselves. Watch this short video to catch a glimpse of their development culture and ideals: We are about to make a big splash of our own in the world of iOS with the eminent release of a new iPad app for ambulatory physicians. Designing fast, smart, and easy workflows, and creating an elegantly robust and beautifully simple interface to go with them has helped foster the rapidly growing culture of flawless execution at Cerner. From the outset of the iOS initiative, user-centered design has been the main focus. We created simple and intuitive interactions for complex user processes. Then, when we thought we had it right, our team of user researchers ran our designs through extensive usability testing, which validated our concepts or helped us discover where we could improve. Based on their feedback, we tweaked the designs, and re-tested. We rinsed and repeated. We are confident that our clients will be pleased. Why? Because we’ve been talking to them throughout the entire process. For Paper by FiftyThree, success means getting their users in touch with their creative side. At Cerner, producing amazing software solutions ultimately means improving the workflows of clinicians across the world, which impacts the health and wellbeing of countless individuals. We have a mission, and that mission is to make our solutions intuitive enough so that they simply fade into the background, allowing clinicians to focus on what is truly important: their patients.", "date": "2013-02-12"},
{"website": "Cerner", "title": "Near Real-time Processing Over Hadoop and HBase", "author": ["Ryan Brush"], "link": "https://engineering.cerner.com/blog/near-real-time-processing-over-hadoop-and-hbase/", "abstract": "This post covers much of the Near-Realtime Processing Over HBase talk I’m giving at ApacheCon NA 2013 in blog form. It also draws from the Hadoop, HBase, and Healthcare talk from StrataConf/Hadoop World 2012. The first significant use of Hadoop at Cerner came in building search indexes for patient charts. While creation of simple search indexes is almost commoditized, we wanted a better experience based on clinical semantics. For instance, if a user searches for “heart disease” and a patient has “myocardial infarction” documented, that document should be highly ranked in the results. Analyzing and semantically annotating can be computationally expensive, especially when building indexes that could grow into the billions. Algorithms in this space may be discussed in a future blog post, but for now we focus on creation of an infrastructure up to the computational demands. For this, Hadoop is a great fit. A search index is logically a function of a set of input data, and MapReduce allows us to apply such functions in parallel across an arbitrarily large data set. The above pattern is powerful but creates a nice problem to have: people want the output of the processing – in this case, updates to search indexes – faster. Since we cannot run a MapReduce job over our entire data set every millisecond, we encounter competing needs; the need to process all data holistically conflicts with the need to quickly apply incremental updates to that processing. This difference may seem simple, but has deep implications.  For instance: With MapReduce we can move our computation to the data, but fast updates require moving data to computation. MapReduce jobs produce output as a pure function of the input; realtime processing needs to handle outdated state. For instance, we build a phone book and a name changes from Smith to Jones, realtime processing must remove the outdated entry, whereas MapReduce simply rebuilds the whole phone book. MapReduce jobs often assume a static set of complete data, whereas realtime processing may see partial data or new data introduce in an unexpected order. And despite these differences, our processing output must be the identical; we need to apply the same logic across very different processing models. These significant differences mean different processing infrastructures. Nathan Marz described this well in his How to Beat the CAP Theorem post. The result is a system that uses complementary technologies: stream-based processing with Storm and batch processing with Hadoop. Interestingly, HBase sits at a juncture between realtime and batch processing models. It offers aspects of batch processing; computation can be moved to the data via direct MapReduce support. It also supports realtime patterns with random access and fast reads and writes. So our realtime and batch layers can be viewed like this: So new data lands in HBase but how does Storm know to process it? There is precedent here. Google’s Percolator paper describes a technique for doing so over BigTable: it writes a notification entry to a column family whenever a row changes. Processing components scan for notifications and process them as they enter the system. This is the general approach we have taken to initiate processing in Storm. Google’s Percolator strategy does not translate directly to HBase. Differences in the way regions are managed versus BigTable tables made using a different column family impractical. So we use a separate “notification” table to track changes to the original.  Updates to HBase go through an API that writes notification entries as well as the data itself. We then wrote a specialized Storm spout that scans the notification table to initiate processing of updates. The result is processing infrastructure like this, with Storm Spouts and bolts complementing conventional MapReduce processing: The processed data model may be another set of HBase tables, a relational database, or some other data store. Its design should be centered on the needs of the applications and services, letting the processing infrastructure build data for those needs. It is important to note that MapReduce output should be done with a bulk load operation in order to avoid saturating the processed data store with individual updates. This basic model turns out to be robust. Volume spikes from source systems can be spread throughout the HBase cluster. There are a couple key steps for success here: Regular major compactions on the notification HBase tables are essential. Without major compactions, completed notifications will pile up and performance of the system will gradually degrade. The notification tables themselves may be small in size, but should be aggressively split across the cluster. This spreads load to handle volume spikes and improve concurrency. Also note that MapReduce is still an important part of the system. It’s simply a better tool for batch operations like bringing a new data set online or re-processing an existing data set with new logic. There are a number of moving parts in this system, and good measurements are the best way to ensure it’s working well. For example, in development we found our HBase Region Servers would encounter frequent but short-lived process queues during heavy load. This didn’t look like an issue in HBase, but when we measured the performance of the calling process there was a noticeable degradation. The point is, instrumentation built into Hadoop and HBase are great but not sufficient. Measuring the observed performance at all layers is important to create an optimal system. There are many good technologies for doing so. We generally use the Metrics API by Coda Hale. Here is an example of HBase client throughput using an instrumented implementation of HTableInterface. The data is collected by the Metrics API and displayed with Graphite : The same logic needs to be applied to both batch and stream processing despite the necessary differences in infrastructure. This is a challenge since the models speak very different languages: InputFormats describe an immutable and complete set of data, whereas event streams expose incremental changes without context. It turns out the function is the only real commonality between them; simply taking a subset of input and returning useful output. So, our strategy is this: Build all logic as a set of simple functions, then compose and coordinate those functions with higher-level processing libraries. We use Storm to compose our realtime processing and Apache Crunch to compose our MapReduce jobs. Here are some lessons we have learned to apply this strategy effectively: Persisting intermediate state can be expensive and creates complex relationships between moving parts. This is particularly true if a MapReduce job creates intermediate state used by realtime processing or vice versa. Instead, keep processing pipelines independent whenever possible and combine the results at the end. Our MapReduce jobs are typically run on separate infrastructure than realtime processing to ensure expensive jobs do not saturate time-critical processing. A “join” in a MapReduce job sees all data, whereas a “join” in stream processing gets incremental subsets. If a function needs the full context to execute, that context must be externally loaded in the realtime processing system. In our case, external state is loaded from HBase and cached, but projects like Trident are now providing some aggregation facilities over storm as well. The patterns here have been successful but require significant scaffolding and infrastructure to bring together. Near-realtime processing demands over big data are bound to increase, which means there is an opportunity here; higher level abstractions should emerge. Similar to how tools like Crunch and Hive offer abstractions over MapReduce, it’s likely that similar primitives can express the patterns described here. How these higher abstractions emerge remains to be seen, but there is one thing I’m sure of: it’s going to be fun. I’d like to acknowledge key contributors to building this and related systems: Jason Bray, Ben Brown, Robert Farr, Preston Koprivica, Swarnim Kulkarni, Kyle McGovern, Andrew Olson, Mike Richards, Micah Whitacre, Greg Whitsitt, and others.", "date": "2013-02-27"},
{"website": "Cerner", "title": "Modularity in Medical Imaging", "author": ["Cerner Engineering"], "link": "https://engineering.cerner.com/blog/modularity-in-medical-imaging/", "abstract": "Developers often take for granted the level of flexibility and customization that is available within the software they use every day. Consumers of imaging software have traditionally been completely confined to interpret exams a specific way, and frequently in ways that are unintuitive. Every physician, specialist, technologist, med student, and others across the continuum of care has a preference as to not only how exams are laid out, but what information is displayed with the images and what transformations would be applied or processed automatically. As it turns out, the only workflow they have in common is that they all view exams differently and they all want the experience to be clear and understandable. We decided to take this to heart in the development of our viewing solution by allowing the application react to the user, opposed to the user reacting to the application and constraining their workflow. Working in the medical community can already be a high stress environment; introducing a piece of software intended to augment patient care that often times makes delivering care more difficult is not a viable solution. Finding the best way to address these issues starts with knowing the solution’s target audience. Before developers can help streamline a workflow, they first have to understand it. If you understand the user’s workflow and how they expect the solution to function, you can design for usability and they will want to use it, as opposed to feeling forced to use it. A key aspect of this is the reliability and simplicity of the application. If the user feels like the application is unstable or overly complex, they will immediately lose interest and view the whole experience as a chore, as opposed to something that can provide value. The interface is the first application element the user encounters. If it feels intimidating and complex, before they even perform any functions, you have set a negative tone for your application and adoption will be low. In the case of imaging, Radiologists often spend more than 60 hours a week reading over 16,000 procedures per year, all in front of the same application. Imagine working with an application that causes frustration throughout your day.  The first impression can propel an application in the marketplace or stifle it. With the advent of open source software coming to the forefront, there are a variety of options for developing rich applications backed by a large community of contributors. One of those happens to be the Eclipse Rich Client Platform (RCP) and the corresponding Standard Widget Toolkit (SWT). RCP and SWT both provide a way for developers to rapidly develop a professional looking and stable application but are somewhat limiting when viewed from an imaging standpoint. Radiologists typically read on workstations that have three or more monitors and in environments that are dimly lit. Using an application that is primarily grey and white can not only cause eye strain for the user but it can be distracting from what the user is trying to accomplish. Our team addressed this by starting out with the native components and then closely working with radiologists and visual designers to plan a user interface that was not only visually pleasing but also functional. The end result was a set of skinnable SWT widgets that are reusable within any SWT based application and can be specifically themed to match a desired color scheme for an environment. Another aspect of imaging that we needed to plan for was the wide array of uses (diagnostic versus distribution). In a diagnostic scenario, radiologists will use the solution to perform diagnosis and will use the application within a multi-monitor high-resolution environment. For purposes of distribution imaging, clinicians will view images for reference and often times will view these on a much lower resolution single screen device (such as a laptop). While these two scenarios have vastly different hardware environments, users want the same experience across both and do not want to have to learn a new application depending on the workflow they are targeting at the time. As developers, we can address this by using service APIs instead of hard implementations and then create different assemblies that are easily accessible based on what the user is trying to accomplish. This not only helps address production concerns but allows developers to code against mock data sources and stores using the same API that they would within a fully scaled production environment. Once we had the basics covered, we rethought how we approached the workflow, and sought to intentionally design for usability and responsiveness within the application. Users previously were constrained to grids and a limited amount of information while reading. With modular components designed to perform specific tasks and provide specific information, the user can now decide how they want to consume and view exams. So now we have a solid application for use within diagnostic scenarios, but how do we give access to the enterprise without adding overhead? Following the path of reuse, we decided that we would use the main application that we already had. By starting with these components instead of rebuilding it for the browser or an entirely new framework, we could focus on how we could change the delivery of that application. We needed a way to deliver the same high power application without requiring the high power hardware. Traditional IT solutions would use Citrix, VMWare, or some other type of commercially available virtualization solution. While these are all great solutions for certain scenarios, the requirement of plugin installation on the client device accessing the application the heavy graphics processing within an imaging solution does not play nicely in these environments.  We can build on what these other solutions use (MS Remote Desktop Services) but choose to marshal the end result in an entirely different way. If we want to deliver an application via a web browser, we have a limited selection of tools that we can depend on being available. IE has its own quirks and implementations, Chrome has some fancy extras on top of the WebKit rendering engine, and while Safari/Firefox use the same engine they still behave differently. Developing and deploying an application that must be focused on performance for the end user and still usable within a variety of environments has to take all of these constraints and challenges into consideration. First, we needed a way to communicate between the client and server. The HTML5 WebSocket API is great for this, but only has minimal support and user adoption. Socket.IO has a great framework that allows for WebSocket emulation (or their native use if available) that provides great performance. We decided to use this for our real-time web app and it has proven to be invaluable throughout our development process. Now that we can communicate to the server what the user wants to do, we need a way to provide a representation of the images and UI that corresponds to the user input. The HTML5 Canvas API would seem like a good choice but continually redrawing canvas areas is process intensive and as this API isn’t supported within IE (the vast majority of our user base), it simply wasn’t an option. Instead, we decided to use a streaming mechanism, but couldn’t use the HTTP Streaming spec as this inherently introduces latency, which wouldn’t provide an acceptable user experience. All browsers support JPEG and PNG decompression and we can easily compress representations of the screen and shuttle these to the client. Even better, we can do this with a minimal amount of latency (we eliminate the need for key frames within an MPEG stream); we can choose which frames to drop and we can adjust the quality of each frame on the fly. This allows the client to adapt to the current network constraints and machine capabilities by tracking the effective FPS and adjusting the parameters on a sliding scale accordingly. With all of these tiers in place, we now have an application in the browser that looks identical to the desktop deployment with plugins or installation required. Beyond that, we have an SDK where third parties can build on top of our current deployment and provide even more specialized functionality within the imaging space, while still allowing users to customize their layout and how the data is delivered. We have now managed to deliver the same application to any device with a browser and our developers can develop from a single code line. This pleases our users and makes our developers’ lives easier. A true testament to how ease of development and a simple, yet powerful, user experience can exist in the same application.", "date": "2013-03-29"},
{"website": "Cerner", "title": "Cerner and Tycho", "author": ["Jonny Wright"], "link": "https://engineering.cerner.com/blog/cerner-and-tycho/", "abstract": "Tycho is a build tool for integration between PDE and Maven .  Cerner has a long history of working with Eclipse RCP but P2 integration is something of a more recent phenomenon.  This post is going to talk about how Cerner is using Tycho, what prompted our transition to Tycho and how we accomplished moving our build to using Tycho. The first question when evaluating any tool is asking yourself: “why would I use that”?  In our case, Cerner already has an extensive Maven ecosystem.  For years it has been common practice, when developing Java, to use Maven as the go to build, site, and deployment tool.  What we needed was a set of extensions that allowed us to use all the facets of Maven while creating PDE artifacts.  Traditionally, the set of PDE artifacts that we needed to create and manage was limited to products, targets, and bundles.  Using the Maven conventions, we could build out all of these PDE artifacts while allowing the Maven POM to be the single source of truth for most build projects.  This worked well enough until we needed to rework our assemblies to be comprised of features, inserting another PDE artifact into the build lifecycle. When we first began RCP development, no tooling existed to help us with creating PDE artifacts through Maven, so we created our own.  When the time came to start integrating additional PDE concepts, so that we could begin the path of integrating with P2 director, there was Tycho. We use tycho generally to build the following set of PDE artifacts: While the mirrors are not technically Eclipse artifacts, we use Tycho to mirror the Eclipse (for now Juno) repositories so that our builds are not hammering the Eclipse update site.  When you have 10’s to 100’s of users updating targets and running builds, it is just common courtesy to have your own mirror. We began our journey with Tycho by using it to generate P2 repositories from feature definitions.  The bulk of the heavy lifting is accomplished through the use of the tycho-packaging-plugin (as well as the base plugin, tycho-maven-plugin).  Since we deploy all of our plugins through maven, it is ideal to have Maven (Tycho in this case) do the dependency resolution for us and fill out the contents of our feature.  It also does all the work of turning that feature into a p2 repository. This was actually a nontrivial exercise for us.  While we could have used a single feature definition, we tightly controll the dependencies that are pulled into our assembly.  Therefore, as we looked to breakdown our feature definition it was more complicated than simply pulling in the Juno feature and heaping everything into a feature on top of that.  Since we only use a subset of the Eclipse platform, we only want to pull forward the pieces we use explicitly (we also ran into some rather strange behavior with the Eclipse Jetty features).  We wanted to define the features in such a way that consumers of our platform could select a single feature that represented our core platform + Eclipse (as well as layer in additional functionality related to building their application).  In the end, we defined a set of about 20 features that rolled into a single working repository.  This way, consumers only needed to know the location of a single repository and then could use the metadata in the features to determine which features they needed. Starting at features made sense because there was no existing infrastructure for us to overhaul to build out the features.  The products were a different story.  With the products, we had an entire set of plugins that managed which version of Eclipse was pulled in for the product, how the .product was generated, and how we rolled that into an .exe. The extra time we took in order to define the features thoroughly actually made this process significantly easier.  Today, we build a set of 8 applications: Each of these products also has a corresponding mock build (where we substitute in mock resources to populate data for our views).  The definitions of these products was actually very simple and each ended up being around 8-10 features to create the product.  With the aggregated repository, we had a single repository in each POM where the versions for the features were determined and filled out.  Using the tycho-packaging-plugin and the tycho-p2-director plugin, we were able to automate the build of the corresponding p2 repositories which resulted from the Tycho builds. Being the kind citizens that we are, we didn’t want to abuse the Eclipse p2 release repository (also we didn’t want them to cut us off like Maven Central).  To be good citizens in this build world, we decided we needed to mirror the Eclipse repositories ourselves.  Luckily for us, Tycho developers had already anticipated that need!  By using the tycho-p2-extras-plugin , we were able to easily automate the mirroring of the required features from the Eclipse release repository.  Score (another) 1 for Tycho. As part of the process for deploying these mirrors, we actually used the build-helper-plugin to deploy .target files to point to the mirrors into our maven repository. The all important question: Why didn’t we do bundles?  The two biggest things stopping us from uplifting our bundles was the lack of an easy way to incorporate non bundle sources into builds and the difficulty in generating sites with code coverage for projects that use both Java and Groovy (a sad state of affairs I don’t recommend).  In the end, these hurdles plus the amount of time it would take to uplift our own projects to match the expected build structure was a significant overhaul.  We decided that, for now, the maven-bnd-plugin and m2eclipse can support our needs well enough for bundle development. So I’ve mentioned throughout this blog post about how we moved from custom maven to tycho; so what does that mean exactly?  Back at the dawn of time (as I reckon it, others may know it as 2007), Cerner set down the path of doing Eclipse RCP development.  At the time, there weren’t many good tools to automate your PDE builds.  Until very recently, most of the Eclipse projects have used Ant as their build system.  For our ecosystem and needs, it was Maven or bust.  So we set about creating plugins that would help us at all levels of the build.  Maven plugins were created for all of the following actions: This turned out to be a rather extensive amount of work.  The model worked relatively successfully until a consumer came along with a set of requirements that read an awful lot like: “we’d like P2 integration included if we move to the iAware platform”.  This threw a wrench in our system, because while P2 can work (and does) work by simply using .product files to define content sets, we had no tooling to generate P2 repositories.  We also had no mechanisms that would allow us to update only parts of their application (since this was 5 development teams working in tandem to produce a single assembly).  When all of this converged, we realized our build system had become insufficient and it was time to explore alternatives.  As you can see from the path we took, we started with the least intrusive changes and scaled up from there.  As of this blog, iAware is preparing for the first release using features and products. Get ready for one ugly POM configuration: If that isn’t enough to scare you, I don’t know what is.  I wrote it and it terrifies me.  What it really means though is that I haven’t found a great way for deploying p2 repositories as part of the maven build process.  Options exist, such as deploying the zip file of the repository into the maven repo or writing shell scripts to automate the deployment, but neither of these sat well with me.  So I went for the least distasteful choice and wrote some basic ant scripts :). All in all, Tycho has become a large part of our development ecosystem.  This blog only really touches the surface, but Tycho has become integral at all stages (besides bundle dev) to managing our assemblies.  So, thank you Tycho contributors!", "date": "2013-03-27"},
{"website": "Cerner", "title": "Ruby at Cerner", "author": ["Cerner Engineering"], "link": "https://engineering.cerner.com/blog/ruby-at-cerner/", "abstract": "Cerner’s journey with Ruby started in the summer of 2009. We selected Ruby on Rails for rapid development of the Cerner Store to prepare for release at the Cerner Health Conference that year. In three months, with three engineers and a designer, we wrote and released the first version of an e-commerce web application. Two of those engineers, including me, had never worked with Ruby before but quickly realized the power and expressiveness of the language due to resources like Why’s (Poignant) Guide to Ruby . Our experience with the Cerner Store taught us that Rails led to high productivity. Ruby is a very natural language to write code in and principles like convention over configuration enabled us to solve our problems instead of spending time wrangling the framework. In addition, we valued the good practices of the Ruby community like easy-to-understand code and thorough unit testing with tools that aren’t painful. In the summer of 2010, we attended the first Ruby Midwest as a team. We learned about developments in Ruby like JRuby and Chef as well as some of the great gems under development. The Cerner Store continued to grow and we learned about maintaining a Rails web app over time. In early 2012, we were planning a massive undertaking to create a new platform for our Clients' healthcare data. It was to be called Millennium+ and it needed an architecture that could scale well to petabytes of data and dozens of engineers across many teams. We planned a service-oriented architecture and chose Rails to serve as the server side of the application. Our Rails services call JVM services that retrieve data from HBase and serves the resulting data as JSON to the client-side applications, including our iOS app, PowerChart Touch Ambulatory. The high productivity we enjoyed on a small team scaled well to a large team of people who had never written Ruby before. This was the start of Cerner’s Ruby community. We developed reusable libraries and development processes that we continue to use today. The complexities of our architecture also led to the adoption of Chef to automate operations and build a devops mindset. Chef is another integral use of Ruby that penetrates teams that did not use Ruby at all. When planning our newest platform, Population Health , we needed to determine which platform made the most sense for a large-scale development of web applications on a tight timeline. We decided on Rails due in large part to its prioritization of convention over configuration, as well as the existing Ruby community at Cerner. These attributes would enable us to develop applications at the planned pace and scale. This decision is now disseminated to dozens of engineers working on brand new web applications and REST services powering them. We’ve bolstered our internal Ruby community with lots of documentation and guidelines. We make use of the Ruby on Rails Tutorial book and Why’s (Poignant) Guide to train engineers in Ruby. Bozhidar Batsov’s Ruby style guide aligns closely with how we write our code, so we use rubocop to keep ourselves in line. Our development devices run on RVM and our servers run on Phusion Passenger . Ruby has quickly become a very important part of Cerner’s engineering culture, primarily as the language behind Rails and Chef. An internal Ruby meetup has begun and there have been presentations involving Ruby at our annual Developer Conference. Additionally, we are sponsoring Ruby Midwest because we want developers to know that Ruby is highly valued at Cerner. We’re also sending a large number of our own associates there to learn. We look forward to bringing on more engineers to use Ruby and other technologies to engineer the future of healthcare.", "date": "2013-04-05"},
{"website": "Cerner", "title": "Learn what the rules don't cover", "author": ["Paul Conklin"], "link": "https://engineering.cerner.com/blog/learn-what-the-rules-dont-cover/", "abstract": "Most technical problems are like games.  All of them have a way to win and all of them have rules; the easiest way to ensure you always win is to learn the rules inside and out, and more importantly what the rules don’t cover!  Paying attention to what the rules don’t cover is what leads to out of the box thinking.  What sets the great players apart from the rest is learning what the rules don’t cover which allows for creativity and, sometimes, shortcuts. Recently, I played a game similar to musical chairs. It was a game of diminishing resources (a management exercise) with about 12 of us and six large sheets of paper.  The only rules were: 1) Walk around while the music is playing and 2) Put your foot on a rectangle when the music stopped (he instructor pointed at one of the sheets as a visual queue).  When we first started, there was ample space for each of us to have a foot somewhere on one of the 2’ x 4’ paper. As the game continued, the instructor started cutting the sheets into smaller and smaller pieces until we got down to 3\" x 5\".  Most of us were precariously trying to balance and keep a portion of a shoe touching the paper, a few just gave up, and two paid special attention to the rules and found an out.  One gentleman took at a business card and promptly stood on it.  Another just lifted his leg and put it up on the door.  We all were following the same rules but two found what the rules didn’t cover and won. Most people think that Printing is easy.  You hit a button or Ctrl + P and the paper comes out, right?  This seamless process appears so to users because of the hard work that goes in behind the scenes.  So when I started down this path, I knew a strong foundation would be important. I settled on CUPS as the basis for my brave new world of printing.  It is open source and has a pretty wide following.  It’s the basis for Apple printing, fairly hardened, and is the backbone of several educational systems printing making it enterprise ready. The more I worked with it, the more I saw its full potential.  CUPS had turned out to be the most useful “Swiss Army Knife” in my toolbox.  It is extremely robust and easy to integrate / stack with other solutions.  One great example of this is my “Coffee CUPS” demo. I picked CUPS as the underlying backbone for my new architecture for the same reason.  It had the maximum amount of possibilities with the least amount of rules.  With a minimum amount of rules, it allows for the maximum amount of creativity.  Sure, I could develop my own solution from the ground up where I get to make the rules and have unlimited creativity, and I have had to do that in the past.  In general, if you write your own software, you have the least amount of rules (constraints of the compiler).  OpenSource Software is a close second with few limitations.  Closed source 3rd party software usually has the most rules (ever read a EULA in your life?).  When selecting a solution to a problem, it’s also important to do a cost benefit analysis.  Is it really worth reinventing the wheel?  I’m often reminded of the below picture.  I’ve seen several variations of it over the years (Credit to the picture unknown, but it wasn’t me). CUPS handles a lot of the general architecture that doesn’t need to be re-invented on either end of the spectrum (managing print queues, accepting jobs, sending jobs to printers, etc) but allows for a lot of creativity in the middle (the middle being what is done to the print job in between getting it from the user and sending it to the printer).  A good example of this (filters) can be found here .  CUPS gives you an overall framework of how it will call a filter, and what it expects as a return, but beyond that, it’s up to the programmer.  You can write in pretty much any language you want and alter the print job as much as you want.  I leveraged this to maximize the amount of devices I could talk to and input file types I could accept, while also being able to make business decisions based on the content of the print job. Source: www.projectcartoon.com I think that picture aptly describes the disconnect in all of the processes that exist in problem solving.  So in addition to learning what the rules don’t cover, we need to be keenly aware of what you are trying to accomplish: what is the problem and the success criteria? How do we win the game?", "date": "2013-04-19"},
{"website": "Cerner", "title": "FIRST Robotics Championship Competition in St. Louis", "author": ["Cerner Engineering"], "link": "https://engineering.cerner.com/blog/first-robotics-championship-competition-in-st-louis/", "abstract": "Cerner places a high value on talent development programs offering students the experience to build practical and tangible skills for the modern work environment. As part of this focus, Cerner supports FIRST Robotics, a competition providing experience in software engineering, where students learn to deal with complexity, time constraints, quality, and technical communications. Sound familiar? I wish they had this program when I was a kid! High school students from Kansas City will be testing their minds, willpower, and teamwork in this global robotics championship competition April 24-27 in St Louis, Missouri. The secret game design was revealed in January, and with just six weeks to build, over 2,000 teams created completely unique robots. Design, engineering, metal fabrication, project management, marketing, and fundraising are all activities that students gain exposure to in this real-world project. Most importantly, they practice creative problem solving in complex team dynamics. The championship teams you may have seen in the Kansas City Regional are: Students are challenged with two software programming components: digital media marketing and robot controls. Students will use a wide range of web, mobile, and media development technologies to create their team’s marketing strategy. Robot controls is broken down into two types: autonomous and teleoperated programs. In autonomous mode the robot responds exclusively to preprogrammed commands based on sensor feedback from a camera, accelerometer, gyro, encoders, and more. In teleoperated mode, the robot continues to use sensors but now can receive input from the drive team using game joysticks. Students can use three different programming languages for the robot control system: The most important tool for control system programmers is a white board. Students diagram and visualize all the inputs and outputs of each system: motors, actuators, sensors, and driver station. Mapping inputs to outputs is important not just during the construction of the program, but it also helps to train the drive team. Students use online resources, collaborate with other teams, and receive guidance from their technical mentors. Practicing resourcefulness prepares them for the complex professional engineering environment they will soon become a member of. Cerner is engaged in the Kansas City community at many levels. We are a sustaining partner in the KC STEM Alliance. Many Cerner associates mentor local teams, volunteer at local events, and are involved parents. These experiences exercise student minds in very real and practical ways. They are more prepared for technical and non-technical Cerner careers. Their passion and commitment is the fuel for delivering our future innovation. Here is this year’s gameplay video: For more information, check out the links below:", "date": "2013-04-23"},
{"website": "Cerner", "title": "Thinking in MapReduce", "author": ["Ryan Brush"], "link": "https://engineering.cerner.com/blog/thinking-in-mapreduce/", "abstract": "This is the blog form of the Thinking in MapReduce talk at StampedeCon 2013. I’ve linked to existing resources for some items discussed in the talk, but the structure and major points are here. We programmers have had it pretty good over the years. In almost all cases, hardware scaled up faster than data size and complexity. Unfortunately, this is changing for many of us. Moore’s Law has taken on a new direction; we gain power with parallel processing rather than faster clock cycles. More importantly, the volume of data we need to work with has grown exponentially. Tackling this problem is tremendously important in healthcare. At the most basic level, healthcare data is too often fragmented and incomplete: an individual’s medical data is spread across multiple systems for different venues of care. Such fragmentation means no one has the complete picture of a person’s health and that means decisions are made with incomplete information. One thing we’re doing at Cerner is securely bringing together this information to enable a number of improvements, ranging from better-informed decisions to understanding and improving the health of entire populations of people. This is only possible with data at huge scale. This is also opening new opportunities; Peter Norvig shows in the Unreasonable Effectiveness of Data how simple models over many data points can perform better than complex models with fewer points. Our challenge is to apply this to some of the most complicated and most important data sets that exist. Our first thought may be to tackle such problems using the proven, successful strategy of relational databases. This has lots of advantages, especially the ACID semantics that are easy to reason about and make strong guarantees about correctness. The downside is such guarantees require strong coordination between machines involved and in many cases the cost of that coordination grows as the square of data size.  Such models should be used whenever they can, but to reason about huge data sets holistically means we have to consider different tradeoffs. So we need new approaches for these problems. Some are clear upfront: as data becomes too large to scale up on single machines, we must scale out across many. Going further, we reach a point where we have too much data to move across a network – so rather than moving data to our computation, we must move computation to data. In fact, these simple assertions form the foundation of MapReduce: we move computation to data by running map functions across individual records without moving them over the network and merge and combine, or reduce, the output of those functions into a meaningful result. Word count is the prototypical example of this pattern in action. MapReduce implementations as offered by Hadoop actually offer a bit more than this, with the following phases: Map – transform or filter individual input records Combine – optional partial merge of map outputs in the mapping process, usually for efficiency Shuffle and Sort – Sort the output of map operations by an arbitrary key and partition map output across reducers Reduce – Process the shuffled map output in the sorted order, emitting our final result. We have our building blocks: we can split data across many machines and apply simple functions against them. Hadoop and MapReduce support this pattern well.  Now we need to answer two questions: How do we use these building blocks effectively and how do we create higher-level value on top of them? The first step is to maximize parallelism. The most efficient MapReduce jobs shift as much work into the map phase as possible, even to the point where there is little or no data that needs to be sent across the network to the reducer. We can gauge the gains made by scaling out by applying Amdahl’s Law where the parallelism is the amount of work we can do in map tasks versus more serial reduce-side operations. The second step is to compose our map, combine, shuffle, sort, and reduce primitives into higher-level operations. For example: Join – Send distinct inputs to map tasks, and combine them with a common key in the reducers. Map-Side Join – When one data set is much smaller than another, it may be more efficient to simply load it in each map task, eliminating the reduce phase overhead outright. Aggregation – Summarizes big data to be easily computed. Loading into external systems – The output of the above operations can be exported to dedicated tools like R to do further analysis. Beyond that, the above operations can be composed into sophisticated process flows to take data from several complex sources, join it together, and distill it down into useful knowledge. The book MapReduce Design Patterns discusses all of these patterns and more. Understanding the above patterns is important but much like how higher-level languages have grown dominant, higher-level libraries have replaced direct MapReduce jobs. At Cerner, we make extensive use of Apache Crunch for our processing infrastructure and of Apache Hive for querying data sitting in Hadoop. Most of development history has focused on variations on Place-Oriented Programming , where we have data in objects or database rows and we apply change by updating our data in place.  Yet such a model doesn’t align with MapReduce; when dealing with mass processing of very large data sets, the complexity and inefficiency involved in individual updates becomes overwhelming. The system would become too complicated to perform or reason about. The result is a simple axiom for processing pipelines: start with the questions you want to ask and then transform the data to answer them. Re-processing huge data sets at any time is what Hadoop does best and we can leverage that to view the world as pure functions of our data, rather than trying to juggle in-place updates. In short, the MapReduce view of the world is a holistic function of your raw data. There are techniques for processing incremental change and persisting processing steps for efficiency but these are optimizations. Start by processing all data holistically and adjust from there. The paper From Databases to Dataspaces discusses a new view of integrating and leveraging data. A similar idea has entered the lexicon under the label “Data Lake” but the principles align: securely bring structured and unstructured data together and apply massive computation to it at any time for any new need. Existing systems are good at efficiently executing known query paths but require a lot of up-front work, either by creating new data models or building out infrastructure for the immediate need. Conversely, Hadoop and MapReduce allow us to ask questions about our data in parallel at massive scale without prior build. This becomes more powerful as Hadoop becomes a more general fabric for computation. Projects like Spark can be layered on top of Hadoop to significantly improve processing time for many jobs. SQL- and search-based systems allow faster interrogation of data directly in Hadoop to a wider set of users and domain-specific data models can be quickly computed for new needs. Ultimately, the gap between the discovery of a novel question and our ability to answer it is shrinking dramatically. The rate of innovation is increasing.", "date": "2013-07-31"},
{"website": "Cerner", "title": "The 30 Days of Code Experiment", "author": ["Carl Chesser"], "link": "https://engineering.cerner.com/blog/the-30-days-of-code-experiment/", "abstract": "In software development, we solve problems. As we solve these problems, we build connections in our minds of how to look at a problem, relate it to previous problems and solutions, and re-apply past approaches and techniques. These behavior habits build dogmatic ways of thinking and limit design choices to selective technologies we’ve used in the past. As we all know, you have to continually learn new technologies and different ways of thinking to stay current in the ever-changing landscape of software development. Unfortunately, keeping up-to-date on technologies and approaches isn’t an easy behavior to maintain when you have many other priorities. This spring, a few engineers wanted to bring focus to this important behavior trait by giving a tech talk on “Honing your Craft,” which discussed how to continuously strengthen and refine skills. Not only did we want engineers that we work with on a daily basis to be part of the tech talk; we wanted engineers from other teams at Cerner to be involved too. Also, we didn’t just want to talk about it; we wanted to pose a challenge to put people into action and re-enforce these behaviors. At the end of the talk, we announced the challenge: 30 days of code. It was a challenge like many other “30 day” programs but was centered on learning new aspects of software development, languages, or just hacking up a tool that you find useful for your day-to-day development. The goal being that after 30 days, new habits and behaviors would be established to help promote continuous learning. To make this a social learning experience, we built a Ruby web application called “mural” using async_sinatra and eventmachine to consume our GitHub Enterprise instance and show gists, which contained a code comment of “30_days_of_code”. The result was really interesting. Similar to Twitter, where you follow a specific hashtag, we were following code snippets of fellow developers. Since most of them were gists, they were small enough so you could easily see what they were doing (without having to go through a mountain of code). With this dashboard, a score was calculated based on count of your posts and was displayed with your avatar. Having scores displayed in descending order added a little peer pressure to keep people active in challenge. By the time I got back to my desk, I received questions of where the URL was to see the app, so they could verify their posts were showing. Soon, people wanted GitHub repos to show up with gists, so a pull request was requested for that. We then wanted anonymous gists to also get pulled in, which required developing our own crawler since these were exposed on the gist search. It was apparent that people wanted to share what they were doing, and people wanted to see it. At the end of the challenge, we returned to the auditorium to highlight some of the posts that came in over the past month. During the 30 days, we had 124 gists posted and 17 different contributions in repos. People were showing their skills over a wide range of technologies. Examples were: Building a plugin to invoke Jenkins commands over a phone (using Siri) Ruby scripts that interact with our Github Enterprise instance API that will send out emails when pull requests or branches are getting old (executed periodically through Jenkins) Clojure application that looks for pull requests based on Github organizations, which have two “+1” comments (alerting which pull request may be candidates to close out) Illustrating Crucible code interactions by extracting data with python and visualizing with D3\nUsing Node.js to flash lights on a Raspberry Pi when a health check from web service is failing Presenting statistics from a storm cluster with Rickshaw Eleven people presented what they worked on and learned. It was amazing to see all of the different ideas people came up with in this time frame. Even more interesting was how quickly people learned from the ideas of others. Not only were developers sharing their code snippets, but they were also sharing the problem they were attempting to solve or the idea of what they wanted invent. For example, the Ruby script which alerted the last committer of a dead branch through email, spawned into other implementations that would send alerts based on different pieces of Github data (ex. old pull requests). Sharing these ideas in their early stages (through code snippets) really accelerates the rate that an idea can be seeded in other minds and helps inspire even more innovation and learning. This wasn’t only a challenge of what we would build, but it was also an experiment of what can happen by taking a small portion of your day and doing something different. By structuring this exercise around a formal challenge that included a competitive aspect, there was additional motivation to get involved and stay involved to the end. In summary, find a way to take a little time out of your day to try something different; you will be amazed with the different perspectives that you gain.", "date": "2013-08-06"},
{"website": "Cerner", "title": "DevAcademy", "author": ["Michelle Brush"], "link": "https://engineering.cerner.com/blog/devacademy/", "abstract": "When I graduated from college, I thought I understood what it meant to develop software in the real world. It required process. It required troubleshooting. It required quality. However, to me, process meant waterfall. Troubleshooting meant trying a few things and then asking for help. Quality meant manual testing. Agile methods were not unheard of when I graduated in 2001. My professors noted that iterative development was better than waterfall; they just only taught waterfall. Debuggers had been around since the 50’s, but my classmates and I still debugged with what I call “Hi Mom!” techniques. (We peppered our code with print statements.) Kent Beck had written the JUnit framework 4 years before, but it wasn’t entrenched in the Java culture yet. So it’s not surprising that my education didn’t cover these topics. It took a few painful experiences in the real world to make me realize the way I programmed in college wasn’t the best way to engineer software. I needed to adopt some new practices. Not much has changed in terms of software education. Being a part of Cerner’s software engineer training program, I am able to ask every group of new engineers three questions: “Do you use an agile process?” “Do you use a debugger when troubleshooting your code?” “Do you write automated unit tests?” Cerner has had explosive growth in engineering, so I’ve asked those questions of hundreds of recent graduates. Almost no one says yes. This told me that while colleges are doing a great job of teaching computer science, many schools are not teaching best practices in software development. Until recently, Cerner wasn’t doing that great of a job of teaching them either. Our training program covered them, but we still saw the new engineers struggle to understand agile development, debug their code, and write their first unit test. One of Cerner’s core values is if you recognize something is broken, you are empowered to fix it. I knew our training program wasn’t working. It became my job to fix it. Before tackling the whole program, I tried a little experiment. I wanted to see what it would take to get engineers in training to write just one unit test. At the time, training included a class on JUnit. In spite of the class, only 5% of the engineers were writing unit tests for training assignments. To correct this, I started telling the engineers that I would take points off assignments that didn’t have a unit test. The idea was to create structural motivation. We immediately saw 40% of the engineers writing unit tests. A step forward, but it wasn’t enough. The biggest obstacle to broader use of unit tests in training was that they didn’t know how to include the testing framework in their Java projects. That, more than the effort of writing the test, was keeping them from doing something we expected of them. Something was wrong. We were teaching Maven in our training program. If you are not familiar with it, Maven helps you manage your project builds, and as a result, it helps you manage your dependencies. The engineers were already attending a class that taught them how to add dependencies to Java projects. They just weren’t able to associate what they had learned with the goal of bringing JUnit into their projects. They weren’t making the connection. This connection was missed because engineers were learning about Maven in the absence of a problem. They were being told it’s an important tool, but we hadn’t given them a reason to use it. Later, when they did encounter the problem - “How do I add the JUnit jar to my project?” – it was too late. They had forgotten about Maven. The key was to move the Maven training closer to when they needed the information. This is called “Just in Time Teaching.” It became the first requirement of the new program. Another interesting aspect of my experiment is that 40% would write the tests even given the delay between training and practice. It should be obvious to anyone that’s ever taken a college programming class that some programmers can get it from lectures alone. Others have to practice. Any one-size-fits-all approach to training is flawed. The second requirement for the new program was that it must flex to meet the skills and learning styles of the engineers. With these goals in mind, I started the redesign of Cerner’s training program. My first step was to interview a large sample of our software leaders. I asked them what they wanted engineers to learn. Time and time again, the top answers would be agile development, debugging, and automated unit tests. Surprisingly, it was not a list of technologies like iOS, Hadoop, JSON, or ReST. The resumes of our newly hired engineers are full of languages and technologies. However, when asked what they wanted of new engineers, our lead architects described practices. If Cerner could get engineers to improve in practices, we could take the great engineers we were hiring and immediately make them more productive. The scary thing is that sharing knowledge is easy, but changing people’s behavior is hard. Once I realized our problems were about software development behavior and not knowledge, I realized we would need to completely rebuild the way Cerner trains its new engineers. The result is DevAcademy. Imagine you are a new engineer starting at Cerner. In your first week at Cerner, you report to the DevAcademy. The first two weeks focus on in-class instruction and assessment. The goal here is to introduce core software development behaviors and then assess your skills. After completing the first two weeks of instruction, you join what we call the DevCenter and are assigned a real project. However, that project isn’t assigned by your team. You get to pick. The projects come from all over Cerner including web applications and services, tools to make engineers more productive, and even contributing to open source projects used by Cerner. In picking a project, you are telling Cerner the types of work you find interesting. This helps us determine the best place for you across our diverse range of solutions and technologies. While working on that first project, you have a dedicated mentor. You are expected to make progress on the project while receiving feedback. You also get just-in-time training on user stories, source code management, unit testing, and scrum. You get to use Cerner’s software development ecosystem in an isolated, safe environment. Once you show readiness to join a team, you are allowed to demo your work to the teams that have open positions. Those teams can then pick the engineer that best fits their team. In this way, Cerner makes sure you are assigned to the right team for both Cerner and you. DevAcademy recognizes that you should never stop learning, so the program continues well into your first few months on your team. You are offered classes on different technologies and more advanced topics as part of an elective-based training plan. You work with your manager to decide which classes to take. It’s Cerner’s way of making sure all of our engineers continue to grow. We’ve had 150 engineers join DevAcademy since it was launched. I’ve had the privilege of seeing the new engineers struggle and then succeed on their projects. I’ve seen the light come on when they realize the usefulness good development practices and apply them effectively. I’ve seen them get excited about git and other powerful tools that they didn’t have the opportunity to learn during their formal educations. The best part of my job is that I’ve seen many very good engineers start down the career-long path towards becoming really great ones.", "date": "2013-08-14"},
{"website": "Cerner", "title": "DevCon", "author": ["Kevin Shekleton", "Frank Rydzewski"], "link": "https://engineering.cerner.com/blog/devcon/", "abstract": "This past June, 2,500 associates from across Cerner came together for DevCon , our internal developers conference. Now in its 3rd year, DevCon is a two-day, engineering-led conference that was created to bring together Cerner associates involved in all aspects of development and technology. DevCon is organized and run like many other developer conferences, complete with a call for papers and a talk selection committee. This year, we had 80 talks covering a wide array of topics such as big data, user experience and design, DevOps, and mobile development. A keynote presentation kicks off each day followed by talks across multiple tracks. Chris Brown , CTO of Opscode , the creators of the open-source Chef Platform , kicked off DevCon this year with a talk on the emergence and importance of DevOps. Within engineering at Cerner, embracing DevOps has had a big cultural impact on how we approach the software and systems we write and operate. David Hogue , a leading Interaction Designer and instructor at San Francisco State University, kicked off the second day talking about the importance of interaction design in business and software. Hogue’s keynote was especially relevant to us as interaction designers play a prominent role in the creation and maintenance of our software. As you can see from the two keynote videos, the visual theme for DevCon this year was 8-bit gaming. Two 1980’s living room setups, packed with vintage NES consoles, CRT televisions, and piles of classic games, provided entertainment and nostalgia during the breaks in the conference schedule. At night, 20 teams competed for bragging rights and awesome prizes at Geek Trivia Night, answering questions ranging from name the sci-fi spaceships to identify the programming language from a snippet of code . As in previous years, DevCon ends with lightning talks in which anyone can present on a topic of their choosing provided it lasts for no more than 5 minutes. Lightning talks at DevCon can cover the gamut. Past lightning talks have covered Monads in Scala, a visual tour of Comic-Con, and world record strategies in Donkey Kong. We’ve also made available the DevCon 2012 keynote presentations, both from Dr. Jeff Norris , a scientist at NASA JPL responsible for the robotic spacecraft in the solar system. Norris talked on remaining agile while working on mission critical systems as well as the importance of specialization in the advancement of fields – both of these talks are well worth watching. DevCon allows engineers across organizations to come together to learn and present on a variety of topics, fostering a culture of innovation. It is no wonder then that many engineers at Cerner cite DevCon as their favorite and most anticipated event within Cerner.", "date": "2013-08-26"},
{"website": "Cerner", "title": "Project Lead the Way", "author": ["Chris Finn"], "link": "https://engineering.cerner.com/blog/project-lead-the-way/", "abstract": "Improving the state of healthcare through innovation requires investing in others to join you on the journey; not just for today, but for the decades to come. Project Lead the Way has established the Computer Science and Software Enginering course that teaches computational thinking to high school students, and it will pilot in 60 schools across the country this fall. Providing exposure to a wide variety of computational and computer science concepts, students can program a story or game in Scratch, write a mobile application for Android, and learn about knowledge discovery and data mining, computer simulation, cybersecurity, GUI programming, web development, version control, and agile software development. Recently, I had the privilege of working with experts across the country to define the curriculum for Project Lead the Way as well as the opportunity to sit in on the training sessions given to the pilot teachers, hosted at Cerner’s Innovation Campus. Bennett Brown, Director of Curriculum and Instruction, led the class into the deep end to solve the hard problems their students would face throughout the course. Experts augmented the training sessions from organizations such as Cerner, Lawrence Livermore National Laboratory, Purdue University, Carnegie Mellon University, University of Virginia, and more. While the Computer Science and Software Engineering course (CSE) isn’t focused specifically on programming, students will pick up introductory skills in a number of languages and environments including Python, MIT App Inventor, JavaScript, Logo, and Scratch among others. As software developers, we all have our origin stories with what sparked our interest in programming; there was a particular moment for each of us where we solved a problem and saw firsthand the power of what software can create. The CSE course seeks to generate those sparks through activities, projects, and problem-based learning. This course follows in the footsteps of other Project Lead the Way courses in engineering and biomedical science currently operating in over 4,700 schools with more than 10,000 trained teachers. My role in the class was to teach one simple section and provide some practical answers from the industry trenches. I’ll let you in on a secret: I’m pretty sure I learned more about teaching and how to teach novices than I taught on any particular topic. Problems and projects in PLTW are designed to be approachable by all students; there is no ceiling for the advanced students who want to go beyond what is specifically covered. The teachers receiving training were given the same end-of-unit problems that their students will face during the year and had to work out solutions in a pair programming environment, often working in their dorms or in the Cerner training rooms until 10pm. The experience level of teachers ranged from those having engineering degrees and multiple years of industry experience before going into teaching to some who had never programmed in their lives before this class. What I learned was that even in this environment of mixed skill levels, the activity/project/problem-based approach surprisingly requires just a minimal introduction to a new concept. After simple activities which build quick wins, natural curiosity swells up within each of the students; skill and understanding start to form, not just learning a particular technology but how to problem solve using software. For example, during the App Inventor sections, teachers received step-by-step simple examples of putting together programming blocks to create a simple Android application before working on homework to through one Agile sprint with their partner that evening to come up with their own application from scratch. The next morning, pairs went through their demos in a rotating show-and-tell fashion, showing a variety of apps including a flash card game, as well as a “one of these things is not like the other” game using the accelerometer for randomization and featuring the appropriate Sesame Street music. One night during the course I pulled down the Codea app for my iPad and showed my own kids the relationship between a few behaviors in a physics app and the corresponding code in Lua. Three hours later, all of the iPads in the house were engaged in hacking. In addition to some great instruction over the course from Bennett Brown on a variety of problems in genetics, chaos theory, Tkinter and GUI programming, other highlights included: Vic Castillo , Group Leader in the Quantitative Risk Analysis at Lawrence Livermore National Laboratories with a PhD in computational physics gave a great three hour survey presentation on what he sees as the “big three” topics in the future of computing: computer simulation and modeling (a subject near and dear to my own origin stories in computing), 3-D printing and mobile robotics featuring almost a dozen demos of cases where modeling was used to learn something new about a real-world system or design. Using just the NetLogo multi-agent programmable modeling environment developed at Northwestern University, Vic showed a variety of models that could be built with just a basic knowledge of the Logo programming language: for instance, modeling the heat absorption of a home design and how different designs and window configurations affect the internal temperature of the living space), “solving” the game Lunar Lander, or how a First Robotics team modeled the software algorithms incorporated into their TurtleBot (think Roomba + netbook + Kinect sensor). Joanne Cohoon talked about achieving diversity in STEM classrooms and the state of the job market–how 50% of the jobs available involve some need for computational thinking and the challenges and how teachers can create classroom environments that attract students who might otherwise rule themselves out from the start based on the stereotypes and environments commonly surrounding STEM in schools. With half the workforce needing some kind of computational skill, we can’t drive anyone away. Peter Chapman, a PhD student at Carnegie Mellon University and Technical Leader of the picoCTF project joined us via Skype to describe the program. Set up along the storyline of an interactive adventure called Toaster Wars, picoCTF is a competition among high school students to solve as many of 57 challenges as they can using whatever means necessary–hacking, decrypting, reverse engineering, breaking or whatever. The challenges force students to get their hands dirty, learning on their own via documentation and their browser the role of cookies in web security, how to use grep and tar to find a secret key in a file, and other examples designed to give students the experience of tackling unknown problems using the resources available to them. Cerner interns and former Lee’s Summit High School Team Driven First Robotics team members Dakota Ewigman and Victoria Utter taught a condensed version of the KC Power Source Android App Camp using MIT App Inventor which set the stage for more advanced App Inventor instruction provided by Dave O’Larte around calling and integrating web services and open APIs. We had developed a simple ballot and question and answer database-backed web service that let students and teachers hack around independently. In the category of unintended learning, and no doubt inspired by their picoCTF hacking, some of the teachers quickly found out how to troll other schools' services by injecting funny questions and answers about the course material. At one point during the week, Bennett Brown shared with me the growth potential for PLTW’s CSE program. In the engineering and biomedical engineering pathways, the year-over-year growth has been tremendous which is paving the way for an even faster uptake of the computer science offering. Such an important and widespread channel offers a great opportunity to influence the future of our entire industry and communities. With around 20% of the total pilot schools in this curriculum, Kansas City is getting a head start on building this pool of talent. We can use little advantages such as these to increase the value of KC as a hub for computing and entrepreneurial activity. An instructor asked me at one point, “What is Cerner looking for in high school students who take this course?” I had the sense that the question sought an answer more about a specific technology or language. I don’t really see that as the focus. Instead, I think what we want is defined more by sparking that interest in solving hard problems and coming away with the understanding that these problems exist in their community and that there are people in the community at companies like Cerner solving those problems every day. We have a lot to accomplish in “engineering health,” and K-12 outreach programs can feel like a long game that often take a back seat to immediate concerns around designing and shipping solutions. But seeing how even the less computing-savvy teachers started to get hooked on programming their Android devices or trying to get to the next level in picoCTF while beating their heads against man pages and HTTP dumps, it became clear to me that by getting involved in these programs we can accelerate not only the numbers of students comfortable with computational thinking, but also give them a network of relationships and experiences in their own communities to return to as their career and learning progresses. The long game may not be as long as we think.", "date": "2013-09-11"},
{"website": "Cerner", "title": "Cerner and Open Source", "author": ["Nathan Beyer"], "link": "https://engineering.cerner.com/blog/cerner-and-open-source/", "abstract": "(This post was written by Nathan Beyer, Bryan Baugher and Jacob Williams.) The use of open source software has become nearly ubiquitous in contemporary software development and it is no different for us, here at Cerner. We have been using open source software, directly and indirectly, for decades. Over the past decade, we’ve grown in maturity both in our use of open source software as well as our participation in open source communities. Our associates have long been contributors to open source communities, including helping users, logging bugs and enhancements, and submitting patches. Cerner associates also spearheaded the development of the Java Reference Implementation of the Direct Project . Recently, we’ve decided to take another step in the open source journey by releasing complete projects on our Github organization . Although these projects seem small, they are a big step for us and just the beginning of what we hope to open up and share in the future. We hope you’ll check out these projects and participate in their development. Source: https://github.com/cerner/knife-tar Knife-tar is a Chef tool for uploading and downloading Chef components from a tar file. It can be used for creating backups of your chef-server or for uploading released Chef artifacts from a repository. Source: https://github.com/cerner/scrimp Scrimp is a tool for interactively testing Thrift services in a web browser. It’s meant to fill the same role that browser-based REST clients fill for web services. Given the IDL files for the services, it provides a UI to help construct requests, invoke services, and display formatted responses.", "date": "2014-01-16"},
{"website": "Cerner", "title": "The Raft Protocol: A Better Paxos?", "author": ["David Edwards"], "link": "https://engineering.cerner.com/blog/the-raft-protocol-a-better-paxos/", "abstract": "Among the many compelling talks that attendees come to expect every year at the Strange Loop conference was a session given by Ben Johnson that provided an overview of a new distributed consensus protocol originating from research at Stanford University, named Raft . Distributed consensus can be described as the act of reaching agreement among a collection of machines cooperating to solve a problem. With the rise of open source distributed computing and storage platforms, consensus algorithms have become essential tools for replication, and thus, serve to enhance resiliency by eliminating single points of failure. Examples of distributed consensus in action can often be elusive because such protocols are ordinarily buried inside core systems, and consequently, are largely invisible to application developers. For example, a relational database in a clustered configuration would typically employ a consensus algorithm to coordinate commits with other replicas. And similarly, Apache ZooKeeper, a popular distributed synchronization service used in projects such as HBase and Solr, utilizes a consensus protocol to achieve fault-tolerance by replicating its configuration repository across many servers. The current Raft paper argues that while Paxos has historically dominated both academic and commercial discourse with respect to distributed consensus, the protocol itself is too complicated to reason about and that a more understandable algorithm was needed, not only for educational purposes, but also to serve as a foundation for building practical systems. An obvious question instinctively arises for the inquisitive reader: what makes Raft better than Paxos? Having personally implemented a replicated log using the Paxos algorithm, there was a natural curiosity in understanding how Raft approached the problem of solving consensus. It is worth noting, however, that comparing Raft and Paxos can be a bit misleading. Even though both address the fundamental problem of reaching consensus among a network of connected machines, Paxos is more academic in nature and primarily concerned with the mechanics of consensus, whereas Raft is oriented around the practical challenges of implementing a replicated log. The seminal work done by Leslie Lamport in 1989 with the design of the Paxos protocol was an important step forward in establishing a theoretical foundation for achieving consensus in asynchronous distributed systems. His contributions were largely academic and centered around reaching agreement on a single value, thus relying on software engineers to translate these ideas into practical solutions, such as replicated databases, which must decide on many values. However, the actual requirements necessary to build a real system, including areas such as leader election, failure detection, and log management, are not present in the Paxos specification, yet add a degree of complexity that almost always significantly alters the original protocol. This is precisely where the Raft designers correctly argue that the absence of specificity leads to great difficulty in applying Paxos to real world problems. A subsequent paper by Lamport in 2001 does an excellent job of making the original protocol accessible to practitioners, and to some degree, proposes techniques for designing a replicated log, but it stops short of being prescriptive in the way that Raft does. Raft is unique in many ways compared to typical Paxos implementations, but despite those differences, both are grounded in a similar set of core principles. For example, Raft requires leader election to occur strictly before any new values can be appended to the log, whereas a Paxos implementation would make the election process an implicit outcome of reaching agreement. The Raft designers claim that doing so has the consequence of simplifying log management, particularly with respect to edge cases in which a succession of leadership changes can result in log discrepancies, but the tradeoff is that leader election in Raft is more complicated than its counterpart in Paxos. What both protocols acknowledge, though, is that leader election is imperative if systems want to ensure progress. The notion of progress simply means that a system eventually does something useful. An important discovery in 1985, called the FLP Impossibility Result , proved that consensus was impossible in asynchronous distributed systems with the presence of only one faulty process. The practical implication follows: a system that cannot reach consensus is a system that cannot make progress. To be clear, the finding did not state that consensus was unreachable, just that some executions cannot reach consensus in bounded time. As a consequence, leader election, combined with timeouts, is often used as a technique for eliminating a class of conditions under which reaching agreement could take an arbitrarily long period of time. Interestingly, the Paxos algorithm as originally described by Lamport, makes no guarantees about progress, so implementations are compelled to incorporate timeouts as a compensatory measure. Raft, on the other hand, is prescriptive about the use of timeouts. One especially interesting component of the Raft specification is the mechanism for coordinating changes to cluster membership. The protocol employs a novel approach in which joint consensus is reached using two overlapping majorities, i.e. quorums defined by both the old and new cluster configuration, thereby supporting dynamic elasticity without disruption to operations. The emergence of Raft has clearly seen a positive embrace by the software development community as evidenced by nearly 40 open source implementations in a variety of different languages. Even though Paxos is beautifully elegant in describing the essence of distributed consensus, the absence of a comprehensive and prescriptive specification has rendered it inaccessible and notoriously difficult to implement in practical systems.", "date": "2014-01-24"},
{"website": "Cerner", "title": "Sponsoring the Apache Software Foundation", "author": ["Kevin Shekleton"], "link": "https://engineering.cerner.com/blog/sponsoring-the-apache-software-foundation/", "abstract": "Open source plays an integral role within engineering at Cerner. In addition to using open source software throughout our architecture, we recently released a few projects back to the community via our Github organization . Today, we’re happy to announce that Cerner is now a sponsor of the non-profit Apache Software Foundation (ASF). The ASF is home to several projects that are essential components in many of our systems. We’ve blogged previously about several of these projects: Hadoop, HBase, Crunch , Maven , and Storm . In addition to these projects, there are dozens of other ASF projects that we use and have contributed to via patches and enhancements. Sponsoring the ASF allows us to show support for their mission and to ensure the ASF can continue to provide infrastructure and resources for the open source projects they host.", "date": "2014-01-28"},
{"website": "Cerner", "title": "Migrating from Eclipse 3.X to Eclipse 4.X - The iAware Story", "author": ["Andy Gifford"], "link": "https://engineering.cerner.com/blog/migrating-from-eclipse-3.x-to-eclipse-4.x-the-iaware-story/", "abstract": "This is the blog form of the talk Migrating from Eclipse 3.X to Eclipse 4.X - The iAware Story at EclipseCon 2014 . The iAware development team was formed in late 2007 and in a little under six months we developed our first solution, CareAware CriticalCare, a dashboard application written using the Eclipse RCP targeted for use in ICU settings. The goal of this application was to provide clinicians with a complete picture of the patient’s status and to do it in a manner that was contextually relevant; meaning that related information was presented together. Doing so allows them to make rapid and timely clinical decisions. It was from this first solution that we began the process of building the software development platform. We’ve created a number of reusable and consumable assets that can simplify and speed up development; chief among them is our Application, Platform and Gadget frameworks. Our application framework manages the startup and initialization of applications built on the platform and allows those building solutions to define the layout (navigation bar and perspectives) of their running application. The platform framework provides management of the active application, contexts and navigation and is the connection point for gadgets to communicate with one another. The gadget framework is a wrapper for Eclipse views and provides a common set of operations and UI elements that provide a consistent look and feel across gadgets. It also handles context changes and user authorization for solutions. We provide two different application types: Dashboard and Personalized. The dashboard application type is intended to be shown on large form factor displays, typically in patient rooms in an always-on operation mode. This type lacks personalization options, such as moving views around or adding or removing them as multiple users will be using the application and a consistent look needs to be maintained. The personalized application type is intended for multiple form factors, but it’s primary use case is for laptops and mobile workstations with each user signing into the application with their own credentials. Because of this, we allow users to customize their perspectives by moving, adding or removing views. They can also add and remove perspectives and set preferences such as refresh time. At our core, we develop a reusable application platform built on top of the Eclipse Rich Client Platform to provide the ability to create targeted healthcare workflow applications with the goal of allowing other development teams to focus on solution specific development without worrying about the infrastructure. This means that our team can take on the responsibility and work effort of updating to new Eclipse versions as they become available without having to pass that cost down to those teams that build solutions on our platform. For the first four years or so, we did all of this platform development on top of the 3.x framework. A little over two years ago, we began the uplift process to 4.x, starting first with a feasibility study using 4.1.1 while taking advantage of the compatibility layer. While we wanted to dive in and have a straight e4 application, we had invested time and energy on platform projects based on 3.x that we couldn’t afford to scrap, so we took it one step at a time. Additionally, our team and other teams at Cerner had solutions that Clients were using that expect them to continue to function as they had previously regardless of what we did under the hood. During this feasibility study, we found around 50 items that we needed to address. Some issues were things that we were doing that happened to work in 3.x but no longer did in 4.x. Some were bugs that we either found already logged or we logged to Eclipse. A sampling of things found includes: One of the first things we found was that all views had a view menu whether there were items in it or not. After some checking, we found that there was a bug ( https://bugs.eclipse.org/bugs/show_bug.cgi?id=319621 ) logged for this, and we worked with other contributors to come up with a solution. After it was determined that the correction wouldn’t make it to 4.1.1, we modified the renderer (org.eclipse.e4.ui.workbench.renderers.swt) to never show the view menu for our views since we didn’t use that functionality anyway. Given the ability to save perspective layouts of our personalized applications, we quickly found that perspective saving was broken and identified a number of Eclipse bugs related to the problem. Again, because the issue wasn’t due to be fixed until 4.2, we did our own serialization of the perspective layout and saved that data off into our preference store. After uplifting to 4.2, we removed most of that code and instead use the perspective XML. We also came across another issue that was already logged ( https://bugs.eclipse.org/bugs/show_bug.cgi?id=356252 ) where a perspective will be closed when all of its parts are closed. To resolve this, we added the Cleanup add-on and implemented a patch that was posted to the logged bug. Another issue we ran into was with menu ordering. We had two different plug-ins contributing menu items to the main application menu and both of them declared that they were to be shown after the file menu item; however, we wanted one, Personalization, to show before the other, Help. Once we realized this, it was an easy fix to switch the declaration of the Personalization item to say that it should be before the Help item. While this was an issue with our code, it highlights the passivity problems that we had to be concerned about. Another menu related issue was a dynamic menu that we were building that lists all of the available views for a particular perspective only listed one element. After some investigating, we found that our contribution class was extending ContributionItem instead of CompoundContributionItem as suggested by the Eclipse wiki. We switched the class our contribution was extending and our menu was once again working as expected. Nonetheless we logged a bug ( https://bugs.eclipse.org/bugs/show_bug.cgi?id=354190 ) with respect to the ContributionItem since it was working in 3.7. A handful of issues that we encountered centered on the icons for our various views and how they weren’t being found. It was determined that the slash direction in the icon path was incorrect. Another set of issues that we encountered centered on having new functionality that wasn’t desired, such as extra preference pages in our preferences dialog and extra menu items in our help menu. An evaluation of the dependencies that were added corrected these issues. For a variety of reasons, detached views is a feature that we needed to remove from applications. In 3.x we used the IWorkbenchPreferenceConstants.ENABLE_DETACHED_VIEWS preference. However, this property isn’t supported in 4.x. Our workaround to this is to provide a custom implementation of the DnDAddon which takes away detached views altogether. We logged a bug for this situation: https://bugs.eclipse.org/bugs/show_bug.cgi?id=357289 We also found after uplifting that we had a number of jobs that started to fail sporadically due to authentication checks we had in place not having the necessary information. After investigating further, we found that some jobs that previously were executed after our users logged in were occurring before and as such threads in the job pool were being created without the correct subject in place and subsequent jobs would reuse these threads. We employed a two pronged approach to resolve this issue. We updated existing jobs to obtain the current subject on construction (from the access controller) of the job and then use the Subject.doAs call in the jobs run method. At the same time, we created an extension of the Job class that would do this for consumers. After demonstrating that we could move to 4.x, we began the process of making use of the new functionality that was available and to remove as much of our dependency on the compatibility layer as possible. To do that we added and customized the application model, defined a custom renderer to represent our UI and began removing extension points and implementations of 3.x interfaces in favor of dependency injection and behavioral annotations. We utilize the application model both statically and dynamically within our solutions. In the static file we define commands and handlers for Exit and Show View, define the add-ons we’re including and specify the top level UI elements and corresponding renderer. The add-ons that we consume include: The remaining UI elements, perspectives and parts, are contributed to the model dynamically through our application and gadget frameworks. Within our application we have a couple of shared UI areas that reside outside of the perspective area and as such we needed to define a custom renderer factory for our container to achieve the same functionality that was found using IWorkbenchWindowConfigurer#createPageComposite(org.eclipse.swt.widgets.Composite) in 3.x. Our renderer factory also removes the QuickAccess search field, a piece of functionality we don’t want to include in our applications The following diagram represents our UI model: The top pane, generally holds our navigation bar and toolbar contributions including lock and refresh. The bottom pane, generally holds our notification tray and status line UI elements. The iAware teams solutions don’t currently make use of the left or right pane in any of our solutions but leave those options open to others building their own solutions. The final piece of the uplift was to go through our various platform projects that were either implementing 3.x interfaces or were relying on singletons or static access to PlatformUI and use injection and behavioral annoations. This included changing our part implementation to no longer be an IViewPart and instead have it use the behavioral annotations @PostConstruct and @Focus. We also wired in a lifecycle hander to make use of @PostContextCreate and @ProcessAdditions across our registries (namely our perspective, gadget) instead of being tied to the calls from the WorkbenchAdvisor and WorkbenchWindowAdvisor. We also began use of the @Execute and @CanExecute annotations with a feature that we added to our gadget framework that allows solutions to contribute toolbar buttons for their gadgets. One annotation that we don’t make use of is the @Persist annotation as persistence is a feature that we avoid because we require users to start with a clean state each time they run the application. That brings me to that last topic, where do we go from here. We’ve begun the evaluation of 4.3 and have the evaluation of 4.4 on our roadmap when it becomes available. Specifically we’re working to bring in 4.3 before June in response to some changes to the rendering of menus that occurred between 4.1 and 4.2. We still define our menu items through extension points in a plugin.xml file and we’d created an abstract class that allows other solutions to change the default menu text that we provided (most wanted to change ‘iAware’ to ‘File’). Our application class then used this abstract class and the setText method on the menu item to change the text; however, we found in 4.2 that menu items that come from plugin.xml couldn’t be changed in this manner. So, when we make the move to 4.3, we’ll also change our menu contributions to come from the application model instead of extension points. We will also re-evaluate workarounds that we’ve added for earlier versions that are now fixed in the main line. The final item is something we’ve already been working on for a little while know but are really going after hard this year is moving to P2. We released our last version using features and products and we’re continuing to play with how we can best leverage them to deliver our solutions to clients. The introduction of Eclipse 4.x represented somewhat of a turning point for our team. While the process of uplifting was challenging at times, it was a great learning experience and it provided us the ability to enhance the functionality of the iAware platform, which was a huge benefit to our teams developing solutions. Integral to our ability to enhance the iAware platform was the fact that with 4.x we’re able to use native API where previously we wouldn’t have been able to accomplish it or it required us to use a workaround, usually entailing use of internal classes.  The work also lead to more involvement and participation in the Eclipse community by our team. We were involved in discussions in the forums, logged bugs and provided patches, which is a positive for all involved.", "date": "2014-03-18"},
{"website": "Cerner", "title": "ShipIt - 24-hour hackathon for Millennium+ Platform Dev", "author": ["Shahzad Zafar"], "link": "https://engineering.cerner.com/blog/shipit-hackathon-mplus/", "abstract": "At the end of March, some of our teams held their first 24-hour hackathon, titled ShipIt: Millennium+ Services FedEx Day. We had 41 participants, in 15 teams working on 15 unique projects. The idea was inspired by several teams spending a few hours every so often to work on different projects. After reading about Atlassian’s hack days, we decided to hold one. The event was initially announced early in February, to give teams time to work this into their project plans. The schedule was to start at 10 am on a Thursday and wrap-up at 10 AM on Friday. Teams then presented their awesome projects and then were free to leave for the weekend (and catch-up on some sleep). Each team was free to choose the project they wanted to work on, with the limitation added that they should work on something which can be deployed somewhere in 24 hours (there were bonus points involved for deployed projects).  The winning prize not only included bragging rights, but also the ‘Golden Keyboard’, which will be a traveling trophy. Behold, the Golden Keyboard: We had reserved a large room off campus to get everyone away from their daily routines. Teams immediately jumped into their projects as soon as the hack day started on March 27th. Plenty of food and snacks were on hand, with lunch and dinner delivered to keep everyone fed. A hackavision dashboard (Sinatra application which subscribed to a list of atom feeds) was created, to track all the github commits by the teams. The projects had amazing breadth. These include Neo4j , Riemann , Capistrano and languages such as Ruby and Clojure. Teams not only learned new languages and projects in the 24 hours, but also had most of them fully functional and deployed at the end of the hackathon. There were other activities as well, such as playing Xbox and watching the NCAA Basketball Tournament, which provided to be great breaks throughout the night. Motivational movies, like Robin Hood: Men in Tights , were also on tap through the night. By 10 am on Friday morning on March 28th, everyone was ready to present their projects. We had four judges representing different areas of expertise. The demos were awesome and judges had a tough time picking the top three projects. Third place went to the SplunkOverflow team, who worked on a Maven plugin that would build site documentation for Thrift RPC services. Second place went to the Short Circuit team, who improved the performance of hash calculations in our Storm topologies. First place (and the Golden Keyboard) went to the Minions team, who created “lando”, a set of services that supported monitoring and management tasks on JVM-based Thrift RPC services. All in all, it was an exciting 24 hours where teams showed their innovative abilities. All the projects were demo’ed to a larger audience about a week later. Most of the projects started on during ShipIt are being enhanced further, by the teams during their team level hack time or scheduled projects. This was our first hackathon, but it won’t be our last! We will have at least one more later this year and plan on a recurring event, with the Golden Keyboard traveling around with the winning team. It was amazing to see what people can do in a short amount of time and with the flexibility of choosing what you want to work on, the end result will always be something cool. We hope to continue the innovative thinking, not only by team level hack days, but having larger hack days.", "date": "2014-07-01"},
{"website": "Cerner", "title": "The Plain Text is a Lie", "author": ["Elliott Hoffman"], "link": "https://engineering.cerner.com/blog/the-plain-text-is-a-lie/", "abstract": "“But I see .txt files all the time” you say. “My source code is plain text” you claim. “What about web pages?!” you frantically ask. True, each of those things is comprised of text. The plain part is the problem. Plain denotes default or normal. There is no such thing. Computers store and transmit data in a number of methods; each are anything but plain . If you write software, design websites or test systems where even a single character of text is accepted as input, displayed as output, transmitted to another system or stored for later - please read on to learn why the plain text is a lie! The topic of text handling applies to many disciplines: After reading this article you will … This topic has been extensively written about already. I highly recommend reading Joel Spolsky’s The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) . You should also read up on how your system handles strings. Then, go read how the APIs you talk to send/receive strings. Pythonistas, check out Ned Batchelder’s Pragmatic Unicode presentation. OK, let’s get started! Let’s start off by demonstrating how text handling can fail, and fail hard. The following screen shots and snippets show some of the ways text handling can fail and who should care about the type of failure. The above image shows the English wikipedia article on Résumés with garbled text. Garbled text can happen if your web pages don’t specify an encoding or character set in your markup. Specifying the wrong encoding can also cause garbled text. XML and JavaScript need correct character sets too. It’s important to note that no error or exception was raised here . The text looks wrong to the user, but the failure happens silently. This article on Tokyo above is displayed in a language (Aramaic) that my fonts don’t support. Instead of a symbol, we see a box with a number identifying the un-showable character. If you think that example is too contrived, here is a more commonly used symbol: a 16th note from sheet music . Many perfectly valid characters are not supported by widely used fonts. Specialized web fonts might not support the characters you need. The result of this API call ( example source ) is similar to the last two examples: nonsense text. This can happen if the client and server use different text encodings. By the way, this situation happens so often that there’s a term for it: Mojibake . Here are some client/server scenarios resulting in Mojibake: Database systems can be misconfigured such that characters sent to the database are not stored accurately. In this example, the offending characters are replaced with the imaginatively-named Replacement Character (“�”). The original characters are forever lost. Worse still, replacement characters will be returned by your queries and ultimately shown to your users. Sometimes, offending characters will be omitted from the stored value or replaced with a nearest match supported character. In both scenarios the database has mangled the original data. The top image shows the 500 page of an app that crashed when improperly encoding. In the Scala error message (bottom), a property file was read in ISO-8859-1 encoding but had UTF-8 encoded bytes in it. This caused the unit test to fail. Your source code, web pages, properties files, and any other text artifact you work with has an encoding. Every tool in your development tool chain (local server, terminal, editor, browser, CI system, etc.) is a potential failure point if these encodings are not honoroed. You’ve seen examples of failure and (hopefully) are wondering how such failures can be avoided. To avoid failure you must ask yourself one question: “Can my system store and transmit a ghost?” GHOST (code point U+1F47B ) is a valid (albeit weird) part of the Unicode standard. Unicode is a system of storing and manipulating text that supports thousands of languages.  Using Unicode properly will go a long way to prevent text handling problems. Thus, if your system can store, transmit, read and write GHOST then you’re doing it right. But how to handle this GHOST? You need to know some terms before the rest of this article will make any sense. Remembering the difference between encode and decode can be difficult. One trick to keep them straight is to think of Unicode objects as the ideal state of being (thanks, Joel Spolksy ) and byte-strings as strange, cryptic sequences. Encoding turns the ideal into a bunch of cryptic bytes, while decoding un-weirds a bunch of bytes back into the ideal state; something we can reason about. Some systems use different terms but the ideas still apply. For example: Java Strings are Unicode objects and you can encode/decode to/from byte-strings with them. Now that you’ve got the necessary terminology under your belt, let’s prevent text handling problems in our system by making a sandwich; a Unicode sandwich! Analogy credit : Ned Batchelder coined the Unicode sandwich analogy in his Pragmatic Unicode presentation at PyCon 2012 ( video ). It’s so clever that I can’t resist re-using it in this article! In this analogy the pieces of bread on the top and bottom are regions of your code where you deal with byte-strings. The meat in the middle is where your system deals in Unicode objects. The top bread is input into your system such as database query results, file reads or HTTP responses. The bottom bread is output from your system such as writing files or sending HTTP responses. The meat is your business logic. Your goal is to keep the bread thin and the meat thick. You can achieve this by decoding from byte-strings to Unicode objects as early as you can; perhaps immediately after arrival from another system. Similarly, you should do your encoding from Unicode objects into byte-strings at the last possible moment, such as right before transmitting text to another system. Working with Unicode inside your system gives you a common ground of text handling that will largely avoid the errors we’ve seen at the top of this article. If you don’t deal in Unicode inside your system then you are limiting the languages you support at best and exposing yourself to text handling bugs at worst! Your system ultimately needs to send and receive byte-strings at some point, so you must choose an encoding for your byte-strings. Encodings are not created equal! Some encodings only support one language. Some support only similar languages (for example, German and French but not Arabic). Never assume your system will only encounter languages you speak or write! Ideally you will choose encodings that support a great many languages. UTF-8 is the best general purpose encoding for your byte-strings. You’ll learn why UTF-8 is an excellent encoding choice later in this article in the Unicode: One standard to rule them all section. For now I recommend you: The UTF-8 encoding supports all the text you’d ever want. Yet, in this imperfect world you might be forced to use a more limited encoding such as ISO-8859-1 or Windows-1252 when interfacing with other systems. Working with a limited encoding presents problems when decoding to and encoding from Unicode: not every encoding supports the full Unicode range of characters. You must test how your system converts between your byte-strings and Unicode objects. In other words, test between the meat and the bread . The critical areas to test are where bytes strings are decoded to Unicode objects and where Unicode objects are encoded into byte-strings. If you’ve followed the advice of this article thus far then the rest of your app logic should operate exclusively in Unicode objects. Here is a handy table of how to test regions of your system that encode and decode: Using UTF-8 for I/O, Unicode inside and testing the in-between points will save you from pain and bugs. If you’re building a new system then you have the opportunity to design it with Unicode in mind. If you have an existing system, it is worth your time to audit how your system handles text. With the practical stuff out of the way, let’s dive deeper into computers and text! We’ve talked about how you should use Unicode, encodings and byte-strings in your system to handle text. You may be wondering why text handling is so painful at times. Why are there so many encodings and why don’t they all work together in harmony? I’ll attempt to explain a bit of history behind text handling in computers. Understanding this history should shed some light on why text handling can be so painful. To make things interesting, let’s pretend we are inventing how computers will handle text. Also assume we live in the United States and speak only English. That’s a pretty ignorant assumption for real world software development, but it simplifies our process. Our challenge is to invent how computers handle text. Morse code is an encoding that pre-dates digital computers but provides a model for our approach: Each character has a transmission sequence of dots and dashes to represent it. We’ll need to make a few changes and additions though… Rather than dots and dashes we can use 1’s and 0’s ( binary ). Let’s also use a consistent number of bits per character so that it’s easy to know when one character ends and another begins. To support US English we need to map a binary sequence to each of the following: That’s 96 printable characters and some control characters for a total of 128 characters. 128 is 2 7 , so we can send these characters in seven-bit sequences. Since computers use eight-bit bytes, let’s decide to send eight bits per character but ignore the last bit. We have just invented the ASCII encoding ! ASCII forms the root influence of many text encodings still used today. In fact, at one time ASCII was the law: U.S. President Lyndon B. Johnson mandated that all computers purchased by the United States federal government support ASCII in 1968. We need more space to pack in more symbols if we want to support other languages and other symbols like currencies. It seems reasonable that people typically deal with a block of languages that are geographically or politically related, and when we’re lucky those languages share many of the same symbols. Given that assumption we can create several standards; each one for a block of languages! For each block, we can keep the first 128 characters as-is from ASCII (identical bit sequences) so that the US English characters and Arabic numerals are still supported. We can then use the eighth bit for data instead of ignoring it. That would give us eight bits per character and a total of 256 characters to work with (double ASCII’s paltry 128). Now let’s apply that eight bit. A bunch of countries in Western Europe use the same latin alphabet plus special diacritics (also known as accent marks ) like ü or é or ß. In fact, we can pack enough extra characters in those last 128 slots to support 29 other languages like Afrikaans, German, Swahili and Icelandic. Our Western European language block encoding is ready! We call this type of encoding a single-byte encoding because every character is represented by exactly one byte. We can repeat the same process we used to create our Western European language encoding to develop other single-byte encodings for other language blocks; each a 256 character set! To give one more example, let’s build a single byte coding for Arabic. Again, we take the first 128 ASCII characters as-is, then fill up the last 128 with the Arabic alphabet We’ve got some space left over. Arabic has some diacritics as well, so let’s use some of the leftover slots to hold diacritic marks that are only valid when combined with other letters. Some languages don’t even fit in 256 characters. Chinese, Japanese and Korean for example. That’s OK, we’ll just use multiple bytes per character to get more room. As you may have guessed, these encodings are called multibyte encodings . Sometimes we choose to use the same number of bytes for every character ( fixed width multibyte encodings) and sometimes we might choose to use different byte lengths ( variable width multibyte encodings) to save space. After we’ve built several of these encodings (Russian, Greek, Simplified Chinese, etc.) we can ratify them as international standards such as ISO-8859 for single byte encodings. We previously built ISO-8895-1 (Western European) and ISO-8859-6 (Latin/Arabic) . International standards for multibyte encodings exist too. People who use the same standard can communicate without problems. The international standards like ISO-8895 are only part of the story. Companies like Microsoft and IBM created their own standards (so-called OEM standards or code pages ). Some OEM standards map to international standards, some almost-but-not-quite map (see Windows-1252 ) and some are completely different. Our standards and code pages are better than ASCII but there are a number of problems remaining: Enter Unicode. As mentioned earlier, Unicode is a single standard supporting thousands of languages. Unicode addresses the limitations of byte encodings by operating at a higher level than simple byte representations of characters. The foundation of Unicode is an über list of symbols chosen by a multinational committee. Unicode keeps a gigantic numbered list of all the symbols of all the supported languages. The items in this list are called code points and are not concerned with bytes, how computers represent them, or what they look like on screen. They’re just numbered items, like: a LATIN SMALL LETTER A - U+0061 東 Pinyin: dōng, Chaizi: shi,ba,ri -  U+6771 ☃ SNOWMAN - U+2603 We have virtually unlimited space to work with. The Unicode standards supports a maximum of  1,114,112 items. That is more than enough to express the world’s active written languages, some historical languages and miscellaneous symbols. Some of the slots are even undefined and left to the user to decide what they mean. These spaces have been used for wacky things like Klingon and Elvish. Fun fact: Apple Inc. uses U+F8FF in the Private Use Area of Unicode for their logo symbol (  ). If you don’t see the Apple logo in parenthesis in the preceding sentence, then your system doesn’t agree with Apple’s use of U+F8FF. OK, we have our gigantic list of code points. All we need to do is devise an encoding scheme to encode unicode objects (which now we know are lists of code points) into byte-strings for transmission over the wire to other systems. UTF-8 encodes every Unicode code point in between one and four byte sequences. Here are some cool features of UTF-8: UCS-2 is a fixed width, two-byte encoding. In the mid-nineties, Unicode added code points that cannot be expressed in the two-byte system. Thus, UCS-2 is deprecated in favor of UTF-16. UTF-16 extends UCS-2 by adding support for the code points that can’t be expressed in a two-byte system. You can find UTF-16 in: UTF-32 is a simple 1:1 mapping of code points to four-byte values. C Python uses UTF-32 for internal representation of Unicode if compiled with a certain flag. We’ve seen how text handling can go wrong. We’ve learned how to design and test our systems with Unicode in mind. Finally, we’ve learned a bit of history of text encodings. There is a lot more to the topic of text, but for now I ask you do to the following: Thanks for reading!", "date": "2014-08-02"},
{"website": "Cerner", "title": "Scaling People with Apache Crunch", "author": ["Micah Whitacre"], "link": "https://engineering.cerner.com/blog/scaling-people-with-apache-crunch/", "abstract": "When a company first starts to play with Big Data it typically involves a small team of engineers trying to solve a specific problem.  The team decides to experiment with scalable technologies either due to outside guidance or research which makes it applicable to their problem.  The team begins with the basics of Big Data spending time learning and prototyping.  They learn about HDFS , flirt with HBase or other NoSQL, write the required WordCount example, and start to figure out how the technologies can fit their needs.  The group’s immersion into Big Data deepens as they start to move beyond a prototype into a real product. The company sees the success of using Big Data technologies and the possibilities to solve difficult problems, tackle new endeavors, and open new doors.  The company has now shifted its scalability problem out from the technical architecture into a people problem.  The small team of experts cannot satisfy the demand and transferring their accumulated knowledge to new teams is a significant investment.  Both new and experienced engineers face a steep learning curve to get up to speed.  Learning the API is not difficult but challenges typically oocur when applying the WordCount example to complex problems.   Making the mental jump from processing homogeneous data which produce a single output to a complex processing pipeline involving heterogeneous inputs, joins, and multiple outputs is difficult for even a skilled engineer. Cerner has developed a number of Big Data solutions each demonstrating the 3 V’s of data (variety, velocity, and volume).  The complexity of the problems being solved, evolving functionality, and required integration across teams led Cerner to look beyond simple MapReduce.  Cerner began to focus on how to construct a processing infrastructure that naturally aligned with the way the processing is described.  Looking through the options available for processing pipelines including Hive , Pig , and Cascading , Cerner finally arrived at using Apache Crunch .  Using Crunch’s concepts, we found that we were easily able to translate how we described a problem into concepts we can code.  Additionally the API was well suited for our complicated data models and building integration contracts between teams. When we describe a problem we often talk about its flow through the various processing steps.  The processing flow is comprised of data from multiple sources, several transformations, various joins, and finally the persistence of the data.  Looking at an example problem of transforming raw data into a normalized object for downstream consumers we might encounter a problem similar to the diagram below. If we apply this problem to the raw MapReduce framework we begin to see problems absent in the standard WordCount example.  The heterogeneous data and models, the custom join logic, and follow up grouping by key all could result in extra code or difficulty fitting this processing into a single MapReduce job.  When our problem expands to multiple MapReduce jobs we now have to write custom driver code or bring in another system like Oozie to chain the workflow together.  Additionally while we could fit the steps neatly into 1-2 MapReduce jobs that careful orchestration and arrangement could become imbalanced as we introduce new processing needs into the workflow. This problem is common and fairly basic with respect to some of the processing needs we faced at Cerner.  For the experienced MapReduce developers this problem might cause a momentary pause to design it but that is due to your expertise.  For those less skilled imagine being able to break this problem down into the processing steps you understand and POJO like models of which you are already familiar.  Breaking this problem down using Apache Crunch we can see how we can articulate the problem and still take advantage of the processing efficiency of MapReduce. Apache Crunch allows developers to construct complicated processing workflows into pipelines .  Pipelines are directed acyclic graphs (DAG) comprised of input data that is then transformed through functions and groupings to produce output data.  When a developer is done constructing the pipeline Apache Crunch will calculate the appropriate processing steps and submit the steps to the execution engine.  In this example we will talk about using Apache Crunch in the context of MapReduce but it also supports running on Apache Spark .  It should be noted that pipelines are lazily executed.  This means that no work will be done until the pipeline is executed. To begin processing we need a pipeline instance on which we will construct our DAG.  To create a MRPipeline we need the typical Hadoop Configuration instance for the cluster and the driver class for the processing. With a pipeline instance available the next step is to describe the inputs to the processing using at least one Crunch Source .  A pipeline must contain at least one source but could read from multiple.  Apache Crunch provides implementations for the typical inputs such as Sequence Files, HFiles, Parquet, Avro, HBase, and Text.  As an example if we were to read data out of a text file we might write code like the following: This code utilizes the TextFileSource to generate a collection of Java Strings from files at a certain path.  The code also introduces two additional Apache Crunch concepts of PCollections and PTypes .  A PCollection represents potential data elements to process.  Since a pipeline is lazily executed it is not a physical representation of all of the elements.  A PCollection cannot be created but can be read or transformed.  Apache Crunch also has special forms of PCollections, PTable and PGroupedTable, which are useful in performing join operations on the data.  A PType is a concept that hides serialization and deserialization from pipeline developers.  In this example the developer is using native Java strings instead of dealing with wrapper classes like Writable’s Text class. Processing based off of Java Strings is error prone so typically developers would transform the data into a model object that is easier to work with.  Transformation of a PCollection is done through a DoFn .  A DoFn processes a single element of the PCollection into zero or many alternate forms depending on its logic.  The bulk of a processing pipeline’s logic resides in implementations of DoFn.  Custom implementations of a DoFn requires extending the DoFn class as well as defining the input and output types.  This allows Crunch to provide compile time checking as transformations are applied to collections. Recalling the previous example processing problem we see that we need to perform join and grouping operations based on a key.  Instead of converting the strings into a RefData object it would actually be better to convert the string into a key/value pair (e.g. Pair<String, RefData>).  Apache Crunch has a PTable<K, V>, which is simply a special form of PCollection<Pair<K, V».  Adjusting the function we can instead produce the key/value pair. Utilizing the PTable<String, RefData> collection we could then join that collection with another similarly keyed PTable using one of the many prebuilt implementations.  The built in join functionality helps to avoid developing custom implementations of a common data processing pattern. More functions are applied to the joined data to continue the processing workflow.  The processing of the data is distributed in separate tasks across the cluster.  In the example problem we need all of the data for a given key to be grouped to a single task for processing. A pipeline requires at least one collection of data to be persisted to a target .  Crunch provides the standard targets for data but consumer can also easily create new custom inputs. When constructing the processing pipline for the example problem we would end up with an executable program that looks like the following: It is very easy to see how the processing steps and flow of the original diagram can be mapped to Apache Crunch concepts. Executing the code as written above would cause Apache Crunch to calculate the execution graph to be spread over two MapReduce jobs. In this case having a MapReduce job being reduce only is less than ideal but the team has been able to focus on correctness and functionality first.  Focus can now be shifted on performance tuning or adjusting algorithms as appropriate.  The solid foundation of functionality and the simplicity of the concepts allows developers to easily understand how the processing pipeline.  The ease of understanding helps to allow the team to refactor and iterate with confidence. This blog post is essentially my script for my North America ApacheCon 2014 presentation .  Slides are available here .", "date": "2014-05-09"},
{"website": "Cerner", "title": "Intern HackFest 2014", "author": ["Makenzie Kalb"], "link": "https://engineering.cerner.com/blog/intern-hackfest-2014/", "abstract": "Ten teams of two to four Cerner interns competed in a week-long HackFest this summer, working to solve any problem they put their minds to. This competition cumulated in a presentation and judging of projects, with prizes of Raspberry Pi Kits for each member of the second place team and Leap Motions for each member of the winning team. From mobile apps, to machine learning algorithms, to drones…this year’s Summer Intern HackFest has been one for the books. We called ourselves Team Rubber Duck Dynasty, and it was made up of Umer Khan (University of Notre Dame), Ryan Boccabella (University of Notre Dame), MaKenzie Kalb (Vanderbilt University), and Jake Gould (University of Kansas). We were excited to get to work the first night when the week-long competition had commenced. Since the beginning of the summer, all of us had been impressed with the caliber of talent Cerner brought into the Software Engineer Internship program. All of the nine teams we were up against were made up of remarkably smart, driven college students from all over the country. One of the most difficult parts of the HackFest was deciding on an interesting and competitive project that could be feasibly completed in only a week (without too many sleepless nights). One of our four team members was a member of the iOS team, and convinced us that an iOS game was the way to go. We wanted to make a game that we would be excited to show our friends as well as the judges. We ended up building an app called Encore. It is a musical turn-based game revolving around the creation and mirroring of three second tunes between users. Tunes are created using four arpeggio based tones from real piano, guitar, or tenor trombone recordings. The initiating iOS device and sends the data to the Parse server using the Parse API for iOS. Parse stores this data on the server and sends a push notification to the receiving iOS device. Each time a new game is created, an activity is logged on the server to keep track of the game data.  When the receiving user selects the game, it downloads the game data from the server and starts the game. Once the app downloads the game data, it is programmed to decode an array of dictionaries of instrument key and time and convert the array into an audio playback; this allowed for faster upload and download times, as well as significantly smaller game data files. The receiving user hears and immediately attempts to replay the tune. Scoring is accomplished using a Needleman-Wunsch algorithm for sequence alignment. The receiving user now has their chance to create a tune, and the melodious competition continues. Over the week, we began to get to know our teammates even more than we probably wanted. Passion is the main word that comes to mind when we reminisce on this highlighting week of our summer. From the uncertainty when overhearing other groups huddled in a room talking excitedly about cutting-edge technologies, to the shrieks of excitement when a test finally passed that perhaps woke many a consulting intern roommate, this HackFest was filled with memories all around. As we went for a celebratory completion dinner the night before the presentations Monday morning, the satisfaction of completion was sweet in the air. Sitting there, playing our noisy pride and joy on our phones at the table, we agreed that the week was an excellent experience already…and we hadn’t even started the real judging yet. Sound checks were full of nerves and excitement the morning we presented our project. The knowledge that each team had a mere five minutes to “sell” what had been more time consuming than sleep over the past week was a challenge everyone was hoping to ace. Later on that afternoon, when the esteemed judges Chris Finn, Michelle Brush, and Jenni Syed were announced as the event began, the caliber of the resources Cerner provides for their many interns was standing right in front of us. We heard from many enthusiastic, impressive groups that afternoon. The presentations showcased many feats of great teamwork and skill: a recommendation engine, dashboard for developers, chatting website, facial recognition android app, iOS game, machine learning algorithm, twitter-controlled drone, and music website. After a delicious ice cream break while scores were deliberated and after judges provided valuable feedback for each team, the moment of anticipation was upon us. All teams certainly completed the day with the ultimate reward of new skills learned, friends made, and a fantastic project that some are undoubtedly still building off of. As the first and second place teams were called to the stage, Team Rubber Duck Dynasty was surprised and thrilled to be among them. And as the runner up, Team Marky Mark and the Funky Bunch, received their Raspberry Pi Kits, we were amazed to find out each of us was taking home our very own Leap Motion. We returned to our actual teams late that afternoon, proud of our accomplishments and brand new owners of a cutting-edge technology. We received the congratulations of our superiors and mentors, many of whom were our biggest encouragers to participate and supporters throughout the week. The numerous empowered associates that have guided us through this summer have been an unbelievable community - a community that all of us are incredibly grateful to have been a part of.", "date": "2014-09-08"},
{"website": "Cerner", "title": "Closures & Currying in JavaScript", "author": ["Rory Hardy"], "link": "https://engineering.cerner.com/blog/closures-and-currying-in-javascript/", "abstract": "I have been asked many times what closures are and how they work. There are many resources available to learn this concept, but they are not always clear to everyone. This has led me to put together my own approach to exchanging the information. I will supply code samples. //> denotes an output or return. Before discussing closures, it is important to review how functions work in JavaScript. If a function does not have a return statement, it will implicitly return undefined, which brings us to the simplest functions. Noop typically stands for no operation; it takes any parameters, does nothing with them, and returns undefined. The identity function takes in a value and returns it. The important thing to note here is that the variable (value) passed in is bound to that function’s scope. This means that it is available to everything inside the function and is unavailable outside of it. There is an exception to this, being that objects are passed by reference which will prove useful with the use of closures and currying. Functions are first class citizens in Javascript, which means that they are objects. Since they are objects, they can take functions as parameters, have methods bound to them,  and even return functions. This is a function that returns a function which returns true. Functions take arguments and those arguments can be values or reference types, such as functions. If you return a function, it is that function you are returning, not a new one (even though it might have just been made to return). Creating a closure is nothing more than accessing a variable outside of a function’s scope (using a variable that is neither bound on invocation or defined in the function body). To elaborate, the parent function’s variables are accessible to the inner function. If the inner function uses its parent’s (or parent’s parent’s and so on) variable(s) then they will persist in memory as long as the accessing functions(s) are still referenceable. In JavaScript, referenceable variables are not garbage collected. Let’s review the identity function: The value, a, is bound inside of the function and is unavailable outside of it; there is no closure here.  For a closure to be present, there would need to be a function within this function that would access the variable a. Why is this important? Due to these strengths, and many more, closures are used everywhere. Many popular libraries utilize them internally. Let’s take a look at an example of closure in action: The outer function (foo) takes a variable (x), which, which is bound to that function when invoked. When the internal function (bar) is invoked, x (2) and y (2) are added together then logged to the console as 4. Bar is able to access foo’s x-variable because bar is created within foo’s scope. The takeaway here is that bar can access foo’s variables because it was created within foo’s scope. A function can access variables in its scope and up the chain to the global scope. It cannot access other function’s scopes that are declared within it or parallel to it. No, a function inside of a function doesn’t have to reference variables outside of its scope. Recall the example function which returned a function which evaluated to true: No matter what is passed to foo, a function that evaluates to true is returned.  A closure only exists when a function accesses a variable(s) outside of its immediate scope. This leads into an important implication about closures, they enable you to define a dataset once. We’re talking about private variables here. Without closures, you recreate the data per function call if you want to keep it private. We can do better! With a closure, we can save it to a variable that is private, but only instantiated once. By utilizing closure here, our big imaginary data set only has to be created once. Given the way garbage collection (automatic memory freeing) works in JavaScript, the existence of the internal function (which is returned and set to the variable bar) keeps the private variable from being freed and thus available for subsequent calls. This is really advantageous when you consider large data sets that may be created via Ajax requests which have to go over the network. Currying is the process of transforming a function with many arguments into the same function with less arguments. That sounds cool, but why would I care about that? Let’s pretend that we have a function (curry) defined and set onto the function prototype which turns a function into a curried version of itself. Please note, that this is not a built in feature of JavaScript. By currying the msg function so the first variable is cached as “Hello,”, we can call a simpler function, hello, that only requires one variable to be passed. Doesn’t this sound similar to what a closure might be used for? In the discussion of functional programming concepts, there is often a sense of resistance. The thing is, you’ve probably already been functionally programming all along. If you use jQuery, you certainly already do. Another place you may have seen this is utilizing the map function for arrays. We’ve seen some examples of closures and how they can be useful. We’ve seen what currying is and more importantly that you’ve likely already been functionally programming even if you didn’t realize it. There is a lot more to learn with closures and currying as well as functional programming. I ask you to: Check out how you can utilize closure and currying to manage state throughout a stateful function: Checkout how closures and currying can be used to create higher order functions to create methods on the fly: http://jsfiddle.net/GneatGeek/A9WRb/", "date": "2014-09-29"},
{"website": "Cerner", "title": "Cerner and the Apache Software Foundation", "author": ["Kevin Shekleton"], "link": "https://engineering.cerner.com/blog/cerner-and-the-apache-software-foundation/", "abstract": "At the beginning of this year, [we announced that Cerner became a bronze-level sponsor]({%post_url 2014-01-28-sponsoring-the-apache-software-foundation%}) of the non-profit Apache Software Foundation (ASF). Many of the open source projects we use and contribute to are under the ASF umbrella, so supporting the mission and work of the ASF is important to us. We’re happy to announce that Cerner has now increased our sponsorship of the ASF to become a silver-level sponsor . Open source continues to play an integral role in both our architecture and engineering culture. We’ve blogged and spoken at conferences about how several ASF projects are core foundational components in our architecture and several of our tech talks have focused on ASF projects. Further increasing our sponsorship of the ASF reaffirms our continued support for an organization that provides homes for numerous open source projects that are important not only to us, but the larger development community. ASF Silver Sponsorship", "date": "2014-10-28"},
{"website": "Cerner", "title": "Managing 30,000 Logging Events per Day with Splunk", "author": ["Mike Hemesath", "Rima Poddar"], "link": "https://engineering.cerner.com/blog/managing-30000-logging-events-per-day-with-splunk/", "abstract": "Our team works on a patient facing web application with a thousand live clients with 2,315,000+ users. On an average, the high traffic results into more than 40,000 visits and 300,000 page views daily generating about 30,000 logging events. A considerable portion of these events are of information and warning level in order to aid proactive monitoring or identify potential issues due to clients’ misconfiguration. To handle this large volume of logging, our team created a rotational support role to manually monitor the logs at regular intervals daily. We built a custom log viewer that would aggregate application logs and the engineer on the support role was expected to watch this tool manually to identify issues. Although we were able to identify problems such as bad client builds or service errors, it was not very efficient nor accurate in quickly determining end user impact stats. Since there was no way to tag previously identified and resolved issues, often times newer support engineers lacked knowledge to react to a problem. This led to unnecessary escalation and engagement of next tier support. Below: Our old log aggregator used to identify the occurrences (2) of a log with the stack trace. Once we migrated to Splunk we were very excited about the capabilities it offered, especially around searching and data visualization. In addition to searching logs more effectively, we were able to extract meaningful information from our logs unlike before. Splunk gave us the ability to identify client and end user impact down to the user id across all events in our logs [see image below]. This helped us gain more insight into our problems and trends in terms of impact to users. For a particular problem, we were able to quickly conclude whether all clients were affected, whether clients affected were over a virtual network only, and if the issue was isolated to a specific user. This information gave us the ability to determine the impact of issues coming into our logs, including the area of the site being impacted and frequency. Below: Once we extracted meaningful fields in our logs, we could identify impact. In this case, an issue is spread across 9 orgs and and 28 users. Although we had crossed some of the hurdles which made log monitoring difficult shortly after moving to Splunk, monitoring logs for issues was still not an easy job. It was possible to overlook issues since there was no effective way of studying trends. Initially, we created dashboards which helped identify organizations having problems. This was slightly useful but failed to depict more important graphical representation of the different types of occurring issues for a particular client or for all clients at a given time. Below: Reports like these weren’t very helpful. Clients with more users tend to have more errors, so this trend doesn’t necessarily indicate a client is experiencing a downtime. It didn’t take us long to realize that we had to get a better handle on our logs to stay on top of increasing traffic caused by a growing user base. Although we were able to identify frequently occurring errors, we still needed a more effective way to identify known issues, service issues, configuration issues and application issues. In order to do that, we needed something more meaningful than a stack trace to track issues.  We needed to tag events, and to do that, we turned to the eventtypes feature offered by Splunk. Eventtypes are applied at search time and allow you to create events to tag search results with. Because they are applied at search time, we were able to add new event types and have them applied historically throughout our logs. This also gave us the ability to tweak our event types to add more known issues as we continued identifying them. Once we successfully gauged a way to take advantage of eventtypes, we came up with a query that created a stacked timechart of eventtypes where eventtypes represented known issues. Once we reached the improved level of production monitoring, the following had to be done: Below: Eventtypes give us the ability to see known problems that happen over time. We can even see known problems broken down by client. Once we had our frequently occurring problems categorized, we were able to break it down even further. We could identify problems caused by configuration in our application layer, problems that required escalation or if client side contacts needed to be engaged. Below: We now have the ability to track impact to users from clients not taking a service package [left], or from improper Service Configuration [right]. We’ve also started taking advantage of Splunk’s alerting. With its powerful searching abilities, we have scheduled searches that trigger an alert when a particular condition is met. For example, when a client has misconfigured certain credentials that cause authentication errors all over the site, we can engage support immediately to get it resolved. Although we have a better understanding of our logs now, it can get even better. We plan on continually categorizing our logs so that monitoring our system becomes really simple for everyone. Once all of our known issues are categorized, we wish to have a scheduled search that can identify anomalies in the logs. This would be highly beneficial to find out if a release introduces issues. Since our site is dependent on multiple services, most of the service problems are resolved by escalated support. We are currently working on identifying problems with known resolutions  along with  the people that need to be contacted to perform the resolution steps. Eventually we would like to send alerts/emails from Splunk to Cerner’s general support directly for these issues. We also plan on integrating Jira into splunk with the help of the Splunk Jira app. This will give us the ability to not only track issues in our logs, but also view their current status (investigation, assigned, fixed, resolved). This closes the loop on finding new issues, tracking their impact, and finally their resolution until the end. Splunk has been extremely exciting to work on and has been an invaluable asset to our team. We’d love to continue the conversations on how we can improve our usage of Splunk and how others are using it as well.", "date": "2015-01-29"},
{"website": "Cerner", "title": "JavaScript Logging: We can do better!", "author": ["Garry Polley"], "link": "https://engineering.cerner.com/blog/javascript-logging-we-can-do-better/", "abstract": "Currently in the world of JavaScript these options\nare what we most commonly use to generate logs: These are actually pretty good in most modern browsers. Even\nif you go back to Internet Explorer 8 console.log and friends\nwork as long as you have the developer tools open. Given that we have these logging utilities what is the problem\nwith using them? When in local development these are just fine\nfor helping debug and speed up development. They can be used to help\nyou quickly catch errors or see where you’re starting to go astray\nwhen using a library. console.log and friends allow you to see\nwhat’s going on and leave notes for other developers in the\nfuture. This is okay for local development. However, what do\nyou do once you move into production? Almost everyone removes console commands before code is served in production. Without console commands in production how do you have the same\nlevel of logging you’re used to with standard applications?\nWhen using a web server you get to see most every error that\noccurs. Each 500 is logged to an error log file for every\nerror that occurs for every user. This is not something that\nreally exist for JavaScript. By the nature of how web browsers\nwork we do not get any errors that occur for the end user. Here are the issues with JavaScript logging today: Given the problems listed above you may ask: “Why should I care?”\nWe’ve gotten along for years without getting JavaScript errors.\nTry not to follow this line of flawed reasoning. Despite spending years\nwithout tracking analytics about how people use our sites, we\nnow view those tracking analytics as invaluable. Once you start\nseeing your JavaScript errors at the same rate and volume as your\nserver side errors you will view those logs as invaluable. Most\nimportantly developers will finally be empowered to provide proactive fixes\nfor JavaScript errors the same way we can fix server side errors\nin a proactive fashion. Imagine this scenario, you have an advanced search feature in your\napplication.  This search feature works two fold: it has an\nAJAX call to fill out the search results as well as a two-layer UI\nwith a drop down that shows the results. When a result is clicked, it\nopens a more detailed modal of those results. In most cases this kind of interaction is JavaScript heavy. How\ndo you know when the searches fail due to a scripting error, instead\nof a network drop on the client? What can you do to be proactive\nabout issues occurring here? We’ve released a logging framework, Canadarm ,\nto make identifying and handling these kinds of situations\neasy. Now each time a script error occurs you’ll get to see it. As\nlong as the client can connect to the Internet and execute JavaScript\nyou’ll get to see what went wrong. A common issue you may not realize\nin local testing is a Unicode search error. This logger will tell\nyou what error occurred as well as the language and encoding used\nto read your page. Below are a some topics that are likely to cross your mind.\nThis post will cover each of them in detail. This post will cover all of these questions in detail. Canadarm makes it easy to send logs to a remote server.\nThis alone is nothing novel and isn’t all that impressive. It’s fairly easy\nto setup a try/catch around your code and send that error to\na server via a GET request. The real advantage to Canadarm comes in\nwhat it does to catch the errors. Canadarm has three ways to gather errors: These modes allow you to write your code and not have to worry about\nlogging or catching errors yourself. Any global errors will be caught\nand more specifically all errors bound to events will be caught.\nThe ability to catch errors related to events is the most useful feature of Canadarm. Most errors that will occur on your web pages happen when a user performs\nsome sort of action. Canadarm is able to provide you with context specific\nerror messages by automatically hooking into and monitoring functions\nbound to events. With the Canadarm.watch and Canadarm.attempt functions, you have the power to individually\nmonitor specific functions. Let’s say you have a function that gets\ncalled without an event being fired. You can call attempt on that function which\nwill immediately invoke the function. If an error occurs, the error will be logged.\nWith watch you can watch a function once and every time it\nthrows an error later during execution the error will be logged. If you don’t want Canadarm to automatically log global errors\nand/or event based errors you can opt-out of this feature. With watch and attempt you can write your JavaScript how\nyou want to and not worry about what is going on within Canadarm. Finally, you get out of Canadarm what you really wanted from console functions.\nYou can log specific error messages at the point you want to via\nthese logging commands: Optionally, you can provide two more arguments after msg and error . data followed by options . You can see the usage of these arguments over in the Canadarm documentation . Specifically, data is the most useful\nhere. data allows you to pass an extra object that will get its values passed\nto the appenders. The default appender included in Canadarm will log all these\nvalues for you as key-value pairs. To find out more on how to configure and use Canadarm go and\ncheck out its documentation . It’s pretty easy though. You\nonly need to include the Canadarm code and then configure the logger. As\nseen on the Canadarm readme, you can do the following to get a working\nlocal logger: Now you’ll see all logged errors in your console with all the information the\nstandardLogAppender provides. Obviously you want more than local logs. Next you’ll\nsee how our teams have used this logger. Canadarm is fairly simple. The logger catches an error and then\nsends that error to a central server. Under the\ncovers it uses Appenders and Handlers as the mechanisms to achieve\nthis result. An appender works as a way to process an error or log event that occurs.\nThe appender has this signature: appender(level, exception, message, data) . An appender must return an object. The object should contain simple data types.\nThey are single key/value pairs, usually strings. The return value of\nan appender is then passed to a handler. Handlers take action on the objects produces by the appenders. A handler’s\njob is to send the results of the appenders somewhere. By default\nthere are two handlers that come out of the box with Canadarm:\na console handler that logs all errors\nto the console and a beacon handler that sends\nall errors to a given URL end point. Appenders and handlers work together to create your logs.\nHere’s the break down of what happens during an error or logging event: That’s it for how the logger works on the client. The real power comes when\nyou combine this log gathering with the BeaconHandler .\nThe logs gathered are then sent to a server. The server receiving these logs should\nbe writing them out to a file that is then read into a logging system. We currently use a\nsimple Apache server and treat its access logs as our JavaScript error logs. We then send\nthe logs to a log aggregation tool, Splunk . We have a few applications that have begun using Canadarm. HealtheLife was the first client facing application to go into production\nusing Canadarm. They have over 3 million users and at any given moment they\nusually have at least one thousand concurrent users. These metrics matter\nfor two reasons: first, it shows us that Canadarm\nworks at scale without causing issues to the application, second, we\nhave been able to see trends in JavaScript errors occurring in this application. For those who thought “why should I care?” when it comes to JavaScript logs\nthis go live was an interesting story. Within the first 20 minutes we\nnoticed errors that occurred on every page load. Specifically this error\nwas a reference to $ (jQuery) before it was defined. Since this was in\nan analytics tracking snippet, and at the end of a script tag, it did not\ncause an end user impact, beyond eating processing time to handle an\nerror on every page. However, it did mean that analytics were not getting tracked how\nthe application intended. In fact, without this logger in place the\napplication would have happily continued along with no indication certain actions\nwhere never taking place. Since the analytics tool did not report the\nexpected user interactions, it appeared as if features of the application\nwhere not getting used, or worse, that the analytics were faulty. The actual messages in the errors for this application are interesting. Since\nHealtheLife is used in many countries with many different\nlocales, they support various languages. Because of the various\nsupported languages and users being able to use their browsers in any locale they want, we\nhad a few interesting logs messages. Specifically we have had a few logs\ncome across in English, Spanish, German and more. It was kind of eye opening to know that errors are actually\ntranslated within a browser. Currently a few internal sites are\nusing Canadarm for local development and integration environments.\nMost interesting so far for has been looking at the logs and seeing\nwho has been copy pasting code around. Interestingly enough I found some random logs on our Splunk dashboard in dev. Which lead me to github, specifically a github pages site. Seeing the application and where the logs said the application lived I was able to find the source code.\nThe code then lead me to the owner of the application. At that point I was able\nto contact the owner and get the issue fixed. Finding another application’s errors and\nletting the owner know about is an interesting experience. The whole interaction was\ncool because it was not a use case we had considered when building Canadarm. As mentioned before this has helped to point out two issues:\none for HealtheLife and another for an internal application.\nPointing out issues is not enough to fix them though. Also, Canadarm\ndoes not solve problems on its own. You get the most\nout of logging when you use a tool to aggregate those logs. We’ve\nbeen using Splunk to aggregate our logs. Combined with the searching and reporting of Splunk we’ve\nbeen able to leverage the logs generated by Canadarm to see\na few common trends in our code. Canadarm has helped\nus to see a few common problems we have: Using Canadarm to generate logs doesn’t solve problems on its own.\nIt’s when we combine those logs with the\nreporting capabilities of Splunk that we can see trends and\nidentify areas we need to improve upon in our development. On our teams it has shown that we need to get better at defining\nour APIs for data visualizations. I’ve been able to see many errors\nfrom our developers when they first try to update or modify any of\nour visualizations. Without this logging in place it would not be\nso obvious that our current API is not working well for others.\nThis gives us empirical data that developers are having issues\nusing our software. Without this data, we’d have to rely on complaints\nand hope people reported the issues they encountered when using our\ncode. As a more concrete example, recently one of our teams has begun to try and use react. By analyzing\nthe logs it’s easy to see that we have had some issues getting a handle\non how react works. Over the few weeks of react work we could clearly\nsee a number of errors. This shows we need\na lot more training on how to properly use react. Further it shows me\nthat if we plan to adopt react as our frontend framework that we need\nto put together a “gotcha” or “tips and tricks” guide for getting\nstarted. While the logger is not directly “solving” problems it is helping\nto illuminate issues we are seeing in local development as well as\nin production. By shining a light on these issues we are able to\nmove forward and solve these problems ourselves. Sometimes the\nproblems may be solved by additional training or they may be\nsolved by code changes. Most importantly Canadarm is opening our\neyes to the kinds of issues we’ve had for years with JavaScript. We\ncan no longer ignore these issue because we have solid empirical evidence\nshowing us our problems. After reading along this far hopefully you’re thinking: “This\nall sounds great! When and how can I get started?”. It’s pretty easy: If you are a public facing Internet application or a small startup,\nCanadarm can still be a great investment. Ideally you do not need\nto worry about steps 3 and 4 above. Simple Log management solutions,\nsuch as Loggly should be enough for your needs. Here’s what I did to get a quick setup working: http://logs-01.loggly.com/inputs/WWWWWWW-55555-5555555-55WW55-WWWWW55555/tag/http/ 5. Configure Canadarm with this end point to see the logs: After this setup it was pretty easy to get some graphs going. For example\nyou can easily see what errors occurred by message in this pie chart: Even easier is getting to view the raw output of a given event message: Loggly is a great tool to use for large and small projects. A big bonus for\nanyone starting to use Loggly for their JavaScript logs it that they can\nbegin to use Loggly for their other logs (if they are not already).\nWhile Loggly may not be ideal for every situation when it comes to logging,\nit is really handy when you do not have the resources, money, or time to\nsetup your own log aggregation tool. From desktop, to mobile, to embedded devices, web browsers can be seen\neverywhere. With the help of Canadarm we can now see\nwhat exactly is happening within our applications. An entire world of\nclient side errors and issues can now be properly managed and acted\nupon. Combine these logs with an aggregation tool such as Splunk or Loggly and you have enabled operational intelligence. The next time a user logs an issue for a JavaScript error you can\nrespond by telling them you’ve seen the error and are already working\nto correct it. Gone are the days of reactive fixes. Now you can worry\nabout proactive solutions.", "date": "2015-07-20"},
{"website": "Cerner", "title": "Girls in Technology Movement - Hour of Code", "author": ["Amanda Anderson"], "link": "https://engineering.cerner.com/blog/girls-in-technology-movement-hour-of-code/", "abstract": "As a female Software Engineer, I know the benefits of having other females in the field to fellowship with as well as the benefits of early exposure to coding and the technology field. The Girls in Technology Movement Hour of Code event held December 9th, 2015 at Cerner’s Innovation Campus was a great opportunity for girls from nearby schools to gather and hear a little about the technology industry, the opportunities available for their future, meet some women in the industry, and get a hands-on introduction to coding. The girls worked in pairs to complete exercises using coding constructs.  The exercises from code.org followed a Star Wars theme and included things like moving a robot and escaping Stormtroopers while earning points for certain events. The intent of the event was for it to be easy to learn even for those who have no experience with code. Blocks of code had to be put together to accomplish the tasks. They could then run the code to see what happens and how it needed to be adjusted to work next time if the desired outcome wasn’t reached the first run through. A few pairs were enjoying it so much that they finished early and used the time to work on the Minecraft one as well. It was great to see them enjoying it so much that they didn’t want to stop. The girls at this event came for various reasons, from genuine interest in technology and coding to just wanting to get out of school and go on a fun trip. And who can blame them, right? It was a fun event! They learned teamwork through working in pairs, bouncing ideas back and forth to figure out the solution to the problem. Some of them would get excited when they figured out the harder problems. It reminded me of that feeling I get when working on a project and finally figuring out a tough part that had been stumping me for awhile. If you work with code, you know that feeling. After the exercises, the girls had the opportunity to talk with a female Cerner engineer. This was a chance for them to ask questions and learn more about what it’s like to be a software engineer. I spoke with a senior in high school who asked things about what college I attended, what types of classes I took, how I like my job, and what I do. I was able to share some of the fun events we have at Cerner like DevCon and Programmer’s Day. I didn’t have any exposure to coding in school until I got to college, so seeing these middle and high school girls get the opportunity to see what coding and the technology industry is like is awesome. I’m glad to see something like this being put together and to have the chance to be a part of it.", "date": "2016-01-20"},
{"website": "Cerner", "title": "Deploying Web Services with Apache Tomcat and Chef", "author": ["Bryan Baugher"], "link": "https://engineering.cerner.com/blog/deploying-web-services-with-apache-tomcat-and-chef/", "abstract": "Open source is an important part of our engineering culture and we love when we’re able to contribute back to the community. We recently open sourced our Tomcat Chef Cookbook , which we use to automate deploying many of our web services here at Cerner. The cookbook is meant to be simple, yet flexible enough to change most anything about Apache Tomcat to your liking. It supports installing any version of Tomcat, configuring any file within the Tomcat installation and deploying web applications. The cerner_tomcat cookbook is one of our oldest cookbooks which was originally written when we first started using Chef . We created it as the other community Tomcat cookbooks didn’t include all the features we needed. As of today, the Tomcat cookbook provided by Chef seems to be the predominant cookbook in the community; however, it still lacks the functionality and flexibility we need such as deploying the applications themselves. One of the biggest problems when using Chef to configure Tomcat is that Tomcat uses XML for configuration which does not map very well to Chef’s Ruby Hash-based attributes like JSON, YAML or properties files do. Our first implementation was very similar to the Chef community cookbook in which the cookbook provides a configuration template with a static set of properties. We quickly learned that this solution didn’t scale for us given the varying configuration needs of applications and the ever-changing configuration options across Tomcat versions. Our approach now is to allow applications to provide configuration templates themselves, allowing this cookbook to be leveraged by the various teams deploying Tomcat applications. Our cerner_tomcat cookbook is one of our most used cookbooks here at Cerner. We would love for you to try out our cookbook and let us know how we can improve it.", "date": "2016-02-04"},
{"website": "Cerner", "title": "Spring 2016 ShipIt Day", "author": ["Melanie Carpenter"], "link": "https://engineering.cerner.com/blog/spring-2016-shipit-day/", "abstract": "ShipIt Day is an all day, all night hackathon with the goal of building something awesome, usable, and value-adding\nwithin the given 24 hours. This Spring marked our fifth ShipIt Day, with participation and reach growing with each\nevent. We kicked off the event at 10 AM on Thursday, when teams broke out and got to work and stopped only for food and\ncaffeine. Many stayed until a few hours after dinner arrived, several stayed all night and took naps in their workspace,\nand a few worked through the whole night. The event was initially announced early in February, to give teams time to\nwork this into their project plans. The schedule was to start at 10 am on a Thursday and wrap-up at 10 AM on Friday.\nTeams then presented their awesome projects and then were free to leave for the weekend (and catch-up on some sleep).\nEach team was free to choose the project they wanted to work on, with the limitation added that they should work on\nsomething which can be deployed somewhere in 24 hours (there were bonus points involved for deployed projects).  The\nwinning prize not only included bragging rights, but also the Golden Keyboard , which will be a traveling trophy. The event wrapped up on April 1st at Continuous Campus. We had over 50 participants from nine different organizations\nsplit into teams. They survived several April Fools pranks, two cases of Monster , and 18 extra large pizzas before\npresenting their projects to a room full of other participants, judges, and over 50 people watching the live broadcast\nonline. By the time 10 AM on Friday rolled around, the group was looking sleepy but excited for presentations. We moved over to\na bigger room where a live streaming service was set up for associates at other campuses to watch presentations. A\nvoting form was sent out to the entire audience, and a scoresheet was given to the judges. All 12 teams got through\ntheir presentations despite their lack of sleep, and the judges were thoroughly impressed. One judge, K.K. Kailasam made\nthe remark, You guys made it very difficult for us. It was extremely exciting to see the passion and excitement in each of you guys coming up and doing something outside of normal work. I’m totally humbled and very impressed. The people’s choice votes were tallied and judges decided on the top three winners. Team Mantri (Dinesh Rauniyar, Raju Karki, Dinesh Bajracharya, Mahesh Acharya, Prannav Shrestha) Spoor is a Chrome extension that scans through tools like GitHub , JIRA , Crucible and Jenkins and presents relevant\ninformation to us. Just with a single click, you can view all of the notifications from these tools and drill down to\nthe detailed and hyperlinked description of each one of them. Suicide Cupcakes (Travis Collins, Daniel Lloyd, Andy Quangvan, Gared Seats) The ETS_SHIV Ship IT project came about due to the frustration of being locked into a chroot jail on an ssh jump box\nwithout any ability to lookup the necessary connection info (fully qualified domain name / IP Address) necessary to\nconnect to a node. Shiv is a tool that will query a read-only file system with all the node objects represented as\ndirectories and text files.  The script writes out the files and directories to a file system that is bind mounted read-\nonly inside chroot.  Now users inside the chroot jail can run simple a command like; shiv cloud.  This would return the\nFQDN/IP address of all nodes with a Chef node name containing the the string “cloud”. Really Tardy Measurement mechanicS (Jeff Compton, Himanshi Gulati, Adam Splitter, Timothy Waszac, Sriram\nVimaraju) The Response Time Measurement System (RTMS) framework, which our team owns, is dependent on many moving parts, such as: Olympus , Hadoop and Jenkins in order to produce a timer. In addition, using Vertica , Tableau , LightsOn Network , and the Olympus portal for reporting\ntimer data.  This framework has been consolidated into a single application which can be used for development or\ntroubleshooting of timers.  Currently the application supports Splunk (for Millennium+ ), Millennium application-produced\ncheckpoint CSV files, or produce timers real-time as a Millennium application sends checkpoint data to it.  After timers\nhave been produced, the user can inspect the composition of the timer by exposing checkpoints, incomplete timers, and\nchronological representation of a timer and other timers which may have affected it in a Gantt chart. This application will improve the accuracy of timer definitions developed, improve speed of development on engineering,\nand empower engineers to develop and troubleshoot their timers without depending on the entire RTMS framework. Wolfe Packe (Bryan Baugher, Micah Whitacre, Christian Duranleau, Michael Barker, Alex Hostetler) Kafkaesque is a Web UI to help manage and operate Apache Kafka . It provides insightful statistics about topics,\npartitions and brokers and is able to balance and re-assign partitions to different brokers. ShipIt Day has become an incredibly valuable experience for participants, giving them a chance to not only focus on\nprojects that they wouldn’t normally have time for, but allowing them to meet people from other teams (in different\norganizations) and learn from those they normally wouldn’t work with. To quote a participant, Daniel Lloyd: ShipIt day was a blast.  It was fun, it was serious, it was fun again… We got to make some new friends in IP and\nparticipate in an event that spanned both teams and organizations.  That in itself is enough to recommend ShipIt day to\nanyone.  It provides so much perspective into how the other teams operate and the cool things other teams and\norganizations are doing. Thanks to all that participated, and we’ll see you again in the summer!", "date": "2016-04-18"},
{"website": "Cerner", "title": "RailsConf 2016 Recap", "author": ["Sam Bao"], "link": "https://engineering.cerner.com/blog/railsconf-2016-recap/", "abstract": "At Cerner, we love Ruby and Rails and use it prominently in our cloud based solutions. When we found out RailsConf 2016 was going to be held in the hometown of Cerner’s headquarters in Kansas City, Missouri, we were excited for the opportunity to support the conference by attending as well as being a sponsor and hosting one of the official after parties. RailsConf 2016 took place in the heart of downtown Kansas City. From day one, the atmosphere was buzzing. We usually send a few engineers to RailsConf every year, but since it was in Kansas City, we were lucky enough to send 50+ engineers to soak up all of the knowledge and news that was being shared across three session-filled days. We were a silver sponsor this year, which provided us with a booth space where we could share Cerner’s story with Rails developers from across the world. In addition to meeting all of the wonderful Rails developers and getting the chance to tell them about Cerner and how we are huge into Rails, we also had some fun at our booth with some multiplayer gaming action… …and our daily raffles. We gave away some some awesome BBQ from Joe’s KC and a Phantom 3 Professional drone. Congratulations to both winners. And for even more fun, we hosted one of the official after parties after the closing Day 1 keynote at No Other Pub. During the party, RailsConf attendees got a chance to mingle and talk to our developers and architects in a casual setting…and the free food and drinks certainly made for a great time. In addition to all of the fun, one of our distinguished engineers, Nathan Beyer, gave a talk about lessons we’ve learned using Rails at Cerner. RailsConf 2016 was a great event, and the fact that it was in our backyard, was icing on the cake. We wish it would be held in Kansas City every year, but we are already looking forward to RailsConf 2017 in sunny Phoenix, AZ .", "date": "2016-05-19"},
{"website": "Cerner", "title": "Difficult Debugging: Learning From Utter Nonsense", "author": ["Rory Hardy"], "link": "https://engineering.cerner.com/blog/difficult-debugging/", "abstract": "As software engineers we invest time into thinking about the problems we are trying to solve every day. Sometimes the problems we face are small or simple. Other times they are large and complex. Either way, we can usually draw from our experience to find a path forward. While coding, we run into a lot of different bugs. Often, they are simple typos or a misuse of an API. These problems do not bog us down very much, and they are quick and easy to fix. Even complex bugs, while requiring investigation, often follow a familiar pattern(s) that help us identify a path to a solution. Given this experience, it’s easy for us to think that we are good at debugging. We are not. We are good at pattern recognition and recognizing similarities to problems we’ve faced in the past. Because we recognize the pattern, we can quickly debug most of the bugs we come across in our day-to-day work. Which is great. But today, we will be exploring the types of bugs that do not come up in our day-to-day work. When a bug is brand new to us, we haven’t had the opportunity to derive a pattern from it yet because we have no experience to draw upon to create that pattern(s). Thus, we must investigate using a limited set of tools: Often enough, that set of tools will guide you to a solution within a reasonable amount of time. Be warned, sometimes the solutions online solve a similar problem, but not your root problem. Testing is always in order with new bugs (or any bug more complex than a typo). Debugging gets even trickier and more time consuming when we encounter a bug that resembles a pattern that we’ve seen before. Once, I came across a bug which looked like two other things, but turned out to be a third unexpected thing. One of our teams created an interactive DOM based treemap which worked in Internet Explorer 8 (IE8) and above. It wasn’t lightning fast in IE8, but it worked sufficiently well with a reasonably complex dataset. That is until the browser started crashing intermittently for some of our users. There were a few things that we (four engineers) noticed right away: Our initial investigation was good. We isolated the problem browser and determined that there was something wrong with the treemap, which was causing the issues, we were on our way to solving this problem - or so we thought. Given the intermittent nature of the problem, our experiences in the past, and the patterns we had observed, we assumed we had a memory leak, a race condition, or both. In an attempt to rule one of those options out, we tested with small and large datasets for the treemap. The above image serves as proof that modern browsers were unaffected by the bug we were experiencing. Convinced that the treemap was the issue and that it had to be a memory leak or a race condition, we created automated tooling using sIEve, virtual machines, and AutoIt scripts. The automation enabled us to test dozens of scenarios hundreds of times easily to gain metrics which might help illuminate where, in the code, we should look next. We chose different sized datasets thinking that smaller datasets would not trigger a memory leak while a large one would. Unfortunately, it crashed either way and we were no closer to figuring out what was wrong. After several more days of investigation we were just as perplexed as the day we started. We had a plethora of information which told us nothing. At this point our capacity had to be reduced down to just one developer - me. Frustrated with the resultless weeks spent investigating the issue, I decided step back and take a completely different approach. To re-evaluate everything we did, I removed all of the assets from our application to minimize moving parts; regardless of how unlikely a variable was, I wanted to remove it. I ran our test scripts against the application to find that, unsurprisingly, the browser did not crash. I then added JS and CSS assets separately to find that neither alone caused the browser to crash. As a sanity check, I added both back to the application and it would reliably crash. This implied that there was some interplay between CSS and JS which was causing the browser to crash. In our application, it was easier to add CSS file by file than it was for JS so I went that route. Along the way, I noticed that we were sending the wrong X-UA meta tag so I got sidetracked and fixed that to no avail. Eventually, I added back our Font Awesome CSS which caused the browser to crash. Thinking there was no way that a font could be the issue, I added and removed various pieces of our CSS to determine if it was the cause of the problem. I tried changing selectors, changing the load order, and everything else I could think of to no avail. After a while, frustrated, I commented out every line referencing Font Awesome and the browser stopped crashing. At this point, I just started adding code back line by line until the browser started crashing. What I found made no sense: I looked over this for a while and eventually noticed that we were using single quotes when we referenced the font while the font declaration was using double quotes. Having exhausted most other options already, I tried making the quotes match to see if the browser would crash and, to my surprise, it wouldn’t. It didn’t matter whether I used double or single quotes as long as they matched. I added back all of the remaining assets and tested this again; sure enough, the app did not crash even after 200 tests. Yes, it’s true. It wasn’t a race condition or a memory leak within the treemap, or anything else we thought it might be. It was mismatched quotes. I would never have guessed this in a million years. I did not stop here! Although I solved the problem, I needed to understand what was going on. I was also very curious which part of the JS was causing issues with Font Awesome. I reintroduced the old CSS and started testing the JavaScript. The application broke as soon as I re-introduced Modernizr. The treemap was, at this point, seemingly faultless as I was able to reproduce the crashes without it. I researched mismatched quotes and Modernizr online to try and get a better understanding of what was going on. I found several articles that detailed similar issues, but did not identify the root cause. Eventually I found a post on StackOverflow that enabled me to connect the dots. Browsers do a lot for developers under the hood and IE8 treats different types of quotes differently and kicks off an error handling subroutine to smooth it over. If that error handling subroutine occurs while Modernizr is attempting to shim the browser for HTML5 compatibility, the browser will crash. We weren’t wrong in that we were facing a race condition, but we were very wrong about our presumption about the treemap. The only interplay the treemap had was that it caused Modernizr to take longer to apply the shim thus widening the window of opportunity for the race condition to apply. We worked on this from December 20th to January 9th, a total of twenty days! Oftentimes, our ability to recognize coding patterns enables us to identify and solve bugs in a timely manner. As the example bug shows, however, following them can lead you down a rabbit hole. At face value, twenty days were spent working on that bug, but when you consider that there were up to four developers involved at a time, the lost time actually equates to one to three months. Once you factor in opportunity cost on top of that, you look at two to six months of time lost to debugging this issue. That is a disturbingly significant amount of time to have lost on a bug. It was a difficult bug to solve, but it could have been solved faster had we recognized that our past experience was insufficient to solve this problem. There are some key points to take away from debugging something perplexing like the example bug we looked at: When you find that past experience is not yielding results in a timely manner (you’ll have to define what that means for yourself), take a step back and follow the steps above. Doing this allows your past experience to help you when it applies and gets it out of the way when it doesn’t. Don’t see how far the rabbit hole goes when you can scientifically figure it out. Additionally, do not blindly trust answers found online. When working on the example bug, we found several examples of people correctly identifying a similar problems to ours, none of which correctly identified the underlying cause. Treat online resources as what they are - resources. Sometimes they have the answer you seek while other times they are the dots you must connect yourself. Static analysis, or linting, is a technique to catch common coding mistakes quickly. A linter will evaluate raw source code and give back a report detailing what needs to be fixed. Most languages have some form of linting software available for them. Since this post is front-end oriented, below are a few linters available for JS: All of these tools will notify you when you make typos, aren’t using variables, fail to follow a predefined set of coding styles, and more. They are very fast and are your first line of defense against day-to-day bugs. Linters will catch mistakes much faster than a human. I have a friend who is a very talented software engineer and mathematician. One day he called me to see if I could help him debug an issue he had been struggling with for about 19 hours. His Angular application kept spin locking (freezing up) and he couldn’t figure out why.  We scrolled through the code and luckily for him, a piece of it caught my eye: He had been working on C++ code prior to this project which features block scoping while JavaScript (ES5) only features function scoping. A linter would have immediately caught this mistake and informed him of it, saving 19 hours of debugging. While static analysis is your first line of defense, tests for your code are your second line of defense. Tests should depict the behavior of your codebase and should fail if you deviate from that contract. Just like linters, most languages support automated testing. There are several frameworks that you can leverage for testing JS, a few are: Defining a set of behaviors for your code helps to protect you from unintentional changes and side effects. Tests will take longer to run than a linter, but are still frequently quicker at catching mistakes than a human. Since automated unit tests are like a contract for behavior, they can also help keep developers unfamiliar with the behavioral contract from making mistakes. When automated approaches are insufficient, it’s time to start the manual debugging process. Every programming language has a debugging toolset that can be used to work through code with bugs. Since this post is front-end oriented, we’ll look at browser tools. You could use JS to alert variables at different times, but this is radically inefficient. Today’s modern browsers have a plethora of tools available to you starting with the debugger keyword for JS code. When the browser’s console is open, the debugger statement acts as a breakpoint; it does nothing otherwise. Although simple, it’s very powerful as it gives easy control of adding/removing breakpoints to step through the code. Atop the debugger keyword lay all of the browser tools. These enable you to step through code, inspect code, profile code runtimes, inject assertions, and even monitor the network traffic relating to the current page. Different browsers have different tools; below are some guides for debugging in different browsers: When you are in the process of manually debugging code, it is useful to automate what you can. For example, when testing the IE8 crashes, we used virtual machines and an AutoIt script to recreate and test the scenario as well as to keep logs of what happened for future reference. Whenever possible, always automate recreating the scenario(s) for testing purposes as it: Many bugs will follow the pattern(s) of another bug and this can help you debug them quickly. However, this is not always the case; tools and automated tests will help you speed up your debugging time, but cannot keep you from chasing ghosts. Remember, the following steps are the key to solving difficult bugs without losing a lot of time and effort: When faced with a challenging bug watch how much time and effort you give to it. When it’s time to step back and re-evaluate your approach, do so and success will follow.", "date": "2016-07-13"},
{"website": "Cerner", "title": "DevCon 2016 Word Cloud", "author": ["Ian Kottman"], "link": "https://engineering.cerner.com/blog/devcon-word-cloud/", "abstract": "Every year we hold an internal developers conference called DevCon . This year\nwe had 295 submissions for talks, ranging from a deep technical dive into the inner workings of Kafka to a reflection on the power of office pranks.‌ I wondered what, if anything, do these submissions have in common? Are there any common themes/topics being discussed? To find out I decided to visualize the talk submissions in a word cloud to create an easily understandable (and hopefully aesthetically pleasing) view of what topics were most common. The first step was some basic data cleaning so I could get a good set of words to visualize. I used Python to read in the submissions into a single string, lowercase all words, and then remove all common contractions. At this point I had 32,987 total words. A word cloud of this dataset was too noisy, predominated by common words like “the” and “talk”. In natural language processing these unwanted common words are called “stop words”. To remove them I made a list out of the submissions and then only kept words that were not in a list of common English words, supplemented by words specific to this data set, such as “Cerner” and “presentation”. This left me with a list of 14,291 words and 4,264 distinct words. When visualized I noticed many similar words were taking the highest spots, such as “team” and “teams” or “technology” and “technologies”. To remove some of this noise I turned to lemmatization. Lemmatization is the process of finding a canonical representation of a word, i.e. its lemma. For example, the lemma for the words “runs” and “running” is run. I used the popular Python library Natural Language Toolkit for lemmatization. This reduced the dataset to 3,834 distinct words. Now that the dataset was cleaned, common words filtered out, and similar words combined it was time to create the word cloud using a project called word_cloud . Success! It is obvious what some of the most common themes are, such as “data” and “team”. Possible future steps for cleaning up the data would be grouping noun phrases, such as “software engineer” into single words or possibly removing common spelling mistakes using a spell checking library like PyEnchant . You can checkout the full code on Github .", "date": "2016-09-28"},
{"website": "Cerner", "title": "Michelle Brush receives the Rising Trendsetter STEMMY award!", "author": ["Cerner Engineering"], "link": "https://engineering.cerner.com/blog/michelle-brush-receives-the-rising-trendsetter-stemmy-award/", "abstract": "We are excited to congratulate our own Michelle Brush for receiving the Rising Trendsetter STEMMY award. The Rising Trendsetter Award is given to a woman in STEMM areas with less than 20 years of experience and has demonstrated significant achievements early in their career. Michelle is a director at Cerner who manages an engineering team in population health as well as a team dedicated to improving Cerner’s development culture. One of Michelle’s most notable career accomplishments was her leadership role in reconstructing Cerner’s software engineer development program in 2013, which she personally oversaw until the end of 2014 (read about the details of this program in her blog ). In 2014, she transitioned into an executive role in the Population Health platform, where she fields particularly tricky support issues to help resolve data inconsistencies. Since 2014, Michelle has also managed the Development Culture team at Cerner, a team that focuses on improving associate morale through cultural empowerment, encouraging and providing opportunities for collaboration and innovation, and growing Cerner’s presence in the technical community. Michelle actively promotes women entering and growing their STEMM careers through her leadership in the community and at Cerner. She is an integral member of Cerner’s women in tech committee, which has the goal to provide development, networking and leadership opportunities to women in technical roles in a supportive and inclusive environment that promotes authenticity and values differences. Additionally, she is a member of several other leadership councils ranging from helping design the new software engineer campus, improving development culture across the company, and intern planning. In the community, Michelle is also a local chapter leader of Girl Develop It , a nonprofit organization that provides affordable programs for adult women interested in learning web and software development. Michelle is also an organizer of Midwest.io , a two-day industry tech conference in Kansas City that brings together developers across the Midwest for an eclectic collection of talks covering the latest trends, best practices, and research in the field of computing. As a member of the organizing team, she defines the vision for the conference, which includes finding speakers, promoting the conference in the industry, and ensuring diversity is top of mind at the conference. Michelle also actively speaks at tech conferences, and has spoken at conferences like OSCON, CityCode, O’Reilly’s Software Architecture Conference, and Strange Loop. Michelle is also a MSU Computer Science Board Member. Congratulations, Michelle!", "date": "2016-10-02"},
{"website": "Cerner", "title": "Building a Unified UI Component Library: Lessons Learned", "author": ["Rory Hardy"], "link": "https://engineering.cerner.com/blog/building-a-unified-ui-component-library/", "abstract": "Cerner is building an open source, mobile-first user interface component library for healthcare applications. Pulling years of experience and lessons learned across multiple teams together, we are creating something great. We are very excited to bring this solution to the public and to be contributing back to the community! We didn’t simply decide to create this library, we started with segmented UI libraries focused on different aspects of the company which had smaller isolated needs. Mistakes were made along the way and we have learned much from them. Let’s take a look at where we started and how we got to where we are now. In 2013, a group of engineers with strong front-end skills was put together with the task of creating a reusable component library for Cerner’s HealtheIntent platform. Twitter Bootstrap was used initially, but quickly outgrown as the needs and designs of UX expanded. The project this team built is called Blue Steel. It featured everything from simple typography to interactive data visualizations while adhering to UX guidelines and requirements. Blue Steel is a Rails gem that provides some basic helpers, CSS, JavaScript, and site templates to simplify layout and correct HTML usage. Blue Steel abstracts complex components to keep things simple and because we didn’t feel it was necessary to abstract native HTML elements. In its infancy, the project was well received; other platforms saw the value and wanted to take advantage of the work being done. To accommodate the additional platforms, Blue Steel was merged with another internal UI library called Style Guide. The best features of each library were pulled together, sometimes one overriding the other, sometimes merging feature sets. The merged project was called Terra. The approach of merging existing components saved time up-front, but came with a steep cost. The library was fragmented in the approaches taken during design and development. As such, it became difficult to work on without pre-existing familiarity. It was fairly obvious that we put two frameworks together on a time constraint. Blue Steel still exists but is comprised primarily of Terra components. It persists to provide HealtheIntent-specific styles and functionality, while also being a Rails wrapper around Terra. Terra has been historically kept below version 1.0.0 for rapid development, while Blue Steel has been above 1.0.0. This strategy has caused a lot of pain for the consumers of both Terra and Blue Steel. Keeping Terra below version 1.0.0 allowed breaking changes to occur without cutting a major release. Although this made development easier in some aspects, it damaged consumer trust. We always attempted to communicate breaking changes, but it didn’t always happen and it wasn’t always clear. Any time a team had to upgrade, they had to be prepared to fix their application which caused them not to trust us. The issue was even worse in Blue Steel since it had to accommodate for Terra’s breaking changes. Blue Steel would consume a version of Terra with breaking changes and would still release as a minor update by providing styles and hooks to work around the breaking changes. Deprecation schedules and documentation were written to help keep Blue Steel backwards compatible. Unfortunately, it often wasn’t; sometimes, breaking changes would only manifest in an application. Blue Steel and Terra were both tested separately with their own documentation sites. These tests were extensive and thorough but could not accommodate the complexity of the various applications consuming them. Breaking changes would creep into applications even when we thought everything was backwards compatible in Blue Steel. In Terra, the preference was to style on HTML elements, states, and ARIA-roles whenever possible since they carried far more meaning than CSS classes. Unfortunately, this form of styling is somewhat global in nature. It was easy for style collisions to occur between components within Terra, consumer styles, and 3rd party libraries. Terra was developed in such a way that it discouraged consumers from building custom components. The thinking was that applications would write little to no custom CSS or JavaScript. This didn’t scale well as needs evolved and application developers had to move ahead of the UI library. Terra components had a very high CSS specificity and were difficult to override which forced consumers to write even more complex CSS to override it. In turn, many bugs were introduced to consumer applications. Terra was also built as a monolith with very little modularity in place. Consumers had the option of taking it all, figuring out how to make a custom build, or not using it. This caused applications which only needed a subset of functionality to become bloated. Today, Terra is in the process of being open sourced. We’ve looked extensively at the issues above and are taking measures to address all of them: SUIT CSS bans the use of styling hooks which are considered global and effectively breaks the cascade in a global sense (you can leverage it within a component). At the root node of each component, only classes can be used. As a result, consumers can rest easy knowing that styles will not leak out of components and they have low specificity which allows them to be overridden as needed. By starting with base components (buttons, images, etc), we effectively create a set of building blocks to build more complex web components. By keeping them small, composable, and framework agnostic, they can be used anywhere with confidence. Following SemVer from the start will boost consumer confidence when using our components. Introducing opinion into more complex components enables us to build better and more maintainable components with our most common use cases in mind. This does not preclude consumers who do not wish to consume the frameworks and libraries we use. By keeping each component in its own repository, it’s possible to create alternative versions of componentry to meet application needs. Additionally, keeping each component in its own repo allows for individual versioning, limiting scope of change, and makes it very easy for consumers to omit what they don’t need. Finally, providing helpers for all components will enable easy and consistent development. It will be possible to build complex components solely out of the smaller components leveraging helpers to output the correct HTML. The abstraction also makes it possible to update the implementation in a compatible way while making it so developers don’t have to be aware of complexities like semantic HTML and accessibility. This will enable developers to build solid applications easily and quickly. We’ve learned a lot and are excited to open source the results of our work and learning. Keep an eye on github.com/cerner and engineering.cerner.com while we build out this new library. It’s going to be great!", "date": "2016-10-13"},
{"website": "Cerner", "title": "ShipIt VII Day: Winter 2016", "author": ["Isabella Kuzava", "Carl Chesser"], "link": "https://engineering.cerner.com/blog/shipit-vii-day-winter-2016/", "abstract": "Cerner’s 7th ShipIt Day took place on December 8th and 9th. ShipIt is a 24-hour hackathon with the\ngoal of building something awesome, usable, and value-adding within a single day.  The event was\nhosted at our DevCenter in the Realization Campus, a large open space that hosts our DevAcademy (to\nlearn more about our DevAcademy, check out this earlier post ).  We had 18 teams made up of\nassociates from different Cerner organizations. The DevCenter was stocked with all kinds of snacks,\nwhich of course included a traditional drink: Monster Energy. Participants worked hard all day long\nand when dinner came around they were ready to dig in. Good thing we ordered 8 twenty-six inch\npizzas and 40 breadsticks from a local Kansas City favorite, Pizza 51 .\nLook how big these pizzas were! It was predicted we hit a record in Kansas City for the most pizza in an elevator at this time, but\nwe were not able to get an official count! When getting pizzas of this size, there was a significant amount to fuel the innovation that was\nhappening. To break it down: A = πr 2 = π x 132 ≈ 530.93 square inches = 3.68 square feet x 8 = 29.44 square feet of pizza. Which\nis larger than a twin bed size of solid pizza, and slightly smaller than a queen bed. A perfect\namount for a ShipIt event! Teams took a break from their work throughout the evening by playing\nNintendo and cheering on the Chiefs ! There were some impressive projects that came out of ShipIt Day VII. Team GLADoS , made up of Kyle\nHarper, Sarah Harper, Snehit Gajjar, and Andy Quangvan took first place, and the coveted Golden\nKeyboard, with their impressive idea. This ShipIt, we created a new Alexa skill called Scribe.\nWith our new skill we integrated the Amazon Echo and Echo Dot smart speakers with Cerner’s FHIR implementation . This integration allows\na patient or care provider to access clinical data by simply speaking to the Echo. Judge Sean Griffin was impressed. “Really cool, awesome idea. Great innovation! The applicability\ntowards data entry could definitely be useful.” Team GLADos with the Golden Keyboard Second place went to Team Trogdor (Kyle Lipke, Derek Dobler, Nikki Justice, Mike Harrison,\nand Nimesh Subramanain). Troubleshooting errors or misconfigurations in Millennium OAuth can be time consuming. Missing\ninformation or misconfiguration can lead to hours of checking various sources of information. This\nweb page tool allows us to perform a quick and accurate diagnostic check for what pieces are\navailable and not available. Built with Ruby on Sinatra framework .\nMade to allow for easy additions to various checks that ETS (Emerging Technology Services group)\nneeds to do. Judge Jim Dwyer noted, “This is a great tool to help drive down operational costs and reduce TCO.” Team Trogdor Third place winners, Team 402 Cheeseballs Required (Andy Nelson, Venkatesh Sridharan, Ian Kottman,\nNate Schile, and Anthony Ross) created an application for tracking running mileage for the Healthe Fitness Center . Our goal was to make the process of submitting running milestones for the Cerner Running Club\neasier. The current process involved writing down how far you ran in a binder, and then one of the\ngym staff checking that binder occasionally to see when you accomplished a milestone so they could\nsend a prize. We decided to make a web application that would integrate with Strava that would\nsimplify tracking how far you’ve run. Strava is a platform that\naggregates data from multiple fitness apps, such as Fitbit and Garmin .\nWe used the Strava API to pull a person’s running data so\nwe could total up their mileage, regardless of what fitness tracker they used. In order to\nintegrate with Strava’s public API we had to have an externally facing application. We decided to\nuse an Amazon EC2 instance to do this, since it was easy and cheap.\nWe used Rails to create our web app since that was the web framework\nwe had the most experience in. First, we would ask a person to connect to their Strava account,\nand then we pulled their running data and totaled up their mileage. If a person had reached a\nmilestone they could press a button to send an email to the gym staff that included their\nrunning log, their total mileage, and what milestones they had achieved. Team 402 Cheeseballs Required By Friday at 10 a.m. the participants were ready to present (for the most part). Cerner associates\nfrom different campuses watched the event via livestream and voted for People’s choice. Those\nresults are as followed… Favorite team name : I Shipped My Pants - Brandon Inman and Steven Goldberg Improvements made to the existing Vizceral-based implementation of HealtheIntent Intuition\nEngineering.  Improvements made by the team included features to make it easier to switch between\nstreaming and batch processing, more timely updates of data, and several cosmetic features to\nimprove user experience and differentiate the tool from other Vizceral apps.  Voice activation was\nadded as a “fun” feature. Favorite Presentation : Guys on FHIR - Bhaumik Aniruddha, Bhagat Parthiv, Vetri Selvi Vairamutha,\nNeil Pfeiffer, Sai Praveen Gampa Our ShipIt project was a NICU SMART App, intended for the NICU unit. The app will be used to get a\nlive video of the baby inside an incubator alongside the vitals of the baby. This will prevent\nanyone from disturbing the sleeping pattern of the newborn. The live feedback can also be viewed\nby the parents which will ease the mental tension. The future scope of this project is to hook up\nwith an app, which can read the facial expression of the baby. Best Project : RSA (Readmission Security Admission) - Kristopher Williams, Karthik Nimmagadda, Sai\nInampudi Our project was to create an intuitive dashboard that lets the user visualize the network traffic\nfor readmission solution’s services for a given time period (hour/day/week/month currently). This\ndashboard will allow the user to easily and quickly identify abnormal behavior (e.g. service goes\ndown, or service experiencing a lot of errors) and is designed in such a way as to return results\nextremely fast for longer time spans, compared to ad-hoc splunk queries. People’s Choice Winners ShipIt Day continues to be a great way for Cerner associates to work on projects they don’t normally\nhave time for, meet people in different organizations and learn something new. Participant\nKristopher Williams said: My favorite part of ShipIt Day was being able to work on something entirely different, and in my\ncase, with a different group of people. Just a fresh change of pace. Thank you to all our participants and a special thanks to our judges Jenni Syed\n( @JenniSyed ), Yegor Hanov, Sean Griffin\n( @trenchguinea ), and Jim Dwyer\n( @episodicdata ). Check out some additional highlights of our event in this video: If you are interested in reading more about previous ShipIt Day events, see these earlier posts:", "date": "2017-01-18"},
{"website": "Cerner", "title": "Beadledom - Simple Java framework for building REST APIs", "author": ["Sundeep Paruvu"], "link": "https://engineering.cerner.com/blog/beadledom-simple-java-framework-for-building-rest-apis/", "abstract": "Beadledom is a framework for building Java based web services. It bundles several open sourced components required for building JAX-RS services. HealtheIntent, Cerner’s population health management platform, started 3.5 years ago. We went through the process of investigating different technologies for the platform. We decided on using Java for building  services and arrived on a set of libraries that we believed work well together. The long history of Java has led to an ocean of libraries that are available to Java developers. Picking up the right set of tools is very important to get the job done the right way. The trickiest part is to know what  “the right set of tools” are. This is only the first of many tough decisions that a developer makes during the lifecycle of a project. If choosing the libraries for a project is a game, then getting them all to play well together is a whole new game. Beadledom not only brings all the awesome java libraries together but also binds them well, making the developer’s life easy. It gives a good head start to the developer by letting them start from a strong foundation. At Cerner we understand how important and tedious these decisions are and we always want to contribute back to the community. So we open sourced Beadledom because it  helped us develop our platform quickly and consistently. We hope that others in the community can also leverage its power. Beadledom uses several open source projects that are widely used and well maintained. Below is a list of a few of the major components that Beadledom uses. Guice for  gluing  and bootstrapping components;\nJackson for JSON serialization/deserialization;\nResteasy for JAX-RS implementation;\nStagemonitor for Monitoring and metrics;\nSwagger for API Documentation;\nApache Commons-Configuration for handling different types of configurations consistently.\nWhere?\nYou can find the source code on GitHub.  Please find our documentation at http://engineering.cerner.com/beadledom/ . Below are the core developers of Beadledom:\nJohn Leacox\nSundeep Paruvu\nNimesh Subramanian\nBrian van de Boogaard\nSupriya Lal\nHere is the complete list of contributors who made Beadledom awesome. Cerner takes pride in our open source contributions. When teams contribute to the community Cerner rewards them with a cake. Here is the cookie cake for Beadledom.", "date": "2017-02-15"},
{"website": "Cerner", "title": "Automated Deployment with Apache Kafka", "author": ["Bryan Baugher"], "link": "https://engineering.cerner.com/blog/automated-deployment-with-apache-kafka/", "abstract": "It’s likely not a surprise that Cerner would use Apache Kafka as we have used a number of related technologies like Apache Hadoop along with its Map/Reduce, HDFS and even Apache HBase .\nOur team first started using Apache Kafka in 2014\nwhen Kafka 0.8 first came out. Since then we’ve expanded to using Kafka for a\nnumber of different use cases\n( 1 , 2 )\nand it has become a core piece of Cerner’s infrastructure. Just like the applications we create ,\nwe also needed to automate the deployment of Kafka to handle the ever growing\namount of operations work and to ensure a high level of consistency with our\nenvironments. Chef was an obvious choice for us since\nits been our deployment tool of choice for the last few years. We quickly put together a cookbook to help us automate the\ndeployment of the many Kafka clusters here at Cerner. We have continued to support and use this cookbook after open-sourcing it upgrading it to handle newer versions of Kafka (0.9, 0.10),\nhandle Kerberos authentication, and many other improvements . If you use Apache Kafka, feel free to try it out and let us know how it works for you.", "date": "2017-02-16"},
{"website": "Cerner", "title": "One Cerner Associate's Contributions in the Tech Industry", "author": ["Lindsay Ullyot"], "link": "https://engineering.cerner.com/blog/micah-whitacre-recognition-blog/", "abstract": "Micah is currently a software architect in Population Health Development in Healthe Intent Development at Cerner. In 2013, Micah Whitacre received committer status on the Apache Crunch project, and is now a Project Management Committee (PMC) member of the project. In 2012 and 2013, Cerner started using this project within several solutions. We also wanted to give back to the community and decided to invest time in helping answer questions and concerns, and overall project needs. We wanted to get involved in small community like Apache Crunch since it brought great value to our Cerner solutions. In 2013, I was selected as a committer on Apache Crunch . In reference to the technical aspect, Apache projects have some form of criteria for when to accept a committer to a project. Election to being a committer is the result of consistent and high quality contributions to the project, through documentation, code, or helping to build the overall community. Being selected as a committer was an unexpected honor. It’s also an additional responsibility for my involvement with the project. I closely watch for questions and make enhancements and functionality code changes. There are fourteen other committers on the project as well. After being a committer for over a year, I was selected as a PMC of Apache Crunch . The goal of a PMC is to set the pace and the direction of the community, as well as facilitate how the community is interacting. PMCs help drive releases and help grow the committee. Some of my other responsibilities also include participating in discussions about voting people into the community, new releases of code, and which functionalities need to get fixed. One of the coolest parts about being a committer is making new connections with larger tech community. It forces you to build new relationships and talk to people you may have not spoken to before. It’s cool being able to meet new people who don’t work at the company you’re working for. I also enjoy being able to look at the project as a whole and figure out the right way to solve something, rather than solving it right now. I’ve also spoken at ApacheCon twice on Apache Crunch , which have been awesome opportunities. I also like to be involved in our development culture at Cerner, helping improve our internal community and increasing awareness of Cerner in the industry.  I’ve been a member of the planning committee and have spoken at our internal conference, DevCon, given internal tech talks at Cerner, and have also spoken at industry conferences like Kafka Summit 2016 and Midwest.io 2014. I’ve also written an Engineering Health blog (Scaling People with Apache Crunch ). I am an open source reviewer for Cerner projects, helping review and provide feedback on Cerner open source projects prior to them being published.", "date": "2017-03-16"},
{"website": "Cerner", "title": "Engineers on the Road for SMART and FHIR (CHC 2016)", "author": ["Dennis Patterson", "Jenni Syed"], "link": "https://engineering.cerner.com/blog/engineers-on-the-road-for-smart-and-fhir/", "abstract": "Over the past few years, Cerner has been developing a standards-based platform on top of our Cerner Millennium® EHR.  Rather than roll our own API, we’ve been using the HL7® FHIR® standard .  For app integration, we’ve been using the SMART® on FHIR specification .  If you’re not immediately familiar with those acronyms, you’re definitely not alone. As we were developing the services, we were also fielding questions from 3rd party developers, answering a lot of questions internally, and trying to keep up with the specifications themselves. We knew that we would need to help provide education around our ecosystem, our implementation of the FHIR and SMART standards to developers since the standards are new and evolving. This need will become much more pressing because Meaningful Use 3 will trigger deployment of our implementations out to most of our Millennium EHRs. Meaningful Use introduces requirements from the Centers for Medicare and Medicaid (CMS) to modernize the US healthcare infrastructure.  This is nothing new, but we’ve been hard at work to support stage 3, which includes the requirement for patients to access their data via an API.  In order to attest, most of our clients will then require our implementation of the FHIR API. This past November offered up two exciting opportunities to provide hands-on instruction and support, which we’ll cover in separate posts.  In this first post, we want to highlight the work we did at our annual Cerner Health Conference .  We decided that this year’s event would be a great opportunity to offer a small Code Learning Lab - providing education for some of the APIs that Cerner has available. The Code Learning Lab was run from November 14th - 17th in Kansas City with our annual CHC conference. The goal: give developers a hands-on training session with the different APIs that are part of the Cerner Open Developer Experience ( code ). One of the tracks covered SMART and FHIR specifically, while another went over some of the APIs available with our HealtheIntent and HealtheLife platforms. We saw participation from many of our clients and partners, from both the US and Canada. Over the four days, the lab was in the format of an overview or “lecture” followed by a longer hands-on lab to put what participants were learning to use. During the labs, our engineers would walk around to check on how everyone was doing, answer questions, and help troubleshoot. We received a lot of comments and excitement from everyone participating around the fact that our engineering team itself was involved in the learning lab. It was also very exciting for the engineering team to watch everyone try out our newer developer tools, read our documentation, and then put it to practice. Not only were the participants able to actually see the data coming back, but there were a lot of conversations that occurred that will help us improve the class (and our tooling) in the future to make the event better. In the end, it was a great experience for everyone involved! If you’re interested, you can check out some of the presentations and labs that were created for the learning lab here: https://github.com/cerner/ignite-learning-lab and our SMART tutorial: http://engineering.cerner.com/smart-on-fhir-tutorial/#prerequisites", "date": "2017-03-24"},
{"website": "Cerner", "title": "Bad Design is Bad for Your Health: Why Data Visualization Details Matter", "author": ["Jody Butts"], "link": "https://engineering.cerner.com/blog/bad-design-is-bad-for-your-health-why-data-visualization-details-matter/", "abstract": "Presentation Abstract by Jody Butts, Sr. User Experience Designer\nGiven on August 11, 2017 at the UX Australia Conference in Sydney Full presentation audio and slides are available here: http://www.uxaustralia.com.au/conferences/uxaustralia-2017/presentation/bad-design-is-bad-for-your-health-why-data-visualization-details-matter/ The proper design of data visualization details is imperative for accurate and actionable data visualizations and dashboards. In the health care technology industry, even the smallest design detail on a graph of patient data can have a dramatic impact on patient safety and outcomes. For example, auto-adjusting y-axes in vitals graphs across patient charts in the EMR could cause change blindness which impacts the perception of patient data trends. As a user experience designer for Cerner, a health care IT company, patient safety is the foundation of my work. However, we see startling patient safety statistics around the world today. According to a Johns Hopkins study from 2016, medical error is the third leading cause of death in the United States, following heart disease and cancer. The World Health Organization (WHO) estimates that in developed countries across the globe, as many as one in ten patients are harmed while receiving hospital care. Based on research the User Experience team at Cerner has done, there are several challenges the health care industry faces to ensure patient safety. From workflow and communication to external rules and regulation, there is a complex and interconnected web of factors. While human error is unavoidable, there should be several defensive layers for every system. Despite the training, regulation, and process our health systems have, health care providers are still working in high-risk environments. Understaffed health systems are flooded by people in need of care. On a bustling hospital floor with alarms going off at all hours, care team members running between patient rooms, and the need to use a computer to document patient data to meet government regulations, it is no wonder that medical error occurs. Although we cannot ensure software will always work perfectly, we do have responsibility to ensure that the UI and UX layers of our systems are designed to protect our users and enable them to perform their tasks well. I work for Cerner, a company at the intersection of health care and IT. Cerner technologies support the full continuum of care including clinical, financial, and patient engagement solutions. We have products licensed at more than 25,000 facilities across 35 countries. As a part of Cerner’s UX team, I spend most of my time designing in the clinical space, working on projects for our main product: the electronic medical record (EMR), mainly used by doctors and nurses. Our technology is subject to a long list of legislation and regulations. Cerner has an entire team dedicated to reviewing standards for each country to ensure we are meeting all health IT requirements. Our UX team also has training on how these standards impact our designs and research. Accessibility guidelines, patient privacy, and medication safety are just a few examples of regulation themes that guide our work. I focus on the rules that impact data visualizations and incorporate additional research to create safe and usable experiences. The accurate graphical display of patient data has become my passion over the past two and a half years. Data visualization is a very powerful tool across industries, but I’m biased toward believing that its most amazing applications are in health care technology. For many years, the EMR was simply an input tool for charting patient data–most data review was solely displayed in rows and columns. Now we are creating meaningful displays of information for clinicians to support their decisions. For example, the pediatric growth chart has been used by health care providers to track children’s growth from birth into adulthood for almost 40 years, starting on paper. The WHO and Centers for Disease Control and Prevention (CDC) have standards for healthy growth of height, weight, head circumference and more based on age, sex, and so on. These standards are displayed as percentile lines that create a unique curve for each measure. When patient data is plotted, it becomes easy to identify if a child’s growth is progressing normally, or if there are abnormal patterns. For the first 20 years of an individual’s life, the growth chart is essential for tracking healthy growth progression. It is also an invaluable tool for communication between parents and clinicians. As I have worked on a redesign of Cerner’s interactive growth chart tools, I have had the opportunity to observe clinicians using the growth chart in hospitals. In a Neonatal Intensive Care Unit (NICU) setting, or neonatal intensive care unit, physicians are making life-saving decisions for premature infants based on this information. For specialists, such as pediatric endocrinologists, the pattern of the trend line is often the first indication of diagnoses such as hypothyroidism. Can you imagine being able to recognize a disease by looking at a line graph? In a matter of clicks, we can take tables with thousands of numbers and transform them into graphics that provide immediate insights. From problem solving to storytelling, this information drives action across every industry. There are countless examples of confusing and even misleading graphs–just try a Google search for “bad data visualizations,” and you’ll see what I mean. This assumed legitimacy is dangerous, especially when the design is sleek and appears trustworthy. The details that lead to accurate and impactful data visualizations fall into three categories: clarity, context, and creativity. Clarity Clarity is the foundation of data visualization design. We want to avoid all potential for confusion in order to empower our users to confidently make decisions. To achieve this, we must start with the basics. The title of a visualization should be succinct, yet provide the information needed to understand what data are being viewed and how they are being measured. If additional details are important to include, consider using supporting text below the title. A dangerous mistake I regularly see while consulting on visualizations and dashboards is improper or missing labels. Do not make users guess what data points are being shown or how they are being measured. All axes in visualizations should be labeled. It is also important to include the unit of measure, which I like to include in parentheses next to the label. While labels are important, I believe the most powerful element for clear data visualizations is proper scale. A simple mistake I often see is to start the y-axis of a bar graph at a value other than 0. Human eyes interpret the value of a bar to be equal to its length, so when the y-axis does not begin at 0, the values are not interpreted accurately. Similarly, the perception of data measured in percentages is also often skewed by scale. This sometimes occurs because a tool automatically adjusts the scale to fit to the data, but sometimes it is done to manipulate the data display. This example of an ad from Chevy illustrates the power scale has on perception (starting at 0:45). Context Once there is a truthful and clear foundation for data, the following techniques can bring users quicker insights and increase their confidence in data interpretation. If there is a normal, expected, or ideal range of values for the data, adding a target range is a simple but powerful way to add context. A light shaded region behind the grid lines can allow users to easily identify high and low values. Similar to a target range, a goal line can be added to indicate a discrete value that the plotted data should be achieving. A goal line and target range can be used together as well. It’s important to mark visualizations and dashboards with the date it was published. I also recommend including the date or date range of the data as well in order to maintain trust and relevancy of your visualization. Annotations are another simple way to provide your user with context. On-canvas notes and labels should be used sparingly, but can help highlight insights. These can be placed in context of a specific data point, a date or timeframe, etc. Creativity Once we have created a clear visualization and provided our user with additional context, we are ready to polish our design. There is great potential for creativity in data visualization, but it can cause more harm than good if done poorly. Let’s look at visual encoding methods, colors, and other visual design details that can enhance your visualization or dashboard. Qualitative and quantitative data is visually encoded, or translated, through a variety of graphical methods that make up each visualization type. Research has been done on these visual encoding methods, giving us insights for human graphical perception. Statistician William Cleveland and his colleague, Robert McGill, tested the visual encoding methods and ranked them by accuracy of perception. Their findings give us a scientific foundation for data visualization. Position is the best encoding method for accuracy, followed by length, angle, area, volume, and finally, with color taking the last spot. This means that color is not reliable for communicating data information in a graph. This aligns to the WCAG 2.0 accessibility guidelines , which state that color cannot be used as the only visual means of conveying information, indicating an action, and so on. Second, bar, and line graphs are common for good reason–they are most effective for our brains to accurately understand. Ultimately, it’s always best to try a couple of options and compare their effectiveness. Use color sparingly. Think of it as a tool for drawing attention to the most important insights. If everything has color, especially on a dashboard, nothing will stand out. Since approximately 10% of the population is color blind, we should avoid using red and green together on a visualization. Color Oracle is a great color blindness simulator that you can use to see how your colors appear for users with these vision deficiencies. Ensure you maintain high contrast between colors by avoiding very light or very dark shades. The Delta-E calculation measures the distance between two colors. At Cerner, we ensure a difference of at least 3.5. The best way to test the colors you are using is to print it out in grayscale. Make sure you can still match plotted data to its labels. Better yet, get additional eyes on it. This test ensures that color is not being utilized as the only way to communicate meaning. The world is overflowing with big, complex problems, but we also have big, complex data to solve them. This big, complex data exists as numbers that can go on forever. Data visualizations are the key for humans to understand it all. Data must be visualized clearly and effectively to enable professionals across industries to understand it, make decisions, and take action based on it. We can revolutionize health care and other fields through the way we visualize data. The design details matter. Through clear, contextual, and creative data visualizations, we can drive action that will change the world and even save lives.", "date": "2018-01-04"},
{"website": "Cerner", "title": "Cerner and the SDLC", "author": ["Matt Anderson"], "link": "https://engineering.cerner.com/blog/cerner-and-the-sdlc/", "abstract": "Cerner has a very compelling story that many want to learn from. Cerner’s Agile Champions are regular presenters at local, national, and global conferences. We routinely host calls and site visits for interested companies around the world who want to figure out how we were able to adopt Agile so quickly and sustain it so successfully. Despite our success, there has been a general feeling at the engineering leadership and Agile Champion tiers that we aren’t getting the benefits from Agile that we once were. We talk Agile well, but have far too many teams that could be classified as “ cargo cult ” - going through the motions rather than having an Agile mindset. We were great teachers, but somehow had gone stagnant in our learning and growth. Two events have ignited Cerner on a renewed learning journey that has already impacted Cerner in many business areas and allowed Cerner to help shape the future direction of business agility across the globe. Ironically, the first came shortly after delivering a presentation at Agile 2014 that included a section on avoiding cargo cult Agile. Ahmed Sidky , CEO of ICAgile, invited me to connect for a brief meeting. When our “brief meeting” stretched over several hours, we knew we had a strong partnership opportunity. Thus, was born the certification of Cerner’s Agile courses against an independent, internationally recognized curriculum and Cerner as one of ICAgile’s first Member Organizations. The second came shortly after Agile 2015 while hosting a call for CH Robinson and their delivery lead, Vanessa Adams . As the call wrapped up, Vanessa disclosed that she served on the board of directors for a newly formed group under the leadership of Steve Denning and Ahmed Sidky that was bringing together forward looking Agile companies to discover best practices in enterprise agility and share them back into the community. After talking with us, she felt that Cerner was an ideal candidate given our story and our willingness to learn and share. I was intrigued yet skeptical. I honestly didn’t know who Steve Denning was so being a part of the “Steve Denning Learning Consortium” held no appeal. I respected Ahmed as a voice in the Agile community that understood how to address real business issues with an Agile mindset. Some quick research on Steve exposed his background in the business and banking world. He is an author and regular contributor to Forbes magazine and Harvard Business Review. While he had written books on Agile management and was on the Scrum Alliance board, my mental image was of a business school professor, not a practicing agilist. I voiced my concerns. Vanessa felt confident they would be resolved and invited us to attend an upcoming visit (May 2016) at Riot Games and Microsoft that would be partially hosted by Ahmed who had joined Riot Games as head of Delivery. One of the keys to Cerner’s successful transformation was leveraging the idea of Connectors , Mavens , and Salesmen along with Skeptics as articulated by Malcolm Gladwell in our early pilot groups so we could address potential issues head on and build momentum. We were especially successful as some of the biggest Skeptics were also Connectors or Mavens and they have been crucial to our broad adoption and continued sustainability. Our results were our best Salesmen . On Cerner’s first site visit to meet with the SD Learning Consortium, the roles played out again. Microsoft, Riot Games, Ericsson, CH Robinson, Spotify , Steve Denning, and a representative from the Scrum Alliance were all in attendance. Over the four days in LA and Seattle, we got to see inside Riot and Microsoft and see what was working as well as what wasn’t. The open and frank conversations as well as the “No BS” attitude among all participants was extremely refreshing and productive. At conferences, messages are polished and even “learning” (a.k.a. failure) is always painted in the best possible light. This was completely different. Failures that were failures were shared and discussed. Current experiments were both admired and challenged, often by the same individual. Probably the most significant moment for me was sitting next to Joakim Sunden from Spotify, who co-authored what I still consider to be the best introductory book on Kanban for software development ( Kanban in Action ), and having him explain how the Spotify videos were created and what really goes on. (Hint – they are Marketing videos and highlight models that Spotify does not rigidly follow. They don’t know what the Spotify Way is that many talk about.)  Spotify is great, visionary and different, but they aren’t a unicorn or perfect. A future site visit to Spotify solidified that fact and made me respect them even more. I found that each existing member was a Connector in their organizations, a Maven in terms of enterprise agility, and a healthy Skeptic towards the bulk of commercialized Agile. Ahmed was the Salesman . Steve was a little bit of the professor, but clearly demonstrated an Agile mindset and a vision to change the world of work. I caught his vision. It was refreshing and for the first time in several years, I learned several new things that I could try at Cerner as soon as I got back. The Skeptic was converted. As I took a hard look at Cerner after returning from the visit, I realized that our success with things like DevCon and the DevAcademy where we were truly industry trend setters had allowed us to be complacent in other areas that the other SDLC members had as strengths. In some cases, entropy was setting in and some of the benefits we realized from going Agile were no longer as prevalent. We weren’t in the danger zone, but some trends were heading in the wrong direction and merited a fresh look. In other cases, SDLC companies were branching out with experiments in areas like HR, Finance, or Marketing that were bearing fruit. Their success led to a desire to see what we could do at Cerner. Over the past two years, site visits have been held at Microsoft (Seattle), Ericsson (Stolkholm and Athelone Ireland), Barclays Bank (London), BMW (Munich), Riot Games (LA), Spotify (New York), Vistaprint (Boston), Fidelity Investments (Boston), CH Robinson (Chicago), and Cerner. Without exception, from each set of site visits, at least three action items have been taken by the Cerner attendees to improve Cerner. There is a renewed vigor towards an Agile mindset that had waned over the past 3-4 years.\nAt the beginning, Shahzad Zafar and I were the only representatives for Cerner. Now we have four Cerner associates at each site visit and have many other associates engage in deep dive conversations on special topics like DevOps, Shadow IT, or HR. Part of the SDLC is to learn from each other and then share those messages back into the broader community. There is deep, rich learning that can be gained from blending large companies like Ericcson with over 100K employees with a mid-tier company like Cerner (20K+), smaller companies like Vistaprint (5k+) and “born Agile” start-ups like Riot Games. While each SDLC member has software components as part of the services that we offer to our clients, we operate in different verticals. The lessons we learn are universal and are not limited to a specific market or niche. At the end of 2016, the SDLC members got together and summarized our learnings from the year in a paper that Steve then presented at the 2016 Drucker Forum. Personally, I have gone from the Skeptic to a very vocal evangelist, sharing the combined Cerner and SDLC story at the 2016 KC PMI Conference, 2016 LeanAgileKC , 2017 Business Agility Conference, Agile 2017, 2017 Global PMI PMO Symposium as well as 2 of Cerner’s 2017 DevCons. Barclays Bank and Riot Games have been equally as active in sharing the SDLC learnings across the globe. Steve Denning recently published an article in Forbes with the findings that are starting to be cited as the core fundamentals for business agility across the globe with Cerner listed as a prevalent contributor. Like the Agile Manifesto, the principles are simple, yet powerful: The SDLC is currently viewed as the leader in business agility practices and is shaping the speakers and agendas for the Business Agility Conferences globally. We continue to expand membership to include new companies who meet the criteria of a leader in agility and have a story to tell. Current members have grown to include Barclays Bank, Vistaprint, Fidelity Investments, Target, American Express, and All State are all candidates to join in 2018. The brightest future is not about the SDLC but about Cerner and what we can accomplish based on learning from other SDLC members. Our 2018 Vision aligns well with the SDLC learnings and several strategies to achieve our vision include models based on what other SDLC members have already proven to be successful.", "date": "2018-01-23"},
{"website": "Cerner", "title": "Cerner Open Sources its Kafka Utilities", "author": ["Bryan Baugher"], "link": "https://engineering.cerner.com/blog/cerner-open-sources-its-kafka-utilities/", "abstract": "At Cerner, we often make use of many open source projects in our infrastructure. I work on a team responsible for Cerner’s Ingestion Platform, a critical piece of infrastructure that takes in TBs of data and over a billion messages per day. The platform’s responsibility is then to make this data available for downstream teams to consume. When designing the Ingestion Platform, we felt Apache Kafka was perfect for ingesting and consuming these massive streams of data. We originally built the Ingestion Platform in 2014 and it has grown with Kafka ever since. Along the way, we’ve become experienced with Apache Kafka and how it works. We know how to monitor it, perform common administrative operations, and more. We’ve even made contributions to Kafka for various improvements [ 1 ][ 2 ][ 3 ] and Apache Crunch, our processing framework, to add Kafka support . We have also given talks about our usage of Apache Kafka at Kafka Summit and we’ve written a blog post about it as well. We are still interested in supporting the Kafka community, and today we hit another milestone as we open source some of our utilities for Kafka . This includes: Try it and let me know what you think! You can log issues for improvements, or email me your thoughts: bryan.baugher@cerner.com . We hope to continue to improve the library and always welcome contributions and ideas from others.", "date": "2018-02-06"},
{"website": "Cerner", "title": "ShipIt X Day: Winter 2017", "author": ["Isabella Kuzava", "Carl Chesser"], "link": "https://engineering.cerner.com/blog/shipit-x-day-winter-2017/", "abstract": "On December 14th and 15th, 18 teams of Cerner associates competed for the Golden Trophy during our tenth ShipIt Day. Being this was our tenth, it was fun to reflect back on how this event started and how much it has grown. It has been great to see ShipIt Days start from small hackathons organically run by individual teams to now an engineering-wide quarterly event that brings together engineers from disparate organizations, allowing for a better cross-pollination of ideas and team members. - Kevin Shekleton This was our third ShipIt Day hosted at our new Innovations campus. It continues to be a great location for ShipIt because of our awesome game rooms and collaboration spaces. As part of this ShipIt, we had a sponsored theme, which focused on building React applications using Kaiju that are based on Cerner’s consistent Terra UI theme. Team members that support these projects were available to help participants if they wished to use these technologies, helping to gain more feedback on the projects, and to expand the exposure of these projects to ShipIt teams. By including this theme, we also introduced a new category for someone to win by using these projects. Shirt for participants. Whiteboard action happening bright and early. Teams checked in bright and early for kick-off.  Vouchers to the cafe were provided so participants could fuel their hard work throughout the day. Workflow was constant with lots of collaboration. Teams planning out their strategy. The Assembly filling up before presentations. Demos always arrive quicker than anticipated. Ready or not, 9am rolled around and demos were in full swing. Teams got 4 minutes to present their projects to a panel of judges, consisting of Michelle Brush, Robert Farr, Tim Erdel, Chad Moats, and Chuck Schneider.  Special thank you to those folks for spending their morning with us and supporting ShipIt Day! Coming in first place was Team Delayed Gratification (Russ Star, Ben Hemphill, Morgan Patterson). Our ShipIt team, Delayed Gratification, worked to complete  an existing side project called the “RUSS test,” aka Really Useful Speed Service. The goal is to allow service owners to make smart decisions about where to  geographically place their services by providing response time, bandwidth, and other key network metrics. Congratulations Team Delayed Gratification! A much deserved win. Second place went to Team ShipIt X Files (Santosh Manughala, Carlos Salazar, Matt Horn, Seada Ali). Our project was to create an application for discovering related database rows that let’s the user quickly navigate back-end data in Millennium, (e.g. look up lots of useful info for a given personnel id, person id, or event id). Each bit of data could be clickable to do a new search on that id (search the person you found on a clinical event in one click). The app displays information so that it is easy to navigate. Way to go, Team ShipIT X Files! Team A Near Field Far Far Away (Aurko Ghosh, Aaron Kaplan, Vijay Ayilavarapu, Nikhil Agrawal, Neil Pathare) took third place. Their project utilized our MakerSpace! For ShipIt, we assembled a 3D printed light bridge that provides 180% of bright light for the soldering workbench. In addition to assembly, we designed and attempted to implement an RGB LED controller using a Raspberry Pi 3 and mosfet chips. The controller uses both a physical switch, as well as a mobile app to control the lighting. The mobile app is developed using React Native, and communicates wirelessly with the controller via Near Field Communication (NFC). Judge Chuck Schneider noted, “I like that they took on a lot of different pieces, which adds to the complexity of the project.” The winners of the sponsored project theme was a veteran team, Flying Mongoose (Mahesh Acharya, Taylor Clay, Cihan Kaynak, Matt Stramel). They were able to start the migration of Cerner’s RxStation to a React application utilizing Terra UI. They were able to utilize Kaiju to build the prototype UI, which had dynamically loaded tables and mock buttons. By using ShipIt to start this transition, they were able to learn a lot of what it would take to make these next steps. Nicely done Team Flying Mongoose! Winning teams walked away with some awesome prizes. Wireless headphones, TileMates, portable chargers, Echo Dots, drones, Google Cardboards, and Lego tape were all up for grabs. Each ShipIt Day, the projects get more intricate and more creative. The projects presented show off the talent we have. After surveying the participants on their projects, 45% of the projects were identified to improve daily work at Cerner. ShipIt Day remains one of our most popular dev culture events at Cerner. First time participant, Ted Segal said, “I’m a new developer, I’m still in the DevAcademy , and I really appreciated the chance to learn about tools and ideas that I had not been exposed to yet from more experienced developers.” Veteran participant, Alex Harder said he likes participating in ShipIt Day because he enjoys exploring new technologies and applying them to a problem. ShipIt Day continues to be successful among Cerner associates. Whether you want to work on something you don’t normally have time to, or you’re interested in meeting new people, ShipIt Day is a winner! Save the Date: ShipIt Day returns March 8th and 9th! If you are interested in learning more about other ShipIt Days, see these earlier posts:", "date": "2018-02-19"},
{"website": "Cerner", "title": "One Year Calling Innovations Home", "author": ["Lindsey Bromberg"], "link": "https://engineering.cerner.com/blog/one-year-calling-innovations-home/", "abstract": "One year ago this month, Cerner development associates in Kansas City moved into a new place to call home for their workdays, Innovations Campus. The first two towers that stand on this campus provide 805,000 square feet of workspace for more than 3,000 associates. Eventually, the campus will grow larger, including 16 buildings and 16,000 Cerner associates. As that growth approaches, it is important to reflect on what these first two towers represent for associates and their development careers at Cerner. The investment in this workplace is a testament to Cerner’s dedication to and appreciation of associate values. It is a reflection of who the people at Cerner are and what they strive to be. Many of the features in the building were designed with input by associates, for associates.  Other spaces and elements of the building were voted on by associates. A Momentum Wall, a series of installations that change and grow with Cerner’s culture and development platforms, capturing architectural history, open source projects and commits, a giant monthly calendar of events, and a continuously changing installation showcasing Cerner’s commitment to usability and solid design. Distinct “Neighborhood Nodes” were designed by committees of development associates to provide opportunities for associate to relax, learn, and create. The game rooms Bowser’s Castle and Flynn’s Arcade are a nod to the sci-fi and gaming culture popular among many development associates at Cerner. The Harry Potter and Lord of the Rings libraries house books and quiet work space for associates to get away and learn. The MakerSpace provides space, machinery and materials for associates to work on personal projects and think about work projects in a different way. Associates voted on various options for workstations and landed on a configurable option that provides flexibility in terms of work space and style. The Assembly provides a space for associates to share and learn, and creates opportunities for ambient conversations for those passing through a meetup or knowledge transfer session. On a normal workday at Innovations you can see how the physical elements embedded in the buildings contribute immensely to our development culture, from the collaborative layout of the campus with unique seating and whiteboards around every corner to unique representations of Cerner’s values and history composed in binary ascii. The functionality and aesthetics of the campus have enhanced the cultural experiences associates have had over the past year. In honor of the one year anniversary in Innovations, we’ve asked a few of our associates for their perspectives on the parts of the campus they interact with daily. I love the MakerSpace Neighborhood Node. Not only because it’s given me a chance to learn and use tools I have never used before, but for the networking experience it has turned out to be. I’ve met dozens of associates across Cerner while babysitting a 3D print, or attending the Makerspace learning events the admins host. We put together a group for ShipIt IX, where we used the MakerSpace to design and create a 3D printed notification template that hooked up with an Arduino and LED light strip we programmed to give the user notifications from their desired apps. ShipIt was a first for all of us, and we wouldn’t have had a project without the availability of the MakerSpace to do 3D printing and soldering. We all had a blast, learned a lot, and plan on doing another ShipIt in the future. Plus, now I have some great embarrassing photos of my team feeding each other ice cream and failing to properly use the outdoor exercise equipment because of the ShipIt scavenger hunt challenge. The funny thing is the MakerSpace is the one neighborhood node I did not vote for when the new Innovations was being built. I had never used a 3D printer, CNC, or laser cutter and, not knowing how, I wasn’t able to imagine a personal use for those tools. But the MakerSpace team put together great tutorials in their training and connected me to resources like Thingiverse and other associates co-using the space that now I am using the space at least once a month to create decorations or make utility pieces for around the house or office. The entire MakerSpace community has been a wonderful addition to my overall experience working here at Cerner. I’ve enjoyed the people, the projects, and the enthusiasm to help each other and share ideas. The new Innovations campus shows us that Cerner is a company you can grow with. The old Innovations campus (I’m still refusing to call it Realizations) wasn’t designed for programmers, and certainly not for as many as we have. My team is as collaborative as ever, but now we have space to focus and get good work done. The all wifi floors perfectly represent everyone’s preference to remain wireless (untethered) during the day. My favorite feature of the campus is the Assembly. I’ve run the Python Developers’ Meetup since its inception across the road (at the former campus), and I remember how ill-suited the game room (8 byte café) was for it. The Assembly is perfect for meetups of any size, and the self-service microphones and projector rock. The DE Tech Talks we’ve had there have all been fantastic, and the external speakers’ Tech Talks are always good as well. When there’s not something going on, the furniture surrounding the Assembly is comfortable, and there’s plenty of outlets for working away from your desk without your battery dying. Spanning two floors in the heart of Innovations, the Assembly is home to many cultural events such as meetups, Tech Talks (be sure to check out our YouTube channel for recordings of our impressive speakers), lightning talks, ShipIt presentations, and more! In many ways I feel the new campus is a reflection of the cultural shift Cerner took over half a decade ago to create a fun, creative, and interactive development culture. The cafeteria is practically a food court, offering a range of diverse choices that reflect our many individual associates’ tastes. The work areas are open, and modular, allowing for easy communication between associates; and creativity in customizing your work environment. The many nodes on campus offer creative outlets for innovative minds. Revenue Cycle uses Bowser’s Castle for monthly after work board game meetups. These are not only fun, but great ways to network within our organization. My team has even decided a few times to stay on campus rather than going out for team lunches and play board games over lunch. These have been great opportunities to bond, and become more relaxed around each other. Innovations has become an integral part of our development culture here in Kansas City. It has also become a showstopper in the Kansas City workplace architecture and development realm as it recently won the Special Judges’ recognition for the Kansas City Business Journal’s 2018 Capstone Awards . We are very proud of our newest campus and can’t wait to see how it grows and transforms physically and culturally over the new few years!", "date": "2018-02-28"},
{"website": "Cerner", "title": "Cerner and iControl", "author": ["Jerrod Carpenter"], "link": "https://engineering.cerner.com/blog/cerner-and-icontrol/", "abstract": "At Cerner, we manage our own infrastructure and a big part of that is managing F5 load balancers. If you are a Ruby developer or a network engineer and regularly work with F5s, then I have good news! I’ve created a gem that abstracts iControl’s REST API. It makes working with the API easy to understand and eases the effort required to automate changes across many devices. After logging in, one can build a query via an instance method: The gem builds REST queries given a method delimited by underscores. The first word is the REST method and the following words are the path you’d like to send your request to. For example, if you’d like to get the version of the F5, given this iControl REST API doc You’d call a method that looks like this: So in practice you could put all this together to do something like audit the versions of F5s in a collection: This would print out the hostname and version of any F5s that aren’t on version 12.0.0. This is just one example, I encourage you to look through the iControl REST API docs to get a feeling for what’s really possible.", "date": "2018-03-13"},
{"website": "Cerner", "title": "ShipIt XI Day: Spring 2018", "author": ["Kyle Harper"], "link": "https://engineering.cerner.com/blog/shipit-xi-spring-2018/", "abstract": "On March 8th and 9th, 17 teams of Cerner associates competed for the Golden Keyboard during our 11th ShipIt Day. During this 24-hour hackathon, associates were challenged to create something innovative, usable, and value-adding. This was the fourth ShipIt held at our new Innovations Campus, leveraging the wonderful collaborative spaces available. With Shipit XI, we expanded upon a time-honored tradition of this event, the traveling trophy Golden Keyboard. This keyboard, adorned with symbolic trinkets from past winners, provides an opportunity for winning teams to defend the trophy and leave their mark in ShipIt history. Golden Keyboard However, we realized as ShipIt grew, the category for winning the “People’s Choice Award” (the favorite as voted by the audience agnostic of the judges’ selection) needed to have something more special to symbolize its importance. As a result, we introduced the Golden Mouse! This trophy, while separate from the Golden Keyboard, is another peripheral that a team can win by vote of their peers. With ShipIt XI, the Golden Mouse is now added to the tradition. Golden Mouse This ShipIt featured Reliability as its theme. This theme highlighted the work that is continually done to improve the reliability our systems. Reliability, it is the most fundamental feature! By including this theme, we also introduced a new winning category for best project that improved reliability. Like all ShipIts, signs of creativity, collaboration, and intense problem solving were abundant all night. Registration and kick-off Strategy planning Focused concentration Abundant caffeine In addition to café vouchers for breakfast and lunch, ShipIt XI featured a taco bar for dinner. Participants enjoyed unlimited tacos and delicious churros for dessert! ShipIt XI also hosted a ping-pong “March Madness” tournament to provide an opportunity for the participants take a break and flaunt their athletic prowess. The dreaded Jake Kramer power serve Demos always arrive quicker than anticipated. Ready or not, 9am rolled around and demos were in full swing. Teams were given four minutes to present their projects to a panel of judges, consisting of Julie Schlabach, Kevin Shekleton, Jeff Dittmer, and Chad Todd.  Special thank you to those folks for spending their morning with us and supporting ShipIt Day! Team SPACES RUNNER – 1st Place Coming in first place was Team SPACES RUNNER (Ryan Scheidter, Darius Washington, Miguel Fernandez, Aarthi Gunda, and Sundeepa Godavarti). “Space Runner is a visualization tool to show previously run, currently running, and scheduled to run Apache Spark jobs. It also provides some color coding for jobs that take over the average amount of time of previous runs, along with links to the Spark UI where we can view more details about the job. This tool is being used as a visualization of my team’s (Spaces Platform) dev domain to see what ran at what time, what is running, and what is about to start. This is a huge asset in terms of seeing how the domain is running, when it has too many things scheduled, and when it has free time coming up to manually run a job. This tool takes far less time than manually looking at the frequency jobs are scheduled and determining if the domain is free or overbooked.” Congratulations SPACES RUNNER! Team PSA – 2nd Place Second place went to Team PSA (Aaron Noll, Bryce McDonald, Jesse Gilbert, Bennet Lovejoy, and Snehil Wakchaure). “Team Patient Safety Analytics (PSA) developed a SMART on FHIR app with the aim of making hospitals safer and more reliable. This goal of this app is to decrease the time required to report patient safety events while also increasing accuracy by 1) Populating basic demographic fields automatically for a report (currently manually entered) and 2) Embedding the app within PowerChart to reduce the number of clicks necessary to create a report. Over time, the app could also enable Cerner to add more value by helping hospitals improve through analysis of their data and comparison against peers while also providing reporting utilities for clients to monitor and improve their processes.”\n \nWay to go PSA! Team Pedantic Beasts – 3rd Place Team Pedantic Beasts (Robert Ryterski, Paul Dennis, Ian Kottman, and Anthony Ross) took third place. “MCop is a linter for Marathon app definitions. It statically analyzes definitions for problems such as syntax errors, unstable deployment configuration, and departures from best practices.” Great job Pedantic Beasts! Team Reliabilibuddies – Best Reliability Themed Project and People’s Choice: Best Team Name The winner of the sponsored project theme of “Reliability” was Team Reliabilibuddies (Siri Varma Vegiraju, Nathan Schile, Vu Tran, Lucas Chandler, and Venkatesh Sridharan). Reliabilibuddies also won the people’s choice Best Team Name! Nicely done Reliabilibuddies! Team The Centurions – People’s Choice: Best Project Winning the brand new Golden Mouse for people’s choice Best Project was team The Centurions (Mohan Chamarthy, Sanket Korgaonkar, Saranth Govindaraju, Vamsi Krishna Guntupalli, and Shyam Gopal Rajanna). Congratulations and enjoy your cheeseballs! Team Whalenado – People’s Choice: Best Presentation Winning a glorious tub of cheeseballs is Team Whalenado (Mithun Singh, David Crowder, Nick Overfield, and Michael Rzepka) for people’s choice Best Presentation. Winning teams walked away with some awesome prizes such as wireless headphones, TileMates, Echo Dots, and drones. ShipIt continues to be a great way for associates to explore new solutions to problems, build relationships within different organizations, and most importantly, enjoy themselves. ShipIt Day XI was a great success. The projects were creative and have potential to bring value Cerner’s associates and clients. Save the Date: ShipIt Day returns August 9th and 10th! If you are interested in learning more about other ShipIt Days, see these earlier posts:", "date": "2018-04-18"},
{"website": "Cerner", "title": "Code Reviews: There is a Better Way", "author": ["Chris Fagyal", "Carl Chesser"], "link": "https://engineering.cerner.com/blog/code-reviews-there-is-a-better-way/", "abstract": "At Cerner, peer review of code is an important job. We have found it to greatly improve the quality of code and improve a team’s knowledge of the codebase. Through code review discussions, newer engineers are able to learn on valuable areas to question or challenge when something is being changed. While we heavily embrace the practice of peer code review, we also recognize it can make an engineer’s life challenging when you are overwhelmed with code to review. Furthermore, it can be large source of project delay when points of coordination are not managed well. In this blog, we will cover some tactics that you can apply to improve how you approach code reviews. The first step of making a code review inviting for others to provide feedback, is by making the scope of the review small. This generally requires thought and planning on how to incrementally present code for review, allowing each review to be smaller in size which can be safely included into the codebase in isolation. This can ultimately help morale of a team by avoiding monster-size reviews from being stagnate, and never reaching a point of completion due to amount of work necessary to review or the length of feedback that exists. By having the review small, a laser set of focus can be achieved to promptly and effectively complete the review, providing a positive reinforcement of being able to also complete the work. We recently had a “GitHub Day” at our Innovations campus, where GitHub and other Cerner engineers shared how to improve your use of source code management. In these presentations, a quote was shared that rang true: We are more receptive to feedback from pedantic robots than pedantic people. When it comes to a code review, it is easy to point out simple flaws in the code which relates to formatting or basic anti-patterns in a given language. However, should humans be spending time on these types of issues in a code review? Machines are well suited for testing code through linters and static analysis, where formatting rules and known bad patterns can be quickly identified. By focusing on having your CI system do these types of tests on your code, and failing your build when formatting problems emerge, you now have a filter which blocks a large set of noise from entering into a code review. As a result, humans don’t engage in a code review until the code actually builds (which would encompass both static and dynamic forms of tests). Furthermore, it is easier to receive these minor points from a machine (a bot which is communicating the results of your build), versus having a team member spend time pointing out all the minor things you forgot to check or format correctly. In the end, humans apply their creative mind on discussing deeper topics on the design and intentions of the code, versus getting distracted with noise of simple things that can be addressed earlier by machines. Building upon the previous point, teams should have agreed upon conventions on things like style and code formatting, so that those things can be automatically found (and even fixed with the proper tooling).  This eliminates the propensity for style/formatting comments in code reviews which detract from the actual purpose of finding substantive issues within the code. If you find code styling questions or debates are occurring in the code review, ensure the team comes to an agreement on the convention, and that styling rule gets fed into your linter (ex. CheckStyle, Rubocop, IDE formatters) to avoid it from entering into a code review again. Furthermore, this helps reinforce the “boy scout rule”: Always leave the campground cleaner than you found it. By having your linter now embrace this new formatting rule, existing codebases that you start changing in the future will be invited to be updated as well, versus allowing the drift to continue to occur. When comments emerge on a code review which are being conveyed as opinions, like “I’d have done it this way,” this can detract the focus of the code review away from correctness and more on personal preferences. In this section we will discuss two challenges when including personal preferences in code reviews: the distractions and how they can be interpreted. While personal preferences may help the readability of the code (based on the viewer), it is also important to provide the larger “why” when sharing opinions. When multiple opinions are being shared within the review, it may invite for a larger debate that might simply derail the ultimate focus of the code review. As a result, the code review may languish and not be completed due to the debate of opinions. When opinions like this are shared, especially when communicated from someone more senior on the team, their opinion may be viewed as “law”. In that, it is simply accepted by the engineer to apply the change because the source of the opinion carries a large amount of weight in the discussion. However, it is expected that no personal preference would simply be applied without additional thought and reason. When personal preferences are brought up in a review, it is best to capture and understand the “why” versus assuming the opinion is simply true or accepting it as its face value. All code reviews should have a moderator for the review to ensure the review doesn’t get off track (discussing items not applicable to the review, circular discussions, personal preference comments, etc).  The moderator isn’t necessarily an engineer reviewing the code, but could be the technical project manager for the project where the code resides.  The moderator is also responsible for ensuring comments are addressed and the review is closed when appropriate. Code reviews should be given priority, as the author is being blocked waiting for the review to be completed.  In order to achieve review agility, code reviews must be small (as mentioned above), so that reviewers are able to process the content of the review and provide substantive feedback easily. In order to give reviews priority, your team should actively work to ensure people are planning accordingly when they are assigned to reviews. Based on the expertise on the team, it is not uncommon for some team members to become a bottleneck on the team due to amount of code reviews that are assigned to them. It is important for the team to identify this issue, and ensure the team is growing other team members to shed the load of reviews by building up the expertise. While peer code reviews are a crucial component of our software development process, they also require evaluation on how they can be improved. These are simply a few tactics that can be applied to help improve how you approach code reviews. As with the Agile process, we want to continuously improve how we build software, and by focusing on how to improve this process, you can ultimately improve the quality and delivery time of your software.", "date": "2018-05-08"},
{"website": "Cerner", "title": "Say Hello to the 2018 Summer Interns", "author": ["Joe Geris", "Jennifer Lambert", "Andie Young"], "link": "https://engineering.cerner.com/blog/say_hello_to_the_2018_summer_interns/", "abstract": "Throughout the month of May, teams across Cerner’s Engineering space have welcomed 197 software interns to our campuses in Kansas City and Malvern. Ranging from rising seniors in high school to rising seniors in college, these interns will be working alongside Cerner’s full time engineers to enhance and expand upon their current skillsets. While interning at Cerner, these associates will experience firsthand the look and feel of a day in the life of a full time software engineer. Potential projects include: The variety in projects interns will be working on allows for work in a robust development ecosystem using tools like JIRA, Crucible, Github and languages like Java, Javascript, and Ruby. Our interns have opportunities to work on real projects with real world impact in the healthcare space. In addition to their work, interns will attend a variety of culture and networking events like our internal developers conference, DevCon , a bi-weekly Tech Speakers Series, an intern-only hackathon, and a family day where interns are encouraged to bring their families to see their work environment and projects.\nCerner has welcomed three intern groups in Kansas City and one in Malvern, with one more group to start in Kansas City next week. Stay tuned this summer for an in-depth look at a few of the projects our interns work on.", "date": "2018-05-24"},
{"website": "Cerner", "title": "SREcon18 Recap", "author": ["Jared Moore"], "link": "https://engineering.cerner.com/blog/srecon18-recap/", "abstract": "This past spring, I had the pleasure of attending SREcon18 in Santa Clara, California. If you have never heard about SREcon or the term SRE then let me diverge for a moment to describe. SRE, or Site Reliability Engineer, was coined by Google employees back in 2003 when a team of software engineers were tasked with running a production environment. It’s the new hotness in the technology world, so an internet search will turn up a bunch of topics. If you are interested in learning more, I recommend sticking with the more trusted sources: Since SRE concepts have been codified, organized, and documented by the creators, the above resources are easy to explore without getting conflicting information from other entities who have taken the ideas and modified them to fit their organization. There is a lot of excitement in the industry around the SRE concept and there’s a lot of overlap between SRE and DevOps. Having been working in this industry for a while now, this is a much-needed evolution in the operations and development roles. SREcon started in 2014, so it is relatively new and is hosted by Usenix Association . Usenix has been hosting technical conferences since 1975 and one of their tenants is to host vendor-neutral events. I have been to a few vendor hosted conferences over the past few years and they are always heavy on the sales and light on the ideas. The sales aspect of these conferences got to the point where I was burned out and looking for something idea based. SREcon was the complete inverse, heavy on the ideas and light on the sales. It fit my need perfectly. My only complaint, and this is minor, was that most of the speakers selected this year were backed by big name tech companies, i.e. Google, Facebook, LinkedIn, Netflix, etc. A fellow attendee I was talking to mentioned that they enjoyed the company diversity this year over previous years, though. Apparently, the tone of the conference in previous years was Google heavy, i.e. “This is the Google Way” which is not surprising since that’s where it all started. The conference was being held in the heart of Silicon Valley therefore, the proximity made it reasonable to have many presenters come from companies in the valley. I fully expect to see more diversity in the coming years. SREcon19 was announced to be held in Brooklyn, NY, so I would not be surprised if there are many east coast companies represented. Additionally, I attended the SREcon Americas conference. Showcasing another aspect of diversity, there are SREcons in Europe/Middle East/Africa and Asia/Australia, so it’s become a world-wide conference. The idea-sharing phenomenon of SREcon played an important role in my decision to attend, since I’m leading an automation development team in CernerWorks℠. CernerWorks is Cerner’s managed services organization responsible for hosting, managing, and monitoring our client’s systems in order to provide the most reliable, highest performing and cost effective delivery of technology services for healthcare. My team is focused on automation in the client aligned systems management space. That’s just a fancy way of saying that our focus is on the non-cloud solutions. We are focusing on solution provisioning, configuration management, and upgrades. The work we are doing aligns well with the tenants of SRE, DevOps and other philosophies that have been discussed in the past few years. I am very interested in these philosophies simply because change is hard, especially when it comes to changes in a culture with an established mentality of doing things a certain way. The technical part of our job is easier in my opinion since technology is always changing. Some of the more interesting sessions I attended talked about patterns and behaviors that we can learn from other industries. For example, the way that firefighters and other first responders react to incidents and the command structures they put in place outline ways that we can improve how we deal with incidents. Having a plan and structure when engaging with an incident has many benefits. It allows you to define what normal operations for your team look like versus what emergency operations looks like. Determining the chain of command and defining pre-set roles that anyone can fit into can help streamline the time to respond and the emergency command hierarchy of the team. Other industries, such as oil and gas refineries, must have strong contingency plans in place in order to not go boom when things go south. There are a lot of patterns, behaviors, and philosophies that we can learn from almost any industry out there if we open our eyes. A key topic for the conference is learning from failures and mistakes. The idea of a blameless postmortem was discussed at length in multiple talks and conversations. The need for such a thing stems from our human nature to want to assign blame and fault when dealing with problems. But in many ways this type of behavior causes actions to be hidden out of fear of retribution. A blameless postmortem comes from the idea that learning and preventing is more important than assigning blame. Etsy , for example, has an annual award for the employee who’s made the biggest mistake. The culture they are building is one where accidents are a source of learning rather than a source of embarrassment. This is quite an interesting concept. Working in the technology industry and never having been to Silicon Valley was surreal. Seeing big company names on the side of buildings and realizing those were the corporate headquarters of companies whose software I have interacted with for years was an experience. One of the nights, I took a trip to the Googleplex to walk around the campus. It was a beautiful campus with trees and flowers everywhere and the weather was amazing. The campus was abuzz with people and activity that you would expect to take place at Google late at night. While walking a thought started forming in my mind around the type of work we do at Cerner. A thought that continued to form later at the conference when I spoke with development teams from various big-name tech companies. It was rewarding to talk about how we are using technology to make healthcare better. These conversations would usually end with the team from the big tech company expressing admiration, respect, and praise for the work we are doing. The Googles, Facebooks, Twitters, and the like are doing interesting things, and it’s easy to get distracted by those names, but the problems that we are solving have real impact on the lives of our fellow humans and we shouldn’t take that for granted. SREcon18 was worth the trip, but if you couldn’t attend this year’s, previous years, or future SREcons you don’t need to worry about missing out. Usenix has an open access policy which means that slides, videos, and audio of all the talks are available after of the conference. I strongly recommend reviewing the program for this year’s conference and watching any of the talks that catch your fancy.", "date": "2018-06-19"},
{"website": "Cerner", "title": "Cerner's Open Source Contributions for Interoperability Developers", "author": ["Kevin Shekleton"], "link": "https://engineering.cerner.com/blog/cerners-open-source-contributions-for-interoperability-developers/", "abstract": "Open source and open standards encourage collaboration around innovation that advances the health care industry through improved interoperability. Developers across health care can come together and use open source code to share information, as well as develop and continually improve apps to support better health outcomes for patients. At Cerner, developing open platforms that support interoperability standards like SMART® and FHIR® is integral to our mission of transforming health care. In addition to implementing these standards in our platforms, we also participate in organizations like HL7 and the Argonaut Project to help shape and develop these standards. Cerner attends and speaks at conferences on interoperability like FHIR DevDays this week in Boston, MA . In addition to all of this work, we’re continually developing open source projects and work for the benefit of all interoperability developers. As we speak on and support interoperability this week in Boston for FHIR DevDays, we thought it would be great to highlight our current open source interoperability work over the past couple of years. A couple of years ago, Cerner developed a tutorial walking developers through the process of creating their first SMART on FHIR application . With this hands-on tutorial, developers can write and deploy their SMART on FHIR application, running it against Cerner’s code Console and our FHIR Sandbox . Because SMART applications are interoperable with other vendors, the tutorial also walks the developer through running that same app against other sandboxes like the one provided by the SMART Health IT project . Over 700 developers have completed this tutorial as of June, 2018. You can find the source code for the tutorial at https://github.com/cerner/smart-on-fhir-tutorial Over the years, Cerner has contributed to several changes to the popular open source JavaScript library ( fhir-client.js , maintained by the SMART Health IT organization), used by many SMART app developers for launching and interacting with the FHIR API from their web-based SMART application. Running these types of applications within an embedded IE browser in a Windows application (like Cerner’s PowerChart® EHR) requires a bit of added effort to ensure SMART apps running concurrently do not exhibit critical patient safety issues. Cerner released an open source library , used in conjunction with fhir-client.js, to prevent these issues from happening. Our open source library is in use by SMART app developers running their apps not only within Cerner EHR environments, but also within other EHR vendor environments. You can find the source code for this project at https://github.com/cerner/fhir-client-cerner-additions Through collaborative efforts, Cerner and Duke University Health System recognized a need for the development of a more clinically relevant cardiac risk calculator app. To solve for this, together we developed the open source ASCVD Risk Calculator SMART application . Not only is the app freely available to all Cerner clients, but all our work on the app is open source so that other hospitals and health systems can  run and modify it to meet their needs. You can find the ASCVD Risk Calculator source code at https://github.com/cerner/ascvd-risk-calculator CDS Hooks is an emerging standard for interoperable clinical decision support. Similar to our SMART on FHIR tutorial, Cerner wrote an open source, hands-on tutorial that walks a developer through writing their first CDS Service. Cerner is also the primary developer and maintainer of the CDS Hooks Sandbox , an open source Sandbox used by the CDS Hooks community. The CDS Hooks Sandbox has been proven to be incredibly valuable and helps developers test and demonstrate their CDS Services against any FHIR server of their choosing. You can find the CDS Hooks Sandbox source code at https://github.com/cds-hooks/sandbox Big data can have a great impact on health care when used correctly. Health care organizations are analyzing big data to improve health care in a number of ways, from increasing revenue and improving efficiency to predicting diseases and improving patient care. As technology advance, health care organizations will continue to gather more data and developers will need to find ways to make that data useful and actionable. Cerner has many years of experience working with big data. To support big data developers, we made Bunsen open source. Bunsen is a project that makes it easy for developers to explore, transform and analyze FHIR data with Apache Spark. In addition, Cerner recently released a tutorial for getting started with Bunsen . You can read more about Bunsen on our blog post here . You can view the Bunsen source code at https://github.com/cerner/bunsen", "date": "2018-06-21"},
{"website": "Cerner", "title": "Scalable Data Science with FHIR", "author": ["Ryan Brush"], "link": "https://engineering.cerner.com/blog/data-engineering-with-bunsen/", "abstract": "The FHIR standard started as a better way to exchange healthcare data, but it also provides a solid basis for deep analytics and Machine Learning at scale. This post looks at an example from the recent FHIR DevDays conference that does just that. You can also run the interactive FHIR data engineering tutorial used in the conference yourself. Our first step is to bring FHIR data into a data lake – a computational environment where our analysis can easily and efficiently work through petabytes of data. We’ll look at some patterns for doing so, with concrete examples using the open source Bunsen and Apache Spark projects. The schema for every dataset you see here was generated from a FHIR StructureDefinition . There is a big gap between building a FHIR-based schema by hand and generating it directly from the source. Every field in every query here is fully documented as a FHIR resource , making the FHIR documentation itself the primary reference to our datasets. This means the data is well-defined, curated, and familiar to those who have used FHIR. Organizing data in files and directories is convenient, but it becomes unwieldy when working with a large number of complex datasets. Data catalogs can meet this need – and to offer a foundation for further data governance. The Apache Hive metastore is the most common way to catalog data in Hadoop-based environments and has native integration with Spark, so we organize data as one FHIR resource per table. Here’s an example from the tutorial used at FHIR DevDays : Which prints a table like this: …and so on. This makes it trivial to use intuitive database metaphors like use tutorial_small and select * from condition . FHIR ValueSets – collections of code values for a specific purpose – are essential to querying or working with FHIR data. Therefore they should be a first-class construct in our healthcare data lake. Here’s a look at using some FHIR valuesets in our queries as supported by Bunsen . Now we can use these valuesets in our SQL queries via the in_valueset user-defined function: It’s worth looking at what’s going on here: in a few lines of SQL, we are going from the rich (but somewhat complicated) FHIR Condition data model to a simple table of onset times of Coronary Heart Disease conditions. Users see a clear catalog of FHIR datasets, but something important is happening behind the scenes. Most data stores or serialization encodings like JSON keep data in a row-wise format. This means all columns from a given record are physically adjacent on disk, like this: This is a good fit for many workloads, but often not for analysis at scale. For instance, we may want to query the “code” column of several billion observation rows, and retrieve only those in a certain valueset. This is more efficient if columns are grouped together, like this: This is completely transparent to the user; she simply sees FHIR data from the specification. So while users see the FHIR data model, it is encoded in a columnar file like Parquet. In such files, all of these “code” columns next to one another, allowing the queries to do tight scans over columns of interest without expensive seeking past unneeded data. These are the building blocks that simplify otherwise complex analysis. For instance, if we want to identify people with diabetes-related risks, we can create a collection of simple views of the underlying data customized for that purpose. You can see the full example in the Bunsen data engineering tutorial , but we’ll start with a dataframe of people with diabetes-related conditions as defined by a provided ValueSet: We can inspect and validate this dataframe, and then move onto the next part of our analysis. Let’s say we want to exclude anyone who has had a wellness visit in the last two years from our analysis. We just build a dataframe with them: Now that we’ve loaded and analyzed our dataframes, we can simply exclude those with wellness visits by doing an anti join between them: The result is a simple table containing the cohort we’re looking for! Check out the complete tutorial notebook for the full story. Repeatability is an essential property for deep analysis. Re-running the same notebook in the future must load exactly the same data and produce exactly the same results. This gives us the controls needed to build on and iteratively improve previous analysis over time. Fortunately, using immutable data partitions are a common pattern in this type of system. We won’t go into depth here, but will touch on a couple good practices: Finally, building on such a FHIR-based data lake enables portability. The predictive model or analysis output is fully captured starting with portable data – which means it can be more easily deployed into other systems. FHIR has made great progress in exchanging data in online systems, and we see a lot of promise for data science at scale as well.", "date": "2018-07-02"},
{"website": "Cerner", "title": "Summer of Engineering Health: A Software Intern's Story", "author": ["Pratik Vaidya"], "link": "https://engineering.cerner.com/blog/summer-of-engineering-health-intern-story/", "abstract": "Pratik will be starting his Junior year at the University of Michigan this Fall studying Computer Science and Biomedical Engineering. This summer he’s a Software Intern on Cerner’s LightsOn Network team. This is the story of his summer experience: As a college student, it’s not every day that you get to work on cutting edge technology to transform healthcare as we know it. Every morning, I walk past a wall on the way to the elevator: “Health care is too important to stay the same.” Simple enough, huh? Healthcare has been one of the few fields that has yet to fully embrace the possibilities new technology has afforded. Throughout my internship, there’s been an overall culture where associates understand they wouldn’t be working on something unless it was important. As a result, associates are visibly passionate and driven about the problems they’re working on, constantly striving to improve and add value. In my experience thus far, the environment at Cerner has been both engaging and supportive, allowing me to grow both personally and professionally as I strive to help improve healthcare and better the lives of others. As interns, we had the opportunity to attend our two-day developer conference ( DevCon ) where we learned from talks on topics ranging from development technologies, innovating for maximum value, and understanding the perspectives of our clients when they use our solutions. I found it interesting to reflect on the differences between the “best solution” from a technology/development standpoint versus that of the end-user, maneuvering the product every day. More importantly, we discussed how we as engineers can provide users with opportunities to streamline their workflow and increase efficacy, without causing frustration among clients accustomed to a more traditional process. Changes need to be implemented gradually and seamlessly in order to get the most buy-in. Introduce change too fast and the users will be frustrated, regardless of the overall improvement. This summer, I have had the opportunity to work as a Software Intern on the LightsOn Network team, which develops a solution that provides both internal associates and external clients with the pertinent information necessary to make organizational decisions, whether that be in workforce experience, system configuration optimization, or organizational value. I’ve been contributing to a project aimed at providing users with additional clarity about the status of their data at the metric, dashboard, and data source levels. The status indicators update in real-time to provide clients with the most up to date information about their data. Users can then examine the dashboards with this information in mind or drill down to identify what specific feed may be causing the issue to troubleshoot. The ability to immediately make such a large impact on clients around the world as an intern has been a humbling experience. The internship experience thus far has not only helped me learn about useful development languages including Django , Angular , and JQuery , but also opened my eyes to the considerations taken into account when delivering a successful analytics product and software application in general. I’ve experienced the importance of breaking down the product into individual problems to target and then working on telling a clear story in the final product. All around me, the decisions made are not based on technology alone, but first truly consider the value added to the individual providers we aim to serve. By reducing inefficiencies in healthcare, we allow providers to do their jobs more effectively and in turn, allow more people access to healthcare.", "date": "2018-07-26"},
{"website": "Cerner", "title": "SSH Steps for Jenkins Pipeline", "author": ["Naresh Rayapati"], "link": "https://engineering.cerner.com/blog/ssh-steps-for-jenkins-pipeline/", "abstract": "Pipeline-as-code or defining the deployment pipeline through code rather than manual job creation through UI, provides tremendous benefits for teams automating builds and deployment infrastructure across their environments. Source of image Jenkins is a well-known open source continuous integration and continuous deployment automation tool. With the latest 2.0 release, Jenkins introduced the Workflow plugin that implements Pipeline-as-code. This plugin lets you define delivery pipelines using concise scripts which deal elegantly with jobs involving persistence and asynchrony. The Pipeline-as-code’s script is also known as a Jenkinsfile . Jenkinsfiles uses a domain specific language syntax based on the Groovy programming language. They are persistent files which can be checked in and version-controlled along with the rest of their project source code. This file can contain the complete set of encoded steps (steps, nodes, and stages) necessary to define the entire application life-cycle, becoming the intersecting point between development and operations. One of the most common steps defined in a basic pipeline workflow is the Deploy step. The deployment stage encompasses everything from publishing build artifacts to pushing code into pre-production and production environments. This deployment stage usually involves both development and operations teams logging onto various remote nodes to run commands and/or scripts to deploy code and configuration. While there are a couple of existing ssh plugins for Jenkins, they currently don’t support the functionality such as logging into nodes for pipelines. Thus, there was a need for a plugin that supports these steps. Recently, our team consisting of Gabe Henkes , Wuchen Wang and myself started working on a project to automate deployments through Jenkins pipelines to help facilitate running commands on over one thousand nodes. We looked at several options including existing plugins, internal shared Jenkins libraries, and others. In the end, we felt it was best to create and open source a plugin to fill this gap so that it can be used across Cerner and beyond. The initial version of this new plugin SSH Steps supports the following: Below is a simple demonstration on how to use above steps. More documentation can be found on GitHub . At Cerner, we always strive to have simple configuration files for CI/CD pipelines whenever possible. With that in mind, my team built a wrapper on top of these steps from this plugin. After some design and analysis, we came up with the following YAML structure to run commands across various remote groups: The above example runs commands from c_group_1 on remote nodes within r_group_1 in parallel before it moves on to the next group using sshUserAcct (from the Jenkins Credentials store) to logon to nodes. We have created a shared pipeline library that contains a sshDeploy step to support the above mentioned YAML syntax. Below is the code snippet for the sshDeploy step from the library. The full version can be found here on Github. By using the step (as described in the snippet above) from this shared pipeline library, a Jenkinsfile can be reduced to: An example execution of the above pipeline code in Blue Ocean looks like this: Steps from the SSH Steps Plugin are deliberately generic enough that they can be used for various other use-cases as well, not just for deploying code. Using SSH Steps has significantly reduced the time we spend on deployments and has given us the possibility of easily scaling our deployment workflows to various environments. Help us make this plugin better by contributing. Whether it is adding or suggesting a new feature, bug fixes, or simply improving documentation, contributions are always welcome.", "date": "2018-09-20"},
{"website": "Cerner", "title": "Girl Scouts earn Cybersecurity Badge at Cerner", "author": ["Denisse Osorio de Large"], "link": "https://engineering.cerner.com/blog/girl-scouts-cybersecurity/", "abstract": "As a software engineering executive from Colombia, I thought that the best way to honor Hispanic Heritage Month (September 15-October 15) was to help the local Girl Scout Hispanic troops earn one of the new cybersecurity badges. October is Cybersecurity Awareness Month, having the activities nearing October seemed very fitting. Girl Scouts has been focusing on STEM badges for all ages with several new initiatives in the past couple of years. The cybersecurity badges are some of the latest additions to the growing list of STEM badges that girls can earn. I was excited to help the troops earn their badge and I wanted to make it very special and appealing to these girls, so I decided to run the event entirely in Spanish. I thought that this approach would be a great way not only to engage the girls, but also their family members who often accompany them to the events. The badge was designed to be very interactive and engaging. The first activity was a game that helped the girls think about all the different kinds of electronic devices that exist, such as Fitbits, iPads, cell phones, TVs, etc, and how they interact with them in their daily life. The activity then focused on how security in everyday life can translate to security in cyberspace. The next part of the event focused on helping the girls understand how much of our life is connected to the internet and how different our life would be if it wasn’t so. The girls had fun describing how they would live without internet. They all agreed that life without internet would be challenging but they would still be able to read and go camping! To help illustrate the concept of how layers of security are important, the girls drew on whiteboards how they could defend a castle. The girls were quite inventive on this interactive activity! The girls secured their “castles” effectively. One of the most technologically advanced castles included dragons, a force-field and a keypad for the password. The subsequent area of emphasis was regarding how electronic devices communicate. It was interesting to break down networking concepts like TCP/IP, network topology, and firewalls to a group of 1 st and 2 nd graders, but I made these concepts relatable. IP packets were explained by building key chains using beads that spelled “brownies.” The activity involved the girls acting as senders, messengers and receivers, along with individual beads acting as packets with headers, contents, and other metadata. One scout displays her finished IP packet keychain! Safety rules were an integral part of the badge; the Brownies formed groups and made posters to help others understand the importance of the following internet rules: We wrapped up by touring our Innovations campus and highlighting how technology helps us improve our health and the health of our communities. Overall, this was another phenomenal opportunity for the Girl Scouts to engage with Cerner. It exposed them to  key technology concepts and it expanded the prevalence of STEM education throughout the community. To see more about Denisse’s involvement in the Hispanic Girl Scout community, check out this blog from 2017 . Denisse has played a vital role in bringing STEM opportunities to the local troops. In fact, she was recently recognized by the Hispanic Chamber of Commerce of Greater Kansas City for her philanthropic efforts by receiving the STEM and Education Nuestra Latina 2018 Award. Congratulations, Denisse!", "date": "2018-10-05"},
{"website": "Cerner", "title": "ShipIt XII", "author": ["Caitie Oder"], "link": "https://engineering.cerner.com/blog/shipit-xii/", "abstract": "This month we held our quarterly hackathon event- called ShipIt Day. We would like to thank all the participants for making ShipIt Day XII a huge success. With 75 participants, we had associates representing 13 different organizations across Cerner. These associates were given 24-hours to meet the requirements of this hackathon which included making something that was innovative, usable, and value-adding. As the clock ticked down, our associates worked in their teams to complete a project while taking some time to enjoy the fun activities happening throughout ShipIt day. It is an unwritten rule that the balance between fun and project time during ShipIt days should remain even. Associates are encouraged to enjoy the collaborative spaces of Cerner’s Innovations campus and participate in various activities such as ping-pong, eating snacks, playing games, and building relationships with other Cerner associates. Participants were given vouchers to use in the Innovations Café for breakfast and lunch. They were also served a fully-catered taco bar to stay energized for dinner. This year, WarGames and ColecoVision gameplay were brought in for participants to enjoy throughout the evening. As said best by Cerner associate, Carl Chesser ( @che55er ): Software creation requires additional breaks… ShipIt at Cerner is no exception of this software craftmanship approach. The morning crept up on the teams quickly, and they had to begin preparing for their 4-minute presentation in the Assembly at Innovations campus. The goal of the team presentations is to explain the project, how it was implemented, and show a brief demo to the judges and audience. All the teams delivered competitive pitches, which made judging very difficult. Huge thanks to Micah Whitacre, Nick Smith, Jon Miller, and Jared Moore for serving as our rockstar judges! The judges gathered together after the completion of the presentations to decide the top three performing teams. Congratulations to the following teams on your outstanding performance! Team Members: Scott Grissom, Alex Harder, and Matt Nelson This team was awarded the Golden Keyboard for their first-place achievement. They will defend the traveling trophy for the upcoming months and add a symbolic trinket to the collection for their team to be remembered in ShipIt history. The focus of this project was to provide a mechanism for customized cloud-based ACL testing. Utilizing serverless functions through the OpenFaaS framework , the team enabled rapid development and deployment of serverless functions which test a specific type of network access, e.g. using cURL to access a ReST endpoint or a JDBC client to access a database instance. Deploying these functions to the same cloud instance which hosts services provides a more accurate testing of network dependencies from the perspective of a service as opposed to testing from a developer machine. Team Members: Ian Kottman, Heather Boveri, Robert Ryterski, and David Crowder #crowdercrowd created a dashboard showing cluster utilization of applications across both on-premise and public cloud clusters. The metrics show how much CPU and memory an application is using versus how much it is requesting, along with an estimated yearly cost to run the application. This dashboard will be used to identify what applications can be scaled down to better fit their actual resource needs. Team Members: Jan Monterrubio, Maximilian Schroeder, and Jacob Zimmermann Bravo Avocado created a Maven plugin that starts a Docker container of a ReST service. It also allows the integration tests to dynamically connect to the correct URL and port of the container. This lets users run integration tests directly against the working code as part of the Maven lifecycle. Before the team wrote the plugin, they had to manually stand up the service and update the test configuration to validate any changes. By tying these eight steps to an existing command, users can streamline development for ReST services. Aside from our talented official judges, many associates join in on the ShipIt Day fun to act as peer judges in the People’s Choice category. The large audience during the ShipIt presentations was asked to get involved and vote for their favorite teams in certain categories. Favorite Team Name: Chef BoyarDeployments could be better (Steven Goldberg, Ryan Neff) Favorite Project: Audio Bot (Mitali Paygude, Kunal Suryavanshi, Vinay Datta Pinnaka) Best Presentation: Team Rhinos (Venkata Adusumilli, Veda Bhaskar Bhamidipati, Naga Prashanth Chanda, Sunand Kumar Matam, Prashanth Gajula)", "date": "2018-08-30"},
{"website": "Cerner", "title": "Infusing a Culture of Cybersecurity within Cerner Engineering", "author": ["Andy Nelson"], "link": "https://engineering.cerner.com/blog/culture-of-cybersecurity/", "abstract": "With October being Cybersecurity Awareness month , we thought it would be a good time to reflect on some of the things we do in engineering to educate our associates and infuse security into our culture. We have over 28,000 associates worldwide supporting hundreds of solutions with millions of lines of code. Each associate has a specialization, such as software development, system support, and consulting. Keeping everyone up to date on the latest in security is a difficult task. So how do we do it? We have teams dedicated to security that work directly within engineering. These teams have various responsibilities such as ownership of scanning tools and vulnerability tracking. My team’s goal is to bridge the gap by injecting security as a first-class citizen in the software development lifecycle. When working with developers, you have to make the right thing to do the easy thing to do. This is no different when it comes to security. In order to make security easy, we scan, assess, and create a plan for our developers to remediate their vulnerabilities. We promote the tools for scanning, help teams understand the results, and identify fixes for vulnerabilities. We run a monthly cybersecurity meetup which we use as a venue for associates to speak and learn about varying security topics. Andy Nelson opening the September edition of the Cybersecurity meetup Sebastian Brown presenting at the July edition. We also take advantage of opportunities like Cybersecurity Awareness month. We bridge organizational gaps to host a variety of security focused events, engaging associates in development, security, and operations to facilitate better relationships and collaboration. Events like these lower the barrier to entry for our developers to learn more secure practices, and embrace and celebrate the progress we are making in our security journey. We kicked off the activities this month with an external tech talk from Britney Hommertzheim . Britney, the Director of Information Security at AMC Theatres , presented on how we can better integrate security teams and developers. It was a great talk and you can watch the talk on our YouTube channel. We invited another external speaker for our Cybersecurity meetup a few weeks ago too. Caleb Christopher, a Technical Business Adviser at Allegiant Technology , gave a great talk titled “Defeating Email Fraud with DMARC”. Along side those 2 events, we have held lunch and learns throughout the month, and are wrapping it all up with an hour of security focused lightning talks tomorrow. Britney Hommertzheim giving an external tech talk about integrating security across organizations Security is not easy and we always have to strive to get better. Our development, operations, and security teams must work together, so we are doing our best to provide a forum for collaboration and sharing.", "date": "2018-10-29"},
{"website": "Cerner", "title": "2^5 Coding Competition 2018", "author": ["Jenny Saqiurila"], "link": "https://engineering.cerner.com/blog/2-to-the-5th-coding-competition-2018/", "abstract": "At Cerner, associates are always encouraged to develop tools and methods to improve their productivity. With this year’s theme being “Engineering Productivity,” Cerner’s fourth annual 2^5 Coding Competition provided associates another opportunity to dive into this subject and get involved in Cerner’s development culture . As usual, this year’s 2^5 Coding Competition was kicked off alongside Programmer’s Day festivities on September 13th. The competition lasts 32 days, and associates are encouraged to submit code every day. During this year’s competition, over 200 code snippet submissions in a total of 59 different languages marked another successful run of the competition. This year the participants were judged based on the following categories: Even though each submission had to be 32 lines of code or less, it didn’t stop the participants from brainstorming and executing great ideas ranging from Jenkins Pipelines , creating shell commands, and automating tasks for the tools we use every day. The many entries received exemplified how easy it is to increase productivity in an engineer’s daily life with just a few lines of code. As a participant, my biggest takeaway from this event was the learning experience it provided me. I was able to not only expand my skill set by working on the projects that I was passionate about, but also learn about different technologies. I found myself often inspired by reading through other participants' submissions. The restriction of 32 lines of code drove all of us as participants to write clean and concise solutions. It also made learning from each other’s code snippets fun and straightforward. This event also helped me understand how easy it is to find new project ideas. Some ways participants could easily get inspired include discovering a new library heard about in a Tech Talk , a cool new framework that your team started using, or even a complaint about how difficult it is to use a tool. All these things can inspire and drive participants to develop projects that are going to enhance their own personal knowledge about the new technologies or even help improve other people’s work efficiency. By combining the newly learned technologies and the inspirations I found, I was able to develop tools that make my daily work easier. By learning how to develop chrome extensions, I automated some steps in our timesheet submission system including automatically adding holiday timecode. Furthermore, by researching the ins and outs of shell scripting, I simplified various tasks in command line that would otherwise be complicated to execute. During the competition, I benefited from code snippets submitted by other 2^5 participants such as a bat file that enables users to open the GitHub repository URL from the local git repo. 2^5 provides a platform for anyone who wants to learn and share ideas, and it doesn’t require big time commitments. I believe it is a perfect opportunity for our engineers to get involved and innovate. The judges agreed that Jenny’s collection of submissions were most closely aligned to the 2018 2^5 theme of “Engineering Productivity.” Specifically, the judges noted the high usability of her automated timesheet submission tool. Mithun captured the award for the Most Obfuscated submission using an esoteric programming language that was comprised of symbols that outputted “Hello World.” None of our judges were familiar with this language and found his submission impressive, yet confusing. As the 2^5 competition runs for 32 days, the maximum number of languages that could be used by a participant is 32 languages. Susmita was named this category winner because she submitted all 32 days worth of code and used 32 different languages.", "date": "2018-12-14"},
{"website": "Cerner", "title": "DevCon: A tech conference for engineering, by engineering", "author": ["Melanie Taylor", "Brandy Poiry"], "link": "https://engineering.cerner.com/blog/devcon-recap/", "abstract": "In 2010 a group of Cerner engineers went to their leadership with an idea: a Cerner tech conference just for the internal development community, built and run from the ground up. The idea for a conference like this had been brewing at Cerner for several years. It was seen as an opportunity to create a large scale, sharing and learning experience for associates in Cerner’s technical workforces. From the beginning, buy-in from leadership was essential for the conference to succeed. In their original pitch, these engineers demonstrated how DevCon would create opportunities for knowledge sharing, networking, and collapsing long standing organizational silos by placing the teaching and learning opportunities into associates’ hands. The idea of associates getting together to share experiences  and lessons learned around topics they were genuinely passionate about resonated right away as an intrinsic benefit to the company. Leadership’s support combined with the efforts of associates driving the conference forward year to year have pushed DevCon to an entirely new level since 2011. The first DevCon had 600 attendees, all in Kansas City; now DevCon spans three locations - Kansas City, MO; Malvern, PA; and Bangalore, India - and in 2018 hosted over 7,500 attendees.  The three experiences included 25 corporate sponsors, over 150 unique, associate-led sessions, and 5 keynote sessions delivered by industry thought leaders. Each year involves a new quirky visual theme, ranging from 8-bit vintage video games to large film and book franchises like Star Wars and Marvel Comics. In addition to the keynotes and breakout sessions, attendees are given the opportunity to mingle and network with coworkers through trivia night, a social, lightning talks, and quick breaks for lunch and snacks throughout the day.  Also, DevCon is fun. It’s two days to step away from your desk and become immersed in a creative, accepting, and open environment. DevCon is a great example of the work that can be done across a wide variety of groups collaborating, allowing us to create something amazing. Some of these groups include: See some of our favorite DevCon talks over the years on our CernerEng YouTube channel: The Power of Pranks with Carl Chesser and Cornel Codrea Leagues of Sea & Sky with Dr. Jeff Norris Live Coding the Intersection Between the Arts, Research and Education with Sam Aaron Don’t Blink - a Pursuit in Cognitive Bias with Brandy Poiry & Michelle Brush Less Risk Through Greater Humanity with Dave Rensin", "date": "2019-02-08"},
{"website": "Cerner", "title": "ShipIt XIV", "author": ["Caitie Oder"], "link": "https://engineering.cerner.com/blog/shipit-xiv/", "abstract": "It is winter time in Kansas City, which also included our fourteenth edition of ShipIt, our routine hackathon competition we host at Cerner Engineering. Although the snowy weather may have kept participants between the warm walls of our Innovations Campus, the winter storm did not disturb the initiative and enthusiasm of the fourteen participating teams.\nCheck out the video below to hear from ShipIt Day participants and supporters as to why ShipIt Day has become an engineering-wide program that supports Cerner’s development culture. 1st Place- Bravo Avocado (Max Schroeder, Jacob Zimmermann, Jan Monterrubio) “We created “HCCgle” (pronounced “WHO-gull”), a search application for looking up HCC information by ICD-10 diagnosis code (e.g. - “E13.11”) or by term (e.g. - “Diabetes mellitus”), including across physician-friendly terminologies like IMO and SNOMED CT. The app presented which HCC categories the requested term belongs to (if any) and shows how its categorization in the HCC specification has changed over time between revisions. This was created using Java, React, and DropWizard as well as an Oracle database. Our primary use case for this project is for aiding support troubleshooting, as there was not a good way to find the HCC codes used in production.” – Bravo Avocado 2nd Place- Risky Salt (Kevin Eilers, Ryan Rickard, Pepper Pancoast) “We created an innovation/strategic roadmap prototype for Cerner’s clients that shows past, current and future projects, along with crucial data points for each. The data was directly from a Microsoft SQL server that houses all of our project management data, and the application was built on Ruby on Rails and React.”– Risky Salt 3rd Place- All the Data (Taylor Clay, Bilal Ahmad, Eric Ringle) “Our team worked on a prototype of a real-time flow sheet, that was a customizable data visualization tool for patient device data. In the real world, this would be used to view trends in a patient’s health to proactively identify risks. The goal of this project was to build a prototype UI support a dense flow sheet and graphical view of critical care data. This would include Cerner CareAware iBus and Cerner Millennium data sources, as well as, understand complexity to better gauge what functions to bring into the solution. This was created using Node-RED (created a mock service that published data to the app) and React (the application UI).” – All the Data Best Team Name - :(){:|:&};: (Ian Kottman, Sam Livingston, Paul Dennis, Bobby Ryterski, Anthony Ross) Best Presentation - Ship O’Holics (Sowmya Mathukumalli, Shrutha Kashyap, Yasho Jhamvar, Tejendra Velaga) Thank you to our talented judges Ben York, Adilson Ribeiro, and Scott Julius. Thank you to Kyle Harper for taking awesome pictures throughout our event.", "date": "2019-03-08"},
{"website": "Cerner", "title": "Pi Day 2019: Build your own alarm system", "author": ["Carl Chesser"], "link": "https://engineering.cerner.com/blog/pi-day-2019-build-your-own-alarm-system/", "abstract": "At Cerner Engineering, we love to celebrate Pi Day . This day is not only a fun time to enjoy eating pie and reflecting on mathematical properties, but we also share big announcements internally for our developers conference, DevCon . Drew a crowd to celebrate Pi Day this afternoon with snacks, lightning talks, and our DevCon 2018 theme reveal! #314day #cernerdevcon pic.twitter.com/wyy5eLKHVl For this post, we thought it would be fun to share a simple example of how you can hook your existing monitoring system to a physical alarm system powered by a Raspberry Pi . This alarm will be a red spinning light, the universal symbol of “something is going wrong.” We will build a program that will integrate with New Relic to determine if there are issues in our environment. If there are any issues, it will trigger the alarm and your monitoring system will come alive! Once you have this setup, we are going to use the Raspberry Pi to control your AC output by communicating to the PowerSwitch Tail through our GPIO pins . A Go program will control the logic of flexing the alarm on or off by polling a monitoring system: New Relic . New Relic is a real-time monitoring platform that gives you powerful insights about the applications you are operating. One of the features of New Relic, is that you can build alerts about different indicators of your application (ex. high memory utilization of a service). These can be rolled up to an “incident” concept, when you have a violation on an alert condition . For this physical alarm, it made sense to pair it with this concept that we use from New Relic. Therefore, if you build something that would trigger human engagement with your alerts (like an incident), this alarm can generically pick these up, without you having to manage anything else. Here is the code snippet of what we will implement. It is a Go program, which will interact with the GPIO pins using go-rpio . It will essentially run in a loop, and poll New Relic’s Alert API every minute. To ensure we aren’t running the alarm in the after-hours, we will also flex when this can trigger (ex. Mon - Friday, 9 - 5 PM). First, we will build something that can invoke the New Relic Alerts API . This will offer a single function ( hasOpenIncidents ) that will dictate if there are any open incidents when checking with New Relic. We will then manage the GPIO pin state in a simple loop which will check to see if there are any open incidents. If so, it will set the pin to High , which will trigger the light switch. Otherwise it will set it to low. We will also include a handler for setting the pin to low when we terminate the application (ex. via a SIGTERM). Example of managing the state: When you put it all together, the full picture of code looks like this ( alarm.go ): For my example, I have this in a alarm.go file within my nr-pi-alarm directory. I then issue the following command: This will produce a nr-pi-alarm you can then transfer to your Raspberry Pi for execution. One example of doing this is with scp : For this example, we are using the physical pins #12 and #14 (GPIO18 and GND). We will use our jumper wires to then hook this to the AC switch: You will then hook your alarm light to the AC switch (PowerSwitch Tail). If the alarm light has its own on/off switch, turn it to on, as we don’t want this manual switch to block what our Raspberry Pi is going to control based on the flow of power. After you have transferred the build to your pi, you can then configure the alarm to use your New Relic account. This is achieved by creating a config.yml file in your alarm directory which currently hosts the nr-pi-alarm program (set to 600 for file permissions): After you have configured it, simply invoke this to run the alarm: We hope you are having a great Pi Day and maybe this example will give you other ideas of what you can build to bring your systems alive. 😀", "date": "2019-03-14"},
{"website": "Cerner", "title": "DevAcademy Six Years Later", "author": ["Jake Kramer", "Brandy Poiry"], "link": "https://engineering.cerner.com/blog/devacademy-six-years-later/", "abstract": "Cerner hires a lot of software engineers.  In fact, we have hired over 1,600 engineers since our early career onboarding program, DevAcademy, went live almost six years ago.  In particular, we use DevAcademy to onboard early career software engineers. DevAcademy is a three part onboarding program built by engineers for engineers.  The first part of DevAcademy is DevEssentials which covers a lot of the basics new engineers need to know about development ecosystems in our development environment.  After spending a few days hearing from the software engineers who teach DevEssentials, our new hires move into DevCenter.  DevCenter is a performance based, hands-on training opportunity.  Engineers dig into real projects on small teams with the help of experienced mentors from teams across Cerner’s tech stack.  They work through their projects, adding new functionality to existing applications, and honing their craft with the help of the full-time engineering instructors who keep DevAcademy running as well as their mentors.  Over 700 engineers have participated as mentors providing feedback in code reviews, office hours, and scrum on more than 120 different projects.  One engineer said of DevAcademy, “I was able to (and encouraged to) work independently, make decisions and find solutions without always having to seek approval for them. I also got feedback when those decisions turned out to be bad, so I could make better decisions in the future.”  As soon as new engineers are writing, testing, and documenting their code in a way that meets the criteria Cerner has established for new hires, they go to teams across all areas of the company. New engineers working on their projects in the DevCenter Engineering managers report that DevAcademy engineers are able to make meaningful contributions to their new teams very quickly, usually within two months. Before DevAcademy existed, it would often take six months before a new engineer could provide valuable contributions to their codebase. Engineers go to their new teams already equipped with knowledge of the process, tools, and best practices that are standard across engineering teams at Cerner. New engineers working on their projects in the DevCenter After joining their teams, new software engineers very quickly dive into the work, learning the architecture and tooling as they go.  When they do need additional training to help level-up their skills, they utilize the third part of DevAcademy, DevElectives.  DevElectives focus on practices and tools engineers have identified as needs for their teams and architectures.  Like DevEssentials, DevElectives are taught by engineers and provide opportunities to hear from more experienced people while digging into new technology. In addition to providing an optimized orientation to writing code at Cerner, DevAcademy helps people cultivate relationships and networks that they will utilize throughout their careers.  Many groups continue to meet regularly to discuss the work they are doing and get pointers from their colleagues.  One Cerner associate said, “When I first joined Cerner, I had just moved and was alone in a new city. I was apprehensive, but excited. I had no idea that DevAcademy would be so formative to the friendships I created and networks I joined. DevAcademy was where I met my friends, some of whom I can already tell will be lifelong, where I met mentors who became the start of my network, and became inspired to be a mentor to continue expanding my network.” A mentor helps a new associate set up his code review In many ways, DevAcademy has become a staple of engineering at Cerner.  It is where engineers have a safe space to learn and grow as they begin their career.  It has become a proving ground for exploring new technologies and possibilities, and it gives more advanced engineers a way to improve their leadership skills.  More importantly, it helps Cerner shape its engineering culture; focusing on the future state of engineering and cultivating an environment in which engineers are prepared to join a team and spread the knowledge they have gained during training to improve engineering as a whole.", "date": "2019-06-14"},
{"website": "Cerner", "title": "CCL Unit and CCL Testing released to open source", "author": ["Fred Eckertson"], "link": "https://engineering.cerner.com/blog/ccl-unit-and-ccl-testing-released-to-open-source/", "abstract": "We are pleased to announce the release of CCL Unit and CCL Testing as open source code projects,\nas well as the release of the CCL Testing plugin artifacts to Maven Central . Cerner Command Language (CCL) is a database query and scripting language used with Cerner Millennium databases. CCL Unit is a unit testing framework for CCL written in CCL. The CCL Testing plugins generate reports for viewing the results of CCL Unit tests and static analyses of CCL programs and for generating code documentation from comments in the source code for CCL programs. These tools were created to help developers improve the quality of their CCL programs. We have released the code for these tools to open source to facilitate frictionless contributions of enhancements, corrections, and documentation improvements by the CCL developer community at large and to allow CCL developers to opt in for automatic notifications about version updates by subscribing to the CCL Unit and CCL Testing source repositories. We have released the CCL Testing plugins to Maven Central to eliminate the need to configure the location and access credentials for a proprietary plugin repository in order to use them. We invite you to visit the change log of each repository to see the many improvements made since the original releases. Most notably:", "date": "2019-08-09"},
{"website": "Cerner", "title": "Carbon Graphs: An Open Source Visualization API", "author": ["Abhijit Rao"], "link": "https://engineering.cerner.com/blog/carbon-graphs-open-source-visualization-api/", "abstract": "We are pleased to announce Carbon Graphs as an open source code project! Carbon is a lightweight, vanilla JavaScript visualization API built with D3 that integrates well with any consumer’s tech stack. It provides a variety of graph types that are framework agnostic and responsive out of the box. Over the past few years, we have gradually updated our user interface to use modern JavaScript libraries such as mithril.js and React . Our components needed a graphing solution that would work well with our current framework while providing visualizations based on leading industry solutions. We did an extensive audit of various Cerner solutions already available, the graph types they were using, and the libraries used to plot the data. We discovered that most solutions had graph implementations baked into their product such that they were modifying or extending open source libraries in an effort to support Cerner’s unique clinical and accessibility needs. These implementations, however, could not be broadly reused. As such, we saw an opportunity to collaborate with our User Experience team to create an attractive, modern, and flexible graphing solution that not only would meet Cerner’s design standards, but that could be open sourced to give back to the engineering community for their own graphing needs. With Carbon , you get: Graphs come with following settings that be customized: Carbon also supports functionalities that are not provided with popular open source libraries such as: Let’s see how easy it can be to get started! To create a line graph, first create an HTML element that will hold the graph. Here, we are specifying a main element with an id of root . From there, we will initialize a JavaScript object that configures various aspects of the graph, including where the graph will be drawn and how the axes should appear. Next, we’ll configure the dataset we want to plot. And to wrap it all up, we’ll call loadContent to draw the content. That’s it! We are continuously working on improving Carbon to support our ever-increasing clinical needs! Help us make it better by reporting issues using the GitHub issues queue or feel free to contribute with pull requests . NPM: https://www.npmjs.com/package/@cerner/carbon-graphs Site: https://engineering.cerner.com/carbon-graphs/", "date": "2019-08-27"},
{"website": "Cerner", "title": "Introducing F-Twelve, an Open Source Dev Console", "author": ["Patrick Gross"], "link": "https://engineering.cerner.com/blog/introducing-f-twelve-an-open-source-dev-console/", "abstract": "Many modern web browsers come with tools that can help developers debug their websites. If you are using Google Chrome or Firefox for example, try pressing F12. A new panel should open containing various tools. These tools provide a look “under the hood” for the current page. Common tools include a JavaScript console, JavaScript debugger, DOM explorer, network request viewer, performance profile, local storage manager, and more. Usually these tools are just a keypress away, but some environments don’t have browser tools. Alternate options exist such as Windows F12 chooser (which inspired the name F-Twelve) and Firebug Lite. These are nice but neither of them is a perfect solution. The former requires a certain version of Windows and the latter has not been updated in over 6 years. The kicker is, neither work inside an embedded IE frame. We ran into this issue while developing a SMART on FHIR application. Local development in a web browser would go smoothly but then we’d deploy to our test environment which used an embedded IE frame and it would not work. This was frustrating because it gave absolutely no indication of what the issue could be. It was typically either a blank white screen, or an eternal loading icon. The only way to troubleshoot was making a guess, adding alert calls, redeploying, and hoping it had something useful. This situation was occurring frequently and very inhibiting to development. We didn’t have access to F12, so we wrote our own JavaScript tool, F-Twelve. The initial version was simply a div at the bottom of the page that would print window.onerror events and anything sent to console.log. The functionality was very limited, but it solved our problem of being able to identify errors. Since then we have cleaned up the UI and added functionality. Current features include: For security reasons the console input does not execute arbitrary code, it can only parse and evaluate variables (e.g. window.location). Potential features for the future include: The project is still very young and the functionality that it has now is just the tip of the iceberg. The end goal is ultimately to provide all the functionality of modern browser’s Dev Tools without the need for a modern browser. If you want to contribute or learn more about the tool, check out the code or try the live demo .", "date": "2019-10-08"},
{"website": "Cerner", "title": "Terra UI: A Health-Care focused UI Component Library", "author": ["Brett Jankord"], "link": "https://engineering.cerner.com/blog/terra-ui/", "abstract": "Back in 2013, like most other companies, Cerner was heavily invested in building user interfaces with Twitter Bootstrap. Around that time, Dave Rupert wrote about the concept of Responsive Deliverables and touched on a key concept, creating “Tiny Bootstraps, for Every Client”. Along with this, Brad Frost had started promoting the idea of Atomic Design . We saw a lot of value in these ideas and saw that we needed to evolve how we were developing UI at Cerner. We could see that Bootstrap was no longer meeting our needs and we felt we needed to start the process of building our own “tiny bootstrap”. A small team was formed to begin work on a project named “Terra” to establish a component library for reusable UI patterns needed in our health care applications. We begin work building responsive, accessible, and internationalized components. The work was very similar to Twitter Bootstrap, but tailored to our specific needs at Cerner. We found great success and adoption with this new project but as tech evolved, we found our jQuery based UI components were showing a bit of age. Fast-forward a few years, ReactJS and ES2015 had started to grow in popularity. At Cerner, we started to see more and more teams developing solutions with ReactJS. We could see that if we wanted to continue to provide consistent UI solutions that incorporated responsive design, accessibility, and internationalization concerns, we needed to evolve our component library. We took this as an opportunity to build something new and share it with a wider audience. We decided to build a new version of our component library and base it on ReactJS. We also decided we wanted to make the project open source so anyone could help contribute to it. Today, we have a wide offering of UI components with a focus on health care applications in our open source component library, Terra UI . Terra UI is developed with the goal of helping consumers (including many app teams across Cerner, as well as external consumers in the open-source community) focus their efforts towards higher-level app concerns. We’ve leveraged the expertise of our UX team to create attractive and intuitive UI components that provide a consistent look and feel backed by usability research. Additionally, we’ve put a heavy focus into abstracting styling, accessibility, responsive design, cross-browser support, and internationalization considerations into our components so that consumers can get their projects up and running quickly. Cerner is utilizing Terra UI for healthcare web applications at scale with great success. We’re proud to make it available to other via open source and we hope you’ll check it out. Keep reading below to learn more about how you can consume and/or contribute to Terra UI. The Terra UI ecosystem covers three types of components. A repository containing a collection of common UI components ranging from buttons, to alerts, to form components, and more needed for building accessible, responsive, and internationalized applications. A repository that contains higher-order and composable UI components that build on top of terra-core components that help with application layout concerns and progressive disclosures. A repository that includes clinically focused UI components that build on top of terra-core. Along with our component repositories, we provide webpack configuration and testing utilities via terra-toolkit , linter configs for stylelint and eslint , a component doc site webpack plugin via terra-dev-site , and tooling to help aggregate application translations . To get started, we recommend checking out our guide to installing your first Terra UI component . We welcome contributions to Terra UI . Check our Github issues for ways you can get started with contributing, and be sure to check out our contribution guidelines .", "date": "2019-10-25"},
{"website": "Cerner", "title": "ShipIt Day XVII", "author": ["Sharynne Azhar", "Jordan Bush", "Harish Pendyala", "Vu Tran"], "link": "https://engineering.cerner.com/blog/shipit-xvii/", "abstract": "ShipIt Day, Cerner Engineering’s 24-hour hackathon, provides associates an environment to be creative, work on new projects, and network with other associates. This event is one of the many things that sit at the core of our Engineering Culture at Cerner, and is rapidly growing- now being hosted globally in Romania, Sweden, India, and the UK. In October, our team had the chance to participate in the seventeenth ShipIt Day at Kansas City’s Innovations Campus and managed to take home the Golden Keyboard trophy as the first place team! Left to Right - Sharynne Azhar, Vu Tran, Harish Pendyala, and Jordan Bush During this ShipIt challenge, our team’s goal is to create a dashboard that tracks newly discovered CVEs and displays any affected Cerner artifacts. The Z3R0 D4Y dashboard is a centralized place where teams can quickly go to identify and remediate any new security vulnerabilities and threats. New security vulnerabilities come up every day and keeping up with the latest updates can be tough. To take a more proactive response, we needed a way to get real-time updates of the latest vulnerabilities and identify which Cerner artifacts are affected. But where can we get this data? We created the Z3R0 D4Y dashboard (shown below) to help solve this problem. The dashboard gives an overview of different CVEs and which products are affected. It then allows you to drill in and see details about the affected products. Below is an example of a test app to show a vulnerability. The project is comprised of two components: the backend engine and the dashboard site. The engine subscribes to the National Vulnerability Database ( NVD ). The NVD provides a collection of data feeds of published vulnerabilities which are updated approximately every two hours. The Z3R0 D4Y engine runs nightly to retrieve the list of most recently discovered CVEs . The data then is parsed to identify details of the new CVEs and the list of vulnerable dependencies. Using the GitHub API ,  the engine then checks all of Cerner’s artifacts available in Github for matches based on that project’s dependency file. In Ruby, this would be the Gemfile.lock, for example. Finally, it publishes the findings to the Z3R0 D4Y dashboard. The dashboard site is a simple Github page that displays all CVEs, their description, publish date, and affected artifacts. Its content is updated automatically nightly whenever the engine produces new data. The site also supports sorting and searching across the CVEs and Cerner artifacts data. The nudgers teamed up to create a ‘Nudge’ behavior-changing native iOS or Android application that can be quickly delivered to Cerner Associates to begin using. The app has users select a single behavior they’d like to improve upon (drink more water, less screen time, etc), they will manually enter their day’s total, and the app will begin ‘nudging’ them to make small changes based on their entries Left to Right - Pat Walsh, Anna Luckey, John Moses, Justin Eddings This team worked on part two of a previous project they had worked on in a ShipIt Day, which is a virtual ICU Mobile Cart. In this hackathon they were able to implement device event listeners, encounter and location device association, conference calling integration, and data calls to the eHospital API. Left to Right - Taylor Clay, Bilal Ahmad, Duncan Dufva", "date": "2020-02-24"},
{"website": "Cerner", "title": "We’re Back with a New Look", "author": ["Carl Chesser"], "link": "https://engineering.cerner.com/blog/we-are-back-w-new-look/", "abstract": "If you have been to our blog before, you might notice that it has a new look. Our past version of the site was one that we had used for several years. It utilized Jekyll for the static site generation, which is a popular project that leverages the Ruby ecosystem. Over the last few months, we recognized that our site needed an update. This change wasn’t only with its look and feel, but also with what we used in building the site. Today, we have uplifted our site to use Hugo . We have been using this project internally for project documentation, and have found it to be a powerful, yet simple, static site generation tool. In this uplift, we wanted to make sure we could simplify our site generation to make it easier for other Cerner engineers to contribute to our site. To get this started, Alex Bezek and I decided to leverage Cerner’s last virtual ShipIt event to get through a first pass of the site. We were able to get through the first round of moving to Hugo, which greatly simplified generating the site, and it removed the additional burden of managing Ruby gem dependencies. We were also able to start reducing some issues we discovered in our site around accessibility, something that sparked our interest after listening to Jennifer Luker’s talk on this topic . Thank you Alex Bezek for your help and the Engineering Culture team for giving feedback along the way. We look forward to sharing new blog posts in this updated site format!", "date": "2020-08-04"},
{"website": "Cerner", "title": "Creating the Giant LiteBrite", "author": ["Paul Sites"], "link": "https://engineering.cerner.com/blog/creating-the-giant-litebrite/", "abstract": "Have you ever wondered what happens when you let two engineers loose on a project with a broad vision and little direction? In our case, LiteBrite happens. Every year Cerner hosts DevCon , an internal 2-day conference for all of Cerner engineering to come together, give talks about technologies, processes, and ideas - all with a little fun sprinkled in. DevCon always has a theme, for 2019 that was set to be “The 80s.” Along with many other decorations, the Engineering Culture team had a dream of having a giant LiteBrite for attendees to interact with at the conference. In the Fall of 2018 the Engineering Culture team approached us, Aaron Kaplan and Paul Sites, with their idea. Both of us are Cerner associates who are involved in Cerner’s MakerSpace administration, heavily involved in mentoring FIRST Robotics, and are known to be quite handy. We were asked if we could create a giant LiteBrite for DevCon to match the 80s theme. Culture team showed us a few examples, the primary one being the following YouTube video . With a few vague hints and videos, a budget of a few thousand dollars, and the goal of making something cool that lit up with pixel art, we got to work. Talking with the Culture Team we established “giant” to be somewhere around 8 feet wide and 6ft tall. We knew we wanted it to be big enough that multiple people could play with it and that any adult could comfortably reach all the pegs. We ran with that and started to think about peg density. We wanted to have enough pegs in the board so that fairly complex images could be created. Consulting with Cerner UX designer Jordan Lawrence, we experimented with different grid layouts and peg spacings to determine what is possible for resolution and content. While the staggered layout gave a better result on curved designs, we ultimately decided it was less intuitive for the users, and increased our design complexity, so we settled on a square grid pattern.\nAt this point we also decided for ease of construction that we would keep the board within the confines of a 8 ft x 4 ft sheet - a size many materials are readily available in. To balance cost with resolution, we settled on using 912 pegs total -  38 wide by 24 tall. We began by exploring the most simple route to creation: a white lit background, a bunch of holes, and a variety of colored pegs. After looking at the costs of colored acrylic rods we determined that to recreate the same spectrum as the original LiteBrite, while having enough pegs of each color, would cost over $8,000 dollars, just for the pegs. This design was quickly shelved based on the cost. Knowing that the “classic” option wasn’t going to work we thought back to that initial video. Pegs that are fixed, but that are interactive and respond to users pressing them. Paul contacted the owner of the YouTube video to find out additional details of his build. We learned that his version had 225 pegs, each peg was independent (they couldn’t be remote controlled or modified), and the project cost more than $10,000. Knowing we could build something more advanced than a “standard” LiteBrite within our budget, we started building upon the “modern” LiteBrite concept. We needed to ensure the following components were accounted for: We explored countless concepts and configurations until we finally landed on the following concept: At this point we were excited. We demonstrated we can have an interactive peg. Excellent. Then it dawned on us we want to have nine-hundred and twelve of these. We began to realize that what was easy for one was going to become very challenging to scale. Luckily, Adafruit sells an amazing product, the FadeCandy . This board is capable of controlling 512 LED’s, supports daisy-chaining, and it has great libraries to make displaying content moderately easy. The next challenge to solve was reading the 912 user inputs. Microcontrollers and other devices that are designed to handle large numbers of inputs still do not come close to meeting our needs, plus, they are expensive. Instead of creating one monolithic controller, we decided to use Arduino Nanos as addressable units of inputs. Each row of 38 inputs would be wired to a networked Nano with 5 shift registers allowing us to “expand” their inputs to 40 each. After the design was established Hans Perera, a fellow Makerspace Admin, suggested that to get all 24 of these done, a custom printed circuit board (PCB) would be our best option. Hans then stepped up and did the development on the PCBs while we forged ahead with the rest of the project. Custom printed circuit board (PCB) Hans placing components on the custom PCBs. The Culture team was unable to provide the space or equipment needed to create the LiteBrite. Fortunately, the FIRST Robotics team we mentor (Team #1987, The Broncobots), has a full machine shop that they allowed us to use for the project. With an estimated weight of close to 500 pounds, the LiteBrite was going to need a sturdy and stable frame. Joshua Wentworth, a Broncobots team mentor, volunteered the solution of a welded steel H-frame to hold the grid. This would allow for a compact, strong, and relatively low cost solution for the frame. The frame pieces prior to being welded together. Paul used the team’s CNC router to cut the 912 holes in the main board. Pre-cut acrylic pegs are expensive, instead we purchased 4ft sticks of raw acrylic and cut them to length ourselves. Each of the 912 pegs required an RGB LED, and due to the spacing of the pegs, off-the-shelf LED strips wouldn’t work. Instead, we used individual RGB LED chips, which required 6 soldered connections each, meaning 2,736 wires cut to length and stripped. Early estimates showed that we would need over 36 hours of just stripping the wires. Given our impending deadline we purchased an automatic wire stripper. Over 36 hours of wire stripping, reduced to about 2 with the automatic wire stripper. With Aaron’s soldering jig, a soldering pot for wire tinning, our LED testing controller, plus help from Cerner associates and several robotics members, we got to work mass producing our LED strands. Once we had a handle on the LEDs we turned to the buttons. Each button needed power and a discrete signal wire running back to the control board. With some forethought towards service and troubleshooting significant thought was put into wire management. Buttons were distributed among 6 separate panels that were able to be disconnected and removed, simplifying assembly and troubleshooting. We calculated the lengths of wire required to each button to prevent excess wire.  Using the automatic wire stripper we precisely cut staggered lengths of wire and created a wiring harness for each row of buttons. Precision cut staggered wires to use for making wiring harnesses for buttons. Starting to install the button panels behind the LED pegs. Time was of the essence and we could not wait for the physical hardware to be complete to develop the software. Our technology choices allowed for software development to happen in parallel to the physical construction. We needed the color of a peg to change the instant a user pressed it to give the best possible user experience and encourage interaction. This necessitated that certain software components be optimized to minimize input lag. Each Arduino is responsible for the 38 buttons attached to it, and because of the multi user need, extra care was taken to ensure there were no double presses or miss counting occurring. Every Arduino is assigned a static IP address that correlates to the row it is listening to. Whenever a button in its charge is pressed, the Arduino broadcasts a UDP packet containing the state of the entire row in the form of a binary string. Using the IP and the index in the string one could know the (X,Y) position of  the button that was pressed. Bitwise operators were leveraged to ensure buttons were only registered once per press. With 38 buttons to connect to one tiny Arduino, all with white wires there was a high risk of connecting a button to an incorrect input. Starting to install the button panels behind the LED pegs. We considered labelling each wire, but that was expensive, time consuming and still error prone. Then it hit us, lets plug in buttons at random and make the controller do the hard work. From this the Arduino “training'' mode was created. In training mode each peg is pressed in order. The Arduino records the presses and creates a map to correlate expected input index to the physical input pin. Doing this added minimal processing overhead but streamlined assembly and will aid in troubleshooting. This is the main control software, it receives the UDP packets sent from the Arduinos and sets the LEDs through the FadeCandy board. When a peg is pressed the packet is sent that contains the (X,Y) coordinate for that button. The computer then updates the corresponding LED pixel at the matching coordinate. Each tap of a peg cycles a pixel through a list of available colors. Although we set out to create a LiteBrite, what we really created is better described as an interactive dot matrix display. This has several advantages, the largest of which being able to display premade static, and dynamic content. A good example of this in practice is our clear functionality. When the clear button is pressed Pac-Man will chomp across the display eating peoples designs and resetting all pegs to black. A big question we knew would come up is, “Is my work lost?” As the LiteBrite records ever press, we can state that no work is lost. We can even recall unique designs and play them back as a timelapse to showcase people’s work. By collecting this data we also know exactly how many presses the LiteBrite has had. As of the time of this writing, we’ve had 443,449 presses and counting! Ultimately, LiteBrite was completed at about 1:10 a.m. the first day of the DevCon event. It was loaded into a trailer and delivered hours before the event started that morning. LiteBrite was a whirlwind of a project that involved at least 1,100 people-hours over the course of around a month and a half. Although we led the primary design and construction, over a dozen people volunteered their time to help us light up DevCon. LiteBrite has occasionally made appearances around Kansas City, appearing front and center at Maker Faire for over 17,000 attendees to interact with. LiteBrite was invited to Cerner Health Conference in October 2019. Today, when LiteBrite isn’t traveling it lives at the entrance to Cerner’s Innovations Campus. It greets associates every morning with a hot cup of coffee, plays back select LiteBrite creations, and encourages associates to take some time to create. This article includes contributions from Aaron Kaplan.", "date": "2020-08-26"},
{"website": "Cerner", "title": "Lessons learned building an accessible web application framework", "author": ["Matt Henkes"], "link": "https://engineering.cerner.com/blog/lessons-learned-building-an-accessible-web-application-framework/", "abstract": "Since 2017, we at Cerner have been building Terra , a single page web application framework, and accessibility has always been a primary goal. Over the years we’ve learned a lot about how to build and test accessible websites, as well as the diverse group of people we’re serving. With this blog post, we’d like to take a moment to share what we’ve learned so far. At Cerner, we build software for many different users in many different roles. Doctors and nurses may first come to mind as the primary users for our EHR, but hospitals aren’t made up of providers alone and neither are our users. We also provide applications for administrative users and with our patient portal application ( HealtheLife ), our users are the general patient population. Naturally, with such a broad user base a percentage will require assistive technologies to be able to use our applications. To be as inclusive as possible, Cerner applications must be as accessible as possible, and the Terra Application Framework is one way we are working towards that goal. I’ll be honest, when we started in 2017, I thought creating an accessible website simply meant appropriately applying ARIA labels to DOM elements. In the last few years, we’ve learned, with the help of our accessibility researchers, that creating an accessible site is so much more than that. So, let’s hop in our time machine and discuss how we should have approached accessibility with appropriate hindsight (foresight? Time travel is confusing). As we discussed above, our broad user base includes a diverse array of people and an equally diverse array of accessibility needs. When discussing web accessibility, it’s common to assume that means ensuring a screen reader appropriately can navigate and read your site. Screen reader compatibility is an important aspect of accessibility, it assists users with impaired vision. However, a screen reader won’t help users with impaired mobility. To help us cover a wider array of expected interactions and needs, we’ve compiled the following use cases. These categories may also be experienced temporarily, where individuals may need improved accessibility due to an injury, limited device capability, or location. For each of the above use cases, we’ve identified the appropriate assistive technologies to apply. Focus on helping the user access the site content by ensuring appropriate contrast ratios, font sizing, spacing, color usage, and screen reader support. A good way to get a feel for this user’s experience is to enable a screen reader (all Macs have VoiceOver ), close your eyes, and navigate your site. Does it make sense? Focus on helping the user navigate the site using devices other than a cursor by enabling keyboard navigation and supplying redundant information access. For example, providing alternative methods to take actions that require gestures on mobile devices. Focus on applying layout simplicity to our workflows and components as well as improving keyword comprehension by emphasizing common design patterns and expected behaviors. For example, buttons should not navigate a user to a new page, that is the role of links. Provide a consistent semantic layout and access to information regardless of form factor (desktop, mobile). Tooltips are a good example of this. On the desktop, tooltips are accessed when hovering over the element; where as on a mobile devices touch screen you cannot hover. The information contained in the tooltip has to be offered in a way that can be accessed on both devices. Accessibility cannot simply be layered on any component as an afterthought. This was a mistake we made early on. We’d work through designing and building a complex component, only to test the screen reader compatibility at the end. Unsurprisingly, the component would fail, and we’d have to repeat the process again to correct the issues, sometimes with a complete redesign. Just as engineering constraints inform a design, so to do accessibility constraints. Designing for them up front will reduce expensive re-work late in a project. As you can see, there is far more about accessibility to consider than ARIA labels. We understand that implementing accessibility on the web is a complex and difficult task and we’ve built the Terra application framework to help. Terra provides a stable base on which to build accessible applications. The Terra application framework and each Terra component are designed and built to meet our accessibility standards . To help ensure that our components meet these standards, we perform manual testing, as well as, automated testing using axe-core . While using Terra gives you a good head start on building an accessible application, you cannot assume your application is fully accessible. The way Terra components are assembled and labeled in an application have a large impact on its accessibility. Any custom components need to be vetted with the same care as Terra components to ensure accessibility. Application owners should make sure to test their apps using a screen reader. Developing applications for the web is inherently complex. Adding accessibility does not simplify development. By ensuring accessibility is applied to the Terra application framework and components, we are making the right thing, the easy thing to do for applications utilizing Terra. Furthermore, this has made our products more predictable in how users access their functionality since it is being consistently applied. It has been our experience that implementing a consistent semantic layout and a sensible keyboard navigation order lead directly to a logically organized information architecture. If you would like to learn more about Terra, check out its documentation . It’s all open source and we’d love to hear what you think. In closing, I’d like to provide a quote from my friend, Neil Pfeiffer, UX Designer, that I think sums up our mission nicely. Our job is to help all users better understand their surroundings, providing them not only with clear and direct informational context, but also what choices and actions they can make based on that information, and always put them in control. Thanks for reading, and remember, we’re all in this together :)", "date": "2020-09-24"},
{"website": "Cerner", "title": "ShipIt from Anywhere: Our Lessons on Going Virtual", "author": ["Whitney Roark"], "link": "https://engineering.cerner.com/blog/shipit-from-anywhere-lessons-learned/", "abstract": "ShipIt Days are Cerner Engineering’s 24-hour hackathons where associates are empowered to brainstorm, plan, build, and present a project of their choice. This quarterly event is something associates enjoy by taking a break from their normal routine and focusing on a problem that they seek to achieve within a short time period (one day). The enjoyment of these projects often come from just working on something different, working with other colleagues, and applying their passions to an area they have been desiring to solve. With the pandemic this year, we had to rethink how can we continue to achieve this same associate enjoyment and fulfillment while only being in a virtual format. Moving this event to a virtual format has created a lot of challenges! During a typical in-person ShipIt event, all participating teams would gather in one room to collaborate on their projects. Participating teams would be provided with awesome swag, a fully loaded snack bar and an evening movie to give a fun atmosphere for those who enjoyed to be attending in the evening ( check out our recap video for what it was like on our campus). Another very anticipated part of the event is the live demo portion where each participating team shares what they created in the last 24 hours. At the end of the presentations, the judges and people-choice winners get announced in our assembly, where each team then gets to pick from a line-up of prizes, with the 1st place team receiving the traveling trophy: the golden keyboard. It was a fun moment to wrap up the event and capture photos of all the teams. Our goal for the virtual format was to keep the core aspects of the event the same. ShipIt participants enjoy collecting our signature t-shirts quarter-over-quarter, building a variety of colors into their wardrobes. We knew we had to keep this tradition alive! To get our associates excited for the event, we mail a swag box directly to each participant’s home. During the actual event, we utilize Microsoft Teams to communicate and collaborate with our participating teams. To keep our teams engaged with each other during the event we host several virtual activities throughout the day and night. Our most popular activities have been Trello’s GIF battles , virtual bingo and a JackBox game hour! The live demo portion is streamed using Microsoft Teams as well and is available for all associates within the company to tune in. While moving this event to a virtual format has been challenging, it has also created some new opportunities and growth! When ShipIt Days were hosted in person, participation was limited to local associates. Now that the event is virtual, participation is open to all associates in any location. Associates from different parts of the world can now team up and compete together. We have been able to bring associates together from the US, Brasov, London and India to all compete in the same competition! Even in this new virtual world our associates continue to turn their creative ideas into reality during these hackathon competitions. Continue reading to learn about some of the new things our associates have developed during our virtual ShipIt days and how we are filling the void of the golden keyboard trophy! One of our favorite activities to do with our virtual ShipIt participants is virtual bingo! The traveling golden keyboard trophy is safely locked up on campus. Our virtual winners receive a custom 3D printed version of the trophy that is mailed directly to them. These trophies are made by one of our own associates, Kyle Harper! Naresh Rayapati, Scott Buchholz, Matt Nelson The SquirrelOps team wanted to increase the speed that we would go from commit to deployment in our development environments, as well as provide a more holistic view of the state of the system across our various tools, which included GitHub , Spinnaker , New Relic , Artifactory , and Jenkins . Previously we had some automation however there were gaps and missing signals that kept the process from being as seamless or as face up as we’d like. While a commit to GitHub would trigger a Jenkins build and publish the new artifacts to Artifactory, human intervention was then required to in turn get these artifacts deployed where they could be consumed. By integrating these existing automation tools we were able to use the various APIs to not only go from commit to deployment, but to push additional facts to New Relic and GitHub allowing for improved observability into the system. The existing GitHub webhook and Jenkins CI flow was enhanced to invoke the Spinnaker API to kick off the appropriate pipelines to deploy the artifacts in the Kubernetes cluster. The Spinnaker pipelines were in turn enhanced to pull the docker tag information Artifactory and to then publish that information to both GitHub and New Relic using their deployment APIs. This allows the state of the system to be reflected in those tools in a way that provides additional context to the developer. Upon completion of the deployment, the pipeline would also then report the state on the deployment to GitHub, allowing quick visibility of the deployment history and status of a given change for that specific commit. Ryan Lokugamage, Kyle Berkley, Juzer Zarif As working at Cerner became 100% virtual, our ShipIt team looked for ways to fast-track parts of the software engineering process. Every developer (and every associate) knows keeping up with their email can be tedious, often relentless. Because GitHub sends an email for all activity on every pull request an engineer is involved in, our team realized we could organize this information in a user interface that was easier to access and more concise. Thus, we decided to use Google Chrome’s API to develop a browser extension with a simple, meaningful UI that organizes all activity related to a developer’s active pull requests. We decided we would call the extension Git Selfish . We utilized the React library to quickly construct a pop-over style user interface. We also leveraged GitHub’s GraphQL API to select specific pieces of information that felt most relevant to a developer in order to keep them up-to-date on a pull request’s state. Using GitHub’s GraphQL API also allows us to create a subscription-style listener for updates to each active pull request that a user might be interested in. The team was particularly excited about this project because of all the possibilities it presented. As with all ShipIt projects, there is so much room for creativity and inspiration that it is difficult to know where to start, as well as where to end - after all, we only have 24 hours! David White, Evan Rauner, Garrett Atwell, Max Alvarez, Rahul Joshi On-call can be a stressful time for any engineer, no matter their experience or what technology they are supporting. A typical shift can largely be consumed by alerts that take manual, repetitive actions to resolve. We wanted to enhance our support automation tooling to apply good mitigations when it proactively detects issues with the system and avoid engineers from having to be initially engaged to apply these actions. We found this automation could greatly reduce the number of hours a week that is spent on first engaging on alerts, which can reduce on-call fatigue and continue on our path in building a self-healing architecture that requires minimum human engagement. Services on existing infrastructure can be impacted from several different causes. This can be from a lack of system resources (memory, swap space) or just network segmentations with other service dependencies. Often, there are good mitigations that can be first applied to these services the point of alerting. We wanted to make sure these mitigations are automatically applied before any human operators are involved. We tackled the problem by starting with the code base we inherited. The existing code supported monitoring and identifying issues with other system dependencies. This evolved into becoming our on-call tool which could then take actions. At a high level the automation would have to take alerts (in this case from Zabbix ), apply good mitigations to resolve (ex. restarting a process), and then inspect that it is resolved (ex. run commands to verify system state). Given this view, a basic workflow is represented below: The project is comprised of a client-side library implemented in Go and a Redis database. The client performs three operations: poll alerts from the Zabbix API at regular intervals, run a workflow for each alert from the poll, and reports the automation outcome. The workflow for an alert first checks if the dependencies of the services running on the node are healthy, documents services failing based on their health checks, and attempts good mitigations to resolve the service issue. If they return healthy, the workflow captures the results and terminates. The Redis database is used to track the state of the automation across the different parts of infrastructure. Our transformation of ShipIt in becoming a virtual offering required rethinking some of our traditions. As we made this transition, we found this event continued to live on through teams participating and even had an increased in audience participation to see the presentations. We are excited to see how our ShipIt event continues to grow as we adapt in this ever changing world.", "date": "2020-12-03"},
{"website": "Cerner", "title": "Introducing Inspec Delta", "author": ["Jeremy Perron"], "link": "https://engineering.cerner.com/blog/inspec-delta/", "abstract": "Today, we are excited to announce our newest addition in our open-source family, Inspec Delta ! In this blog post, we will explain how this is utilized at Cerner to reduce the time in comparing updated infrastructure security benchmarks. Whether you are new to automating your security compliance or have had to deal with maintaining updates to existing benchmarks, this blog post is for you! As security standards continuously evolve, the Defense Information Systems Agency (DISA) releases a new revision to their STIG (Security Technical Implementation Guide) for each Operating System major version, as well as certain applications. These updates can range from minor metadata-only changes to a bevy of new, highly technical, system configuration assertions. To maintain the highest security posture, systems need to implement and validate compliance quickly after new standards are released. Compounded with the above is the complexity of altering so many profiles and profile controls at once with accuracy. The metadata provided by DISA for tools like Inspec are delimited by architecture, but as humans, we have to break this down even further into manageable chunks. However, even with stringent organization into separate profiles and controls using a compliance tool like Inspec, the sheer volume of the data can be overwhelming when handling the process manually. Since the STIG is just a set of guidelines and rules, the owners must write implementation details themselves. With a new release of a STIG, we cannot simply replace the old with the new. Implementation details would be lost in this case. There was no tool on the market that would allow us to merge in the changes from a STIG into our existing profiles. To maintain compliance with the STIG and audit our systems, we maintain this codification of the various STIGs as a set of Inspec profiles. Inspec is a scanning tool from Chef Corporation. These profiles encode each rule from the STIG as a “control” in its own file and each profile contains on the order of 200 controls. We can then use these controls to check machines to determine whether they are in compliance with the STIG. The following steps are the way a profile would need to be updated without the help of inspec_delta. This assumes that no other tools are going to be utilized for the updates. The following steps are the way we internally update a profile utilizing inspec_delta (a command-line utility). This ultimately arrives at the same outcome as the manual steps. If we compare the workflow between the manual steps and the steps we use with Inspec Delta, we can see many improvements in not just efficiency, but also accuracy. Inspec Delta has helped us save time in staying current on technology guidelines from DISA. We are excited to open-source this project so that it can help others like you! If you would like to learn more, check out Inspec Delta .", "date": "2021-05-11"}
]