[
{"website": "NextDoor", "title": "swift packages in ios app", "author": ["Abhijeet Kumar"], "link": "https://engblog.nextdoor.com/swift-packages-in-ios-app-fc12ee030f15", "abstract": "Culture Open Source Technology Our app has been around since summer of 2013 (codebase since late 2012). It was 100% Objective-C and our dependency manager was Cocoapods. We adopted Swift late 2014 and since then, we’ve continued to adopt Swift as a preferred programming language for iOS. Barring third party dependencies managed by Cocoapods, our codebase has pretty much been a monolith. While having a monolithic codebase does have few advantages , it creates a few practical challenges. Entire codebase is compiled even when you just change networking code. You end up running hundreds of unrelated tests locally and in CI, which doesn’t make for a fast developer iteration cycle. That led us down the path of breaking down our monolith into Frameworks and more recently into Swift Packages managed by Carthage . In 2015, we broke our monolith application into separate frameworks. This allowed us to separate concerns and speed up the developer iteration cycle. Just like our application target, Frameworks had a mix of Swift and Obj-C. These frameworks sat next to our third party dependencies managed by Cocoapods. However, these frameworks were in the same repository and shared the same Xcode Workspace file. While it did help to be able to build an individual framework and test your changes, we were still committing to the same repository and riding the same CI pipeline. We’ve used Cocoapods since we first released our application in 2013. Cocoapods makes adding a dependency in Xcode Projects simple. However, third party dependencies are built inline with your application’s source code that Xcode has to know about and manage. Cocoapods is centralized in nature and though possible , it is a tad difficult to maintain private dependencies. Above challenges led us down the path of trying Carthage as our dependency manager. As soon as we tried it, we made two quick observations. First, Carthage doesn’t fight Xcode Project files and settings, it works with it. Second of all, it’s fast. It has built-in support for caching. Incremental builds can skip the build part for dependencies, they just need to link with dependencies. The advent of Swift Package Manager (SPM) had always interested us. Carthage allowed us to start splitting our app’s infrastructure code into “Swift Packages” in a fairly straightforward manner. Some Swift Packages are private repos, some are public . These repositories have their own CI pipelines and some even run on Linux. I strongly recommend you to consider Swift Packages as a strategy to break down the monolith. Carthage relies on xcodebuild and needs the Xcode Project file. After downloading all the Carthage dependencies, we can now go into each SPMs and generate respective Xcode Project files (s ee image and appendix below ). That’s right, we do not check-in Xcode Project files into SPM repositories. Once Xcode Project files are built, Carthage can now build Frameworks for our app to link with . Carthage will also seamlessly work with SPMs that have another implicit SPM dependency. That’s all I have for now. I have included a few code snippets and screenshots that can come in handy in the Appendix below. Happy to answer questions should you have any. If challenges in mobile engineering interest you, get in touch — we’re hiring ! Authored by Abhijeet Kumar Appendix Custom steps for SPM post dependency downloads Carthage can now build Frameworks Sample Xcode configurations for iOS SPM’s command that generates Xcode Project file honors xconfigs . This allows us to easily override defaults and apply custom settings. Nextdoor is the neighborhood hub for trusted connections… 69 Thanks to Mikhail Simin . Swift Nextdoor iOS 69 claps 69 Written by @Nextdoor, @workspotinc Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by @Nextdoor, @workspotinc Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-27"},
{"website": "NextDoor", "title": "bender", "author": ["Slava Markeyev"], "link": "https://engblog.nextdoor.com/bender-ff65a6edee92", "abstract": "Culture Open Source Technology Serverless ETL is becoming the future for teams looking to stay focused on their core responsibilities rather than running a large infrastructure to power data pipelines. Nextdoor’s mission is to build community, and I joined the Systems Infrastructure team to help support product engineers in achieving that goal. The team’s responsibilities include managing the data ingestion pipeline which services 2.5 billion daily syslog and tracking events. We found that, as our data volumes grew, keeping the data ingestion pipeline stable became a full time endeavor that distracted us from our other responsibilities. Upon joining Nextdoor, my first project was to replace the aging Apache Flume -based pipeline , which served the business well for years. As we investigated alternative solutions, we had two use cases in mind: Streaming logs into Elasticsearch with an SLA of 1 minute. Writing partitioned batches of data into S3 with an SLA of 5 minutes. Specifically, the current hour’s data must be available no later than 5 minutes after the end of the hour. This is to support downstream ETL processes. Obviously, a streaming solution lends itself well to these requirements and there are a lot of options in this space. The popular traditional solutions include Flume, Kafka+Storm , Kafka Streams , Flink , Spark , and many others. As well as up and coming solutions like Kinesis+Lambda from Amazon, Cloud Dataflow from Google, Azure Functions from Microsoft, and IronWorker from Iron.io. All of these solutions have scale, performance, operational, and monetary tradeoffs, but there are two overarching schools of thoughts which stem from the advent of cloud based infrastructure as a service. You can either invest in operating your own infrastructure or offload that responsibility to an infrastructure as a service provider. Amazon Web Services (AWS) pioneered this field and this now allows many companies, like Nextdoor, to focus more on developing product rather than running infrastructure. Cloud providers like AWS have gone beyond simply offering to offload the hardware aspect of infrastructure by packaging and offering SaaS for commonly used services like webservers, databases, and now Big Data platforms. For instance, Hadoop’s core services haven’t gone away but rather have been packaged into services like EMR, Qubole, and Altiscale to decouple running a cluster from running your queries and ETL jobs. Nextdoor’s Data/BI team uses Qubole in this manner. Each of the Serverless options from Amazon, Microsoft, Google, and Iron.io have subtleties but the premise is you are able to create and upload code that is automatically run for you. You no longer need to worry about managing servers, services, and infrastructure. All of that is handled for you. This, of course, does not come without monetary cost — but in most cases, you will find cost savings in no longer running servers or not having a dedicated engineer operating your pipeline. How do you fit ETL into this concept? Most of these providers also have various data store or data stream technologies which can be linked with your code. For instance, AWS Lambda is able to trigger your code each time a file is uploaded to S3 or events streamed to Kinesis or written to DynamoDB. All you have to do is supply is code to process that data. Here’s an example of a Amazon Lambda function written in Python: You can see how Serverless is a powerful shift in how ETL is performed. However, it is still a young approach and is missing some pieces such as standardization of code. Engineers are able to write lightweight functions to process data but this often leads to fragmentation of approaches within teams, organizations, and the greater open source community. Boilerplate logic for configuration, exception handling, deserialization, transformation, transport, retrying, and monitoring is constantly reimplemented, mis-implemented, and in some cases punted on. I aimed to solve this problem by developing Bender. Bender provides an extendable Java framework for creating serverless ETL functions on AWS Lambda. Bender is split up into two components, Bender Core handles the complex plumbing and provides the interfaces necessary to build modules for all aspects of the ETL process, while Bender Modules implement the most common use cases. When designing Bender, we wanted a function that we could easily reconfigure to handle different use cases while leveraging common boilerplate logic like error handling, statistical reporting, retrying, configuration, etc. Bender Core performs the batching and routing of data and provides interfaces for aspects of the ETL process. Out of the box, Bender includes modules to read, filter, and manipulate JSON data as well as semi-structured data parseable by regex from S3 files or Kinesis streams. Events can then be written to S3, Firehose, Elasticsearch, or even back to Kinesis. It is easy to implement additional support for different data log formats found in AWS and on machines such as syslog, apache logs, VPC flow logs, etc, as well as different data sources such as DynamoDB and destinations like a SQL database. github.com A common use case is to re-partition data files written by Firehose into a different structure. With Bender, this is easy to do with the partition operation. For instance, if your json has 3 fields ( HOST , FACILITY , and EPOCH ) that you wish to partition by, your configuration may look like this: Additional examples with full configuration and documentation are provided in Bender’s sample configurations . In a future part of this series, we will focus on the ins-and-outs of how we use Bender in our infrastructure and the implementation and design decisions we made when working with Lambda, Kinesis, and Elasticsearch. Working with cloud based services doesn’t eliminate engineering effort — rather, it shifts it. My team, Systems Infrastructure, helps bridge the gaps between servers and services so that our engineers can focus on the main goal of Nextdoor: building a private social network for neighborhoods . This means that sometimes we run services and other times leverage cloud offerings while writing projects like Bender to make life easier. Find this sort of stuff cool? The Nextdoor engineering team is always looking for motivated and talented engineers. nextdoor.com Nextdoor is the neighborhood hub for trusted connections… 300 3 Thanks to Vikas Kawadia . AWS Serverless Etl AWS Lambda Data Pipeline 300 claps 300 3 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-10-02"},
{"website": "NextDoor", "title": "how nextdoor made a 10x improvement in release times with docker and amazon ecs", "author": ["Niall O'Higgins"], "link": "https://engblog.nextdoor.com/how-nextdoor-made-a-10x-improvement-in-release-times-with-docker-and-amazon-ecs-35aab52b726f", "abstract": "Culture Open Source Technology At the beginning of 2016, Nextdoor production releases took about an hour. That is to say, once all the code in the new release was tested and verified, it would still take an entire hour before our users saw those changes. Packaging the software took about 25 minutes and then actually deploying that package to production would take another 30 minutes. “That’s madness,” you say. “Why on earth did things take so long?” Let’s dive deep into what made this so slow, and how we made a 10x improvement. Before we release code to production, our engineers perform acceptance testing for their changes on a production-like environment, which we call staging. Product managers, designers, and engineers can use staging to help verify new features or behaviors before the code is released to all of our users. Staging has a similar dataset to production, and runs all the services in the same way — the only difference is how many EC2 instances are powering it. Our staging deploy pipeline used the same architecture as production; hence, staging was similarly slow to update. This meant that we were heavily limited in the number of changes we could test per day on staging. If you’re an engineer working on a complicated new feature and you only have 2–3 opportunities to test it in a full production-like environment per day, that severely reduces your speed of iteration. Slow staging and production turn-around was hurting our engineering team’s ability to move fast and get new features out quickly. Not only was this process slow, it was also far more manual than we wished. Each release would require running several scripts and inputting values by hand, which introduced potential for operator error. None of the release process should require manual intervention — we knew there should just be a single “deploy button” to push. Finally, a frightening implication of all this lag was that if we needed to push an emergency fix, we’d be looking at a minimum of a 1 hour turn-around — and potentially almost double that if we needed to test it on staging first. This meant that when things went wrong, we tended to roll back the entire release rather than roll forward with a patch. Rolling forward is preferable to rolling back, since important features and fixes aren’t reverted due to an unrelated bug. It was obvious to us all that the slowness in our build and deployment pipeline was becoming a major drag on productivity. We needed to dig in and solve this. One of the non-technical reasons why things were so slow was that the various components of the build and deploy processes didn’t have clear team ownership, and had been neglected. The innards of the build & release systems had grown organically over the years and consisted of spaghetti code with various fixes slapped on top. We’d never taken a step back and thought holistically about the entire system and its architecture. We decided we needed to create a new team which could focus on repairing our build and release pipelines. Staffed with 3–4 engineers, the high-level goal of the Dev Tools Team has been to maintain an engineering platform that supports rapid development and release of high quality code. Our philosophy is that our engineers are customers, and our job is to make their experience developing, testing, and deploying their work pleasant and efficient. Our plan was simple and consisted of 3 steps: Understand the existing build and deploy pipeline. Automate it. Optimize it. And our team for this project: Our primary service is a large Django/PostgreSQL monolith which runs under nginx/uWSGI. Python is an interpreted language. It doesn’t require compilation. This might make you think that there’s nothing to build, but that turned out to be not entirely true. We distributed our release to our servers as a self-contained binary Debian package. This Debian package contained all the application source code, the Python virtual env, uWSGI and various other binary requirements. In order to build Debian packages in a clean, reproducible way, you need to use a tool such as sbuild or cowbuilder which effectively build them in an empty chroot environment . This is great — except for the fact that you need to populate the contents of the chroot with system libraries. Anyone who has used a Debianoid Linux is familiar with waiting for apt-get install to complete. We also needed to compile frontend assets ( using Webpack and associated tooling ), and push those to the CDN before starting the deploy. Finally, the Debian package itself needed to be uploaded to our Debian package host — which could take several minutes in practice for bizarre reasons which I’m not going to delve into here. Since we had not architected any of this for end-to-end performance, we ended up with a breakdown like the following: apt-get install system package into Chroot (10 minutes) pip install Python packages (5 minutes) Compile frontend assets & upload to CDN (5 minutes) Upload resulting Debian package to hosting provider (5 minutes) This added up to around 25 minutes. So we have a binary Debian package. It should just take a few minutes to push that out to our servers, right? Not so fast. We had architected our deploy to use a Red/Black process inspired by Netflix . We would boot an entirely new set of servers to run the new release, install the Debian package onto those and then switch traffic at the load balancer to point to the new servers. In case of a rollback, we still had the old release running, so it would simply be a matter of pointing the load balancer back at the old servers. This architecture has a number of nice properties — such as exercising our ability to replace machines frequently. However, the cost was that it was very slow: Bidding, booting, and configuring the new EC2 instances (25 minutes) Wait for new version of services to become healthy and join load balancer (5 minutes) This added up to around 30 minutes. Staging used the same Red/Black architecture, and so had a similar performance profile. One final note on this architecture is that you end up booting tons of new instances. Each time you do this, there is a chance of a failure. EC2 itself can have outages, Apt repos can go down, etc. At a certain scale, we began having quite frequent failures in bringing up new instances — which very often required painful manual intervention to work-around. This kind of operational toil was a drag on our team. There is an old rule-of-thumb in software engineering that, if you find yourself performing the same manual task three times, it is worth automating it. Our release engineers were doing a lot of repetitive manual tasks, and we had tons of opportunities to apply this adage. Here are some of the most onerous examples of manual tasks in our staging pipeline: If an engineer wanted to get a commit into the existing release branch, they would manually add a comment with their SHA to a special JIRA ticket. The release engineer would periodically run a “git cherrypick” script by hand to scan the JIRA ticket for new SHAs and apply those to the branch in git. The release engineer would be the first to encounter any merge conflicts, even though they don’t have any context on the changes. The release engineer would manually trigger a new build, wait, and then babysit the deployment to staging. We knew that none of this was necessary. With better tooling, engineers could be responsible for performing their own cherrypicks and resolving any merge conflicts. Builds could be automatically triggered whenever new commits landed on the branch. A new build could be automatically deployed to staging and a Slack notification sent to inform the developer that their change is ready for acceptance testing. Our first major achievement was automation of the existing staging pipeline, which freed us from operational overhead enough to work on building the new architecture. As we gradually dug ourselves out of the hole of manual operational toil, we continued to invest in improving automation and reliability. While we already had some organizational experience with both Docker and ECS — our dev boxes were built on Docker Compose, and we had numerous microservices in production which we deployed on ECS. This alone wasn’t the driving reason for our migration. As described above, we specifically wanted to increase the speed at which we could build and release new code. On the build side, we realized that we could gain huge speed-ups through caching infrequently-performed but expensive operations. Perfect examples are installation of system packages — which rarely change — and Python requirements which change more frequently, but still not that often. While we could have built caching into the legacy Debian packaging system, we felt that it would be at least as much work as moving to Docker. Debian packaging tools are byzantine and nobody on our team had a good understanding of them. Furthermore, Docker has quickly emerged as a standard packaging and runtime format. If you package your application as a Docker image, you can build and run it in tons of different environments. Since we had adopted Docker already for our microservices and development environments, and therefore already had a fair amount of in-house familiarity with it, it was a natural fit. In re-architecting our package build process under Docker, we were obsessive in optimizing our use of the provided layer caching. By fanatically avoiding doing more work during builds than we needed to do, we were able to get the build time from ~25 minutes to ~2–3 minutes on a machine with a warm cache. Since our cache hit rate is very high (system packages and Python dependencies change very rarely), this easily provided a 10x improvement in average build time. It was fantastic to decrease our build times. However, we still had an unacceptably slow deployment speed. This was mainly because we would bring up a whole new set of machines on each deploy. This is inherently time-consuming and error-prone. What if we instead maintained an elastic pool of servers which were always ready? That’s essentially what Amazon ECS offers. You run a cluster of Container Instances (which can be scaled easily), and then ECS takes care of deploying and scheduling your application containers on that cluster. Since we had migrated our monolithic Django app to Docker, it was fairly straight-forward to see how this would work by migrating our staging system to ECS. We quickly saw huge drops in deployment times — to about and average of 2–3 minutes on staging. Production takes a little longer because there are so many more container instances, but is on the order of 7–8 minutes. This could be sped up further by having additional ECS host capacity. It’s one thing to migrate your staging environment to a new architecture, but it’s another to migrate the full production workloads and release process. Not only did we need to come up with a plan to perform the migration with zero downtime to users, we also needed to have some way to do this gradually and roll back in case we found issues. Like many real-world monoliths, the Nextdoor Django app in fact runs as seven different “service flavors”. These are instances of the exact same code, running in a slightly different mode and serving different sets of traffic. For example, there is an “api” service flavor for mobile clients and a “taskworker” service flavor for asynchronous job execution. Each service flavor runs under a dedicated Amazon ELB. These service flavors provided a natural grouping upon which to perform the migration. We could migrate production one service flavor at a time, using the DNS on the ELB to roll back to the EC2-based system if necessary. We started with one of the low traffic service flavors, and worked our way gradually to migrating the highest traffic ones as we gained confidence. The entire migration took about 3 weeks, mainly due to two tricky issues which took us quite a lot of time to debug: processes dying mysteriously and occasionally hung asynchronous taskworkers. There isn’t a direct way to translate from a system where the minimum unit of scaling is an entire EC2 instance to a system where the unit of scaling is a single container. We had previously thought only in terms of scaling coarsely — by booting or terminating EC2 instances. We hadn’t thought of more fine-grained scaling — i.e. by X units of CPU and Y memory units. We needed to think deeply about our application’s resource requirements and tune the number of worker processes given the amount of memory we were allocating. We couldn’t simply say “let’s just boot another X machines and let the application have as much memory as it wants”. ECS will require you to set clear limits, and if your containers exceed those limits they will be killed unceremoniously. In an environment like Python where the interpreter seemingly never returns memory once it requests it from the OS , your processes can seem to grow in memory forever. This manifested as us initially setting memory limits too low and suffering OOM kills of important processes as a result. It took a week of careful watching and tuning to get things to a point we were happy with. We found ECS memory usage graphs were problematic as — given metrics are collected only one per minute — they could easily miss spikes. One of our largest service flavors is our asynchronous job processing system we name “taskworker” . We run a big pool of taskworker processes which pulls tasks from Amazon SQS queues. We migrated a portion of this service to Docker/ECS and started to notice some very strange behaviors. In certain cases, the Docker/ECS taskworkers were getting stuck and hanging forever. However the legacy Debian/EC2 taskworkers didn’t have this problem. Doing some deep system call-level debugging, we managed to track down the source of the hang. Only under Docker/ECS, our taskworkers would read from a PostgreSQL connection without a timeout — and for some reason the result would never arrive nor would any error occur on the socket — thus hanging the process. We came up with various crazy theories about why this was happening such as Docker networking issues, kernel bugs, and more. We also started to work on handling stuck taskworkers by doing operations in a child process, and handling a timeout in the parent process which would notice the child was stuck and restart it. However the Infrastructure team found the underlying issue independently — in porting the Debian package build to Docker we had inadvertently upgraded the PostgreSQL C library “libpq” from 9.1 to 9.6. This later version of libpq seemed to have some subtle backwards-compatibility issues communicating with our 9.4-version database servers. It would issue a read on a socket without a timeout and no error would ever occur — hanging the process forever. When we pinned the version of libpq back to 9.1 — where it had been in our Debian build — this problem disappeared. It took our team around 5 months to re-architect the build and deployment system to run under Docker/ECS and completely overhaul all our automation. This investment has paid off enormously in terms of the following benefits: Build & deployment down from about an hour to 5–6 minutes (ECS capacity willing). Fully automated staging environment. Developers push commits onto a release branch and they are live on staging for verification within 5 minutes — no manual operation required. Vastly simplified build and release process for production. Operator simply types “!release” in a Slack channel and a Slack button is displayed to confirm deployment of release. Click the button and the production deployment process is begun. Massively improved release reliability. ECS and Docker have helped us simplify our systems so that they have many fewer moving parts and points of failure. Not having to worry about bringing up large numbers of new EC2 instances on every release drastically reduces our exposure to random failures. However, it’s not all rainbows and unicorns. We have run into numerous issues with both Docker and Amazon ECS, which we plan to go into in future blog posts. Key learnings: Docker layer caching works, but it is coarse-grained. Furthermore, due to some changes, the cache cannot be populated by a Docker pull . It must be built locally. Expect to invest quite a lot of time on optimizing if you have a complex build and you want to make it very fast. Amazon ECS works reasonably well, although it has rough edges. In particular, you cannot differentiate between a failing health check and startup grace period. If your application takes a while to become healthy on startup, you must set a long health check duration. You cannot say “wait for 10 minutes for my application to pass health check on startup, then ratchet down the threshold”. To migrate to Docker, be prepared to think deeply about your resource utilization in terms of CPU and memory required per container. What’s your deployment process like? We’d love to hear about your experiences making builds and deployments fast! Nextdoor is the neighborhood hub for trusted connections… 175 2 Thanks to Mikhail Simin , Matt Terry , Vikas Kawadia , and Andrew Brown . Docker AWS Cloud Computing Continuous Delivery 175 claps 175 2 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-10"},
{"website": "NextDoor", "title": "summary of 22 product engineering patterns that college didnt teach me", "author": ["Mikhail Simin"], "link": "https://engblog.nextdoor.com/summary-of-22-product-engineering-patterns-that-college-didnt-teach-me-77790331d8fa", "abstract": "Culture Open Source Technology I’ve been very fortunate to work on many different teams at Nextdoor and learn from many great engineers. While it’s impossible to write down everything I’ve learned, I’ve written down some of the important patterns that any engineer should follow. Note that some of these items tug each other in opposite directions. Your experience as an engineer will help you decide when to leverage which one. I’ve grouped these learnings into sections regarding Architecting, Developing, and Debugging software; but other than that — these are in no particular order. I’ve also written an article on management values that you may find useful. It’s always important to understand how long your work is going to live. Frequently you will be in situations where you need a solution right now , and this project isn’t going away anytime soon. Chances are some quick-n-dirty solution will pop in your head very quickly, and the right-but-time-consuming solution has to be tabled. Take a little extra time to come up with several short-term solutions. For your final decision make sure that your approach lays the path toward the wanted future. (aka “Sane by Default”, or “Doing the wrong things should be hard”) Your work will always serve as an example for others on how to do things. If you’re adding new functionality make sure that any assumptions it makes are sane. If you want this new feature to be leveraged by other engineers — make it really easy to use. The easiest thing to do is to not do anything. Automate what you can so that anyone leveraging your technology gets the benefits for free. Consider a situation where you are refactoring code, or introducing a new pattern for analyzing data in a spreadsheet. It may be tempting to get your work done quickly and be done with it. Take the extra time to generalize your work and make it the new standard for all your peers. Whether it’s moving all database calls into a DAL, or disseminating your spreadsheet with formulas to all employees — do it right, make it easy, and getting everyone to use it will be simple. Monitoring and alerting is the bridge between product and engineering. Where unit tests confirm that your code works — monitoring confirms that your product works. The historical data of monitoring can be invaluable when making changes, so adding it early on is imperative. Hopefully adding monitoring is as easy as 1 line of code (see “Doing the right thing should be easy”) and everyone will start monitoring vital parts of your product. For self-hosting consider Graphite or Munin. Some monitoring companies have a decent free tier: geckoboard, librato, and New Relic. Nextdoor uses Datadog for monitoring, alerting and performance analytics. Edit : I’ve written about the challenges here in a separate blog . The hindsight on “the stack is too big” is very clear, and scary. Have the foresight by always questioning if the new technology is worth the complexity. It’s not just about avoiding the “shiny-new-toy” syndrome. Avoid increasing the learning curve for new engineers. Avoid distributing sources of truth. Avoid duplicating business logic. It’s also possible that the new tech stack you’re considering is only useful for 1 of its features. Let’s say ElasticSearch is better at some things than Postgres. But if you’d be leveraging only 5% of ElasticSearch’s features and Postgres is already up and running, then weigh that against having to learn and maintain a new system, and remember the engineering and financial costs also. Contrary to the point above if you find yourself commonly thinking “I can trick it into doing what I want if I…” then perhaps you should look into other tools that are better suited. Hacking is fun but eventually it becomes unmaintainable, and errors become un-debuggable. Find the best tool for the job and use it. Experience will give you a good gut feeling when you’ve outgrown your current stack, and the overhead of adding new tools is worth the time savings you’d get otherwise. Engineers love to fantasize, and get off on tangents. Many conversations can be a network of, “And another way we can do this is…” When your initial insight tells you that there are a million ways to approach a problem — it’s important to come to a conclusion in a reasonable amount of time. Do not spiral down into a demagogue oblivion. Set aside a finite amount of hours to think over this issue or discuss it with colleagues. This will help you be lean about approaches and not discuss really silly ones. It’s important to realize that continuing to come up with solutions for the same problem will have diminishing returns over time. Go with your gut feeling and say, “we should be able to answer this in 30 minutes” before your next meeting. As your work grows you’ll inevitability have to split it into parts. These parts should have a clear separation of responsibilities. If the backend is already doing the heavy lifting—try to limit the work on frontend. Small, straightforward services/components don’t break — so move responsibility up the chain. Clear separation of work also helps with debugging. You’ll save a lot of time if you never have to ask, “where is the logic for handling/generating this data?” This guideline might be my most favorite, and ought to be thought about in both directions: those in power to fix an issue should feel the pain of the issue, and those who feel the pain ought to be able to fix it. Here “pain” means any issue, burden or annoyance within the work. To create a culture of ownership and sustained improvement it is critical that any mistakes or failures are first seen by those who can fix the root cause. This applies whether talking about being paged at night, comfort of the chairs, or even on a more technical level — a service losing access to a shared resource. If you’re in a situation where you feel the burden of a failure in another department do everything you can to either become empowered to address the failures, or redirect the burden to it’s origin. When multiple parties can fix the issue then you are at a luxury of asking who should fix it. Try to find a balance between causality assignment, and disseminating the knowledge. Example 1: You may be in a situation where the cause of an issue comes from a different department — one that is unable to fix the issue. Take, for example, deadlines set by management which cause workers to cut corners too much. This can be rephrased as “the management does not have sufficient visibility into downsides of cutting corners.” Empower yourself with data, and dashboards showing that going faster in the short term is actually slower in the long run. Example 2: As an opposite example, a graphics designer may feel the pain (negative feedback from customers/management) of an incorrectly implemented solution. One approach is to have this designer daisy-chain the feedback to engineers. This works, but this loop is slow. Shorten the loop by having the designer and engineer work close together. Possibly, even make it requirement that an engineer’s work is reviewed by the designer before it is shipped. Example 3: Lastly there are situations where the pain is simply not necessary. In the situation of a “shared resource” between parties A and B it can be crucial that a culprit A does not affect the innocent bystander B. If someone is late for a train and squeezes in by jamming the doors it can slow down or even break the train causing everyone, not just the culprit, to be late. Observe situations where innocent bystanders pay the price, and see how the cause and effect can be limited to just the culprit. This is sometimes referred to as controlling the blast radius . When developing some augmentation, something temporary, or something that is possibly throw-away code — it pays to architect your solution such that undoing it all is very easy. Deeply embedded solutions will likely remain in the code or in your system for too long because people won’t be interested in rolling up their sleeves and figuring out how to untangle the mess. Clean engineering is more durable, and debugging it is a pleasure! Not everything requires writing code. Also, sometimes people problems can be solved through code. Learn to question various solutions and always think if it’s possible to “engineer out of a problem.” On the flip side if your engineering work is very large — see if it’s possible to have a human solution. For example: employees are supposed to fill out TPS reports but keep forgetting put a cover sheet. One solution is to send a memo to everyone in the company and remind them to do it. But you can engineer yourself out of this problem by making TPS reports done via Google forms and mark the field as required. Think about it — with this change on one will ever again make this mistake. Another example: some data in your database is corrupt for one reason or another. You have 400 records out of hundreds of thousands that are wrong. You can write a script that fixes it, but it’ll take a couple days to really make sure the script works and doesn’t make it worse. Alternatively… just handle these all manually. All in all it will be faster. Nextdoor engineers send a “changelog” email whenever manual changes to production are done. This is a cultural agreement that helps communication across the board. Some changelog emails are automated, but many still have to be done manually. Ideally you can follow the Test Driven Development paradigm. But even if you aren’t — it’s important to think about your tests while you’re writing the code. This thought alone will help you write cleaner code with necessary abstractions around the messy parts. Tested code should be your pride, and a bragging right. Taking the time to write tests makes the development faster in the long run. Zero people can write code that never breaks. So make sure that your code is easy to debug when it breaks. Someone down the road who doesn’t have the necessary context or institutional knowledge will be debugging your code. Write it such that it is self explanatory. Your code already explains what it does, so limit your comments to explain the why and not the what . If you’re relying on log statements then make sure they make sense out of context also. One pattern I follow is to explain how to debug directly in the log message that handles a failure. Multiple failures caught. Search logs for XYZ to find details. This sort of log message will help anyone, even engineer that starts next week, figure out what to do. This is also useful directly in the development section. In Nextdoor infrastructure if a unit test tries to reach out to a third-party service you get an error such as: You are trying to access service XYZ but it’s not mocked. Use subsystem_mock(XYZ) or subsystem_use_real(XYZ)! This error message tells you exactly how to resolve itself! Great! Understand when some extra work can become a slippery slope, and cut that work into a separate step. You can apply this even to important features and have a quick-follow up to your main project instead of allowing for a scope creep. Only experience will really tell you what is a right amount of work to break off into a quick-follow. Make sure your team mates also agree! Often when preparing for a significant overhaul of some functionality you may end up changing the logic significantly while keeping the end result nearly the same. Use the “no worse than before” approach to understand your impact, and make sure that all changes are improvements. This is also a good argument to prevent scope-creep when a teammate asks for additional features. When you are changing a pattern, or introducing a new concept/class — aim for replacing all instances of the old pattern. This can be a daunting amount of work, but it’s the right thing to do. Getting stuck in half-transition, or allowing your code to have multiple ways of doing the same thing can be detrimental to future development. Being able to ask “What just changed?” is crucial. With proper architecture you can look at the relevant dashboards, note events (like production releases), and note manual changes (email changelogs are a blessing). Software is not a baby. When it screams it should also give you details about what exactly is wrong. Of course sometimes it’s impossible, which makes it even more important to be as detailed as possible when an alert is fired off. Create a tradition of going back to your alerts and improving them. Whenever your alert goes off ask yourself, “what is my first step in debugging this? Can the alert provide me this information?” Canary alerts (via tools like Pingdom ) have their utility. They are high-level alerts that something is wrong. Go figure out what… In order for an alert to be actionable (and to minimize debugging time) it’s good to have fine grain alerts in addition to catch-alls. Using Datadog you can create a generic latency monitor, but leveraging their tagging system your monitor can tell you which host is latent, or even which specific view. The challenge with fine grain alerts is to make sure they are not noisy! Start with “quiet” alerts (just email or Slack) and keep tuning them before making them into real pager-worthy ones. Lastly, when you create your monitors and alarms — take an extra 10 minutes to write down first steps you recommend in debugging when this alarm goes off. Collect these “runbooks” in an easy to find location, and add a link to the specific runbook directly in your alarm. If debugging is time sensitive — start gathering more minds. Escalate the page if needed, or get a group of people around the table. Cost of the site being down is always worth bringing in extra help. If nothing else — the rubber-ducky effect is real! If an alert is not actionable — change it. Changing it can mean adjusting the value, or disabling the alert altogether. The danger of an alert being useless noise can often outweigh the danger of this one alert not going off. With sufficient alerting — if something really bad happens then multiple alerts are fired. This gives you the freedom to be strict about cleaning up annoying and useless alerts. Too many fake alerts can drown out an important alert. Have fine-tuned alerts, and keep them lean. There are countless wisdoms that life outside of academia has to offer. And unsurprisingly these wisdoms are applicable mostly just outside of academia. Some are specific to tech, some are for all types of engineering, others — applicable to all fields. Got another good rule of thumb? Got a situation you’d like to share? Start a discussion below! I’ve been extremely fortunate to work with the brightest minds in the industry. I love learning and sharing my knowledge with others; if you like pushing yourself to be the best you can be — we’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 394 2 Thanks to Isabella Levin and Vikas Kawadia . Engineering Graduation Product Development Life Hacking Tech 394 claps 394 2 Written by Reverse Engineer Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Reverse Engineer Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-11-26"},
{"website": "NextDoor", "title": "migrating to swift 3", "author": ["Daisuke Fujiwara"], "link": "https://engblog.nextdoor.com/migrating-to-swift-3-7add0ce0655", "abstract": "Culture Open Source Technology The iOS team at Nextdoor recently migrated our iOS codebase to Swift 3. In this blog post, we wanted to share the approach we took and learnings from this process. Since our iOS team was small (before 6, now 9 members) and our codebase was relatively young, we decided to adopt Swift at its early age when version 1.2 came out. The team had collectively learned the new language during our iOS co-op , and once we saw that the language was evolving quickly and the toolchain was getting better, we felt comfortable investing full effort into writing production code in Swift. We set the standard that all new files will be written in Swift. Fast-forward a year and a half: today, we have 550 files in Swift, and 20% of our codebase, consisting of production and test code, which made this migration a non-trivial task. When upgrading to Xcode 8, we had to decide whether to go Swift 2.3 or Swift 3. Due to our company priorities and resource constraints at the time, we decided to first migrate to Swift 2.3 to get to Xcode 8 and iOS 10 faster. Decoupling Xcode and Swift 3 upgrades was definitely the right choice for us. We were able to troubleshoot iOS 10 specific issues earlier, minimize the impact on other on going projects, and make the workload manageable. Once Apple officially announced that Xcode 8.2 would be the last version that supports Swift 2.3, it was time to tackle Swift 3 migration. After investigating the technical risks of upgrading to Swift 3, we had to make a few decisions: Because we couldn’t merge the ongoing migration back to our master branch, we had to decide on the workflow around the migration branch. To facilitate code reviews by other developers, we set the rule to break up the migration work into smaller logical commits. This also aimed to avoid any duplicative migration effort since developers were constantly reviewing the ongoing work. We have dependencies on third party libraries and internal frameworks that are in Swift. We laid out the order of migration, so that everyone was on the same page and could help out in the appropriate areas of the codebase. Also, once the framework was declared complete, any developers who modified files in the framework were responsible for porting their new work to the migration branch. Note that our codebase is a mixture of Swift and Objective-C. Some of the challenges we faced were due to that fact. We use Cocoapods as our dependency manager, and because it’s shared among different branches of your repository, it made sense to have a logic in our Podfile to manage different versions of Swift. With the changes above, all you had to do to set yourself up for Swift 3 was: $ SWIFT_VERSION='3.0' pod install Swift 3 introduced new bridged structs for corresponding Foundation classes. For example, NSDate in Objective-C now corresponds to Date in Swift; similarly NSLocale to Locale. We had our own extension on Objective-C classes that we used in our Swift code. Instead of doing casting everywhere, we thought it was best to write Objective-C extension in Swift as follows: Using the Xcode migration tool not only took a long time on a bigger project but also made changes that were not necessary. For example, a lot of access control declarations were converted to open from public even though it wasn’t our intention. We decided to make decisions on a case by case basis to keep our integrity of our code rather than completely relying on the migration tool. With Swift 3, Objective-C code that is used in Swift now returns optional types instead of force unwrapped optional; ? instead of !. This caused a lot of compilation errors. Instead of taking the shortcut of littering our codebase with !s or if let , we decided to diligently annotate nullability in our Objective-C code to better reflect our intention. You can find an excellent reference about this topic here . Also, Xcode static analyzer does some work to detect incorrect annotation, which is very helpful. Because of interoperation with Objective-C, many of our dictionary declaration used to be [NSObject: AnyObject] . With the change outlined in the Swift blog , the dictionary declaration became [AnyHashable: Any] . Instead of taking the compiler’s suggestions of casting them back to [NSObject: AnyObject] we decided to annotate declaration appropriately. Objective-C code complained that method name didn’t match up with what was defined in the Swift code, especially if the protocol was written in Swift. We had to annotate swift methods with @objc(<method signature>) to match up the names in Objective-C. The following snippet from Apple’s documentation explains what the annotation does. In some cases, you need finer grained control over how your Swift API is exposed to Objective-C. You can use the @objc( name ) attribute to change the name of a class, property, method, enumeration type, or enumeration case declaration in your interface as it’s exposed to Objective-C code. Interestingly, closures no longer accept parameter labels. Even though the compiler suggests an appropriate fix, we thought it was worth noting because it was confusing to us at first. More information can be found in this swift evolution proposal . There were many other different types of syntactical errors that the compiler did a great job of suggesting fixes. Not all compilation errors are reported at once. As we fixed errors, we noticed that Xcode reported new errors, even in the same file that you were working in. This made tracking progress in a large Xcode project difficult. This migration process was truly a team effort. After a couple of developers initially got the code base to build successfully, the entire team helped us to get our CI back to green and eliminate all lint errors and warnings. Our +3,400 unit test suite helped us give confidence in our migration, but the team tested the app, reported issues, and fixed bug in their specific domain areas. Ultimately, this migration took around 2 weeks, but it’s really good to be able to declare that we are fully on Swift 3. This has set up a good starting point for the team in 2017, where we get to focus on pushing our platform and service forward. Nextdoor is the neighborhood hub for trusted connections… 15 Thanks to Abhijeet Kumar , Alex Kring , and Vikas Kawadia . Swift iOS Migration 15 claps 15 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-12"},
{"website": "NextDoor", "title": "how to trick airflow to reduce wasteful processing", "author": ["Siva Pandeti"], "link": "https://engblog.nextdoor.com/how-to-trick-airflow-to-reduce-wasteful-processing-b1744f4aaece", "abstract": "Culture Open Source Technology At Nextdoor, the data team uses Airflow to orchestrate data transfer between various data sources and data lake, our central data store. In this post, I will talk about one particular challenge we faced when we started migrating to Airflow and how we solved it. In particular, I will describe a typical ETL (extract-transform-load) scenario, how Airflow’s implicit architecture causes wasteful processing if we don’t take special care, and discuss how we were able to manage Airflow to do its job without any unnecessary processing. Batch jobs are very common in data pipelines. They run at a regular cadence, e.g. hourly, daily, etc. One such common job in most ETL pipelines is to extract from a source database, transform the data and load into a target data warehouse. These extracts can be either incremental or full table refreshes. For example, at Nextdoor, member posts, thanks, clicks are very large datasets. It can be very expensive to reprocess such big dataset over and over. They are good candidates for incremental processes. On the other hand, lookup data such as city agency information are smaller and are good candidates for full table refreshes. In Airflow, we stitch together many processing tasks with dependencies into a graph called DAG (Directed Acyclical Graph), which is a container of jobs. These DAGs typically have a start date and a frequency. Sometimes the start date set in the DAG code may be many days before the DAG is deployed to production. In such cases, Airflow will try to “backfill” from start date to current by running for all the periods in between. This can be wasteful or even destructive in some cases, such as when you perform full table extract from a source database and load it into data lake. Airflow’s architecture is great for running incremental jobs. An important paradigm built into Airflow is “execution_date”. It forces one to design your DAGs to be idempotent, meaning you can re-run for any execution date without worrying about any detrimental impact. The DAGs run once per execution date. As an example, a monthly job will run once for January, February, etc. During execution of January run of a sales table extract, job can pull the sales on January in source database and apply it to target database. The diagram below shows a simple extract, transform, load pipeline. Sometimes back processing is needed because: you just deployed the pipeline and need to backfill the history, or there were errors in source data which are now fixed and you need to re-run the ETL, or ETL logic has changed and must reprocess the historical data. Airflow makes it very easy to do back-processing. You just need to pick the very first DAG execution and “clear” all the future runs with a couple of clicks in the UI. Airflow will methodically re-run executions for January, February, etc. It makes the ETL loads very idempotent. You can re-run as many times as you want easily as long as you design ETL carefully. For small tables such as lookup tables, it is easier to just to do full table refreshes than to do an incremental changes every month. It may be because you may not gain much performance improvement given the complexity of keeping tracking changes since the table is so small. Or perhaps, there may not be a good reference column such as sale date using which you can determine which record belongs to which execution date. Processing such full tables is not totally idempotent per se. Running such extracts for a table in parallel for multiple execution dates in not desirable because it is unnecessary to do full table extract and load in every run if there is no change in data. Another reason to not do this is because parallel loads will lead to process errors or undesirable duplicate data in target database. So how do we fit our Airflow DAGs into expected idempotent design and still not incur the cost of wasteful processing? Glad you asked! Here is the core idea: When there is a backlog of back processing, don’t run the same thing over and over if it adds no value; instead skip ahead to get caught up and then run just once. Sounds logical and straight forward, right? Airflow makes this possible with the use of ShortCircuitOperator (SCO). To implement this, add a SCO task to the DAG. Then set dependencies such that it is the ancestor task of all the other tasks in the DAG. Inside the SCO, check whether a) current execution is in the past or b) there is another execution of the DAG already running; if either of these conditions is true, then let SCO skip all the downstream tasks. This will achieve the following: Normal day to day runs will run just like any other DAG since SCO won’t skip anything. When back processing needs to be done or scheduler needs to catch up to current, all past executions will be skipped so as to not run the same task over and over. Below is a code snippet using SCO. Here is the full source code for this example on Github. At Nextdoor, we have many such non-idempotent jobs. We use this skipping pattern very widely in our DAGs in extracting full tables from production databases and many other DAGs for which historical processing is not needed. Below is a screenshots of Airflow UI for a toy example DAG that uses the same pattern that we use in our production jobs. In Diagram 2 above, “skip_check” task is a ShortCircuitOperator. When the DAG started running from January 1st onwards, it had a lot of days in past to process. However, since we don’t really need to do that full table refresh over an over, skip_check SCO task will skip all downstream tasks when it runs for past execution dates. When it catches up to current, it will run normally as you see in the Diagram 3 below. In summary, we witnessed how Airflow can do unnecessary processing if you do not design the DAG with care in certain ETL scenarios, and we solved the problem by using ShortCircuitOperator, which resulted in ETL processing resources allocated to more important tasks. This article wouldn’t have been possible had it not been for the efforts of the whole data engineering team at Nextdoor (Zack Shapiro, Jay Thomas, and myself) working together to build data pipelines using the technique and many more. We will write about a few other challenges we faced in migrating to Airflow and how we overcame those in the future articles. If you enjoyed this article and are looking forward to future posts, let us know by recommending this article! Nextdoor is the neighborhood hub for trusted connections… 166 3 Thanks to Mikhail Simin , Zack Shapiro , Vikas Kawadia , and Jay Thomas . Big Data Data Science Etl Airflow 166 claps 166 3 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-24"},
{"website": "NextDoor", "title": "introducing ndphraseparser a named token replacement library for ios", "author": ["Sean McQueen"], "link": "https://engblog.nextdoor.com/introducing-ndphraseparser-a-named-token-replacement-library-for-ios-aef342ca19e4", "abstract": "Culture Open Source Technology We recently open-sourced NDPhraseParser , an iOS library to do named token string replacement, like Python. For a recent project I worked on we wanted to be able to display strings from the server with missing tokens on our mobile platforms. At first this seemed like a job for the standard library. The team quickly realized however that to increase the velocity of our product experimentation, ideally the strings we sent from the server would have tokens that were: swappable removable human readable These properties would give us the maximum amount of server-side control and give us the most flexibility while writing copy. For the feature we wanted to support a number of services that users could provide to each other (examples: dog walking, babysitting and yard work). In a given view on a client, we wanted to have the flexibility to display strings with different tokens depending on service type. For example, some service types include copy that reads like this: For other service types, we wanted to be able to display strings that had different orderings or had different inputs, so that their meaning could be changed or adjusted more precisely. For example: Python allows us to do named token string interpolation super easily (we ❤ Python at Nextdoor): We also found a very nice Java library built by Square that allowed us to do the same thing with the same tokens in our Android app: For our iOS app, we couldn’t find a good solution in the open source world. So we wrote our own and open-sourced it . NDPhraseParser is a port of Square’s Phrase library to Objective-C that allows us to do named token replacement on iOS, like Python! Swift example: Objective-C example: This is a super simple library that solves a small problem. But we find it useful and it solved a key problem we had while building a new feature at Nextdoor. We hope you find it useful too. Pull requests welcome! https://github.com/Nextdoor/NDPhraseParser Love it? Join us. Nextdoor is the neighborhood hub for trusted connections… 9 Thanks to Wesley Moy , Vikas Kawadia , Mikhail Simin , Daisuke Fujiwara , and Wenbin Fang . Programming API iOS Open Source 9 claps 9 Written by sean@mcqueen.net Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by sean@mcqueen.net Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "meet the interns 2016 edition", "author": ["Prakash Janakiraman"], "link": "https://engblog.nextdoor.com/meet-the-interns-2016-edition-53a996d2c696", "abstract": "Culture Open Source Technology As we wind down another summer here at Nextdoor, it’s a great time to celebrate the contributions of this year’s exceptional batch of college interns. Since the founding of the company nearly 6 years ago, we’ve had the pleasure of working with some of the country’s most talented students each year who share our passion for building stronger and safer communities across the globe. With the help of our investors at Greylock Partners & Kleiner Perkins and a great team of university recruiters, we had the opportunity to connect with students from schools across the country, including Carnegie Mellon, UC Berkeley, MIT, Chicago, RISD, Rice, Smith College, UC San Diego, Washington, and many more. Eventually, we narrowed the field to a final group of 14 interns who joined us at Nextdoor HQ in San Francisco for the summer. Working with their assigned mentors, our interns played meaningful roles on our monetization tests, international site launches, growth and engagement experiments, infrastructure projects, community support, and core developer productivity initiatives. Whether in neighborhood operations, design, engineering, or business development, interns could be found working on teams across the entire company. As is our annual tradition, we’re thrilled to introduce you to the students who helped make the summer of 2016 a huge success! Along with their stories you’ll find a description of their “Special Talent” — a skill of any kind, an icebreaker presented during their first day introduction. Applications for internships in 2017 are waiting for you . Studying : EECS & History @ UC Berkeley Special Talent : Crow pose! Mentor: Wenbin Fang Project I was super lucky to work on the monetization/businesses team as we rolled out a pilot of our product. Instead of doing an independent project, I asked to be thrown into the fire and do anything to help move the new product forward before and after the pilot launch. This really helped me in becoming a full-stack engineer; I got to work on front-end (HTML, CSS, Javascript), backend (Django), and databases (Postgres) and built a set of internal tools from the ground up to monitor the businesses’ platform. I also helped modernize the iOS app by adding a few 3D Touch features to the iOS app. What I Learned Before Nextdoor, I primarily worked on iOS, with bits of backend experience where scale hardly mattered. At Nextdoor, I learned what it means to build and launch a feature that must deal with the challenges of scale. For example, I got to schedule emails on a system that sends out millions of emails per day. In addition, I got to be hands-on with some technologies that previously only existed in my mind as buzzwords (Elasticsearch, Docker, and probably a few more that now feel much less hand-wavy.) Favorite Memory Early in my internship, the team I worked on went on a day trip to Angel Island — it was a great way to get to know people outside the work environment (rather than as just little Slack avatars)! Studying : EECS @ UC Berkeley Special Talent : Perfect pitch Mentor: Abhijeet Kumar Project I worked in the recommendations team which is a smaller part of the monetization/businesses team. Instead of having one summer long intern project, I opted to work on a wide variety of different small projects as an actual full-time engineer might do. I worked on things like prefetching webviews, smoothing out the rollout of sponsored posts, a new recommendations flow, as well as tagging businesses on our new redesign for the iOS app. My mentor gave me great flexibility on projects, and I was also able to get experience working with the Django backend in an effort to become more comfortable as a full-stack engineer. What I Learned On a technical level, I gained experience with Swift and the Django framework, neither of which I had ever used before. I was also exposed to many new design patterns and coding practices throughout the entire internship through code reviews and pair programming. On a more abstract level, I learned the value of thorough planning before coding. The amount of time wasted by charging into a problem headfirst is not trivial, and by thinking through designs carefully with the team, we were able to come up with slick solutions that fit seamlessly into our codebase. Favorite Memory Intern Olympics — we had a bunch of challenges in a open-ended scavenger hunt across San Francisco. It was a nice breath of fresh air to be running around the city on a weekday and the tasks were a lot of fun. Studying : Computer Science @ Carnegie Mellon University Special Talent : Baking brownies Mentor: Ananth Chandrasekharan Project I worked with the Geospatial and International team this summer on several different projects. The first project I took on was generating neighborhood boundaries using demographic data. I built an unsupervised learning model and evaluated its performance with San Francisco data. Inspired by the first project, I went on detecting holes, or clusters of plots that are too small to form a neighborhood on its own. I also implemented the address autocomplete feature in the sign-up flow for the iOS app. What I Learned From working on different projects, I got better at diving into an unfamiliar code base and picking up a new technology. In particular, I was new to iOS development and had great fun learning Swift. Beyond learning new language and environment, I learned good engineering practice and coding style from the feedback I received. Besides engineering, I also improved my communication skills from collaborating with people across teams. Favorite Memory Bubble-tea breaks and getting to know people outside of work. Studying : Electrical Engineering @ Rutgers University Special Talent : Whistling Mentor: Andrew Brown Project I helped create nd-toolbelt, a command line interface program for centralizing internal tools for discoverability and ease of use. The toolbelt is easily extendable, self-updating, and provides help for all commands within its namespace. I also helped create a webapp for creating changelog emails that posts changelogs with #-mentions and @-mentions to slack. What I Learned The most valuable thing I’ve learned over the course of this summer is the fundamentals of Test Driven Development. I admit that while I was a bit skeptical at first, I quickly came to realize the strengths and merits of writings tests first and how much time is saved by iteratively approaching a solution this way. I feel this will be an invaluable skill for years to come. Favorite Memory On a Wednesday evening, my coworkers Hussam, Bobak, Daniel and I played a board game called Star Wars Rebellion until 11pm. Though the game ended in a draw, it was a blast! I really enjoyed the experience, and I greatly appreciated the fact that the office was a place that everyone felt comfortable taking it easy playing a board game after a long day’s work with Star Wars music playing the entire time. Oust the rebel scum! Studying : Computer Science @ Rice University Special Talent : Write multiple Chinese calligraphies (Regular script — Kai Shu, Semi-cursive script — Xing Shu) Mentor: Rahul Sureka Project I worked on our infrastructure team, building a sustainable and scalable search architecture for indexing userprofile into Elasticsearch. I also worked on making the setup easier for developing and testing locally. On a dev box, a single command would start the entire search pipeline which includes an additional ES cluster, different flavors of Indexer micro-services consuming from local pubsub. What I Learned It’s the most fruitful internship I have ever had! From publishing messages in Django, to the pub-sub design pattern, to consuming from Kinesis and Kafka stream, to designing the pub-sub schema and elasticsearch schema in json schema, and to AWS EC2, Docker and Elasticsearch, I learned tons! I got a broad and deep idea about what infrastructure is, and learned about the code flow from our Django side to the micro services part, and from implementation to deployment. Favorite Memory Definitely the Intern Olympics — it was epic! It was an outdoor game, similar to scavenger hunt, with an Olympic theme. We were randomly assigned to four teams, and my team picked Saint Kitts and Nevis as our country. We had a surprisingly great start — we did a synchronized swimming routine like we had practiced the previous night. It was a lot of fun to run all over the city and do a bunch of crazy challenges. We won the 2nd place and got a silver medal! I’m grateful for these memories. Also, seeing the member stories in the weekly all-hands makes me feel so positive about life and feel grateful about how Nextdoor can help bring people together! Studying : Industrial Design @ Rhode Island School of Design 2016 Special Talent : Building/making physical things (furniture/bags/etc) Mentor: Emily Schwartzman Project I worked on the Monetization/Businesses team. During the summer, I designed the mobile web experience for the pilot in San Francisco, communications for the launch, and a couple pages in the business product. I also interviewed local businesses to help inform the parts of the product that needed to be improved post-MVP. Aside from working on the business team, I helped the design team on some design framework explorations as well as the community guidelines pages. What I Learned Being from an industrial design background, I learned to transition my design skills from physical to digital in a high-paced, working environment. I picked up and improved a lot of my skills while working on the businesses team by seeing all of the intricacies in creating a new product for such a wide variety of users. Favorite Memory Either the lunch with the leaders talk sessions, or the Intern Olympics — more specifically, synchronized swimming with Cassie and Max! Studying : Computer Science @ MIT Special Talent : Catching 17 coins off of elbow Mentor : Mike Grafton Project This summer I worked with the Recommendations Team to connect users to local business pages. To do this, we wanted to extract the most semantically interesting business recommendation on the business pages to show to a user. I approached this problem by analyzing user’s comments for structure and word frequencies. The next subproblem was to actually retrieve these comments for users to see slowing down their experience. Our privacy model only allows users to see content within a given zone that is unique to their neighborhood making the problem more complex. We combatted this by storing the optimal comments in Redis and retrieving them in constant time upon rendering a user’s page. Finally, before releasing this feature, we needed to know that it actually increase the business page views. Using A/B testing techniques, we partitioned users into buckets that would decide if the comment would be rendered on their page or not. Using this data we were able to get a more in depth look into our user’s click through habits. What I Learned I learned that social media is a science. Everything needs to, and should be, tested thoroughly, whether that be unit tests, integrations tests, or A/B tests. Testing is for our own good. I really enjoyed the scientific approach to the problems I was attempting to solve. This wasn’t a matter of me just implementing new features on our system, but really learning at every level. From very concrete knowledge like some of our main APIs to user behavior to my own approach to problem solving, growth was definitely made and valued. At the conclusion of our testing, my feature didn’t perform as we would’ve liked, but these skills quickly translated to later projects I worked on. This is why we test. Favorite Memory Nextdoor provided an environment where, if I had an idea, I was given the time and resources to pursue it. After gaining some experience and expertise with a particular subsection of the codebase, I was able to look at the company’s needs and make an evaluation on where I could be beneficial. Usually these ideas are backlogged and stored until a company hackathon, but Nextdoor, more specifically Mike, gave me guidance and resources to solve these problems. I really appreciate this. Studying : Computer Science @ UC San Diego Special Talent : K-Pop dancing Mentor : Mitali Gala Project This summer, I worked with the core mobile team on various projects on the Nextdoor Android app. I’ve mostly been working on user-facing features such as editing replies and internationalization, as well as deeper challenges that improved our app’s internal architecture. As Android Nougat rolled out this summer, I helped prepare our app to be both forward and backward compatible with this latest version. I was also fortunate to join in the middle of a significant redesign of the in-app navigation, and had the chance to work closely with both project managers and designers in implementing these new changes. What I Learned Besides learning lots of invaluable technical skills, patterns, and technologies, I had a glimpse into how the product team operates at a (relatively) small company. For example, when building a new feature for Android, I often needed to immerse myself in the web’s server side in Django to make related changes, as well as check-in with the design team to ensure what I’m building is up to spec. This allowed me to gain a better understanding of what’s happening in all areas of the company — and what it’s like to be part of a fast-moving startup. Along the same line, I discovered that considering tradeoffs is such a big part of software engineering — and learned to make decisions after weighing all options from all stakeholders. Favorite Memory Going to karaoke with my team, and hearing everyone’s beautiful voices. There’s actually a surprising amount of singing talent at Nextdoor! Studying : Computer Science @ University of Washington Special Talent : Magic Mentor : Vikas Kawadia Project Towards the beginning of the summer, I focused on changes to the newsfeed and Nearby Neighborhoods feature to help drive user engagement. Later in the summer, I went through multiple iterations of an ML model to perform text classification on user posts. Lastly, I deployed a test to evaluate the performance of the final model I trained. What I Learned My learnings at Nextdoor came in multiple flavors. From a technical standpoint, I learned the nitty gritty of training and launching a text classification model outside of the classroom. From a product perspective, I gained an appreciation for thinking deeply — not just about how to make changes to the product, but what changes should be made in the first place, and using data to inform those decisions. Lastly, from a business point of view, I attended weekly all-hands meetings and had the opportunity to hear from executives about what it takes to build a company and deliver a product that makes a real difference. Favorite Memory Most Thursdays, Vikas and I (and frequently others) went to a pizza truck in Mint Plaza. The truck used a wood fire oven, and the pizza was excellent. There was also frequently live music playing in the plaza. The great company, fantastic food, and rare San Francisco sun made Thursday lunch something to look forward to. Studying : MBA @ University of Chicago Booth School of Business Special Talent : Reciting poetry in Latin Mentor : Emily King Project As the Business Development intern, I was able to try my hand at a few different projects. Throughout the summer I’ve focused on newspages, specifically contacting and coordinating with news outlets, to encourage them to post local news stories to dedicated pages on Nextdoor. Through the course of that project, I also learned a lot about product development. Apart from that, at different points throughout the summer, I’ve performed analysis for the monetization team and assisted with ad operations. I even wrote some ad copy! What I Learned I thought this might be the case, but now it’s confirmed via first-hand experience: I am passionate about consumer-facing products, and it’s exhilarating to work on a small team with super-smart, fun people. The problems that the monetization team are thinking about are challenging and will have a big impact on Nextdoor. It’s been a really exciting place to work, and the people are still low-key and friendly! Ideal work environment. Oh, and some SQL… Favorite Memory Several good ones: the Sacramento Bee integrated our “Share on Nextdoor” button to their site, and my team won the Intern Olympics! Thanks again to all of our interns for another great summer! If you’re interested in helping to build stronger, safer communities across the globe, we’re hiring ! nextdoor.com Nextdoor is the neighborhood hub for trusted connections… 19 1 Startup Internships Nextdoor San Francisco Culture 19 claps 19 1 Written by Co-Founder of @Nextdoor. Former @GoogleMaps engineering manager. @Warriors season ticket holder. Hip-hop head. Friend to one and all. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Co-Founder of @Nextdoor. Former @GoogleMaps engineering manager. @Warriors season ticket holder. Hip-hop head. Friend to one and all. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-09-15"},
{"website": "NextDoor", "title": "meet a nextdoor engineer tim wong", "author": ["Wenbin Fang"], "link": "https://engblog.nextdoor.com/meet-a-nextdoor-engineer-tim-wong-237b78d5533c", "abstract": "Culture Open Source Technology Tim is a senior engineer who, in his three years at Nextdoor, has helped build a small neighborhood social network into a nationwide platform serving over 89,000 neighborhoods across the US. This week, we sit down with Tim to find out a little more about him, working in infrastructure engineering, and life at Nextdoor. I’m a Senior Engineer on the Infrastructure Team. We build and maintain many of the backend services at Nextdoor, including the newsfeed, email, Taskworker infrastructure, and more. I’ve been at Nextdoor for 3.5 years, working on the Nearby Neighborhoods feature before the formation of the Infrastructure team. I’m a Bay Area native and studied at UC Berkeley for my bachelor’s degree and UCLA for my master’s degree. When I joined, the company was pretty small and the website (there were no other platforms at the time) could use a lot of work. While the idea was not proven, I definitely had a sense that this type of social network should exist and there was great potential in the idea. After meeting the team, I was really impressed with the quality of the people from all ranges of experience and loved the jovial company culture. From those things, I knew it was a place that I could contribute, learn, and enjoy myself at the same time. This would probably be the inaugural Infrastructure Team project to re-architect our newsfeed backend. This came after the release of the Nearby Neighborhoods feature and the site redesign around two years ago. With Nearby Neighborhoods, we allowed users to post content visible to neighborhoods adjacent to their own. This fundamentally changed how we had to view our content model, and this combined with built up technical debt over the years made to create an increasingly complex and slow process to render a newsfeed. This became a project of taking these news feed features we had already built, unifying the data model into something more consistent and pluggable, and making the resulting system faster than what came before. We ended up simplifying our newsfeed generation to a couple of different queries that we could optimize and then separating this into our first standalone service outside of our front end application. There were new technologies to learn and come to grips with as well as older application code to profile and optimize. The news feed service works more smoothly and scales better than the old system ever could. But with changing requirements and new features in development, the work of Infrastructure Team is never done. Over the past year or so, our team has started to build our new core services in Go . There was a lot of thought that went into moving away from Python for building some of these services — how we arrived at our decision might be a future topic for the Engineering Blog. I think it has turned out well and something we’d like to push outwards to the broader Engineering organization as we think about building new services. There are plenty to choose from. In my experience as a user of Nextdoor, a neighbor of ours discovered a family of kittens living in an empty lot in the neighborhood a couple of months ago. This person posted this on Nextdoor with pictures and was able to get donations from the neighborhood to shelter the kittens and find new homes for them. When you think about how someone else might accomplish something like that, you start to understand the efficacy of the platform for local communication. As I mentioned before, part of it is the friendliness and liveliness of the engineers here; it’s enjoyable to be at work every day. Everyone is helpful and good willed towards each other. What I really like is that at the same time, we communicate openly and directly: people voice their opinions and are heard. Besides rooting for my favorite sports teams (Go Bears!), I’ve lately been getting into cycling. Every weekend, I’ll try to go out with friends for a ride around the city or up to Marin. On weekdays, I’m usually on the indoor trainer. I’m also into the mechanical aspect of cycling too, so I’ve built my own wheels (to varying degrees of success) and done my own maintenance on my bikes. This was brought up in a recent all-hands that we had in the company: that our platform brings people of all different backgrounds together. The types of communication that take place on other social networks are very different: users normally opt into voices of common interest, so there’s usually not much variation with the types of opinions one hears. On Nextdoor, you’re exposed to many people you don’t know and are tied by physical location rather than interest. So inherently, you’re exposed to a lot of different opinions. We live in a time where it seems like society is becoming more and more divided on issues and people are unable or unwilling to hear different points of view. As a platform, getting people together to have these types of conversations is a healthy change when we need it most. Nextdoor is the neighborhood hub for trusted connections… 13 2 Thanks to Vikas Kawadia . Startup Nextdoor Engineering Culture 13 claps 13 2 Written by Founder and CEO of ListenNotes.com Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Founder and CEO of ListenNotes.com Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "nextdoor loves webpack", "author": ["Harry Kao"], "link": "https://engblog.nextdoor.com/nextdoor-loves-webpack-796fc3249c12", "abstract": "Culture Open Source Technology A few months ago, my teammate Rob and I were adding a feature to the newsfeed that would appear for some members but not for others. We didn’t know at the time of the client request whether the member would be using the feature and, for performance reasons, we weren’t keen on sending the code to the client unconditionally. Instead, we wanted the client (web browser) to do an asynchronous fetch of the feature’s code only when it was needed. The most expedient solution was to create an ad hoc AJAX endpoint that would return HTML / CSS / JS. However: We’re moving toward REST APIs for client/server communication and returning code for the client to execute isn’t remotely RESTful. No — it’s gross and shameful. Asynchronous loading of code is generally useful but an ad hoc solution doesn’t make life easier for the next person who needs to do it. In fact, we already have a ton of code that should be loaded asynchronously that isn’t, precisely because we haven’t had a nice way to do it. After considering the ad hoc solution for longer than I want to admit, we decided to find a better way. We evaluated RequireJS and Browserify before deciding to adopt Webpack. Here are our notes on the pros and cons of each as they apply to our use case. RequireJS is a dependency tracker and asynchronous loader that implements AMD . By default, it makes a ton of HTTP requests: one for each file. Since well-architected JS is often broken up into lots of little files, RequireJS takes you to round trip city and will make your site super slow. There’s an optimizer that addresses this by concatenating individual files together into a bundle. We struck RequireJS from consideration because: Managing the include and exclude options to get the right code into each bundle seems difficult. As our front-end code grows in complexity and we have more people writing code and editing the configuration, I worry that we’ll introduce bugs where a page ends up missing code or code is included more than once. RequireJS doesn’t support content-based cache busting. We want our static asset URLs to change if and only if the bits included in the resource change, which lets us cache them forever. This means that, once you download a resource, you never have to talk to the server about it again until it changes. (For cache busting, RequireJS provides the urlArgs configuration option. This applies globally so it’s not as fine-grained as we need it to be.) Browserify brings the NodeJS / CommonJS require() syntax to the browser. The require() calls establish a dependency graph which is then used to bundle all of the code into a single file. Unfortunately, Browserify doesn’t handle asynchronous loading at all. In fact, the author states that async loading is against the Browserify philosophy and that people who need it should use RequireJS instead . So that’s that. When our colleague Handsome Dan Masquelier first mentioned Webpack , I looked at the docs, didn’t understand any of it, and thought “Okay, whatever, let me get back to RequireJS and Browserify.” After I read a writeup from an Instagram engineer , I finally understood what Webpack does, and it’s really swell . In short, Webpack is a dependency manager and a build system in one. I’ll explain what that means with a few examples: Webpack, like Browserify, uses require() calls to establish dependencies. The dependency graph determines which files are included in the resulting bundle , and in what order. Here’s where things get interesting. Webpack extends/abuses the require() mechanism to allow loaders to determine how a file is processed before it’s included in the bundle. In this example (assuming proper configuration), styles.less is compiled, the resulting CSS is included in the JS bundle as a string, and code is injected that autoloads the CSS into the page. Note that including CSS in the JS file eliminates an additional round trip to the server. require(‘./icon.png’) copies the image to the output directory and adds a content-based token to its name. (This satisfies our requirement that cache busting is based on content.) The new name is returned by the require() call. The CSS loader does the content-based token trick on any large file that’s referenced by CSS too. Or, if the file is small enough, it’s simply included as a Base64-encoded data URI , which saves a round trip. Loaders can be installed for JavaScript code as well. We’ve configured an ES6 transpiler that lets us use some nifty ES6 language features even though browser support is not yet universal. Webpack has a long list of loaders that can simply be plugged in. And if you need something custom, it’s not too hard to write your own . Finally, what about async loading, which set us on this path in the first place? Webpack uses an async version of the require() call which cleaves off part of the dependency graph and places its code into a separate bundle. When this require() is encountered in the browser, the async bundle is fetched and can then be used by the code in the callback. For the particularly curious (and slightly masochistic), let’s look at the generated code to see how Webpack works under the hood. We’ll start with this simple JS file: The corresponding output from Webpack looks like this: JavaScript is kind of hard to read under the best of circumstances (my main complaint about it!) and machine-generated code is often more so. But Webpack’s output is surprisingly comprehensible. The entirety of the code is a single IIFE that’s passed an array of module definitions. In this case, since there are no require() calls, there’s only the single entry module. The body of the anonymous function defines a __webpack_require__() inner function that does the work of loading modules and simulating the behavior of server-side require() calls. Note that the last thing the anonymous function does is to require the entry module. Let’s require() something: The new array of modules (that’s passed to the anonymous function) looks like this. The rest of the output is unchanged: Note that the entry module, which is called by __webpack_require__() , recursively calls __webpack_require__() to load module 1. The call to require() in script.js was changed to __webpack_require__(1) at build time. Webpack does this by parsing the JS of each module, walking the AST , and looking for require() calls that should be replaced with __webpack_require__() . Output (just the modules again, with some content removed): In our Webpack configuration we chained the CSS Loader (which injected modules 2 and 3) and Style Loader (which injected modules 1 and 4). Those injected modules work together to take the CSS string in the bundle and insert it into the page. You’ll notice that there’s some hot module replacement code in there too. I’m not going to go into HMR now but it’s great and everyone loves it. It makes frontend development way more fun (by making it suck less; let’s be honest about how low the bar is) and I think it’s driving a lot of Webpack adoption within Nextdoor Engineering. That’s the super super basics of what Webpack does and how it does it. If you’ve followed this far, you can probably imagine how it works under the hood for more complex examples. In short, there are three features that make Webpack a good fit for us: Dependency management and asynchronous loading. Build system: The pluggable loaders replace Gulp/Grunt/Broccoli/WhoKnowsWhatElse/etc. Hot module replacement: This lets us see code changes live, without reloading and losing state. Webpack’s technical strengths, however, are only part of the story. Every organization is different and has its own approach to change. For us, Webpack started as a grassroots solution (proposed by two engineers to address a problem encountered during a specific project) and has become the de facto standard for new development. We did it by: Having a manager who’s willing to spend extra time up front to improve engineering health in the long run. (Thanks Alex Karweit !) We could have achieved our primary goal way faster if we hadn’t stopped to do this additional work. Researching solutions to our problems, working closely with the front-end team to brainstorm and check our learnings. Prototyping with Webpack, reading code, and stepping through it in a debugger to see how it works. Presenting Webpack to engineering review. This is a regular meeting with a small set of standing attendees (engineering managers and a few senior engineers) where upcoming technical changes are discussed. We received valuable feedback from the wide range of viewpoints that were represented. Implementing our feature using Webpack, taking care to document it well and make everything about it as easy to use as we could. Giving a Share Your Knowledge presentation. SYK is a biweekly all-engineering meeting where anyone can give a 10-minute talk on a topic that’s of interest to the engineering team. Checking in on how people are doing with Webpack during the Frontend Co-Op meetings. If you want your idea to take off, it’s important to make yourself available and go out of your way to help people who are trying to use it. ??? Profit! Watching engineers use Webpack because they want to . As it often is in life, it’s not enough to imagine how you want the future to be. You also have to find a viable path that leads you there and put in the effort that’s required along the way. Want to use Webpack at Nextdoor? Come work for us! Nextdoor is the neighborhood hub for trusted connections… 11 Thanks to Vikas Kawadia , Daisuke Fujiwara , and Wesley Moy . JavaScript Webpack Web Development Open Source 11 claps 11 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "easy 1 click deploys from your phone with slack", "author": ["Mikhail Simin"], "link": "https://engblog.nextdoor.com/easy-1-click-deploys-from-your-phone-with-slack-9b79942a2ac3", "abstract": "Culture Open Source Technology Our original release process was crude: SSH into the machine. Yes, we had only one. git pull Restart Apache. For a private-beta company with 5 employees this technique satisfied all the needs. Since then, Nextdoor’s release process has matured, and the requirements of the release process have changed. Our long-term goal is to enter a world of Continuous Deployment (CD), where every commit will trigger a production release. Prior to achieving that goal we still have room for improvements in our current process, which is time consuming and involves manually entering critical values. Manual work is prone to errors, takes up time, and relies on some internal knowledge. We want the exact opposite. A production release should be error-free, quick, and simple! Our team has discussed various automation approaches. One solution sounded crazy, and crazy appealing. We’ve decided to leverage ChatOps — a term coined by GitHub. Using Slack for automation allowed for several quick wins for the release process: A way for others to learn by example. Slack’s Message Buttons leave no room for errors or typos. We get mobile friendliness for free. The build and deploy process are both done from Slack. My coworker Steve demonstrates: The build command is initiated through a message in Slack. When the build is finished the release is offered as a button rather than requiring a new command. This ensures that there is no room for typos. A few minutes later he decides to press it :) This is a quick win for our team, and it does not require much code. Our build infrastructure is run by Jenkins. Jenkins jobs can be triggered by web hooks such as from git commits, via direct API calls, or in our case - a chat button. The following, trivialized, Alphabot code provides the build command and the release Slack button. Alphabot is Nextdoor’s open-source bot for interacting with Slack. It’s written in python and uses Tornado for coroutine executions: One key piece of infrastructure that is abstracted from this code is that Slack requires the bot to be associated with a Slack App for this to work. The app is then configured with a URL which can receive a Slack payload for Alphabot. Slack has extensive documentation on this. We’re constantly working on improving our release process. We welcome comments and questions on how you can do this at your company, or what else could benefit from ChatOps automation. Nextdoor is the neighborhood hub for trusted connections… 19 Slack Chatbots Software Development Automation Technology 19 claps 19 Written by Reverse Engineer Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Reverse Engineer Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-03-27"},
{"website": "NextDoor", "title": "opinionated and non comprehensive notes from the spark summit", "author": ["Vikas Kawadia"], "link": "https://engblog.nextdoor.com/opinionated-and-non-comprehensive-notes-from-the-spark-summit-ddd703ee80b2", "abstract": "Culture Open Source Technology For those who have somehow missed out on the buzz/hype regarding Spark, Spark is a faster Hadoop. One of the ways it accomplishes this is by using memory much better so that it can perform multiple operations on data without persisting to disk. There are other optimizations too. Spark also has APIs for Graphs and ML. The upcoming 2.0 release of Spark promises significant performance enhancements and new APIs as well. Since we use Spark at Nextdoor Engineering (mainly for feature extraction in our ML pipeline), I attended the 2016 Spark Summit West in downtown SF to get the low-down on the new shiny. I am used to attending academic conferences, and was amazed to see the size of the conference hall and the number of attendees at the Spark Summit . You could only see the keynote speakers via the webcast. It did not matter since almost all of the keynotes were not relevant from an engineering point of view. Some technical sessions in the afternoon were very good. Here are highly subjective, opinionated, and non-comprehensive notes about stuff I found useful. Spark 2.0 is coming soon (preview-release is out). It has many performance enhancements and improved structured APIs. Streaming, in particular, and been re-built to work on DataFrames. Hopefully, Spark Streaming will now be usable in production settings. Spark 2.0 also unifies all contexts (streaming, batch, hive, sql). Here is a summary of whats new in Spark 2.0, and here is a deeper dive into Streaming . Mark Grover and Ted Malaska from Cloudera gave a wonderful talk on Top 5 mistakes when writing Spark applications . This is a must-read if you Spark at all. PySpark performance in practice is not always at the level of Scala/Java Spark. Many people suggested using the Scala APIs if you are ambivalent between Python and Scala. If you have reasons to use PySpark, holden karau gave some great tips on optimizing PySpark performance . You are generally fine if you use higher level APIs (DataFrames and DataSets) and don’t write custom lambdas or UDFs much. If you are using the RDD API from Python, your cluster is spending its life serializing/deserializing objects and volley-balling them between the Python interpreter and the JVM. There are a gazillion parameters you can tune on a Spark cluster to significantly improve your app’s performance. Defaults are terrible, even on hosted providers such as AWS EMR. Databricks cluster has better defaults. Miklos Christine from Databricks shared some Operational tips for deploying Spark . Wes Mckinney, creator of the Pandas project, gave a great talk about Apache Arrow that is defining a common Data Layer/Memory format for distributed computing systems, so that computers don’t have to spend most of their life serializing and deserializing. Looking forward to this going mainstream. arrow.apache.org Was pleasantly surprised to discover that Google also has a managed Spark and Hadoop offering called Dataproc . It already has Spark 2.0 preview release available, clearly ahead of other managed Spark providers such as EMR. Google also has a cloud notebook called Datalab that is built on Jupyter. cloud.google.com Heard good things about the Google cloud platform in general. Google cloud machine learning is definitely worth looking at (currently in alpha). It already has APIs for Vision and Speech, and an NLP API will be released soon as per Jeff Dean ’s keynote at the summit. It also offers Tensorflow as a service. cloud.google.com Turns out there are good reasons to put Spark in containers. Tom Phelan from Bluedata had a nice talk on Dockerizing Spark. Its really useful for on-premises deployment of Spark clusters. Heard about spark-timeseries — A library for time series analysis on Apache Spark. sryza.github.io Not directly related to Spark but found out about Paratext, a csv reader that can parse and read csv at 2.5 GB per sec, nearly saturating IO bandwidth. Written by the super folks at wise.io . www.wise.io Finally, Daniel Rodriguez from Continuum had good tips on using Spark in the Python ecosystem . Anaconda is popular Python distribution for scientific computing. Conda-forge is a community channel worth checking out for stuff not in the core distribution. Surely, there were other great talks that I could not attend because of the inability to quintuply clone myself. All slides and videos are available online . Please leave comments here if you find something interesting. There was a pretty nice Expo with many Big Data companies. A common theme was convergence of OLTP and OLAP, using Spark of-course. Btw, if you like to work on Data, ML and the like, we are hiring. nextdoor.com Thanks to Prakash Janakiraman , Wenbin Fang and matt for great comments. Nextdoor is the neighborhood hub for trusted connections… 10 1 Apache Spark Machine Learning Python 10 claps 10 1 Written by Engineering Lead at @Nextdoor Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Engineering Lead at @Nextdoor Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-07-08"},
{"website": "NextDoor", "title": "we don t run cron jobs at nextdoor", "author": ["Wenbin Fang"], "link": "https://engblog.nextdoor.com/we-don-t-run-cron-jobs-at-nextdoor-6f7f9cc62040", "abstract": "Culture Open Source Technology We open sourced Scheduler on Github . Pull requests are welcome! At Nextdoor, we run a lot of scheduled jobs for various important purposes, such as sending tens of millions of digest emails to our users daily, generating internal reports on our growth, and some operational tasks. Like many other internet companies (e.g., Airbnb and Quora ), we started with Cron and ended up building our own cron replacement, which we called Nextdoor Scheduler. We’ve been using Nextdoor Scheduler for over 18 months and we are extremely happy with it. There are four main problems with Cron. First , the way we use Cron was not scalable. We ran all Cron jobs on a beefy scheduler machine ( c3.8xlarge ). As we gained traction, Cron jobs pushed the machine to its limit, in terms of compute resource usage. Second , editing the plain text crontab is error prone for managing jobs, e.g., adding jobs, deleting jobs, or pausing jobs. For instance, an extra asterisk prevented all production jobs from running the other day: Third , we incurred a lot of operational overhead with Cron. We have near two hundred production jobs that are run thousands of times a day, at different frequency (e.g., minutely, hourly, weekly). Job failure is common. The oncall person had to manually restart failed jobs several times a day, sometimes after midnight. Here is an example of a typical oncall experience: 1) get paged with the command line of the failed job; 2) ssh into the scheduler machine; 3) copy & paste the command line to rerun the failed job. This is certainly not good for engineering happiness — yes, we do care about the happiness of our employees! Fourth , we had little visibility for production jobs during runtime. There was not easy way to know what jobs were running or whether they succeeded. Enough is enough. We decided to build a cron-replacement. But why didn’t we use open source solutions? We just couldn’t find a suitable one. We speak python. We wanted to leverage existing infrastructure components in the company. We wanted to build it, understand it, and own it. To address the scalability issue, we made each job an async task that can run on a cluster of Taskworker machines. We can easily configure jobs that run on Taskworker to automatically retry when they fail, which requires only a single line code change. Our oncall engineers love this auto-retry feature! To replace Cron, we used the excellent python module ApScheduler to schedule jobs, which enabled us to manage jobs programmatically — we built REST APIs, command line tools and human-friendly web UI. The following picture shows the architecture of our Scheduler system. Nextdoor Scheduler is implemented with Python / Tornado . It is run as a single daemon process (Scheduler Process) on a single machine, which consists of three components. Scheduler (or Core Scheduler). It replaces cron and schedules jobs to run. When a job is triggered to run, the Scheduler Process simply publishes a message for the job to Amazon SQS . A cluster of Taskworker machines grab messages from Amazon SQS and run corresponding jobs. As mentioned above, we use APScheduler to implement core scheduler. Scheduler API. It provides a REST interface to manage jobs, e.g. adding jobs, pausing/resuming a job, removing jobs, modifying jobs, and manually kicking off a job. We’ve built command line tools on top of Scheduler API to make operations easy, for example, pausing a group of jobs all at once. Web UI. It is a single page app talking to Scheduler API. We used Backbone.js and Bootstrap to implement the Web UI. Human operators primarily use the Web UI to interact with Nextdoor Scheduler. Information of all jobs and job executions is stored in a data store. We use Postgres primarily here at Nextdoor. Engineers love Web UI of Nextdoor Scheduler, which provides an intuitive way to manage jobs rather than dealing with blackbox-like Cron in the old days. Jobs Page On this page, we can see what jobs we have and when they will run next time. We can also click “Custom Run” to manually kick off a job. Editing a Job We can easily edit a job, e.g., change its schedule and pause it with one button click! This is way better than modifying plain text crontab in the old days. Executions Page Finally, we have great visibility for what jobs are running and whether they succeed or not. Writing code is easy. Productionization is hard. By the time we finished the implementation of Nextdoor Scheduler, we had close to 200 production Cron jobs that need to migrate to the new system. We applied what we’ve learned from the Taskworker project to roll out the Nextdoor Scheduler system. Four steps: We dark launched Nextdoor Scheduler to production — no production jobs were running with the new system yet. We added a feature switch to the base class of all jobs. We slowly and carefully turned on feature switches for each job over two weeks. We shut down the old beefy scheduler machine that ran Cron. With the new Nextdoor Scheduler, we are able to run a much cheaper scheduler EC2 instance ( c3.2xlarge ) than before ( c3.8xlarge ), while keeping the load super low as we offload jobs to run on distributed Taskworker machines. Here’s the CPU usage comparison between old scheduler machine (top graph) and new scheduler machine (bottom graph): We’ve been using Scheduler jobs for over 18 months. We are happy so far. If you’re interested in working on these kinds of problems and other interesting infrastructure challenges, we’re hiring ! We open sourced Scheduler on Github . Pull requests are welcome! This blog was originally posted at engblog.nextdoor.com Nextdoor is the neighborhood hub for trusted connections… 568 9 Thanks to Wesley Moy and Anne Dreshfield . Nextdoor Cronjob Open Source 568 claps 568 9 Written by Founder and CEO of ListenNotes.com Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Founder and CEO of ListenNotes.com Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-01-13"},
{"website": "NextDoor", "title": "scalable code organization one big repo", "author": ["Chuck Karish"], "link": "https://engblog.nextdoor.com/scalable-code-organization-one-big-repo-59b74dd82a73", "abstract": "Culture Open Source Technology Don’t look back. Something might be gaining on you. — Satchel Paige As Nextdoor’s codebase grows we’re looking forward to the challenges of managing its size and complexity. The approaches that other companies use provide lessons in how to design a development system that can grow. Such a design should include a source code management strategy that is consistent with the coding style, maintenance practices, and release practices that meet the organizations business needs. Here I consider the strategy that Google uses for the code that runs on their web site. Rachel Potvin gave a talk at the @scale 2015 conference The Motivation for a Monolithic Codebase , in which she described the central source control system that Google uses to manage a large codebase that changes very rapidly. She did not present the development strategy that draws power from this. The motto for this strategy could be “Don’t look back.” Google’s approach has always been to have engineers work in small teams for a short length of time with a specific goal in mind. When the goal was accomplished each engineer would move on to another project, often with a different team. Engineers are happy that they’re empowered to work on cutting-edge projects with minimum responsibility for overhead. It is important that when each project is finished it is left in a state where the code is self-explanatory and can be maintained or extended by other people, without need to pull the original authors off their new projects to be consultants. The rule of thumb is to design to support rapid growth in the medium term, 18 months or so, and not to over-design. Since the code will be replaced by the time it is three years old, there is no need to build it to last longer than that. Existing architecture and existing code always need to be improved, so there is no shortage of challenging new tasks. When the Google codebase was three years old, it had already become difficult for a developer to keep a stable workspace that wouldn’t be broken frequently by other developers’ changes. The first step to address this was to define self-contained source code packages whose dependencies on other packages were explicitly defined, as Nathan York illustrated in How the Build System works . This allowed each team to work in the subset of the codebase on which their work depended. This also made it clear how tangled the dependencies had become. The next step was to define an explicit API for each package, and to prohibit linkage that didn’t go through these APIs. All the later improvements of the build and test infrastructure, as described in the Google Engineering Tools blog, were built on this code structure. The development strategy survived because it was designed to be scalable and because engineers were encouraged to take the initiative to develop new, more capable tools years before it became clear that they would be essential. It’s a win to devote fewer resources to maintaining past versions of libraries. It’s a win to be able to make global changes that upgrade the infrastructure for hundreds of services at once. It’s cumbersome to make global changes in a huge codebase. Tools like Rosie, which Rachel discussed, make this practical. The issue is the scale of the codebase rather than whether it’s in one or many repositories. Still, when all the code is in one place it’s faster to make global changes and it’s possible to make those changes atomic. Orphaned code is still an issue. Today Google software engineers can advance their careers by putting effort into paying down technical debt. It took many years for that to become true. It’s important to think hard about code structure and scalability surprisingly early in your product’s development history. Peter Seibel’s excellent blog post on developer productivity, Let a 1,000 flowers bloom. Then rip 999 of them out by the roots. includes war stories in which he reports on the difficulty of fixing a codebase for which this lesson was learned later in its history. Don’t look back can work in a huge repository or in a flock of small ones. If a service’s owners allow bit rot to accumulate as the libraries on which it is built are extended, it’s their responsibility to pay down their technical debt before their next release. (Previously worked in Google’s Release Engineering team from 2003 to 2013.) Nextdoor is the neighborhood hub for trusted connections… 6 Repository Engineering 6 claps 6 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-02-22"},
{"website": "NextDoor", "title": "at night the ice weasels come a chat with our co founder david wiesen", "author": ["Vikas Kawadia"], "link": "https://engblog.nextdoor.com/at-night-the-ice-weasels-come-a-chat-with-our-co-founder-david-wiesen-43e5b7d71e8b", "abstract": "Culture Open Source Technology This week, Vikas Kawadia from Nextdoor Engineering is interviewing one of Nextdoor’s co-founders and leading light David Wiesen , a.k.a Ice Weasel. Well, as someone with the last name Wiesen, the nickname “Weasel” was something that both my brothers and I shared. The “Ice” part was actually added on in college. My roommates and I encountered the term “Ice Weasel” because someone had written the phrase “The Ice Weasels are coming” onto a desk in one of our lecture halls. They were referring this quotation by Matt Groening: Love is a snowmobile racing across the tundra and then suddenly it flips over, pinning you underneath. At night, the ice weasels come. We had never heard of an ice weasel before and thought it was pretty funny, and over the next few months Weasel became Ice Weasel and now, 20 years later, it clearly has stuck! I was born in New York, just north of the city, and lived there until I was 7. My family moved out to California (Marin County) and I have lived in the Bay Area ever since. I studied computer science at Stanford and have been a professional engineer since graduating in 1999. Most of my experience has been in small to medium sized startups. The only exception was a 3 year stint at Google. One of the highlights of my professional career was recruiting and on-boarding an engineering team in Hyderabad, India. I lived there for 7 months in 2006–2007. I left Google in 2008 to join a small and ambitious sports startup called Fanbase . After 2.5 years of trying to be successful, we decided that we didn’t have a successful product and that we should do something else. About half of the Fanbase team stuck around in the summer of 2010 and began brainstorming the next great idea. For much of the summer, we had the inkling that there was a great platform around hyper-local communication that hadn’t been built. After thinking about the problem from a few different angles, we decided to build what was essentially a neighborhood-focused social network. Despite the proliferation of social networks, and many people asking “does the world really need another one?” we felt that the world truly did. We released the first version of what would become Nextdoor to a neighborhood in Menlo Park, CA in October of 2010 and haven’t looked back. The experience has been almost exclusively wonderful. Every idea comes with predictions and aspirations about how people will use it, and almost from the beginning, people used our product in a manner that was very consistent with how we hoped they would. In addition, we naturally hoped that people would continue to communicate with each other on Nextdoor and not get fatigued and stop using it. This has also turned out to be true in most cases. Nearly all of our first neighborhoods that have been using Nextdoor for 4–5 years now are still among our most active. I have had the privilege to work on a number of projects that have been both technically challenging and important for the company. Some of the highlights have been: Creating both v1 and v2 of our e-mail infrastructure which includes the generation and sending millions of e-mails a day. Implementing the Nearby Neighborhoods feature which, for the first time, allowed our users to communicate more broadly than just to their specific neighborhood Leading the team that implemented GAIA which is our geospatial database and APIs that our applications use to conduct any geospatial operations. As part of creating this, we also had to migrate all our geospatial “on-the-fly” from the old database to the new one. Helping implement our Agency platform which enables municipal agencies such as police departments and city halls to have a presence on our platform and send targeted messages to a whole city, a specific set of neighborhoods, or a set of geographic areas defined by the agency itself. There have been so many wonderful moments in the past 5 years, both concerning Nextdoor itself, and also the usage of our product. I will force myself to pick one of each: My favorite moment involving a member was a story that happened in Orlando Florida in early 2014. In short, a woman had let her yard fall into disrepair and one of her neighbors called the city to complain. The woman posted an emotional and somewhat angry message on Nextdoor explaining that she has been suffering from cancer and had no money/energy to spend on her yard, and that her husband was filing for a divorce. Instead of her neighbors fighting her on the issue, they instead banded together (without being asked) to help her with her yard work and other chores. The Orlando Sentinel covered the story here. My favorite internal Nextdoor moment was in the summer of 2013. We were getting ready to announce a partnership with Mayor Bloomberg and the city of New York, and the Mayor was going to come visit our office to meet the team. In the week before this announcement , we realized that the Nextdoor experience for members in New York was not as good as it could have been. There was room for improvement with neighborhood boundaries. The result was a HUGE cross-functional effort to redefine all our neighborhoods, and make some changes to the product for NYC users to make Nextdoor better for them. While staying in the office until 3–4AM every night wasn’t exactly fun, it was a great bonding experience and really showed the passion and commitment of the team. We have built a team of engineers who do great work without a lot of ego and who constantly strive to get better. The result is an environment where feedback is shared openly through code reviews and other forums. In addition, we have a culture of generosity where help is freely given from one person to another, even on different teams. I often spend time here and there helping other engineers on other teams with issues they are facing, and I often get the same in return. It feels like we are really a team in the most basic sense of the word. We all want to help each other and see Nextdoor succeed. I love to travel, eat delicious food, and take road trips with my wife and two dogs. In addition, I am an avid curler. After first being introduced to the sport during the 2010 Winter Olympics, I have been fairly obsessed with it ever since, and curl regularly here with the San Francisco Bay Area Curling Club , as well as traveling 5–6 times a year to events outside the state. That’s all from the Nextdoor Engblog this week. If you want to join the fun, we’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 54 Startup Nextdoor Cofounders Culture 54 claps 54 Written by Engineering Lead at @Nextdoor Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Engineering Lead at @Nextdoor Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "managing mobile app releases", "author": ["Abhijeet Kumar"], "link": "https://engblog.nextdoor.com/managing-mobile-app-releases-4b5b3cf6cc3b", "abstract": "Culture Open Source Technology Mobile app releases pose a number of challenges to teams trying to move fast. This is how Nextdoor moves with mobile. Mobile engineers at Nextdoor want to push releases of mobile apps as often as web. The nature of mobile poses several challenges that work against this goal. Mobile releases are a one way street: there is no “Undo” button, and there is no rollback. You can’t take your app “off the market” either. Be it on Android or iOS, once a user has updated their app to a defective version, developers can’t force the user to downgrade to an older version. Shipping monthly is too slow. With the above challenges in mind, how do keep moving fast? Here’s what we’ve learned so far to overcome these challenges. A good release process yields a good fault tolerant system. Unlike the web, where ad-hoc deployments are possible, we release our mobile applications based on scheduled dates and not on milestones. Getting into a strict release cadence has helped small teams concurrently developing features to continuously make improvements without getting in each other’s way. For Android, we cut a release branch every Friday and publish the new version from this branch a week later. We use a similar schedule for iOS, except we use a biweekly cadence. This is a direct result of iOS app review turnaround time . Development continues on master for the next release while the current release branch accepts critical fixes for the week leading up to the release. From “When should we release this feature?” to “What are we releasing today?” Inspired by companies like Facebook , Google, Yammer , and others, we at Nextdoor have shifted our expectations around testing to include “dogfooding”. Employees use pre-release mobile apps on a daily basis and provide feedback. This stream of continuous feedback helps a great deal in catching regressions and improving our product. We’ll talk more in detail about how we implement dogfooding in future posts including the technology behind it. If a software component is deemed faulty post-release or even in the last stages of testing a release, a good fault-tolerant system allows for graceful degradation. The system keeps working for users without exposing the faulty component. For example, the app should have the ability to switch back to the old photo viewer if landscape photos don’t work well on the new photo viewer. At Nextdoor, every new feature that we ship can be turned on or off by using a flag (which we call “feature config”) that comes from the server. These flags are configured on a server side management console. This allows you to keep the train running — it unblocks engineers from shipping other great new features at speed. We have learned to say “no” to last-minute, though well-intentioned, changes. This has posed an existential problem — those changes age and leave users unhappy. However, last minute changes always pose the risk of destabilizing a release. By releasing often, any change can be deployed with the next release with minimal time penalty. This turns a last minute change into a normal change for the next release. The change now has higher failure transparency earlier in the release cycle, thereby reducing mean time to repair. It can’t be stressed enough how important it has been for us to adopt a strict regimen for mobile releases. We went from engineers firefighting a shippable release on Friday to a release engineer pressing the button after sign-offs from Product Managers. We don’t believe our release process is perfect yet, but we’re constantly learning and getting closer. We will be talking in more detail about the benefits of dogfooding in an upcoming post. We’d love to hear your thoughts — what works and what doesn’t work for you? If you’re interested in challenging problems on mobile, we’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 32 Startup iOS App Store Technology 32 claps 32 Written by @Nextdoor, @workspotinc Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by @Nextdoor, @workspotinc Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "intro to data science analytics at nextdoor", "author": ["Carly Villareal"], "link": "https://engblog.nextdoor.com/intro-to-data-science-analytics-at-nextdoor-a7fc8fec30d8", "abstract": "Culture Open Source Technology One of the first questions I ask any data scientist I meet is, “What type of data scientist are you?” Data science is an extraordinary role; the thing I love most about it is the diversity of backgrounds and skill sets that data scientists bring to the table. But, because the discipline of data science is growing so rapidly, and because “data scientist” can mean so many things, someone will say “ I’m a data scientist…” and what they’ll actually mean is: I’m a statistician I build production-level models I’m an ETL whiz I’m an analytics-driven product strategist … or many others! These are all important, and hypergrowth companies like Nextdoor need them all! But, they’re all very different, and we needed to define a common thread. Data Science & Analytics at Nextdoor is, at its core, about decision-making. If we look across the entire team, across all the disciplines listed above, the thing that unites them is that all of our Data Science & Analytics employees are contributing to decision-making every single day. Our analyses help stakeholders make decisions. Our models make decisions in the product. We make decisions ourselves about how Nextdoor should access and use data. The most important thing we’ve found in hiring is to make sure that the type of decision-maker we’re bringing in is well-suited to the needs of the team and the product they’ll be working with. A mismatch in expectations between the data scientist and the team is a recipe for unhappy data scientists, unsatisfied stakeholders, and high turnover. But, here’s the problem. People are not just one thing. When I started at Nextdoor in 2015, the data team (including data engineering) was 4 people in total. Since then, the data science & analytics organization has grown significantly, and we’ve had the pleasure of working with and talking to many more along the way. The skill sets and interests in that group are extraordinarily diverse; the only thing they have in common (at least from a technical skills perspective) is that they defy classification. Data science is an evolving role, and at Nextdoor, we want to give people the option to grow, explore, and build their skills across a variety of disciplines. And, we’re very much of the opinion that that diversity of skills is beneficial in the data scientist’s work; the best modelers are the people who can test and analyze the output of the model, the best ETL writers are people who intimately understand how that data will be used in analysis, and the best analysts are people who can identify how their work might lend itself to simple or complex models to improve the product. With that in mind, we needed to figure out how to define “what a data scientist is” at Nextdoor. How do you support people who grow across disciplines? How do you support people in roles that require deeper specialization? How do you measure career progression? And, finally, how do you make sure that data scientists are able to see how they themselves can grow and be successful at Nextdoor? We’ve developed two frameworks to define a Data Science role here at Nextdoor: Big Decisions and Core Technical Skills . The Big Decisions framework helps us evaluate the impact of the employee’s work, while Core Technical Skills helps us evaluate their expertise. To be successful, you need both. Using the two frameworks together has allowed us to show the wide variety of different ways that a data scientist can be successful here, and has allowed us to better define the roles and levels of the people we’re hiring. Big Decisions Because the core of Data Science & Analytics at Nextdoor is about decision-making, we can define a concept called a Big Decision , which happens when a person or a product makes a decision that impacts the company. We can build a model, which can make Big Decisions at a small feature level all the way up to a major system level. We can formulate and run an analysis, which can drive Big Decisions at a team level up to a company strategy level. We can make Big Decisions about how a team (or the whole company) should use or interact with our data. We can build a data set or automate data delivery, which can drive Big Decisions at a team, organization, or company level. We can determine how tests and experiments should be run & analyzed, which defines how the company makes Big Decisions about feature changes. Fundamentally, we measure our team’s impact by the impact of the decisions that we help make. The framework works because it’s inclusive of all of our data science disciplines, and because it ensures that we’re continuously driving value, regardless of the type of data science work the person is focused on. Core Technical Skills The other framework we’ve implemented is the concept of Core Technical Skills. One of the strengths of our team is that we have many types of people with many diverse skills. We do not want to dictate or limit learning to particular areas — rather, we want to demonstrate excellence in the areas that matter most to an employee’s particular role, and we want to reward people with deep specialization as well as broader skill sets. Core technical skills at Nextdoor include (but are not limited to) the following: Mathematics (Linear Algebra, Calculus) Statistics and Probability Machine learning Data manipulation Data table design & architecture Data engineering Data visualization Experiment design Cartography and GIS Product analysis ETL design Computer language fluency Qual/quant crossover analysis Survey design and sampling Software engineering We have had successful data scientists in all of these areas, including people who go deep into one area and people who can do a little bit of everything. When we combine the Big Decisions framework with the Core Technical Skills list, we’re able to build a comprehensive picture of each data scientist’s role, and determine what growth looks like for them at Nextdoor. Our head of engineering, Antonio Silveira, has been heard to tell new managers that “people are the hardest problem you will ever try to solve.” As data scientists, we’re natural quantifiers, and there’s a tendency to want to map people out, put them on a chart, and set them on a clear trajectory. But — people are more complex than that, and paths change. (We’re growing quickly enough that our needs as a company change all the time, too!) The important things, for us, are that we figure out a way to define what a data science role looks like, focus on decision-making and impact across all of our disciplines, and give our data scientists visibility into what their career development and growth path looks like here at Nextdoor. On that note, we’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 152 Data Science Analytics Engineering Machine Learning 152 claps 152 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-05-26"},
{"website": "NextDoor", "title": "sitevars¹ at nextdoor how engineers quickly push configuration changes in production", "author": ["Luiz Scheidegger"], "link": "https://engblog.nextdoor.com/sitevars¹-at-nextdoor-how-engineers-quickly-push-configuration-changes-in-production-d40fd4f6ba5d", "abstract": "Culture Open Source Technology Nextdoor engineers are always looking for ways to move faster. Making changes quickly and safely allows us to deliver more value to our members in neighborhoods around the globe, as well as to minimize the impact of bugs and disruptions we may come across. In this post, we’re going to share our experience building Sitevars, an internal system at Nextdoor which empowers engineers to push configuration payloads to all our servers across multiple regions in seconds. Before Sitevars, engineers at Nextdoor had to go through a full write-commit-deploy cycle to update configuration values. This meant that the iteration cycle rarely took less than an hour — too long for fast product development. The problem As we iterate on our product, we come across many areas where features can benefit from having an easily-editable configuration payload with domain-specific parameters. Some examples of this include: ML parameters — we use machine learning models extensively at Nextdoor to help moderate content and improve our members’ experience. As we iterate on these models, it’s very helpful to be able to quickly experiment with different threshold values for classifiers, as well as weights for model features, etc. Global kill switches — we use circuit-breaker style switches to protect new features in production. If anything goes wrong with a new feature, we can quickly turn it off before it causes problems for our members. Logging configuration — we use Sitevars to quickly update dynamic logging levels for different parts of our codebase without performing a new deploy to production, or even making code changes. These examples all share a common need for a system that can propagate configuration changes — typically small JSON payloads — to our server fleet quickly and safely. When an engineer updates a configuration, that change should propagate to our servers across the world within seconds. We rely heavily on monitoring and automated alerts to notify engineers when a Sitevar change breaks something, so it can be quickly reverted to a healthy value. In addition, the Sitevars service needs to be robust — a transient failure shouldn’t prevent a request from being served successfully. Finally, Sitevars payloads must be cheap to read. Some of our requests access dozens of Sitevars, so reads should take no more than a few microseconds. Sitevars: fast-propagating, versioned JSON Sitevars consist of two main parts: a Go service which manages the backing store for Sitevars payloads, and a set of client APIs in our Django application which handle service failure and local caching for improved performance. In addition, we built an internal tool that gives engineers a simple UI to create, search, and edit any Sitevars payload. The Sitevars Service The service component of Sitevars is a Go application which provides an API for callers to create, update, and fetch Sitevars payloads. Each payload contains a small piece of JSON (we currently limit this to 16KB in size), as well as common metadata fields — last author, update time, version, etc. Sitevars objects are stored in a globally-replicated DynamoDB table. The schema for this table is very simple — its hash key consists of the unique name of a configuration, and its range key is a version number: When a Sitevars object is updated, we insert a new row into the table, with the latest value of the payload and a new version number. This ensures that updates are non-destructive, and that reverting to a known-good configuration in case of problems is trivial. Single-row fetches from DynamoDB typically take a few milliseconds to complete. While that cost isn’t too high for a single Sitevar, many of our endpoints fetch dozens of configurations, so minimizing this latency is critical. To accomplish this, the Sitevars service keeps an in-memory cache with the latest version of each Sitevar. As the working set is relatively small (each Sitevar is limited to 16KB, and we have a few hundred configs to date), the service can easily hold all Sitevars in memory. Because of this cache, the majority of fetches never make a roundtrip to DynamoDB. Another advantage of a small working set is that it allows us to trivially refresh the entire cache at a set interval. At the moment, this is done every 60 seconds. Finally, we use a Redis pub-sub channel to notify the service when a Sitevar is updated, allowing it to more quickly invalidate a single entry in the cache. This serves a key iteration requirement — it allows engineers to observe their change in the product almost immediately . Fast access to a Sitevars payload in the service is only half of the equation to ensure Sitevars fetches are efficient. Communication between our Django containers and the Sitevars service must also be as fast as possible. We address this in two ways: we deploy the Sitevars container as a sidecar to our Django application, and we use gRPC as a transport mechanism. Deploying the container as a sidecar ensures that calls between Django and Sitevars never leave a single host, and using gRPC (instead of, e.g., JSON over HTTP) reduces the p50 latency for requests from about 3–5ms to about 800µs. Since we built the gRPC server using grpc-gateway , that change was trivial to implement. We were quite surprised to find such a big performance improvement! Client-side APIs The second main component of Sitevars is the set of APIs for developers in our Django codebase. These APIs provide a convenient, type-safe way to access Sitevars in our web application. One of their key features is that they require developers to provide, in code, a default value for their Sitevars. This serves two purposes: it allows engineers to write and commit code ahead of creating a specific Sitevar, and it provides a last-resort fallback in case the Sitevars service goes down for any reason. However, using the last-resort code fallback for a Sitevar can be risky, especially as it becomes stale for configurations that have been heavily edited over time. Even if the API returns the default successfully, there’s a large chance the calling code is no longer factored to handle the default value without crashing. To mitigate this scenario, we maintain hourly S3 snapshots of all Sitevars, and bake them into our Django container. This allows us to ensure that, in the event the Sitevars service goes down, our web application can still use Sitevars that are no more than about one hour old. When discussing the Sitevars service above, we talked about a caching and transport strategy that brought down the cost of fetching a configuration to just under a millisecond. However, we have one more trick up our sleeve to make this number even smaller: we maintain a request-scoped cache of any fetched Sitevars in our web application. This means that any Sitevar payload is never fetched into Django more than once per request. Any subsequent fetch of the same configuration is only a Python dictionary access away, at the cost of a few microseconds. This is especially useful for configurations that are fetched frequently, such as ones used to drive core pieces of our web infrastructure. When all of these strategies are put together, latency for fetching Sitevars falls into a bimodal distribution, where about half of all configuration fetches takes less than 100µs to complete (when they hit the per-request cache), while the other half takes between 500µs and 800µs (when they require an RPC to the Sitevars service). Lessons Learned As we developed and shipped Sitevars, we capitalized on the value of a few important lessons. First, simplicity is a key factor in the success of an internal tool. Even though Sitevars is a service aimed at engineers with a good understanding of our tech stack, its focus is still on a simple, predictable API and an easy to use editing tool. This encourages engineers across Nextdoor to use Sitevars in many more places than we had originally envisioned. Our use of DynamoDB global replication also automatically ensures that the values of Sitevars are unified across all AWS regions in which we operate. Finally, we were really surprised by the latency difference between HTTP+JSON and gRPC. When we made that change, we expected to see performance benefits, but were delighted by how significant the gains were. Going forward, we plan to expand Sitevars functionality beyond our Django application, to support it across our other services as well. Conclusion Sitevars are deployed at scale at Nextdoor, where they help power and customize many different features. With the caching and transport architecture we shared here, we brought down the cost of fetching a single configuration from 5–7ms to less than 100µs. Sitevars serve close to 100k QPS during peak times, without impacting site performance or stability. If working on practical, performant infrastructure like this gets you excited, we’re hiring — come join us ! [1] The name “Sitevars”, as well as the inspiration for the benefits of this system, come from previous work in the industry. If you’re curious to learn more, check out this great paper . Nextdoor is the neighborhood hub for trusted connections… 252 1 Dynamodb AWS Distributed Systems Configuration Management Deployment 252 claps 252 1 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-04-28"},
{"website": "NextDoor", "title": "data migrations dont have to come with downtime", "author": ["Sameer Suresh"], "link": "https://engblog.nextdoor.com/data-migrations-dont-have-to-come-with-downtime-eabc15893700", "abstract": "Culture Open Source Technology At Nextdoor we invest heavily in Redis , a highly-performant in-memory datastore, to power many of our services by providing low latency access to data. Today, we own a dozen different instances of Redis via Elasticache ; many of these correspond either to our microservices or to specific use cases within our primary Django application, like AB Test Tracking. They can serve as a source of truth for data or as a pure cache for performance improvements. With the exception of a few outliers, the majority of our Redis deployments were initially created as single instances of primary and replica nodes, so we could be certain that the failure of a single node wouldn’t cause a permanent loss of data, and also that we could continue to serve traffic in the event of a failover. Moreover, by isolating specific use cases to their own Redis instances, we could also contain different failure modes to limit the impact of individual issues. While that sounds a lot like the makings of a highly-available datastore, we realized that we weren’t quite there, especially as some of our instances reached the limits of their load capabilities. In this post we’ll explain some of these shortcomings in availability, which ultimately led us to re-architect our entire set of instances, as well as the approach we used to oversee the transition without affecting any of our core services. Let’s consider, for example, a scenario in which a Redis instance consisting of one primary and replica node each, and operating near max CPU capacity, triggers a failover (perhaps the replica experiences a hardware issue and a new one needs to be provisioned). The primary now begins to take a snapshot of itself to send to the new replica, which causes the primary to reach capacity. At this point, requests to this Redis instance begin to time out, and maybe some users begin to see limited functionality. We can even take this a step further, though; if our Redis client isn’t smart enough to handle these timeouts appropriately, we could cause a ripple effect with the potential to back up all traffic to our critical services. Although this is just one specific example, we’ve already identified a set of key underlying issues that make our Redis configuration vulnerable. For one thing, CPU utilization becomes an important bottleneck in guaranteeing availability, which is never great for services which tend to be bursty and see spikes in traffic throughout the day. More important, however, is the fact that there isn’t a clean method of scaling up our Redis instances to meet the demands of our services (other than purchasing larger instance types, but this is not a very forward-thinking solution). Fortunately, this type of issue is relatively common, and Redis itself offers a clustered mode to address it. With clustering, our data is distributed across multiple shards, and each shard owns some fraction of the entire keyspace. From an availability standpoint, this is beneficial because it allows us to scale our Redis instance horizontally when necessary (simply by adding new nodes to the cluster), without having to incur significant periods of downtime. Additionally, the impact of major issues like the one described above is mitigated somewhat; if one shard hits CPU capacity, the remaining shards can continue to operate normally, so issues are constrained to only a fraction of requests. As we continued to scale, we began an initiative to upgrade all of our Redis deployments, with several priorities in mind: Switch over to clustered Redis: this would address our need for horizontal scaling and essentially solve all of the problems described above. Upgrade old instance types: most of our Redis instances were missing out on the performance improvements incorporated into newer hardware, so upgrading would additionally alleviate some of our concerns regarding high CPU utilization. Minimize downtime in the transition process: as many of our Redis instances are integral to our core services, it was important that the process of switching over to new ones wouldn’t affect the member experience. Here we arrived at the biggest challenge: figuring out the process for the migration. Though we were very clear on what our end state needed to be, the last requirement of minimizing downtime ruled out a lot of the approaches that we might have used to finally switch over to our new instances. Notably, our goal of migrating to clustered Redis meant that we likely could not go directly through AWS, which only offers support for upgrading engine versions without changing the cluster scheme (as in, non-clustered to non-clustered or clustered to clustered). To that end, our best option would have been to manually create a new single-sharded Redis cluster, restore it from a backup of the old instance, and then scale up the number of shards later — all without being able to accept write-traffic during the several hour-long transition [1] . In order to achieve all three of our goals, we’d instead need a little bit of creativity to get us through the transition. It was around this time that we had just started investigating the usage of Envoy , a distributed proxy built by Lyft that provides more observability into large microservice-based architectures. Envoy gave us the ability to add things like fault injection and health checking to our services for the purpose of testing our feed’s stability, simply by deploying it alongside those services. It also had the ability to serve as a Redis proxy, and more specifically to handle the routing of Redis requests across different upstream clusters [2] . Here we found the perfect abstraction we needed for a large-scale Redis migration; by adding Envoy in as a middle-man to all of our Redis instances, we could more intelligently coordinate the migration of data to our new Redis clusters while continuing to serve traffic to users. To illustrate this, let’s observe the full sequence of events required to switch over from an old instance to a new one: Step One: Proxy all traffic to the old instance Envoy runs as a sidecar Docker container alongside our service, and we simply modify the service to treat Envoy as its upstream Redis host. In turn, Envoy, which is listening on a specific port for Redis requests, directly forwards the requests (both read and write) to the actual old Redis instance. Importantly, other than modifying the address for Redis, no changes are needed on the client side, and the client is essentially unaware that its requests are not directly going to Redis (an exception to this occurs if the original Redis instance is already clustered; in this case, the client may need to be modified to use a normal Redis client library rather than a clustered Redis library, as Envoy behaves as a “non-clustered Redis”). Step Two: Mirror writes to the new cluster After provisioning a new Redis cluster with our necessary specifications, we modify Envoy’s Redis listener definition to mirror, or dual-write, to the new clustered Redis, so that any new writes go to both the old and new Redis instances. Note that Envoy adopts a fire-and-forget model for dual-writing, so it does not wait to determine whether or not the write to the new cluster is successful at all, but in general any issue here is rare. At this point, the new Redis cluster is still far behind the old one, but we have a relative sense of certainty that the two won’t begin to diverge again after the actual synchronization happens. Step Three: Run a full backfill between clusters This step, the most computationally intensive, entails scanning across the old Redis instance and copying all of its contents over to the new one. To do this more efficiently we use RedisShake , an excellent open-source tool from Alibaba, and wrap our own ECS service around it to automate the synchronization. Note that much like taking a snapshot, running this script has the ability to peg CPU, so special care must be taken to run the script with a reasonable QPS, or queries per second, setting, and also ideally to run it during low-traffic periods. The two instances should be perfectly in sync now, and any dual-writes that were missed have most likely been caught. Step Four: Verify and switch over To confirm that the two instances are in sync, we run the RedisFullCheck tool, which identifies keys that differentiate between the old instance and the new instance after several rounds of comparison and stores them in a sqlite database. In our case, key differences were more likely an indication that a particular usage of Redis somewhere was not compatible with Envoy (these incompatibilities are described below) rather than an indication of a missed write from the backfill script; some sleuthing may be required to see this step through. Once the check script does run without issue, we finally update the Envoy listener configuration again to now route all traffic entirely to the new cluster, and delete the old one. Employing the strategy above, we were successfully able to migrate all of our Redis instances to an end state where each instance was clustered and operating the latest version of Redis. In practice, however, we found that this approach was not without its limitations, especially in the usage of Envoy. Most significantly, we found that Envoy only had support for Redis requests that hashed to a single shard, which ruled out all cluster-specific commands (though since most of our instances were not previously clustered, we were fortunate to not have many of these). As a consequence, however, any transactional logic using the Redis MULTI command was also unsupported. To address this, we modified our transaction-based code to use Lua scripting instead, which offered the same atomicity benefits in a manner that was compatible with Envoy [3] . At Nextdoor, we’re constantly investing in improving the quality of our product and the stability of our infrastructure. Upgrading our Redis deployments was just a single piece of a several month long initiative that brought together engineers from multiple teams across the company to make our feed, one of the centerpieces of our product, something that members can rely on. These types of problems, and the brainstorming that comes with them, are what excite our engineers — as always, if they’re interesting to you too, join us ! Nextdoor is the neighborhood hub for trusted connections… 383 Redis Caching Envoy Proxy 383 claps 383 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-27"},
{"website": "NextDoor", "title": "accessibility nextdoor", "author": ["Jun Lee"], "link": "https://engblog.nextdoor.com/accessibility-nextdoor-4f1375bf5240", "abstract": "Culture Open Source Technology Accessibility: the quality of being able to be reached or entered (from The Oxford dictionary) Nextdoor is the neighborhood hub where you can share information and communicate with your neighborhood. We want our platform to be broadly accessible, and have implemented features to help accommodate all abilities: Dynamic type : Adjusts the font size automatically when the user zooms in for better readability (Larger text, easier to read) Screen reader : Helps the user to read content without looking at the screen Switch control : Helps those with limited mobility to navigate, select, and manipulate the screen Voice control : Helps the user speak commands to their devices In addition to employing these features, we are also working to create an engineering culture at Nextdoor that is continuously focused on making Nextdoor more accessible. Creating an empathetic environment Building with empathy across the company is the basis of making the Nextdoor platform accessible. With empathy, employees are able to think more broadly about the experience we are providing to all of our neighbors. One simple example is to guide engineers and designers through the mobile app using the screen reader feature with the screen off (which helps the user read content without looking at the screen). Experiencing Nextdoor in this way goes a long way toward helping employees understand how the experience can vary with the user’s abilities. Building training materials We have created internal guidelines for engineering and design teams around how we can improve accessibility. We use these guidelines to train product teams and create awareness on the importance of accessible products and as a result, have seen wider adoption of accessibility principles. Adding Linters As we improve accessibility, we also need to maintain quality by preventing regressions. For the Web, we added accessibility linter which is a tool that analyzes source code to flag programming errors, bugs, or stylistic errors that are specialized for accessibility. By doing that, we can maintain the quality of Nextdoor’s accessibility. Creating spaces to meet We have weekly office hours, where anyone can drop in to ask questions, share their opinions, suggest improvements, and get help testing features. In addition to weekly office hours, we have also created a monthly co-op session where anyone can share their knowledge and discuss a wide variety of accessibility projects. Previously, if the user used a keyboard to navigate the site, the user had to tab through every single element in the navigation menu (left side of the screen) to get to the newsfeed. With the new bypass block, users can bypass the menu using skip links to go directly to the news feed. Last year we focused on creating a culture where accessibility is the essential part of Nextdoor and started building our component library with a focus on accessibility in mind. In 2020, we will continue to improve the accessibility of Nextdoor. If you are interested in helping, we are hiring! Nextdoor is the neighborhood hub for trusted connections… 103 5 Nextdoor Accessibility 103 claps 103 5 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-04"},
{"website": "NextDoor", "title": "what is the technology behind nextdoor in 2019", "author": ["Gonzalo Maldonado"], "link": "https://engblog.nextdoor.com/what-is-the-technology-behind-nextdoor-in-2019-ccc3d2173eda", "abstract": "Culture Open Source Technology (this post is adapted from an earlier article: https://engblog.nextdoor.com/what-is-the-technology-behind-nextdoor-5b74b3efcc31 ) On a typical day at Nextdoor, we are: Deploying to production 12–15 times Inserting billions of rows to our Postgres and DynamoDB tables Handling millions of user sessions concurrently Adding new neighborhoods to Nextdoor (over 247,000 neighborhoods worldwide) Getting ready or launching new countries (we just went live in Canada !) How do we do this? Keep reading for a behind-the-scenes look into how we scale at Nextdoor (psst, we’re hiring !). Python is our Lingua-franca both for our serving stack and for our offline processes. We make use of the Django Framework for our web applications, NGINX and uWSGI to serve our Python 3 code, and just like all of our services, we serve it behind an Amazon Elastic Load Balancer . We also use Conda to manage our Python environments, and we’re investing heavily on MyPy to add type safety to our codebase. Our primary data store is PostgreSQL . To scale horizontally we use a combination of application-specific read replicas as well as a connection pooler ( PGBouncer ) and Load Balancer as a custom microservice in front the databases. For more specialized tasks, we use DynamoDB for documents that need fast retrieval. Like most of the Internet, we still make heavy use of Memcached and HAProxy to help with our performance, but we’re also now leveraging Redis -via ElastiCache - to use the right data type for the job. Amazon CloudFront is our CDN, and we use Amazon SQS for scheduling jobs. For running those background jobs, we use a Python-based distributed job processor (which we’ve simply called “ Taskworker ”), which consumes tasks published to Amazon SQS queues. We also built a Scheduler system on top of Taskworker to replace Cron for time-based jobs. We strive to make our services as lean and fast as possible, and our minimalistic Go Stack reflects this. Most of our services use gorilla/mux as their router, and we’ve built our own custom Go framework we are hoping to release as open source soon. For communicating between services we use a mix of SQS , Apache Thrift and JSON APIs. We also use Zookeeper for service configuration. For storage, we mostly use DynamoDB. Most of our Data processing is done via AirFlow, where we aggregate our PostgreSQL data to S3 that we then load it into Presto. For Machine Learning, our team uses Scikit-Learn , Keras , and Tensorflow . Here at Nextdoor we have a Dev Tools team exclusively dedicated to making sure all of our engineers are productive and can easily deploy their code without having to worry about the infrastructure. All of our services are deployed as Docker images, we also use docker-compose for local development, ECS / Kubernetes for prod/staging environments. We’re discussing eventually moving everything into Kubernetes. Our Python deployments are done via Nextdoor/conductor , a Go App in charge of continuously releasing our application via Trains -a group of commits to be delivered together-. Releases are made using CloudFormation via Nextdoor/Kingpin . Our Frontend is now almost exclusively powered by React , with Less for base stylings and Redux for state management. We use Storybooks for creating reusable components. Our Frontend consumes a mix of GraphQL and JSON APIs. We also use OpenAPI specs to create schemas for our JSON APIs. For Android, we’re majority Kotlin . We use Firebase for crash logging, Datadog for analytics, ViewModels and MvRx for architecture, Moshi/Retrofit/OkHttp for network, RxJava for Observables, and Glide for images. We test our app using JUnit and Robolectric via CircleCi. On iOS, we’re majority Swift. We use our own Nextdoor/ndlogger-ios for logging and Nextdoor/corridor for url parsing, Fabric for crash reporting, Datadog for Analytics, and Firebase for storage. Some of our architectural models are using ViewModels, Factories, etc. We test our app using unit tests via CircleCI , and Fastlane for releases. Geospatial Technologies Given that we work with a significant amount of location data (goal: every residential housing unit and business in the world!), we have designed data services atop our storage layer for fast and efficient access to the information contained within. Our PostgreSQL database makes heavy use of the excellent PostGIS extension for spatial operations. We use libraries like GDAL and GEOS for spatial algorithms and abstractions, and tools like Mapnik and the Google Maps API to render map data. We are currently in the process of developing a brand new data store and custom processing pipeline to manage the high volume of geospatial data we expect to store (1B+ rows) as we expand internationally. This includes storage, serving, and updating of the data at very high volume. If any of this sounds interesting, we’re hiring across the board! Nextdoor is the neighborhood hub for trusted connections… 280 1 Nextdoor Docker Django Python React 280 claps 280 1 Written by Señor Engineer at @Nextdoor. Scaling teams to Scale apps. @Yammer, @YouTube alumni. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Señor Engineer at @Nextdoor. Scaling teams to Scale apps. @Yammer, @YouTube alumni. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-26"},
{"website": "NextDoor", "title": "topic tagging for posts on nextdoor using machine learning", "author": ["Hari Krishn Gupta"], "link": "https://engblog.nextdoor.com/topic-tagging-for-posts-on-nextdoor-using-machine-learning-2a57dfb2e6f0", "abstract": "Culture Open Source Technology Topics are spaces on Nextdoor where members can have conversations with neighbors who share common interests. We have many curated Topics, each with its own feed, where members can participate in the discussion relevant to that Topic. Previously, members had to post in the Topic feed in order to make a Topic-tagged post. We wanted to make it easier for our members to make a Topic-tagged post, choose an appropriate Topic for their post, and discover all the various kinds of Topics available on Nextdoor. To address this need, we decided to add an optional step at the end of the general post creation flow which would show some Topic options relevant to the content of the post to encourage members to tag their post with one of the suggested Topics. Tagging a post with a Topic adds the post to the Topic feed and surfaces the post for other members interested in the Topic. This tagging suggestion appears immediately after a member had made a post. The member can then select the appropriate Topic, or ignore this step. This step disappears if the member does not choose an option. To verify our hypothesis that members will find these Topic suggestions helpful, we conducted an A/B test where one variant of members were shown Topic suggestions. These suggestions were obtained by running a basic logistic regression classifier which suggested a Topic out of our five most popular Topics if the post’s contents were a match. Our members responded very positively, so we decided to move ahead with this project. One of the main challenges of building a Machine Learning solution for this use-case was that we had a lot of Topic tags. So, we had to develop a classifier which could perform low-latency, high-precision classification while still making sure the classifier checks the viability of all the Topic tags for a post. We also had to make sure that this system had a high throughput since posting was one of the higher traffic use-cases compared to other features where we had used live prediction. For this supervised classifier we used the dataset which was a combination of previous posts made to different topics spaces and hand-labelled data. We then mapped this data to vectors of features comprised of tf-idf vectors and a set boolean features which indicated presence of words or phrases specific to a particular Topic class. We tested the performance of logistic regression, random forest, gradient boosting, and XGBoost algorithms over our dataset and found that XGBoost gave the best results in terms of prediction speed and overall precision. To further improve our performance, we tried using a smaller set of classes and eventually reduced the set of classes to 30 most popular Topics. This greatly enhanced our accuracy while still maintaining a good overall recall for finding a match for an incoming post. The model was deployed to production through our in-house machine learning system Adroit, which is a restful microservice used to serve models and do online predictions. The service is horizontally scalable and has abstractions designed for serving a large number of models. For each model in Adroit we add a corresponding Adroit client in the API layer, so we did that for the Topics-classifier. We completed the remainder of wiring for this use-case by writing APIs which the front-end clients could use to get predictions from the Adroit server. Using this system we were able to make it easy for our members to discover these Topic tags and spaces where they could have conversations with other neighbors who share their passion. We found that the vast majority liked these tagging suggestions, and we could use the accept/reject signal provided by them to further improve our model. Here at Nextdoor, we are constantly looking for new ways to use machine learning to establish Nextdoor as the essential local app for neighbors everywhere. Is this the kind of work that excites you? If so, we are hiring! Join us at https://about.nextdoor.com/us-careers/ . Nextdoor is the neighborhood hub for trusted connections… 180 Machine Learning Nextdoor Engineering Technology Data Science 180 claps 180 Written by Coder. Distance runner. Wannabe Triathlete. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Coder. Distance runner. Wannabe Triathlete. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-13"},
{"website": "NextDoor", "title": "challenges of monitoring sparse data and what to do about it", "author": ["Mikhail Simin"], "link": "https://engblog.nextdoor.com/challenges-of-monitoring-sparse-data-and-what-to-do-about-it-c9576b725289", "abstract": "Culture Open Source Technology Whether you’re starting a new company or adding a new feature in your existing product, I strongly recommend adding metric monitoring early on to both your engineering as well as product data. Whether that be database query latencies or number of page views — start recording it early in your development. I know it may be hard to justify this work and you may be thinking, “I don’t have time to add monitoring” or “The data is too sparse and will be much too noisy to be useful” or “I’ll debug it in production when it’s bad.” It will require a judgement call to invest in this work, so consider this scenario when you are prioritizing your next tasks: You are running a bootstrap company and all employees (all 3 of you) are working hard to get the product up and running. There is no time to write unit tests or add monitoring. You have 100 customers and you want to 100x that number. A year into development, you see glimpses of success of your future and, naturally, your course of action is to double down on the features. Then, at some point, your CEO (acting customer support & relations) starts receiving emails about how sluggish your product is. Your instinct is to blame the database and add some indexes. This helps in one instance, but not another. Suddenly the sluggish report comes about a section of code you haven’t touched in many months. What now? Should you stop developing those features you promised and investigate? How did we get here? When did it slow down? Why didn’t we know about this? Should we invest into “monitoring?” Now it’s too late. So… when wasn’t too late ? I’ve previously published a blog on a number of wisdoms I’ve learned while working at Nextdoor. One such lesson is in regard to monitoring metrics in production. Having a hand in helping this company grow to millions of users, I know that one of the keystones of our success has been to be data-driven early on. That means having early monitoring and alerting. Frequently, I hear a counter argument for collecting metrics too early saying that the data will be just pure noise. While it’s somewhat true, it’s not a reason to ignore the data. Storing metric data, especially when there is little of it, is very cheap. Many services offer a free tier, or you may want to self-host something like Nagios or Munin in your private network. Advantages of storing metric data from the start is that you always have something to go back to. Even with the data being rare, it becomes very useful in such cases as the anecdote above. But your CEO should never be the alerting mechanism for your metrics — automated monitoring and alerting should be the first signs of systemic issues. Consider these three types of “hard to alert on” data: Always sparse : measuring latency of a rarely-used internal tool. Sporadically sparse : traffic-dependent (e.g. sparse only at night). Early-on sparse : feature popularity hasn’t grown yet. Setting up alerts on metrics in these situations is hard, and can result in bad alerts: non-actionable, too aggressive, or too passive. Bad alerts are evil alerts and should not exist. So what can be done for complex situations with sparse data? In these cases, alerting is hard because mathematical aggregations such as average() do not work well. The concept of data trend vs. data noise loses meaning. Consider a situation where you want to detect a 20% error rate over 5 minutes. In the graph below the green stripe shows the “band of norm” as calculated by historical data . From the figure above it’s clear that the same data point can have a different significance due to other data points. You can envision that adding 1 more sample on the right image would bring the anomaly from 100% down to 50%, and one more — 33%. To have 1 data point represent 20% anomaly within 5 minutes the monitor would have to observe at least 1 data point per minute. Being smart about your data volume is the crux of the solution to alerting on sparse data. Being most familiar with Datadog over other monitoring solutions, I’ll use it as an example. While I am a big fan, I don’t doubt that there are other services that offer a comparable solution. As your product grows, so will the volume of your metrics. At some unknown point in time you will have enough data to no longer be trapped by the aforementioned sparse data gotchas. How do you know when this time arrives? … Monitor it! For the example above, we could monitor for amounts of data exceeding 5 data per 5 minutes to guarantee that an excess of 20% anomaly is not a matter of noise. You will likely have to go back and adjust the actual alarm thresholds once you have a significant amount of data, or better yet — use automatic anomaly detection . Given one monitor for the erroneous data and a separate one for amount of data, you can create a single composite monitor. With such setup you get the best of both worlds: monitoring and alerting early on without the noise of periodic and useless alarms. Let’s consider how this setup would behave in the three circumstances above. Always Sparse : The alerts would be mostly silent. If enough data happens to come in, and the metric is actually in error-state, then the alert will fire. Sporadically Sparse : The alerts will be “enabled” by volume during the high-traffic and “disabled” during low-traffic. Get a good night’s rest even if a few latent measurements happen. Early-on Sparse: The alert will simply sit quietly until the feature gains enough traffic to be significant. This set up guarantees as early enabling as possible. I welcome all feedback and discussions! Feel free to leave a comment and I’ll make sure to get back to you :) If you want to work with data-driven people in a company with a wonderful mission — you should learn more about our job openings at Nextdoor! Nextdoor is the neighborhood hub for trusted connections… 120 Thanks to Vikas Kawadia , Jade Hua , Abhijeet Kumar , and Wesley Moy . Data Science Startup Productivity Tech Programming 120 claps 120 Written by Reverse Engineer Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Reverse Engineer Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-29"},
{"website": "NextDoor", "title": "how design and engineering partner at nextdoor", "author": ["Patrick Dugan"], "link": "https://engblog.nextdoor.com/how-design-and-engineering-partner-at-nextdoor-5b9610f01469", "abstract": "Culture Open Source Technology At Nextdoor our design and engineering teams have worked hard to refine and improve the ways in which we communicate and work together. Shipping new features and products effectively and efficiently is never easy, and at Nextdoor we constantly evaluate the practices we have put in place with the aim of delivering value to our users. While the effectiveness of certain processes design or otherwise are largely dependent on the makeup of your team and the culture within your own organization, here are some of the practices our team has adopted that work particularly well for us. Nextdoor’s engineering teams practice agile software development and our design team recently adopted portions of the agile methodology that we think works well within a design context. In practice, this means that a typical week will involve our entire design team sitting down on Monday morning and reviewing what we accomplished the week prior. As a team, once we understand where individual designers have progressed and may be road-blocked on their respective projects, we can then more effectively map out what we can then work on in the week to come. On Wednesdays and Fridays, we have scheduled critiques where any designer can volunteer to showcase in-progress work and receive structured feedback from the rest of our team. In addition to scheduled planning and critique sessions, our team has also tried to make it as easy as possible to receive feedback from one another in non-structured ways as well. Our team sits together in an open floor plan, we are constantly printing and pinning up our in-progress designs, and we are always exploring new ways to share designs and receive feedback from one another both in person and digitally. Although our design team sits together in an open floor plan, there are occasions where we have found it useful for individual or pairs of designers to temporarily move and co-locate themselves with project teams for short periods of time. We’ve found the practice of working from two desks to be particularly useful early in a project when design directions might be changing quickly and in the run up to a product’s launch. Embedding and sitting with engineers allows us as designers to communicate ideas and concepts far more effectively face-to-face in ways we might struggle to do via channels like email or Slack. Sitting next to the engineers who are translating our designs into real products and features also allows us to ensure that the implementation of our work is being done in ways that will require minimal refactoring or polish down the road. For members of our design team who are more technically inclined, co-locating has also been a great way for us to further build out our own development skill sets. Not all of our designers code, but those of us that do have enjoyed opportunities that allow us to focus on building the presentation layer of the features we are shipping. This also provides the added benefit of freeing up our engineers to focus on more complex tasks which has ultimately helped condense our project timelines and enabled us to ship things faster. While it’s not ideal for every project, co-locating with our engineering teams has in many cases resulted in quicker development cycles and less overall re-work of design implementations. Our team employs a variety of tools in support of the design work we take on. In much the same way that we evaluate the processes through which we produce designs, we also continually assess the effectiveness of the tools we use. A partial list of some of the more common design tools we employ includes: Balsamiq : for creating lower fidelity mockups and wireframes Sketch : for producing designs in a higher fidelity Invision : for mapping linear and dynamic user paths and application flows Pixate : for demonstrating highly specific interactions and motion based design Slack : for sharing in-progress work (and a fair amount of GIFs) Dropbox : for organizing and sharing all of our design resources In many cases, the types of tools we use for a specific project will change based on the needs of the engineering team and the types of ideas we are trying to communicate. While choosing the right design tool for the job is always important, at the end of the day we’re really just making sure that the designs that we end up shipping exceed a minimum quality bar that we’ve set for ourselves. The tools we use are simply a means to that end. If you ask any member of the design team at Nextdoor why they chose to work here, one of the first things you’ll hear is that we all have a strong belief in Nextdoor’s mission of bringing together neighbors. We often say that “when neighbors start talking, good things happen,” and we believe the same can be easily be said of designers and engineers. Our growing team consists of six designers, a studio manager, and a design director. As our team evolves, we will continue to explore ways to improve both our internal processes and the quality of the designs that we ship. If we sound like a team you might enjoy working with, we are currently looking for designers , copywriters , and user researchers who share our passion for helping neighbors build stronger and safer communities. We’d love to hear from you. Nextdoor is the neighborhood hub for trusted connections… 6 1 Design Startup Culture 6 claps 6 1 Written by Design at Chan Zuckerberg. Building Hikearound. Previously Quora, Nextdoor, Google. Washington Husky. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Design at Chan Zuckerberg. Building Hikearound. Previously Quora, Nextdoor, Google. Washington Husky. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-04-27"},
{"website": "NextDoor", "title": "meet the interns spring 2018", "author": ["Jade Hua"], "link": "https://engblog.nextdoor.com/meet-the-interns-spring-2018-2a917871af39", "abstract": "Culture Open Source Technology This spring, we were fortunate to have four awesome interns join the Nextdoor team. Our spring interns typically take a semester off of school to spend their time fully-immersed in our HQ office, learning and developing their skills. Our engineering and product design interns work on high impact projects with the support and guidance of their mentors and teams. As someone who has had the privilege of being a mentor twice, it’s been incredibly rewarding to see the growth our interns experience in their few months here. When mentors and teams are scoping out work for their interns, we prioritize projects that align with their interests and optimize their learning opportunities. Our interns worked on building and shipping meaningful features, from rewriting our public business onboarding flow to creating a new email microservice to upgrading our newsfeed infrastructure. By the end of their four months here, many of us even forget that they were interns — rather we see them as full-time members of the team. In this blog, our engineering and product design interns share, in their own words, their internship experiences and highlights. Studying: Computer Science and Engineering at Ohio State Special Talent: Making buckeye candy! Fun fact — making just 75 buckeyes required over 2 pounds of powdered sugar to be added to the mix. Mentor: Jade Hua Project I was lucky to be a part of the Real Estate team at Nextdoor. For the first half of my internship, my project was rewriting Nextdoor’s business onboarding flow in ReactJS. I had never worked with React before, so this was a huge learning opportunity for me. Going into the second half of my internship, Nextdoor gave me several choices for what to work on next and as a result of that, I feel really fortunate to have worked for a company that prioritized my learning and the skills I wanted to develop. I decided to delve more into backend work with my second project and took on the task of building an onboarding flow from start to finish for hundreds of thousands of real estate agents. As part of a real estate brokerage partnership that Nextdoor signed, we needed to onboard their real estate agents to our platform. This project allowed me to own every part of the development cycle, from writing a technical spec, to frontend work, to data analysis, to backend work. Finally, throughout the course of my internship, I helped with several smaller tasks like adding third party tracking pixels throughout several of our pages and bug-squashing. What I learned I developed a large number of technical skills and experience with developer tools while at Nextdoor. When I came to Nextdoor in January, I mainly had experience working on frontend code. Throughout my projects at Nextdoor, I touched everything from frontend code (learning ReactJS and rewriting an onboarding flow into React) to backend code (adding to several internal API’s as well as writing my own API for my second project) to databases (learning to use SQL for data analysis). I also learned good programming practices, design patterns, and workflows. Favorite Memory My favorite memory at Nextdoor was the bike lunches with the real estate team. When the weather was nice outside, we would rent bikes to get lunch and enjoy the gorgeous SF weather. Studying: Electrical Engineering and Computer Science at UC Berkeley Special Talent: Rapping about Nextdoor Mentor: Aaron Webber Project I was a part of the Infrastructure team during my internship and mostly focused on creating a new microservice to handle the delivery of all of Nextdoor’s emails. The service (which we called Malone, as a reference to Karl Malone “The Mailman”) allowed us to cut down on almost 50% of the workload from the previous taskworker-based approach, freeing up the taskworkers to execute other tasks with lower latency. Some of the core features of my project included: Creating a server component capable of accepting email send requests and enqueuing formatted email templates in Amazon SQS and S3 Creating a consumer component to receive messages from SQS and deliver the final email to Sendgrid, our email provider Hosting the server and consumer as containers within ECS, providing more control over how we scale them throughout the day Implementing a smarter idempotent API using Redis to prevent duplicate email sends atomically Leveraging Go’s concurrency support to distribute workload amongst hundreds of goroutines per ECS task, maximizing our throughput in each task and making better use of our memory allocation Compressing our final requests to Sendgrid, allowing us to reduce outbound bandwidth by 90% and significantly reduce AWS costs What I learned This was my second internship at Nextdoor (I interned previously last summer on the Real Estate team). When I came out of the first one, I thought I had learned everything about software development, and I expected to have a relatively small learning curve when I came back. Instead, I realized just how vast Nextdoor’s codebase was and how much potential there was for growth just by moving from one side of the office to another. As someone who had the opportunity to work at Nextdoor for over six months in total, I think that most of my knowledge of engineering has been shaped by my experiences here — this was the place where I first learned basic HTML, and by the end, I was being exposed to some of the most complex parts of the codebase. Most of the credit for that goes to all the amazing mentorship I’ve received here — Aaron Webber and the rest of the Infra team (as well as my previous mentor Jade Hua) went out of their way to give me a good learning experience, in addition to some big responsibilities, and I’m very grateful for the time they invested in shaping my career. Favorite Memory Getting handed a Karl Malone shirt from our CTO Prakash the week I deployed my service Malone to our staging environment — it was amazing to see how much the rest of the team was looking forward to my project launching! Studying: Computer Science at University of Maryland, College Park Special Talent: Typing 140 WPM Mentor: Denise Szeto Project I spent a big chunk of my internship implementing ads pagination. This allows us to show ads/promos on every page, instead of just the first one. It was an incredibly fun experience, and gave me the opportunity to work with one of our most important products at Nextdoor: the newsfeed. During this project, I was able to: Evaluate different ways of implementing pagination Design and write the object that carried all pagination data Refactor the newsfeed and promos fetching code on mobile to accommodate for the object Modify ads selection code to enforce competitive exclusion Add serialization and deserialization logic as well as comprehensive safety nets to prevent anything from pagination breaking the feed In addition to my project, I worked on a few other parts of our Ads platform. I built or helped build: A tool to monitor and detect ad rendering failures, abnormally high/low ad impressions, abnormally high/low ads clickthrough rates, etc. An ads data populator script that generates ads data for new engineers’ local environments Ads manager changes, such as suggested CTAs for ads and handling of 4-digit zip codes A tool to force multiple specific promos in a person’s feed, designed to be used for testing purposes. What I learned Where do I begin? I learned a lot of Python, since most of my work was in our Django codebase. I also learned some React. I learned to ask questions relentlessly, because otherwise I’d be blocked for days. I learned to bounce ideas off of my mentor(s) to help me think. I learned to write better, more airtight code. I learned to communicate progress, deal with criticism, and organize my thoughts when tackling enormous code changes. I learned about interesting design patterns. I also learned about poké and how to do dips! Favorite Memory The biggest highlight of my internship was deploying my project to production. I started in the afternoon and had to make three emergency fixes before it was finally deployed at around 6PM. After the first or second fix I found myself in a booth with the rest of the interns around me, waiting for me to finish so we could go out. It was intense — after the third fix was deployed, I made all the other interns check to make sure I hadn’t broken ads for everyone. It was a pretty intense few hours, but it was a great way to finish off my project. Bonus memory: Two weeks before my last week, a couple of interns/full-time employees and I went to trivia — and won. Studying: Systems Design Engineering at University of Waterloo Special Talent: I wanted to show off this cool (though some may disagree) eye trick that I had planned out, except I didn’t realize I’d squeeze the contacts right out of my eyes. So instead, I showed a video of me playing Gu Zheng, which I like to call “the Asian harp”. Maybe next time… Mentor: Lizzy Gregory Project As an intern on the Design Team, my projects were very spread out: Real Estate — My primary project team! I helped design features for both the member and business-facing product experiences. Post flow — This was the first and last project I touched at Nextdoor. I explored a variety of ideas to revamp our current post flow. I also participated in executing user tests! Neighborhood Favorites — I helped design the window sticker, postcard, and business winners claim website Interests — Made emojis for some of the newly-added interests CCC — Exploratory design sessions with the Design Team What I learned Bond with your team and also those outside your team. Time flies by FAST as an intern. Initiate your own coffee (or Project Juice) chats if that’s what it takes! Prioritize prioritize prioritize. This was the first time I’ve been pulled onto multiple projects teams at once. I really learned to multitask and communicate with my different teams to make sure I worked on what was most important and urgent first. Social media is a complex space. You would think that being a user of social media makes it easier to design for it — that’s not true! Favorite memory: Hands down it would be running around the office on my very last day for our Wintern “photoshoot”. The three photos we wanted to get in were in the pods/booths, the conference rooms, and the Nextdoor frame (a must-take photo for every intern– see below). Thank you to our interns and their mentors for a productive and inspiring few months. If you’re interested in checking out Nextdoor’s internship program or full-time opportunities, we’re hiring :) Nextdoor is the neighborhood hub for trusted connections… 133 Internships Nextdoor Engineering Design 133 claps 133 Written by Product @ Affirm | Cal Bear | Human Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Product @ Affirm | Cal Bear | Human Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-15"},
{"website": "NextDoor", "title": "share your knowledge", "author": ["Daisuke Fujiwara"], "link": "https://engblog.nextdoor.com/share-your-knowledge-98119b4f7c4b", "abstract": "Culture Open Source Technology A forum to promote continuous learning and knowledge sharing. At Nextdoor, we strive to encourage continuous learning among engineers. Even though we are busy committing code at a fast pace on a daily basis, we consciously take time to learn new technologies and engineering concepts. The “Share Your Knowledge” sessions started when a few of our mobile developers decided to create a space to discuss new learnings on a biweekly basis. This was extremely important for the mobile developers as mobile platforms were and still are evolving at a fast pace every year. After piloting this idea for more than a year, we discovered that the mobile engineers appreciated these sessions as an effective tool for knowledge sharing. As we developed “Share Your Knowledge”, our engineering team grew to include many teams working on various focus areas, such as infrastructure and geospatial. We decided to expand the concept to the entire engineering team, because we saw that “Share Your Knowledge” could be a great way to keep the entire team up to date with expertise developing in each area. Today, “Share Your Knowledge” covers all aspects of software engineering including: Caching best practices in Django Image service written in golang Client side event tracking system for web and mobile platforms Load testing infrastructure using Gatling iOS functional testing with KIF Site performance monitoring using New Relic How to be a Jira power user The format of “Share Your Knowledge” is specifically designed to be simple and practical. We assign a couple of presenters beforehand, and they select a particular topic that they want to discuss for 5–10 minutes. They are encouraged to present prototypes, demos, or live coding. The time constraint also forces the presenters to identify key discussion points, and they have the freedom to choose their topics as long as it is related to the theme of the session. At Nextdoor, the topics that have been chosen frequently have nothing to do with presenters’ current work. We recently hosted the first external “Share Your Knowledge” session with mobile developers in the community. We were excited to see so many participants, and many of them expressed interest in attending future sessions. We had five speakers from the following companies present exciting topics for both iOS and Android: Nextdoor Citizen Code Medium Apartment List Yelp Here are the materials from the event. Video & Slides Mobile Share Your Knowledge from Nextdoor on Vimeo . From now on, we will be sharing our learnings on this blog. So keep an eye out for more entries related to “Share Your Knowledge”! We also encourage you to try “Share Your Knowledge” at your company. We would love to hear about your experiences in the comments. If this sounds like the place for you, we are hiring! Nextdoor is the neighborhood hub for trusted connections… 51 Startup Culture Nextdoor 51 claps 51 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-02-22"},
{"website": "NextDoor", "title": "maps on android", "author": ["Mitali Gala"], "link": "https://engblog.nextdoor.com/maps-on-android-d299e89f28c1", "abstract": "Culture Open Source Technology As Android developers at Nextdoor, we spend a lot of time and care on maps as they are central to the product. The documentation Google provides for their maps API looked straightforward and the implementation details looked fairly easy the first time we read it, and for the most part it was. However, we ran into some issues when we tried to use Markers , InfoWindow and Polygons . Here are some of the hurdles we needed to clear to make our maps work just right. Markers are points on maps that are associated with a certain location. They are usually paired with an InfoWindow that provides a description of the location. Markers, however, don’t hold any additional data other than a title, description and an icon. In addition to showing a title and description on the InfoWindow, we also needed to show a photo. This was the end result we wanted to achieve: Using MarkerOptions, we could easily add a title and description. The icon poses more of a problem: the marker doesn’t accept a URL. Markers are also final classes, so they can’t be extended to contain a custom data structure. We solved this in two steps. First, we wrote a custom InfoWindowAdapter to render a layout of our choosing. Next, we passed a HashMap that maintains a relationship between a marker ID and a photo URL into it. When the adapter renders the InfoWindow, it looks up the HashMap to find the photo URL associated with the marker ID and fetches the photo. InfoWindows are rastors — they don’t get updated once they’ve been rendered. When we needed to show an asynchronously loaded photo in the InfoWindow, it wouldn’t appear. By then, the InfoWindow was already rendered and wouldn’t automatically redraw. We worked around this limitation by attaching a callback to the photo fetching call. The callback would in turn call showInfoWindow() which redraws the InfoWindow. We wanted to show more than one type of InfoWindow on the same map. However, a map can only accept one custom InfoWindowAdapter. We overcame this limitation by putting all the logic for various types of InfoWindows into one class. We then separated the markers into buckets and passed them to the adapter. There, the adapter would look up the marker ID to implement the different behaviors. The Google Maps API doesn’t provide an interface for attaching click handlers on Polygons or showing an InfoWindow on a Polygon. We needed to react to taps on polygons by showing an InfoWindow, but there was no straightforward way of doing this. We worked around this in several steps: We overrode the map click handler provided by Google Maps to accept a custom polygon click handler. We implemented a point-in-plot algorithm to detect taps within a polygon. We added an invisible marker to the polygon that represents the entire polygon. This marker is used to show the associated InfoWindow. When put together, the result is PolygonActivity . To explore the solutions we found for these problems, check out the MapDemo GitHub repo . The Google Maps API makes implementing basic map functionality very easy. At Nextdoor, we engineered more flexible and powerful maps to better serve our neighbors. If you’re interested in working on these kinds of problems, we’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 3 Android Google Maps Open Source Apps 3 claps 3 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "our ios ci cd journey at nextdoor", "author": ["Su Khai Koh"], "link": "https://engblog.nextdoor.com/our-ios-ci-cd-journey-at-nextdoor-2456cfdca82b", "abstract": "Culture Open Source Technology Two of the most useful tools for making software development run smoothly are CI & CD. Over the years our stack has evolved from a humble Jenkins box into a cloud-native platform. In this post we’ll share our learnings from this journey. We started off our iOS CI/CD using on-premise infrastructure, running Macs with Jenkins. The build performance was great because all the builds were running on barebone machines, and we had full control over these build instances. However, this implementation had its limitations: Maintenance: Keeping everything up to date from Jenkins to the machine itself was a huge overhead for us. Engineers spent hours fixing issues like networking and hardware failures. Unstable builds: Due to the lack of isolation between builds, the cache from an old build frequently made a new build fail. Small, unpredictable changes like network failures caused a lot of instability. Lack of elasticity: Because of the limited number of Macs we had, our queues grew larger and larger as we increased the number of features on our app and the engineers on our team. We had a hard time spinning up new machines. Unable to remotely access the build servers: Setting up remote access to the build servers was a challenge for us. The workarounds we made to enable remote SSH were fragile and could have become security liabilities. Given all the challenges described above, we decided to get our iOS CI/CD builds into a cloud solution. In addition to that, we also set the following criteria: Leverage the cloud to achieve elasticity. Reduce the cost of creating new instances by removing the maintenance burden of having our own infrastructure. Make builds stable. Improve the user interface so workflows are easy to pick up and understand by other engineers. Create the ability to test any new Xcode beta version shortly after release. Create the ability to cache build data per branch to speed up builds. Enable remote access into the build servers. Simplify code signing management . We considered a few services like CircleCI, Bitrise, TravisCI, and GitHub Actions and selected Bitrise because they were able to fulfill our success criteria. With any technical decision, there are no perfect solutions so it is important to establish what compromises you are willing to make. In our case, migrating from an on-prem Macs-Jenkins infrastructure to a cloud base solution was one of those compromises. For us, having slightly longer build times is a tradeoff we are happy to make in order to get more stable builds. Here is a benchmark comparing the build times of two of our CI jobs before and after the migration: From the table above it is clear that the cloud build times take longer but that is offset by the faster queue times. Not a bad tradeoff in order to increase build stability and reduce maintenance costs! Create Shareable Workflows Be sure to create utility workflows or normal workflows that can be easily added to your primary workflows (basically, plug-and-play). You can configure these workflows through environment variables. For example, here we have 3 utility workflows _swiftlint , _build_ipa , and _deploy_to_firebase . These 3 utility workflows now can be plugged into primary and rc workflows. This allows both primary and rc workflows to produce different IPAs and deploy to different Firebase App Distribution channels without having to edit them at all. Let’s look into how _deploy_to_firebase looks like: This syntax allows us to use different environment variables for each workflow, as you can see on the screenshots below: 2. Use Remote Access With Screen Sharing Remote access with Screen Sharing is very useful when troubleshooting a build issue. This is especially useful when we want to troubleshoot issues like validating certificates. Having access to the Keychain with UI is much easier than accessing it through CLI because we don’t need to know all the commands, just the familiar user interface with Mac is enough. We’ve been running our new cloud iOS CI/CD for several months now, and we’re very happy with the results. Instead of spending hours troubleshooting CI/CD, our engineers can now focus on making our products better for our customers. Let us know if you have any questions or comments about our cloud CI/CD with Bitrise. And, if you’re interested in solving challenging problems like this, come join us! Visit our career page to learn more! Special thanks to the Client Platform team, Developer Experience team, and iOS engineers who have helped in this fun journey. Nextdoor is the neighborhood hub for trusted connections… 78 iOS Nextdoor Bitrise Continuous Integration Continuous Delivery 78 claps 78 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-17"},
{"website": "NextDoor", "title": "what is the technology behind nextdoor", "author": ["Prakash Janakiraman"], "link": "https://engblog.nextdoor.com/what-is-the-technology-behind-nextdoor-5b74b3efcc31", "abstract": "Culture Open Source Technology (This post is adapted from the answer of this Quora question: What is the technology behind Nextdoor? ) Some behind-the-scenes insight into the technology behind Nextdoor. We primarily use Python as our language of choice, both for our serving stack and for our offline processes. More recently, we have also started to build backend services in the Go programming language . We make use of the Django framework for our web applications, and use the Jinja2 language for rendering. We use nginx and uWSGI on the web server side, load balanced behind an Amazon Elastic Load Balancer . Our primary data store is a PostgreSQL database, which we replicate for read capacity. We make use of Pgbouncer to pool connections to our databases, and use a custom Django DB backend to maintain persistent connections from our application to the pooler. Like most of the Internet, we make heavy use of Memcached to help with performance, and also use an Amazon CloudFront CDN to edge-cache our static content. Our web application is written to the HTML5 standard, using LESS for styling, and Bootstrap and jQuery for interactivity and responsive design for mobile clients. We’ve also created native mobile applications for both iOS and Android. We are entirely hosted on Amazon Web Services , and make use of RightScale for systems management and Puppet to automate our systems setup. We have built a series of custom plugins hosted on GitHub to help manage our services via Apache ZooKeeper . We are in transition towards a service oriented architecture, beginning with a Tornado -based service to handle generating our neighborhood News Feeds. The service makes use of Apache Thrift for all RPC and is massively parallelized. We are also beginning to develop additional backend services in Go, which we have found to be a compact and elegant language for developing highly concurrent applications. We were previously using Celery backed by RabbitMQ to process millions of daily jobs asynchronously. For example, when new content is submitted, new neighborhood boundaries are created, or caches need to be invalidated. However, we found Celery to be overly complicated and very unstable, often encountering deadlocks and hung workers. As a result, we decided to write our own system from scratch: a Python-based distributed job processor (which we’ve simply called “ Taskworker ”), which consumes tasks published to Amazon SQS queues. We also built a Scheduler system on top of Taskworker to replace Cron for time-based jobs. Additionally, we make use of Amazon’s DynamoDB service for some ephemeral data, which we use across this distributed set of asynchronous work processes. Examples include sending emails and push notifications to our users at very high throughput. Given that we work with a fairly significant amount of location data (goal: every residential housing unit in the world!), we have designed data services atop our storage layer for fast and efficient access to the information contained within. Our PostgreSQL database makes heavy use of the excellent PostGIS extension for spatial operations. We use libraries like GDAL and GEOS for spatial algorithms and abstractions, and tools like Mapnik and the Google Maps API to render map data. We are currently in the process of developing a brand new data store and custom processing pipeline to manage the high volume of geospatial data we expect to store (1B+ rows) as we internationalize. This includes storage, serving, and updating of the data at very high volume. We collect and analyze data from our systems using a combination of in-house and third-party systems. We collect events from our various systems via a Flume pipeline that writes data out to Amazon S3 . From there, we use a data processing pipeline hosted by Qubole to process and aggregate statistics to Apache Hive tables and to an Amazon Redshift based data warehouse. For easy access to the data for the entire company, we use Tableau to navigate through our tables and produce visualizations. We’re hiring across the board! https://nextdoor.com/jobs/ Prakash Janakiraman Co-Founder and Chief Architect at Nextdoor Nextdoor is the neighborhood hub for trusted connections… 146 1 Startup Django Technology 146 claps 146 1 Written by Co-Founder of @Nextdoor. Former @GoogleMaps engineering manager. @Warriors season ticket holder. Hip-hop head. Friend to one and all. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Co-Founder of @Nextdoor. Former @GoogleMaps engineering manager. @Warriors season ticket holder. Hip-hop head. Friend to one and all. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "meet the interns summer 2020", "author": ["Gonzalo Maldonado"], "link": "https://engblog.nextdoor.com/meet-the-interns-summer-2020-11dd47dbc6c3", "abstract": "Culture Open Source Technology This summer was unlike any other. With the pandemic, there was uncertainty about whether our internship program would be possible. After some major work across multiple teams, we were beyond excited to (virtually) welcome 11 summer interns to the neighborhood! Typically, our summer internship allows our interns to spend their summer fully immersed with their teams at our Nextdoor HQ in San Francisco, learning and developing key skills that will (hopefully!) set them up for success over the course of their careers. However, this year, things were a little different and we needed to get creative. Although virtual, with support and guidance from our wonderful mentors and the flexibility and great attitudes of our interns, we had a fun and unforgettable summer. Our interns play a crucial role on their teams and are involved in projects that create real impact, some shipping out their first piece of code within their first two weeks. We work closely with each intern to define their areas of interest and match them with a team that will align well with their career goals. Along with their projects, our interns get 1:1 mentorship, participate in a variety of fun summer events, and meet with each member of our executive team. Interns meeting with co-founder, Prakash Janakiraman Our goal is that each of our interns leaves with more than just some branded swag after the summer. We hope that they’ll sharpen their technical skills, gain real-world experience, develop personally and professionally, and ultimately, leave with fond memories of each other and the Nextdoor team. Despite the virtual nature of this summer, we’re confident that we achieved this. And with that, let’s get to know our Summer 2020 Interns! Why did you choose to intern at Nextdoor? “Right from the first interview, I was really struck by the honesty and compassion with which everyone spoke. You got a real sense that everyone here wants to improve the product to facilitate a kinder, more equitable digital space. Everyone I met in the process was kind, wicked-smart, and a great designer. I also knew that there was a real opportunity at ownership, even as an intern.” Chanel Kim, Product Design Intern — Design Team “Coming into this recruiting cycle, I was looking forward to working at a high growth startup with an amazing engineering bar. While interviewing for Nextdoor I had a really great time, and I decided that this experience was indicative of a phenomenal culture that this company has carefully cultivated. I could not have been more correct, and have thoroughly enjoyed meeting such passionate people who are trying to make positive changes within communities!” Ruban Rengaraju, Software Engineering Intern — Infrastructure Team “Nextdoor’s mission and purpose resonate with me. I really value community and kindness, and it’s amazing to work for a company that places these two values at the forefront. I love how Nextdoor not only prioritizes these values in their product but also in its own community.” Iris Chow, Software Engineering Intern — Feed Infrastructure Team In a few sentences, describe the main project you worked on this summer. “My main focus for this summer was reworking the notification center for the web platform. I have implemented a visual redesign and separated notifications into sections on a time basis so users can more easily view their recent notifications. A new feature I have been working on will allow users to receive notifications in real-time, which I am really excited about!” Angela Pan, Software Engineering Intern — ME Retention Team “To keep our members safe on our platform, I built a browser redirect service that decreases exposure to potential fraud and misinformation.” Bilal Lafta, Software Engineering Intern — Neighborhood Vitality; Trust & Safety Team “I worked on content understanding for the content ranking team so we can understand our content data more comprehensively and deeply. We explored using embeddings to represent texts and capture the meanings. I compared performances of different state-of-the-art embedding methods by benchmarking on different datasets and models as well as analyzing their latencies. I also got the chance to deploy some models to production and performed testing on them.” Sabrina Luo, Software Engineering Intern — Member Experience; Content Ranking Team What was the best part about your internship at Nextdoor? “Nextdoor opened my eyes to the possible positive impact I can have with my career. I loved meeting so many people that are passionate about making the world a better place, and it was inspiring to learn how I can use software engineering to contribute towards a cause I truly believe in. While I learned so many technical skills, the most important thing I’ll take away is the life lessons and advice I gained from other Nextdoor employees!” Cidney Wang, Software Engineering Intern — Local Identity Team “The Meet the Leads (Nextdoor’s leadership team) sessions where the interns got to hear everybody’s career stories were really insightful! It was interesting to listen to how they ended up joining Nextdoor and it was a great opportunity to learn how Nextdoor functions as a company at all levels.” Danny Lee, Software Engineering Intern — Local Discovery Team What impact did you feel like you had on Nextdoor? “I was able to immediately work on delivering a feature for our ML training and serving client. From there, my team trusted me with building out performance monitoring from the ground up, and I spoke with various model owners to gather feedback and tailor the service to their needs. This will provide visibility into how our ML models in production are performing over time and whether they need to be retrained.” Richard Huang, Software Engineering Intern — Core ML Team “I wrote a SPICE (our decision-making framework) to provide detailed explanations and context to the project I worked on during my internship. By sharing this SPICE, I was able to educate a significant portion of the company on gender identity, gender expression, gender variation, and the importance of pronouns.” Ari Fromm, Software Engineering Intern — Neighborhood Vitality; Connections Team “I hope I was able to bring an interesting perspective to the company! It was really cool working on a team dedicated to driving growth and I am excited about the work I did contribute to the efforts of the Growth team.” Maansi Manchanda, Software Engineering Intern — Acquisition & Activation Team As our summer internship program came to an end, we hosted an Intern Showcase where our interns presented their projects to the entire company. Like years past, it was filled with great ideas, thoughtful questions, and lots of smiles. It was the perfect way to end our remote internship program. Join us in applauding our wonderful interns for their contributions this summer and for sharing their personal experiences! If you’re interested in our internship program or full-time opportunities, be sure to check out our careers page for more information! Nextdoor is the neighborhood hub for trusted connections… 1 Internships Software Internship 1 clap 1 Written by Señor Engineer at @Nextdoor. Scaling teams to Scale apps. @Yammer, @YouTube alumni. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Señor Engineer at @Nextdoor. Scaling teams to Scale apps. @Yammer, @YouTube alumni. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-09-17"},
{"website": "NextDoor", "title": "r3 2019 48 hours to reflect refine and reinvent nextdoor", "author": ["Gonzalo Maldonado"], "link": "https://engblog.nextdoor.com/r3-2019-48-hours-to-reflect-refine-and-reinvent-nextdoor-eb9e6983ac1b", "abstract": "Culture Open Source Technology R3 is an annual event to Reflect, Refine and Reinvent Nextdoor. R3 goes beyond a hackathon by being more of an idea-thon . Everyone in the company, from engineers to sales to Neighborhood Operations, gets together to reimagine our product. We asked some folks why R3 is important to Nextdoor: “R3 provides the opportunity for everyone to step out of their comfort zones and try building things out of their day to day work.” “R3 gives you the opportunity to work with people outside of your team and get together on a similar idea.” “R3 is a chance for people to showcase their creativity purely for creativity’s sake.” “We’re a really mission-driven company that attracts folks who are passionate about civic and community connection, so I think it’s really important to have a breeding ground for new ideas that are entirely grassroots and cross-disciplinary.” This year we had 23 teams come together, reflect, reimagine, and compete for four prizes based on our company’s core values . For R3 everyone was encouraged to suggest ideas and form teams of up to 5 people. For 2 days, almost all work in the office stopped as people brainstormed and ideated their way to the final product: a 3-minute video that would be shown to the entire company (and our lovely panel of judges) on the final day. Teams were encouraged to explore areas of the product outside of their day-to-day work and collaborate with people they may never have worked with before, which led to some truly fun teams and videos. One of the teams tackled a very personal problem for some of the folks at HQ: ping pong tables. “How often do you walk all the way to the ping pong table only to find out that it’s already in use?” their video asked. By connecting a Raspberry Pi to an accelerometer, the team was able to create a website that would tell you when the table was in use. Another team looked at the relationship between landlords and tenants in big buildings. They created a small fake “Message landlord” button on the app, allowing neighbors to ping their landlord when something went wrong. For example, when you get stuck in the building’s elevator! “Our team experimented with designing a space where our members build community by helping people better understand their neighbors’ perspectives” “We reimagined how people can tap into the resources of their community” “We completely reimagined the look and feel of our app and figured out ways to think about what comes next” The judges had a lot of excellent projects to choose from, which means we had to pick the coolest possible judges for the job. On our panel was: Prakash Janakiraman , Cofounder and Chief Architect Lauren Nemeth : Chief Revenue Officer Craig Lisowski : Head of Data, Information, Systems, and Trust The criteria for evaluating our projects was based on our core values : Earn Trust Everyday Invest in Community Customer Obsessed Think Big Experiment and learn quickly Act like an owner Our judges simply selected the projects that they thought reflected these values the most! The goal of R3 is to create a space for radical innovation and experimentation. While not all of the winner’s projects will become Nextdoor features, some of our users favorite features are direct products from previous R3s. Check out these examples of features that came directly from R3 events: Using the Pet Directory , neighbors can post photos and include descriptions and identifying features of their pets, building a comprehensive hub for the community to get to know other pet owners, and, in times of need, be a valuable resource for when a pet is lost or has gone missing. Our annual Treat Map is the go-to guide for who in the neighborhood plans to celebrate Halloween either by passing out treats or hosting a haunted house. On the hunt for sweets? Keep an eye out for the homes that are marked with a candy corn. Eager for the scare of your life? Visit the homes marked with a haunted house to get your share of scare this Halloween. Neighbors can also mark their homes with a teal pumpkin to indicate they are passing out non-food treats for those who may have allergies. We’re excited to see even more folks across the company participate! Everyone had an amazing time, so much that we received many requests to extend R3 from 2 days to 3 days! Does this sound interesting to you? We’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 99 1 Startup Hackathons Design Thinking Engineering 99 claps 99 1 Written by Señor Engineer at @Nextdoor. Scaling teams to Scale apps. @Yammer, @YouTube alumni. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Señor Engineer at @Nextdoor. Scaling teams to Scale apps. @Yammer, @YouTube alumni. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-28"},
{"website": "NextDoor", "title": "the kindness reminder", "author": ["Daniel Masquelier"], "link": "https://engblog.nextdoor.com/the-kindness-reminder-868252995140", "abstract": "Culture Open Source Technology Neighbors in 11 countries communicate on Nextdoor. When a comment is reported, volunteer moderators (Nextdoor Leads) vote democratically to determine whether the communication was unkind. As a result, Nextdoor has accumulated examples of communication that ultimately was deemed uncivil by local neighbors. Using this example data, we built a machine learning model to predict when a comment may be deemed potentially offensive or hurtful. The process of building a machine learning model involves several steps: 1) collect data about the thing you want to predict, 2) experiment with different model architectures to determine which model configurations perform the best, 3) deploy the model into the product, and 4) monitor its performance to make sure it doesn’t degrade. We worked with activists, academics, and experts like Dr. Jennifer Eberhardt to create experimental user interfaces to get people to slow down and think about the words that they use when interacting with others. All this led to the Kindness Reminder user interface that is triggered by a machine learning model that interjects during a potentially unkind communication. Preparing the data is usually the most difficult part of creating a model. To accumulate an “unkind” dataset, we fetched data month by month for 2 years, collecting the comments that were the most reported (voted by neighbors), then collecting the reviews of those reports during that interval with an extra month grace period. Subsequently, we collected a “kind” dataset that had zero reports. First things first, we anonymized the data by making replacements for emails, phone numbers, addresses, etc. We then ensured that no single user or region dominated the dataset and filtered out the reports made by highly reported users themselves. We assigned toxicity scores to each comment based on 50 different parameters, which became our machine learning model features. We created scores for features like “thanks” that are positive feedback about the comment, subtracting a small value from the toxicity score for each thank. Inversely, we added to the score for reports of incivility weighted by subcategory. Looking at these scores, the minimum was -221 for the nicest comment we have ever seen, and the maximum was 1425, which was objectively unneighborly. This scoring heuristic showed that the 75th percentile of the scores was at 10 points. A score of 10 and above turned out to be 99.9% unkind content via our manual evaluation and this felt like a strong breakpoint for a binary classifier. We combined the two datasets into one and created unigram, bigram and trigram features with a term frequency/inverse document frequency (TF-IDF) vectorizer. This guarantees words and phrases that appear more frequently like “the” or “and” have less weight in comparison to words that appear less frequently like “hate” to have greater weight. Furthermore, we sourced toxic words and phrases from open source databases, blog articles, and pull-quotes from the most-reported, most-reviewed comments. We used them to add boolean features about whether comments contained common profanities, insults, etc. We trained the model using logistic regression because it was easier to implement and performed well on the linearly-separable aggregate scores that we created for the dataset earlier on. There are no hidden layers like neural networks, so it is trivial to reverse engineer, should we discover any peculiar biases by aligning the model coefficients to the original TF-IDF feature names. The base here is a logit function that calculates the logarithm of the odds ratio for the “kind” text probability (p) , so logit(p) = log(p/(1-p)) . That logit function handles values between 0 (kind) and 1 (unkind) and transforms them into a real number range. This reveals a linear relationship between the features we’ve accumulated and the logarithm odds. If y=1 is our unkind label, p(y=1|x) will give us the probability that x is unkind. The inverse form of our logit function, the logistic or sigmoid function, actually helps us check if a sample is unkind or not, so φ(z)=1/1+e^-z . This takes real numbers and transforms them into values between 0 and 1. We use this sigmoid function as our activation function during a fit. We then fit the model with our data to the aggregate scores. We evaluated that model using the training set and found ourselves looking at usable precision, recall, F1, ROC, and AUC scores. We then deployed the model to our machine learning microservice, Adroit , and called that service from the main application just before the comment object is created. This model now takes in bodies of text and predicts the probability that this text violates guidelines in milliseconds. We did extensive internal testing within Nextdoor, using our private groups feature before externally running small split tests. We found probability thresholds with minimal false positives and observed that it would trigger the reminder on double the amount of content that was naturally being reported by members each day. The model has some confusion problems, which is to be expected given diverse communities disagree on different things. Fortunately, the model shows that there are some unkind things that we universally agree on: Racism, racial profiling, public shaming, threats, and excessive profanity. You can read more about the experiment we conducted here . Our first version split tests gave us a 20% reduction in uncivil comment reports. In our limited beta testing in communities around the US, we’ve already seen Kindness Reminder motivate the many it reached to rethink what they post, with a drop in reports of incivility. With this baseline model, we plan to experiment with more sophisticated recurrent neural networks and multi-lingual word embeddings to make sure we can help defend civility everywhere in the world for hundreds of languages. Leveraging all of our reported content to build models that work equally well in all languages is important, especially as we launch in new countries. If you’re interested in working on this, we’re hiring ! We encourage everyone to be kind to each other, online and offline. More kindness among us could someday mean that we make this feature obsolete. We will be glad when that day comes! But in the meantime, if Kindness Reminder will make neighbors a little more understanding and allow more constructive conversations, we will all be better for it. Nextdoor is the neighborhood hub for trusted connections… 209 1 Machine Learning Design Social Media Civil Society Software Development 209 claps 209 1 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-16"},
{"website": "NextDoor", "title": "our product development principles", "author": ["Daniel Clancy"], "link": "https://engblog.nextdoor.com/our-product-development-principles-f3854692bb60", "abstract": "Culture Open Source Technology At Nextdoor, we build products that help create stronger, safer neighborhoods all over the world. From listening to our members, to designing new features and engineering the best solutions, our product development process requires constant teamwork and collaboration. As our service continues to grow, working together efficiently to build great experiences for our members is an increasing challenge. That’s why our Engineering, Product, Design, and Data teams sat down to identify seven Product Development Principles to guide the efforts as a team. Each principle highlights an aspect of our company culture that applies directly to engineering and product development. With these guidelines in writing, we have a clear way to assess how our process is creating value for members while making progress to achieve Nextdoor’s mission. We place responsibility on our peers to hold us accountable and help us make better decisions. This is in contrast to a more traditional, hierarchical feedback structure. Our peer-based culture relies on hiring talented people who love to collaborate — no assholes. Great collaborators are empathetic to the needs of their teammates and make everyone better. The fundamental building unit at Nextdoor is the project team. Project teams are designed to be small, which means that each team member can feel their impact. To minimize dependencies and move as fast as possible, our teams are composed of people with all the necessary skills to complete the project. Collective ownership gives everyone both the right and the responsibility to work on all parts of the product and codebase. This removes bottlenecks to making progress, encourages new perspectives, and prevents knowledge silos. When we encounter something that’s broken or should be better, we take action. Effective teams make clear decisions in a timely fashion. The right people should participate in the decision-making process, but we don’t always need to reach consensus to make a decision. Once decisions are made, we all need to commit to them — even if we disagree. We stick to our decisions until we have clear evidence that we should change course. To build a great product and maintain our platform as we scale, we need to do the dirty work. It means shipping polished UI, refactoring code and cleaning up technical debt, and removing noisy monitors while we’re On Call. All the little details matter. Doing the dirty work is not glamorous, but it’s critical to our success. We seek the shortest path to learn what we should build for our members. Once we understand what resonates, we can start to invest and refine. Overbuilding early on is tempting, but it can ultimately slow us down. As soon as we test and validate the member need, it’s important that we invest in building a long-term, scalable solution for our platform. Building a platform used by every neighbor in the world requires a systematic approach to solving problems. Platform thinking means looking for general, scalable solutions when faced with specific problems. Although they come with risks and may not have an immediate payoff, we invest in projects with long-term horizons. Nextdoor is the neighborhood hub for trusted connections… 102 Startup Culture 102 claps 102 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-06"},
{"website": "NextDoor", "title": "meet the interns fall 2018", "author": ["Mackenzie Dolginow"], "link": "https://engblog.nextdoor.com/meet-the-interns-fall-2018-2d779b5c2f44", "abstract": "Culture Open Source Technology Every summer and fall at Nextdoor, we have a new class of interns that join us. They work on teams according to their interests, experience, prior knowledge, and how they want to grow and build out their skills. As full-fledged members of the team, they contribute valuable features during their three-month stint. Each intern is paired with a mentor who assists by helping them set up their dev environment, pairing on tasks, or scoping work and projects. This fall, projects ranged from making a Go microservice that makes our data pipeline more robust, to implementing the highly-desired @mentions feature. Interns also worked on projects that varied from implementing new ads features to working across the stack to make valuable contributions to the Interests product. Check out some of the amazing things our fall 2018 interns contributed to and what they had to say about their experience at Nextdoor! Studying: Computer Science at University of Waterloo Special Talent: SSBM (Super Smash Bros. Melee Video Game) Mentor: Mackenzie Dolginow Project : Going into the internship, I had the goal of gaining a breadth of knowledge in areas I had less experience with. I was lucky to be on the Interests team. Everyone was really open to me grabbing whichever tasks interested me the most. Some of my favorite projects I worked on include: Creating a new promo to promote Local Schools (lots of exposure to the backend promo infrastructure, the data lake, and the client) Improving the performance of our Interests pages by using Redis sets to keep track of member and count list deltas (learning how to write quality tests, safely shipping updates, and monitoring changes) iOS task to revamp our content promo in a way which increases CTR (exposure to iOS) What I learned: Pair program as much as you can. Past the awkward “who’s driving” dance, you actually learn a lot with two pairs of eyes. Less room for mistakes Learn from the people around you. Specifically learn the way that people approach problems and work together effectively Buy a ping pong paddle when you first join so that by the end of the internship you’ll be unstoppable Drink lots of Oi Ocha Favorite Memory: Competing with Fuad for the title of best intern ping-ponger. I think we all know who won, don’t @ me. Studying: Computer Science at the University of North Dakota Special Talent: Public Speaking. Gave an opening speech for Magic Johnson at a Fundraiser Event Mentor: Jingjie Zhan (JJ) Project: For my internship, I wanted to try full stack development and work on a project which would allow me to have end to end ownership. I got just what I was looking for by working on the Ads Team. I worked on a project called Lead Gen, short for Lead Generator. Imagine the Nextdoor timeline to be a series of buckets, like an array. Some of those buckets are reserved for ads and I was in charge of creating a brand new one! Some of the things I did as part of my project include: Eliminated dependencies for my ad unit by creating modularized and independent front-end components Followed Test Driven Development. Wrote lots of tests for my front end and back end changes Worked closely with my mentor to break the project into actionable steps which we would then turn into tickets for me to work on (this was really challenging) What I learned: I walked in knowing nothing about React and had very little experience writing in Python, but by the end, I found myself comfortable writing code the “React” way…and grew to love Python! I also learned that with enough time and patience, you can become decent at anything…except CSS. Favorite Memory: Doing the Spartan Challenge with some of my Ad teammates and Arlen. I trained for two months and barely made it through to the end of the course, but finishing and receiving my medal at the end made me realize just how much of a family we are at Nextdoor. No one was left behind. The after-Spartan steak we had also will forever be ingrained in my memory. It was so good. Studying: Computer Science at University of Waterloo Special Talent: Balancing multiple textbooks and various other objects on top of my head while walking across a soccer field. Mentor: Dan Masquelier Project: I was placed on the core client web team and I loved it. I think Nextdoor has done a wonderful, phenomenal job in team-matching in terms of personality and goals. Core client focuses on the general infrastructure of web, Android, and iOS. With my background as a jack of all trades, I felt like being on this team really solidified my understanding of software development. I knew I wanted to work on a feature on social media or messaging, so the team gave me @mentions. Many other teams told me that @mentions was a feature they wanted for years and I felt so empowered to own it. The experience was amazing. It was challenging, complex, and rewarding. I built the frontend and backend, from where the custom-made text input and clean experience that encompasses many hidden edge cases to the complicated email sending and saving data to multiple different databases and caches. This has been the best internship experience I’ve ever had and the most in-depth, with the perfect amount of breadth to encompass an end-to-end feature development learning experience. What I learned: The main focus I had for this internship was to gain mastery in full-stack development. With Dan’s 10+ years of expertise, I can finally say I understand more CSS, React, emails, and Docker. I’m so grateful to have him as my first mentor and be his first mentee. Another thing I learned was culture. I’ve worked for many other startups before, but Nextdoor’s engineering culture of how if something breaks is not one person’s fault, but a problem in the structure. Having lots of developer tools to catch mistakes early on and to reinforce the idea of writing better code has helped me to personally feel safe to learn and try again. Aside from technical skills, I think the biggest thing I’ll take away from Nextdoor isn’t the large projects I’ve worked on, but the little things that tweak how I view code now — which is unmeasurable in the grand scheme of things, but have the highest impact in my career as it shapes the way I see problems. Favorite Memory: I have two competing favorite memories at Nextdoor. One was the holiday party. It was a wonderful experience to see the people who supported each of my teammates — the ones who supported them every night when they go back home. I felt instantly more connected with everyone on my team. I wish this happened more often in real life and on a daily or weekly basis. The second one was more subtle. It was Nextdoor’s pairing stations — they were a game changer. On my first day of work, Dan and I went to a pairing station where there were two monitors, two keyboards, two mice, and one laptop. This eliminated so many communication problems that came with traditional pair programming, but kept so many of the pros that lead to more effective development, knowledge transfer, and teamwork. Studying: Computer Science at the University of Maryland, College Park Special Talent: Short travel films Mentor: Tristan Eastburn Project: During my internship at Nextdoor, I had the pleasure of working with the Data Engineering team. My project, Steamboat, is a microservice that resurfaces data stored in our data lake back to production. Steamboat cuts down direct reads and writes to our redis clusters which allows us to better manage the data stored in our cache, as well as extract better metrics for the data we query for. Some highlights about my project are: Lightweight goroutines handle requests with sub millisecond latency Leveraged SQS, Lambda, and S3 to automatically load data coming in from our ETL pipeline to Redis Client side library in Python support peak throughputs of 2000 rps Resources for the server and consumer are provisioned and deployed across multiple regions using Cloudformation and CircleCI What I learned: This internship, I focused on bringing up a new microservice and integrating it with the existing Nextdoor ecosystem. Through this, I was able to pick up a new language, Golang, which turned out to be the perfect fit for my project. I was also able to productionize my project by adding the necessary metrics and setting up a deployment workflow. Talking with my team and mentor opened up a lot of opportunities to ask questions about my career path, as well as seek advice on how to grow as a developer. Chatting with Nextdoor cofounders Nirav and Prakash led to great advice on taking risk and their experiences from growing Nextdoor as a company. Favorite Memory: My favorite memory from my time at Nextdoor was all the fun sights and weekend trips I got to do while I was in San Francisco. Nextdoor’s office is in the heart of SF which gives you the perfect opportunity to explore all of the quirks that each neighborhood has to offer. If you’re a bit more adventurous, you can drive out on the weekend to any of the nearby parks. My favorites were Yosemite and Redwood. Studying: Information Science at the University of Maryland, College Park Special Talent: Basketball Mentor: James Wang Project: I had the opportunity to work on a new product feature as part of the Local Business team. Additionally, I worked on the new business experience where I was focused on redesigning business pages. Some of the features on my projects included: Lightweight filters for the section view (Web and mobile) Creating the stats email for business about performance Creating a new drag component for images on post flow Designing new drag icon and adding it to the NDUI library Updating the copy on the post flow Updating the post component on the new product feature page on the business experience to include only the feature Adding cover photo, profile picture, progress bar to the business profile redesign Creating new cropper and cropping flow on business profile Creating new designs for the new product feature section view for businesses to match the member experience What I learned: I had many goals going into my internship. I wanted to grow as a product thinker while contributing to my product team and the design team as well. Some of my learning included: Don’t be scared to take initiative . My first project was to redesign the employees directory. After looking at our current directory, I felt like it was out of date so I took it upon myself to redesign it. Additionally, I led the initiative behind the creation of the new Nextdoor design blog. Get feedback from different stakeholders. One thing I really enjoyed was to make engineering and product an integral part of my design process. By working closely and getting feedback from engineering and product, I was able to learn early about business and technical constraints. Voice your opinions with conviction. As an intern, it can be intimidating to voice your opinion at times especially, when you work with people with more experience. By thinking of myself not as an intern, but a full contributor to the team, I gained confidence in voicing my opinions with conviction. Favorite Memory: My favorite memory was the holiday party. It was really fun to see everyone well dressed up and spend time with coworkers celebrating the current year’s successes. Thank you to our interns and their mentors for a productive and inspiring few months. If you’re interested in checking out Nextdoor’s internship program or full-time opportunities, we’re hiring :) Nextdoor is the neighborhood hub for trusted connections… 111 Internships 111 claps 111 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-24"},
{"website": "NextDoor", "title": "how 72 lines of code can impact your company culture", "author": ["Mikhail Simin"], "link": "https://engblog.nextdoor.com/how-72-lines-of-code-can-impact-your-company-culture-308a79203991", "abstract": "Culture Open Source Technology It was 6 p.m. on a Thursday and the office was getting empty. I walked to the common area and saw the lonesome ping-pong table waiting patiently. “Who is the best nowadays?” I wondered, “Who all even plays?” I began to think of ways to keep track of games, and scores, and preferably rankings. Paper and pencil seemed dated. Using some app seemed heavy handed since I’d have to get everyone to use the same one. “What is the most lightweight way to get people to track their games?” I thought. The way most games originate is by calling out someone in Slack or just asking if anyone wants to play. Often the discussion would go back and forth a bit about being too busy, or new players figuring out how good others were. Wouldn’t it be nice if all this were automated, and done so directly in Slack? As a former chess player I was quite fond of Elo’s scoring system . The premise there is simple: everyone starts with a default rank and gain or lose points based on who they play against. If you play someone who is much better than you and lose — you barely lose any points in your rank. If you win vs someone who is much better — you gain a lot of points, and they lose a lot! I’ve wanted to introduce this system to our ping pong games, directly as part of the conversations. Here’s how I did it via our bot “Mr. Rogers” powered by the Alphabot engine. When someone wants to take on a challenge the easiest way to accept it, even easier than saying yes, is to click a button. Slack makes it convenient by providing “ interactive-messages ” as a feature. The activation keyword I chose is !challenge (we have a convention for bot commands to begin with !), and the rating you see below is the Elo score. Everyone in this Slack room can now click the button, and the first one to do so will claim the spot in the competition! To prevent further confusion, the alphabot engine removes the buttons and replaces it with information about who clicked it, which button was clicked (in case there is more than one) and adds a timestamp: Along with the changes above Mr. Rogers gives some new information. A new interactive message is provided, this time with four choices of the outcome of the match: Selecting one of the names will increase that person’s score and lower the other person’s score. From the graph above — if the difference in scores is huge (600) then the expectation of win is high, and therefore % of points to earn is low. However if the difference is in the opposite direction (-600) then then expectation to win is low — and % of points to win is high. Converting this to code is easy, especially if you’re good at debugging ! When ranking entered the scene, everything changed. Discussions around the ping pong table and on Slack were no longer “she’s good” or “I’m about same as him” — it was all in the data. This made everyone curious about their standings and more people started participating. As per huge demand — a coworker quickly wrote another Mr. Rogers script to display the leaderboard. In less than a week we had over 15 participants, compared to 6 before. Two weeks, and another coworker decided to organize a company-wide tournament; it has become a 32 person bracket! (Hat tip to challonge.com ) So how did this code ignite such a change in our attitude toward pingpong? The real change was of course not because of the specific lines of code, but because an action that people already liked was made super simple, and a level of gaming competition was added. I loved this project because it brought together two things I’m passionate about. To me, bringing together software and cultural impact is like bringing peanut butter to jelly. Nextdoor is the neighborhood hub for trusted connections… 47 Thanks to Nick Brinkerhoff . Slack Automation Culture Software Open Source 47 claps 47 Written by Reverse Engineer Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Reverse Engineer Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-03"},
{"website": "NextDoor", "title": "automating aws acm cert validation options", "author": ["Slava Markeyev"], "link": "https://engblog.nextdoor.com/automating-aws-acm-cert-validation-options-9d1cd1081f0f", "abstract": "Culture Open Source Technology At Nextdoor, the Systems team uses AWS CloudFormation stacks paired with Kingpin , a tool for orchestrating multiple APIs, to manage our cloud resources via code. Resources can be S3 buckets, Kinesis Streams, IAM policies, or in this article’s case Security Certificates. When a certificate is requested via AWS Certificate Manager (ACM), a validation must take place before the certificate is issued. ACM looks up the WHOIS contact information for the specified domain and sends emails to those contacts to validate ownership and approve the cert creation. As noted in the AWS documentation , the apex domain should be used as the validation domain. For example, foo.bar.example.com should use example.com as it’s validation domain. This is what the an ACM certificate resource looks like in a CloudFormation stack. Note the DomainName and the ValidationDomain properties: A templated version of the above would be: The above snippet becomes fairly convoluted when you want to template this for a multi-domain certificate. A user would have to specify the DomainName, SubjectAlternativeNames (SANS), and the mapping for each SAN to put into the DomainValidationOptions. Since the apex domains are different for each SAN — separate validation domains need to be specified. A non-templated stack resource would look like: CloudFormation does not currently generate the DomainValidationOptions from the SubjectAlternativeNames list. However, one can leverage a simple Lambda function within a stack to do just that. CloudFormation can manage a custom resource through an API endpoint. Combining this with AWS Lambda we can generate validation options from a list of SANs domains. The function accepts a list of SANS: and returns the validation options to the stack in the following format: The stack is then able to use the map by calling: Here is the complete CF stack that utilizes the function. Note for clarity I’ve separated the python code out of the stack, but it is meant to be pasted inline. Here’s the python lambda code that does the lookup to find the apex domain for a provided subdomain. Nextdoor’s engineering culture has always encouraged automating little things like cert validation options. This project started off as a personal hunch for an easy win, and now we are able to re-evaluate our use of web certificates across the company. Find this sort of stuff cool? The Nextdoor engineering team is always looking for motivated and talented engineers. Make your impact on Nextdoor’s engineering culture . Nextdoor is the neighborhood hub for trusted connections… 57 1 AWS Automation DevOps System Administration Software Development 57 claps 57 1 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-07-11"},
{"website": "NextDoor", "title": "3 hard lessons from scaling continuous deployment to a monolith with 70 engineers", "author": ["Niall O'Higgins"], "link": "https://engblog.nextdoor.com/3-hard-lessons-from-scaling-continuous-deployment-to-a-monolith-with-70-engineers-99fb6dfe3c38", "abstract": "Culture Open Source Technology Like many successful tech startups (Instagram, Slack, New Relic, and many more) a large portion of Nextdoor is a monolithic application. While we’ve also created a significant number of microservices — especially in places where high throughput is needed — a large amount of our product and feature work still takes place in our Django monolith. This means that the majority of our 70-member (and growing) engineering team is regularly working on a large monolithic Python codebase. We recently completed a very significant project to move from ~4 releases of the monolith per week to Continuous Deployment. See our related post How Nextdoor Made a 10x Improvement in Release Times with Docker and Amazon ECS . With Continuous Deployment at Nextdoor, changes are released to production as soon as they are ready and as quickly as our build and deployment infrastructure can manage. Engineers land directly to master and we avoid long-lived branches. Furthermore, the workflow around deployment and rollback of the monolith is fully automated. There isn’t even a button that an engineer needs to push! Typically, we have dozens of releases per day in a fully-automated system where any engineer can deploy or rollback production. Making this operate smoothly has required lots of thought and polish on our workflow and the tooling around it. There isn’t much public information on how these details are managed at scale nor what kinds of tools you need to do this well. Since releasing code is at the core of our business, we invested in building a great tool to manage this complexity, named Conductor. We hope to open source Conductor in the near future. In this blog post we aim to share the biggest lessons we learned the hard way, and give a sneak peek into our tooling and workflow. It’s one thing to practice Continuous Deployment (henceforth abbreviated to CD) with a microservice owned by a team consisting of a handful of engineers. A small team will introduce changes at a slower rate than a large team. When you have 70 engineers working on the same large application, the rate of change is much higher. Changes are being made in completely different areas of the application by different teams at the same time. Furthermore, in a microservice the surface area for defects is much smaller. A defect in a microservice will generally be limited in terms of its potential blast radius compared to a defect in a monolith. In contrast, one small bug in a monolith can take down all functionality of the site. This is one of the downsides to a monolithic application — slowness or errors in one part can adversely effect the entire system, rather than just their local area. There is little isolation. Therefore, managing the high rate of change and blast radius is a critical ingredient in successful CD workflows. With CD of a microservice, you might be able to have the release size consist of a single change — i.e. do one release for each individual change. However at the scale of the Nextdoor monolith, this isn’t usually possible. Given the test, build and deploy overhead of our monolith (about 25 minutes total), if we released a single change at a time, changes would get backed up and it could take hours between engineers landing their changes on master and them going out to production. We aim to get changes live as quickly as possible once they’re landed on master. It becomes difficult and inconvenient for engineers should there be too large a delta or too much uncertainty around when the change they landed will be live on Production. This is where we introduce the concept of a “train”. A train is basically the smallest unit (or batch) of releasable changes, and we use the following rules to negotiate what is on a train. There can be only one train “running on the tracks” at a time. When a change lands on master, if there is already an existing train, it is queued for the next train. To use a train-based metaphor, the changes are “waiting on the platform” until a new train arrives. Note that before a change can be landed on master, a large suite of automated tests must pass and it must be accepted by a human reviewer. If there is no existing train, a new train will be created containing all changes up to HEAD of master. Thus, all of the queued changes or those “waiting on the platform” are put on this new train. Trains go through three phases. First, Delivery. This phase includes build and deployment to our Staging environment. Then, Verification. This includes both automated verifications such as unit test runs, smoke tests along with human verification. Human verification is tracked by polling the state of tickets — we use JIRA. Tickets are automatically created for each engineer with non-dark launched changes on the train once it is delivered to Staging. Finally, Deploy. Once all of the verifications are complete — which means all the automated tests have passed and any manual verification tickets have been closed by the engineers— the train will auto-deploy to production. Once a train starts to deploy to production, a new train is implicitly created as soon as there are any queued changes. To maximize throughput, we don’t wait for deployment to complete before starting on the next train. If there is a problem with a train, it can be manually extended via a button in the UI. Any engineer can press a button to include a fix or revert commit for something urgent. Train extension simply pulls in all queued commits — up to HEAD of master. This will slow down the current train, and therefore everyone else on it, so it should be used sparingly. Ideally, automated test coverage is good enough that you don’t find problems at this stage and there is no need to extend for a revert or fix. One of the advantages of the way we have implemented trains is that releases are typically quite small. The smaller your releases the better — since the likelihood of a problem increases with every additional change. Furthermore, when something does go wrong, it is much easier to find the offending change from within a small batch size than a large one. Engineers are encouraged to put their changes behind a feature flag and we provide a framework called Feature Config which powers this. If a change is behind a feature config, we let engineers bypass the manual verification process. The rationale here is that the feature can be manually verified at any time, potentially with a small number of users initially, and if it has a bug, it can be switched off instantly. If you’re an engineer who has just finished a change, you have a strong incentive to see it released as soon as possible. Similarly, if you as an individual engineer introduce a defect you have a very strong desire to fix it quickly and get that fix live on production immediately. You’ll be much happier and more productive if you can drive this workflow yourself. If the workflow is instead driven and controlled by a dedicated team (e.g. a “release team”) the incentives aren’t necessarily the same. For example, if a bad data migration is in a release, it’s best to give the people who had changes in that release visibility so that they can quickly debug it and fix it. One of the philosophies at Nextdoor is to align incentives at the right levels to boost productivity as much as possible. Toward this goal, we pick a random individual who has a change on the train to be the “train engineer”. The train engineer is responsible for frontline triage of issues. They act as a release engineer but for a very small release. For example, if an automated test fails during the verification phase of their train, they will be sent a Slack message telling them to triage the issue and resolve the problem. This is typically resolved by them reading test failure output, triangulating a likely candidate change on the train, and then co-ordinating with that change’s author to get a fix or a revert onto the train to get it moving again. In practice, the train engineer system works well. If a particular train is delayed or has issues, there is clear individual responsibility for tracking down verifiers, performing a rollback or extending the train with a fix. Since the train engineer by definition has a change on the train themselves, they also have a strong incentive to resolve problems quickly. By selecting the train engineer at random, release process knowledge and load is gradually spread throughout the organization. Another observation was that human verification can be extremely slow. For example, perhaps an engineer has a change on the train and then they go into an interview for an hour or a series of meetings — meanwhile everyone else is held up. To tackle this problem, we have created a culture where it’s agreed that excessively slow verifications aren’t acceptable behavior. It’s considered as serious a breach of etiquette as breaking tests in master. Other people on the train will go and track down the person with some urgency, or possibly find someone else to verify it — or in extreme cases revert the offending commit entirely. While this culture helps to a significant degree, humans will make mistakes. The best way to avoid having a human forget to do their verification is to make it unnecessary for them to verify in the first place. To aid in this we have introduced additional tools and workflows where you can bypass needing to verify your change on Staging entirely and therefore not risk holding everybody up: Pre-land verification environments, called Preview Environments. These are unique, per-code-review, fully-isolated Staging-like environments which engineers can use to verify their changes before they land them on master. If they use this capability, they bypass Staging and its manual verification requirements entirely. Thus there is no human verification requirement for their change during the release process. Feature Config. If your change is behind a feature config, you don’t need to verify manually on Staging since your change has no impact on Production by default. It can be rolled out to a limited audience and quickly turned off if it has bugs or issues. At Nextdoor, we have been continuously deploying dozens of releases per day of our large monolith for about 10 months. We have 70+ engineers working on this codebase. The benefits of Continuous Deployment to the speed of our product development organization have been significant. However, the unique challenges posed by both a monolithic application and a rapid pace of change have required us to invest heavily in workflow optimization and tooling to achieve this. It took a team of 3 engineers about 9 months to figure out all of the intricacies of the workflow, developer user experience, and to then build the associated tooling. At the core is the Conductor microservice, which glues together Jenkins, Slack, Github and JIRA to present a coherent user interface and enforce rules. Since every engineer at Nextdoor interacts with Conductor to ship their code we wanted to make it as pleasurable as possible to use. We plan to open source Conductor — which was designed to be modular with pluggable support for third party services— in the near future. I’d like to give a huge shout out to the engineers on the Dev Tools Team who have contributed to this project and this blog post: Steve Mostovoy , Rob Mackenzie , Mikhail Simin and Alex Karweit . Find this sort of stuff cool? The Nextdoor engineering team is always looking for motivated and talented engineers. nextdoor.com Nextdoor is the neighborhood hub for trusted connections… 410 Thanks to Steve Mostovoy , Vikas Kawadia , and Isabella Levin . Continuous Integration Technology 410 claps 410 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-12-08"},
{"website": "NextDoor", "title": "meet the interns spring 2017 edition", "author": ["Kelli Ryan"], "link": "https://engblog.nextdoor.com/meet-the-interns-spring-2017-edition-e91c77c3beb", "abstract": "Culture Open Source Technology At Nextdoor, few things get us more excited than the arrival of a new class of interns. In addition to being incredibly talented, our interns bring much more than technical contributions to Nextdoor. Each wave of interns brings fresh perspectives, new ideas, and creative ways to approach challenges. We know that deciding where to intern is a big decision. Finding the right fit for both technical and personal growth can be tough. That’s why we structure our intern program to set our interns up for success! Through a dedicated mentorship program and placing our interns on important, high priority projects, we make sure that our interns are solving interesting problems and equipped with the resources and guidance they need to be successful. With weekly 1:1 meetings, performance reviews, and being regarded as full time team members, our interns gain a tremendous amount of professional growth, in addition to learning new technologies. At the end of their internship, interns are proud of the work they’ve accomplished and have a holistic understanding of what it’s like to work at Nextdoor as a full time employee. We are confident that by the end of any given internship, our interns leave Nextdoor with more than just new technical skills and branded tshirts. Our interns move on from their time here with sharper skills, fond memories of our team here in San Francisco, and take with them a little piece of our own Nextdoor neighborhood! Studying: Computer Science & Physics @ University of Maryland, College Park Special Talent: I can draw a square with one hand and a triangle with the other hand simultaneously (the challenge is to draw one side of both polygons in every “tick” of time) Mentor: Keiji Oenoki Project I spent most of my time within the new classifieds team. I built the new backend microservice for the classifieds product in Go from scratch. Although the service was written in Go, it involved touching many different components in our tech stack to get the project up and running. As part of the project, I: Designed efficiently queryable classifieds schemas in PostgreSQL and Elasticsearch Setup NGINX and containerized the microservice using Docker Wrote RESTful APIs in the new microservice (Go) and Nextdoor.com (Django/Python) that are SQLi-proof. Built a new internal Go library for handling PostgreSQL database migrations Refined Elasticsearch pub/sub indexer architecture to bump classifieds listings based on chat activities Integrated CircleCI and Phabricator What I learned I did learn a lot of technical skills — for example, I didn’t know any Go before joining the team –), but, more importantly, my internship at Nextdoor helped me grow as a person. I’m especially thankful for my mentor, Keiji, for helping me grow a habit of self-reflection. On that note, I would also like to recognize and thank the following incredible Nextdoorians (in lexicographical order) for always being neighborly, sharing their life and career advices, and just being amazing teammates: Ben Springwater, Daisuke Fujiwara, Dan Clancy, Eunkwang Joo, Keiji Oenoki, Mitali Gala, Prakash Janakiraman, and Satya Yalla. Favorite Memory I had tons of fun geeking out and having engineering trade-off discussions with my mentor, Keiji. My favorite times include doing some back-of-the-envelope calculations on the Birthday Paradox to decide to use UUIDs and having hypothetical discussions on what it will take to support Bitcoins in our new classifieds microservice. Studying : Computer Science and Engineering @ University of California, Irvine Special Talent: I can do the impossible–lick my elbow. I’m also an advent drummer and guitarist, hmu. Mentor: Harry Kao Project: Since I’ve been at Nextdoor, I’ve been treated no differently than any other engineer. I was thrown into my work headfirst, while being given a whole team of resources to reach out to if and when I was blocked. On my very first day, I expressed my desire to be exposed to as many layers of Nextdoor’s stack as possible so that I could maximize my impact over my time here. Since then, I’ve been given a breadth of interesting projects which range across all sections of Nextdoor’s codebase. Some of the projects I’ve worked on include: Adding the ability to asynchronously cancel postcards in Nextdoor’s synchronous postcard task Implementing postcard safeguards across nextdoor.com and iOS to prevent users from “accidentally” sending postcards Applying neighborhood stats to email invitations to promote invitation acceptance rates Designing and building an AB test to test acceptance rates of those emails with applied stats Refactoring our RedisMock to prevent engineers from shipping bad code involving Redis into production Implementing new verification methods on iOS and Android to overall improve conversion rates of mobile users Writing a script to verify all verifiable users who signed up via mobile in the past Creating an all encompassing throttling API which is now used to prevent users from using Nextdoor to spam emails, comments, etc. Reskinning the iOS login and create account flow to push our new design focuses, overall unifying Nextdoor’s user experience across all platforms What I Learned: Nextdoor has been an incredible experience for me. My mentor and team have done a tremendous job of balancing mentorship and freedom, allowing me to contribute my own ideas without becoming overwhelmed by Nextdoor’s steep learning curve. From learning their technologies such as Django, React, and iOS, to skills such as best design practices, AB tests and experimentation, and thorough unit, integration and acceptance testing, I have learned a collection of hard skills useful for engineers at all stages of their career. I also learned soft skills such as knowing when to ask for help and building lasting relationships with your team. Nextdoor has easily been my most fruitful internship to date. Nextdoor moves and iterates fast, while still maintaining their very high bar for code quality. My time here has been unforgettable and it has been an honor to be a part of the Nextdoor family. Favorite Memory : My favorite memory is the time the Winterns randomly decided on a Thursday evening to make a shotgun Yosemite trip to climb to the top of Yosemite Falls. We woke up at 3:30am Saturday morning to leave by 4am. We got there, hiked all the way up the falls, and stayed on the top for a few hours. The view was absolutely stunning, especially after the huge amount of rain California has been getting. After that, we ran to the bottom, then drove back to San Francisco — only after stopping by La Vic’s for some dank burritos. I was in bed and asleep by 1am, after one of the most incredible last minute decisions of my life. Studying: Computer Science @ Texas A&M University Special Talent: NATO Alphabets Mentor: Andrew Brown Project: I was a part of the International and Growth team at Nextdoor, and was treated like any other full-time engineer on my team. That meant receiving the flexibility, autonomy and support to shape the direction of my projects and make important technical decisions. Looking back, I think it is safe to say that the past four months I’ve spent at Nextdoor have been the four months of my life I’ve learned the most from. During my time here, I: Refactored our React component to handle postcard invitation codes. Developed a full-stack internal tool for the mapping team in a Django and PostgresSQL architecture to render, preview and plot neighborhood boundaries in European countries. Built and deployed a containerized geolocation service that normalizes addresses and provides latitude and longitude coordinates for addresses we cannot autocomplete. Automated builds and deploys and set-up a continuous integration pipeline with CircleCI and Jenkins for the above-mentioned service. Streamlined and unified the login process for all staff users across Nextdoor’s top level domains in international countries. Implemented and pushed out privacy screens internationally for the new user sign-up flow on iOS and Android. Designed, built and ran an AB test on iOS to measure and analyze the effect of various invitation mechanisms on viral user growth in Europe. Worked on reducing racial profiling in urgent alerts by analyzing context in user text. What I Learned: Nextdoor has a very high bar for code quality and I think it is directly reflected in quality of the products we ship. Through extensive yet fruitful code reviews, pair programming, architectural meetings, and 1:1 sessions, I learned best design patterns and practices across our stack and fostered a deeper understanding of technologies we use. I realized that evaluating deep technical trade-offs that affect hundreds of thousands of neighborhoods and the company itself is not easy, but can be made easier by prioritization, early validation, and constant iteration. Right after our acquisition of Streetlife (a British social networking company), the Neighborhood operations team, our equivalent of a support team, was flooded with customer feedback and questions, and everyone in the company had the opportunity to address them. As an intern, it was enlightening to be on the forefront interacting directly with users across the world using your product, hearing their unique use cases and understanding their varying needs. I learned user empathy and this informed my engineering decisions that followed. Also, I now dislike writing tests a little less. Oh, and I learned to ski during our engineering off-site at Tahoe! I woke up each morning excited to come to work and learn from some of the smartest people I know. A big shout out to my team for placing an astonishing amount of trust on me. And especially to my mentor Andrew for being extremely helpful, patient and genuinely invested in my growth and development. Andrew created an atmosphere of trust where I did not hesitate to ask anything on my mind. I couldn’t have asked for a better mentor, and my gratitude for his contribution to my future success is immeasurable. Favorite Memory: During a team dinner after our beta launch in a new country, one of my coworkers, Carly, mentioned her trips to national parks. I had never gone hiking before and yet she convinced me to go to Yosemite. A few days later, I convinced Justin (the other intern) to do the same. Yosemite was absolutely amazing and probably one of best trips I have taken in a very long time! Nextdoor is the neighborhood hub for trusted connections… 49 Internships Nextdoor Startup San Francisco Culture 49 claps 49 Written by College Program Manager at Nextdoor Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by College Program Manager at Nextdoor Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-05-19"},
{"website": "NextDoor", "title": "ditch your buckets of scripts for a tool belt", "author": ["Andrew Brown"], "link": "https://engblog.nextdoor.com/ditch-your-buckets-of-scripts-for-a-tool-belt-ea61fd9f584a", "abstract": "Culture Open Source Technology If you’re like us at Nextdoor, you have a lot of scripts that do a lot of useful things. Maybe, like us, you have some wiki pages that tells you the full path names of these scripts and the situations and order in which you should run them. You may have also found that you’d rather ask your co-workers how do anything than to try find what you want in that wiki. If this is your story, it might be time to stop and ask yourself: “Why shouldn’t your set of tools be at least as easy to use as other tools like git, heroku or aws that you use on a daily basis?” The good news is that you don’t need a centralized tool that does everything in your toolbelt. We decided to embrace decentralization and create a way to run tools as if they were centralized, simply by having them adhere to a naming convention and reside in the search path. This behavior is enabled for us by a fairly simply open-source tool we call buckle . buckle performs the following magic: You place an executable script, somewhere in your executable path, with the name nd-dev~createdb You run this script by typing nd dev createdb . That’s right, you didn’t even have to type those hyphens and tildes. Maybe you’re not impressed yet, but you can also see what commands are available simply by using autocomplete: You can also ask for help: What’s different about everything above from your average command-line interface (CLI) toolbelt is that is that there is no single nd executable that has all these commands in it. There are a just a bunch of separate scripts. Here’s a simple bash session where we create a toolbelt called ‘sash’ with an ‘echo’ command: Note that while the above example uses an alias to initialize your toolbelt, you can also create a bash script as your sash executable: By this point, you’ve probably already downloaded buckle from pypi and have redesigned your whole developer environment around it. In case you haven’t, let me tell you about the philosophy of buckle . buckle was designed with the belief that great command-line tools have 3 properties. They are: memorable discoverable extensible Having a single entry point to all of your commands reduces barriers to remembering what to run. In our case, all our commands start with nd . Couple this with autocomplete and pretty soon you can easily find that command you ran again without digging through your bash history, searching your wiki or pinging the whole company on slack. You don’t have strive to remember commands like git, aws or heroku, so why would you want to have to remember the name of each of your own in-house tools? A bit about autocomplete buckle delivers autocomplete by allowing you to create a script with the .completion.sh suffix. For example, the nd-django-command script might be coupled with an completion command called nd-dev~django-command.completion.sh . buckle will find this command in your path and feed it the autocomplete arguments, which you might then forward on to django’s autocomplete to generate results. buckle provides discoverability by offering a built-in help command. The help command simply calls each commands in your namespace with the “ — help” option and summarizes the output. In our case, we just type nd help and we can see all of the available commands. The help text parser is designed around standard python argparse help but it will try its best to parse more general help messages. Discoverability is also aided by autocomplete, which will show you the list of available commands without you even having to type help . The goal of buckle was to bring a bunch of disparate scripts together. You could achieve a unified namespace simply by renaming all your scripts, or bringing them together into a common tool using argparse subparsers or a library like click . Our feeling was that centralizing stuff wasn’t the right answer. We wanted to give developers maximum flexibility without creating large monolithic tools or even requiring an entry in a common registry of tools. The fact that buckle finds all the available commands from the path is an import piece of that decentralization effort. Since it just searches your path, you can add commands simply by installing an additional python package that inserts those commands in your path. As you see in the nd dev createdb example, buckle also supports nested namespaces and even multiple concurrent toolbelts. The name of the script is nd-dev~createdb (we use ‘~’ as a namespace separator for commands because it seemed unlikely to be used in a real script). It’s worth noting that your script doesn’t even need to be named nd-dev~createdb . It could still be called build-my-database or whatever you chose to call it; you would simply put a link to it in your path so that it shows up in the nd dev namespace. One additional feature of buckle, is that it has a mechanism for running shared scripts before any one of a set of toolbelt commands. Buckle calls these “dot-commands .” In our example, we might have the command nd-dev~.check-venv . This command would be run before any nd dev commands and could be used to check if your virtual environment is up-to-date before allowing you to run nd dev createdb . Often developers find out about missing dependencies too late in the process and it becomes difficult to identify what went wrong, so “dot-commands” allow you to check and update dependencies without modifying each individual tool to initiate the check itself. Now that you understand all that buckle can do, let’s step back and separate the “user interface” from the implementation. The python command buckle is just how we’ve chosen to implement this behavior at Nextdoor. If you favor a different programming stack, there’s no reason you couldn’t write your own implementation of this same or similar behavior in a language or framework of your choice. buckle is really just a way of arranging scripts in a way that is memorable, discoverable and extensible to make the developer experience more sleek. Work on buckle began last summer when I started on the project with one our summer interns, Hector Ramos , who wrote the majority of the initial code (and tests, first). Thanks to the entire Developer Tools team ( Steve , Niall and Mikhail ) at Nextdoor for supporting this effort and extending the tool space to include all sorts of useful tools. The source for this project can be found at: https://github.com/Nextdoor/buckle . Like this post? Be sure to recommend. Follow Andrew on Github and Medium . Make your impact on Nextdoor’s engineering culture . Nextdoor is the neighborhood hub for trusted connections… 18 Thanks to Daisuke Fujiwara and Vikas Kawadia . Programming DevOps Open Source Technology 18 claps 18 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-21"},
{"website": "NextDoor", "title": "nextdoor css guidelines", "author": ["Harry Kao"], "link": "https://engblog.nextdoor.com/nextdoor-css-guidelines-47eb0b0f7565", "abstract": "Culture Open Source Technology Do you remember playing Hungry Hungry Hippos as a child? Yes, that’s the game where you try to encourage a colorful plastic hippo to munch on marbles. Well, CSS is the hippo of the web stack. Here’s what I mean: Despite being portrayed as cute and kid-friendly , they are, in fact, murderous beasts. Widely regarded to be the most dangerous animals in Africa, they run and swim faster than we do, attack without provocation, and fling poo everywhere (and I mean everywhere ). They’re more deadly than lions, crocodiles, and an impressive collection of poisonous snakes . Likewise, CSS appears cuddly and non-threatening at first glance. But if you’re not careful, you can quickly find yourself trapped in its jaws, being pulled under the surface of the water with no hope of escape… As Nextdoor’s codebase got bigger and, more importantly, older, it became apparent that our CSS wasn’t scaling well. A few of the problems we ran into were: Collisions between CSS rules. This happens when multiple rules unintentionally select the same element and cause the wrong styles to be applied. It’s worse when you’re writing reusable components or implementing a single-page application because you don’t know what other CSS rules might be loaded on the page. Not knowing how an element’s class is being used. Class attributes are used for both styling and element selection in JavaScript. This makes it easy to break things when you refactor, either by borking the styling when you work on JS or borking the logic when you’re modifying the layout. Poor performance. We have a lot of DOM elements and a lot of CSS selectors and the client (web browser) spends a lot of time rendering the page. Let’s just say that you don’t need a profiler to tell that there’s an opportunity for improvement. To help fix these problems, we decided to enforce a subset of Medium’s CSS guide . Here are some of the rules that are working well for us: CSS rules are global so care must be taken to avoid collisions. If two unrelated elements use the same class name for styling, their CSS rules can be merged, causing unwanted cascading. ( Cascading is the first “C” in CSS, and is the cause of much grief.) Since CSS doesn’t have a built-in namespacing facility, we must disambiguate class name by applying prefixes: For a widget called “searchbox”, the classes can be prefixed with “searchbox-”. Classes on the user profile page can start with “profile-”. When class names are unique, it’s much less likely that multiple CSS selectors will accidentally match the same element. Class names used for selection in JavaScript always start with “js-”. This means: “js-” prefixed class names never appear in CSS. Class names without “js-” never appear in JS. The “js-” prefix makes it easier to understand what a class is used for and where to look for the code that references it. In general, uncertainty about how something is used makes us reluctant to change it, which is never a good thing. Using !important to override conflicting declarations is usually a sign that something is wrong, either due to collisions and/or specificity . If class names are properly prefixed, this shouldn’t be necessary. Note: One situation where !important is needed is in user stylesheets, where users can override the site’s styling to suit their own needs. A descendent selector looks like this: This means that divs that have a parent with the class “foo” should have their color set to white. This is inefficient. When the browser renders the page, it iterates over the DOM elements looking for CSS rules to apply to them. For this particular rule, each time the browser sees a div, it needs to follow its parents until it either finds the “foo” class or reaches the root. The complexity of matching DOM elements to CSS rules is, roughly speaking: (number of elements) × (number of selectors) × (how hard it is to decide if a selector matches the element). The last part of the equation is easier when the browser can make a quick accept/reject decision. Since a properly prefixed class name uniquely identifies a set of elements, descendent selectors are not needed. Instead, the selector should contain only the class name. Then the browser can decide if the rule matches without looking at any other elements. We use the LESS preprocessor to generate our CSS. One of LESS’s most commonly used features is nested rules , which help organize the code and make selectors more specific. Unfortunately, nested rules are implemented as descendant selectors: Again, the descendent selector problem is solved by prefixing the class names and flattening the rules. For organization, related CSS rules can be placed in a separate file and imported . It turns out, surprisingly, that once you get used to being mauled by hippos, it’s hard to break the habit. In addition to documenting and communicating our CSS guidelines, we also recruited a group of front-end developers to review all CSS changes. The css-review group is automatically added to any code review that modifies a LESS file. Through a bit of well-timed nagging, we’ve seen a marked improvement in the quality of our CSS. An improvement that I think will be permanent, as we’ve replaced old habits with new ones. One of my favorite parts of working at Nextdoor is having the opportunity to change culture and improve quality and efficiency across the engineering team. Want to join us? We’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 22 Thanks to Daisuke Fujiwara . CSS Web Development Best Practices Front End Development Technology 22 claps 22 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "scaling disaster response at nextdoor", "author": ["Sean Bromage"], "link": "https://engblog.nextdoor.com/scaling-disaster-response-at-nextdoor-85d1007a9e63", "abstract": "Culture Open Source Technology Nextdoor , at its core, is a communications platform. We not only emphasize communication within your neighborhood, but also build tools that enable municipal agencies to reach members of their community. Recent events, such as the flooding in Houston , have highlighted the utility of one of our core features: the Urgent Alert. Sending an Urgent Alert allows agencies to target individual neighborhoods with critical information. When an alert goes out, Nextdoor members are alerted via text message, push notification and/or email. The conversation then continues within their neighborhood and nearby neighborhoods. Substantial technical infrastructure is needed to deliver Urgent Alerts timely and reliably, especially for large cities. We’ve had to devise an architecture that will be rock solid in these times of need. Municipal agencies on Nextdoor can reach upwards of 1M households in dense urban areas. Since an Urgent Alert is just that - urgent - our infrastructure needs to be able to handle sudden spikes in activity. There are several things to consider when designing an architecture with this use case in mind: Timing is unpredictable Scope can be very large Messages must be delivered Latency must be kept to a minimum Site load times can’t be affected All content routing on Nextdoor is dependent on the member’s location. We chose PostgreSQL as our database system because of its performance characteristics related to executing geospatial queries. When an agency is created, a large geospatial query is executed in the background that finds the intersection of the agency’s geometry with geometries of household plots (read more on how we store and quickly access these geometries on our writeup of Atlas ). This information is stored in a separate table for quick lookup when determining which members to route alerts to. When an alert is created, a series of asynchronous tasks are created and sent to an array of Taskworker machines. These tasks include creating and sending text messages, push notifications and emails. In order to give priority to these tasks over other types of tasks (we run tens of millions per day), we utilize the concept of priority queues. Every async task that a Nextdoor engineer creates must be given a priority. These priorities are respected by Taskworkers by giving resources to higher priority tasks. They also allow tuning of alerts related to Service Level Agreements (SLAs) given to each type of task that will notify oncall engineers of unusually slow queue consumption. The priorities range from high — > default — > low — > large, with high being reserved for very important, sub-second latency communication events, such as Urgent Alerts. As millions of tasks flood the Taskworker array, lower priority tasks can be starved of resources. We use Rightscale to monitor metrics related to our arrays and have setup autoscaling triggers. If queue depth becomes too high for any priority queue, more machines automatically spin up. In order to protect our main database from so many machines vying for resources at once, we’ve created a fleet of read replicas to quickly process queries without introducing more load on the main DB that would affect site load times. As tasks are processed, we send text messages using Twilio’s API using a short code for higher send rates and create our own push notifications and emails. The Houston Office of Emergency Management had little time to react and mobilize when some communities got more than 17 inches of rain in less than 24 hours. Houston officials utilized Nextdoor for Public Agencies to reach their residents during the severe flooding: The messages provided up-to-the-minute details to keep residents out of harm’s way. There are currently more than 1,400 agencies nationwide using Nextdoor to connect and communicate with their residents. By integrating asynchronous tasks, priority queues, and autoscaling Taskworker arrays into Nextdoor’s infrastructure, we are able to handle any number of messages municipal agencies broadcast. Like this post? Be sure to recommend and follow Nextdoor Engineering . Make your impact on Nextdoor’s engineering culture . Nextdoor is the neighborhood hub for trusted connections… 19 Thanks to Gabriel Chang and Vikas Kawadia . Nextdoor Engineering Scalability Infrastructure 19 claps 19 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "how to dry up your tests without making mummies of them", "author": ["Andrew Brown"], "link": "https://engblog.nextdoor.com/how-to-dry-up-your-tests-without-making-mummies-of-them-7de79a8e3df1", "abstract": "Culture Open Source Technology During my formative period as a web programmer at Pivotal Labs, I learned to be merciless when refactoring code. We would squeeze every last droplet of repetition out of our implementations but we were much more circumspect about DRY-ing up our tests (for the uninitiated, DRY = Don’t Repeat Yourself). I was asked recently by a colleague about why I wasn’t very excited about a particularly DRY test suite he had written. His code was terse and his shared method names were clear, but I still didn’t feel comfortable with it and didn’t have a great explanation. In this article, I’ll reveal the “dos” and “don’ts” of DRY-ing up tests. Let’s first review what makes a test suite great. In my experience, great test suites adhere to the following guidelines: They don’t break when their target’s implementation changes. When developers talk about “brittle” tests, this is what they mean. This rule is particularly important to consider before you mock out an internal method for testing. The purpose of this guideline is that it shouldn’t be a headache to refactor (i.e. improve) your implementation. They read like a behavior specification. This is why some developers call their tests “specs.” Not only does writing tests this way make them more readable, but it helps ensure that you consider only the interface and avoid writing “brittle” tests that violate rule #1. If you adhere to the philosophy that “documentation and comments are lies,” then it is also reassuring to have real code — that will hopefully break when it lies — serve as your specification. Adding another test should be easy. This is at the core of why DRY-ing up tests can be tempting and also why it can be troublesome. We’ll dive deep on this later, but for now, let’s agree that if we want writing tests to be easy, and writing N tests was easy, then writing N+1 tests should also be easy, even if we don’t write all the tests at the same time. Failures are reported in a way that make it easy to find the fault. I think of this as a follow-on from the last rule. Writing tests should be easy and anything (e.g. compile time, test environment, etc) that makes it painful will encourage developers to take shortcuts that will ultimately lead to less well-tested code. There are probably other good guidelines out there, but these are the ones I feel are most relevant to our discussion about DRY-ing up tests. If following these guidelines ever feels difficult, remember this: If you are finding it hard to write a test, you probably need to refactor— most likely, split up — your implementation. A note about Test-Driven Development (TDD): This article is not about test-driven development, and I don’t practice it for all code I write, but TDD forces you to address all these guidelines before you write your code so that you are less likely to let your implementation lead you to violating any or all of them. Testing should be easy. You do not want developers to be discouraged from adding a new feature or another test because the test suite makes it difficult. It seems like DRY-ing up tests should help with this if it means you don’t need to add a lot of code — usually “boilerplate” code, copied from elsewhere in your test suite — to write a new test. While that’s true, merging all this duplicate can also backfire and force you to have to refactor your tests because your tests all share the same code. Hopefully this is old news to you, but first rule of testing is: If the test you’ve written includes an assertion, you must see it fail. If you didn’t see it fail, you cannot be confident that you have tested anything. Never seeing your assertion fail is comparable to deploying your code without ever running it first (if you are comfortable with that, you probably don’t need to be reading this article). In fact, not only should you see it fail, you want to be absolutely certain of the reason it failed . It’s worth noting that with TDD — because you hadn’t even written the implementation when you first ran the test — you are guaranteed to see the test fail and much more likely to immediately and correctly know the reason why. The first rule of testing leads to this corollary: When you refactor a test suite — if you really want to do it right — you must now make each of your tests fail again for the the reason you expected ( sigh ). I doubt much of anyone, myself included, really does this thoroughly. It adds to both the amount of work and the amount of risk incurred when writing tests. It becomes even more work to refactor tests for code you aren’t familiar with, because you don’t easily know how to make them fail in the way the test was designed to cover. I love refactoring code — particularly when there is a solid test suite that allows me to do so confidently — but refactoring tests is a place of discomfort for me. When I’m adding a test to a test suite, I pretty much assume that any shared test method name I see in that test is a lie. Why? For starters, it probably has no tests itself. Tests don’t 100% insure against method names being lies, but any tests for a shared method should at minimum provide a specification for exactly what that method does and how I can use it. Testing requires precision, and even if a test claims to have some general purpose, I feel the need to read through it completely and learn exactly what it does. In reading through a test-only method, I may well find out that it is overkill for my purpose, or I may find that I need one extra feature, and adding it would require me to make sure I didn’t make any existing tests start passing trivially. Let me offer a couple of guidelines for writing tests suites that don’t need to be refactored when you extend your implementation: This fits well with the “AAA” (Arrange, Act, Assert — see http://c2.com/cgi/wiki?ArrangeActAssert ) testing pattern, which other developers will instantly recognize and understand. Typically, these tests have a “money line,” which shows an invocation of the method-under-test as concisely as possible. In python code tested using py.test, the pattern might look like this: An arrangement like this gives you more freedom to refactor your setup and your assertions without changing how the code-under-test is invoked. Once you do change your shared setup code, all bets are still off about whether your existing tests are receiving the inputs they were expecting. Nevertheless, separating setup from the money line allows you to minimize the risk that your changes make existing tests do things like pass trivially because you no longer have execute the code-under-test. The top-level “money line” arrangement also makes it very clear to the reader what exactly is being tested. The pattern I most prefer in tests is to create an expected result and compare it to the actual result using a well-tested matcher. Ideally, the actual result is something that 10,000 monkeys with typewriters couldn’t come up with in a million years. Just because your matcher works for one test that is already passing doesn’t mean it will always continue to work as expected once someone refactors it. It’s better to have separate control over your matcher. As a programmer coming across some new code, I will trust and be willing to reuse a matcher or assertion method much more if it’s in a separate file that has its own set of unit tests. If it does — it could be this is just prejudice — I’ll be more likely to believe that it doesn’t have the limited scope of passing just the tests that are in the suite. Let’s work through an example of how we might refactor a test to be DRY. Consider the following implementation of “loud” integers (“ONE!!!”, “TWO!!!”, and so on — you may remember these from PhysEd in grade school). A LoudInt stores an integer value and prints out that number when you want to use it. Here is a python implementation of LoudInt : You’ll see that LoudInt screams at you whenever you access it (by calling int() on it). Assume that LoudInt has a thorough test suite. Also assume for now that our test runner cannot suppress stdout so we really don’t want this extra output. Fortunately, LoudInt has a “quiet” context which we’ll want to use to silence while doing our testing. As you’ll see, using this “quiet” context will force us to jump through some hoops when writing our tests. Let’s now extend LoudInt to support division: We write the following test using the py.test framework: There is a lot of repetition in this test, so perhaps we refactor the common code into what I call an “it works” method: While this is DRY, the “money line” (the line with the division) is embedded deep in within the it_works helper method. Suppose at this point, our requirements change and we need a custom exception to be generated when there is a division by zero error. Our new implementation looks like this: So how do our tests change to support this new requirement? Simply augmenting the DRY version of our test, we might get: Of course, our new case doesn’t share the it_works method, creating a strange asymmetry. We could create a different test classes to make this asymmetry less glowing, but we could also just roll the zero division behavior into it_works : Now we have a really complex “it works” method that obscures what the test is doing. The reader is forced to think through which side of a branch you are on to understand each test case. Our original attempt to DRY up the tests using an “it works” method has backfired, preventing us from DRYing up all your tests the same way without introducing a bunch of complexity. A better pattern is to introduce shared setup and matchers. Consider the following test: By using make_divisible_loud_int as a shared setup method, we were able to add a test for a zero division error while keeping things DRY, avoiding changing any other tests and still silencing all the noise of the loud integers. While the matcher may be a bit contrived in this example, the key thing is that it is thoroughly tested and designed for maximum readability in a test. When it comes down to it, the easiest way to keep your tests both DRY and extensible is just to keep your “money line” at the top-level of the test. Doing so will force you to DRY up only your setup and your matchers, which will set you up well for testing any future changes to your implementation. You can find the full source code for all the above examples as a gist . Don’t just read about testing: come write some code and test it with us at Nextdoor. We’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 29 Thanks to Vikas Kawadia , Wenbin Fang , and Mikhail Simin . Software Development Technology Software Testing Python 29 claps 29 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-10-05"},
{"website": "NextDoor", "title": "unit testing apache spark with py test", "author": ["Vikas Kawadia"], "link": "https://engblog.nextdoor.com/unit-testing-apache-spark-with-py-test-3b8970dc013b", "abstract": "Culture Open Source Technology Nextdoor uses Apache Spark (mostly PySpark) in production to process and learn from voluminous event data. Spark’s API (especially the DataFrames and Datasets API ) enable writing very concise code, so concise that it may be tempting to skip unit tests (its only three lines, what can go wrong). But its wise to resist the temptation . All production code needs tests. Back to testing. Spark supports a local mode that makes it easy to unit tests. Local mode creates a cluster on your box . In this post, I’ll show how to write unit tests using my favorite test framework for python code: py.test . Writing tests in py.test is less verbose, provides great support for fixtures (including reusable fixtures , parametrization in fixtures), has great debugging support with contexts, makes parallel/distributed running of tests easy, and has well though out command-line options options. Elaborating on these is perhaps another blog post (see this reddit for more details). I am also not going to talk much about Spark itself. For that, I recommend the excellent material at Databricks . Today the focus is writing unit tests for PySpark jobs. We’ll go through a few examples, the code for all of which is available here: github.com Lets start by writing a test for the following simple word counting function: To test this function, we need a SparkContext fixture. A test fixture is a fixed state of a set of objects that can be used as a consistent baseline for running tests. We’ll create a local mode SparkContext and decorate it with a py.test fixture: Creating a Spark Context (even in local mode) takes time, so we want to reuse it. The scope=session argument does exactly that: allows reusing the context for all tests in the session. One can also set the scope=module to get a fresh context for tests in a module. Now the SparkContext can be used to write a unit test for word count: Note that we made the local SparkContext available to our test function by the following pytest magic: pytestmark = pytest.mark.usefixtures(“spark_context”). One can also decorate the test function with @pytest.mark.usefixtures(“spark_context”) to achieve the same effect. Also note that the do_word_counts function takes an RDD as an input which we can create with SparkContext’s handy parallelize method. That completes our first unit test. Here is another example for testing a function using the Dataframes API which is also used by Spark SQL. This function counts the occurrences of a name in a dataframe (read from a bunch of json). Its even more important to test functions that consume semi-structured data. It is really easy to misspell the name of a field in a json blob or forget that a field is actually optional. In the best case, this blows up in production. Worst case your pipelines silently do nothing. The messier the data you can run through your tests, the happier your oncall engineers will sleep. To test our json counts function, we need to create a HiveContext test fixture so that we can read in json using its nifty jsonRDD function. The rest of the function is similar to the word counts test. This final example tests a Spark Streaming application. Spark streaming’s key abstraction is a discretized stream or a DStream, which is basically a sequence of RDDs. Here is a streaming version of our word counting example that operates on a DStream and returns a stream of counts. You guessed it, we need a StreamingContext test fixture . We also need to create a DStream with test data. StreamingContext has a really nifty queueStream function for just that. Here is the complete unit test for streaming word count: There is a lot of code here and a bit scattered. Its easier to get it from the pyspark.test git repo. You are also welcome to Fork the repo and contribute more examples. You can also write spark unit tests using the unittest2 framework as the spark-testing-base package does, but IMHO using py.test is a lot more concise, cleaner, and more powerful. Nextdoor is a really fun place to work and a high impact product. We are connecting neighborhoods in unprecedented ways while solving really hard and interesting problems. If you are passionate about using big data, machine learning and the like to make a meaningful difference, we are always hiring . nextdoor.com Nextdoor is the neighborhood hub for trusted connections… 371 6 Thanks to Daisuke Fujiwara and matt . Apache Spark Big Data Python Unit Testing Open Source 371 claps 371 6 Written by Engineering Lead at @Nextdoor Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Engineering Lead at @Nextdoor Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-24"},
{"website": "NextDoor", "title": "what your mocks do when you aren t looking", "author": ["Andrew Brown"], "link": "https://engblog.nextdoor.com/what-your-mocks-do-when-you-aren-t-looking-b278e0d9e201", "abstract": "Culture Open Source Technology At the end of your test suite, have you ever thought to ask where your mocks are? If this prints 0 for you, give yourself a pat on the back. If not — or if you just want to learn more about how to manage mocks — please read on… At Nextdoor, we run thousands of unit tests. Our continuous integration workers have a lot of cores so we can still verify changes in minutes rather than hours, but the lone vagrant instance on my Mac is not so generously equipped. I recently noticed that it took a long time for unit tests to run on my local box. A glance at htop showed that the kswapd process was very active: the test suite was leaking memory. There are several python tools that can help you find and understand leaks. The simplest way to see leaked objects is to use built-in python garbage collector’s gc.get_objects() method like we did at the very start of this post. Of course, in that example, we already new what kind of objects we were looking for. The pympler library’s muppy package offers a similar get_objects() method as well as nicely formatted summaries. It can also count primitive objects such as tuples and strings, which are not tracked by the Cpython garbage collector. Here’s what leaking a lot of mocks looks like with pympler : One last great tool is objgraph , which draws graphs showing the dependencies between objects. These graphs can help you figure out which references are keeping them from being garbage collected. Here’s how to get a back-reference graph using objgraph: This generates an image showing how a reference from the stack frame would prevent garbage collection of ‘my_mock’: NB: In all cases, be sure to run the garbage collector with gc.collect() before investigating any leaks. It could be the object you think you leaked just hasn’t been collected yet. I’m sure you won’t be surprised when I tell you that near the top of the list of objects leftover at the end of the test were mock.Mock and mock.MagicMock . This was definitely unexpected. We generally create mock objects in tests to verify specific features targeted by tests and those objects should be destroyed by the end of the test. Not only is this a source of memory leaks, but persistent mocks could mean that some tests were faking out portions of our libraries in ways that subsequent tests were not expecting. For comparison, in the rspec testing library for the Ruby programming language, mocking and stubs is done with rspec library methods and they are automatically removed when a test finishes. I couldn’t come up with a good reason why our requirements were different and I definitely saw no reason why mocks should hang around after the test suite was done. Looking closely through our test suite, I couldn’t find any cases where we really needed mocks to persist beyond a test boundary. Still, I did find mock objects persisting between tests for a number of unintended reasons. This led to a crusade to reduce memory leakage and improve correctness by ensuring out test suite complied with the following rules: Test case instances shall be garbage collected. It turns out the nose multi-processing plugin has a bug that prevents test case instances from being garbage collected (one of my colleagues created this fix ). Test case instances can have a lot of stuff, not just mocks, attached to self. Failing to garbage collect these test case instances can cause memory usage to pile up quickly. Patches shall be stopped at the end of each test. Call stop() for any patches for which you’ve called start() . It’s definitely worth taking advantage of self.addCleanup() to avoid adding extra tearDown methods to your test cases. Using mock.patch.object(…) as a context manager also works well within an individual test. Here are what these approaches look like: Mocks shall be created using mock.patch(…) . Don’t monkey-patch methods and properties in tests! Some of our tests were not using mock.patch. Objects in python are very malleable and in some cases a test writer had written: Patching methods like this without cleaning them up could affect subsequent tests and can also cause tests to behave differently based on the order in which they are run. Note that this sort of behavior is a problem whether the method or property you are setting happens to be mock or just some other ordinary object. That said, we often were using a mock in these cases, so we replaced them all with mock.patch(…) . Lesson: Use mock.patch… it’s designed for this sort of thing and makes cleaning up easier. Mocks shall not be cached. You may discover that there are caches you don’t know about or are not clearing between tests. For example, consider: This creates 12 additional mocks which won’t even be released when you call urlparse.clear_cache() . See this example: Create no mocks before your test starts. Some of our tests defined top-level mocks in tests using the @ patch decorator from the mock library as follows: In the example above, mock.Mock() is called before the test is run and the instance it returns can potentially be passed to multiple tests. I don’t recommend re-using mocks between tests, but if you do try to re-use them, you should at least plan on cleaning up your mock after your test using the reset_mock() method. If you don’t, another test could read stale metadata about how the mock was called (i.e.the called , call_args , call_args_list properties of your mock). Not only could this interfere with both the correctness and the determinism of your test suite (your tests could be come order dependent), but failing to call reset_mock() can actually leak whole chains of mocks. In the python mock library, calling get on a undefined attribute from a python mock returns a new mock. Given this behavior, it’s not hard for a test to create many child mocks from an initial parent mock, none of which are garbage collected because the parent retains references to all the children. So why not just inline the creation of the mock? Say you wrote: It’s definitely harder to re-use the return value of the mock in this situation, but the decorated test method will still retain a reference to the mock, to any children that were added to that mock, and to all values ever passed as call arguments to any of those mocks. You could still call reset_mock() explicitly on the mock you pass, but that will traverse your mock hierarchy unnecessarily and you might just be better off letting it get garbage collected when it’s convenient for the interpreter. If you must use the patch decorator… If you still want to use the patch decorator but want to be sure you never re-use a mock, you can use the new_callable argument to Mock() in order to create a new mock each time: Full disclosure: We did not find all of our bad mocks by simple inspection. Not only did we want to fix these problems, but we wanted to keep them from re-occurring. So we created the Dutch-Boy plugin for the Nose 1.x test runner that detects these sort of leaks. This plugin enforces the following rules: Any new mocks created during a test must be removed by the start of the next test. Any mocks created prior to all tests must be reset by the start of the each test. While I was hard-pressed to find cases where it was too painful to create a new mock per-test, this will permit mocks to persist so long as they are created before any tests are run. You can find the source for the Dutch-Boy at https://github.com/Nextdoor/dutch-boy and download from pypi at https://pypi.python.org/pypi/dutch-boy . Feel free to try it out yourself and contribute back to it. I’d like to expand Dutch-Boy to look for more types of leaks and report memory usage deltas. Also, the rules for test hygiene I listed above should be just as true for tests run by pytest and any other python test frameworks, so I plan to look at creating plugins for other frameworks. Like this post? Be sure to recommend. Follow Andrew on Github and Medium . Make your impact on Nextdoor’s engineering culture . Nextdoor is the neighborhood hub for trusted connections… 30 3 Python Testing Nextdo Technology 30 claps 30 3 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "building vpcs with cloudformation templates", "author": ["Matt Wise"], "link": "https://engblog.nextdoor.com/building-vpcs-with-cloudformation-templates-c5d2c1f8c10f", "abstract": "Culture Open Source Technology Back in August of 2015, I wrote about how we were Migrating from EC2 Classic to multiple VPCs . I mentioned that we did it with CloudFormation as our automation template, and promised to actually share those templates. Well, time flies, doesn’t it? Today, I promise, I’m going to share the CF stacks…my apologies for taking so long! First though, a few updates. Back when I wrote the original post, Amazon did not have any mechanism for managing NAT instances — so much of our CF stack was focused on building LaunchConfigurations, AutoScalingGroups and IAM roles for these home-grown NAT instances. As we scaled our cloud presence, we found at one point that we were running 70+ NAT instances around the world in various accounts. Sure, they were mostly hands off — but it was an expense and complication we didn’t like. Well, just a few months after we built our initial framework, Amazon announced managed NAT Gateway instances! Yay! The only problem? No CloudFormation support for them. This was a blocker for us, as we didn’t want any manual steps in creating our VPCs. Fast forward over a year, and Amazon announces CloudFormation Support for Amazon VPC NAT Gateway ! Finally, a reason to update our CloudFormation stacks, purge all those old NAT instances, and leverage the NAT Gateways. Another interesting thing happened while we were waiting on these NAT Gateway instances… Nextdoor launched in the Netherlands ! This was a huge project for us across the company. Translating our site, iOS and Android apps and deconstructing assumptions that were US-specific are just some of the technical challenges we faced. On the infrastructure side though, we also made the decision to launch a completely new EU-focused set of services rather than sharing our existing US-based farms. I won’t bore you with the details of launching new servers, security groups, etc. — we’ll save that for another time. Instead, let’s talk about our uberBastion hosts. If you read the Solving Easy Administrative Access part of our VPC blog, you’ll remember that we built a series of TRUST VPCs (one in each region we have services) that each had a VPN tunnel to all of the other regions. At the time, that meant that our uber-bastion-uswest1 host had VPN tunnels connecting it to uber-bastion-uswest2 and uber-bastion-useast1 . Combining this with local VPC Peering allows us to provide our engineers with secure global access to all of our resources, while also ensuring that services in one VPC cannot reach another VPC. Well … how does this “three ringed circus” of a VPN setup work when you scale up to five regions (adding in EU-West-1 and EU-Central-1)? Simple, you add a few more lines to your Puppet code and launch some servers. We’ve been running this now “five ringed circus” of VPN tunnels and bastion hosts for 6+ months and it’s worked great. No matter where our engineers are in the world, they VPN into the closest uberBastion host they have access to and from there they can reach any servers they need to work with around the globe. All access is encrypted multiple times (SSH on top of VPN), tightly authenticated, and very explicitly routed through our tunnels. We’ve created a CloudFormation stack that creates our VPC, Private and Public Subnets/Routing Tables, Internet Gateway, NAT Gateway and even S3 Endpoints. The JSON is available at our repo below, but if you make any changes to it, you’ll have to upload it to your own S3 bucket due to some CF limitations. github.com Launching a new VPC is as simple as entering in the Name, Network CIDR and finally the individual subnet CIDRs you want to build. If you omit any of these CIDRs, then that subnet won’t be created… so if your account only has access to Zones A and C, you can easily only identify those zones in your config. Here’s an example of creating a new VPC in the AP-Northeast-1 region with Public and Private subnets in Zones A and C: Once we create the stack, we can see the VPC, NAT and Internet gateways, Subnets and more all created for us! We know that there are many great automation frameworks out there for this kind of stuff (Terraform, Ansible, Puppet, etc.), but frankly CloudFormation was the simplest thing we could do at the time. Almost two years later, we’re still happy with it. One of the biggest benefits of doing this in CloudFormation is that when we’re done with a VPC, tearing down the entire thing is as simple as destroying the stack in the UI. The code is available to use, steal, leverage, etc.! If you’re interested in working on these kinds of problems, we’re hiring ! Sr. Systems Architect Nextdoor is the neighborhood hub for trusted connections… 32 Thanks to Vikas Kawadia . DevOps Cloud Computing Security Open Source 32 claps 32 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "meet a co founder prakash janakiraman chief architect", "author": ["Sean Bromage"], "link": "https://engblog.nextdoor.com/meet-a-co-founder-prakash-janakiraman-chief-architect-ce735e88e15c", "abstract": "Culture Open Source Technology Prakash Janakiraman is Co-Founder and Chief Architect at Nextdoor. Prakash began his career as a software engineer at Excite@Home in 1996 before moving into engineering management at Shopping.com and Epinions. Prior to Nextdoor, Prakash spent nearly three years at Google, where he managed engineering teams for Google Maps, Google Base, and Froogle. Prakash is from Hayward, California, and attended the University of California, Berkeley. This week we sat down with our Chief Architect to get the lowdown on what makes him tick. Prakash is known for his charisma both in and out of the office. He has the unique ability to put a smile on anyone’s face, but can cut to the core of burly technical challenges without batting an eye. Lets learn more about him. I’m here today, as a co-founder of Nextdoor , because of the incredibly talented people I met along the way in my career. Every job I’ve worked at, I’ve always prioritized learning above all else; the best way to learn in your career is to work with really great people who are generous with their time. At my first job out of school at a company called Excite, I had the privilege of working closely with David Sze , who I consider a dear friend and mentor — he’s now on our board of directors. I was fortunate enough to meet my co-founders Nirav Tolia and Sarah Leary at a company called Epinions back in 2002 which Nirav had co-founded. Through the ups and downs of that company, we got to experience firsthand what working in the trenches with one another looked like and we developed a strong bond. Our first few employees at Nextdoor were all close connections, either people who had worked for me in the past or were referred by close friends. It may sound trite, but building your network of great people ultimately is one of the greatest investments you can make. Every opportunity I’ve pursued since my first job was introduced to me through people I had worked with previously. My father was the one who introduced me to programming back in the 1980s, first on a tiny microcomputer called a Timex Sinclair 1000 , and later on a more powerful IBM PCjr . Despite working in finance, he was always interested in computers — building them, programming them, reading about computing, going to computer expositions. He actually wrote a program that trained me on how to touch type, he taught me how to write my first programs in BASIC when I was just 9 or 10 years old, and then later he introduced me to Pascal and C programming. What really got me excited about programming was discovering a predecessor to the web, bulletin board systems (or BBS for short). We would use dial-up modems to access these BBS sites and exchange software — usually video games! Through doing so, I met a whole community of folks who were actively developing BBS software on an open-source platform called Forum-PC. I soon began to operate a BBS of my own, running a customized version of Forum I’d hacked up. By the time I got to college, I found myself well ahead of the curve in terms of my understanding of how to write software — all thanks to the early exposure my Dad had shared with me. One of the specific parts of our culture that I really love is our Engineering Co-Ops. We encourage our engineers to self-organize into small teams that work on various aspects of our engineering culture. Through our co-ops, we’ve given rise to our engineering blog, organized events for women in technology, brought greater focus and attention to performance issues, and developed new features and functionality for our service. Unlocking the latent creativity of our team and tapping into their inner motivation is a powerful tool for improving our product and our company together. I very fondly remember the day of our public launch, back on October 26, 2011. Our small team had been cranking away for a little over a year building the product and tapping our network of friends and family to bring their neighborhoods online. At the end of that first year in private beta, we had 176 neighborhoods in total, and it felt like we had worked really hard for every single one of them. By the end of our first day publicly launched, nearly 200 additional neighborhoods had come online, thanks to some great press in the New York Times. Watching our map of neighborhoods start to light up and observing the traffic to our site increase was a really proud moment for the team. I remember going home that night feeling really inspired and feeling like we were on to something big. I love sports, especially basketball. My wife and I have had season tickets to the Warriors for over a decade now, and it’s been incredible to watch their ascent to NBA Champions. I’m also a supporter of the men’s basketball program at my alma mater, UC Berkeley, and it’s been great to see the resurgence of the team this year as well. Anyone who knows me well knows I bleed blue and gold! Like any other discipline, it’s important to continually find ways to stay inspired. For me, that means spending time with people who are in similar roles at other companies who I can learn from and trade experiences with. Having a community around you where you can authentically ask questions, no matter how banal, and get help and advice without judgement is really important. We’ve all made our mistakes, and it’s great to be able to learn from one another. Secondly, staying connected to the core technology and understanding how new technologies can give your company any advantages in efficiency, scale, or development velocity is really important. Find time to read and stay abreast of what’s happening outside of the four walls of your office! As you’re building a team, you always feel the urgency to get the next seat filled and to bring a productive new employee online. But finding the RIGHT person often takes time. Articulating your core values and being able to quickly pattern match incoming candidates to those core values is tremendously helpful for finding people who aren’t just functionally strong, but are great matches for your team and culture. There’s plenty of people who can write code to satisfy the requirements of a spec; but you need ways to evaluate their ability to communicate well, collaborate with others, make decisions autonomously, and come up with great new ideas. I look for engineers who are intrinsically motivated, rabidly curious, great communicators, hard working, and resourceful. At startups, all of these traits are particularly important in a world where you’re often resource constrained, having to innovate quickly, and needing to find creative ways to achieve goals that often seem difficult to achieve. Like this post? Be sure to recommend. Follow Prakash on twitter and Medium . Make your impact on Nextdoor’s engineering culture . Nextdoor is the neighborhood hub for trusted connections… 35 2 Thanks to Vikas Kawadia , Daisuke Fujiwara , and Wenbin Fang . Startup Entrepreneurship Nextdoor Culture 35 claps 35 2 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "using puppet to set up docker distribution and portus", "author": ["Matt Wise"], "link": "https://engblog.nextdoor.com/using-puppet-to-set-up-docker-distribution-and-portus-17e41143af17", "abstract": "Culture Open Source Technology Here at Nextdoor we’ve been using Docker for about 18 months in various development-focused use cases — and even in production in a few small places. In recent months we’ve completed a large transition of all of our non-Django services into Docker, leveraging Amazon ECS as our control plane. We are now running 10 unique services in two countries across multiple development and production environments, deploying code in minutes, and loving life! At the heart of every great Docker deployment is a great Docker Registry (or Hub). If you’re able to publish your Docker images to the public Docker Hub service, that’s fantastic. However, if you have a need to keep your images private and hosted “internally”, you’re going to find yourself needing to run your own Docker Registry service. Until last week, we ran the traditional Python-based Docker Registry service that has been around for a few years now. It’s simple, reasonably stateless (when configured to use S3 as its backend), and pretty painless to set up. We’ve previously blogged about how we leverage Puppet for our system configuration, so setting up the initial Docker Registries looked something like this for us: Setting up the registry was easy — but if we wanted to provide any real separation between our Dev, Staging and Production namespaces, we had to run three registries and then migrate images back and forth between them. If you ask our developers, this was rough. If you ask the admins who ran these registries, it was worse. On a side note, if you’re curious about the getsecret() method, see our Thycotic Puppet module. When we began our transition to really using Docker in a significant way, we knew we had to come up with a better system. Thankfully, we didn’t have to look too far. The newest Docker Distribution (aka Docker Registry v2) service has been available for a while and is gaining a lot of traction. Beyond the fact that its been re-written in Go to be much faster, the Docker team has added a ton of goodies to the service to make it much more reasonable to run in a large organization. The two biggest features for us were Bearer Token Authentication and S3/CloudFront integration . With these two features in place, and the new and very cool Portus project, we are able to build a new Docker Registry for our teams thats secure, fast, and provides uniquely authorized namespaces for different teams and projects! Even better .. we can run a single registry server, but provide extremely high availability and performance! It took us a few days to get everything working properly, so we thought we’d share the actual Puppet code we use to launch the service and configure it. The code above is a near copy of what we use — but obviously there are a few resource types in there that you’ll have to replace with your own (mainly around how we manage Apache). Fundamentally, though, the model is simple. Docker Distribution acts as a layer management service, and hands out Signed Amazon Cloudfront URLs to the Docker Clients, allowing them to download the files fast, and for them to be cached at all of the cloudfront endpoints. Portus acts as our authentication and authorization service, providing our employees with different groups that they can work in without clobbering other teams. Finally, S3 is the backend of course. There’s a ton that we could talk about here — but if you’re interested in this set-up I strongly recommend you check out the Docker Distribution github project and the Portus github project . These are both great projects, and I really want to send a thanks out to everyone on both teams for helping us get this up and running. Nextdoor is looking for great Engineers! Check out nextdoor.com/jobs if you want to help make neighborhoods better! Nextdoor is the neighborhood hub for trusted connections… 10 Thanks to Mikhail Simin . Docker DevOps Puppet 10 claps 10 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-03-04"},
{"website": "NextDoor", "title": "diving into ops", "author": ["Mikhail Simin"], "link": "https://engblog.nextdoor.com/diving-into-ops-962589436aad", "abstract": "Culture Open Source Technology I started learning software in ’98 when I came to America at the age of 13. My only experience prior to this serious attempt was BASIC and FORTRAN which I learned starting at the age of 7. My first interaction with languages was HTML, closely followed by JavaScript and CSS. I was starting to be known for quality static pages with some DHTML love, and would design and develop them for everyone from family to friends and even my middle school. I was always a hacker. I never learned through normal channels, and just like with everything else — learning a new language was a hack. I found existing chunks of code and modified them until they made sense. By 2000 I knew the bleeding edge of front-end development and a little bit of PHP3.In high school my interests included most science related classes, and I avoided literature and biology. This was mainly because back then English still felt like a foreign language. Quickly through the years I became known as a computer nerd. While this had a negative connotation in jock-world, it was positive amongst the elder folk that were trying to use Microsoft Word to create a flyer. By the end of high school I learned Visual Basic, a little bit of Java, some fundamental Linux commands, and vim. College included C, more Java, the newest front-end advancements and databases; grad-school — Perl, Python and C++. As I continued learning software languages a few things became obvious. There are certain things all languages do alike. Therefore there’s a certain skill set that transfers between all languages. For this reason there is an entire wiki page for Hello World programs in various languages. On the other hand many features are language specific. Language-specific skill set does not transfer, but does open up your mind to a different method of thinking. For example in Python, this would be decorators and generators; in Ruby it would be symbols; in Go — channels. Of course some of these things have language counterparts or libraries that allow for the same result. However, the important thing about learning them is knowing what is possible. One thing that I took for granted throughout all of my experience, and I never thought twice about it, is that at all times I was working with incomplete products. Being a software engineer means your tasks are to create something from scratch, or modify something existing. Either way, as soon as you start working, your software is incomplete. Incompleteness of your own software means something is not working, and it can have only two results: you either know why and are about to change it, or you have no idea why and you have to debug it. If you are a novice developer, or the task is very simple: debug means change something and try again. A more sophisticated debugging session usually involves some interactive trace runner, whether it’s your browser’s console for JavaScript, gdb for C++ or pdb for Python. When I first joined the Ops team as a DevOps liaison I tried to estimate how long it would take me to get onboarded and become a ninja. I expected there to be a learning curve presented by the syntax of new languages. I also expected some extra Linux tools that aren’t needed unless I’m debugging something. I estimated three weeks. I was wrong. Being “handy” in software development means knowing algorithmic approaches, quirks of a specific language, or libraries available for that language. This may also apply to frameworks within a language, such as jQuery in JavaScript, Django in Python, or Ruby on Rails. Becoming excellent in a small area of software development often helps you become better in other areas. In operations being handy implies knowing what programs/scripts/commands Linux has, where services save their logs by default, where their configs are by default, common permission issues, quirkiness of particular daemons, how they interact with the OS, and a number of other things. This is a severe difference between working in an environment that you or your peers have created and own, and trying to get something done using tools that someone else created for their own needs. To figure out an unexpected behavior you often do not have an option to look at the source code of a program. Nor would you want to! The first assumption has to be that things are working as intended, and some configuration is messed up. So how do we debug that? I can’t enumerate the millions of lines of logs that I’ve sifted through, tailing and greping, seding and awking to get what I want out of someone else’s logs. Watching two, three logs at the same time to observe some race condition; scrolling through thousands of lines of logs in two separate files to align timestamps and figure out why something didn’t get installed; just seeing that “something” is happening is sometimes good enough. A good log is a difference between pulling your hair out to solve something and it being effortless. Logs exist outside of time. You don’t have to reproduce the crash if you have good logs. You just have to read them! All code starts small and grows into something big. At some point, by design or by demand, the code needs to be split into services. It is often a rather painful procedure from both perspectives: engineering and product. For engineers it’s tedious to invest into developing an API and segregate code that’s coupled very tightly and works well. For product it’s a dedication of time that improves nothing; if all goes well users will not notice a thing. This maturation is a natural flow of events, and it happens in Ops as well. It is likely that at one point or another you ran your database on the same machine as you ran your code. This works great when you have a small number of users. As demand scales your application “overheats” unevenly. This forces us to think about individual components as services, and to scale these services accordingly. The scaling portion here is referring to horizontal scaling of hardware, not optimizing if statements inside of loops. With horizontal scaling in place, the health of the system becomes measured by the cumulative health of servers in clusters. If the entire cluster is healthy except just 1 server, chances are the code is OK but the actual server is faulty. We don’t always go debugging in these cases; sometimes it’s just not worth the time. Kill the faulty server, and launch a new one — that is the cluster way of solving problems. If you have an opportunity to transition to your DevOps team there are a few key things you will notice very quickly: On call experience There are several major differences between general engineering on call and DevOps. The DevOps team is usually significantly smaller. A smaller team means that your rotation schedule is not the same. Being on call in 3-day rotation when your entire team is only 3 people (vs 40!) would mean that you rarely get a weekend off. My team agreed to do 7-day rotations. When you do get paged, something is really bad (or nothing at all!) You get paged for issues that are related to servers — not the code. Sometimes the two are causal. Out of swap space? Some code has a memory leak! But sometimes the issues are not due to code that you control. You may see issues with network connectivity issues, certificate expirations, or RAID mounting fails. This said, you cannot afford to have issues! Building stable infrastructure is absolutely paramount. And doing so correctly will allow you to be paged zero times on a week-long on call (true story!). Change of mentality Stability and reproducibility are your number one priority. Hacky, one-off solutions are a recipe for failure. This knowledge is not intuitive until you have a chance to value stability, and experience the pains of not having it. Once you can sleep at night knowing that all systems are guaranteed to keep running, it becomes easy to think about other important factors. Defining a standard vs. allowing teams their own choice. Breaking up monolithic server definitions into reusable modules. Lowering server cost vs. providing growth headroom. These and many other considerations become part of your everyday thinking. Relationship with developers Some engineers envision a solution and have expectations of what the system infrastructure ought to provide. You might be approached with a request to double the cache cluster. It is often the DevOps job to provide some friction in those knee-jerk requests to ensure that all the appropriate considerations were made. Perhaps the request is valid. Or perhaps the engineers are unaware that the current cluster is already 3x over-provisioned. This friction becomes even more critical when a request is for a brand new part of infrastructure. Never cave-in to a shiny-toy syndrome. This works both ways. Upgrading a hosted service has consequences on engineers, and the communication there must be at its best. Whether you are enabling new features, or disabling some deprecated ones — you must get a green light from the people that actually leverage the service. Imagine flipping a switch on some authentication module and consequently breaking all automation scripts that expect a certain format. DevOps Engineering (or Systems Infrastructure as we are increasingly called) is still engineering , and some skills from your classical software education and experience certainly transfer. It is, however, a whole new layer of thinking and a different pace of work. Most importantly — it is fun and fulfilling to create stable infrastructure used by all engineers in the company. Get a kick out of stable systems? Want to work to automate everything ? Check out our jobs page. :) This blog was originally posted at engblog.nextdoor.com Nextdoor is the neighborhood hub for trusted connections… 4 1 DevOps Engineering Culture 4 claps 4 1 Written by Reverse Engineer Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Reverse Engineer Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-01-15"},
{"website": "NextDoor", "title": "upgrading to the new permissions system in android 6 0 marshmallow", "author": ["Grant Zhu"], "link": "https://engblog.nextdoor.com/upgrading-to-the-new-permissions-system-in-android-6-0-marshmallow-839046f34356", "abstract": "Culture Open Source Technology We at Nextdoor like to keep up-to-date with new things. We recently got around to updating our Android app to support the latest version, Marshmallow. Part of this update is to accommodate the new permissions system. Playing well with the new permissions system also lets us present a clearer message to users. Where we would previously need to request a permission at install time, we can now wait until the user expresses their intent to use a particular feature to ask the user for the relevant permissions. We’ve put together some tips and advice on how to make a smooth transition for your app concerning the new permissions changes. As with most apps, Nextdoor uses a number of permissions that we declare in AndroidManifest.xml. It was fairly straightforward to use our existing manifest to help pin down which permissions we would needed to audit our code for. Then, we filtered down the permissions declared to only the specific ones that require run-time prompts to request permission from the user. We repeatedly referred to the Android permissions table to help find code that needed to be addressed. Another helpful tool that we used to find where these permission checks were needed was Android Lint. The Lint inspection you need to run isn’t quite named what you’d expect but you can run the “Constant and Resource Type Mismatches” inspection on your project and it will create a list of places that make these kinds of API calls among other things. Invitations were one of the interesting parts of our app to retrofit for runtime permissions. After a user taps the invite button, we help by suggesting people the user knows from their contacts as part of an autocompleting text field. These suggestions were read from the user’s contacts. In a world of runtime permissions, we needed a number of different behaviors. While the Android docs describe an algorithm for requesting permissions, it doesn’t list the permissions states explicitly. Listing these makes reasoning about the expected user experience much easier. In our case, we needed to implement the “Permission denied” and “Permission permanently denied” states to give the user adequate feedback. We use a toast to show explanations when needed. We also disable the feature entirely if the user has permanently denied access to contacts: If you are not already using AppCompatActivity or FragmentActivity as a base class for your activities, you might want to consider it for the added benefit that these classes already implement the AppCompat.OnRequestPermissionsResultCallback interface, otherwise you will have to have implement this interface manually. This interface provides the onRequestPermissionResult callback method, which is how the Android permissions system notifies you about the result associated with your permission request. You can read more about this here . The way to express that you would like to opt into the new permissions model is to update the target SDK version to 23 (Marshmallow). In addition to the work needed to directly support the new permissions model, some code needed to be modified for compatibility in other ways. The largest change that needed to be addressed was a number of unexpected uses of the Apache HTTP Library. The Apache HTTP Library has been recommended against , then deprecated, then removed outright in Marshmallow. Converting existing project code to use HttpURLConnection is reasonably straightforward, but we found a number of unexpected places where we needed to take extra steps to fully squeeze out last usages. Make sure any libraries that you are using, third-party or otherwise, support target SDK 23. The reason for checking is that libraries need to do the right thing when a permission isn’t available. For example, we updated our google maps library so that it can handle the case where location permissions have not been granted. Since the org.apache.http package was removed , we discovered that a few of our third-party libraries had this dependency and while most issues were resolved by updating the versions for these libraries, there was one that we had to actually work with the developer on a workaround while they also made the corresponding update. You may need to work with library developers to get around similar issues. Finally, we needed to update our testing framework. We use Robolectric to provide a mock implementation of the Android SDK. Robolectric 2 uses the Apache HTTP library to its core — several core Robolectric classes extend Apache HTTP classes. Adapting the library to work without Apache HTTP is a non-starter. While we could have included the Apache HTTP library as a dependency, we used the opportunity to upgrade to Robolectric 3. The changes we had to make are outside the scope of this article but may come as a separate article in this blog. Once upgraded, the core dependency on Apache HTTP was removed. Being a good citizen on a platform means doing things the way the platform suggests. The permissions model changes in Android 6.0 Marshmallow are some of the biggest changes to the platform in years. We strive to make the Nextdoor app one that is useful to neighbors, and one of the best ways we can do that is to make the app do the right thing on our users’ devices. If you like working on modern apps like ours, we’re hiring . Authors: Grant Zhu and Wesley Moy Nextdoor is the neighborhood hub for trusted connections… 11 Thanks to Wesley Moy , Mikhail Simin , Daisuke Fujiwara , and Vikas Kawadia . Android Android Marshmallow Android Permissions 11 claps 11 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-01-06"},
{"website": "NextDoor", "title": "a powerful way to integrate hubot with slack", "author": ["Abhijeet Kumar"], "link": "https://engblog.nextdoor.com/a-powerful-way-to-integrate-hubot-with-slack-5a3e4ab4eac9", "abstract": "Culture Open Source Technology Communication through messaging is integral to our lives. Here is how Nextdoor uses messages for useful automations. After Nextdoor started using Slack for chat, many things evolved: a huge number of webhooks got set up, over 250 rooms were created for topic-specific discussions, our emoji-fu has gotten much stronger, and we’ve started using SlackBot for witty responses. Of course, witty responses don’t help productivity. We wanted in-Slack helper tools that would do common things that people do already, performed in an automated way. Like many other companies, we have a few services for tools and development which are both hosted and in the cloud. Not every service works well on a mobile interface. If through Slack we could issue commands into the cloud it would bridge the gap on mobile by providing an easy to use interface — messaging. So what can be useful, customizable, and work through messaging? A light-weight bot has come to the rescue which we named Neighbor. Slack Although Slack was already in place and not a part of the decision made during this project it still proved to be an excellent platform for our implementation. Slack allows bot users with a well documented API, and more than just RichTextFormat posts. Hubot Developed by GitHub, Hubot already has a Slack integration, and is easy to develop new features in Node.js (CoffeeScript). New functionality can be added by simply adding a new file in the scripts directory. This flexibility allows for very swift collaboration among multiple engineers. Redis For Hubot’s “brain” we use Redis data-store. To minimize management headaches, we’ve launched a Redis instance in AWS ElastiCache on the smallest instance. Github Enterprise (GHE) Like many our internal projects we start development on GitHub Enterprise and then opensource to github.com. Using GHE allows for easy collaboration on random ad-hoc projects without impacting any other repo. Docker via Rancher Containerization makes sense. Hubot is built on node and has very few dependencies. It was reasonably trivial (and fun) to dockerize Hubot. Here’s how our Dockerfile looks For demonstration purposes we’re showing the REDIS_URL setup. It has a unique FQDN format that is specific to Rancher’s flexible method of linking containers and external services. We could easily launch a new Redis service or run Redis as a container without having to change any code in Hubot service. Rancher makes it easy to leverage our auth (GitHub+LDAP) and private docker registries. We can link any container to the external Redis service in a way that is transparent to Hubot itself. Rancher also has an extensive API which allows for easy deployments. Our rancher nodes are hosted on AWS on t2.large nodes for cost efficiency. Here’s how our Neighbor “stack” looks on Racher with dedicated Hubot and Redis services. The redis “service” has no containers because it’s a simple pointer to a DNS record on Amazon. CircleCI as the glue Speaking of deployments — we’ve leveraged CircleCI for our tests, builds, and deploy tasks. Shortly after merging code on GHE, CircleCI builds the docker image, tests that Neighbor is functional, pushes a new version to our private docker registry. We then perform an in-service upgrade on Rancher with the new Neighbor version. Putting all the pieces together was effortless, and we had the “hello-world” version running in less than an hour. Quickly engineers throughout the company started contributing to expand Neighbor’s functionality. We already have quite a number of helpful actions that Neighbor does. Here are a couple of examples: Librato check Neighbor passively listens to chat in some of our rooms and saves “annotations” in Librato when it sees the #librato hashtag. If you’d like to use this script — it’s open source: librato.coffee Mobile App version checks More complex examples In an upcoming blog post we’ll talk about Neighbor’s interactive communication for organizing our company’s Lunch Roulette. Neighbor actually asks questions and does different things based on your answers. The interface gap between tools used by engineering departments and our neighborhood operations team is narrowing. Messaging remains the lowest common denominator for automating chores and it is available on all platforms. Slack combined with Hubot allows to automate chores for typical (and atypical) workflows. We believe there is a huge potential to put various information systems at employees’ fingertips at Nextdoor and Neighbor has an important part to play. If you care about tools that empower engineers, get in touch — we’re hiring ! Authored by Abhijeet Kumar and Mikhail Simin . Nextdoor is the neighborhood hub for trusted connections… 13 Slack Github Docker Technology 13 claps 13 Written by @Nextdoor, @workspotinc Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by @Nextdoor, @workspotinc Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "functional testing with nightwatch django and docker", "author": ["Nextdoor Engineering"], "link": "https://engblog.nextdoor.com/functional-testing-with-nightwatch-django-and-docker-cd82a375ce55", "abstract": "Culture Open Source Technology How custom framework extensions to Nightwatch, along with Docker based automation help Nextdoor write functional tests quickly and efficiently. At Nextdoor, we maintain a culture of collective responsibility for the quality and functionality of our product. This means that the engineers or teams who develop a product feature are also responsible for maintaining the quality of that feature. So, how did we verify the functionality of features before we had a functional testing framework? We did manual regression testing of course. This was an extremely tedious and error prone process! We obviously needed a framework to automate our regression testing. After trying out several frameworks we settled on Nightwatch.js . We had an initial enthusiasm for, and some rapid adoption of, the new functional testing framework by our engineers, and many tests were indeed written. But it quickly became apparent that we still had a problem. With few exceptions, most of our engineers are not specialists in a testing discipline and are otherwise very busy. While much of the core functionality in nightwatch was straightforward, the complexity of setting up the test environment, preloading the test data, and bootstrapping the various conditions to setup a specific test were still a tedious and far from easy process. Furthermore, the tests were hard to troubleshoot, fairly brittle, and hard to read. Therefore it was necessary for us to further refine the framework and deploy additional testing automation and extensions that will significantly reduce the learning curve for writing functional tests while simultaneously packing enough punch to cover the diversity of our testing needs. In this post we describe two extensions we build on top of nightwatch to make writing functional tests easier. We will also describe the automation we build using docker to allow us to quickly and deterministically launch application servers to test against. To address the verbosity of nightwatch and the ambiguous and often non-specific error messaging associated with many failure scenarios, we developed test wrappers to surround our test modules. Wrappers are a good way to clean up, cut down and make your tests more readable and resilient. In them we perform these critical actions to make sure the test author only focuses on what the test is responsible for: Error handling: Since nightwatch is primarily built on node.js we use domains to handle asynchronous errors and report meaningful error data. This is needed so the process does not crash abruptly on exceptions and the test suite can run to completion. Data setup & cleanup: The functional test database is populated with default test data during every test run. Options to customize the data can be defined by the test author using declarative JSON. We will talk more about this in the next section. We also do a data cleanup process between test modules that is triggered by the wrappers. Error screenshots uploaded to s3: We use phantomJs for the basic functional test executions. Debugging can be tricky in a headless browsers. When there are failures, the wrappers trigget a screenshot capture of the error, uploaded it to s3, then pipe the image to the channel of communication to notify test authors. Functional test reports: A HTML report with the test suite execution statistics is uploaded to s3 and includes test run time, failures, number of tests etc. In addition to the test wrappers, we also develop and maintain a library of custom commands that define common user actions on the site (e.g. login, joinGroup, createEvent etc.). We found this to be an incredibly powerful and convenient practice which makes test code more succinct and comprehensible. One of the issues we had to solve was being able to specify how the environment should be set up prior to a test. It’s important to make sure that the database is consistent yet customizeable. For example, let’s imagine a test for post deletion. The user should be able to take a post that already exists on their feed and delete it. The test could create a post first through the web interface. In that case, what happens if the post creation flow is broken? The deletion test will fail, but it won’t even get to test what it’s responsible for. A good rule for writing tests is that tests should have a narrow field of responsibility ( Separation of Concerns ). If post creation fails, the creation test should fail, not the deletion test. It would also take longer. Imagine if we were to rely on the post creation test to lay the ground work for the post deletion test. This approach would prevent any amount of parallelization, because now the tests aren’t isolated units but dependent on one another. Additionally, if the post creation was broken, we wouldn’t be able to run the deletion test at all. If both were broken, we wouldn’t find out about deletion being broken until we fixed creation. We want to know everything that’s wrong, as soon as possible. What we need is for a test data setup process that is completely independent from other tests. To this end, we designed a JSON schema that can be used to specify what customization to make. Here’s an example of a JSON object for the post deletion test. We set that JSON from the functional test and pass it along to create_setup. Then, it is processed and handled by the database. You may wonder why we didn’t go with different application server endpoints to set up the data that can be called directly from the test code. We do have those endpoints available as well, but they were verbose and hard to parse when looking at a test. We wanted to maximize readability and minimize server calls. The direct server calls are still useful for certain cases where state can change multiple times throughout a test. Another thing we wanted was a way to have a validated and consistent schema for this JSON data. This allows us to immediately fail tests if their JSON options object does not match the schema, without having an ambiguous 500 from the application server. We created a JSON validation library for this. The definition for the post creation schema in our library looks like this: If the JSON passes the schema validation, it’ll be silent, but otherwise it’ll pretty-print the issue with the JSON. Let’s introduce some errors and see. When run through the validator, it prints: This helps speed up developer velocity. This also helps when changing the schema, by ensuring that the changes are reflected in all the tests. In order to have deterministic, consistent, and repeatable testing results, we need to start each testing run from an identical state, i.e. the database schema and data must be the same. Because our application is written mostly in Django which is a Model-View-Controller (MVC) system , this means that the database schema is generated from the data model definition which is part of the code. Since the code is constantly changing — including the data model — we frequently have to recreate the database schema prior to running the functional testing suite. Like any sufficiently complex web application, recreating the Nextdoor database from scratch is a multi-step and involved process. Fortunately we do have this process automated. Unfortunately these scripts are rather time consuming — at least for an operation that is intended to be run frequently and interactively. We did, however, observe that changes to our data model are much more rare compared to general code changes. This means that we don’t have to recreate the database for all types of code changes — only ones that involve modification to the data model. In other words, we can recreate the database — schema and data — and snapshot the result, then create and run a clone of the snapshot in between successive functional tests. To accomplish this, we utilize Docker . We start by creating an image based on our database specification. We create and start a container based on that image. We then run our schema and data creation scripts against that instance. Finally we take a snapshot of the running container and save it as an image which becomes the basis for launching. This process looks something like this: In between functional test executions we simply destroy and create a new docker container based on the image we created in the previous step Functional tests can be slow as they test user experience. Using a headless browser speeds it up to an extent, but it can still be slow compared to unit tests or integration tests. This mandates the tests to run in parallel in order to scale well. We recently started using Circle CI for our functional testing continuous integration builds. Circle CI allows us to define the environment and the set of tests, and the system will take care of creating the environment, replicating it, and then distributing the tests across the many replicas… The following sample build shows tests running in parallel across 5 nodes. Functional testing is a critical discipline to establish and automate sooner rather than later. In our experience, the mere presence of a framework is not enough. The process of writing, debugging, running, and maintaining these tests needs to be as easy and frictionless as possible. To accomplish this, it is often insufficient to rely on open sourced or public frameworks, and sometimes you will need to consider your specific engineer’s and application’s needs and customize around that. While we have come a long way in establishing functional testing as a discipline at Nextdoor, we still have some work to do. We are not yet using functional testing as an acceptance criteria for new code commits nor do we rely on them exclusively for release “go” decisions. The database initialization process takes in the order of tens of minutes, and the continuous integration job takes several tens of minutes. These numbers are much higher than where we want them to be. As a culture, functional testing remains an afterthought that is performed after a project’s conclusion and we need to escalate their priority such that they are an integral part of the development process (this will also bring us one step closer to TDD ). These and several others remain as challenges for our engineering team to address in the near future. If you’re interested in working on these kinds of problems, we’re hiring ! Written by our engineers Hussam Mousa, Radhika Parthasarathy, and Steve Mostovoy. This post first appeared on the Nextdoor Engineering blog . Nextdoor is the neighborhood hub for trusted connections… 11 Testing JavaScript Docker 11 claps 11 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2015-12-10"},
{"website": "NextDoor", "title": "engineering a culture of fluidity and learning at nextdoor", "author": ["Vivek Karuturi"], "link": "https://engblog.nextdoor.com/engineering-a-culture-of-fluidity-and-learning-at-nextdoor-85a7d1e71de", "abstract": "Culture Open Source Technology For me, part of the allure of small companies and startups has always been that the delta between what they are today and what they can become tomorrow is often very large. Bridging this delta is where the risks, inherent challenges, and rewards all ultimately lie. Teams need to be able to adapt quickly and often as the organization grows, product requirements change, resources fluctuate and different types of expertise are suddenly needed. At Nextdoor, we’ve recognized that deliberately supporting a culture of continuous learning and organizational fluidity enables us to adapt and move faster. We think it’s better for the company as a whole but also better for our people at an individual level. From the way we’ve organized our Engineering teams to the support systems and processes we’ve put in place for people to grow: these all contribute to the type of culture we want to encourage. How an organization is structured can have a profound effect on fluidity. It can influence how often people move around an organization and their comfort working outside of their comfort zone. At Nextdoor, we like to think of our organization and various sub-teams as being “loosely coupled, yet strongly aligned”. There’s no single “iOS Team” or “Android Team” or “Web Team” but rather, there are Engineers with various skillsets that work on particular products together. If I’m an engineer who works on Nextdoor for Public Agenices, for example, then I’d sit in a pod with everyone who was working on that project (from Marketing to Engineering and everything in between). I might primarily be working on the iOS component of that product, but I can also jump in and do some Web tasks or help design the APIs — whatever helps push the project forward. This prevents our engineers from thinking they are an “iOS Engineer” or an “Infrastructure Engineer”. It allows for fluidity of roles and encourages people to look to contribute wherever they can rather than only in one area. There are other great companies that we respect that do something similar. Spotify comes to mind with their detailed description of their engineering culture . So does Dropbox, which started off with more rigid roles and then had to blow that notion up in order to scale their team. Here’s an excerpt from a Fast Company article on why they thought that was necessary: Up until about a year ago, Dropbox organized itself based on platforms, with an infrastructure team, a web team, an iOS team, an Android team and a desktop client team. After coming up with a product roadmap, each team focused on the product work for its own platform. “We ran into the problem that you’d anticipate,” Agarwal says in hindsight. “We didn’t have enough cross-platform coordination, and there was a reluctance for engineers to expand outside of their core expertise. We had to blow that up.” “The key was to essentially break that model, where somebody thought that they could only work on one particular platform,” says Agarwal. “This was painful, but we had to pay that learning cost.” We’ve found that deliberately supporting our employees’ interests and curiosity is one of the best ways to grow our team and cultivate in-house expertise. It’s not some crazy or unique notion that we’ve stumbled upon, but being deliberate and recognizing that there are ways to accelerate and encourage the process is important. I’ve always been a huge Apple enthusiast, so it was natural for me to eventually want to contribute to Nextdoor’s iOS app (which is awesome by the way, you should go download it! ). However, I’d never even opened Xcode before or written a line of Objective-C, so there were a lot of things I needed to learn. Having a dedicated mentor, pair programming early and where it made sense, and deliberate managerial support were all essential catalysts and sources of support for me as I jumped into iOS. Managerial support is important because it sets the tone for the engineering culture from the top down. Having support from both your direct manager and your team throughout the process is invaluable; after all, you’ll definitely move slower when picking up something completely unfamiliar, and that’s to be expected and accounted for by everyone. Perhaps deadlines need to be moved or you need to switch teams to work on a different part of the product where you can be more experimental with your learning. Whatever it is, having an environment around you that can adapt to your “active learning state” is necessary. When I first started learning iOS, my manager came to me and gave me a bunch of resources that he felt would help my learning. He offered to let me expense any books or course material that would help me learn faster or more efficiently. In addition, both he and my team offered to give me some space in our sprints so that I had the opportunity to slow down and learn the fundamentals. Having this breathing space was vital in the long run. I was able to better understand the things I was doing rather than trying to keep up my usual pace by taking shortcuts. Counterintuitively, going slower in the beginning ensured that I can be faster in the long run. Having a healthy understanding of when it’s okay to go slow and steady is a key piece of any engineering culture. Direct and available mentorship is another important piece of the puzzle. At Nextdoor, we enable this by pairing engineers learning a new platform with someone who already has deep expertise on that platform. These pairs will sit next to each other and the mentor is the go-to person for any questions that the mentee has. If they’re on different teams, it’s no problem — one of them can switch teams for a while. It’s also understood by everyone on the team that the mentor will be spending time outside of their core responsibilities to help the other engineer. This understanding is necessary because it exemplifies the “available” part of “direct and available.” If this mutual understanding isn’t created, the mentorship is treated (however unintentional it may be) as an afterthought and the mentor will be too swamped with “real work” to take the mentorship seriously enough. Matt Johnson, a senior engineer at Nextdoor, was my mentor. Because he’d been working on iOS at Nextdoor for a while, I was not only able to learn the fundamentals of iOS from him but also about “The Nextdoor Way” of doing iOS. I also received a ton of support from the entire group of other iOS Engineers when I needed it, which was truly invaluable. I started going to the iOS “Share Your Knowledge” meetings and standups and exposing myself to their way of working. They welcomed me with open arms, but when they were too busy or swamped to help, Matt was always working right next to me to help me solve problems. A quite underappreciated and often overlooked way to quickly ramp up in a new environment is through pair programming. There are many other benefits to pair programming — in fact, there are entire books dedicated to the topic — but for now we’ll focus on how pairing helps us ramp up on and learn new systems quickly. There are many misconceptions out there about what pair programming actually is. At Nextdoor, we like the setup where two engineers share a desk and have two monitors (mirrored, of course) with two keyboards, and two mice. When working on a sizable chunk of work, there’ll be an ongoing discussion among the pair about best practices, tools, and engineering design. The process of engineering becomes a continuous dialog between the pair with one person “driving” (controlling the computer) and translating the dialog into code while the other focuses a bit more on the problem at hand and can think slightly ahead so no time is wasted. The feedback loops are very fast, and this is what enables a higher rate of learning. I had the opportunity to pair with Sean McQueen (another one of our wonderful Engineers) for a couple of weeks. This was when I wasn’t totally new to iOS, but it was clear that I still had some fundamentals to cover. Sean had been doing iOS for quite some time at Nextdoor, but he too went through the process of learning the platform from scratch since he initially started out doing Infrastructure work. During my time pairing with him, I was able to pick up some productivity tips from his workflow as well as general iOS design patterns and principles that I wasn’t totally comfortable with yet. It’s always interesting watching someone else work because we’re so used to our own ways of doing things that it’s easy to work in that silo without knowing if there’s a better way of doing things. It’s always a tradeoff between seeking to improve our processes for the long term and getting work done in the short term. With pairing, I was able to accomplish both. It’s also important to point out that this isn’t a one sided process. From Sean’s perspective, he was able to see how I approached certain problems or how I used certain tools and it ended up that we both learned from each other even though he was the more experienced iOS Engineer. Though I’ve been at Nextdoor for only a year and a half, I’ve been fortunate to have worked on a range of teams/projects encompassing infrastructure, frontend, growth, and mobile, the latter being where I spend the majority of my time these days. I joined Nextdoor right out of college (UT-Austin, hook em’ horns!), and I’ve found that having an organizational culture that explicitly emphasizes learning and fluidity is very valuable, especially early on in one’s career. It helps me achieve a breadth of experience, which becomes an important guiding light when I decide to achieve depth. Many people through college and even at the beginning of their career don’t necessarily have a good idea of what they want to achieve depth in over the course of their career. How should they? Internships are short and only offer a glimpse into real work. College itself certainly doesn’t help cultivate the right kind of experience to obtain this insight. I’ve interviewed a lot of new grads during my time here and this sentiment rings true for many of them. Unfortunately, if you’re too siloed into a role early on, then it’s easy to get get trapped into that role for a defining chunk of your career. Even more experienced engineers can experience these effects, so it becomes important to work in an environment which values learning and encourages fluid roles. If the Nextdoor environment sounds like something you’re interested in being a part of, we’re hiring across the board for Engineering and Product! Vivek Karuturi, Engineer Follow me on Twitter: @Vivekxk Follow me on Email: vivek@nextdoor.com Nextdoor is the neighborhood hub for trusted connections… 60 Startup iOS Careers Culture 60 claps 60 Written by Software, Basketball, Hip Hop Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Software, Basketball, Hip Hop Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "co ops at nextdoor", "author": ["Prakash Janakiraman"], "link": "https://engblog.nextdoor.com/co-ops-at-nextdoor-e0d64c2830b2", "abstract": "Culture Open Source Technology As the Nextdoor engineering team has grown, so has the surface area of our tools, infrastructure, and processes. Many of these important initiatives don’t map directly to the formal organizational structure of our teams. Examples of these types of initiatives: Coding standards Developer productivity Fixit days Testing practices Innovation events like hack days We all agree that these are important areas to invest in on a continuous basis, but we also agree that as a small team, we don’t want to hire specialists or charter long-lived teams for each of these areas. Borrowing from Google (“grouplets”) and Spotify (“guilds”), a cooperative is an informal team of volunteers who are united by a common mission to improve a specific aspect of our engineering culture. To quote from an article written by Bharat Mediratta, a distinguished engineer at Google: … when the thing you really want to work on is to make a broad change across the whole organization, you need something new — you need a “grouplet.” These grouplets have practically no budget, and they have no decision-making authority. What they have is a bunch of people who are committed to an idea and willing to work to convince the rest of the company to adopt it. More importantly, these initiatives are usually bottom-up and driven by engineers without specific mandate or prescription from management. It’s an instrument for engineers to solve their own problems. Many important features of Nextdoor’s engineering culture were created through co-ops. Examples: This engineering blog! R3, our semi-annual innovation week (Reflect, Refine, Reinvent) Engineering onboarding documentation and exercises Unified structured logging framework JavaScript unit testing Site speed Women in Engineering … and more! Thanks to these teams we’ve improved our site performance by ~30%, streamlined the onboarding of new engineers, met several great engineering candidates who read our engineering blog, and spearheaded new product features like Polls and Bookmarks. Without these self-organized teams of volunteers, these initiatives had no formal ownership or sanctioned charter. Through the activities of these cooperatives, we’ve laid the foundation upon which our team can continue to scale and remain effective. We have many more initiatives yet to cover! Joining an cooperative is really simple: JUST DO IT. We maintain a list of existing co-ops and anyone can create a new one if a particular topic area doesn’t exist. Practically speaking, activities of a co-op are generally considered of secondary importance to your main project assignment. That being said, the co-op is designed to provide more formal visibility to colleagues and peers about other activities you’re involved with beyond your primary project. The best practice is to make sure your teammates know what co-ops you’re involved with and what specific tasks you’re working on; in some cases it may make sense to track those tasks in your sprint plans so that you get feedback on whether the time being spent is too great. Typically anywhere between 2–8 people participate in a single co-op, much like any other project team. This ensures diversity of opinions without having too many cooks in the kitchen. If a team grows beyond that size, it likely makes sense to break down the co-op into smaller, more specific teams. Co-ops of one are also discouraged; a lone wolf should find someone to pair up with at minimum. Mike Bland’s thoughts on grouplets Craig Silverstein’s role in the creation of cooperatives at Google Scaling Spotify (see section on “Guilds”) If you’re interested in learning more, reach out — we’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 19 Startup Agile Nextdoor Culture 19 claps 19 Written by Co-Founder of @Nextdoor. Former @GoogleMaps engineering manager. @Warriors season ticket holder. Hip-hop head. Friend to one and all. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Co-Founder of @Nextdoor. Former @GoogleMaps engineering manager. @Warriors season ticket holder. Hip-hop head. Friend to one and all. Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "offline data composition at nextdoor", "author": ["Sean Bromage"], "link": "https://engblog.nextdoor.com/offline-data-composition-at-nextdoor-3ae82c277d6", "abstract": "Culture Open Source Technology Anytime you need to run an expensive query or kick off extensive computation, it’s common practice to do so asynchronously. Here at Nextdoor , we’ve built Taskworker to do just that. But what happens when you need to run queries that take longer than just a few minutes? And what if you know when and for which objects you will need data beforehand? Scheduling queries to prefetch data seems logical, but anyone who’s been woken up a few times by PagerDuty knows that long running queries don’t make for a happy database. Locks get hogged, transactions pile up, everything slows down… alarms trigger. So, how do we compose data by scheduling expensive queries, without making both our database and oncall engineer cry? In order to better understand the solution, let’s first examine what prompted the need. Nextdoor has partnered with over 1,300 agencies across the US. We provide them tools to very specifically geo-target content in order to keep it relevant and unique to individual neighborhoods. (Learn more about our agency program .) One such tool is the Map and Metrics page. It displays information about the agency’s area that enables them to better target content by displaying anonymized reach and engagement data overlaid on a map. This is a data-hungry tool. Agency areas can span thousands of neighborhoods and the data on display is the result of several complex joins. The first step was to define our SLA. We decided that the data needed to be fresh, but not up to the second. An SLA of a couple of hours was just fine for what we needed. Once we knew the schedule in which we would need to pull the data, we began defining a pipeline. KETL Data for each agency on our platform is aggregated using jobs scheduled via KETL , an ETL (Extract Transform Load) tool. KETL is an open source development stack that enables complex ETL transformations to be executed over several servers with multi-CPU support. It executes jobs defined in XML that contain SQL thats runs on Redshift. An example job definition snippet is below: Redshift We use Amazon Redshift as our data warehouse. It’s far more efficient for running long, data-intensive lookups than our production DB. Agency stats use geospatial queries to determine which neighborhoods, members, and content fall within the tool’s purview. The stats are anonymized and presented via the tool from the perspective of high level metrics. Neighborhood content is never visible to agency partners unless explicitly made so by the author. S3 Once the ETL jobs complete, the data is compressed into a text file and sent up to Amazon S3 for storage. S3 provides flexible storage that doesn’t cost a lot and requires very little operational overhead. It also serves as a chronology of extracts, keeping the data available for analysis (and nostalgia). Scheduler, PostgreSQL A Nextdoor Scheduler instance kicks off a task to import the data into our production PostgreSQL database, allowing for very quick reads of the denormalized data on page load. You’ll notice that S3 serves as the bridge between our analytics and production infrastructures. Redshift is a useful tool for running analytical queries, but we do not route queries directly to Redshift from our application since it doesn’t have the necessary query performance and SLA’s for our production requirements. The pattern defined above has proven to be useful for products beyond the agency Map and Metrics page. Neighborhood Pages display interesting information about neighborhoods, such as unique characteristics that members like best about their area. The creation process pulls from various data sources to put together a comprehensive information sheet about each neighborhood on our platform. Various stats and data that date back to the creation of individual neighborhoods are composed offline in the same manner and are made available to pieces of the application that do not require strict precision due to the SLA mentioned above. This allows us to build internal tools and features while greatly reducing load on the DB. We were able to create a relatively simple data pipeline that provides denormalized data for quick lookups within our application. Each step of the pipeline lives within AWS, allowing us to quickly spin up or spin down machines and leverage existing authentication tools that we’ve built into our infrastructure. Redshift was already being used by our Data team for analytics and business intelligence tasks, so there was little added overhead to our existing stack. We were able to put tools in the hands of our members and can now rapidly build products that rely on large data queries without putting our production database at risk. A happy database makes for a happy engineer. As our data needs within the application evolve, so will our tools. This is version 1.0. Want to build 2.0? We’re hiring . Nextdoor is the neighborhood hub for trusted connections… 3 Big Data Web Development Nextdoor 3 claps 3 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-02-22"},
{"website": "NextDoor", "title": "building a scalable geospatial infrastructure", "author": ["Bhuvan Bamba"], "link": "https://engblog.nextdoor.com/building-a-scalable-geospatial-infrastructure-5be352aa9e74", "abstract": "Culture Open Source Technology Nextdoor is built on neighborhoods with well defined boundaries. To join Nextdoor, one must be verified to live within the boundary of one of these neighborhoods. Geospatial data forms the basis of the neighborhood definition as well as the user verification process. We are a geospatial data-driven company, and the various applications at Nextdoor make regular use of this data by firing a large number of spatial queries. Spatial queries can be computationally intensive and may retrieve large amounts of geometry data adding to the heat on the Database. As Nextdoor grew, the single database of our monolithic architecture became a performance bottleneck, especially because of expensive spatial queries. In order to provide scalability, we had to chip away at the monolith to create a geospatial service backed by its own separate database. Our geospatial service is called Gaia and it consists of: The Gaia APIs to access geospatial data. The Atlas geospatial datastore. With over 70 APIs, Gaia provides a powerful, but simple interface to interact with our geospatial data. For example, one can query neighborhoods whose geometries overlap a given geometry by some fraction: Atlas uses Postgres due to the excellent geospatial support offered by PostGIS. All geospatial data at Nextdoor, which includes plots, residences and neighborhoods, is stored within this Database. As it is a purely geospatial datastore, it naturally shards well by geography. Creating Gaia required these major steps: Porting the application to use Gaia APIs Migrating the existing geospatial data to Atlas Rearchitecting our map tile-server to use Gaia APIs After designing the schema for the Atlas database, the first step was to define the Gaia API. The next step was to port our Django ORM-based application to use the APIs. The ease of use the ORM offers is at the cost of modularity, making the task of porting the application to use APIs long and tedious. For example: Django ORM code: Ported to use Gaia API: Porting our code meant moving literally thousands of such ORM constructs to use the Gaia API instead. Porting was complete when all uses of geospatial data went through the Gaia API layer. We will cover details of the porting effort in more detail in subsequent posts. While designing the schema for the Atlas Database, we ironed out flaws and cleaned up the original design for the geospatial tables in the Nextdoor database. Because the old and new schemas were different, migrating this data to Atlas would need the utmost caution. We designed this process with verification at every step and, most importantly, a well-defined path to rollback the migration in case of any issues. Important components of the migration: Audit database triggers. Tracks deltas between the Nextdoor and Altas databases during migration. Dual-write to both databases. Keeps them in sync, allowing a rollback during migration Read from either database. Allows a controlled switchover. Asynchronous verification tasks. Compares data from both the Nextdoor and Atlas databases ensuring consistency Instrumentation for performance. Allows monitoring of the APIs, helping find bottlenecks. In a future blog post we shall go in the details of the schema we designed and the migration process. The use of maps is prevalent throughout Nextdoor, and we perform server-based map rendering with Google Maps as the background using our tileserver. For the purpose of rendering information like neighborhood boundaries, parcels, city agency boundaries, and various other useful pieces of information, we use Mapnik , an open source mapping toolkit for server-based map rendering, written in C++. We use Mapnik to incorporate data-rich maps for an enriched user experience. Prior to the Atlas data migration, the tileserver used Mapnik’s PostGIS datasource to simply convert the results of a SQL query into a vector layer for rendering. The SQL queries would retrieve geometry information as well as certain non-spatial aspects related to the geometry which would be used for rendering tiles. Now that our data was split between two separate databases, we needed a mechanism to query both databases and present the data to Mapnik for rendering. We used Mapnik’s python plugin in order to allow us to write data sources in Python. This implied using the Gaia API’s to retrieve relevant geospatial data from Atlas in conjunction with APIs for accessing data from the main database and building Python objects in memory to construct a datastore. We have constructed close to ten different datasources related to neighborhood boundaries, parcel data, and city agency boundaries which can be reused by different layers to render tiles. A side benefit of this switch has been that it becomes easier to reuse these data sources as building blocks for new map layers as opposed to writing slightly different SQL queries every time we want to build a new layer. A simple example of building a layer is as shown below where we construct a parcel layer using the parcel data source. The parcel layer uses the parcel tile data source to render the plot data belonging to each hood for the internal tool map shown below. The data source is constructed by extracting relevant geometry features (plots) from Atlas through the Gaia APIs and relevant metadata from the main Database through other APIs. We construct parcel features for rendering from the WKB format geometry and a dictionary of (keys, values) associated with each geometry which determines how the feature is displayed. For example, the color of each rendered plot depends on the value associated with its color key. However, the python plugin is experimental and comes with its own caveats. We will elaborate on the issues faced using this plugin in a future post. For now, it serves our purpose exceptionally well since we have been able to provide a tileserver which delivers similar performance as it did in the pre-Gaia era. We will cover different aspects related to building this infrastructure in greater detail in future posts. If you are interested in working on solving challenging geospatial problems, we’re hiring !! Authored by bhuvan and Ananth Chandrasekharan . Nextdoor is the neighborhood hub for trusted connections… 60 Maps Microservices Geospatial Technology 60 claps 60 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "meet the interns part 1", "author": ["Nextdoor Engineering"], "link": "https://engblog.nextdoor.com/meet-the-interns-part-1-54776a6589fd", "abstract": "Culture Open Source Technology This summer, we had several interns hard at work across different teams, ranging from engineering to data to design. Check out what they’ve been working on and hear about some of their fondest memories and technical (and non-technical) lessons learned from Nextdoor. Studying : Computer Science @ Boston University 2016 Special Talent : Reciting Dr. Seuss’s The Lorax from memory Mentor: Niall O'Higgins Project This summer I worked with the Growth team on a variety of projects, including building an automated data pipeline from Amazon S3 to our Redshift database. The data needed to be decompressed and error-checked first, but the files were large and would only continue to grow as Nextdoor does. Instead of downloading the entire dataset to memory, the data is decompressed on the fly as it is streamed to the database. In addition, the data update is fully atomic — it takes place in a transaction and is automatically rolled back if any errors are found. What I Learned Among a few different projects, I improved my skills in test driven development, data analysis, and API interaction. In addition to technical skills, however, my biggest enlightenments were learning how a company like Nextdoor functions. I bettered my understanding in Agile development, and saw how it was practically applied and tailored to fit particular teams. Observing quality executives and managers build team culture and delegate responsibility showed me how a company can move quickly, make users happy, and keep everyone engaged. This fall I will be president of the Boston University Outing Club, and I leave this internship far more confident to be part of and to lead an organization. Favorite Memory Refine week: For a week in July, normal operations of the company were put on hold to form small teams working to improve the site, internal tools, and more . I worked with a team of eight to understand why and when people deactivate and reactivate their Nextdoor accounts, and how we can improve our member experience to this regard. Even though I had only been an intern for a few weeks, I suddenly found myself the only engineer on the team! I was excited by the role, and worked hard to produce results as I was the only person implementing the changes that we spent each day designing. Studying: Computer Science / East Asian Studies @ Princeton University 2016 Special Talent: Dialpad music Mentor: Grant Zhu Project I work within our core Android team to build a sustainable and scalable architecture for current and future feature development. I’ve helped create internal tools such as an interface for developers to toggle the state of the app by flipping switches to control the reachability of certain code paths and am now preparing the client for realtime, synchronous messaging. What I Learned It’s an exciting time to be working on an application that’s beginning to tackle the challenges of innovating at web scale. On the core mobile team, we’re tasked with making decisions that affect the ongoing process of development on Android and as a result I’ve learned firsthand from the amazing people around me how to evaluate deep technical tradeoffs that carry consequences for real, consumer-facing ramifications. Favorite Memory Watching one of our senior engineers navigate a ropes course in pristinely shined dress shoes. Studying: Computer Science / Applied Math @ Princeton University 2016 Special Talent: Read ekphrastic poem I wrote Mentor: Wenbin Fang Project I created a Compose button to send private messages from the Inbox on our website. This button has been a highly requested feature for web so it’s been exciting to work on. The compose Bootstrap modal uses the Select2 plugin to allow users to select a recipient from their neighbor list (with search/auto-complete functionality). In preparation for launch, I created a Tableau dashboard to track how the button was impacting our private messages. I’m also learning some iOS development through adding an embedded browser view to allow neighborhood Leads to verify neighbors on the Nextdoor iOS app. This is a highly requested feature from our valued Leads and the embedded web view is a low-cost solution that will help us quickly deliver this functionality to them and also quickly iterate. This will also serve as an example of a native-feeling web view that can be used as first iterations of certain appropriate features. What I Learned As this is my first internship at a tech company, I learned that the lifecycle of a feature, from the idea to the launch, requires you to exercise many of your “soft” skills in addition to your coding ones. I coded, yes, but I also worked on product specs, wrote documentation/user-facing blog posts, and discussed product roadmaps with people. At a company like Nextdoor that’s building a product that impacts so many users, you really need to think and test carefully to make your code more than a hackathon project. Favorite Memory Being a part of the internal co-op responsible for getting this Engineering Blog set up. We’re a data-driven “startup” within another data-driven startup. :) Studying: Computer Science / Math @ University of Texas 2016 Special Talent: Flipping toothpick in my mouth Mentor: Shiv Ramamurthi Project I work on the TenX, or Triggered Engagement and Newsfeed Experience team; we’re responsible for all engagement related features. Everything I’ve done, and everything the team does, revolves around bringing the user back to Nextdoor. I worked on large scale analysis studying the distinct features that convert a casual Nextdoor user into what we call a Weekly Active User, built an interactive dashboard to visualize the results of a variety of A/B tests we run on a regular basis, and started working on a service that notifies our data team about failures with our ETL pipeline. What I Learned Through studying the distinction between casual users and active users I had the opportunity to become familiar with the ipython notebook environment, learn how to write optimized Postgres queries, and do a bit of simple statistical analysis. In building the A/B testing dashboard, I got to hone my skills using advanced features of Tableau and learned how to optimize Tableau dashboards for performance. Writing the service for ETL errors, I had the chance to interact with Sendgrid and Django. Favorite Memory My favorite memory from Nextdoor was actually at one of the many fun intern events we had through summer: the panic room, a game where you’re locked in a room with the backstory of being a time machine lab and have an hour to solve puzzles to lead you to a key. The moment was when we’d been minutes away from coming to the solution as the buzzer went off and Eric, our infrastructure intern, figured and yelled out the solution just as we ran out of time. Though we lost, we had a great time and laughed it all off. Studying: Computer Science (Artificial Intelligence) @ Stanford University 2016 Special Talent: Taiko drumming with Stanford Taiko Mentor: Vikas Kawadia Project I work on the Growth team at Nextdoor, where my primary focus is on postcards. A unique way in which Nextdoor draws in new members is through its postcard program, which allows members to invite neighbors onto the site by sending physical postcards at no cost to the sender. This summer I’ve conducted and contributed to analyses exploring, for example: the effects of message customisation and social sharing on sign-ups, and the UX-driven incentives that bring people to invite new neighbors to Nextdoor. Our analyses involve creating A/B tests as well as predictive models. My next project is to create a predictive model relating to postcard conversion rate. What I Learned Working with Nextdoor’s data pipeline has impacted how I think about systems architecture, and is fascinating from an intellectual standpoint! I hadn’t worked with such a complex dataset and pipeline before, and so getting the chance to tinker with Nextdoor’s codebase using a combination of in-house tools, PostgreSQL, Tableau, and other third-party services has been a great learning experience. Favorite Memory Seeing all new interns’ and hires’ special talents! They say “the bar is set very low”, but all of the ones I’ve seen so far have been great. Studying: Geospatial Analysis & Computation, PhD @ Arizona State University 2018 Special Talent: Country/Bluegrass Music Mentor: Jay Thomas Project I worked with our Business Intelligence and Geospatial team to build a pipeline to integrate US Census demographic data with Nextdoor data. To do this, I built cenpy, a package to explore and use US Census data products interactively and dynamically. In addition, I worked on space-time statistical modelling for forecasting and trend detection. What I Learned Coming from a PhD program and working in industry was both exciting and challenging. Learning the real-world demands on the statistical techniques and tools I’ve been building for my degree has strongly improved my understanding of how end users actually end up applying the models academics develop. In addition, I’ve learned a bit of how to bridge the gaps between the two communities. Favorite Memory The Town Hall meeting. It was amazing to see the company all come together as one community and collaborate openly and effectively on communal issues. Tune in next week for part 2 where you’ll meet the rest of our interns! Nextdoor is the neighborhood hub for trusted connections… 3 Internships Culture Engineering 3 claps 3 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-02-22"},
{"website": "NextDoor", "title": "publishing blogs with github circleci and google analytics", "author": ["Daisuke Fujiwara"], "link": "https://engblog.nextdoor.com/publishing-blogs-with-github-circleci-and-google-analytics-d8872b9e5322", "abstract": "Culture Open Source Technology At Nextdoor, we treat our engineering blog like an independent engineering project. A handful of engineers, forming a committee (co-op), manage the process of publishing a weekly blog entry. The tools that we use to draft a blog, review the entry, and publish to the world are all centered around our main users, engineers. Our goal is to make writing as frictionless as possible for our engineering team so that they can focus on the most important aspect of it — sharing the awesome work we do to help make neighborhoods stronger and safer. We publish one blog post every week. We have over 50 engineers and we take turns writing blog posts, which is similar to our oncall rotation — except that you write a blog post once a year, rather than once every 3 months. :) Engineers do things in the engineering way. We write posts using Markdown and we manage posts using Github Enterprise, where we have an engblog.git repo. In the repo, each blog post has a separate directory, where there is a Markdown file for post contents, a meta.ymal file for meta data, and a subdirectory for images. Oftentimes, more than one engineer works on the same blog post. We’re already used to working together on the same code base, mostly for plain text files. Therefore, we have no problem dealing with Markdown files, which are plain text files, too. We work on a new feature branch for each blog post. We create a pull request and add reviewers on Github Enterprise for “code review” before publishing a blog post. We typically include our peer engineers as reviewers, as well as people from the communications team, who are surprisingly happy with such “code review” process on Github Enterprise! Here’s how it looks like to “code review” a blog post on Github Enterprise: The published blog would appear on WordPress, so the content still needs to get exported from GHE to WordPress. This is where continuous integration comes in. For regular projects, CI is used to run tests, publish artifacts, etc. For engblog, we leverage CircleCI to create a WordPress draft. This script walks through our directory structure looking for meta.yaml file and parses out the information required to publish a WordPress blog. Future improvements include automatically uploading images and creating gists from code snippets, but for now we do this manually. The script that publishes the blog to WordPress is here. Once the content is reviewed and published on our blog site, we use various channels to promote our content. First, we announce the new blog entry in our internal company wide forum. The message contains a brief summary of the blog and encourages our colleagues to read it and share it on various social channels. Also, this helps to raise awareness of the engineering blog and nudge our fellow engineers to be part of the movement. Additionally, we have a one page description of the blog that we print out and post in our bathrooms. Surprisingly, this has been a very effective way to spread the word about the blog to the entire company. For example, our CEO, Nirav, mentioned it in one of our all-hands. The members of the engineering blog co-op share the blog through social channels including Facebook, Twitter, LinkedIn, and Hacker News to kickstart the virality. We carefully analyze the effectiveness of each channel and continue to optimize how we promote. We want to know our audience — understand what people like to read, when they choose to comment, and why certain posts are more viral than others. Google Analytics is an excellent, off the shelf, free tool we leverage to track out progress. We meet once a week to take a look at how the most recent blogs have performed. We also look at former blogs that continue to have new readers. This means that the content is widely relevant and has good SEO. When engineers start writing, good things happen. One of our core values as a company is to communicate openly. With engineering blogging, just like with open source software, we aim to share the tools and the knowledge we have about current technology with the rest of the community. We also believe that writing about the great work we do every day promotes a better engineering culture. Not only does it help spread the word about Nextdoor engineering, but writing the blog also allows engineers to feel greater ownership of their work. It’s also great to receive feedback from other developers in the community and gain new perspectives on the problems we tackle. We started to put more emphasis on our engineering blog a few months ago, and it’s amazing how much content we have created so far. Of course, we have so much more to share, so stay tuned and subscribe for more updates. Also, if you’re interested in joining us, we’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 2 Social Media Circleci Blogging 2 claps 2 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-02-22"},
{"website": "NextDoor", "title": "meet the interns part 2", "author": ["Nextdoor Engineering"], "link": "https://engblog.nextdoor.com/meet-the-interns-part-2-b4ff74db8006", "abstract": "Culture Open Source Technology Last week, you met 6 of our summer interns . This week, meet the other 6 design, engineering, and product interns and hear their stories. Studying : Computer Science @ Stanford University 2016 Special Talent : Welding (I made the bench I’m sitting on!) Mentor: Kevin Liu Project Over the course of the summer, I’ve been working on a variety of projects, ranging from squashing bugs for our city platform to performing a consistency audit of our website’s styles and assets. In preparation for Refine Week , a week for the company to refine our product’s features, I took the learnings from the consistency audit to create a style guide as a resource for projects. I’ve also spent some time working on both frontend and design for the Recommendations team, and I’m now designing and coding pages for Nextdoor’s open source projects. What I Learned I think the biggest thing I learned was how to be flexible and adapt to work at a startup. No two weeks have been the same for me, and I’ve spent some days fishing through the code base to fix consistency problems and others working with other designers to examine and iterate on a new released feature. Favorite Memory Late one Friday afternoon, I challenged Paco Viñoly, our VP of Design, to redesign the new employee welcome sign for a new designer starting the following week. We went head-to-head in a design sprint of sorts that ended up being a lot of fun. It was great to see our divergent ideas come together and to be able to joke around with him and the other designers spectating. Studying: Stanford Graduate School of Business Special Talent: Moonwalking Mentor: Paul Howe Project As a PM on the Recommendations team, I was lucky to operate on multiple levels of resolution: the 20,000 foot strategic level, the 5,000 foot feature-set level, and the ground-level of UI treatments, analytics, and user testing…all in a short summer! The Recommendations team has been hard at work structuring the experience of finding and giving local recommendations, an activity that by default happens organically through the newsfeed. Structuring recommendations is a particularly important initiative, not only because it makes it easier for Nextdoor members to find neighbor-recommended businesses, but because it lays the groundwork for two-way communication between members and local entities like plumbers, restaurants, churches, and schools — the foundation of a true local graph. This summer I researched monetization models and how they might be incorporated into Nextdoor’s product in a way that is accretive to the member experience; I contributed to the ongoing refinement of elements like the entity page, search results, and category taxonomy; and I assumed primary responsibility for the business tagging feature, the first version of which just shipped. What I Learned As a novice product manager, I was fortunate to learn from some particularly talented and experienced product folks (most especially my mentor/manager Paul) and work with a first-class engineering and design team. I learned the principles of agile development (including implementation of the kanban system), how to conduct virtual user-testing, how a retro meeting works, and how to contribute to a design sprint. Equally importantly, I had the chance to observe how a highly effective technology company is run — with transparency, a commitment to first principles, and a culture of continuous learning and improvement. Favorite Memory Discovering that Prakash (co-founder & CTO) and Aaron (21-year-old intern) were both independently partying with the Warriors at Temple Nightclub following the finals victory. That was week 1 of my summer. That’s when I knew Nextdoor was the real deal. Studying: Computer Science @ UC Berkeley 2016 Special Talent: Reciting the greek alphabet in under five seconds Mentor: Kip Kaehler Project I wanted to focus on web development for the summer and so my projects mainly revolved around building features. Throughout the summer, I worked on several features including a neighborhood map, photo comment reply, and an address suggestion tool that allows users to suggest new residences for approval. These projects were mostly front end based and were written with the underscore.js library and the backbone.js framework, which helped to clearly separate the model and views for the pages into different files. My last project, the address suggestion tool, was a full stack project. It involved creating a new migration and table in our geospatial database, an API for the data involving user suggested residences, and client rendered templates for both the view of the users and our customer support team. What I Learned Project by project, I became familiar with the entire stack from querying and understanding databases, to creating APIs, to creating the web pages and user interfaces. I also learned how a project idea is put in motion, as the idea is refined between several people before an MVP of the feature is built, polished and then launched into production behind a feature configuration. On a non-technical scale, I got my first glimpse of a team’s scrum methodology and a tech company structure and culture by working alongside the self-motivated individuals of my team. Favorite Memory The product manager on our team got married in July, and while he was on his honeymoon, our team brought in a giant purple monkey in a Warriors shirt as his replacement. For those two weeks, we referred to the monkey as our product manager, made some awesome puns, and even brought it to our team offsite to Napa. Studying: Computer Science @ Stanford University 2016 Special Talent: Juggling Mentor: Steve Mostovoy Project I am interning on the backend and infrastructure engineering team. This summer I’m designing and implementing a new data pipeline to log events in our systems using Apache Flume and Elasticsearch. I also created an internal tool for content management on the About Us page to streamline changes. What I Learned I learned how to evaluate technology for specific use-cases at Nextdoor. Doing so not only requires full knowledge of our current technical requirements for a proposed system, but also how we might expand this infrastructure to encompass new use-cases in the future. This critical thinking greatly improved my understanding of technology behind the Nextdoor stack — from Flume to AWS services. Favorite Memory The other interns and I were given the opportunity to have lunch with each of the Nextdoor founders. I really enjoyed learning about how Nirav, Prakash, and Sarah’s experiences shaped their involvement with Nextdoor and their views on the tech industry. This experience was invaluable for understanding the motivations that drive company development at Nextdoor. Studying: Mechanical Engineering, M.S. @ Stanford University 2017 Special Talent: Disturbing hand contortions Mentor: Cliff Williams Project I’ve worked primarily on two teams here at Nextdoor: one is secret, and the other is the Recommendations team. For the secret team, I worked on a new onboarding flow for the new feature. This turned into a cross-platform design challenge that is about to ship. On the Recommendations team I worked on the modal web flow for recommending an existing business and adding a new business from a category page. What I Learned What has stood out to me most here at Nextdoor is the speed with which we ship features. We push MVP products that we know are not perfect with the goal of learning very specific lessons from each release. As a pixel-focused designer, it was at first hard to let go of the perfectionism I harbor toward each of my designs. The data we collect and experiments we run with each release is, however, invaluable and I’ve come to believe the best way to learn what works. Favorite Memory My very first Friday at Nextdoor the ‘secret’ team went to Napa to celebrate the soft-launch of our feature. I’ve never gotten to know a group of people so quickly or so hilariously. Studying: Computer Science @ UC Berkeley 2017 Special Talent: Raising one eyebrow at a time Mentor: Sophie Zhou Project As an iOS developer, I’m fascinated by the underlying constructs of the application and the language from which it is built. Calculating metrics has always been a key part of every mobile feature. To help streamline this, I implemented the process of client-side event tracking (user instrumentation) by swizzling through certain UIControl event flows, dynamic delegate-swapping for certain protocols (UITableView, UIAlertController, etc), and adding associative objects to allow developers to hook data to the logs. This effectively implemented the “measure everything” philosophy that also ensured no effort for developers to implement tracking in their features. My interest in the runtime characteristic of Objective-C also sparked a curiosity in the application build process. As the Nextdoor application grew larger, it became harder to enforce certain design patterns/code architectures. To support this, I converted all of the existing static libraries to dynamic frameworks and removed the model layer of the application into its own dynamic framework, allowing modularity as well. What I Learned The user instrumentation project heavily utilized the dynamic nature of Objective-C. Through this I learned to harness the powers of dynamic dispatching and how it can make broad changes to the software that it runs in conjunction with — in this case, user instrumentation. By dealing with dynamic frameworks, I learned a great deal about the iOS application build process. From how different types of dependencies are integrated into a resulting binary, to how xCode (the IDE for iOS development) manages and builds its projects step-by-step under the hood. Favorite Memory The off-site trip to a ropes course that my mentor organized for the mobile team was pretty fun! I’ve never seen my coworkers in such a hilarious setting. Find me at siddhantdange@berkeley.edu! Thanks to all of our interns and their mentors for a productive, inspiring, and fun-filled summer! Nextdoor is the neighborhood hub for trusted connections… Startup Internships Culture Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-02-22"},
{"website": "NextDoor", "title": "migrating from ec2 classic to multiple vpcs", "author": ["Matt Wise"], "link": "https://engblog.nextdoor.com/migrating-from-ec2-classic-to-multiple-vpcs-8724b996b6b5", "abstract": "Culture Open Source Technology Like so many companies today, Nextdoor started from scratch in the “cloud.” Our first server was launched in Amazon ( shocker ) using Amazon EC2-Classic — pretty much the standard for anyone who has had an Amazon AWS account for more than a few years. As we’ve grown we have continually iterated on our server management and deployment strategies. I’ve talked about some of this in past posts — red/black deployments , making server boots faster and using a combination of puppet and image-based deployments for flexibility and speed. Recently though, our server footprint grew to a point where we decided it was time to build some network-level protection around our infrastructure. Every component of our infrastructure had been built from the beginning with security in mind … but at some point you just have to admit that your DB hosts don’t need public IPs. At the highest level, EC2-Classic provides shared public and private IP addresses for all servers — security is achieved through Security Groups. VPC networks though allow you to completely isolate your servers into private IP space, and completely hide your private servers by never giving them public IP space. Depending on how far you take this network model, you can actually isolate your servers so that they can’t even reach the internet . Yes. Really. We built every single portion of our infrastructure with SSL encryption and authentication from the beginning. Where that wasn’t something we could easily control ourselves, we inserted tools like stunnel to add security. Ultimately it’s a risk mitigation issue. By moving our servers into private networks we are able to massively shrink our visible “public footprint”. By shrinking this footprint, we limit the number of entry-and-exit points in the network and are able to focus more of our efforts on securing those endpoints. This does not negate the need for application-level security. We still use encryption end-to-end for every internal and public facing service we run. VPCs are not a substitution for application-level security. One of the first questions we asked ourselves was how many VPCs we wanted to build. Building a VPC in Amazon is not hard, but it’s not trivial either. It involves planning out IP space, routing rules, network ACLs, NAT translation hosts, VPN connections, and more! Why would we do this more than once?! Multiple Regions. We’re already running in multiple regions — both for operational and data redundancy reasons. Amazon VPCs only span one region — so running a single VPC was out. Build three? Build a ton! If we knew we were going to build at least a few VPCs ( let’s say, one-per-region ), we decided we might as well become experts at running and building a whole lot more. It’s marginally more work to jump from managing 3 to managing 20 VPCs — if you do it right. Application Segregation . So once you admit that you are going to run many VPCs, you can start talking about using VPCs themselves as full application level segregations. It makes it really easy to know that our Staging environment can’t even remotely reach our Production environment. Once we knew roughly what design limitations we had to work with, we set out to build the thing. We had to gather some network and design requirements, come up with an implementation plan, build some CloudFormation scripts, and finally just start plugging away. For us, the requirements were simple. Since we had decided that network level security would never replace application security , we knew that we were not building some global network where every server had the ability to reach other servers. In fact, we explicitly did not want that. Here were our requirements. Easy Administrative Access : Engineers were used to being able to SSH from our bastion hosts directly into every server in our network. Therefore Engineers would still need single points of access into every server in the infrastructure. Clear Environment Separation : The staging VPC should never be able to reach servers in the production VPC. Ever . Note, this actually is a blessing in disguise — it means that we never had to worry about performance or reliability between VPCs. We can focus on management access only between them. Automation : If we’re going to build many VPCs, we need to be able to automate the process. Period. The latter two issues were pretty easy to solve. Amazon’s CloudFormation templates were used to create every VPC in our network entirely from scratch. We even use the CF templates to build our NAT gateway hosts that provide our private subnets with internet access. ( Look for Part-2 of this blog that will go into detail on the CloudFormation templates we built ) The first issue though was tricky — and it took us some iteration to come to a model we liked. Ultimately we decided on a combination of our own IPSec Tunnel servers and Amazon’s VPC Peering . Within a given Amazon Region we create what we call a TRUST VPC. Inside that VPC we create a management host that we call an uberBastion host. These uberBastion hosts allow inbound SSH and VPN connections from our employees. They also have one more trick up their sleeve — I’ll get to that in a minute. Once we’ve created the TRUST VPC, we use Amazon VPC Peering to connect that VPC to all of the other VPCs within that region. So, if a region has three VPCs ( DEV1 , DEV2 and TRUST ), there is a peering relationship between the TRUST VPC and each of the other two VPCs. This solves our management access problem in a given region… but doesn’t solve it world-wide. If we stopped at this point, Engineers would still have to know which region their servers lived in, and log into different bastion hosts to get to those servers. Now, bring in IPSec tunnels. Our uberBastion hosts also maintain IPSec tunnels between themselves and every other uberBastion host in our worldwide network. Combining this, a little iptables magic and some puppet automation gets us the ability to route traffic globally through these simple Linux hosts as if we were on a single network … but really crossing regions, landmasses, etc. Ultimately this part was easy. I can’t stress the importance of automation here. We spent weeks working on getting the design right. We built and tore-down more VPC networks than I can even remember. We experimented with a dozen different models for managing world-wide access to our servers. We failed … a lot. That said, once we had the model that we liked, implementation was easy. We effectively followed a set of steps for each service we migrated: Determine the user-facing network access requirements. Use CloudFormation Templates to create new VPC and link to regional-TRUST network. Rebuild Security Groups, ELBs, etc for new VPC. Use Red/Black deployment model to launch new services inside VPC. Transition user traffic. We iterated on this, and of course every service is a bit different … but we just kept plugging away. The final migration of the main nextdoor.com service happened seamlessly in the middle of the night with zero downtime. Ultimately, the project took about 4 months end-to-end and was worked on primarily by Charles McLaughlin and me. The planning phase took about 6 weeks of trial and error — a really long time, if you ask me! However, once we had a solid and well defined plan, the actual migration was easy. Every week we would tackle migrating a new service; some took the whole week, others took just a few hours. At the end of the day though we had zero user impact to either our actual customers, or the engineering teams in the office. I can’t stress enough how important the planning phase was here. This project would have been a disaster if we hadn’t spent that time testing, testing and testing again! Now that we’re done, we’ve gone from hundreds of publically visible servers on the internet to less than 20. Today we run 18 VPCs, and we’re in the process of adding another 6–10 right now. The model has been in place for about 9 months now, and we’re happy to say that we don’t have a single Non-VPC-protected host in our network! ( Look for more posts on this topic… Future posts will dive deeper into the CloudFormation automation, as well as the Puppet code for our Region-to-Region IPSec Tunnels ) If you’re interested in working on these kinds of problems, we’re hiring ! Sr. Systems Architect Matt Wise (matt@nextdoor.com) Nextdoor is the neighborhood hub for trusted connections… 3 1 AWS Vpc DevOps Technology 3 claps 3 1 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "easy a b testing at nextdoor", "author": ["Rob Mackenzie"], "link": "https://engblog.nextdoor.com/easy-a-b-testing-at-nextdoor-30ef9cf53858", "abstract": "Culture Open Source Technology Nextdoor makes heavy use of A/B testing to improve our service. We’ve built a suite of tools and leveraged third party services to enable our A/B testing habit. Our guiding principle has been ease and accessibility: when testing is effortless, more people ship more tests, and our product steadily improves. This post will walk through the tools we use heavily. Our core framework handles bucketing and tracking, and a basic call to it looks like this: “Bucket” refers to a cell or variant of a test, i.e. the A’s and B’s of the A/B tests. This is the high-level logic for get_bucket: Fetch the specified test. Return None if the test is missing or inactive, or if the subject does not belong to the percentage of subjects exposed to the test. Return previous bucket if the subject was previously bucketed into this test. Bucket the subject and return the newly assigned bucket. There are a few important properties of our bucketing system: It is stable within the same test: once a user is bucketed into a given test variant, they will always be in that variant for the life of the test. It is random across tests: If we run 3 A/B tests, the same users will not be allocated to A for all tests (this is a flaw of a naive bucketing function like id % number_of_buckets). It is deterministic: We can determine which bucket a subject will fall into, without actually allocating them to one. For instance we can check which of our test users will fall into a given bucket, without actually exposing them. To accomplish this, we hash test name with subject id, and return the result mod the number of buckets in the test: Finally, a few advanced options round out the framework: We can check if a user is already bucketed in a given test: This is useful in situations where a test variant has multiple dependent touchpoints (such as a chain of reminder emails), or when exposure to a test impacts eligibility for a different one (such as concurrent tests which would disrupt each other). We can use custom bucketing functions: As an example, a test that affects invitation virality might bucket a user based on his or her inviter’s bucket, to measure the effect of the change in steady state. We can do hard bucketing: This is for cases when we know our subjects’ buckets in advance and want to pre-bucket. The tests themselves are created through a web tool: In addition to defining the test here, at any time we can also perform the following actions: Enable or disable the test Ramp the number of exposed subjects up or down Change the scheduling of the test We use additional or alternative technologies for certain kinds of tests: We use Optimizely for quick, frontend-only tests, such as design and copy changes. These don’t require an engineer. Though we let Optimizely do the bucketing for these tests, we use our own tracking to record those events. This lets us connect the tests to our own data and do more sophisticated analysis. Email is very important to the Nextdoor ecosystem — we use it for growth, content delivery, and many kinds of notifications. We built a system we think of as “Optimizely for email” — it allows us to A/B test email content without an engineer and without pushing code. It layers email-specific functionality on top of our core A/B test framework. We use the same server side framework for native mobile A/B tests, except that a user is pre-bucketed into all active tests when they open the app (rather than just-in-time bucketed) and those buckets are cached by the mobile client. This ensures the user’s experience on the mobile apps can’t be blocked by a web request for an A/B test bucket. Tests can also be targeted to particular platforms and app versions. All bucketing events are recorded in our data analytics warehouse. A dashboard allows us to see how any test affected our core metrics, and whether the differences were statistically significant: We also have a wiki template for “one-pager” write-ups on tests — this lets us more easily document the goal of the test, what was tested (with screenshots and descriptions), results, and conclusions. This all combines to enable a culture of rapid A/B testing. Individuals are empowered to run their own tests easily, including non-engineers: designers, product managers, marketers, and neighborhood operations specialists all run A/B tests at Nextdoor. If you’re interested in working on these kinds of problems, we’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 5 Ab Testing Python Growth Technology 5 claps 5 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "structured logs at nextdoor", "author": ["Stefan Krawczyk"], "link": "https://engblog.nextdoor.com/structured-logs-at-nextdoor-4004c7b4c2fd", "abstract": "Culture Open Source Technology Our application logs are output in a structured JSON format for simpler debugging and downstream consumption. For example, developers can add a field to their application log and it will automatically turn up as a parsed field in Elasticsearch for easy dashboarding and querying via Kibana . Not to mention it also simplifies doing larger log analysis via map reduce since you don’t have to deal with fragile regular expressions. Most application logs are unstructured. You’re probably familiar with seeing something like the following in your code base: which produces the following log line: Sure, the square brackets allow you to parse a few things easily, but what if: You wanted to find all logs relating to a particular neighborhood identifier? You wanted a report or dashboard to automatically capture new log lines? You had to figure this all out under pressure because the site is burning and the person oncall is the new fresh-grad who doesn’t know the code base well? You don’t pay for an expensive service like Splunk ? You wanted to easily search for and/or display all stack traces? The typical requirements to make use of unstructured logs are: Knows regular expressions Knows the code base well enough to know what regular expressions are required To illustrate why searching for all the logs relating to a particular neighborhood is challenging, here are some other ways that developers could have written the log line above: I personally have yet to come across a company that enforced best practices about what is put in a log line message! We developed a library at Nextdoor to output structured application logs for easier debugging and firefighting, as well as simplifying downstream consumption. For every service that we have that outputs logs, e.g., nginx, uWSGI, django, we output JSON objects. For example: Which comes from the following line of code: This allows us to easily consume and create dashboards like the following: Come firefighting time, there is an easy path to find and gather all logs for a particular HTTP request, neighborhood or user. This takes the guesswork out of figuring out how to search for particular pieces of information, and incentivizes developers to make their application log lines richer in detail. In short, we get our apps to output JSON, which are then pushed through Flume to Elasticsearch and S3. It isn’t difficult to output JSON, since all you’re changing is how the log line is formatted. Specifically, we wrote custom configurations (for nginx, uWSGI) and overrode libraries, where appropriate, to produce JSON in each of our apps. Diving into more detail, to get our Django app to output JSON, we overrode the standard Python logging libraries. If you’re using Python, you’re in luck — I’m going to show you how easy it is to implement: The above extends the standard LogRecord object, by providing two extra fields that we will use to store extra information in. The above overrides the Logger class to change the way the LogRecord is instantiated and to populate the extra LogRecord fields. The handler is where the magic happens. We override the format function and instantiate an object to build the JSON string that gets returned. To make use of the above code, you then just need to set the appropriate Logger & LogHandler in your log configuration. E.g. Then to emit your first structured log, you invoke calling the log function like normal, but instead pass in a dictionary to the extra keyword argument. This dictionary will contain mappings like ‘ neighborhood_id ’ to 123123123 , and is how you will be able to capture all logs that have passed in a neighborhood_id for instance. E.g. There are several caveats that we found with our approach to structured logging: Tooling: you have to make sure you write a simple tool to pretty print and make it simple to parse JSON logs from the command line, otherwise people will complain that they can’t scan JSON easily. Performance: you will be paying a larger price to log a line, since you’re now transforming things into JSON; make sure you’re using an appropriately performant JSON serializer. Dynamic vs Typed Languages: we ran into issues with enforcing types. A specific problem crops up when you create a field in elasticsearch, you need to specify the type. Since we’re a python shop, we needed a way to tell downstream components what type a particular field should take. This was accomplished by appending an identifier to the field name. E.g. neighborhood_id would become neighborhood_id__long. Then downstream we would inspect that information to correctly cast data. Structured logs at Nextdoor provide a standard and systematic way to process and consume application logs. It has been a success here at Nextdoor in aiding fire-fighting, along with general system metrics & monitoring. It has been in use for over eighteen months and powers over fifty dashboards that our engineers use every day. If you’re interested in working on these kinds of problems, we’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 13 3 Python Logging Technology 13 claps 13 3 Written by My Professional Persona: Algo-Dev Platform @stitchfix_algo; Former Idibonite @Idibon, Ex-Nextdoor Engineer, Ex-Linkedin Engineer, Stanford MSCS Grad Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by My Professional Persona: Algo-Dev Platform @stitchfix_algo; Former Idibonite @Idibon, Ex-Nextdoor Engineer, Ex-Linkedin Engineer, Stanford MSCS Grad Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "android qa at nextdoor", "author": ["Wesley Moy"], "link": "https://engblog.nextdoor.com/android-qa-at-nextdoor-7448cb7f97df", "abstract": "Culture Open Source Technology Quality assurance is an important part of developing software. Bugs detract from the perceived quality of software and can turn both new and existing users away. Here at Nextdoor, we recognize the value of reliable software and have built up a software quality assurance set-up to help us deliver software we can be confident in. When a defect is discovered in an app, a bug fix can take an agonizingly long time to actually land on users’ devices. A fix can often be prepared in hours, and the Google Play Store takes only a few additional hours to process a new version for publication. Unfortunately, a lot of variability starts after the app is published. While the Play Store encourages users to turn on automatic app updates, many users don’t. We see about 80% of our users upgrade to the newest version of the app within 3 days, but we also still see many users actively using app releases from a month or more ago. The only surefire way to prevent that a user from encountering a defect is to ensure that defective code is caught before it ever reaches a user’s device. We’ve built up a number of layers to give us confidence that, whenever possible, a defect never leaves the building. A test written for a computer is more scalable than one that needs to be executed by a human. When a feature simply needs to behave a certain way, an automated test catches deviations instantly. Many modern software engineering tools depend on having such tests. Tests that run instantly and repeatedly pave the way toward continuous integration and more frequent releases. We use JUnit with Robolectric to run our unit tests. These tests are fast and can run completely on the JVM of the developer’s machine with no need for an Android device or emulator. The Android SDK included JAR files representing only empty interfaces of all the Android APIs. The implementations would only exist on Android devices. This was fine for compiling — the interfaces provide enough information for an IDE to verify correctness. If you ever try to execute code that calls into these libraries, you quickly see this in action: Just logging a message to the console or deserializing JSON would be enough to trigger this exception — those classes are provided by the Android JARs. Even with the experimental Android unit test support, this is a real and known problem . This is where Robolectric comes in. Robolectric exposes the Android implementation of many of these classes and provides sensible implementations for others. Coupled with its support for emulating the Activity lifecycle, this makes JUnit and Robolectric an excellent tool for catching many logic errors before they ever get merged into the master branch. We increasingly use Espresso to automate the testing of UI flows where a tester would simply be following a script and verifying that the interface is in a specific state. The Espresso framework and API are sufficiently expressive that many manual tests lend themselves well to direct translation to a functional test. By running these tests in an automated way, we gain two major advantages. Tests can run more frequently. Rather than have a person run through some test plan periodically, we can run our functional tests against master as often as our hardware resources will allow — usually almost one build for each change delivered. Automated functional tests shield manual testers from “boring” defects. Any human tester will only file a particular number of bugs. That number changes from person to person, but every person has a point at which they encounter too many bugs to file. Additionally, human testers running the same test repeatedly can become numb to recurring defects and may start to “see” the expected behavior through the defects. By catching the most straightforward defects early, humans are freed from boring and repetitive testing to explore and document more subtle, complex, or subjective problems. Computers can catch the problems that occur at the level of a single method or class. Computers can also catch when individual views in a UI are in a state other than the intended one. What computers can’t do is evaluate an app as a whole and recognize new classes of problems — especially ones where features, behaving correctly, interact in a way that hinders the user. Tap the notification button in the toolbar. Verify that the text of the notification matches the text of notification in the Android notification tray. Verify that the notification has a red “NEW” badge. When a release branch is cut, test scenarios are run manually against the release candidate build to verify the correctness of established functionality. These manual tests are often tricky to automate, have ambiguities or complicated success criteria, or simply don’t have engineers currently assigned to automate them. The purpose of these tests overlaps widely with the purpose of automated functional tests. However, they tend to coexist very well. As tests are ported to the automated functional test framework, a manual tester has more time to be more thorough and careful with the remaining tests. As the tests that remain manual tests are generally ones that are more subjective, the reduced load helps to stave off the tunnel vision that forms during a marathon manual test session. Dogfooding is an excellent way for everyone to take an active role in improving mobile apps. Internal users — our most highly invested users — are our last line of defense. Whenever a defect has escaped detection in all other testing, this testing helps us catch it. This also extends to much larger issues than simple logical defects. Coworkers, especially non-engineers, have the opportunity to keep abreast of the newest features and developments. All these internal users can then give our product managers early feedback about the way a new feature works, the design of the feature, and how it interacts with the rest of the app. To empower dogfooders to provide feedback, we use a shake-to-report system. A user running into an issue shakes their device. The system takes a screenshot, provides a way to type an explanation of the problem, and even allows for annotation of the screenshot by doodling with a finger. When the user is done writing feedback, the app sends the report to our servers. These servers in turn create tickets for us in our bug tracking system so that we have easy access to all dogfood feedback in a way that’s easy to assign to the appropriate team. Inevitably, defects find their way into production. When they do, we watch for them so that we can fix them. In addition to crash logs in the Google Play Console, we use Rollbar to keep track of log messages describing any error conditions in the app. Nextdoor is a platform built on the trust neighbors invest in us. As a company, we focus on providing a trustworthy and private platform for neighborhoods and the neighbors that live in them. As engineers, we work hard to ensure that the actual interface between a user and our platform is trustworthy. All of these pieces help to keep it that way. If you like writing testable code and the tests that test it, we’re hiring ! Software Engineer Wesley Moy (wesley@nextdoor.com) Nextdoor is the neighborhood hub for trusted connections… 1 Testing Android App Development QA Technology 1 clap 1 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"},
{"website": "NextDoor", "title": "what to log in production environments", "author": ["Aaron Webber"], "link": "https://engblog.nextdoor.com/what-to-log-in-production-environments-cc9ad82bdce9", "abstract": "Culture Open Source Technology You can log some of the events all of the time, or log all of the events some of the time, but you can’t log all the events all of the time. In a production environment, you can’t log everything. Not only can logging at a DEBUG level negatively effect performance, it can make it hard to sort through a flood of log events to find the relevant ones. But seeing all of the rich, informative logging you have added to your code can be critical when investigating issues on production. So at Nextdoor, we built a tool on top of the standard Python logging library and Zookeeper to allow us to change the logging level for individual loggers on specific servers, without having to deploy any code changes or restart processes. The general way that we use logging in Python is to define a logger per module. Each logger object is a singleton, so any time logging.getLogger is called with the same argument, it will return a reference to the same Logger object. Loggers can have the level they are logging at changed by calling setLevel . The next piece of technology that enables dynamic logging levels is Nextdoor Service Registry , a tool that uses Zookeeper for dynamic configuration. We set a watch on a particular Zookeeper node in all processes that we want to have dynamically updatable logging levels. The value of the node we use to control logging levels is a JSON-serialized dict, whose keys are regular expressions that will match one or more of our servers, and whose values are dicts of logger names to logger levels. When the callback is called, it deserializes the dict, and loops through each of the keys and sees if it matches the hostname of the current machine. If it does, it gets a reference to each of the loggers in the dict and sets their levels. We have a very simple tool that allows you to set the value of the node. You enter a regular expression, a logger name and the level you want that logger to be set at. Changes take effect across the entire fleet of servers within seconds. Being able to control the logging levels on running servers has been very useful at Nextdoor, both because we can turn logging levels up in order to debug hard-to-reproduce issues on production, and turn them down to allow us to focus on just some logs. Also, knowing that you can log at a DEBUG level in production makes DEBUG logs more valuable and encourages engineers to write more DEBUG logging. For instance, some of our more complex and performance-sensitive functions will write DEBUG logs after every step in the function, so that we can see the duration of each step and quickly identify which are slow. More commonly, DEBUG logs will log the inputs and outputs of a function. Logging is a crucial tool for any application, because it won’t always be easy to reproduce issues in development where you can use an interactive debugger. Being able to control logging at runtime lets us spend less time trying to set up the conditions to reproduce issues in dev and more time writing code. If you enjoy building useful tools like these, we’re hiring ! Nextdoor is the neighborhood hub for trusted connections… 6 Python Django Logging Technology 6 claps 6 Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Written by Nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. We believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-31"}
]