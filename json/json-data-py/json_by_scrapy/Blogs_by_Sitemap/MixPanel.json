[
{"website": "MixPanel", "title": "dynamic configuration at mixpanel", "author": ["Nikunj Yadav"], "link": "https://engineering.mixpanel.com/dynamic-configuration-at-mixpanel-94bfcf97d6b8", "abstract": "Move fast and (un)break things Your code is often powered by many constants and configurations. Imagine passing this static configuration to your code: You wrote your code with this static config and deployed it to 100 machines. All is good in the world until you need to change it. For us, these changes were frequently motivated by one of the following reasons: Adding a customer to a new alpha feature Remove a customer from said alpha to debug query failures Rate limit a particular client to avoid resource starvation Rolling out a feature to a stateful system carefully Changing timeouts, number of retries, rate limits, etc In the static configuration world, you make the config change in code, rebuild your service binary, and redeploy. This can be time-consuming, particularly for systems with large numbers of replicas or stateful systems that have elaborate graceful shutdown procedures. Ideally, you could update these configs “live” (without requiring a restart), for much faster iteration speed. A dynamic configuration system allows developers to update configs without redeploying their services. Companies like Facebook and Twitter have built pretty sophisticated systems for this very purpose. At Mixpanel, we decided to write our little way of managing dynamic configuration for our Golang services, built on Kubernetes. Fast: When a change to configuration is made, all the related deployed services should see the effect in under a minute. Flexible: Configs should be stored in a format that allows for arbitrary values Safe: It should be possible to write validation on the configs. Version Controlled: As a natural extension, it should be easy to revert to an older version of the config. Easy-to-use: There should be easy to use SDKs for services, that make it seem like they are just fetching regular config values. With these design principles in mind, we set out to write something quickly that covers 90% of our use cases. Mixpanel runs on GKE and most of our services are written in Go. Kubernetes already has something called configmaps , which allow developers to use Kube’s SDKs to create, update, and apply configurations to services or deployments. Mounting a configmap simply places the configs in files (key being the filename, the value being the contents) in the pod’s filesystem. Any changes to the configmap are done just like applying any other changes to a kube object. This reliably solves the problem of shipping configurations to pods in a consistent and real-time way. Our configurations are written in YAML files as described in a section below. These files are checked into our git repo, and changes to these follow the same dev workflow as changes to code. This way, all our configurations have version control and go through a review + CI process. Initially, we decided to group configurations for the same service together. For example, for our querying service, we would have a single scope “query” under which all the configurations used in that service live. Later, we decoupled configs and services (though they are often closely related) for the following reasons: Some configurations (eg whitelists) supported a large feature that spanned multiple services Two different teams who work in a single service want to update configs independently without the mental overhead of reasoning through each other’s configs. Today, most of the configurations are scoped based on the product vertical as opposed to individual services. Here is an example of what our configmap ends up looking like: The example above will end up mounting the configmap and creating a file called configs.json with the JSON contents inside it. You can notice that each config is At the moment we only have one SDK written in Go. The SDK does the following: Load the contents of configs in the configs.json file into memory whenever it changes. This is achieved using fsnotify . Provide an interface to fetch the configuration by the config name. At the time of writing, this is what the SDK interface looks like: This SDK is open-sourced! You can find it at https://github.com/mixpanel/configmanager Kubernetes allows developers to describe resources (services, jobs, configs) as declarative YAML. At our scale of services, we’ve found hand-writing this YAML to be duplicative and error-prone. Instead, we write Jsonnet , which is then compiled to generate YAML files. Both the Jsonnet and YAML files are checked into our repo. To make config changes declarative and easy for our developers, we wrote the following Jsonnet library. Given this kind of syntactic sugar, we can write our sample config: Compiling this will produce the YAML example above. Configmanager (which is what we creatively call this library) is used widely across our backend services at Mixpanel. Here is just a small sampling of its uses: Storage: Pause and resume button on CPU intensive features such as fulfilling GDPR deletion requests. Ingestion: Rolling out critical features such as event de-duplication and identity management on our ingestion layer. Configmanager was used to add and remove from the whitelisted set of customers and gradually ramp up traffic. Query: Invalidating caches for a single customer or globally in case of caching format changes or bugs. Everywhere: In various parts of our stack, it is used to control the amount of parallelism available to handle our workload. Another common use case is changing rate limits. To debug the configmanager itself, we have integrated it with our metrics reporting and logging system to report various kinds of errors. It also has some debugging HTTP endpoints that return the in-memory config state in memory or force a configuration reload. We also wrote a dummy configmanager implementation with the same interface for use in unit tests. We are pretty happy with the flexibility and developer speedup provided by the configmanager. That said, there is room for improvement; here’s what we plan on changing in the future: Adding a UI Currently, developers use their favorite editor and Kubernetes CLI to view and edit configs. Though these are simple and get the job done, for gigantic config files, it may be easier to work with a UI rather than a single massive JSON file. You can edit kube configmaps on google cloud, but it is not the best experience. More powerful configuration language Currently, configs are simple key-value pairs. Any additional logic must be encoded by the client. Here is an example of the hypothetical config key route_to_queue: To do this kind of switch statement style routing, you can store a JSON object and write some code that evals the JSON object and write the switch statement in code. Alternatively, you can express config values themselves in a flexible grammar and encode this switching logic in the configs themselves. It also makes it so that we don’t have to write one-off methods like IsCustomerWhitelisted. In the past, I used to work on the dynamic config team at Uber and we modeled our configuration values so that every config key had a default value and list of exceptions each of which had a rule written in PEG grammar and associated value. This proved to be quite flexible and useful for a large number of teams, but with the downside that it was harder to reason about a large config with exceptions. Here is a post I wrote for that. At some point, if there is enough need for it at Mixpanel, we may extend the configmanager in this way. https://medium.com/google-cloud/kubernetes-configmaps-and-secrets-68d061f7ab5b https://jsonnet.org/ https://github.com/mixpanel/configmanager https://medium.com/@nikunjyadav/generic-rules-engine-in-golang-using-antlr-d30a0d0bb565 Stories from eng @ Mixpanel! 77 1 Programming Golang Configuration Management Kubernetes Mixpanel 77 claps 77 1 Stories from eng @ Mixpanel! Join us at https://mixpanel.com/jobs! Written by Stories from eng @ Mixpanel! Join us at https://mixpanel.com/jobs! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-04"},
{"website": "MixPanel", "title": "live downsizing google cloud persistent disks for fun and profit mixpanel engineering", "author": ["Tyler Chae"], "link": "https://engineering.mixpanel.com/live-downsizing-google-cloud-persistent-disks-for-fun-and-profit-mixpanel-engineering-c84869a3dd73", "abstract": "At Mixpanel, we heavily utilize Google Cloud Platform(GCP)’s SSD provisioned persistent disk (PD-SSD) to store the event data that underlies our service. We chose it to support our low-latency analytics database, which serves customer queries. We found one major problem with PD-SSD: the cost. PD-SSD is 5x the cost per GB stored than Google Cloud Storage (GCS). Our infra team designed Mixpanel’s database, Arb, utilizing both PD-SSD and GCS; it stores recent, frequently changing data in PD-SSD for low-latency reads and much older, immutable data in GCS for a lower cost and acceptable latency. As all engineers do, we first engineered for correctness/performance and later optimized for cost. PD-SSD initially held a large proportion of data for performance reasons, accounting for significant portion of our infrastructure cost. However, given that the vast majority of our data is immutable, we could shift the balance to GCS to drastically reduce our storage cost. While read latency was initially a concern (average latency in GCS is worse than in our PD based solution), we realized that in a system like Arb, where a single query touches hundreds of disks, p99 matters more than average for end-user latency. We confirmed that while GCS was slower on average, its p99 was consistently equal to or better than PD-SSD at our throughput. After the latency experiment, we shifted data older than 3 months to GCS, leaving our PD-SSD utilization was incredibly low. To realize the cost savings, we needed to downsize the disks. Unfortunately, GCP does not support downsizing a PD. This blog post describes how we built our own solution to do just that. Live : The most important requirement is not impacting our customer’s experience. PD-SSD downsizing should not cause data delay / query downtime. Idempotent : Repeat downsizing of the same disk should be a no-op. Automated : Downsizing should be triggered by a single command. Revertible : Design should allow rollback to the prior disk in case of error. Multiple Zones : Arb uses multiple GCP zones for redundancy with each zone holding an independent copy of data and its own compute nodes. This design comes with several benefits: distributes load during peak time, relatively simple query and storage logic to maintain and improve, and no downtime during deployment or canary. We leveraged this for downsizing. Kafka : Kafka is a key component of our Ingestion pipeline. Our ingestion servers push events to Kafka, which then get consumed by Storage Servers to be persisted to PD-SSDs. In our configuration, Kafka maintains data for 7 days, so that even if a Storage Server reverts to six days ago, it can catch up to the most recent data without any data loss. GCP PD Snapshot : GCP provides a feature called Snapshot, which is what it sounds like: it creates a snapshot of a PD. Moreover, GCP supports PD-SSD creation based on an existing snapshot. It essentially clones a PD-SSD to a new PD-SSD (of the original size) with the same data. GCP intelligently creates snapshots incrementally based on previous snapshots, so it is storage and time efficient. Kubernetes Jobs : A Kubernetes job creates one or multiple pods and tracks the status of the pods. If a specified number of pods are completed successfully, the job is completed. GCP node pool : A node pool is a specified number of nodes with the same configuration. The configuration includes the number of CPUs and GPUs, size of memory and storage, etc. Here is our solution with the given primitives. The first step is cloning a disk under a different pod called Downsizer using snapshot. The Downsizer pod is created in a separate node with ample resources. This guarantees the downsizing process has no impact on live Storage Servers, and allocates enough I/O throughput to perform downsizing as quickly as possible. There is a short period where reads/writes are blocked on this disk (up to 15 min) while taking a snapshot. After the snapshot is created, the Storage Server will catch up on backlog quickly and start serving traffic normally. Creating the cloned PD-SSD is performed independently in the Downsizer pod. If something goes wrong, the process is fully revertible using the snapshot. The Downsizer pod is created by Kubernetes Job. Each PD-SSD downsizing job is named with the PD-SSD’s unique id, and the job creates a Downsizer pod with the same name. This Kubernetes configuration provides idempotency as a Downsizer job/pod for the same disk cannot be created multiple times. This pod gets assigned to one of the nodes in the Downsizer node pool, with 16 CPUs per pod. This guarantees high PD-SSD performance in finishing the downsizing job in a timely manner, as the disk I/O throughput is roughly proportional to the number of CPUs of an instance. The node pool is specifically created for the downsizing process, allocated with enough CPU, memory, and the specified number of nodes. We successfully created a clone of the original disk, so it’s time to create a smaller PD-SSD and start copying the data. Because of the amount of the data and the throughput limitation, this process may take up to 10 hours. This process is performed in an independent Downsizer pod while Storage Server is serving traffic as usual, so there is no downtime. The smaller PD-SSD size is determined by the disk usage of the cloned disk, and utilization factor. We used rsync to perform data copy/synchronization. rsync is reliable, fast and comes with built-in transmission checksum. Note that we did not use --checksum flag, which performs an additional checksum on the existing files and slows down the whole process significantly. After the rsync is finished, we unmounted the smaller PD-SSD to guarantee buffer flush. During step 2, the Storage Server is serving data as usual which means there shall be data added to the original disk that was ingested during the rsync process. The added data will turn into a backlog for the disk to catch up. When there is a backlog for a disk, Mixpanel’s database ensures that reads for data on that disk goes to the other zone. Once the backlog drops below a watermark, that disk becomes readable again. Thus a higher backlog leads to longer query downtime for a disk in one zone. To minimize the backlog, we decided to perform step 1 and and run an second rsync to the disk created in Step 2. The advantage comes from the fact that we already copied most of the data in the downsized disk. rsync compares the metadata of all files and copies only the difference. The data transfer rate of one PD-SSD to another in a dedicated node with ample resources is much faster than going through the ingestion pipeline, including Storage Server, which may share the I/O throughput with multiple PD-SSDs. This second rsync process takes only about 10 to 20 mins, and this additional time is well worth it. Mixpanel infra engineers have been leveraging GCP’s snapshot as a periodic backup. Also, we take snapshot before performing any disk-related engineering work in case something goes wrong. We implemented and utilized the capability to restore a Storage Server disk from a snapshot, and we are going to use this for Downsizer. After the second pass of rsync is finished, Downsizer takes a snapshot of the downsized disk. This downsized snapshot contains complete data and disk size information. By triggering restore from the snapshot, Storage Server stops the read and write to the original disk, deletes the disk, and creates a smaller disk with the downsized snapshot. Now we have the downsized disk mounted to a Storage Server, but we are not quite done yet. Even after the second rsync , there is still a 20 ~ 30 mins gap between the time that the second snapshot is taken, to the time that the restore from the downsized snapshot is finished. If we serve this disk to query data right away, we may serve stale data and we need to avoid this. Downsizer is responsible for blocking reads to the disk, monitoring the amount of backlog for the disk, and enabling read only when the disk catches up to the most recent data. This step usually takes less than 10 mins. The previous steps show the process of downsizing one disk, but we have hundreds of disks. It is not scalable to run Downsizer manually, one at a time. We need a tool to automate downsizing for many disks. One zone at a time : From the downsizing description above, there are some steps that require unavoidable downtime: taking snapshots, restoring from a snapshot, and waiting for backlog. If we restrict the downsizing process in only one zone, then the other zone would serve the traffic normally, so no downtime would caused by Downsizer. 10% of the disks in one zone at a time : Conservatively, stopping about 10% of the disks in one zone has shown minimal impact on our customer’s experience. Based on this, we built a tool that performs downsizing for one zone: Get a list of all disks in one zone Check the number of Downsizer Kubernetes jobs that are completed and running. If the number of currently running jobs is lower than 10% of the total number of disks, create more jobs to meet 10% Repeat Step 2 and 3 until all of the disks are finished This approach is even more conservative as it is highly unlikely to overlap the disk downtime all at the same time to reach 10% of the disks in a zone. The last problem left to solve was how to execute Downsizer with a minimal impact on productivity of other engineers. At Mixpanel, deployment and/or canary happens almost every day, and running Downsizer means blocking concurrent Query Server and Storage Server deployments. We use a lightweight deployment lock system to allow engineers to take “locks” on resources, preventing concurrent deploys. We initially estimated ten hours per disk for downsizing, and we have about 300 disks per zone. Running 10% of the number of disks at a time, it would take about four full days to complete. Even considering the weekends, we needed to block deployments for two work days, and that was not acceptable. We could have just run it during weekends, but that was also not acceptable at the beginning because we didn’t know if downsizing would reveal unforeseen side effects, and we wanted to observe and catch them early before downsizing all of the disks. The solution was rather simple: acquire deployment lock at the end of work day and start Downsizer; stop Downsizer around midnight, and release lock in the morning after all running Downsizer jobs are finished. Of course, communicating with other engineers (especially those holding the pager) was a must. In this way, we started off with a smaller number of disks and observed any changes in production behavior. We found that it takes about four hours on average to downsize a disk, because we have less data than what we had originally planned for. After a couple nights of downsizing, we confirmed that there were no side effects observed, so we kicked off full downsizing for one whole zone during the weekend. Now that we knew it takes less than two full days to downsize with no side effects, we confidently ran the other zone during the weekend, several weeks later. After the whole process was finished, we reduced Arb PD-SSD usage from 1.05PB to 390TB. That’s about $112k per month cost reduction minus the increased GCS cost. The GCS cost increased approximately about $20k including storage and operations cost. Thus the overall cost reduction is about $90k per month, $1M per year! There are several things that we learned from this project: Kubernetes and GCP : Idempotency could have been something tricky to achieve. With Kubernetes jobs, however, we just needed to configure the jobs correctly. Snapshots and node pools were very handy as well. Understanding the provided primitives is key for efficient tool building. rsync and buffer flush : rsync is a powerful and efficient utility. We tested parallelizing rsync by using parallel , and we found no difference in return. The second pass sync would have been hard without rsync . Unmounting the disk was the method that we used to guarantee the buffer flush. 2nd pass sync for minimal backlog : When we first implemented without the second pass sync, we set a timeout for backlog catch up time to several hours, proportional to the time that it took to rsync data from a cloned disk to a downsized disk. Depending on a Storage Server’s available write throughput, it may take up the entire time allotted, and during that time, read is blocked. This problem is solved by the second pass sync and minimized query downtime. Back-of-envelope calculation : We planned out each step by back-of-envelope calculation first, and then performed implementation and execution. After execution, we checked if it matched our expectations. If it didn’t match we investigated and figured out why there were discrepancies. This way, we predicted ops impact accurately and was able to execute the downsizing process with minimal pain. Communication : We looped in service owners at the design and planning stage. We learned all the available in-house utilities that we could use, and the service owners understood what we were planning to do, giving us valuable advice. This communication continued throughout the project and was the key factor of finishing it efficiently with minimal trial-and-error and side effects. If you enjoyed this article and are interested in working on high-performance distributed systems at Mixpanel, we are hiring ! Originally published at https://engineering.mixpanel.com on July 31, 2018. Stories from eng @ Mixpanel! Cloud Computing Persistent Storage Distributed Systems Google Cloud Platform Mixpanel Stories from eng @ Mixpanel! Join us at https://mixpanel.com/jobs! Written by software engineer Stories from eng @ Mixpanel! Join us at https://mixpanel.com/jobs! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-03"},
{"website": "MixPanel", "title": "petabyte scale data deduplication mixpanel engineering", "author": ["Karthick R"], "link": "https://engineering.mixpanel.com/petabyte-scale-data-deduplication-mixpanel-engineering-e808c70c99f8", "abstract": "Mixpanel ingests petabytes of event data over the network from the mobile, browser, and server-side clients. Due to unreliable networks , clients may retry events until they receive a 200 OK message from Mixpanel. Although this retry strategy avoids data loss, it can create duplicate events in the system. Analyzing data with duplicates is problematic because it gives an inaccurate picture of what happened and causes Mixpanel to diverge from other client data systems that we may sync with, such as data warehouses . This is why we care deeply about data integrity. Today, we’re excited to share our solution to deduplicating event data at petabyte scale. To address this problem, we needed an approach that was: Scalable: Scales to an ingestion volume of 1M+ events/sec Cost-Efficient: Optimizes cost/performance overhead to ingestion, storage, and query Retroactive: Identifies duplicate events sent arbitrarily later Lossless: Preserves duplicates to rollback in case of misconfiguration Maintainable: Minimizes operational overhead The industry has a lot of creative approaches to solve the deduplication problem. The central theme involves architecting an infrastructure that performs deduplication at the ingestion layer. Customers send a unique identifier, $insert_id as a property on each event. The deduplication infrastructure stores the $insert_id for all the events within a limited retention window (e.g. 7 days) and checks it against every new event for duplicate identification. A key-value store like sharded RocksDB or Cassandra is often used for storage. The lookup costs in the storage can be improved using a bloom filter . This kind of architecture ensures that the duplicates are weeded out of the system at its entry point. However, this approach fails to meet our requirements for the following reasons: ✅ Scalable : Sharded key-value stores can scale horizontally ❌ Cost-Efficient : Requires a separate data store and infrastructure for duplicates ❌ Retroactive : Can only capture duplicates in a limited retention window ❌ Lossless : Drops data at ingestion, so it’s impossible to rollback ❌ Maintainable : Dedupe becomes an additional service that must be up 24×7 We architected a solution that satisfies all of our requirements, by ingesting all events and deduplicating them at read-time. A simple approach to read-time deduplication would be to build a hashtable of all $insert_ids on each query; however, this would add non-trivial overhead to our system. But before describing our solution, let’s review a few key aspects of our architecture . Mixpanel’s analytics database, Arb, shards its data files by project, user and event time. This allows us to ensure that all data for a given user is co-located, so that behavioral queries can run across many users simultaneously over relevant portions of time. In Arb, all the events are written to append-only files, which are periodically indexed (compaction) into columnar files in the background. Append-only files are indexed when they reach either a size or age threshold. This approach ensures that queries are both real-time and efficient, by scanning both small, real-time, append-only files and the large, historical, indexed files. We leveraged these two aspects of our architecture to make read-time deduplication efficient. By first principles , event duplicates have the following properties: Event duplicates belong to the same project Event duplicates belong to the same user Event duplicates belong to the same event-time We glean the following key insights from these fundamentals: We can scope down the search space for event duplicates to the project, user and day — ie, to a single Arb shard. We can minimize the overhead of deduplication by amortizing it alongside our lambda architecture to maintain both real-time and efficient queries. These insights lead to a solution that satisfies all of our requirements. Deduping within Mixpanel infrastructure happens both at indexing time and query time. Our indexer maintains an in-memory hashset by $insert_id for all the events from the files that are being indexed. If it sees a hit on an event, that event is marked as a duplicate by setting a bit on the event in the indexed format. This process has minimal overhead since indexing occurs at the fine-grained shard level. At query-time, due to our lambda architecture, we scan both indexed files and append-only files. For indexed files, we can check if the duplicate bit is set, and if so, skip processing the event. For the small, append-only files, queries do hash-based deduping on $insert_id. This allows us to be both real-time and efficient, leveraging the power of the lambda architecture. From our experiments, we found that indexing of files with 2% duplicates has the time overhead range of 4% to 10%. This does not have any direct impact on our user experience, as indexing is an offline process. For query-time, we found that reading an extra bit for every event adds around 10ns to the reading of data. This is close to a 2% increase in the query time because of the additional column. Reading 10 million events adds a time overhead close to 0.1 seconds (100ms). For reference, Mixpanel’s largest columnar file as of today contains around 2 million events (thanks to project, user, time sharding). We think the trade-off on time overhead is quite acceptable given the win we get on unlimited retention window and minimum operational overhead. Our solution doesn’t perfectly handle the following scenario: An event duplicate pair spans an append-only file and an indexed file for the current day. We properly identify duplicates within a given indexed file or within an append-only file, but not across both. We chose to accept this tradeoff for the following reasons: This is extremely rare: 99.9% of customers are small enough that a full day’s ingestion can fit into a single, append-only file. This means 99.9% of customers will not be affected by this issue. For the largest customers that could encounter this issue, we estimate ~0.5% chance that an event and its duplicate will span two files. Our system will eventually heal itself in that once the day concludes, we will index all files for that day into a single file. So duplicates will only transiently appear for that same day. We found that the advantages of our approach outweighed this trade-off. We leave it as future work to perform real-time deduplication across files for the latest days as well. In this blog, we discussed our architecture that distributes duplicate identification at the indexing layer and duplicate filtering at the query level. This solution has been live in Mixpanel for the last 6 months. If you enjoyed this post and are interested in working on similar distributed systems problems, we are hiring ! Please feel free to leave a comment or connect on LinkedI n if you have any questions or comments. Originally published at https://engineering.mixpanel.com on July 18, 2019. Stories from eng @ Mixpanel! 17 Big Data 17 claps 17 Stories from eng @ Mixpanel! Join us at https://mixpanel.com/jobs! Written by krazy! Stories from eng @ Mixpanel! Join us at https://mixpanel.com/jobs! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-03"},
{"website": "MixPanel", "title": "safely rewriting mixpanels highest throughput service in golang mixpanel engineering", "author": ["Evan Noon"], "link": "https://engineering.mixpanel.com/safely-rewriting-mixpanels-highest-throughput-service-in-golang-mixpanel-engineering-62cd69b5ebdb", "abstract": "It’s always important to use the right tool. I grew up working on small outboard engines with my father and learned this lesson the hard way. I only had to strip a bolt once to realize that using the almost-right tool could result in a mistake that took hours to correct. At Mixpanel, we try to use the right tools to allow us to move quickly, and with confidence. One great example of this is how we used Diffy to check against live production traffic when migrating our ingestion API from Python to Golang. A few years ago, we decided that Golang was the best tradeoff of readability and performance for our critical infrastructure services. In order to realize the gains and maintain a homogenous code base, this meant we had to migrate the existing performance-critical Python services. Since then, almost all of our existing critical infrastructure has been migrated to Golang, with the glaring exception of our data collection API . The main reason for this, honestly, is summed up by the cliché adage, “if it ain’t broke, don’t fix it”. The data collection API, where all incoming data is first processed, is one of the most critical services at Mixpanel. Downtime can result in lost data, increased latency can ruin user experiences, and incorrectness can cause permanent data corruption. The risk of migrating to brand-new code, in a completely different language, outweighed the potential benefits. In January, we finally decided that in order to meet our 2019 goals, the data collection API needed to be migrated to Golang. First, let me describe how our existing API was setup and why. Figure 1 shows the environment the Python API was originally built for. At the time, Mixpanel was running our code directly on real, 8-core machines (not VMs), and since the Python API was designed to only use a single processor, we ran eight instances of the API serving eight consecutive ports. In front of the group of API machines was a handful of nginx instances which accepted the HTTP traffic and load-balanced it to the API servers. After processing the request, the data was added to a series of kestrel queues, and a simple error code returned to the users. In 2018, we migrated from using real machines to using a kubernetes deployment hosted on Google Cloud Platform. We made several setup changes while keeping the core Python API code untouched. Figure 2 shows the end result of that migration. A request was directed, via the Google Load Balancer and kubernetes to a kubernetes pod, where an Envoy container load-balanced between eight Python API containers. The Python API containers then submitted the data to Google Pubsub queue via a pubsub sidecar container that had a kestrel interface. From the perspective of the API, nothing had changed, it still received events, processed them, and submitted to a “kestrel” queue, which was really just a proxy for submitting to Google Pubsub via Golang. At this point, the next logical step was to update the actual API. The Python API wasn’t built with the current GCP environment in mind, and it took several hacks to get it working. The only hesitation was that the API’s functionality was very complicated and nuanced. We needed a way, in addition to unit tests, to guarantee correctness for all the edge cases we see in the wild. Mixpanel processes over a trillion data points a month, and at this scale, exceptionally rare edge cases are seen every single day. We needed to test directly with a large amount of live traffic, without affecting production. To enable testing against live traffic, we created a dedicated setup. The setup was a separate kubernetes pod running in the same namespace and cluster as the API deployments. The pod ran an open source API correctness tool, Diffy, along with copies of the old and new API services. Diffy is a service that accepts HTTP requests, and forwards them to two copies of an existing HTTP service and one copy of a candidate HTTP service. It compares the responses, and then shows the differences via a web page. Using the two copies of the existing service, it categorizes expected differences, such as timestamps and random values, as noise, so that unexpected differences are easier to discern. We chose Diffy because we were able to filter out random noise, the comparison code could handle deeply nested lists and maps in JSON, it was able to handle our scale, and it was super easy to deploy within our existing infrastructure. With this setup, the comparison code was nicely encapsulated, and easily portable to all of our API clusters. There were two additional changes that needed to be made. First, we had to modify our services to get the best analysis with Diffy. Typically, the services submit data to a queue and then return just an error code to the client. Instead of this, the modified services returned the data and error code. This was to ensure the processed data was correct, and fully utilize Diffy’s correctness checking. Second, we needed to direct live shadow traffic to the comparison pod, which we did using the existing Envoy containers. Using Envoy, this requires two simple steps. First, we added the Diffy service as another type of cluster, which can be seen below: Second, we added a request_mirror_policy to our regular route. From Envoy’s documentation , a request_mirror_policy is implemented as “fire and forget”, which is exactly what we needed for this scenario. Further, Envoy allows you to limit the amount of traffic sent to the shadow cluster, using a runtime value. Since we only wanted to run one instance of the comparison pod, we opted to limit this quite a bit, typically only directing between 0.5% and 2.0% of traffic. The change to add the request_mirror_policy is below: The final Diffy pod setup, as described above, can be seen in Figure 3 below. Diffy was an invaluable tool during this migration. We were able to compare the processed data from the existing API and the new API, patch the code, and quickly deploy again to verify the fix. At least a dozen bugs were found and fixed in quite a short time frame. We eventually were able to run at least ten million live samples at a time through Diffy without seeing any unexpected differences. Checking the new service with this many live samples made us especially confidence in its correctness. Though this setup helped us avoid many critical bugs, it wasn’t perfect. In particular, we needed to change how the APIs handled the processed data in order to return it in the response. For the new APIs, we did this using a specific error type. This generally worked well, but resulted in one bug since Diffy was unable to test a particular non-error code path. Also, we weren’t able to do any type of load-testing using this setup. Still, despite these issues, this setup sped up the migration process and greatly reduced the number of bugs. The final setup of our data collection API can be seen in Figure 4. You can see that only the multiple API containers and pubsub sidecar have changed, otherwise the surrounding services are identical. One huge improvement is we only need to run a single API container per pod, since the new Golang code uses multiple processors. Now, we have a clean slate to begin further optimizations on our data collection API. We’ll definitely continue using Diffy to ensure correctness and allow our team to move fast on changes. At Mixpanel, we strive for a balance between reliability and innovation. Our customers rely on our services to make critical business decisions, and any downtime can impact their ability to make these decisions quickly and correctly. However, we are constantly trying to improve the tools we offer to better suit customers’ present and future needs. Using tools like Diffy, we can achieve both goals, and do so confidently. There was a comment about the gains we saw from this migration. I checked the data, and from a rough calculation we saw about 40% decrease in the amount of CPU resources used. See Figure 5 for a chart of CPU usage over time. There was a comment about the gains we saw from this migration. I checked the data, and from a rough calculation we saw about 40% decrease in the amount of CPU resources used. See Figure 5 for a chart of CPU usage over time. Stories from eng @ Mixpanel! 36 Docker Golang Python Google Pubsub Envoy Proxy 36 claps 36 Stories from eng @ Mixpanel! Join us at https://mixpanel.com/jobs! Written by Stories from eng @ Mixpanel! Join us at https://mixpanel.com/jobs! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-03"},
{"website": "MixPanel", "title": "power of 2 choices in practice", "author": ["Vijay Jayaram"], "link": "https://engineering.mixpanel.com/power-of-2-choices-in-practice-b6097020dfad", "abstract": "Access patterns dictate how data should be stored. Frequently written data should be in a row-oriented format, while frequently queried data should be in a column oriented format. Mixpanel’s in-house database, Arb, supports both real-time event ingestion and fast analytical queries over all history. Storage nodes read data from a queue and write it to disk using a row-oriented write ahead log (WAL). We run a service called compacter which converts data from the row format to a columnar format. This service runs in an autoscaling nodepool on Google Kubernetes Engine (GKE). When a storage node has a row file of a certain size or age, it sends a request to a random compacter node to compact it. The compacter returns either a handle to the resulting columnar file or an error. If all compacters are busy, they shed load and storage nodes retry with some backoff. While compacter runs outside the critical path of queries, it must be reliable and efficient, since all new data in Arb passes through it daily. Given the nature of the work it has to do, the compacter service is very computationally expensive. Compacter costs grew steadily in the latter half of 2019, in part due to our growing customers and in part due to new use cases for data transformation. Worse yet, compacter failed to autoscale at the right times to absorb spikes in load. This resulted in engineers manually setting the autoscaler’s minimum number of replicas to a high number when load was high or during a service-wide incident. This cost us both in engineer time and money spent on wastefully provisioned instances. We first looked at utilization, which for a compute-bound autoscaling service, we expected to be 80–90%. This assumption seemed reasonable because of the way Kubernetes’ Horizontal Pod Autoscaler (HPA) works. You can configure it with a target service (compacter) and a target metric for that service (average CPU utilization of 90%). If CPU utilization exceeds the target by some threshold, it schedules more pods, and vice-versa if utilization is lower. Some hysteresis is built in to avoid constant thrashing. In our case, however, average CPU utilization was ~40%. Averages can hide skew, so we plotted the median and 90th percentile utilization. This showed that half of the compacters did little work, while the top 10% were maxed out! Aggregate utilization was low because the autoscaling algorithm uses averages, not percentiles, to make its scaling decisions: At first, we found this skew surprising: Storage nodes send requests to compacters based on a uniformly random distribution Compacters receive 1000s of requests per minute, so through law of large numbers, these requests should have balanced over the 30–40 nodes we had. It turns out that even if you randomly distribute requests, skew can occur when the individual load items have a very uneven distribution. This happens in our case because of the vast range of customers we have, ranging from startups with thousands of events per day to large companies with billions per day. This type of power law, where your largest users are significantly larger than your smallest ones, makes simple averages a lot less useful. Once we identified skew as the issue, we considered a few solutions of varying complexity: (High) Use a queue : Insert a queue between our storage nodes (clients) and compacters. This switches from the current push-model to an asynchronous pull-model, where compacters only take work when they have capacity. While this leads to optimal utilization, it requires a fundamental architecture change, adds a queue component, and requires keeping track of what compaction work is in-flight. (Medium) Use a proxy: Insert a load balancing service between storage nodes and compacters. This service can keep track of the load of all compacters, and forward incoming requests to the least loaded compacter. This maintains our current push-based architecture, but adds a proxy in between. We considered this, but decided to validate our hypothesis with something simpler. (Low) Power of 2 Random Choices (P2C) : The idea here is simple. Instead of storage nodes randomly picking 1 compacter, they randomly pick 2. They then ask each compacter for its current load, and send the request to the less loaded of the two. In theory, this comes within a constant factor of the above two solutions. We also wrote a quick Python simulation to confirm our intuition on how this works. We started with the P2C approach because we could implement it in less than a day. And we were not disappointed. Here is the median and 90th percentile load across our compacter nodes before and after implementing Power of 2 balancing. With the above improvement to balancing, and a few other smaller tweaks, we could reliably use average utilization to autoscale and saw compacters react predictably to load spikes. In turn, we saw both our average utilization increase to ~90% and our error rate drop to nearly 0 in steady state since we no longer needed to shed load. Given our increased confidence in the system’s behavior, we decided to take on a more advanced autoscaling strategy using preemptible VMs. Preemptible VMs are low-cost VMs offered by Google which are ideal for stateless, background workloads. While 1/3rd the cost of regular VMs, they come at a reliability price: Google can arbitrarily terminate preemptible VMs when their capacity is limited. We previously used preemptible VMs exclusively for Compacter. This was fine in the happy case, but catastrophic when Google would terminate them. So we decided to switch entirely to regular VMs, which improved reliability, but was more expensive. This time, though, we wanted the best of both worlds: what if we could get the cost benefits of preemptible VMs in the common case, but with the reliability benefits of regular VMs in the worst case? Turns out, we could! After doing some research and finding this excellent article , we configured our nodepools such that Kubernetes will automatically try to use the cheaper VMs (when available) and fall back to the more expensive ones otherwise. We shipped both of these optimizations over the course of 2 weeks in January 2020, and they dropped our compacter service costs nearly 70%. Importantly, these stood the test of time with minimal follow-up from the team. We learned a few lessons along the way that we hope are useful to teams building high scale services in the cloud: Autoscaling is hard to get right ! It requires a high-signal target metric that is resilient to skew. In some cases, you can front your autoscaling service with a queue and autoscale on queue length. However, when callers block waiting for the result or an error, introducing a queue can add substantial complexity to the system. Be suspicious of averages . Always look at percentiles or a full distribution to better understand your load. Because of power law, there may be a few requests that constitute a significant part of your overall load. P2C is a remarkably simple trick to address skew . In subsequent months, we applied it to a few other compute-intensive autoscaling services at Mixpanel with similar success. That said, it’s not without tradeoffs. Here are two caveats to consider before using P2C: It requires all clients to cooperate. If your service has just 1 client that you control, you can ensure that the client first checks for load and always sends requests to the less-loaded node. However if you have many clients, or ones that are out of your control, this does not work. If even one client doesn’t respect the power of 2 protocol, all bets are off in terms of skew! In our case, there are now a small number of internal clients to Compacter. We created a client library that encapsulates the P2C logic and other conveniences, which all clients use. If clients are truly external, it’s better to use an off-the-shelf proxy/load balancer to route requests (which may itself use P2C under the hood!). It introduces two extra round trips on each request to check load on 2 random nodes. This overhead is fine when the actual requests are somewhat heavy (like in our case) and client-server latency is negligible (eg: internal services in a single GCP zone). It’s not fine when the requests themselves are small or the round-trip latency between client and server is high. If you’re interested in creative solutions to challenging infrastructure problems, we’re hiring! Reach out to us via our careers page . Stories from eng @ Mixpanel! 23 Distributed Systems Performance Infrastructure Columnar Databases 23 claps 23 Stories from eng @ Mixpanel! Join us at https://mixpanel.com/jobs! Written by Stories from eng @ Mixpanel! Join us at https://mixpanel.com/jobs! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-02-18"},
{"website": "MixPanel", "title": "a study in a mixpanel engineers engineering progress", "author": ["Tiffany Qi"], "link": "https://engineering.mixpanel.com/a-study-in-a-mixpanel-engineers-engineering-progress-3c9903d1fd6a", "abstract": "I’m Tiffany, a Product Engineer at Mixpanel, and have been an engineer here for a year and a half. I’ve been tracking various time and Github data for fun during my time here and I thought I’d share my results to start a conversation and be completely transparent about how engineers spend their time, and how some aspects of their code changes over time. For the methodology and other graphs, view the original version of this piece . Some background: I started at Mixpanel as a Technical Support Engineer after graduating university. A year later, I transitioned to become a Software Engineer, working on features such as improving our Dashboards product and Custom Alerts using Python and Javascript. You can read my full story in my post Becoming a Software Engineer . Along the way, I wanted to ensure I was on the right track by logging my progress. It’s not an easy thing to quantify, but it’s something I think is necessary to concretely see improvement over time. I could gather time spent data and statistics from Github pull requests (PRs), and with that hopefully discover a personal “north star” metric that I would aim for every day towards engineering betterment. Disclaimer: this is a purely data-driven perspective on progress . Aside from code quality and process, there are many factors that contribute to the success of an engineer, such as the way she solves problems, works with other engineers and project stakeholders, and takes feedback, and I wish I knew how to track or quantify those aspects. Not letting perfect be the enemy of good, I chose to track time spent and Github statistics because it was quantitative and it’s not something that has been documented before. I understand that these are surface level metrics, but I hope the data on my own progress sparks a deeper discussion on what a “better” engineer means and what sort of data we can use to prove that and make it mainstream. There are many facets to progress and being a better engineer, and at Mixpanel, we highlight technical ability, technical leadership, and teamwork, which are integrated into our leveling guide. For the sake of quantification (and since I’m still early in my career), I focus this analysis loosely on technical ability. In terms of technical ability and code quality, the goal is to get a LGTM (“looks good to me”) on a significant PR and an approval to ship from a fellow colleague, or even better, a senior engineer. This single phrase shows that they approve of your process and code. In an attempt to quantify my personal progress towards this goal, we’ll look at four of these points in the data I collected: Number of comments : How many comments do I get on average? Number of PRs and their cadence : How often do I deploy code to our codebase? Pre-review and post-review time : “Pre-review” is the period of time in which I spend on beginning research and coding before asking others to review, while “post-review” is that which I spend incorporating comments and feedback from reviewers before shipping code to production. Deploy time is not added to the post-review unless I needed to revise something that came up during staging. How much time do I spend in these areas? Unique lines of code : How much code do I contribute? (This is by no means the way we evaluate or measure engineers at Mixpanel, but it was a piece of easy to get quantitative data that might be interesting to look at) Average working time : How long do I spend coding? (you check out the results in the extended data ) Ultimately, I’m looking to determine whether I can use correlations between my time spent at Mixpanel and these statistics to increase my chances of getting a LGTM more quickly. All the information displayed below are from pull requests, and the time spent is recorded in Google Calendar. I then piped the data into Mixpanel. You can read more specifically on my method in College Productivity Analysis or in the extended version of this piece with slightly different data . In this section, I pin a graph with a finding. The following data comes from 275 workdays (14 months), 182 PRs, 8,305 lines of code added, and 1,271 comments . For those who care about the 10,000 hour rule, up to this point I have 3,961 hours of engineering practice. Please note that while I took basic statistics in high school and in college, I do not have a background in data science. Thus, my analysis is limited to the lens of linear correlations and timescales, as well as some handy graphs Mixpanel has provided. For another lens to view the data (there are correlations, more graphs, and time groupings with vacation filtered out), check out the extended version of this piece . For each graph, the three speech bubbles above the dates (annotations) represents the start of a new project. The legend displays what each line represents from January 1, 2018 to March 31, 2019. Note that no data is filtered out, so some of the dips in the data are from PTO or vacation time. Here, we see the number of average comments and maximum comments per PR by month decrease over time. In general, with each new project there’s an increase of comments, and then a decrease. Here, we see the total number of PRs started, reviewed, deployed, and closed (not deployed) by month. Over time, we see that more PRs are pushed through over time, and PRs are going through faster. Luckily there aren’t too many PRs that were closed without being pushed to production. As a general trend, it looks like a few PRs were pushed in the beginning of a project before increasing in frequency. As a note, sometimes pre-review time also includes time redoing a PR when a reviewer lets me know that I either created the wrong feature or have to change the overall approach to account for an extra detail. In this way, the wording of this section is a little misleading-my primary goal for this ratio is to determine how much time it takes to refactor or address the comments of others on coding style or existing behavior, and if I started on the wrong foot or did not include something, I don’t feel that’s completely accurate in that representation. However, I do admit that moving some PRs’ post-review time into the pre-review time fudges the numbers a bit, and thus I advise to take this graph with a grain of salt. Here, we see the percent of the total amount of time spent on the PR as the “pre-review”. In general, this number increases over time. There’s a slight dip after the beginning of each project, but then increases again. Here, we see a graph of the median time by week rolling 5 weeks of the pre-review, post-review, and total time spent. Aside from the deep dips in data associated with vacations, in general there’s a higher percent of time spent on pre-review than post-review, and both numbers are gradually decreasing over time. There’s a drastic jump in time spent after each project, but generally declines from there. Here is a previous time comparison of the total hours spent per PR between the first project (the dotted line) and second project (the solid line). The first project started off slow before hitting a peak, then decreased. The second project was much more consistent over time. Note: lines of code are not the way we measure or evaluate our engineers, nor are they by any means a good measure of progress, but it is fun to look at and see changes over time. Here, we see a graph of the average number of unique lines of code added per PR. On average, the number of lines decreases, and in general the start of a project leads to increase of lines of code before decreasing again. At Mixpanel, we encourage engineers to create smaller PRs and ship with higher cadence, hence the gradual increase in PRs above and a gradual decrease in lines of code. This way, we can test and deploy within 30 minutes. Having a consistent stream of small wins that make things better for customers is a great motivator. Here, we see a graph of the total number of hours per month per PR divided by the total unique lines of code added per month per PR over time. It looks like in general this ratio is decreasing, which suggests that less time is spent per line of code over time. This could suggest that I have become more efficient by gaining deeper context on the project but it could also suggest that I’ve added more tests, documentation, or fixtures to increase more lines of code without necessarily requiring more time for review. While Mixpanel doesn’t have scatterplots, here’s a close approximation of the correlation between comments and total hours by comparing the ratio of comments to unique lines of code and total hours to lines of code by month. Both ratios are decreasing over time, which suggests fewer comments and hours over time. But this also suggests that the number of total hours are tightly coupled with comments, which makes sense given my inclusion of the post-review, which is essentially the comments I get per PR. Did I improve? Probably. It’s clear that there are fewer comments, more PRs deployed, and a higher ratio of pre-review to post-review in general and within projects, which does appear to be closer to the LGTM state. In addition, the difficulty of tasks most likely has increased given my level increase. There were some fluctuations in accordance with the first half of a project, which suggests… Starting projects at first will look less productive than usual. When I start a project, within that month I average more comments per PR, less time spent, a lower pre-review ratio, and less code added to production. Perhaps this is because I’m researching or still figuring things out, which leads to PRs that have more comments and more non-PR work. However, over time when I’m in the zone or know what’s going on, these numbers all trend towards the positive. “Simple PRs” don’t necessarily mean “small PRs”. When we look at the raw numbers rather than the averages by month from the above, the correlations between these amount of comments, lines of code, and hours spent per PR are weak to moderate at best, which suggests that it’s hard to define quantitatively what it means for a PR to be “simple”. However, there’s a higher chance of simplicity if we break PRs up into smaller chunks, which allows reviewers to understand a larger whole piece by piece and catch things that might have otherwise be missed on a larger PR. This is a common practice at Mixpanel and something that we encourage all of our engineers to do. I’ve learned a lot about software engineering over the past year, and there’s always more to learn. I definitely have a long way to go before I can create a significant PR with LGTM, but for now I’m pleased with the fact that there is progress and the numbers back it up. One of my goals for this project was to find a personal “north star metric”, but even after all this data I wasn’t able to find a single source as there are so many factors that contribute to progress. For now, I will continue to track these numbers and distill them in a way that can hopefully lead to more insights. In addition, I plan to track time estimates, difficulty, and break down my original pre-review statistic so that I can have more accurate data (more information about this in my original post ). I also hope to leverage Github statistics to make useful insights across all of our engineers. In the meantime, my action items to increase PR quality will be to: Continue to break up larger features into smaller PRs to decrease the complexity of the overall whole Ask for feedback early and often to ensure that the methodology is correct Like reading stuff like this? Check out another piece by technical lead manager Aniruddha who analyzed our engineering Github commit history . Feel free to leave a comment or connect on LinkedIn if you have any questions or comments or advice on what to track. We’re always hiring engineers and support engineers (if you aren’t ready for engineering quite yet)! I hope this article has inspired someone out there to track their data and draw more conclusions that can help the engineering community. Originally published at https://engineering.mixpanel.com on July 10, 2019. Stories from eng @ Mixpanel! 11 Software Development Data Producitivity Improvement 11 claps 11 Stories from eng @ Mixpanel! Join us at https://mixpanel.com/jobs! Written by Engineer @ mixpanel, quantified self enthusiast, UC Berkeley graduate. Soul seeker, still figuring things out. Stories from eng @ Mixpanel! Join us at https://mixpanel.com/jobs! Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-04"}
]