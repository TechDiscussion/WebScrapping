[
{"website": "Olx", "title": "designing a custom progress bar with timed breaks in android", "author": ["Sarina Dhamija"], "link": "https://tech.olx.com/designing-a-custom-progress-bar-with-timed-breaks-in-android-2937bb4d4f0f", "abstract": "About OLX Group Have you ever come across a special requirement of customizing the progress bar by introducing breaks in its progress. If yes, then you are at the right place. In this post, we will draw a circular progress bar with breaks which can be used as a video recording button in which breaks are an indicator of paused time of the video. Let’s understand the requirement for the video recording button. All the images shown above are different states of the recorder button. i. Initial state ii. Paused state iii. Resumed state with pause break Initially, the button will be shown by the outer circle. Now when the user clicks on this button, the recording starts, and the arc will show the progress of recording based on time, and as the video is paused, progress will stop. After this when the video recording is resumed the paused time will be shown by a small arc drawn in a different color which represents a break in the video recording. The Colour of the circles in the progress bar can be customized. This post is divided into the following sections Rendering UI : Here we will understand the initial requirements of the progress button Progress Bar State Logic : This part will cover different states maintained by the button to show progress and breaks Drawing Phase : Then finally we will cover how this button is drawn based on different recorder states. Now as we know the requirement, let’s see how we can paint this image. Attribute set : The color of the circles are customizable. They are added to the attribute set and can be added during the initialization of the button with other attributes. The variables defined in XML are retrieved while initialization of class which is then added to paint object of the circles and arc. Value Animator : This property animation is used to handle the expansion of the inner circle and progress of the outer circle. In this inner circle, the size will be used as animated values to handle animation of the inner circle, and the duration of the video being recorded is used as the animated value to show outer border circle animation as a progress bar. Interpolator : This defines the rate of change of an animation. This allows basic animation effects to be accelerated, decelerated, repeated, etc AccelerateDecelerateInterpolator: This interpolator helps in the transition of the outer circle to track the progress of recording. LinearInterpolator: This interpolator helps in the transition of the inner circle when the recording starts. Paint : This class is used to draw circles and arcs with different colors and styles. To show breaks in progress, the Paint object will be set with a different color. Each time the progress arc is drawn, the color of the Paint object, along with sweep angle and start angle, is changed to show paused state and resumed state. In this project Color. WHITE is used to show progress and Color. TRANSPARENT is used to show breaks in the progress bar. Recorder states : The RecorderStateManager manages four states of video recording : INIT RESUMED RECORDING PAUSED When a user clicks on the recorder button then based on the current state different actions are performed. For example, If the current state of the button is INIT, this means the button is just initialized and on click, the recording will start. In this case, onClickStart() method will be called which is responsible for tasks like saving resume time or pause time, start value animator, reset values when video recording is ended, etc and the current state value of the button will be changed to RECORDING. As the animation changes, the arcs will be drawn as per the start state. Similarly, if the current state is PAUSED state then onClickResume() will be called and the state of the button will be changed to a RESUMED state in which resume time will be saved and outer border animation value will also change. Actions : The callback to the events of the recorder button will be handled at view using IRecorderActions. The view can override these methods to handle camera-related actions for example onStartRecord() can be used to start video recording in-camera. In this phase as the animation starts the circles and arcs are drawn based on new animated value and the current state of the recorder. Let’s see what the onDraw() method will do as per the current state of RecorderStateManager. Init : If the current state of the recorder button is INIT state then the only inner and outer circle will be drawn as the recording is not started yet. Start : If the current state of the recorder button is START then along with the inner and outer circle the arc for recording progress needs to be shown which depends on the current time and the maximum time allowed for recording. Pause : In the paused state, the color of the arc can be changed as per the users' requirements. For the time being this is kept as white. Based on a sweep and start angle a separate arc will be drawn to mark paused time of the video. Resume : As the video is resumed the start angle of the arc change based on the last paused time and the sweep angle will be calculated using the current time. Now that we know what should be drawn in different recording states, let's do some math to draw arcs to represent resumed and paused state In the above diagrams S: Start Time P1, P2: Paused Time R1, R2: Resume Time x, y: Sweep Angle for arc The sweep angle for the pause break is always 10f and the sweep angle for the arc while recording a video will be calculated using the following method. Based on user action, all the times will be stored. Whenever a user pauses a video that time is recorded and stored in the pauseTimeList . Similarly when recording is resumed the time is stored in a resumeTimeList . All the arcs will be drawn based on the times stored in the list. In the onDraw method of view, we need to draw an arc as per the current recording state of the button. For every state first step will be to draw an inner and outer circle. The arcs will be drawn as mentioned below: Started State : draw an arc for started state 2. Paused State : In this state pauseTimeList and resumeTimeList will be used to draw an arc and the following two steps will be implemented for all the time saved in the list. For both steps sweep angle and start angle will e calculated based on paused time and previous resume time. draw an arc for resumed state draw arc for paused state 3. Resumed state : In this step also different pauseTimeList and resumeTimeList will be used to draw an arc in the same manner as done in the paused state but in this case, when all the arcs are drawn based on both the lists then a final arc for the resumed state will be drawn that will show recording and its sweep angle will be calculated based on the current time. This customizable progress bar is a reusable component and can contribute in multiple use cases like video recording, uploading, etc. The implementation for video recording has already been discussed in this post. Future Scope: The future scope for this project includes the introduction of a template for the button in which this video recording button can be further customized with attributes like shape, size, etc. We can introduce different icons in the center of the button to represent paused, resumed, and stopped state to make this more interactive. The complete code for this project can be found in this link. We operate a network of online trading platforms in over 40… 306 Thanks to Jatin Juneja and Hitesh Das . 306 claps 306 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-02-15"},
{"website": "Olx", "title": "xcode targets arent schemes", "author": ["Shashwat Kashyap"], "link": "https://tech.olx.com/xcode-targets-arent-schemes-a087ac4bbe3a", "abstract": "About OLX Group When developing an application, We most often follow the standard development life cycle pattern — development, testing, and production. For each of these stages, we have different API environments. For instance, when the app is in the development stage, it will communicate with olx.dev.api.com. Testing and production each have their respective API environments, too. Similarly when developing an application for different countries from a single source code, For each supported countries we need to take care of configuration, asset, etc. We at OLX Engineering team decided to utilize the knowledge of targets, schemes & configuration to improve the overall structure of developing an application considering multi-country support. Today let us discuss how we at OLX manage, apps in multiple countries using the same codebase. What was the requirement? We needed to manage apps in multiple countries with one codebase, where we could even have a different set of features available as per country, or have different sets of configuration for the same. How we did it? We first used different targets for different countries, which used the same set of resources except for some configuration files which were different. This lead to problems such as whenever we needed to change something which was earlier common to all targets, multiple files had to be created in order to accommodate the changes, also increasing the build time as we had to compile the source code for each and every target. What could be done to make it better? We tried to further optimize our process thus coming up with a solution of using schemes for different countries, thus giving us the flexibility we were pushing for by having different configurations for different countries yet all running using the same codebase. So now even our build time was reduced as we didn’t have multiple targets to build but only one, which now could be resigned as per the configuration required for the country. In order to appreciate what we are doing, we need to understand what are targets and schemes. A Target specifies a product to build and contains the instructions for building the product from a set of files in a project or workspace. An Xcode scheme defines a collection of targets to build, a configuration to use when building, and a collection of tests to execute. What makes it to the list of targets and how many of these can we have? Anything and everything you get as an end product, whether it’s your app or some extension to the app is defined as an Xcode target. Even UITest and XCTestCases make it to the list of targets. Hence a single project can have numerous of these. What are the building blocks of a target? A target consists of everything that is required as input to the Xcode build system . For instance the source files and instructions for processing those source files etc. Can you give some real examples of targets that might be part of my project? Select your project in Xcode now in the general tab you will be able to see the targets of your project. It usually comprises TestCases or if your app supports today’s extension it also appears as a target, these are some usually visible targets in any app. Even if you have separate apps for OSX / WatchOS / iOS app they also appear as separate targets. Is there a practical use case to some of the usual problems that can be solved using targets? A target can have some build settings which if not defined are inherited from the project itself, it is just additional customisation available to the developer. So some apps use two different targets for release and debug builds thus separating out stuff. This even provides them the ability to have two same-named files with different contents, if needed. Now lets discuss about schemes How many schemes can we have in a project? You can have as many schemes in a project, but there has to be one for each target, as scheme relates target to build configurations, so in order to build any target there has to be once recipe a.k.a scheme How does a scheme work? Xcode uses schemes to understand which build configurations (Release / Debug / Custom) to use that are defined in a build target under build settings. When you add a new target Xcode automatically adds a default scheme for you which can be later modified. You can also duplicate / add / modify schemes as per need basis. If we look carefully at the above screenshot we can see multiple sections in the left column that corresponds to the specific settings or customization we could provide when we build / run / profile a project. For instance, in the build section, I can provide all the targets I want to build in the scheme, while in test I can provide a particular test plan to run under that scheme. Any practical use case where schemes can help me out? A scheme can help you build multiple targets. Executing scripts before and after any action Can help you send emails if needed after a certain action Can be used to run the project with memory diagnostics Produce debug/release build for any action Conclusion / Summary Using the multiple schemes for different countries, helped us significantly reduce build times (when all builds had to be created for all the countries). We could now just create multiple builds by resigning with different country configurations, which also provided us the ability to have different sets of features and variations for different countries while using the same codebase. I would like to end with the analogy, consider target as your ‘ingredients’ which will be part of your end product ‘dish’ you are making and the scheme is your ‘recipe’ to make that dish. References developer.apple.com developer.apple.com We operate a network of online trading platforms in over 40… 110 Thanks to Sandeep Chhabra , Rahul Khanna , and . Xcode Swift iOS App Development Ios Development Olx 110 claps 110 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-12-02"},
{"website": "Olx", "title": "load shedding with nginx using adaptive concurrency control part 1", "author": ["Vikas Kumar"], "link": "https://tech.olx.com/load-shedding-with-nginx-using-adaptive-concurrency-control-part-1-e59c7da6a6df", "abstract": "About OLX Group Author’s Note: This is a two-part series about how we implemented and operationalized load shedding in our services using adaptive concurrency control with NGINX. Part 1 sets the context/background and part 2 focusses on the implementation. I wrote about this technique in much detail last year (Part 1 , 2 , 3 , and 4 ). Some of the content here is adopted from these posts. Although I have tried to make this post self-sufficient, if you want further deep-dive on this topic, I’d recommend going through the other four posts and their attached reference material. Here is the link to part 2 . When we talk about resilience in a microservice architecture, it’s highly likely that circuit breaking will come up. Popularised by Michael Nygard in his excellent book Release It! and Netflix’s Hystrix library, it has, perhaps, become the most discussed technique for making services more resilient when calling other services. Though there are many different implementations of circuit breakers, the one most popular is akin to client-side throttling — when a caller (client) of a service detects degradation in responses from the service i.e. errors or increased latencies, it throttles the calls to the service until it recovers. This is more about being a good citizen — if the service you are calling is degraded, sending more requests will only make it worse, so it’s better to give it some time to recover. Much has been written (in blogs and books alike) about circuit breakers, but the other side of it, that is — protecting your own services from overload, doesn’t get as much attention, in my humble opinion. When your service starts to degrade, how can you prevent it from slipping into an overloaded and completely unusable state (at worst, requiring manual intervention to resurrect it) caused by callers (users, other services) that keep sending requests unbeknownst to its plight, is a critical and more important question to ask. This degradation usually happens because of resource saturation (CPU, memory, IO, network, etc.) or something like prolonged GC pauses in either the service itself or its dependencies. The saturation can be : Negative: Incoming traffic remains normal, but the service or its dependencies (database, cache) are experiencing saturation (CPU/memory/IO pressure, long GC pause) Positive: Service and its dependencies are normal, but the traffic suddenly increases and causes saturation The end result is that the service will not be able to process requests in a timely manner, and if there are no safeguards in place, the issue propagates to dependant services causing cascading failures. Raise your hand if you have seen this — PagerDuty alert fires for multiple services/systems at once when there is an issue with a service. Right now, you must be thinking about autoscaling. It has its place but falls short in handling overload situations on its own. Autoscaling works best when there is a gradual increase in incoming traffic. It usually can’t react fast enough when there is a sudden increase in traffic. One effective technique for managing overload is load shedding (akin to server-side throttling). Instead of trying to accept and process every single request, just accept what you can process in a timely manner and reject the excess requests. The key idea here is graceful degradation — when degraded, it’s better to stay up and do some useful work rather than crumble under pressure trying to do all the work. Every request processing system has a finite capacity (which itself varies from time to time depending on various factors). When incoming work exceeds this capacity, the system has a choice —either reject it or accept it and put it in a queue to process later. If requests are queued, the queue is unbounded or has a very large size, and the request arrival rate remains constantly high, the queue will keep growing, which has 2 adverse effects: The queue will consume resources (e.g., memory), potentially further slowing down the processing of active requests The callers will experience increasingly high latencies since, for callers, latency = time waiting in queue + processing time . As this latency increases, callers will potentially timeout and retry, further exacerbating the problem. To protect against this, we need to control the queuing. Some queuing is still desirable as it allows us to handle small bursts, but excessive queuing should be avoided. Queuing in Tomcat Let’s illustrate the effects of queuing using a Spring Boot application with embedded Tomcat server. We can define the maximum number of worker threads using server.tomcat.max-threads (default 200). It defines the maximum number of requests that can be processed concurrently. Excess requests will be queued. Queue size is defined by server.tomcat.max-connections (default 10000 for NIO connector and 8192 for APR). There is also a queue at the OS level where connections wait if Tomcat queue is full. This is defined by server.tomcat.accept-count (default 100). We can easily simulate queuing by creating a simple GET endpoint that sleeps for 1 second before returning a 200 OK response. I set the max-threads property to 10 and created a simple Node.js script to send a constant flow of N requests per second. If N < 10, there should be no queuing. Here’s the result: Indeed, there is no queuing, and response time hangs around 1 second. All good. However, if N > 10, we should see the effects of queuing. Here’s the result with N = 12: We can see the response time increasing monotonically, but no errors. This is because requests spend increasingly more time in the queue. One effective technique for load shedding is concurrency control— limit the number of concurrent requests. In this context, concurrency is defined as the number of requests that have been accepted for processing at any point in time, also known as In-flight Requests (IFR). This essentially is the sum of requests being worked on right now and requests waiting in queue. For example, in case of the Spring Boot application, we can limit concurrency to, say, 250 using the following configuration: This allows for 200 requests to be processed concurrently by worker threads and a maximum of 50 requests being queued if all worker threads are busy. When the queue is full, excess requests will be rejected. Another technique is rate-limiting — as in, limiting requests per second. Rate-limiting is useful in certain scenarios like rate-limiting on a per-client basis to protect from a single malicious client. It’s not, however, very effective as a load-shedding mechanism to protect services from overload. Rate-limiting could work in an ideal scenario where request processing time for all requests is steady at all times, which couldn’t be far from reality. When service degrades, rate-limiting mechanism can send more requests than the service is capable of handling, causing overload. If you wish to know more, I recommend this amazing video: Stop Rate Limiting! Capacity Management Done Right by Jon Moore . You can read more here and here . Although the static concurrency limiting approach is better than nothing or rate-limiting, it still suffers from a problem — it can be hard to pick the right concurrency. If it’s low, there will be unnecessary throttling. If it’s high, the efficacy of the whole mechanism will be reduced. Since service health and performance are dynamic and keep changing, it can be very hard to pick a reasonable limit. Enter adaptive concurrency control — adjust the concurrency limit dynamically by constantly observing the state of the service. Adaptive concurrency control is a type of closed-loop feedback control system as depicted in the diagram below (think thermostat or air conditioner): It treats the service as a black-box, observes the output (response time), and periodically adjusts the concurrency limit to keep output within acceptable limits. The first implementation of this technique that I came across was Netflix’s concurrency limits library (and the associated blog post ). It’s a java-based library that can integrate with GRPC or servlet-based applications. It implements multiple algorithms based on TCP congestion control mechanisms. Some of the algorithms are: Additive Increase Multiplicative Decrease (AIMD) , based on this Gradient Vegas , based on this I have done a detailed breakdown of these algorithms from the library’s implementation here . Although this library is great for its implementation of these algorithms, it has a severe drawback — it has to be integrated within the application and does not have visibility into the webserver queue which, as we saw earlier, is the main culprit. I have done a detailed analysis on why this library falls short of its intended purpose and why the concurrency limiting logic should be in the proxy fronting the application that can measure processing as well as queuing delays. Since we, at OLX Autos, are using NGINX as a reverse proxy sidecar with most of our services, it seemed like the right place for us to have the concurrency control mechanism. Unfortunately, I couldn’t find any such implementation. So I decided to take the algorithms from the concurrency limits library and implement them on top of NGINX so that we could use them with our services which I’ll explore in the next part of the series. Before we move on to the next part, I would like to share an interesting case study of overload using an outage we had where load shedding would have been useful. We deploy our services in Kubernetes. Our traffic patterns are quite predictable — high during the day and very low at night. We use autoscaling for most of our services using Horizontal Pod Autoscaler (HPA) to save cost. The relevant section of the architecture looks like this: Service A receives traffic from users. It calls service B to fetch results from our SolrCloud cluster. A single request to service A results in multiple requests to service B (on average, ~2.5). Service A then aggregates and sorts the results before returning them back to the users. The focus of this case study is service B. It had autoscaling set up with CPU usage as the scaling metric, minimum replicas 5, and maximum replicas 80. It was the peak traffic time. Service B was running at full throttle with maximum (i.e. 80 ) replicas. Because of some issue in service A, it stopped sending requests to service B. No traffic, no CPU usage. HPA kicked in and quickly killed the pods, bringing the count to the absolute minimum (i.e. 5 ). When the issue in service A was resolved, it unleashed all of the peak-time traffic on service B. Five pods received the traffic that would normally be served by eighty. They crashed immediately and violently. Any new pods that dared to rise met with the same fate. Eventually, we had to stop all the traffic and manually scale the replicas before allowing the traffic back in. If we had load shedding in service B, the five pods could have stayed up, doing their part and rejecting the rest of the excess requests. It would have given HPA time to gradually spin up more replicas. Initially, there would have been a lot of 503 responses which would have gradually decreased as more replicas came up. ( There is also an interesting learning here about autoscaling configuration and things like cooldown/delay, but that’s for another day ). In the next part , I describe our journey of implementing adaptive concurrency control in NGINX and operationalizing it with our services. https://landing.google.com/sre/workbook/chapters/managing-load https://www.evanjones.ca/prevent-server-overload.html Load shedding lessons from Google: https://www.youtube.com/watch?v=XNEIkivvaV4 https://blog.acolyer.org/2018/11/16/overload-control-for-scaling-wechat-microservices https://medium.com/@NetflixTechBlog/performance-under-load-3e6fa9a60581 https://aws.amazon.com/builders-library/using-load-shedding-to-avoid-overload https://cloud.google.com/blog/products/gcp/using-load-shedding-to-survive-a-success-disaster-cre-life-lessons https://ferd.ca/handling-overload.html https://www.infoq.com/articles/anatomy-cascading-failure https://www.datadoghq.com/videos/the-anatomy-of-a-cascading-failure-n26 We operate a network of online trading platforms in over 40… 80 Thanks to Johannes Scharlach . Load Shedding Microservices Nginx Kubernetes Circuit Breaker 80 claps 80 Written by Principal Engineer @ OLX Group. https://twitter.com/vikaskx We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Principal Engineer @ OLX Group. https://twitter.com/vikaskx We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-02-25"},
{"website": "Olx", "title": "intro to our jobs recommender service", "author": ["Viacheslav Dubrov"], "link": "https://tech.olx.com/intro-to-our-jobs-recommender-service-4317b55fc075", "abstract": "About OLX Group OLX not only helps people buy and sell cars, houses, goods but also find jobs. OLX Jobs is a high priority direction for the company right now. One of the new achievements is delivering new jobs specific recommender service based on Alternating Least Square (ALS) model. In this blog, we are going to talk about the architecture of our system for serving job recommendations to job seekers. We will cover: Use cases ALS model The architecture of the service The ALS recommender system The Email/Push delivery system Generated recommendations are used in two places right now: Email and push campaigns for active job seekers Candidate profile page Email campaigns consist of 10 job recommendations, similar to what job seeker has listed or applied before, whereas push notifications provide only one of these recommendations. Here is an example (translated from Polish) of a recommendation email. On the candidate profile page, we provide a dashboard with the best recommendations. Part of the recommendations in the dashboard served also by the ALS recommender system (see the translated example below). The effect of the recommendations has been measured in the emails/pushes campaigns and it showed a significant uplift in our main business metrics: ~5% in number of job applicants and ~2.5% in applications. ALS is one of the collaborative filtering recommender algorithms based on matrix factorization . ALS finds relevant items for users based on his and group interactions and uses for that matrix multiplication approach. Matrix factorization algorithms (and ALS in particular) work by decomposing the user-item interaction matrix (check illustration below) into the product of two lower dimensionality rectangular matrices: User matrix, where rows represent users and columns are latent factors (factors, that are not directly observed but are inferred from other variables); Item matrix, where columns represent items and rows are latent factors of items. In our specific case items represent job positions, where job seekers (users) could apply: We have tested two matrix factorization frameworks in python: Implicit and LightFM . We evaluated these frameworks in offline experiments, and Implicit showed the best results in terms of precision, recall , NDCG . There are some other metrics also, but we will cover our offline evaluation system in detail in the next articles. We have chosen the Implicit framework for the production system because of a better precision score in combination with other offline metrics. Implicit was also faster than LightFM in training and prediction. As mentioned before, there are two use cases for provided recommendations, but here we’ll describe the architecture for sending notifications only, which consists of two subsystems: ALS recommender system for generating recommendations; Email/Push delivery system for delivering recommendations to job seekers. Separation of the recommendation and delivery parts helps easily reuse ALS recommendations in future steps in the candidate profile dashboard. To make recommendations we need two steps: Preparing data for training the model; Training the model and generating recommendations. More detailed steps presented in the next diagram: We use Airflow for orchestrating the pipeline. The first step in the workflow is getting User-Item hourly interactions. Every hour we request user interactions with the jobs category from the global database and save it in our internal table. We use SQL for that and Presto engine for executing the queries. The second step is getting aggregated the last 7 days user-item interactions from history on hour interactions and saving them into a bucket accessible for AWS Batch . Unlike first step interactions aggregation is scheduled only two times per day. Provided solution of two steps queries combination significantly reduces the time of getting aggregated 7 days User-Item interactions and reduce costs of the architecture. After aggregated interactions have been prepared, Airflow triggers the AWS Batch job with a prepared docker container. The container includes the python script for training the ALS model based on provided interactions and recommendation predictions. The predictions are then saved to an S3 bucket. User recommendations from the S3 bucket are used later in email/push campaigns and candidate profile dashboard. Let’s look at the next part on the implementation of the Email/Push delivery system. The pipeline, which prepares emails and pushes, uses user-item interactions and recommendations prepared by the ALS recommender pipeline. From recommendations are filtered out users who declined in settings any emails and push notifications. A list of users which have to be filtered out is prepared by SQL and executed by the Presto engine. Preparing a list of active job ads is executed by Presto in parallel. After the lists of users and active ads are prepared, Airflow triggers the AWS Batch job where the filtering job happens. Python script inside AWS Batch job at first filters out recommendations based on the prepared user and active items lists. Then it filters out users who got the email or push with job recommendations in the last 3 days. For that, the AWS Batch job has access to the history of prepared campaigns. As the last step, python script sends API requests for the campaign sending to our internal campaign tool and saves the results of the sending into the campaign logs S3 bucket. The limitation of the current system is executing some ETL operations inside the AWS Batch job in python script instead of outside in SQL queries. We have to do it to support combining and filtering different sources of data. Preparing and delivering job recommendations in a batch way helped us quickly implement and deliver a significant impact on our main business metrics: the number of job applicants and applications. Developing two separate systems for generating and delivering recommendations helps us reuse prepared recommendations in other systems. Optimizing your SQL queries could bring significant improvement in its performance. As an example: separation of interaction aggregation into two query steps. Airflow in combination with AWS services could provide very convenient and flexible instruments for building recommender pipelines. We operate a network of online trading platforms in over 40… 186 1 Thanks to Alexey Grigorev and Maryna Cherniavska . Recommendation System Data Pipeline Data Engineering Machine Learning Data Science 186 claps 186 1 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-28"},
{"website": "Olx", "title": "work smarter how to manage time wisely", "author": ["Jatin Juneja"], "link": "https://tech.olx.com/work-smarter-how-to-manage-time-wisely-391436cdf414", "abstract": "About OLX Group Time is a finite resource (only 24 hours a day). You should manage it properly to live a balanced life. If there is too too much on your plate, either you need to do less or do it more efficiently. Time management is the ability to plan and control how you spend the day to effectively accomplish your goals. Time Management is required in both professional and personal life. Golden Rule:- You should learn to say NO. What I try to mean is: - Don’t say YES immediately when a task comes to you Listen to the work and try seeing what’s currently on the plate If you already have a day packed, can you replace any of the task with the work discussed, if no, then say NO Attempt to Rely on memory No system for tasks and to do’s Attempt to Rely on memory Your memory is not a great place to store the tasks. Never rely on memory. You will forget few tasks each day and feel more stressed in thinking what else is left for the day No system for tasks and to do’s Build a system which you will use daily Write it all down when it pops in your brain When you have all things written down, you will feel less stressed and more focused on present work Keep track of all the things in a system Let’s try and find your Personal traits and tendencies before finalising what works best. Chronotype is fancy word for body clock. Learn your body chronotype and decide what works best for you and what hours of the day are productive for you and work with your body accordingly, not against it. Broadly, there are 3 types of learning styles. Find out your learning style and work accordingly. Visual Auditory Tactile (kinetic) Find out your way of working and act accordingly Integrator → are the ones who blur the lines been work and home, switching back and forth between the two. Segmentor → are the ones who create rigid boundaries between their personal and work lives. Find out your personality and work according to it Extroverts → Extroverts are generally outgoing and sociable. They often love discussing their ideas with others and spending time with other people, and gain their energy from being around others. Introverts → Introverts are more reserved and tend to listen more than they speak. Introverts often work best when they work alone. The isolation allows introverts to focus deeply and produce high-quality work. Ambiverts → Ambiverts are in the middle. They may lean more toward extroverted or introverted behaviour depending on the situation. Ambiverts know when to speak up and when to listen. Keeping the above things in mind, you need to manage your work and time, and work towards achieving the end goal. Now is the time to build a system that will work with your personal traits and tendencies, not against it. Task Management is a central part of time management. Here are some simple tips and tricks for effective Task Management: Single place for all the tasks This will help you prioritise Tasks can come from many sources (slack, email, work, personal etc) Mark all the tasks in single place rather than remembering and attempting to remember again and again what is left to be done. Keep brain stress free and productive in doing the task instead of remembering what tasks on our plate Microsoft TODO List, Google Tasks etc. are some of the useful apps that can help you manage task at one place Brain dump ensures Your mind can let go Information is never lost All the data at one place to prioritise No matter how small a personal or a professional task, it needs to be written down. Prioritise If we have, say, 25 tasks in the list for today, then the priority order should be: Must todos Quick hits (takes few minutes each- responding to email/slack etc.) Reprioritise the task at the end of day and mark the acton date Handling Emergencies Self imposed emergencies should not arrive as we have already done prioritisation If a new task comes up as emergency, try to get the due date for it first and prioritise accordingly. Most of the time due date is a future date and we have time to prioritise it for upcoming days If the emergency is like, the tasks need to be done in an hour or by EOD today, check for the Must TODOs, if they are done, just do the emergency task Try to push back if your must todos are not done yet by explaining what’s on your plate for today Suggest if someone else can pick up the task Mono-Tasking Multi-tasking is a myth When we say a person is doing multi-tasking, then actually that person is switching from one task to another quickly Multi-tasking is detrimental to the productivity and mental health Multi-tasking is stressful and does not work Avoid distraction Distractions are everywhere, be it email or app notifications. Studies says, human needs around 23 minutes to refocus after an interruption Turn off notification except meeting notifications Check email every few hours and stop notification Turn on Do Not Disturb on your phone Access technologies mindfully Clear off/uncluttered desk Wear headphones or find a secluded place to work Time-Blocking Useful tips on Time Blocking: Using calendar to block off time when you will work on a particular task If time blocking is not happening, the task won’t happen Time blocking helps you keep commitments and set boundaries Block time to check/respond emails/slack everyday twice/thrice The key for any of the system is consistency. You need to follow whatever works best for you religiously and make it a habit. On high level, planning helps in achieving the long term goal. Planning saves time in the long run. If a task is helping you achieve your long term goal, then plan it. Planning is the real work When you plan , you accomplish your goal 20% faster Planning is waste if you are not sticking to the plan Plan your Planning effectively Planning should be done at the end of day Schedule the planning for 15 mins on daily basis (review finished/unfinished task and task for tomorrow and reprioritise) Goal planning Create a SMART goal and goal plan Break Goals into milestones and work backwards from the timeline of the goal to determine the pace you need to achieve the goal Break milestones further into actions for week/month Determine your deadline for goal and break down the time you have remaining between now and the deadline Identify intermediate milestones required to achieve the goal Establish what you need to do to accomplish each of the milestones to achieve your goal ultimately Hope you learnt something new and will implement your new skills. Above mentioned strategies, tips and tricks will help you use your time wisely and efficiently. You can use all of the above mentioned tools and techniques or choose according to circumstances. In the recent past, I have also improved on managing time and productivity, and now there is a close to zero chance of me getting any information lost. Hope it works wonders for you too! (mentioned above in the blog as well that as per the study if the days are planned, time to accomplish the goal is 20% faster) https://www.udemy.com/course/do-more-stress-less/ We operate a network of online trading platforms in over 40… 159 1 Time Management Learn Share Management Time 159 claps 159 1 Written by Learn -> Implement -> Share -> Repeat We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Learn -> Implement -> Share -> Repeat We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-11"},
{"website": "Olx", "title": "mvi architecture in android a simple livedata based approach", "author": ["Hitesh Das"], "link": "https://tech.olx.com/mvi-architecture-in-android-a-simple-livedata-based-approach-b4b23896fd32", "abstract": "About OLX Group Model View Intent (MVI) is a reactive architecture pattern where the model is updated based on some interactions from user or the system and the view is updated based on states emitted from the model. Model holds single or multiple dynamic data structures and the business logic associated with it. View , same as other architecture patterns , is the interface for displaying/rendering information to the end user by listening to the outputs of the model or state. Intent ( ≠ Android Intents) is an action or event from either the end user interacting with the user Interface or the system with a desired result. And, Last but not the least , User ; Either the actor interacting with the software system or the system itself. Why another architecture pattern if we have already MVVM ? Well, one of the integral part of MVVM architecture is the ViewModel. Nevertheless, the concept of model defined in MVC pattern is not being followed religiously. The view is not driven by the model , directly. View can observe multiple observable properties from ViewModel for state changes which causes state overlapping issues (sometimes two different states are shown unexpectedly, both error and loader state) . MVI pattern solves the state problem by adding the actual ‘ model ’ layer which is observed by view for state changes. As this model is an immutable and single source of truth for the current view state, overlapping of states will not happen. One of the core concepts of MVI is Unidirectional Data flow . Some of the benefits of unidirectional flow are: It reduces Edge Cases. It solves the State Problem . It enforces Single Responsibility Principle (refer S.O.L.I.D Principles). Hence, MVI provides a more decoupled architecture pattern. MVI also provides a Single Immutable State which is more predictable. Also, it simplifies logic surface, enhancing maintainability and testability of the project. With Single Entry and Exit Point, MVI also consolidates points for validation , analytics collection and debugging . MVI is based on reactive mechanism. The action performed by the user/system are considered as a part of MVI architecture. In layman terms, our View observes and reacts to any change in the state of the model as a result of some action either by the user or the system. MVI is Functional i.e MVI tries to consider everything as a function. user(view(model(intent(user())))) Now, since we all know about the benefits of MVI and its principles, it’s time to work on the boilerplate. The boilerplate code will help us setting up any MVI based Android project with LiveData and ViewModel . Below we define some key components of the architecture. ViewState represents the current state of the view at any given time and is a part of our model. All the dependent component of our view should be encapsulated in the ViewState. The code snippet below defines a generic ViewState as a data class. ViewEffect : In most of the apps we develop, we use to have certain actions that are more like fire-and-forget or one time viz.- Showing a Toast Message. In those cases, we do not want to maintain the state of the toast in the viewState as it’s a kind of fire and forget thing rather than a state. We consider all such cases separately as effects, although they are also a part of our model. Example class for the ViewEffect is shown in the snippet below as a sealed class. ViewEvent is the action emitted by the UserInterface or the system interacting with the app. e.g LoadData, ButtonClicked, TrackEvent etc. etc. Now, let’s see the high level depiction of the architecture: Now, let’s define the generic base view model class to create a ViewModel . It needs three classes ViewState, ViewEffect, and ViewEvent . Moreover, it implements an interface that has only one method i.e processEvent which takes a ViewEvent object as a parameter. This is where the user/system interaction/action is processed and state of the model under consideration is manipulated. The viewState and the viewEffects are exposed to the View as LiveData to listen to any state changes. A sample view model MyBookingViewModel inheriting the above base view model can be defined as below: Note that, our viewModel has one and only one entry point i.e processEvent. It can talk to an use-case or a repository via composition to fetch data or compute some business logic accordingly and can update the viewState/viewEvent to which the view reacts That’s it, the rest of the part (i.e. updating the LiveData for the viewState) is being handled by the base class i.e BaseMVIViewModel. Likewise, any viewEffect is being manipulated simply by setting the viewEffect to the desired value. Let’s check now how we define a view that observes the viewState and viewEffects from the model and renders them accordingly. Every view implements the BaseMVIView interface below. The BaseMVIFragment is a generic base fragment which requires four classes, viz. ViewState, ViewEffect, ViewEvent. It also holds an instance of the BaseMVIViewModel and implements BaseMVIView. A sample fragment MyBookingFragment inherited from the above base fragment class can be defined as below: Voila!!…. With this we have everything in place, communicating each action and content we are processing. Now, there is no chance of state overlapping as the model is a single source of truth for the view’s state change. Last but not the least, we do achieved the immutability of our Model this way as well. So then, ready for implementing the MVI pattern in your next project? “It depends.” There is no single architecture that is perfect at all times. Whenever you feel there is a unidirectional and cyclical data flow, MVI can help you to solve common concerns across development. In this article, we have read about the MVI principles , components and finally the boilerplate with Android LiveData based approach. For all those, who are new to ‘ Model-driven Design ’ may think we are adding more complexity than straight forward processing of the actions. But, trust me, in the long run, it will pay off as debugging cum tracking down the root cause of any issue/crash will be super easy as we can add or track new features easily. If you liked this article, do not forget to hit the clap 👏 button as many times as you can. We operate a network of online trading platforms in over 40… 113 Software Architecture Model View Intent Software Design Android App Development Best Practices 113 claps 113 Written by “When we share, we open doors to a new beginning” — Software Professional. Leading the Android Chapter at OLX INDIA We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by “When we share, we open doors to a new beginning” — Software Professional. Leading the Android Chapter at OLX INDIA We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-11"},
{"website": "Olx", "title": "the video experiment at olx part 3 video analysis tool vat", "author": ["Hitesh Rishi"], "link": "https://tech.olx.com/the-video-experiment-at-olx-part-3-video-analysis-tool-vat-476d7f7ba99c", "abstract": "About OLX Group In the first two parts (part one and two ) we covered the infrastructure, backend component, and the client that supports video features. If you have not read the first two parts yet, I would recommend you to read them first to have a sound idea of the video classifieds ecosystem. tech.olx.com tech.olx.com Now that the video is recorded from the Android App and is uploaded to the server, we aimed to conclude the experiment and choose a leader. There were multiple parameters that had to be reviewed — quality (video and audio), relevance, duration, and contents — that led to the discovery of a new tool which we called VAT (Video Analysis Tool). So without wasting any time, let’s dive into the Architecture and the challenges and solutions for VAT. The main purpose we are trying to solve is to assign a task (which consists of a set of questionnaires about the video) to operation users based on an event-driven approach . The questionnaire is purely configurable up to n-level , i.e based on certain answers new child questions appears and so on up to n-level. Operation user flow:- New Task:- If no pending task is present for that user, a new task is assigned to that particular user. Pending Task:- Operation user is given the flexibility to resume the pending task if such exists in that user’s bucket, similar to a draft system. The above diagram shows the high-level design of the architecture for our Video Analysis Tool. The client analyzer tool is a web-based tool that allows the operations users to log in via OKTA (a 3rd party authenticator). The client VAT is authenticated, and OLX is using OKTA as an access provider for in-house apps/tools. The client is purely based on React with client-side rendering handled via the Nginx web server. The HTML Video element ( <video> ) embeds a media player that supports video playback into the document. We have provided support for HLS (HTTP live streaming) for the playback of videos at web client level. Since our VAT needs to be used by the operation team, we can extract 3 user roles :- 1 Operation User:- a person who actually reviews the video and answers specific questions based on that video content. 2 Reviewer:- Reviewer user is a kind of admin for the operation user. A reviewer gets a list of all the tasks for each user and can also review or modify the operation user’s answers for a particular questionnaire. 3 Viewer:- We consider a viewer to be like a reviewer but without hands , i.e he can just see the tasks and the responses to the questionnaires but can’t modify the answers. How do we ensure that operation team has seen the whole video before answering the questions? We added a listener to our <video> element at the client side that prevents the user from going forward, though the user can go back. Once the video is watched fully we switch on a flag and the user is now allowed to forward the video, since he has seen the whole video already. What if video content is inappropriate? Does the operation user have to watch it? Of course not. For cases like that, we have an alarm on the UI console tool upon clicking which the video gets hidden and the user answers some generic question about why the video is inappropriate. Since the analysis data we get from our VAT tool is 90% structured, this data can be fed into various Machine learning models built by our analytics team to discover user history and what kind of video that user generally posts. We have also learned that moderation and object detection will be the key going forward when the feature is introduced in the buyer funnel as well. We have rolled the first phase of Video Experiment in February 2020, with this experiment we were able to conclude on how the user behaves when exposed to different video upload flows (i.e zero guidance, partial and complete guidance). We were directly able to map video quality and duration to the type of flow the user was exposed to. Interestingly the number of samples we were able to collect for unguided flow was more than the ones for the guided flow, however, videos were more structured content-wise in the guided experiment. Now, we need to come up with a flow that gives us quality content at the same time stays user friendly as well. We operate a network of online trading platforms in over 40… 229 Thanks to Hitesh Das and Maryna Cherniavska . Pwa Video React Hls HTML 229 claps 229 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-28"},
{"website": "Olx", "title": "detecting fraud rings with unsupervised learning", "author": ["Hagop Boghazdeklian"], "link": "https://tech.olx.com/detecting-fraud-rings-with-unsupervised-learning-554bedf29dbf", "abstract": "About OLX Group It’s common to hear about fraudulent acts on e-commerce platforms. In recent years, the spotlight has been put on fraud rings/networks. They are likely to cause the strongest damage to the customers and thus the platform itself. Fraud investigation teams intensively rely on Supervised Machine Learning models to catch fraudulent users. Yet, one major challenge associated with these models is the lack of all likely patterns in sufficient amounts needed for training. Data on fraudulent patterns are not only scarce but also tend to continuously change, making the learning of such models difficult. We propose to leverage Outlier Detection models to isolate fraud rings. The article will cover: Fraud Rings and why they’re dangerous Limits of Supervised Machine Learning Anomaly detection applied to fraud detection Building and expanding on top of anomaly detection E-commerce fraud rings are organized crime groups specialized in defrauding people. Fraud rings can consist of tens, hundreds or even thousands of people. Most of the rings are devoted to specific websites. One can say they have their own preferred “playground”. To maximise their chances of success, fraud rings need to target as many users as possible on a given platform. They need to attack “en masse”. The formula is pretty simple. It consists of creating as many “accounts” as possible, making them look legitimate and start attacking other users. The more content they can generate, the bigger is their chances of grabbing the attention of unsuspecting users. To be efficient, they often even develop software to automate parts or all of their actions. As an example, let’s say the fraud ring hires a pool of 3 fraudsters. These persons create a pool of 15 fake accounts on OLX. The accounts can sleep for several hours, days or weeks before they get activated. Once activated, the fake accounts can be leveraged to commit a specific set of malicious acts. The fraudsters can choose from their repertoire of techniques that they master. They can use stolen credit cards to buy items and never pay the seller. They can try to sell illicit items, such as guns or drugs. They can send spam messages to attract users on external websites. They can send phishing messages to make others reveal personal information, such as passwords, phone and credit card numbers. The pool of malicious acts is brought closer to perfection at each iteration, making them very difficult to detect. Fraud rings know that websites tend to focus too often and too much on a single account at a time when building fraud measures. Hence, websites completely miss the networks of malicious accounts that may exist among their customer base. Fraudsters can then exploit this blind spot, and attack the platform at scale. Fraud rings are one of the sources likely to cause the greatest harm to the business. If they succeed to defraud customers, the website has to face a pool of serious bad consequences. Angry and unsatisfied customers are likely to churn. Ultimately, this will cause monetary loss. That’s why it’s important to leverage links and connections between accounts when building ML-based fraud detection systems, to prevent the wave of attacks at scale. An extra negative side-effect is that malicious attacks may also result in data breaches requiring us to notify respective authorities. By using textual and tabular attributes, such as phone, message or email, it’s easy to connect users to each other. We can then start using Graph Analysis. Supervised Machine Learning models are broadly employed in detecting fraud on e-commerce platforms. They are proven to perform well for most common cases, such as Credit Card Fraud, Identity Theft or Spam. When trained with historical labelled data, in sufficient amounts, they are able to generalize well and detect learned fraud cases in newly observed data. Still, gathering labelled data of fraudulent patterns is a complicated and time-consuming process. Even with the most sophisticated Fraud Investigation teams, it can take weeks to get labelled data of all ongoing fraud patterns. Simply said, labelled data is scarce. The situation becomes worse with constantly changing behaviour. Fraudsters and particularly Fraud Networks continuously try to innovate their modus operandi to bypass the measures in place. It is not surprising to see a fraudulent pattern disappear or a new one appear from one hour to another. The scarcity of labelled data and rapidly evolving behaviours make the learning process difficult and limit the effectiveness of Supervised Machine Learning models, especially to uncover new patterns. When labelled data is there it’s often too late, the damage is already done. Let’s try to flip around the problem. Instead of modelling the fraudulent behaviour, we can focus on modelling the good user’s behaviour. Let’s hypothesize that the networking behaviour of good users remains stable over time and represents the big majority of observations. Under this hypothesis, we propose to use Unsupervised Anomaly Detection models to isolate the fraudulent networks. Unsupervised Anomaly Detection, also known as Outlier Detection, consists of detecting abnormal or unusual observations. This family of models decides whether a new observation drastically deviates from the fitted norm. During training, Outlier Detection estimators try to fit the regions where the training data is the most concentrated. In our case, we are willing to learn the “normal” behaviour of goods users ( inliers ), based on the networking data of millions of accounts, to better isolate the “abnormal” users ( outliers ). Some fraudulent users are likely to be among the outliers. In theory, good users should not be part of big networks or at any network at all. Yet, it does happen that users do share links with each other over time. Because some attributes used to link them have higher collision rates than others. The possibility of collision exists. But the outlier detection algorithm can account for that. Even when certain links are found for good users, the connection between them should be low. The number of exact or similar attributes that the users in the network share should be small. In the opposite case, fraudulent users will tend to share bigger amounts of similar attributes. Using Graph Theory terminology, fraud rings are likely to have the biggest number of nodes and the highest connection densities. The bigger the network size and connection density, the higher the score of being an outlier should be. Let’s compare a normal user to a network of suspicious users. Where we have a theoretical algorithm scoring them. The normal user without any connection with others should be considered as normal, the algorithm is expected to output a low score. The users sharing phones, emails and similar messages sent should be considered as suspicious. The expectation for the model is to assign a higher score. The users/networks with high scores should then be sent to fraud investigators. There are many existing anomaly detection algorithms that can be applied to tabular data. Among the most performing and known algorithms, you can find: Isolation Forest Variational Auto-Encoder Variational Auto-Encoding Gaussian Mixture Model Comparing these models requires a bit of time, it will be the subject of an article that will follow in the OLX blog. Using Anomaly Detection algorithms on real-time data can help fraud investigation teams to quickly uncover new emerging fraudulent patterns. In certain cases, the precision can be high enough to go beyond moderation and establish automated decisions, such as banning or applying frictions. In OLX, when targeted on Spam detection use cases, precision exceeded 95%. Allowing to discover hundreds of malicious texting users every day. Even so, not all anomalies mean bad users: Some outliers are simply behaving in an abnormal manner. They do not have any intention of causing harm. Certain behaviours among the outliers are authorised by company policies or business logics. In a business world context, it might be necessary to complement the outlier detection models with supervised ones. The quickly uncovered patterns can be labelled by moderators in accordance with the policies, and then be used as training data for supervised learning models. Unsupervised Machine Learning allows fraud investigation teams to uncover fraud networks. From the moment they register on the platform to when they wake-up from their incubation to attack at scale. By uncovering these networks at an early stage, it gives a significant amount of time to act upon and avoid any major harm to the customers. Precision can reach levels high enough (>90%) to allow automated decisions in some cases. It’s not surprising to see why the fraud investigation industry builds solutions around Unsupervised Machine Learning. [1] Novelty and Outlier Detection, https://scikit-learn.org/stable/modules/outlier_detection.html [2] Credit Card Fraud Detection in e-Commerce: An Outlier Detection Approach, from eBay, https://arxiv.org/abs/1811.02196 The work was possible thanks to Yuen King Ho, Ara Hayrabedian, Nodari Lipartiya , Jaroslaw Szymczak and Alexey Grigorev We operate a network of online trading platforms in over 40… 235 Machine Learning Fraud Detection Unsupervised Learning Fraud 235 claps 235 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-06-24"},
{"website": "Olx", "title": "data science at olx", "author": ["Alexey Grigorev"], "link": "https://tech.olx.com/data-science-at-olx-7c7406d1713f", "abstract": "About OLX Group Data science is an important topic for OLX. It helps to grow our business, makes our customers happier, and makes the user experience better and safer. We started to use it more than five years ago, and now more than 40 machine learning services are running in production and positively affecting our customers. In this article, we’ll talk about different areas where we use data science at OLX. We’ll also cover some of these areas in more details: Moderation (trust and safety) Search and recommendations Seller experience OLX is a two-sided marketplace: on one side we have sellers, and on the other — buyers. We want to make it easier for them to find each other, and to ensure that the interaction between them is safe and convenient. Machine learning plays an important role here. We use it across the organization in many strategically important areas: Trust and safety to make the interaction between our users safe Search and recommendations to help our buyers find what they are looking for Seller experience to make it easier to sell on OLX Verticals to cater to the specific needs of our users in cars, real estate and jobs categories Monetization to make it more profitable for our users and for us And other categories. To identify projects and areas where we want to invest our time, we consider the following factors: Strategic importance . We want to prepare the stage for what will be important tomorrow. For example, online payments and deliveries are important areas, so we want to work closely with these departments. Estimated business impact . If we know that some applications were successful for other companies in the same or related industry, we also want to evaluate them. Availability of data . Without good data, we won’t be able to have good models. But we also want to set the ML flywheel in motion for as many use cases as possible: this means getting data to create ML solutions that solve real user problems, therefore having more users attracted to our products and generating data to train even better models. One way of doing that in practice is to start with use cases where “good enough” accuracy is already acceptable. In the next section, we’ll talk about the major areas where we use data science. The first applications of machine learning at OLX were in moderation , so we’ll start with it. Every day millions of new ads are created at OLX. Unfortunately, some of this content is offensive or harmful, and we cannot allow it to go live. To stop it before anyone sees it, we have a moderation system. We heavily rely on machine learning to help us identify these problematic listings and remove them before they do any harm. As a part of the moderation system, we have many models, including: NSFW model — to detect explicit nudity Forbidden items model — to detect weapons and other items that are forbidden to sell Duplicate detection system — to fight duplicated listings Chat moderation system — to make sure the communication between buyers and sellers is safe and pleasant Fraud detection model — to detect rings of fraudsters and ban them The moderation system is quite complex, and it includes many components. You can read more about moderation in the article about our duplicate detection system . From the most recent projects in moderation, our fraud detection system is among the most impactful ones. It detects organized crime rings that perform fraudulent activities at scale. You can learn about it here: detecting fraud rings with unsupervised learning . Moderation is not the only area at OLX where data science makes a significant impact. Next, we’ll talk about two other areas where it makes a difference: search and recommendations . At the moment, there are 19 million active listings at OLX.pl. Even with the best possible categorization of items, it would be still difficult to find interesting items without a good search engine. Search is a very important part of our product. Each buyer at OLX is unique and has their own interests. To make our search more effective, we need to tailor it to each user individually and take into account their preferences. That’s what our data scientists do in the search team. We call this project “personalized ranking”. There are more projects where we use data science to improve our search: Search2vec — reducing the number of null-searchers: for queries that return no results, we want to show something relevant from similar queries Query categorization — understanding the intent of the user and showing a category Spell checking — dealing with typos You can read more about our models for search here . In addition to search, there’s another way to make the experience of buyers more personalized: a recommendation system. If you want to know more about our recommender service for jobs, check here . Search and recommendations help our buyers discover items they will love. But the experience of our sellers is also very important for us, and that’s another area where machine learning helps. We want to make it easy for our users to sell on OLX. To simplify the process of creating a listing, we use the category prediction model: it looks at the title of a listing and determines the best category. We also want to help our sellers create great ads. For that, we analyze each listing and determine if there are any areas for improvement. We look at: The number of images it has. The more images, the better. The quality of each image. If images are blurry or too dark, it makes the listing less attractive. The length of the title and the description. We want to make sure that they contain enough information. If we think that a listing could be improved, we give actionable recommendations: add more images, replace blurry images, and other suggestions. You can read more about image quality models in our article about infrastructure for serving deep learning models . We also help sellers of OTOMOTO.pl by suggesting the optimal price. Determining the right price is quite complex: users need to do a lot of research and analysis to come up with the right price. We make it easier for them with a machine learning model. There are many other areas where we use machine learning for positively affecting the experience of our users. To learn more about the work we do at OLX, check other posts from our engineering blog . At OLX, we use data science extensively: we started using it more than five years ago and now we have more than 40 machine learning services in production. To identify potential machine learning projects, we take into account three aspects: strategic importance, estimated business impact, and availability of data. In moderation, we use machine learning for detecting explicit nudity, fraud, forbidden items, duplicates, and spam. Also, we use it for chat moderation. Search and recommendations help buyers find what they want. We do it by making search more personalized, reducing the number of queries with no results, and suggesting items similar to what you like. To make it easier to sell at OLX, we suggest the best category for a new ad and analyze its quality. We also help the sellers at OTOMOTO.pl to find the best price range for their cars. If you liked what you read and want to become a part of our team — check our open positions. Find out more at JoinOLX.com . This post is based on a presentation “Data Science at OLX” https://www.slideshare.net/AlexeyGrigorev/data-science-at-olx . This article is written by Alexey Grigorev and Andreas Merentitis . We operate a network of online trading platforms in over 40… 283 Data Science Engineering Machine Learning Olx 283 claps 283 Written by Lead Data Scientist at OLX Group. https://www.linkedin.com/in/agrigorev/ https://twitter.com/Al_Grigor We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Lead Data Scientist at OLX Group. https://www.linkedin.com/in/agrigorev/ https://twitter.com/Al_Grigor We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-10"},
{"website": "Olx", "title": "improving jvm warm up on kubernetes", "author": ["Vikas Kumar"], "link": "https://tech.olx.com/improving-jvm-warm-up-on-kubernetes-1b27dd8ecd58", "abstract": "About OLX Group JVM warm-up is a notorious problem. JVM-based applications deliver great performance but need some time to “warm-up” before reaching the top speed. When the application launches, it usually starts with reduced performance. It can be attributed to things like Just-In-Time (JIT) compilation which optimizes frequently used code by collecting usage profile information. The net negative effect of this is that the requests received during this warm-up period will have a very high response time as compared to the average. This problem can be exacerbated in containerized, high-throughput, frequent-deploys, and auto-scaled environments. In this post, I will discuss our experience with JVM warm-up issues with Java services in our Kubernetes cluster, mitigation approaches we tried — what worked/what didn’t, and our overall learnings. OLX started out with a monolithic PHP application. A few years back, we started migrating to a microservice-based architecture on Kubernetes by gradually carving out services from the monolith. Most of the new services were developed in Java. We first encountered this issue when we made one such service live in India, a high-traffic market for OLX. We carried out our usual process of capacity planning by load testing and determined N pods to be sufficient to handle more than the expected peak traffic. Although the service was handling the peak traffic without a sweat, we started seeing issues during deployments. Each of our pods was handling more than10k RPM during peak time and we were using Kubernetes rolling update mechanism. During deployment, the response time of the service would spike for a few minutes before settling down to the usual steady-state. In our NewRelic dashboard, we would see a graph similar to this: Meanwhile, we started receiving a lot of complaints from our dependent services of high response times and timeout errors during time periods that aligned with our deployments. We quickly realized that the issue had something to do with the JVM warm-up phase, but didn’t have a lot of time to investigate because of other important things that were going on. So we tried the easiest solution — increase the number of pods in order to reduce per pod throughput. We increased the number of pods almost three-fold so that each pod handled ~4k RPM throughput at peak. We also tuned our deployment strategy to ensure a maximum 25% rollout at a time (using maxSurge and maxUnavailable parameters). That solved the problem and we were able to deploy without any issues in our or any of the dependent services, though we were running at 3x the capacity required for steady-state. As we migrated more services over the next few months, we started noticing the issue frequently in other services as well. We, then, decided to spend some time to investigate the issue and find a better solution. After perusing through various articles, we decided to give the warm-up script a try. Our version of the idea was to run a warm-up script that sent synthetic requests to the service for a couple of minutes in the hopes that it’d warm-up the JVM and only then, allow actual traffic to it. To create the warm-up script, we scraped actual URLs from production traffic. We, then, created a Python script that sent parallel requests using those URLs. We configured the initialDelaySeconds of the readiness probe accordingly to ensure the warm-up script finishes before the pod is ready and starts accepting traffic. To our surprise, though we saw some improvement, it wasn’t significant. We still observed a worrisome spike in response time and errors. Also, the warm-up script introduced new problems. Earlier, our pods would become ready in 40–50 seconds, but with the script, they would take about 3 minutes, which became a concern during deployments, but more importantly, during auto-scaling. We tweaked a few things in the warm-up mechanism such as allowing a brief overlapping period between the warm-up script and the actual traffic and changes in the script itself but didn’t see significant improvements. Finally, we decided that the small benefits provided by the warm-up strategy were not worth it and ditched it completely. Since our warm-up script idea tanked, we went back to the drawing board and decided to try some heuristic techniques with — GC (G1, CMS, and Parallel) and various GC parameters Heap memory CPU allocated After a few rounds of experiments, we finally hit a breakthrough. The service we were testing on had Kubernetes resource limits configured: We increased CPU request and limit to 2000m and deployed the service to see the impact. We saw a huge improvement in response time and errors, much better than the warm-up script. To test it further, we upscaled the configuration to 3000m CPU and to our pleasant surprise, the issue was completely gone. As you can see below, there are no spikes in response time. It quickly became clear that the issue was because of CPU throttling. Apparently, during the warm-up phase, the JVM needs more CPU time than the average steady-state but the Kubernetes resource handling mechanism (CGroup) was throttling the CPU as per the configured limit. There was a straightforward way to verify this. Kubernetes exposes a per-pod metric container_cpu_cfs_throttled_seconds_total which denotes — how many seconds CPU has been throttled for this pod since its start. If we observe this metric with 1000m configuration, we should see a lot of throttling at the start and then settle down after a few minutes. We did the deployment with this configuration and here’s the graph of container_cpu_cfs_throttled_seconds_total for all the pods in Prometheus: As expected, there’s a lot of throttling in the first 5 to 7 minutes of the container start — mostly between 500 seconds to 1000 seconds, but then it settles down, confirming our hypothesis. When we deploy with 3000m CPU configuration, we observe the following graph: CPU throttling is almost negligible (less than 4 seconds almost all the pods) and that’s why the deployment goes smooth. Although we found the bottleneck that was causing the issue, the solution wasn’t very promising in terms of cost. Most of the services that faced this issue had a similar resource configuration and were running over-provisioned in terms of the number of pods just to avoid the deployment blues. But none of the teams warmed up (pun intended :-)) to the idea of increasing the CPU request/limit three-fold and reduce the number of pods accordingly. In terms of cost, this solution could actually be worse than running more pods since Kubernetes schedules pods on the basis of request and finding nodes with 3 spare CPU capacity was much harder than finding ones with 1 space CPU. It could cause the cluster autoscaler to trigger frequently, adding more nodes to the cluster. We went back to the drawing board, again (Phew!), but this time, with some new important information. We phrased the problem: Once we read the problem statement with a clear, peaceful mind, the answer presented itself — Kubernetes Burstable QoS. Kubernetes assigns QoS classes to pods based on the configured resource requests and limits. So far, we have been using the Guaranteed QoS class by specifying both requests and limits with equal values (initially both 1000m and then both 3000m ). Although the Guaranteed QoS has its benefits, we don’t need the full power of 3 CPUs for the entirety of the pod lifecycle, we only need it for the first few minutes. The Burstable QoS class does just that. It allows us to specify the request less than the limit e.g. Since Kubernetes uses the values specified in requests to schedule the pods, it’ll find nodes with 1000m spare CPU capacity to schedule this pod. But since the limit is much higher at 3000m , if the application needs more CPU than 1000m at any time and if spare CPU capacity is available on that node, the application will not be throttled on CPU. It can use up to 3000m if available. This fits nicely with our problem statement. During the warm-up phase when JVM needs more CPU, it can get it by bursting. Once the JVM is optimized, it can go on at full speed within the request . This allows us to use the spare capacity in our cluster (which we checked and found it was sufficiently available) to solve the warm-up problem without any additional cost. Finally, it was time to test the hypothesis. We changed the resource configuration and deployed the application. And it worked! We did a few more deployments to test that we could repeat the results and it worked consistently. Also, we monitored the container_cpu_cfs_throttled_seconds_total metrics, and here’s the graph from one of the deployments: As we can see, this graph is quite similar to the Guaranteed QoS setting with 3000m CPU. Throttling is almost negligible and it confirms that the solution with Burstable QoS works. Caveat Emptor: For the Burstable QoS solution to work, there needs to be spare capacity available on the nodes. This can happen in two ways: — Nodes are not fully packed in terms of CPU — The workloads are not utilizing 100% of the requested CPU In my experience, this is normally true. In cases where it’s not — your nodes are fully packed with pods and the pods are utilizing close to 100% of requested CPU all the time, you may not see the similar benefits of Burstable QoS. As a side-note, I’d also encourage you to understand how CPU quota and CPU time allocation works in Kubernetes if you don’t already. There are a lot of good resources around. Although it took us some time, we were happy to find a cost-effective solution. Kubernetes resource limits is an important, albeit a bit tricky , concept, which we learned the hard way. We implemented the solution in all our Java-based services and both the deployments and auto-scaling are working fine without any issues. Think carefully when setting resource limits for your applications. Invest some time to understand your application workload and set requests/limits accordingly. Understand the implications of setting resource limits and various QoS classes. Keep an eye on CPU throttling by monitoring/alerting on container_cpu_cfs_throttled_seconds_total . If you observe excessive throttling, try tuning the resource limits. When using Burstable QoS, make sure you specify the capacity required for steady-state performance in request and use burst capacity only for occasional spurts. Do not rely on the burst capacity for baseline performance. I would like to thank everyone who lent their invaluable support and worked on various aspects of this — Nikhil Sharma , Sahil Thakral, Arjun PP, Gaurav Bang, Aakash Garg, Akshay Sharma, Sumit Garg, and Nikhil Dhiman . We operate a network of online trading platforms in over 40… 549 3 Thanks to Rahul jain . Kubernetes JVM 549 claps 549 3 Written by Principal Engineer @ OLX Group. https://twitter.com/vikaskx We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Principal Engineer @ OLX Group. https://twitter.com/vikaskx We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-09-17"},
{"website": "Olx", "title": "building corpus for autosuggest part 1", "author": ["Aakash Prakash"], "link": "https://tech.olx.com/building-corpus-for-autosuggest-part-1-4f63512b1ea1", "abstract": "About OLX Group This is the first post in a series describing the work that we did at OLX for improving the user search experience by enhancing the autosuggest . We’ll be going through various challenges we faced and the techniques that we used to tackle each problem. So, let’s start! Autosuggest is an important tool to improve the modern user search experience. It encapsulates various techniques to predict what the user is going to type or what the user is searching for, on the platform. It is more around understanding the user’s intent (making predictions) rather than just giving suggestions . It saves a lot of user’s time as sometimes, it is cumbersome to write the complete search query, specially on mobile phones where the screen size is too small and typing is error-prone. According to Google, on an average it reduces the typing by about 25 percent and saves around 200 years of typing time per day. Yes per day! Well, the above statistic pretty much sums it up why we need autosuggest and why it is widely used across platforms. Before we do a deep dive into the corpus building part, let’s first take some time to define what use cases are we going to solve here. We are going to define the steps used for building the corpus for Most Popular Suggester (MPS) which is the most common type of suggester where we use the past user searched queries on the platform. Following are some of the major problems with the user searched queries: Queries with special characters and/or multiple spaces between words (“ cars %in gurgaon ”). Keyword ordered duplicates (“ fortuner gurgaon ” and “ gurgaon fortuner ”). Missing space duplicates (“ iphone11 ” and “ iphone 11 ”). Incorrect spellings (“ iphine ” and “ iphone ”). Queries with profane words. We’ll be going through possible solutions for aforementioned problems by something we call as a Preprocessor . The input to the preprocessor is Query Logs (List of user searched query terms) and output is Preprocessed Query Logs (User searched query term and its frequency) . The Preprocessor primarily consists of 5 components: The main responsibility of parser is to convert the queries into a valid search query by removing the unwanted characters (like special characters or unsupported characters) and by converting multiple spaces into one. The algorithm is pretty straightforward, just replace the special characters with space, break the query by space and then, merge the words back by adding a single space to form the query. For example , De-duplication is the process of merging similar queries into one. This component mainly does the following three kinds of de-duplication: a) Keyword De-duplication As we can see from the above list of queries that there are a couple of things that we can identify the results: Two queries are keyword ordered duplicates if they contain the same set of keywords. We don’t need to consider the stop words as they add no special meaning to the search query. We pick the highest frequency query as the best one. We merge all the frequencies of the input queries and assign it to the highest frequency query . Since, the no. of queries in the query logs would be way too many. So, we need a fast way to check if two queries contain the same set of keywords or not. Let’s create a hash of each query in the following way: Break the query into keywords by space. Remove duplicate keywords and stop words . Sort the keywords and merge them back by a separator. Now, we can see that all the above queries will have the same hash as “fortuner:gurgaon” where, “:” is the separator used and then, compare the hashes to de-duplicate these queries. b) Missing Space De-duplication After looking at the above queries, we can see that users forget spaces sometimes in between keywords but the queries are all the same. So, how do we merge these queries or how do we identify them in the first place? Well, we can tweak the solution for the above problem and follow somewhat similar steps. We can just remove the sorting step and adding the “:” separator step while merging from the previous algorithm . All the input queries will have the same hash as “iphone11pro” . c) Synonymous De-duplication This kind of de-duplication is a bit different from the previous two. There are some queries where it becomes hard for spell checker to correct the queries where the user’s intent is same but the spelling is off by too many characters or the meaning of two queries is same but we have products or search results for only one of them. We need to fix these queries because of the following reasons: Don’t have results for that query or keyword in the query. So, we shouldn’t be suggesting it. Don’t need to show misspelled queries to the users. It is usually safe to correct the queries where spelling is off by at most one character using the Spell Checker component (Yet to be discussed) but when the spelling is off by more than one character or the spelling of the word is totally different (Synonyms) but meaning is same, then we need to use this. There are two ways we can solve this problem: Provide a list of incorrect spellings along with its correct spelling for correction. Use Phonetics for de-duplication. We can see that a lot of incorrect and correct keywords are similar sounding ones or have common phoneme as a base. We can use phonetic libraries to find which ones are formed from the same phoneme . We need to filter out the queries which are profane and have bad language in them. One basic approach is to use a list of profane words and remove the query if a profane word is a part of that query (Exact matching) . It is a pretty fast and efficient way of filtering out profane queries but it can skip cases when the profane word is misspelled . Another approach is to use a Machine Learning model library which takes in a list of profane words and does score wise matching and discards the query if it is profane query (Similarity matching) . It is a pretty good way but it takes a lot of time to process the queries as each profane query identification takes comparatively more time than the previous approach ( profanity_filter library in Python ). The main purpose of this component is to build a keyword frequency dictionary . Just sum up the frequencies of each distinct keyword across search queries and skip all the stop words (Since, they don’t have any special meaning) . This dictionary will be used in the next component. This component is used to correct the misspelled queries with at max 1 edit distance . Why 1 edit distance and not 2 or 3? Because as we increase the edit distance the keywords whose meanings are totally different start to get incorrectly corrected by the spell checker . So, 1 edit distance seems to be the right bet as changing at most 1 character hardly changes the meaning of the word itself, in most of the cases. Now, what we need is an algorithm which is fast enough to correct a keyword on the basis of the keyword frequency dictionary we built earlier. You must be thinking why do we need frequency in the dictionary, right? Well, because it defines the priority of a keyword as in how likely a misspelled keyword within 1 edit distance is to be corrected to this keyword. For example: So, if a query comes with iphone , iphine or iphene , it will get corrected to iphone as it is the most valuable one (with the highest frequency in our query logs). We used SymSpell Checker which is one of the fastest spell check libraries available in almost all the common programming languages and it works pretty well. You can load the keyword frequency dictionary that we built earlier to SymSpell Checker and whenever, you will ask a query to get corrected it will give you a list of all possible suggestions within the configured edit distance . From the list of suggestions, you can simply pick the best one . However, we came across one problem with picking the best one . It can sometimes change the original correct keyword to another correct keyword . For example: In the above example, our spell checker was correcting TV (television) to TVS (a motor company) which is incorrect as both TV and TVS are correct. We analysed the data a bit and found out the appropriate threshold of high frequency keyword above which all the keywords are valid words . Then, what it means is if a keyword is above the high frequency threshold then, it will not be corrected else it will be. This trick pretty much solved our problem! You can contribute to the source code here ( https://github.com/akki744/autosuggest-preprocessor ). We discussed various techniques around preprocessing (Spell Check, De-duplication, Profanity Filter, etc) the past user searched queries and how they can be transformed into a corpus that can be used for Most Popular Suggester . In the next part, we will be discussing the approach or algorithms that we used to build autosuggest using the corpus that we created in this part. We operate a network of online trading platforms in over 40… 173 Auto Suggest Preprocessors Deduplication Spell Check Algorithms 173 claps 173 Written by Search, Personalisation and Relevance @ OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Search, Personalisation and Relevance @ OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-08-06"},
{"website": "Olx", "title": "presenting dali an image processor service", "author": ["Augusto César Dias"], "link": "https://tech.olx.com/presenting-dali-an-image-processor-service-514e6be00de8", "abstract": "About OLX Group At OLX we have a media server responsible for delivering millions of images per day to our users. The media server processes images on demand by resizing, rotating, adding watermarks, re-encoding to common formats and compressing. In this post we explain how we optimized one of our services to be more effective by moving it to Rust, covering: Decision making on language and libraries; Our journey until rolling out to production; Some insights on our results. We estimate that around 80% of our traffic is cached and served by our CDN provider and the remaining 20% still represents millions of requests a day and tens of thousands concurrently per second. On our busiest cluster, we get ~30000 requests/second at peak. That demands a lot of horse-power. We have a simple architecture to power everything up, consisting of the CDN in front of our users, backed up by a public API and an image processor service. Our first solution was using thumbor in front of our services to do the heavy lifting of image processing. It served us well for some years, but it costs us a lot to scale it to a point where the response time is still acceptable. These services run in Kubernetes on AWS and to give you an idea of the load: at peak traffic with~30000 requests/second we need~6000 pods to run this service. This component was a prime candidate for optimization, so we started designing a more efficient image processor. When you think about processing images and being fast, a language that comes to mind is C/C++, but we didn’t want to write C/C++ (and I believe at this point we don’t need to explain why, right? ). So which language could give us the same performance, with no runtime penalty as C/C++? It happens that Rust, which has been the most loved language by StackOverflow for some years in a row , provides a fast interface with C/C++ libraries through FFI. Fortunately, OLX gives us the freedom to experiment with new technologies, and, although we didn’t have experience with it, we decided to try and see how it plays out. In the worst-case scenario, we could continue running our current solution. After reading the official Rust book , I thought I was ready to start developing, so I was eager to write something with Rust. Writing a drop-in service for thumbor was the perfect opportunity to do so, as it is not a huge project and not even complicated in terms of business logic¹. We chose the language, but we still had some decisions to make, for instance, libraries to power this service. After some research, we’ve stumbled onto Actix-Web . It looked like the most mature web framework and after seeing some buzz about it being the fastest web framework , we thought it would be the best candidate. The missing piece was an image processing library. Firstly we thought about the image crate , but although it would be awesome to use a Rust library, it didn’t support all the required features. In the end, we decided to go with OpenCV. A well-established and mature library and with very good FFI bindings implementation for Rust . And thus we set off on our MVP journey. One of the most common struggles on the internet from people learning Rust is that you have to fight the borrow checker² all the time. I have no idea how many times I saw the compiler telling me some variable didn’t live long enough, was being used after a move or that the compiler could not infer a lifetime for a reference. It turns out the compiler can sometimes guide you for the fixes and the messages are very friendly. With time and practice, we got the hang of it. But one thing was particularly challenging: async programming prior to the advent of the async/await syntax. We started this project when async/await was still in nightly Rust and we weren’t bold enough to go with async/await while still not stable (plus Actix-Web didn’t support it anyway 🤷‍♂️). The code got very complicated and on top of that there were function calls that return futures inside futures, error handling, mapping types and the list goes on… The worst part is that the compiler messages stopped being friendly at this point. As a first time learner, seeing messages from the compiler telling that some type X doesn’t implement some trait Y or Z that you never heard about wasn’t very helpful. This happened frequently with async programming before async/await was in stable Rust³. With some struggles, we managed to get our first MVP running and started trying the service out. Early results were promising despite many surprises like missing features (auto-rotating images based on Exif data for example). One of our more critical blockers was the watermarks produced by OpenCV, which were very different from the ones produced by our service thumbor. We wanted to a drop-in replacement for thumbor, so the results had to be the same. After fighting and searching a lot over the internet, we couldn’t find a satisfactory solution to the problem, so we started to think about adopting another library instead of OpenCV. And libvips proved to be an ideal candidate. Though less known than OpenCV it is as good and as mature as OpenCV. There was one problem though: we found the bindings for it were not complete yet, so we had to develop our own. The good news is that the library supports introspection through glib so we could make a generator for the bindings. After writing the code to replace OpenCV with libvips we were ready, so what next? We were ready to throw some load on our MVP, so we started writing some load tests to compare it against thumbor. We wanted to understand the performance of a single instance of the service given certain constraints. The initial results were promising, the new service could handle twice the load of thumbor under similar conditions. We started to roll it out into one of our clusters and new challenges emerged. The first one was an application issue: as soon as we deployed and rolled it out, we had a huge spike in DNS lookups from our application (it obtains the images through HTTP). After some investigation, we found out that we had not properly set up our HTTP client inside our application. As soon as we fixed the HTTP client configuration, the DNS issue was resolved and on top of that, we observed that the throughput doubled. Then came the second challenge, one which we are still working on: autoscaling . Which metric should we use to scale up and down the service? Our first guess was CPU, but it proved sub-optimal. In our tests, we observed that the application performs well under constant 100% CPU usage, but as soon as it surges above that, it slows down significantly. The auto scaler could not react fast enough to not overload some instances, so we overcommitted to improve availability and lowered the threshold down to 60% average CPU utilization. It has been working well, but we are aware that we are underutilizing our resources. We rolled out to all of our clusters and the service has proven successful. Although we still have some tweaks to make it more effective, it is already production-ready and running reliably for some months. That’s the reason we decided to open source it! Named after Salvador Dali, you can find the source code at GitHub . Contributions are welcome and feel free to get in touch in case of issues or questions (or if you just want to say hi). It was a challenging but fulfilling journey of writing in Rust and we’re looking forward to the next ones. [1] Thumbor supports some advanced features such as smart cropping and several other filters. Those features are not used on our platform, so we limited our scope in the only features we needed. [2] The borrow checker is what rules memory management in Rust. More information from official docs . [3] After the arrival of async/await to stable Rust, Actix-Web migrated soon and the code became way easier to read afterward. We operate a network of online trading platforms in over 40… 238 4 Thanks to JB Lorenzo , Herman Maritz , and Alexey Grigorev . Rust Image Processing Python Microservices Software Engineering 238 claps 238 4 Written by Software Engineer @ OLX We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Software Engineer @ OLX We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-25"},
{"website": "Olx", "title": "solrs synonyms stopwords handling oddities", "author": ["Vikas Kumar"], "link": "https://tech.olx.com/solrs-synonyms-stopwords-handling-oddities-39398100034c", "abstract": "About OLX Group Do you use Solr? Do you use stopwords & synonyms with Solr? Do your synonyms have stopwords in them? If yes, then this article is probably for you. If you are a Solr/Lucene nerd like me who loves to deep dive into the intricacies of internals & under-the-hood implementations, then this article is definitely for you :-). Solr has a long and turbulent history with multi-word synonyms (Just search solr multi word synonyms in Google or read here & here ). SynonymGraphFilter finally solved most of these issues. But recently we (Search team at OLX Autos India) discovered a strange issue when dealing with synonyms which contain stop words, specifically if stop words appear at the beginning or end. ‌For example, if the synonyms file contains the following entry: and i is also defined as a stopword, on account of it being a pronoun. If I search for i phone using the following Solr request: ‌Then this results into the equivalent of the following dismax parsed query: ‌Whoa! Where did the i phone go? I expected the i of i phone being removed, on account of it being a stopword, and search with just phone i.e. I expected the parsedquery to be: ‌Note that this, in itself, isn't great from a good search experience perspective as it will return a lot of irrelevant results, but in terms of Lucene behaviour, this is what I expected (Ideally I would like i phone be searched as a phrase here. We'll talk about this later in this article). If we have stopword somewhere in the middle, then it works as expected. So if I have the following in synonyms: ‌and I search apple i phone , then it generates the expected parsedquery: ‌That’s not it. It veers into even weirder territory if we have multiple synonyms (one or more of which contain stopwords at the end). ‌Let’s take an example — synonyms for iPhone 6s: ‌with s being a stopword. When you search for iphone 6s using the following request: ‌What is the final query that you expect? To my (and I’m sure, your) great surprise, the parsedquery is: which is a weird combination. It’ll try to find documents which contain iphone AND 6 AND 6s . I was expecting it to match documents which contain (iphone AND 6s) OR (iphone AND 6) OR (iphone6) We could not figure out why this was happening. This behaviour is not documented anywhere (or at least we could not find any). So I decided to debug the Solr codebase on my laptop to see what’s going on at the implementation level. I cloned the lucene-solr repo, built it using ant and started debugging using IntelliJ debugger. ‌I’ll share what I found but before that, I want to give some background on how multi-word synonyms are handled by Solr using SynonymGraphFilter. I use Lucene and Solr interchangeably in this article. Some of the functionalities described here are part of Solr and others are part of Lucene. This article provides a very good explanation of how a token stream with synonyms (potentially multi-word) is handled using a graph. Here I’ll help you visualize how this graph is created using the examples we mentioned above. Let’s take the first example ( iphone and i phone ). If we analyse i phone using the analysis tool provided by Solr admin panel, we get the following analysis: We can think of each token as a node in the graph. To see how the graph is generated from this analysis, we need to consider the following: position: at which position does this node start. Here both i and iphone are at position 1 and phone is at the next position (position 2) positionLength: No. of hops that this node takes. ‌The above analysis generates the following graph: Internally Lucene uses an attribute called positionIncrement to determine positions. positionIncrement for a node is defined as the gap (in terms of the number of nodes or positions) between start positions of this node and the previous node. positionIncrement always starts with the value of 1. Let’s see how this graph is represented by Lucene’s internal data structure: ‌In this case, the first token is iphone ( iphon here because of stemming, but you can ignore that) so its positionIncrement is 1. Next token, i , starts at the same position as the previous token ( iphone ), so its increment will be 0. Next token, phone , starts at the next position, so its increment is 1. ‌Here’s a more readable format: positionLength and positionIncrement are important concepts in Lucene’s synonym handling as well as for understanding the issues we are describing in this post. Similarly, let's take a look at the analysis for the second example ( iphone 6s ), again without considering stopwords: And the corresponding graph: Here’s the internal representation (with a readable tabular format): I hope it’s pretty clear now how positionLength and positionIncrement attributes work and how we can visualize the graph with these. Now that we have seen how SynonymGraphFilter works, let’s add the stopwords back and investigate what causes the weird behaviour. First, let’s examine case #1 ( iphone and i phone ), where entire synonym is dropped if a stopword appears at the beginning or end. We add i as a stopword and analyse again. Here’s the analysis: As you can see, i is dropped. Rest everything remains the same. We can now visualize this as the following graph: The corresponding internal representation is: Lucene builds an automaton (which is sort of a graph similar to above) to generate the final query strings. This method takes the above graph as input, does some processing and returns another graph (said automaton) which is finally traversed to generate the query. This conversion is where all the issues we encountered are found. Specifically, two things that it does:‌ Remove dead states i.e. states which are not reachable from the start or the end state is not reachable from it How it handles gaps introduced by removing stopword nodes The current issue is caused by removing dead states. As we can see in the graph above, phone node is not reachable from the start state. So it is marked as a dead state and removed. Hence it does not appear in the final query. This will happen for all the synonyms where either stopword appears at the beginning (not reachable from start) or at the end (the end state will not be reachable). Now let’s examine the other case where searching for iphone 6s results in a weird token combination being searched for. If we analyse iphone 6s again with s added to stopword list, we can see the following analysis: It is equivalent to the following graph: If we go by the intuition gained from debugging case #1, we can say that this will ignore the lower two branches as they can not reach the end state and will only search iphone 6s , but that doesn’t happen. Instead, Solr ends up searching for iphone 6 6s . How does that happen? ‌Let’s look at the internal data structure: In tabular format, this translates to: This will generate the same graph as above, which is then used to build the automaton. The process of building the automaton is as follows:‌ Traverse the tokes in the stream. For each token: Identify start and end index in the automaton (which correspond to nodes) and create an edge (which corresponds to the token) The code is a bit involved but pretty straightforward if we step through it using the debugger. Let’s walk through how the automaton is built. Indexes start with 0 (identified by pos in code). ‌The first token is iphone . It’s start is 0 and end is 1 (based on positionLength ) The second token is iphone6 . Since its positionIncrement is 0, we do not move the start forward (it’ll still be 0). end will be 3 ( start + positionLength ) The third token is iphone . start is still 0 as positionIncrement is 0. end is 4 ( start + positionLength ) Next token is 6 . Since positionIncrement is 1, we move start by 1 so it becomes 1. end will be 2 ( start + positionLength ) The last token is 6s . This is where it gets tricky. The positionIncrement is 3. So we expect the start to move to 4 and create an edge from 4 -> 5 , which will give us iphone 6s ( 0 -> 4 -> 5 ). But as you might have guessed, this is not what happens. Instead start is incremented by only 1. It’s always incremented by 1 ( pos++ ) if positionIncrement is greater than 0. So what we get is an edge from start (which is now 2) to end (which is 5: start + positionLength + gap , where gap is positionIncrement -1 ) And there you have it. 0 -> 3 (iphone6) and 0 -> 4 (iphone) are eliminated on account of being dead states and we are left with iphone 6 6s , which is what’s finally searched.‌ I fixed this issue by adding the gap to pos before creating the edge. But I’m not sure that it’ll not break other cases (such as cases where stopwords appear in the middle e.g. apple i phone ). I think properly handling the gap introduced by removing stopwords requires identifying synonym boundaries and attaching tokens to their synonyms (I don’t have the final solution yet, but I’m working on it :-)) First, we removed single character words from our stopwords list, since most of the problematic cases were because of those. ‌Secondly, for better precision, we wanted the synonyms to be searched as phrases. In the case of i phone , instead of matching documents where i and phone appear anywhere (which could very well be in separate contexts e.g. I want to sell my Samsung phone ), we wanted to match documents where i and phone appear together. Solr provides a way to do that. We just need to add autoGeneratePhraseQueries=”true” in the field type. For example: Note that this can sacrifice some recall in favour of better precision. I had quite a fun time debugging this issue and getting to know the internals of Lucene better. I’ll try to raise an issue on Lucene github or Jira to get help from the experts. We can either get this behaviour documented somewhere or probably fix it in the code. Also, I would recommend everyone working on search with Solr to setup live debugging environment on their local machines. Lucene and Solr are complex beasts with a lot of complex algorithms and it helps immensely if we can step through the code and see what’s happening exactly. Apart from this issue, it has also helped me debug/understand a number of other issues involving spellcheck, proximity boosting and geospatial search. I used the following to help me set up the environment- https://cwiki.apache.org/confluence/display/LUCENE/HowtoConfigureIntelliJ ‌ https://opensourceconnections.com/blog/2015/04/30/debugging-solr-5-in-intellij/ ‌ Thanks for reading through. If you have any questions, don’t hesitate to contact me (vikas.kumar@olx.com). We operate a network of online trading platforms in over 40… 64 64 claps 64 Written by Principal Engineer @ OLX Group. https://twitter.com/vikaskx We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Principal Engineer @ OLX Group. https://twitter.com/vikaskx We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-04-29"},
{"website": "Olx", "title": "best practices to implement core data", "author": ["Rahul Khanna"], "link": "https://tech.olx.com/best-practices-to-implement-core-data-8a2c2feefc2", "abstract": "About OLX Group D evelopers deal with persistent storage almost on a daily basis. Apple provides multiple persistent storage techniques. Core Data is one of the most powerful persistent storage techniques on the iOS platform. At OLX, we use Core Data extensively and require it to be highly performant. In this article, I would like to share some of the best practices that we follow with all our might to get the best out of Core Data. Some of the things we @ OLX love about Core Data are: Provides object graph management Memory management using faults Object models instead of vanilla dictionaries Objects can be subclassed and customised for further behaviours Built-in sorting of objects Support for predicates Relationships are handled as properties & methods Data model editor helps in defining schemas and relationships Before we begin, let me introduce some important acronyms that I’ll be using throughout this article. Concurrency — Start with it! The most important practice when working with Core Data is Concurrency. All the heavy lifting should be done in the background so that the main queue is free for UI tasks only. Wondering how? Next couple of steps should help. A lways, always, always, even if you are only starting with Core data, always think about concurrency. Stack it right! Core data stack contains all the Core Data components you need to fetch, create and manipulate managed objects (Apple Developer Documentation). It is essentially the first step towards creating a Core Data based app. Here’s how we approached this and needless to say, it scaled and worked! Don’t wait, Perform and asynchronise The official developer documentation for the instance method perform inside NSManagedObjectContext class states: Asynchronously performs a given block on the context’s queue. This one-liner fails to highlight just how important this method is. Each managed object contest has a designated queue associated with it. This simple method adds blocks of codes onto the MOC’s queue to be executed asynchronously. This largely helps in Concurrency . (Code sample below) Generics and Wrappers Wrappers are good, in that they provide a good abstracted and reusable layer to work with. And they get even better when they are generic. For instance, look at the protocol below: The protocol above is a simple wrapper layer we wrote for all our DB entity classes. It works for all DB entities. Points worth noting here: For simplicity & word limits, I am going to put the implementation of only one method from this protocol to give you a glimpse of what I’m blabbering about. Don’t mingle entities For all the great things it offers, Core data can get pretty messy pretty easy. One of the things that the iOS team at OLX can die for, is the usage of protocols everywhere. So, while writing the code for chat, we did not want to use direct core data class objects. Instead, we created protocol models, that the core data classes would conform to. Here’s an example: If you are thinking why the overhead, think again. We all know how awesome Protocols are. This one time effort leads to the rest of the app not knowing where the Ad model resides. We can just as easily make a network model conform to this protocol and the app would still work. And I am not even on my main point yet. When I say don’t mingle entities, I mean DO NOT mingle one entity with another. I know you would say “But, we have relationships. They are of course mingled”. Here’s what I mean An AdEntity could have a one-one relation with a UserEntity, but notice that while creating an AdEntity object, I am not supplying the UserEntity with which it is supposed to be related. What advantages do I get by this? Single responsibility, code isolation to name a few. You can always link those relations in the upper layers. Here’s an example where we create ads and users. And simply link them. It was 2018, and we were struggling with a lot of bad ratings on app store related to “Chats not loading”, “Messaging is slow”, “I can not see my messages”, “Loader keeps on loading on chat screen”. Some of us personally went and met a few users with the help of the sales team. To meet an actual user in pain because of slow performing code was an eye-opener for us. We knew we had to refactor our application’s database layer to resolve these issues. After releasing these changes, we were able to get rid of these negative reviews. We even called the users we had met before and were relieved to know that they were happy. In the end, I would like to conclude this with our key learnings. Want to talk more? Let’s get in touch. We operate a network of online trading platforms in over 40… 245 Thanks to Nishant Sharma . iOS Core Data Technology App Development 245 claps 245 Written by Engineering Manager @ OLX, India. Core Tech — iOS. I cook stuff, sometimes off the monitor as well. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Engineering Manager @ OLX, India. Core Tech — iOS. I cook stuff, sometimes off the monitor as well. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-21"},
{"website": "Olx", "title": "load shedding with nginx using adaptive concurrency control part 2", "author": ["Vikas Kumar"], "link": "https://tech.olx.com/load-shedding-with-nginx-using-adaptive-concurrency-control-part-2-d4e4ddb853be", "abstract": "About OLX Group Author’s Note: This is a two-part series about how we implemented and operationalized load shedding in our services using adaptive concurrency control with NGINX. Part 1 sets the context/background and part 2 focusses on the implementation. I wrote about this technique in much detail last year (Part 1 , 2 , 3 , and 4 ). Some of the content here is adopted from these posts. Although I have tried to make this post self-sufficient, if you want further deep-dive on this topic, I’d recommend going through the other four posts and their attached reference material. Here is the link to part 1 . As noted in the previous post , we deploy NGINX as a reverse proxy sidecar with most of our services. NGINX already comes with a static concurrency control feature by default — the ngx_http_limit_conn_module module can limit the number of connections, which roughly translates to the total number of active requests to the upstream service (In case of HTTP/2, each concurrent request is considered a separate connection). Adaptive concurrency control, however, is not supported by default. Enter OpenResty . In their own words: OpenResty® is a dynamic web platform based on NGINX and LuaJIT. With OpenResty, we can extend NGINX functionality using Lua scripting. We already had the algorithms from Netflix’s concurrency limits library . All we needed was a way to implement them in OpenResty with Lua. Thankfully, OpenResty publishes a lot of modules and packages contributed by the community as OpenResty Package Manager . This is where I discovered the lua-resty-limit-traffic library, which had already implemented static concurrency limiting. This library ended up being the foundation for implementing the adaptive concurrency limiting (Thanks agentzh for the library and OpenResty). NGINX processes HTTP requests in multiple phases . Each phase can have 0 or more handlers. With OpenResty, we can hook our custom Lua code in these phases. Here is a depiction of hooks provided by OpenResty for different request phases: Here is how lua-resty-limit-traffic library works at a very high level: Define a shared dictionary to store the number of concurrent requests lua_shared_dict allows you to create a shared memory zone to save state across requests. This memory zone is shared by all NGINX workers and provides thread-safe methods to store and retrieve data. This dictionary will be used to keep track of concurrent requests, which we’ll refer to as In-Flight Requests (IFR). Define the max concurrency We need to define the max allowed concurrency (Let’s call it maxIFR). If IFR reaches this value, further requests will be rejected with 503 status. Handler for incoming requests Handler for incoming requests is defined at access_by_lua_block . Here, we first check the current IFR by retrieving it from the shared dictionary. If it’s equal to maxIFR, reject the request with 503 status, else allow the request and increment the IFR value in the dictionary. Handler for completed requests This handler is defined at log_by_lua_block . Here, we decrement the IFR in the shared dictionary. There are some more functionalities implemented by this library such as bursting, but that’s not particularly relevant for this post. For making concurrency limiting adaptive, We decided to implement Additive Increase Multiplicative Decrease (AIMD) and Gradient algorithms from the concurrency limits library. AIMD is a simple algorithm that works by observing latency from upstream service and periodically adjusting the concurrency limit by comparing it with a pre-defined maximum acceptable latency ( Lmax ). As long as observed latency stays within Lmax , the concurrency limit is increased additively (increment by 1). If observed latency goes beyond Lmax , we decrease the concurrency limit multiplicatively (multiply with x , where 0<x<1 ). The implementation can be broken down into 3 pieces: This is quite similar to the equivalent phase in the lua-resty-limit-traffic library except for the concurrency limit (maxIFR) is not statically defined. It is stored and updated in the shared memory zone. During initialization, we configure the following parameters: initialConcurrencyLimit : Concurrency limit to start with maxConcurrencyLimit : Maximum concurrency limit minConcurrencyLimit : Minimum concurrency limit Initially, maxIFR is set to initialConcurrencyLimit which is then periodically updated as per the algorithm. During the check, the latest value is fetched from the memory zone to compare with the current IFR value. When a request is completed, we decrement the IFR. Additionally, we record the latency of this request in a circular buffer which is later used by the limit adjustment mechanism. This mechanism is the heart of the module. For AIMD, we need the following: Current concurrency limit Observed latency Max acceptable latency We start with initialConcurrencyLimit and periodically (defined by a windowSize parameter) adjust the limit by comparing observed latency with maximum acceptable latency. During initialization, we start a timer that fires every windowSize seconds and triggers the logic to adjust the concurrency limit. The algorithm is rather simple: If observed latency is less than the maximum acceptable latency, increment the concurrency limit by 1, up to maxConcurrencyLimit If observed latency is greater than the maximum acceptable latency, reduce concurrency by multiplying with backoffRatio . backoffRatio is configurable and must be ≥0.5 and <1. Measuring observed latency Observed latency from the upstream service is measured by collecting the latency samples for a defined duration (say 5 seconds) and then taking either the average or the Nth percentile (configurable). This duration is also the frequency at which we update the concurrency limit. The parameters to be configured during initialization are: windowSize : Time window to compute observed latency minRequests : Minimum number of requests that should come in the window for us to be able to compute latency. metric : Metric used to compute latency. It can either be average or percentile percentileVal : If the metric is percentile , we need to specify a percentile value e.g. 99 or 95 or 90. It must be greater than 50. To illustrate, consider the following example: Here, We collect 3 requests in the first time-window with latencies T1 , T2 and T3 . At the end of this time window, we update the concurrency limit using an aggregate (average or Nth percentile) of these values and so on. The Gradient algorithm differs from AIMD in that we don’t need to define a static maximum acceptable latency. It adjusts the limit by comparing latencies in a short time window ( windowSize which is also the update frequency) and a long time window. For the sake of brevity, I’ll not describe the full algorithm here, but you can head to this post for a detailed understanding and dive into the code here and here . After the plugin was developed, we decided to test it in a local setup where we could artificially simulate overload conditions. We used the same test setup as the one in the previous article to show the effects of queuing. As we saw previously, with 10 worker threads in the Spring Boot application, if we generate a load of N (<10) requests/sec, we don’t observe excess queuing and increased latencies. But with N (>10), latencies keep increasing monotonically. Let’s repeat the same experiment (N=12) with adaptive concurrency control plugin in NGINX enabled with the following parameters: When we run the load, we start to see 503s which means throttling is happening. 99th latency percentile swings between 1 and 2 seconds and we see a see-saw pattern which is characteristic of the AIMD algorithm. After testing locally, we decided to first try it with a service on our staging environment. We replaced the plain old NGINX docker image with OpenResty with Lua modules and required config. Once it was working, we load-tested it using our load generators. We saw a couple of minor issues but overall felt confident to take it to production. For our production environment, we decided to go with AIMD first since it was simpler. One tricky aspect of AIMD is choosing the right values for minConcurrencyLimit and maxConcurrencyLimit . We needed to know the steady-state concurrency and its variation with traffic patterns. We didn’t have any monitoring in place for in-flight requests (IFR). So we first deployed OpenResty just to measure the IFR and not do any load shedding. Here’s a simple NGINX config to achieve this: It uses nginx-lua-prometheus to expose a gauge for IFR. After deploying this, we could observe the variations in IFR in our Prometheus monitoring and it was helpful in configuring the concurrency limit range. Currently, this module is running with some of our high-throughput services. We want to gradually roll it out to more services, but in a conservative manner. It’s extremely important for us to keep monitoring it and see how it behaves in various load conditions. The source code (Lua scripts) is available here . The repo also includes some documentation on how to set it up and sample NGINX configuration. I have used the following third-party libraries in this code: https://github.com/openresty/lua-resty-limit-traffic https://github.com/Kong/lua-resty-timer https://github.com/knyar/nginx-lua-prometheus Disclaimer: The code provided in this repo is tested for our use case is being used in production with a few services. You are free to use it any way you like but please keep in mind that this isn’t yet battle-tested in production. Although there are many open source projects that provide adaptive concurrency control, the one I have explored and am excited about is Envoy’s adaptive concurrency . Envoy is becoming more popular every day and it’s great to see it offering this functionality. Also, there are awesome new features planned by amazing folks working on Envoy. I recommend and encourage you to check it out if you are interested in this topic. In my opinion, concurrency control and load shedding are indispensable stability mechanisms in a microservice architecture to prevent overload and cascading failures. If we read about how big companies ( Google , Amazon , WeChat to name a few) go about this, we realize that there is much more to it than described here e.g. rather than just doing indiscriminate load shedding, we can take various factors like request priorities and tenancy into account. But even a simple mechanism can bring immense benefits in terms of system stability under load and it’s great to see that projects like Envoy are making it available for everyone. Special thanks to Abhishek Mishra (Senior Engineer) for driving the testing and integration with services and Anmol Sachdeva (SRE) and Ankit Kohli (Engineering Manager) for their support. We operate a network of online trading platforms in over 40… 78 Thanks to Johannes Scharlach . Load Shedding Circuit Breaker Nginx Kubernetes Openresty 78 claps 78 Written by Principal Engineer @ OLX Group. https://twitter.com/vikaskx We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Principal Engineer @ OLX Group. https://twitter.com/vikaskx We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-02-25"},
{"website": "Olx", "title": "how to ship one binary for multiple ios apps", "author": ["Nishant Sharma"], "link": "https://tech.olx.com/how-to-ship-one-binary-for-multiple-ios-apps-1ac752009081", "abstract": "About OLX Group This article is covering how OLX’s mobile engineering team developed an optimised solution for reducing archive time to generate multiple iOS apps (with varying configurations & schemes amongst several brands like OLX Indonesia, OLX India, OLX South Africa, etc.) from a single source code . If we only want to deliver a few customised binaries, doing it manually is manageable. The big question is: What if we want to deliver dozens? …. or even hundreds? In our case, building one archive took around ~1100 seconds. Add to that multiple binary generations and uploading to App Store and the time significantly rises based on the number of binaries. Before diving into the solution, let's try to understand few things… These are the items that are normally different in each version: App Name and Icon Image assets (Logos, Icons) Text (like headers, titles, buttonNames, etc) Plist settings Server API baseURL Crashlytics/Fabric and Analytics Keys Using Multiple targets in XCode to make two separate apps from single code base, but each app can vary in aspects like branding, logos, splash screens and even distinct theme colors. Place build configuration settings into .xcconfig files and start building a different version of an app by simply changing the scheme. iOS applications are bundled in a single file of type IPA ( i OS A pp-store P ackage). Although they come with an ipa suffix they are just regular ZIP files Modify the suffix to zip and extract internal files of IPA. Contains single folder called Payload , inside of which will be another folder called < APP NAME >.app . Besides the Payload folder, There may also be other folders and files, usually for use of iTunes service. Right-click on app folder , and select “Show Package Contents” Inside we will find: Reuse the build that was done from one archive and only remove the files that are not needed, edit the ones that should be edited, add new files and resign the .app, generate a new .ipa. This reduces the archive time from ~1100 seconds to ~75 seconds for each binary. It’s basically the process of reusing the same binary to do different bundles that are going to be deployed. During this process, We modify .app container as shown below: Provide information about source configuration path, binary root folder, and app-specific name & unzip IPA file. It contains starting from cleaning up the previous app, adding new assets, moving necessary files to the app, adding new entitlements to an app, adding new info plists to the app and the last step is edit app name. Deletion of files that should be replaced or that are generated when signing the bundle Create Assets with actool (For more info: man actool ) and move them to the new app Copy things from Configuration folder into the new app & .mobileprovisions to be able to codesign Get previous entitlements to add the xcode-generated fields to the new entitlement. Adding or updating new plist to application Change the executable name to be consistent with CFBundleExecutable This phase contains the signing an application and generating new IPA. Installed the Xcode developer tools, it included with it an application called codesign (/usr/bin/codesign). This is the application used by Xcode and the Xcode Organizer to sign our app after it has been built. Use this application to replace an existing code signature after changing bundle resources or wanting to modify the entitlements that were used during code signing. Use Apple’s codesign tool to resign the different parts that are found in the app that should be signed: 1. Frameworks 2. App Extensions 3. The app itself An important fact about the signing part: it should be done in a specific order . The signing should respect the folder hierarchy. So, if we’ve got frameworks and extensions, first of all, we’ve got to sign those, and finally sign the app. If we do it in reverse order then signing will fail (or even worse: fail silently and Apple will reject the .ipa when trying to submit it through Application Loader [altool]). This phase responsible for cleaning the generated files like plist, entitlement etc. This script shouldn’t be used to replace and resign apps with different product module names, for example, this script can not generate an X app from Y as they have a different Product Module Name. Even though the script will allow this, it’ll produce crashes in production that are unavoidable . Implementation of replacing and resign script is fast and easy to implement, it improves binary generation time significantly. It improves overall pipeline performance time to build app. That’s a ~50% decrease in archive build time. .As we can see, below performance measurements include archiving and deployment to the apple app store. We operate a network of online trading platforms in over 40… 73 iOS iOS Apps Ios Interview Question iOS App Development Olx 73 claps 73 Written by Chapter Lead@OLX Group, working with lots of crazy engineers :) M: +91–9873265401 Linkedlin: https://www.linkedin.com/in/nishant-sharma-221bb911a We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Chapter Lead@OLX Group, working with lots of crazy engineers :) M: +91–9873265401 Linkedlin: https://www.linkedin.com/in/nishant-sharma-221bb911a We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-24"},
{"website": "Olx", "title": "cassandra compaction trade offs", "author": ["Satish Chandra"], "link": "https://tech.olx.com/cassandra-compaction-trade-offs-848ba51c64c0", "abstract": "About OLX Group At OLX chat we need to save the user chats for a long time. Like Whatsapp we have a lot of users interacting with each other, sending millions of messages per day, but unlike Whatsapp our users can log in from multiple frontend clients and hence we also need to save their chat history on our servers. Since a user tends to access his/her data very frequently, the user should be able to do so in as little time as possible. The data model that we deal with is very easily shardable and hence we had decided to go for Cassandra. Cassandra has been serving us for our archival needs since 2017 after migrating from a legacy system using SQL, after realizing that we are not going to use the ACID goodies that Relational DBMS has been designed for. Each user only needs his data, and hence the data can easily be partitioned and distributed using the user id. It would have been a match made in heaven if our data would have been immutable, but since a lot of times additional states have to be associated with the messages, we also need to update the individual chat messages. Using Cassandra we can write a lot of data rapidly and access it at a later point in time without much of a hassle. Cassandra can manage this by first appending the data in memory and later writing it to the disks as a single structured log file (called an SSTable). After a bunch of these SSTables have been accumulated, it runs a background process (henceforth referred to as compaction in this post) which compacts a bunch of these logs, into one or more SSTables. In this blog, I am going to describe the problems which we faced due to sub-optimal compaction strategy choices, how we identified those problems, the steps we took to mitigate those, and the results we have seen so far . In the past decade, a lot of DB systems have matured for production usage, prominent among them are MongoDB, Cassandra, Redis, ElasticSearch, etc. These can be broadly categorized in a key-value store, document store, and wide columnar store. While the way of storage differs, the central philosophy remains the same i.e. no dependence between two data entities at the database level. A lot of the database engines developed(like Cassandra, RocksDB, WiredTiger), work based on LSM tree data structure. Since all the writes are lumped into memory and then written to the disk-based on the size and time threshold, the writes are pretty fast. When writing, the keys are sorted and written in a single disk file (SSTable) for easy retrieval. After multiple SSTables are accumulated, these SSTables are lumped together to merge the data having the same partition key, the data is then aggregated and again written in a new SSTable. The act aggregating the data in SSTables and writing the data in another new SSTable is called compaction . At the end of compaction, all the data with the same partition key is colocated for easy search and retrieval. As you can see that once a key has been written it does not necessarily remain in the same SSTable and this leads to amplification. The way data is written in Cassandra, any writes to a key leads to multiple rewriting of the same data. This is an undesirable behavior in a DB system, even if necessary. In analytical terms, we usually refer to this as the write amplification factor. write amplification factor (waf) = physical writes/ logical writes A similar thing happens when we try to read a key. The row elements might be scattered across various SSTables. One key access might lead to multiple disks seeks. This is quantified by the term read amplification factor. read amplification factor (raf)= physical reads / logical reads The desirous behavior in any DB system is to reduce these to a minimum. If we talk about SQL ( on a DB level, and ignore the physical media complexity), the read and write amplification are generally one. However, in the case of Cassandra, both ( raf and waf ) can vary and are based on the compaction strategy used. Cassandra has three types of compaction. All three make the tradeoff between reading time latency and disk io. Size Tiered Compaction Strategy (STCS) Triggered when multiple (default value of 4) SSTables of the similar size are present. It works great if you have immutable (because there will be no spread of data in SSTables) and unpredictable data writing rate(relatively speaking the write amplification is directly proportional to the data written). If you have mutable data, it will lead to considerable read amplification as the spread of the data might be large. In the worst-case scenario, it might lead to reading all the SSTables. However, the write amplification is predictable to the size of given data. low waf, high raf Time Window Compaction Strategy (TWCS) Compact all the SSTables based on the time window (say 15 days) in one big SSTable. All the fresh SSTable (younger than 15 days) are compacted according to STCS. This is great if you have predictable data demands and your key data would not be mutated after a given time period. The pros and cons are mostly the same as STCS. low waf, high raf Leveled compaction Strategy (LCS) Compact SSTables based on levels. Each level is 10 times more than the previous level. Once the number of SSTables in a level exceeds its level count, the SSTables are compacted with the SSTables of the next level. The way LCS has been designed ensures that the spread of the data is directly proportional to the levels that will be created, so the amount of raf is consistent. low raf, high waf When we started with Cassandra we naively put in STCS as the compaction strategy for each table. This became problematic for us in due time, due to huge raf . This manifested with a random alert, sustained CPU spikes, and memory pressure. Fortunately for us, Cassandra comes with a default tool (nodetool), which helped us in diagnosing the issue. I will take the example of two tables, one chats where we store chats of a user, and threads where we store the state of a conversation. We want to analyze the spread of data in the SSTables (characterizing raf ) the latency parameters for reads the number of SSTables for each table. Following is the output after running commands for both chats and threads table. Output of nodetool tablehistograms india.chats The associated table info is nodetool tablestats india chats For the chats table our measures are: raf(50%) = 12 latency (99%) = 12 ms count of SSTables = 24 Output of nodetool tablehistograms india.threads Output of table info for threads is ( nodetool tablestats india chats ) For the chats table our measures are: raf(50%) = 6 count of SSTables = 25 latency (99%) = 4 ms There is a lot to cover in the metrics but what interests us is the number of SSTables being touched for servicing 50% of the request is higher than half of SSTables. This may be different for differing cases, but in our case, a huge data spread was taking place. Since the strategy we used was STCS, we knew that for a lower raf we had to change the compaction strategy. This decision was also partially based on the fact that: Our reads to write ratio was ranging from 2:1 for some tables to as big as 20:1 for some heavily used client-facing tables, since we were servicing that many reads it was more prudent and scalable to optimize for reads rather than writes. We had spare storage and io which would allow us to dedicate our excessive io to compaction. These observations led to an inquiry into the change of the compaction strategy. We concluded that since we can go along with write amplification with background writes, we should change the compaction strategy. Thus we can see that LCS offers better read time latency with extra write amplification, it also consumes less space during compaction as compared to STCS. The results after changing the compaction strategy are as follows After changing the strategy our measures for chats table are raf(50%) = 3 count of SSTables = 1198 latency (99%) = 4 ms And our measures for threads table become raf(50%) = 3 count of SSTables = 774 latency (99%) = 4 ms Some inferences that can be drawn from the results shared above: The number of SSTables has reduced a lot for a single read in absolute terms, this is a much-desired improvement because most of the reads can be serviced using a few SSTables, saving on the precious read io. 2. The read latency has also come down by a factor for threads and chats table across all the %iles. This is a direct reflection of the server doing less work in io and the need to refer to fewer SSTables. 3. The number of SSTables has increased by a lot, but this is due to the very nature of LCS compaction. This is tolerable for us because we have spare io bandwidth where these tables can be compacted in the background. Cassandra offers a lot of features. However a lot of caveats should be kept in mind while using Cassandra, particularly relating to the io usage, compactions and tuning of internal parameters. This post show some of the steps we took in improving the performance of our cluster. Some of the lessons that we learnt gradually are Choose a compaction strategy according to the use case. As can be seen, we reduced our data spread by a considerable number, reducing our read latencies. Additionally choosing the correct cloud instance, for io bound application is very important as you will need a lot of bandwidth to accommodate your compacting needs (we used i3 type instances in AWS). http://smalldatum.blogspot.com/2015/11/read-write-space-amplification-pick-2_23.html We operate a network of online trading platforms in over 40… 120 Thanks to Vikas Kumar and Phani Kumar . Cassandra Database Lsm 120 claps 120 Written by Software Developer@OLX/Naspers Classifieds, Scaling chat systems We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Software Developer@OLX/Naspers Classifieds, Scaling chat systems We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-03-26"},
{"website": "Olx", "title": "autosuggest retrieval ranking part 2", "author": ["Aakash Prakash"], "link": "https://tech.olx.com/autosuggest-retrieval-ranking-part-2-14a8f50fef34", "abstract": "About OLX Group In the previous post , we answered questions like “What is autosuggest?” and “Why do we need it?”, followed by the discussion around the techniques that we used to build the corpus for autosuggest at OLX . In this post, we’ll be focusing on the algorithms or techniques that we used to convert the corpus into a full fledged autocomplete suggester . So, let’s get to it! Our older autosuggest algorithm was mainly based on creating suggestions from the Ad fields like title , description , category ( Cars, Mobile Phones , etc), mainly taking into consideration what the users are posting or the inventory that we have rather that what the users search for on our platform. Does not take into account how users search. Does not take into account which suggestions lead to better clicks/responses. Does not take into account user’s interests. Our new algorithm that we are going to discuss will try to overcome the above mentioned shortcomings. Before we do a deep dive into building the suggester , let’s take a look at the problem statement first. The above shown diagram is the flow diagram of what we are trying to build. We already built the corpus in the last post , giving us a list of past user searched queries along with their respective occurrence counts (or frequencies) . Now, whenever the user types some query in the search box , we want to give suggestions to the user from the corpus. So, if we take a closer look, the process can simply be broken down into two parts — Retrieval Ranking Let’s go through both of these steps in detail one by one. Input : User searched query and the autosuggest corpus . Output : Return the list of suggestions from the corpus matching the user searched query along with their frequency counts . We want to build a system which would store our corpus and can answer the following question — What are the top ‘N’ suggestions with max score such that they match with the user searched query? Here, matching refers to the concept of comparing two strings such that there is some part common in both the strings. For example :- If a user types “scoote” , then we want to display suggestions like “ scoote r”, “ scooty ” or “electric scoote r”. Well, there can be various approaches of matching we can use here. We will be going through the following three strategies of string matching - Infix match : Exact prefix match with the whole suggestion . Blended Infix match : Exact prefix match with any word in the suggestion . Fuzzy match : Prefix match within ‘k’ edit distance . Now that we have an understanding of the strategies for getting the list of suggestions matching the user searched query, let’s take a look at the ordering or ranking of suggestions. Input : List of suggestions with frequency counts . Output : Ordered list of suggestions according to frequency counts and/or any other external scoring factor . There are multiple ways of ranking or ordering the list of suggestions among which the following are the common ones — Sort only by frequency of each suggestion (score = f). Sort by frequency and a combination of other important business metrics like the no. of listings opened by the user , the no. of replies after clicking on that suggestion , etc. (score = w1 * f1 + w2 * f2 + ……. + wn * fn, where f1, f2, …. , fn are the features which are important for ranking the suggestion and w1, w2, ….. , wn are the weights or the importance of the corresponding feature to which it is multiplied). By applying any of the above mentioned scoring techniques, you will get a score corresponding to each suggestion in the corpus. So, we can just sort the relevant suggestions according to these scores. Instead of storing the frequency, we could actually precompute this score for each suggestion offline and store it so that we don’t have to re-calculate this score for a suggestions again and again for each user query. Let’s elaborate on the first approach of scoring which uses only the frequency to understand how we can fit it into our design which has different kinds of suggestions we got from infix match, blended infix match and fuzzy match. The concept of merging the different kinds of suggestions is what we call as blending. Following is the algorithm that we use for blending the suggestions from different strategies — Fetch top ’N’ suggestions from each strategy (infix, blended infix and fuzzy) matching the user searched query. Decay (or divide) the frequency of each suggestion that we got from a strategy by the strategy’s decay factor (d) . Add all infix suggestions to the final list . Add all the blended suggestions which are not already a part of final list. Repeat the previous step for fuzzy suggestions . Sort the final list of suggestions by decayed frequencies . Return the top ’N’ suggestions from the final list . Decay factor (d) should obviously be in the following order — infix < blended infix < fuzzy Let’s take the decay factor of infix as 1 , blended infix as 2 (X) and fuzzy as 3 (Y) . Then, the algorithm would work as shown below — The sorted list of suggestions would then be suggested as the user types in the query. NOTE : You should also remove the suggestions which are not going to fetch any results or listings as that would be lead to a bad user experience. As you can see from the above HLD, there will mainly be 4 components involved. We’ll go through each of these components one by one in detail. Apache Solr is an open source platform based on Apache Lucene project. Its major features include full text based search, real-time indexing, database integration, .., etc. We would mainly be using the suggester component which will serve the purpose of our retrieval component that we discussed above. If you have worked with Solr before, you might be aware that we would need to create a collection which would consist of two main files — solrconfig.xml : It is mainly used to configure the high level behaviour of SOLR like the request handlers, components, cache configuration, .., etc. managed-schema (or schema.xml) : It is used to define the properties or the structure of the document that we are going to store in SOLR. Following the configurations that we are going to use — Add the following suggest component to your solrconfig.xml. It defines the 3 strategies of string matching that we discussed above namely, AnalyzingLookupFactory (Infix), BlendedInfixLookupFactory (Blended Infix) and FuzzyLookupFactory (Fuzzy). Now, let’s add the request handler for this component in our solrconfig.xml. Now, that we have setup our request handler for the suggestion strategies that we are going to use to query our corpus in SOLR. Let’s define our document structure (or schema) that we are going to use to store our processed user searched queries along with the score. Let’s go through each of the above mentioned fields — query : It is used to store the actual search query like maruti suzuki, iphone 11 . cost : It is used to store the score corresponding to the search query. queryKT : This field is actual a slight modification of the query field as this is the one which we are using in the queryPrefixStrategy or for the infix matching and fuzzy matching. queryST : This field is also a modification of the query field and it is used in the blended infix strategy . Let’s now go through the field types — string : This is nothing but a normal text field used by the query field. long : This is used by the cost field for storing the score of the query . queryKT : This field type stores (or indexes) the field as is and doesn’t do any modifications as we need to compare the whole string in case of infix matching and fuzzy matching . But on query time it will remove special characters and make all characters of user searched query lowercase before comparison. queryST : This field type stores (or indexes) the field by breaking the multi word queries into multiple searchable indexes as it is going to used in query blended where we need to treat all the words separately so that matching can happen from anywhere. It filters out the stop words , removes special characters and make it lowercase . For example : An entry in the processed queries like “Maruti Suzuki, 200” will be stored as query = Maruti Suzuki, cost = 200, queryKT = maruti suzuki, queryST = [maruti, suzuki] Now, that we have finally built our retrieval component we can run queries like — It will give us results from all three strategies for query = “ca” like below — We are going to use redis for caching the response of autosuggest for a particular query. So, whenever a query comes in for a particular keyword then the response corresponding to the keyword will be cached and can be used to serve the subsequent requests for the same query to improve response time and prevent hits to Solr . One optimisation that you could do to prevent overuse of redis is that instead of storing all the responses just store responses only for those queries whose length is say less than ‘X’. As the no. of possible distinct keys is going to increase exponentially with each character so for ‘X’ characters, the no. of possible distinct keys can be 26 ^ X. Autosuggest service will be responsible for the following things — Serve the ‘/suggest’ endpoint. For each request , get the response from cache if present . Else fetch the suggestions from all three strategies . Blend them and rank them according to the score as we discussed in the ranking section earlier. Finally, store the response containing the top ‘N’ suggestions corresponding to user searched query if its length is less than ‘X’ and return it. There would mainly be 3 responsibilities of this component — Fetch : Fetch the past user searched queries. Process : Process the queries using the algorithms discussed in the previous post . Index : Index (or store) the corpus (or processed user searched queries along with their respective scores in SOLR. We can schedule a cron or a scheduler to regularly run the processor. For example : If you are taking the last month’s user searched queries for processing then you could run the preprocessor every week or every day depending on how frequently your user searched queries change. India : Null searches decreased by 2% and autosuggest adoption increased by 4–5% . Indonesia : Null searches decreased by 5.1% , autosuggest adoption increased by 31% and search conversion increased by 1.2%. Pakistan : Null searches decreased by 20% , autosuggest adoption increased by 4% and search conversion increased by 4.1%. South Africa : Null searches decreased by 5% , autosuggest adoption increased by 8.5% and search conversion increased by 2.4%. LATAM Countries (Argentina, Colombia, Ecuador, Costa Rica, Guatemala, Panama, Peru, El Salvador): Autosuggest adoption increased by 15.7% and search conversion increased by 1.4%. We discussed various techniques for retrieval & ranking and what it takes to build a full fledged scalable autocomplete suggester including the various components involved. In the next part, we will be discussing various techniques to build the catalog suggestions and further improvements or optimisations that can be done in the suggester to increase the adoption. We operate a network of online trading platforms in over 40… 169 Solr Redis Auto Suggest Search User Experience 169 claps 169 Written by Search, Personalisation and Relevance @ OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Search, Personalisation and Relevance @ OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-12-22"},
{"website": "Olx", "title": "why building olx with firebase performance feature", "author": ["Nishant Sharma"], "link": "https://tech.olx.com/why-building-olx-with-firebase-performance-feature-6c6069007352", "abstract": "About OLX Group This article is covering how OLX’s mobile engineering team utilise firebase performance SDK for continuous monitoring as well as reducing app launch time, improving rendering as well as to improve overall network API success rate. Firebase Performance Monitoring is a service that helps you to gain insight into the performance characteristics of your iOS, Android, and web apps. Performance monitoring is based on traces. A trace is the capture of performance data that occurs between two different points within the execution cycle of an app. The Performance Monitor enables you to monitor and record performance information for all activity on firebase dashboard. However, times occur when you need to monitor the performance of a specific business flow or the performance issues that are reported by a specific user. In these cases, you can use a performance trace. The Performance Trace enables you to: Automatically measure app startup time, HTTP network requests, and more Gain insight into situations where app performance could be improved custom code traces to capture your app’s performance in specific situations, like when you load a new screen or display a new interactive feature. And, you can create custom metrics on these custom code traces to count events that you define (like cache hits) during those traces. The collected performance data for each trace are called metrics and vary depending on the type of trace. For example, when an instance of your app issues a network request, the trace collects metrics that are important for network request monitoring, like response time and payload size. Each time an instance of your app runs a monitored process, the associated trace also automatically collects attributes data for that app instance. For example, if an iOS app issues a network request, the trace collects the device, app version, and other attributes for that specific app instance. You can use these attributes to filter your performance data and learn if specific user segments are experiencing issues. The struggle is to find the business flows for custom traces. Once identified, putting custom traces are a piece of cake. In our opinion, adding Firebase Performance SDK helps in monitor and visualise the results in the Firebase Console. For ex: In last 90 days, we were able to reduce app launch time by 11%, critical screen rendering issue to 0.1% from 1.7%. We operate a network of online trading platforms in over 40… 13 Thanks to Rahul Khanna . Olx Olx Engineering Olx Group Engineering Firebase Performance Crashlytics 13 claps 13 Written by Chapter Lead@OLX Group, working with lots of crazy engineers :) M: +91–9873265401 Linkedlin: https://www.linkedin.com/in/nishant-sharma-221bb911a We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Chapter Lead@OLX Group, working with lots of crazy engineers :) M: +91–9873265401 Linkedlin: https://www.linkedin.com/in/nishant-sharma-221bb911a We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-22"},
{"website": "Olx", "title": "demystifying istio circuit breaking", "author": ["Vikas Kumar"], "link": "https://tech.olx.com/demystifying-istio-circuit-breaking-27a69cac2ce4", "abstract": "About OLX Group At OLX Autos, we have started adopting Istio service mesh for our EKS cluster. There are a number of features we are excited to use — circuit breaker being one of them. We currently use circuit breaker in some of our Java services using Resilience4j , but Istio will give us the ability to apply circuit breaking at the network level without having to integrate into the application code of each service. Although it looks simple on the surface, we found that the documentation (and other resources on the internet) isn’t very helpful in understanding the behavior of the circuit breaker in different scenarios. So we set out to perform some tests to solidify our understanding before we start using it for actual use cases. This post is about our learnings from a deep dive into a particular Istio feature. I assume the reader is already familiar with both Istio and Kubernetes. As per the documentation , we need to create a destination rule to configure circuit breaking for a target service. The parameters relevant to circuit breaking functionality are defined under connectionPool (There’s also outlierDetection which is also part of the circuit breaker, but in this post, we’ll only focus on the connectionPool). The relevant configuration parameters are: tcp.maxConnections : Maximum number of HTTP1 /TCP connections to a destination host. Default 2³²-1. http.http1MaxPendingRequests : Maximum number of pending HTTP requests to a destination. Default 2³²-1. http.http2MaxRequests : Maximum number of requests to a backend. Default 2³²-1. From the examples in the documentation, it’s somewhat clear how these parameters work in a simple scenario — one client and one destination service instance (in Kubernetes environment, an instance is equivalent to a pod). However, that’s hardly the case. In a production environment, more likely scenarios are: One client instance and multiple destination service instances Multiple client instances and single destination service instance Multiple instances of both client and destination service The documentation is a bit lacking in defining the behavior in such cases. To gain a deeper understanding, we decided to test how these parameters work in a local setup (Istio on Minikube). Our test environment is: For our tests, we created two python scripts — one for the destination service ( pyserver ) and another for the calling service ( pyclient ). The server script is a simple Flask application that exposes a single endpoint that sleeps for 5 seconds and then returns a “ Hello ” string. Here’s the sample code: The client script calls the server endpoint in batches of 10 i.e. 10 requests in parallel, then sleep for some time before sending the next batch of 10 requests. It does this in an infinite loop. To make sure that when we run multiple pods of the client, all of them send the batch at the same time, we are using system time ( 0th , 20th, and 40th second of every minute) to send the batches. With these scripts, we created the docker images for both client and server and started the pods. Let’s start with the test to understand the behavior of tcp.maxConnections . A basic premise for all the tests is that Istio (Envoy) proxies do not coordinate with each other. They receive configuration updates from control plane, but for data plane, they act independently based only on local information. Let’s first create and apply the following destination rule: It’ll limit the number of TCP connections to the destination service ( pyserver-service ) to 5. Let’s see how it works in different scenarios. In this scenario, we have just a single pod of each service. When we start the client pod and monitor the logs, we see the following: All the requests are successful. However, only 5 requests in each batch have the response time ~5 seconds, rest are much slower. It implies that only using tcp.maxConnections results in excess requests being queued, waiting for connections to free up. As mentioned below, by default, the number of requests that can be queued is extremely high. To actually have the circuit breaking (fail-fast) behavior, we need to also set http.http1MaxPendingRequests which limits the number of requests that can be queued. Its default value is 2³²-1 . Interestingly, if we set it’s value to 0 , it just falls back to the default value. So we have to set it at least 1 . Let’s update the destination rule to allow only 1 pending request: And restart the client pod. This time, we should observe throttling. As you can see, 4 requests are immediately throttled, 5 requests are sent to the destination service and 1 request is queued. This is the expected behavior. http2MaxRequests Another parameter that we mentioned at the start is http2MaxRequests . Despite the name, this parameter is not HTTP2 specific. It dictates the number of maximum outstanding requests to the destination service. In the case of HTTP1, it’s roughly equivalent to tcp.maxConnections as in HTTP1, there can only be 1 active request on a TCP connection. Although the TCP connections can be reused using tcp.tcpKeepalive and http.maxRequestsPerConnection parameters, the previous request has to complete before the next request can be sent. For HTTP2, http2MaxRequests is very important since, with HTTP2, we can send multiple concurrent requests on a single TCP connection. So to control the traffic flow, we need to put a limit on max outstanding requests rather than max TCP connections. Since I have used HTTP1 for the tests, we can either use tcp.maxConnections with http.http1MaxPendingRequests or just http2MaxRequests . The results will almost be the same (I said almost, because with http2MaxRequests we won’t have to deal with pending requests). I’d recommend that for HTTP requests (HTTP1 or HTTP2), we use http2MaxRequests , but since I started the first test with tcp.maxConnections , I’ll continue using it for the rest of the tests for the sake of consistency. Now let’s run the test in a scenario where we have multiple pods of the server. First, we need to scale the destination service deployment to multiple replicas (say 3 ): What we want to test here is whether — The connection limit is applied at the pod level — max 5 connections for each pod of the destination service Or is it applied at the service level — max 5 connections overall irrespective of the number of pods in the destination service In (1) , we should see no throttling or queuing as max connections allowed will be 15 (3 pods, 5 connections per pod). Since we are sending only 10 requests at once, all of them should be successful and return in ~5 seconds. In (2) , we should see roughly the same behavior as before (scenario #1). Let’s fire up the client pod again and monitor the logs: It’s nearly the same as before. We still see similar throttling and queuing, which means increasing the number of instances of the destination service does not increase the limit for the client. The limit is applied at the service level. We can actually see the number of connections the client Istio proxy makes to each pod in the target service: The client proxy has 2 active connections to each pod in the target service. Instead of 5, it’ 6. As mentioned in both Envoy and Istio docs, the proxy allows some leeway in terms of the number of connections. In this scenario, we have multiple pods of the client and only a single pod for the destination service. First, let’s scale the replicas accordingly: Since all Istio proxies act independently based on local information without coordinating with each other, my expectation going into this test was that each of the client pod will exhibit the behavior of scenario #1 i.e. each pod will have 5 requests being immediately sent to the target service, 1 request being queued and rest throttled. Let’s look at the logs to see what actually happens: As you can imagine, I found the results confounding and not at all what I expected. The number of 503s on each client has increased. The system allows only 5 concurrent requests from all three pods combined. How did this happen? Are the client proxies coordinating with each other somehow? I checked the client proxy logs hoping to get some clues and observed two different types of logs for the requests that were throttled ( 503 ): Let’s compare these in a more readable format along with the corresponding field names: The first thing we notice is RESPONSE_FLAGS — UO and URX . Let’s see what these mean : UO : Upstream overflow (circuit breaking) URX : The request was rejected because the upstream retry limit (HTTP) or maximum connect attempts (TCP) was reached. Now it’s starting to make some sense. Requests with UO flag are throttled locally by the client proxy. Requests with URX flag are rejected by the destination service proxy. This is also corroborated by values of other fields in the log such as DURATION , UPSTREAM_HOST and UPSTREAM_CLUSTER . To further verify, let’s check the destination service proxy logs as well: As expected, there are 503 entries here which result in 503 URX entries on the client proxies. So to summarise — the client proxies are sending requests as per their connection limit ( 5 each ) — either queuing or throttling (with UO response flag) excess requests. All three client proxies combined can send a maximum of 15 concurrent requests at the start of a batch. However, only 5 of these succeed because the destination service proxy is also applying throttling using the same configuration ( max 5 connections ). The destination service proxy will accept only 5 requests and throttle the rest, which appear with URX response flag in client proxy logs. Here’s a visual depiction of what happens (actual numbers vary slightly): Client Proxy Retries One important thing I noticed in the destination service proxy logs was multiple entries for a request (identified by X-REQUEST-ID ) which happens because of retries from client proxies (I observed 2 retries in addition to the initial attempt). This brings to our last and likely the most common scenario where we have multiple pods for both client and destination service. We can continue from scenario #3 and just increase the number of replicas for the destination service. If we follow the logic of what we have seen so far, when we increase the destination service replicas, we should see an overall increase in the success rate of requests as each destination proxy can allow 5 concurrent requests. If we increase the number of replicas to 2, we should see 10 requests succeeding amongst 30 requests generated by all 3 client proxies in a batch. We’d still observe throttling on both client and destination service proxies. If we increase replicas to 3, we should see 15 successful requests ( For simplicity, we ignore the queued requests which can be successful later) If we increase the number to 4, we should still see only 15 successful requests. Why? Well, remember the throttling on the client proxy is applied for the destination service overall, irrespective of how many replicas the destination service has. So each client proxy can make a maximum of 5 concurrent requests to the destination service no matter how many replicas are there. I scaled the replicas accordingly and observed the behavior. To avoid verbosity, I’m not including the results here, but results were as expected, which is good :-) With this, we conclude our tests. We have gained a fair bit of understanding and it makes it easier for us to reason about the circuit breaking behavior in different scenarios. Phew! That was a lot. I want to quickly summarise what we have learned so far. We set the limit for circuit breaker using tcp.maxConnections or http2MaxRequests along with http1MaxPendingRequests Each client proxy applies the limit independently. If the limit is 100, then each client proxy can have 100 outstanding requests before they apply local throttling. If there are 80 clients calling a destination service, there can be a maximum of 8000 outstanding requests overall. The limit on the client proxy is for the destination service overall, not for individual replicas of the destination service. The throttling will still happen at 100, even if there are, say 200, active pods of the destination service. Each destination service proxy also applies the limit. If there are 50 active pods of the service, each pod can have a maximum of 100 outstanding requests from client proxies before applying throttling and returning 503. This is the question that we wanted to answer when we set out for this exploration — how can we effectively use this functionality to make my services more resilient. As a service owner, I want to set the maximum request concurrency (i.e. number of requests currently being processed) for each instance of my service —well, ideally, I want to use adaptive concurrency limits , but we’ll have to wait a little bit for that. The circuit breaker configuration allows me to do that. The only caveat is that the same limit is also applied on the client-side. For example, I can configure that each instance of my service can have a maximum of 200 concurrent requests using a destination rule. Anything over it should result in throttling. It can make my service more stable and predictable under various load conditions. But what are the implications for clients? Each client instance will be limited to 200 outstanding requests to my service (i.e. to all the instances of my service), even if I’m running 100s of pods of my service (static or auto-scaled). This limit may not be suitable for all the clients. It‘d be great if we could configure this limit separately for the destination service and individual clients, but I could not find a way to do that (If there is, let me know). As we start adopting Istio, it’s imperative for us to understand its behavior in different scenarios so that we can use it effectively. The documentation and learnings shared by various folks are very helpful but do not always give a complete picture. We started with the circuit breaker and will continue to run such tests for this as well as other features and share our learnings. Hope this is helpful to people starting out with Istio. Thanks to Rahul Jain , Nikhil Sharma , and Abhishek Tomar for reviewing the draft of this post. The information/results in this post are based on my high-level understanding of Istio/Envoy and the observations from my tests. If you find any discrepancy in what I found/presented, let me know (vikas.kumar@olx.com). We operate a network of online trading platforms in over 40… 138 Istio Kubernetes K8s Circuit Breaker Resilience 138 claps 138 Written by Principal Engineer @ OLX Group. https://twitter.com/vikaskx We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Principal Engineer @ OLX Group. https://twitter.com/vikaskx We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-12"},
{"website": "Olx", "title": "fighting fraud with triplet loss", "author": ["Artemii Frolov"], "link": "https://tech.olx.com/fighting-fraud-with-triplet-loss-86e5f79c7a3e", "abstract": "About OLX Group One of the common problems of the online advertising industry is fraud. Even though it varies across markets, fraud methods have a lot of similarities worldwide. In this post, we’ll meet: Fraudsters, who change images Embeddings, which help find distances so we can distinguish images from each other faster Siamese Network with Triplet Loss explained in simple language Tricks and Hacks — so we can train network better One of the known methods of fraud is stealing pictures of the goods from the original seller. Fraudsters pass these pictures off for their own, selling the fake goods, or renting out fake apartments. Often, in order to bypass security systems, they not only copy the ad but also make changes both in the text and attached images. Sometimes the text is rewritten completely and the only way to determine the duplicate is an image. Of course, there are systems that help determine such duplicates. But as the number of images increases, it becomes longer and more difficult to compare them directly with each other. So here comes an embedding to help us. An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors — in our case, images. That means that we can transform an image of any size to a not very long vector and then compare not the images themselves but their embeddings, which is much faster. The speed is important here, as we have a large dataset. We can make it more efficient by converting the embeddings to hashes. For example, we can use locality-sensitive hashing for that. So, how does the process of determining a duplicate looks like? When a new image is uploaded to the database, distances between the embedding of the uploaded image and embeddings of the images, already stored in the database, are calculated. If the distance is less than the “Undoubtedly duplicate” threshold, we can immediately mark the listing as a fraud. If this distance is greater than the threshold “Undoubtedly original”, we can pass the image on to the site. If this distance is between these two thresholds, then moderators come to the rescue — they can determine whether the image is a duplicate or not. Thus, there is another requirement for any system to search for duplicates — it must not only correctly mark the duplicate, but also find the most similar image to it. Now we need to understand how we can get these embeddings. There are a lot of methods, both machine learning and not, but in this article, we will look at one interesting architecture — a Siamese network with triplet loss. In simple language, it is a network that trains to get embedding so that the distance between similar images is much smaller than the distance between completely different ones. That means that through the same neural network, which we will call the base network, we run the first image, then a similar image (for example, augmentation, as in our case!) and a different image. Let’s check what we do next with the formula below (don’t worry we’ll explain it soon!): Here we find the distance between the original and the similar image (‖A − P‖), and the distance between the original and the dissimilar image (‖A − N‖) so that one distance is much smaller than the other. Now let’s try to go one level deeper — where does the alpha come from and why is it needed? It’s simple: since the task of backpropagation is to minimize the loss, the most effective way to train this neural network with loss without alpha would be to zero all weights. Then the distances would be equal to 0 and the distance between the distances — also 0! The neural network is happy with zero loss, but we, on the contrary, are not happy, because no matter what we do run through, the output is always zeros. So that this situation does not happen, there is an alpha — the margin between the positive (original minus similar) and negative (original minus different) distance. This margin will make zeroing not the most effective step, and the neural network will have to train a more effective way of making loss closer to zero. Previously we defined the original image, the positive example, and the negative example. Together, they form a triplet that is used for training. One of the interesting features of a neural network with triplet loss, in our case, that we don’t need a large dataset. Indeed, having one image, with the help of augmentations it is possible to generate several positive examples and select several negative examples. These examples can be any different images, but there are also some techniques on how to choose a proper negative example, which will be discussed later. It turns out that one image transforms into several (or even many!) triplets, and many triplets mean a better-trained network. Another important detail of neural networks with triplet loss is a selection of negative examples. As practice shows, even without special techniques the network trains successfully, but with extra work, it does it both faster and more effectively. The selection is as follows — we enter the following groups of triplets: easy triplets: ‖A − P‖ + α < ‖A −N‖ semi-hard triplets: ‖A − P‖ < ‖A − N‖ < ‖A − P‖ + α hard triplets: ‖A − N‖ < ‖A−P‖ As usual, A is for the original image, P is for a similar image (positive), N is for dissimilar image (negative). Of course, it’s most beneficial to train the network on hard examples (because network works the worst with them), with mixing semi-hard and easy so that the network does not forget its correct state — and one of the easiest ways to do this is the next: 1. Evaluate several random batches 2. Choose a few triplets with the highest loss 3. Mix random triplets with these ones and get, thus, a batch for training The last tool in training will be using a network with pre-trained weights as a basic network. They can be either fully trained or frozen with only the tail of the network trained, but the result of such a transfer learning will only improve the network as a whole. The network is trained, all that remains is to get embedding. To do this, one just needs to run the image through the base network (without using any triplets!), and the output will be the final vector. As it was written before, now it is possible to do anything with it — immediately use it as an image mapping, or convert it to a hash — it depends only on the task. The main thing is that this technology can help to quickly and effectively identify the fraudsters trying to change the image in the ad. Today we learned: Fraudsters can cheat by stealing and changing the image of other users to create a fake listing; We need a fast way to find similar images, so we can find distances between corresponding embeddings; We can compute embeddings using Siamese Network with Triplet Loss; We can generate many positive and select the best negative example to generate triplets; This way, we can quickly and effectively identify the fraudsters trying to use someone else’s image in their ad. We operate a network of online trading platforms in over 40… 372 Thanks to Oksana Kostyuk and Alexey Grigorev . Machine Learning Siamese Networks Triplet Loss Data Science Computer Vision 372 claps 372 Written by Munich-based Samara-born Machine Learning Enthusiast https://www.linkedin.com/in/artemii-frolov/ We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Munich-based Samara-born Machine Learning Enthusiast https://www.linkedin.com/in/artemii-frolov/ We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-01"},
{"website": "Olx", "title": "gitlab ci cd for android part 2", "author": ["Ankur Jain"], "link": "https://tech.olx.com/gitlab-ci-cd-for-android-part-2-3f0cecd32a6c", "abstract": "About OLX Group Note : This is 2nd part of “Gitlab CI/CD for Android”. In 1st part, we covered gitlab basics, pipeline for triggering unit tests on every merge request and finally how to generate the custom build using gitlab CI. We recommend to read it first before starting this part. tech.olx.com This part will cover how to automate the android app release process using Gitlab CI. We have divided the whole process into the following stages: Prepare Build Publish Tag Notify Backmerge For most of the above stages, Gradle tasks have been used to have more control and minimize the dependency on GitLab CI. We’ll go through the Gradle task and GitLab job associated with each of the stages in the pipeline. We have defined a GitLab variable i.e. RELEASE_TYPE, you must need to pass any of the {MAJOR, MINOR, PATCH} value to trigger the release process. This stage will create the RC branch and then bump up the version. Let’s dig into the steps: First, it reads the RELEASE_TYPE variable from GitLab CI and basis on that, it creates the RC branch, if not already created. For example, In the master branch, if we have version 5.2.0 and we want to release a minor version then it will create a branch with the name “ rc-5.3” . Note : Patch releases are done from the same branch. Now, We change the version based on release type(Major/Minor/Patch) and push it back to version.gradle file(we are using separate gradle file to keep version code and version name). Here is the Gradle task to check out the branch and bumping up the version: Here is the GitLab stage to call the Gradle task: This stage generates the APK of our app. You can skip this step while configuring the release pipeline. We have defined this stage in our pipeline because we publish bundles to playstore but we need to provide APKs to our manual QAs. Here is the gist of Build stage: To Publish our artifacts we’ve used the Gradle Play Publisher plugin. It creates Gradle task which can publish the artifacts to playstore upon executing them. You can refer the following blog to know more about how to configure the Gradle Play Publisher plugin: proandroiddev.com After configuring the plugin, we simply need to define the publish stage as below: Now after publishing the app on Playstore, we need to create a tag with the current RC version. we have defined Gradle task to create tag as below: Now after defining the Gradle task to create the tag, We need to simply call the Gradle task from the GitLab stage: After creating tag on repository, Now we need to send changelogs to stakeholders. Notify stage will first fetch the changelogs and then It will send those changelogs to the mentioned mail/group. To get the changelogs, we follow a naming convention for git commits where we put Jira ticket id in square brackets followed by the ticket title. While getting the changelogs we use regex to find commit messages with this pattern. We are using JavaMail API to send mail through the Gradle task. we need to add JavaMail dependency into the project’s Gradle file as below: Now we have defined custom Gradle task to send the mail. It first fetches the changelogs from the rc_last tag to the current rc_tag version. This rc_last tag will be updated to the latest commit after notifying the changelogs to stakeholders. Here is the Gradle task to send mail: Now, We need to call this Gradle task from GitLab CI job as below: This is the last stage of our GitLab CI pipeline. Here we back-merge the content of the RC branch into the master branch. Here the is Gradle task for back-merge: Now, to execute the Gradle task above, we need to call it from GitLab CI as below: So this concludes our article on Gitlab CI/CD for Android. Here are some points which we covered in 2 parts: Gitlab CI Basics Reducing the manual effort of publishing build on playstore, sending mail, back-merge using automated pipeline. Leveraging Gradle tasks so that they can be reused with any CI solution(Jenkins, circle CI etc). Thanks to my awesome team member Vidur Sachdeva for contributing in pipeline and the blog. We operate a network of online trading platforms in over 40… 41 1 Gitlab Ci Android Ci Cd Pipeline Gradle Release Management 41 claps 41 1 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-06-30"},
{"website": "Olx", "title": "gitlab ci cd for android part 1", "author": ["Ankur Jain"], "link": "https://tech.olx.com/gitlab-ci-cd-for-android-part-1-98e45b30ee34", "abstract": "About OLX Group Our team is responsible for maintaining and developing the new features for the OLX android app in 5 different regions(India, Pakistan, Indonesia, South Africa and Latin America). We follow a process of releasing our android app with new features and improvements every fortnightly. Each of the app release includes the following tasks: Individual feature testing by manual QAs Features get merged to master after QA sign-off Creation of Release Candidate branch and Version bump Publish build on playstore Creation of tag with new version Get the changelogs and Notify to stakeholders Back merge RC branch into master branch Phewww!! Lot of tasks for a release. Right? And we need to repeat all these tasks every fortnightly. That’s where CI/CD pipeline comes to the rescue by automating most of the above tasks. We are using Gitlab CI to achieve that. Since our code is also hosted using GitLab, it gives us an extra advantage to have more control over branches. After reading this blog, you’ll be able to: Trigger the build with unit tests in every merge request to ensure code stability. Facilitate QA members to generate the custom build based on build variants(E.g. Olxin, Olxza etc.) and build type(Release/Debug). Automate the release process. Before going into each goal, let’s touch some of the basic GitLab CI keywords which will help you in a better understanding of the CI/CD pipeline. .gitlab-ci.yml : A YAML file which has the configurations for the pipeline. job : defines what to do. For example, jobs that compile or test code. image : Gitlab use docker images to execute the pipeline. image tag is used to specify a Docker image to use for the job. stages : Used to define stages that can be used by jobs and are defined globally. The ordering of elements in stages defines the ordering of jobs’ execution. script : It is the only required keyword that a job needs. It’s a shell script that is executed by the Runner. artifacts : It is used to specify a list of files and directories which should be attached to the job when it succeeds, fails, or always. variables : Define variables on a job level. To minimize the dependency on the Gitlab CI tool, we have used Gradle Tasks to run the scripts. Generate build : This job will start generating the build as soon as a merge request is raised. To trigger build on each merge request we simply need to use predefined GitLab keyword merge_requests under only GitLab configuration parameter. Run unit tests : This job will execute unit tests for all the variants of the app. Gitlab CI does not keep the artifacts across the multiple jobs. To preserve the artifacts, you need to define the path where artifacts will be stored after the completion of the task. This job will create the custom build with any specified variant and build type and store the final artifacts. To generate the custom build for different variants, we need some variables that we can pass to gitlab-ci.yaml. Then yaml will read those variables. So, before going into the yaml part let’s look at How to define variables with default values. Go to your project’s Settings > CI/CD and expand the Variables section and define variables as below: Now when variables are being defined, Here’s how we can define a job which can be triggered to generate a custom build: Now we can generate different builds by passing values of variables BUILD_TYPE(Release/Debug) and BUILD_VARIANT(E.g. Olxza, Olxpk etc.) and setting IS_CUSTOM as true. This concludes the first part of this article. In next part, we’ll look into how we can automate android app release process using Gitlab CI and gradle tasks. tech.olx.com We operate a network of online trading platforms in over 40… 102 Android Ci Cd Pipeline Gitlab Gitlab Ci Gradle 102 claps 102 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-06-30"},
{"website": "Olx", "title": "the video experiment at olx part 1", "author": ["Sahil Ajmani"], "link": "https://tech.olx.com/the-video-experiment-at-olx-part-1-8d93ac2f633f", "abstract": "About OLX Group With the ever-increasing adoption of smartphones and reduced data cost, there has been immense growth in video traffic in the past couple of years. It’s estimated that by 2022, 82 percent of the global internet traffic will come from video streaming and downloads (Cisco, 2019). With most cities supporting in India supporting 4G today, internet bandwidth is no more a challenge. Hence, we decided to leverage this opportunity to enable Video as a media attachment in our current Ad posting flow. What that basically means is that seller will be able to record and upload a demonstration of the product he wishes to sell on the platform. As a buyer, when I look at a video having a working demo of the product I am planning to buy, I develop some confidence (or trust) on the product condition and seller intent. In this series of posts, we’ll tell you how we built adding video upload as a step in the current ad posting flow. In the first part, we cover: Challenges Backend Architecture Components Involved Let’s start! There are ~400K ads being (successfully) added to the platform every day, not to forget there is a definite drop off too in the funnel right before the user reaches the “Post Now” button. In this situation, adding a video upload step could backfire if it becomes a bottleneck in the funnel. At the same time, video upload duration is dependent on multiple factors like the camera quality (resolution), fps, the computation power of the recording device (for encoding) which is quite varied (in android devices), and finally the internet bandwidth. Also, adding a new step into a conventional system where our users are habituated into posting ads in a predictive manner might just break the momentum and become a victim of what is called “resistance to change” among our users. We also had to make this modular in the sense that this is still an experiment so we do not want to move major blocks in a currently stable platform until we are sure about what works best for our users. There were two parts of integration for video as a feature on the platform. One is at the seller side where inventory (video) is created on the platform while the second is when a user consumes the video as a “buyer”. For the first phase, we plan to understand the user journey as a seller. We will run an A/B test to expose the feature to our user base using Laquesis (our internal A/B tool) and decide which flow the user should navigate to on a click of the “Sell” button. There are three variants to our experiment all of which had the same basic sequence Recording video of the product Review and select images extracted from the video (since images are mandatory) Fill remaining attributes bases ad category (video uploads in the background) “Post Now” Let’s now talk about components involved in implementing this seller journey The Media Server has 4 primary responsibilities: 1. Video Upload By selecting S3 as our data store for videos, we leverage the S3 multipart URL signing mechanism to enable clients to directly write to S3, bypassing our app servers and saving not only an extra hop but also infrastructure resources (the game changer). Our backend system exposes a single endpoint for app clients to generate a signed URL with a defined expiry time , which can be used to directly write a specific object (.mp4 file) to S3 at a specific path, with multi-part support to make the uploads fail-safe. In addition, we use the S3 transfer acceleration feature to optimize the flow of video packets to our central bucket via the closest edge server over the “Aws Backbone” network. The multi-part upload in combination with transfer acceleration proves to be the least latency solution for media upload. 2. Video Processing Video Processing is a broad term that includes: 2.1 Transcoding When a video is recorded at source and stored in mp4 format, it still is a compressed version of it depending on the encoding format the recording device supports (mostly H.264 ). Hence, the size of the video is not just dependent on its length but also the format (due to the principle of spatial locality ). Transcoding is accepting an already compressed (or encoded) version of the source file, decompressing it (into raw version), and then compressing it again based on output profile (format and resolution). This is a highly CPU intensive job. 2.2 Watermark Insertion OLX watermark is inserted into the output feed at a specific position ((x,y) coordinates) derived from the resolution of the output profile. Our HLS output is multi-bitrate: 720p, 540p, 480p and 360p. 2.3 Packaging The resultant feed is then packaged into multiple chunks and indexed into a playlist file that makes the feed streamable. This enables the output video to be playable immediately after the download of the first chunk and the subsequent ones can be downloaded in the background to create a buffer. We selected HLS as the output format since it is supported by the majority of platforms (Android, iOS, and most browsers). 3. Smooth Streaming This part takes care of delivering video to the end-user in the most optimised way for a smooth experience with minimal buffering. We use CDN for last-mile delivery to make sure video segments are downloaded from the closest edge server geographically. HLS as an output profile supports Adaptive Bitrate (ABR) to handle low-bandwidth issues where the player at client end automatically switches to the lowest bitrate segments in low network bandwidth conditions to make sure there minimal buffering experience (the time it takes to download video segments while there are no existing segments to play) 4. Video Lifecycle Management System maintenance is a key aspect here as we need to make sure that data being created is archived at a fixed cycle. This makes sure that we do not bloat our storage servers hence, keeps a tab on our storage costs. We run scheduled lambda cleanup jobs to avoid provisioning dedicated VMs and save on costs further. tech.olx.com In the next part , we will cover another critical aspect of the solution which covers the aspect of recording and delivering videos smooth i.e Client Library. Also, there is a need to decide the leader among 3 user flow variants and conclude on the experiment. This resulted in the invention of another tool: “Video Analysis tool (VAT)” that will be used to review and measure the videos being added to the platform on parameters like quality, relevance, and content (audio and video). We operate a network of online trading platforms in over 40… 228 AWS Video Streaming Service Transcoding Storage Trust 228 claps 228 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-02"},
{"website": "Olx", "title": "adapting gps optimised use of core location at olx", "author": ["Nishant Sharma"], "link": "https://tech.olx.com/adapting-gps-optimised-use-of-core-location-at-olx-e4c6d8d6b225", "abstract": "About OLX Group Would you buy used goods from far far away or from someone who lives nearby? Think logistics, travel time, etc. Hence To orchestrate quick, efficient Ads, our OLX app need to know the locations of customer to serve them better. From looking up nearby Ads on OLX to selling an item via OLX, geolocation feature have become a backbone for many thriving businesses. But can you develop an app that would enjoy similar success? Common issues: Unable to decide which API to use for authorization Not choosing the right time to ask for permissions Battery drain: Using multiple instances of location manager Performing location updates continuously Not notifying the system when background activity is complete Mitigate privacy concerns Location-based service features won’t work until users allow you to access their location. Here’s what you can do to persuade them. First, choose the right time to ask for permissions then follow below optimisation techniques “When people know why an app is using something as sensitive as their location — for example, for selling/buying items via OLX or to view nearby ads, targeted advertising — it makes them more comfortable than when simply told an app is using their location” LocationManager should automatically handles obtaining permission to access location services of the device when you issue a location request and user has not granted your app permissions yet. Configure Info.plist in iOS 8–10: Setting a string for the key NSLocationWhenInUseUsageDescription or NSLocationAlwaysUsageDescription in your app's Info.plist file. Configure Info.plist in iOS 11+: Setting a string for the key NSLocationAlwaysAndWhenInUseUsageDescription as well as a key for NSLocationWhenInUseUsageDescription in your app's Info.plist file. Explicitly ask for Authorization: This is especially true for on-boarding screen where you may want to ask for them directly to your user. It’s recommended to turn off location updates when they aren’t required. You can, for example, set a timeout that would stop the updates after a certain amount of time has passed. Use single instance of location manager through out the application. Finally, you can batch the location requests when running in the background and stop updating the location when GPS accuracy is below a certain threshold. People like personalized offers but nobody wants to be tracked by scammers. Make security a priority from the start Secure the data storage Use authorization best practices Assess vulnerabilities in third-party components Mobile R&D team continuosly working on a variety of approaches for improving efficient use of location. We come up with a new Location Manager which support plug-in architecture to reduce direct dependency and follow above 7 bullet points as guideline. Thanks for reading, Keep Learning and growing!! We operate a network of online trading platforms in over 40… 101 Thanks to Rahul Khanna . 101 claps 101 Written by Chapter Lead@OLX Group, working with lots of crazy engineers :) M: +91–9873265401 Linkedlin: https://www.linkedin.com/in/nishant-sharma-221bb911a We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Chapter Lead@OLX Group, working with lots of crazy engineers :) M: +91–9873265401 Linkedlin: https://www.linkedin.com/in/nishant-sharma-221bb911a We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-06-07"},
{"website": "Olx", "title": "the video experiment at olx part 2 client integration", "author": ["Hitesh Das"], "link": "https://tech.olx.com/the-video-experiment-at-olx-part-2-client-integration-a757eb9441a5", "abstract": "About OLX Group One minute of video is worth 1.8 million words — Dr. James McQuivey In the first part , we read about the infrastructure and major backend components we used towards building the video platform at OLX. If you have not read the first part yet, I would recommend you to read the first part of this series to have a sound idea of the video classified ecosystem. tech.olx.com In this part, we will read about the Android app video client module ; all about the architecture, major components, APIs and tools used to develop this feature. So let’s start… Creating a Video Ad in OLX Application Before diving deep into the video module, let’s take you all through the seller experience to create a video classified. Configurations The feature is driven via feature flag to enable support to specific geographical locations. There are 3 variants to this experiment configured via A/B tests (viz. No Video(A), Video with Zero Assistance(B), Video with Minimum Assistance(C) and Video with Maximum Assistance(D)). Each experiment is driven by individual configurations driven by backend API Architecture The video project required complex UI navigation and state management hence a more event driven architecture was preferred for designing the solution. This apart, a better separation of concerns, improved testability and transparent communication has been the motivation in selecting the MVVM ( Model-View-ViewModel ) architecture. Developed in Kotlin the project exposes dynamic UI navigation and pluggable integration to the current seller posting flow. Major Components of the Video Module Now let’s discuss in detail the major components of the module. Media Capture Component For better controls on the device camera in terms of video size, resolution and frame per second , we used an extended and customised version of android Camera API to introduce a configurable camcorder (driven by config API) with the following features Low light detection Motion sensors (viz. detecting device shakiness) Flip and flash functionality with visual elements. Record-pause-resume functionality A custom ProgressBar with play and pause breaks and a count-up timer to show the progress. Video Merger Recoding a product can be done in single shot or users can take multiple shots based on the product type he wants to sell. Hence, in each step, the user can take multiple pauses while recording. Video merging is done at each step and also at the end of all the steps to stitch all the step videos into a single one. Reference : https://github.com/sannies/mp4parser Image Extractor For smart retrieval of images(and metadata like duration) from videos captured, which is mandatory for ads to go live, we have leveraged android MediaMetadataRetriever API that provides a unified interface for the same, thus reducing one step for image selection. Currently, the algorithm to extract images is based on time lapse which is being evolved with object detection techniques as a future scope. Video Playback For playing the videos, we used customised version of ExoPlayer , a lightweight, open source, application-level media player for Android. It provides an alternative to Android’s MediaPlayer API for playing audio and video both locally and over the Internet. In our case, we are using MP4 and HLS (adaptive streaming) video formats for playback respectively. Media Picker The video module also uses an in-house Media Picker library ( OLX open source project ) that can be configured to pick images and videos from the user’s device with predefined constraints(viz. size/resolution of the images/videos, target folder etc.). github.com Media Uploader The figure describes the media upload mechanism. The Video Uploader Component observes for any new video in the video repository via RxJava / RxKotlin observer pattern. The Repository notifies the uploader component. The Uploader component fetches the next video in loop until there is no more videos to be uploaded and listens for upload status. It also notifies the UI subscriber to show the progress status. For all REST API calls viz. f etching configurations for experiment variants , setting up authentication with media server to enable direct upload to S3 bucket with multipart upload support, we have used Retrofit — a type-safe HTTP network client for Android and Java . Conclusion Here we come to the end of Part 2 in this series. Hope, this article gives an idea about the overall objective, challenges, and steps towards the solution. The first phase which was rolled out in February 2020 has helped us build that initial data to test our initial hypothesis and build that improvised version (to come later). I would like to extend my appreciation for the amazing work done by Jatin Juneja , Sarina Dhamija , Amit Pathak, Shalini Prajesh and Pranay Bansal in this project. I would like to thank you for visiting this blog and taking a moment to read. Hope we can provide some useful information here. Don’t forget to hit the clap button more than once :) We operate a network of online trading platforms in over 40… 341 Thanks to Jatin Juneja and . Android Olx Architecture Android App Development 341 claps 341 Written by “When we share, we open doors to a new beginning” — Software Professional. Leading the Android Chapter at OLX INDIA We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by “When we share, we open doors to a new beginning” — Software Professional. Leading the Android Chapter at OLX INDIA We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-07-02"},
{"website": "Olx", "title": "mvvm rb adapting the mvvm pattern at olx", "author": ["Nishant Sharma"], "link": "https://tech.olx.com/mvvm-rb-adapting-the-mvvm-pattern-at-olx-81701c2825b6", "abstract": "About OLX Group Architecture is about structuring the intent of our system. At OLX, our team & iOS codebase is continuously growing, which leads to higher complexity and more dependencies in code. This may end in disaster unless we care about architecture and follow some rules when creating software components Follow the SOLID principle Design for testability Have clear dependencies Keep your code readable and maintainable Having realised that we were facing increasing challenges adhering to the above rules with MVVM, OLX Mobile Engineering Team decided to incorporate their learnings into our own adaption of the MVVM design pattern. An architecture that scales Testability and Isolation Maintainability Readability The view depends on ViewModel and ViewModel depends on Model. When the Model changes, the View needs to be notified and updated accordingly. There are several approaches to achieve this as mentioned in the below image. The RB stands for “Router & Builder”. The group of classes representing one screen in MVVM+RB is composed of the Model, View, ViewModel, Router, and Builder. To create a clear and maintainable way to manage flows in the application we have chosen to add a routing layer to the standard MVVM architecture. The router is responsible for managing flows and replaces segues from storyboards. The Router is just a protocol injected to view model and thanks to the dependency injection we can also mock service responses, making our tests extremely easy to write and read and this unlocks more amazing traits: There are some common UI-related operations that will be performed on almost every screen, like the presentation of activity indicator and error/success messages display — those can be placed in some root protocol for all routers, like RouterType so we can avoid a lot of unnecessary code duplication. Common router functionalities can be encapsulated in protocols with default implementations and composed. For example, we may want our router to be able to present SafariController with some URL — this is not something that all routers need to do, but we might be using it in a few places. All we have to do is create a protocol with default implementation and it can be further composed with other protocols like ImagePickerRoutable. Builder is the key for reusability, declares what data is needed for the Module to be built, injects all the needed components letting the implementations change easily. It is just a simple structure capable of creating all controllers in the entire application. Separating the class creation logic in the Builder adds support for mock-ability on iOS. To lists the dependencies that a builder needs from its parent, we define the Dependency protocol . The component is an implementation of the Dependency protocol and responsible for owning the dependencies that the builder creates for itself and its children. Injected Component responsible to decide what dependencies to expose to the children on its own. Every build method follows the same scheme: Initialises router, injects itself, so such router can request the creation of another controller Initialises view model injects required dependencies, initial data, and router Initialises view controller injects view model Assigns view controller to a weak property of a router The important piece of our architecture is that all pieces of architecture should be implemented using protocols & dependency injection. No class should know about the concrete implementation of another class. With this approach, we are also discouraging the use of singletons and static methods/variables. Both static objects and singletons reduce testability. Each of these patterns can also lead to bad coding practices and create bugs with shared state management. D ependencies are services or objects that a class needs to perform its function. DI is a coding pattern in which a class asks for dependencies from external sources rather than creating them itself. Injecting dependency has two main advantages. Control the instantiation of objects from a central place instead of spreading it across the whole codebase. Help us write unit tests, we can just pass mocked versions of classes instead of actual values. Advantages of Using MVVM+RB Your code is even more easily testable than with plain MVVM. Your code is further decoupled (the biggest advantage.) The package structure is easier to navigate. The project is easier to maintain. Your team can add new features even more quickly. Disadvantages of MVVM+RB It has a slightly steep learning curve. How all the layers work together may take some time to understand, especially if you are coming from patterns like simple MVC. It adds a lot of extra classes, so it’s not ideal for low-complexity projects. OLX Chat (i.e. a reusable framework) has been using this architecture in a production application and we are really satisfied with how it improved the quality of the chat module. Below image depicts some of the key benefits: Some of the best feedback We have received so far is: By the way, it’s never too late to architect your app. You may want to do it sooner than later, of course. Ultimately, you spend less time fixing bugs, more time building great apps, which means happy customers, happy employers, and that means a happy you. For more: MVVMRB We operate a network of online trading platforms in over 40… 233 Thanks to Rahul Khanna . Olx iOS iOS App Development iOS Apps Architecture 233 claps 233 Written by Chapter Lead@OLX Group, working with lots of crazy engineers :) M: +91–9873265401 Linkedlin: https://www.linkedin.com/in/nishant-sharma-221bb911a We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Chapter Lead@OLX Group, working with lots of crazy engineers :) M: +91–9873265401 Linkedlin: https://www.linkedin.com/in/nishant-sharma-221bb911a We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-09-15"},
{"website": "Olx", "title": "the evolution of image model serving at olx part 1", "author": ["Alexey Grigorev"], "link": "https://tech.olx.com/the-evolution-of-image-model-serving-at-olx-part-1-e70f24624a92", "abstract": "About OLX Group Images contain a lot of important information, and being able to use this information is often crucial for an internet company — especially for an online classifieds platform such as OLX. It’s not that easy, however, for a computer to understand what’s on the image: we need to use machine learning and deep learning for that. Training a deep learning model is quite a difficult challenge on its own, but there’s even a more difficult step after that — using this model in production and applying it to millions of images daily. In this series of posts, we’ll tell you how we built a system for serving image deep learning models. In the first part, we cover: Extracting metadata from images Training models with AWS SageMaker Our initial infrastructure for serving image models Let’s start! Many millions of images are uploaded to OLX daily: 3 million images are uploaded to olx.pl , 1 million to olx.ua and 1 million to olx.in . In total, approximately 10 million new images are created every day across all OLX platforms. We use AWS S3 for storing all the images, and whenever somebody opens a page at OLX, we quickly retrieve and present them to the user. However, our knowledge about these images was quite limited: the only thing we knew about them was the listing they belonged to. That was often not enough: we wanted to know more. For example: How good are the images on our platform? What are the objects in these images? To be able to answer these questions, we created a special service: the metadata service. The purpose of this service is to store some extra information for each image. In the beginning, we started with two metadata types: Quality: if the quality of the image is good or not Category: the category of the item on the picture It’s not a surprise that we used machine learning for extracting this metadata: for each metadata type, we developed a model, and then simply stored the output of the model in the metadata service. Image quality is quite important for a classifieds platform: for a buyer, it’s important to see the image of an item before deciding whether to contact the seller or not. When images have poor quality, the buyers may decide not to pay attention to the listing and simply scroll it over. We treated an image as “good” if the item is properly positioned, the light is adequate and it’s possible to see the details. On the other hand, if an image is out of focus, there are problems with light or it’s not possible to see the item well, then the quality is bad. To determine whether a picture has good quality or not, we built a machine learning model — and this model became a part of our metadata service. We have already described how these models work, if you’re interested to learn more about them, please check these two articles: Qualifying Image Quality — Part 1, Cropped Images Qualifying Image Quality — Part 2 Another type of model we needed was an image classification model: we wanted to know what is the item in the picture. Like with image quality, we also used a machine learning model for image classification. This general classification model was another part of our metadata service. In recent years, training deep learning models has become a lot simpler than previously: with all the cloud services available today, in some cases training a model can be done with a few clicks. At OLX, we use AWS SageMaker for training our deep learning models. It’s fairly easy to use. Once we have a dataset, training a model with SageMaker requires a few steps: Prepare the dataset Upload the data to AWS S3 Run a SageMaker job Before using SageMaker, we used EC2 and pre-configured EC2 images: we ourselves were creating a fresh EC2 instance, configuring all the libraries and then saving it as AMI — an image that would later be used as a base for creating new EC2 instances. Overall, it was a nightmare to maintain. SageMaker makes it a lot simpler by providing ready-to-use Docker images with a pre-configured environment. Additionally, it’s quite easy to customize these images and add extra libraries that are missing in the pre-configured environment. To train a model, we need to specify the docker image we’d like to use, the location with the data and the instance type for executing the job. Once the job finishes, SageMaker saves all the job artifacts — including the model itself — to S3. Now comes the difficult part — serving this model. Training deep learning models is already not an easy task. However, once we have a model, the next step is even more difficult. How do we use the model for millions of images per day — in a reliable way? There are many ways of deploying deep learning models. The first option we came across was again AWS SageMaker: in addition to training, it can also serve models. With a few clicks (or python commands), we can roll out a SageMaker endpoint with the model and already start using it. That seemed the easiest way forward, that’s why we decided to use it for the first version of our metadata service. With this setup, the initial architecture of our metadata service was quite simple: The clients of the system communicate with the metadata service. The clients are typically other services that use our predictions — and not the end-users (buyers and sellers) of OLX The metadata service acts as a gateway: it gets an image ID and then talks to the other services to get predictions. Once it gets predictions, it saves them in the database. The model wrapper is a simple service that knows how to fetch images from S3, preprocess them and send them to the SageMaker endpoint for prediction. We quickly realized that SageMaker endpoints are quite expensive. Additionally, our SRE team wasn’t very happy about limited monitoring capabilities and poor integration with the existing infrastructure and tools. This is why we decided to stop using SageMaker for deploying models and started to use own Kubernetes cluster instead. Eventually, we replaced most of HTTP calls with queues: this way it’s easier to scale the system up and down, and gracefully react to peaks in the load. The flow became completely asynchronous: The user submits a request containing the image ID and the callback URL (optionally) The metadata service submits the request to the queue The model wrapper gets the request, processes it, and put the response to another queue The metadata service gets the results from the response queue, saves them to the database, and informs the user that the result is ready using the callback URL. In reality, we had two models, not one, so the overall architecture was a bit more complex. Also, for serving the models themselves we used special frameworks: for TensorFlow-based models, we used TF-Serving for MXNet models, we used MMS (MXNet Model Sevice) This architecture solved most of all our problems. Only one part was missing: how we could quickly react to new images and score them through the models in real-time? For that, for every new image, we needed to execute our flow. We could solve this problem by writing additional code in our image hosting service: every time an image is uploaded, we trigger the flow execution. This solution had a drawback: we didn’t want to couple our image hosting service with the metadata service. Accidentally, we discovered an alternative: S3 event notifications. Every time a file is added to a bucket, the bucket can send a notification event to an SQS queue. This is exactly what we used: when a new image is uploaded to Apollo, we get a notification and trigger the metadata extraction flow. For every ObjectCreated:Put event, we execute the metadata extraction flow By the time the user needs the predictions, they are already available in the metadata database We also used this idea for image deduplication: we listened for S3 event notifications and computed image hashes right after an image is uploaded: Detecting Image Duplicates at OLX Scale . With the metadata service, we could do a few useful things to help our customers. First of all, we use the image quality component to determine if some sellers should update images in their listings. If after some time a listing hasn’t received much attention, we trigger the quality analysis. During the analysis, we determine whether the quality of some images is poor, and in case it is, we send an email with specific suggestions — along with a general guideline on how good images should look. Additionally, we also detect cases when the seller didn’t choose the correct picture for their listing. For example, if a seller accidentally used the picture of the car’s internals as the first image, that image would be used as the cover image. When this happens, we can send a mail to the seller and suggest to re-order the images in the listing. This system is also quite useful for content moderation purposes. There are cases when sellers try to sell a forbidden or illegal item. Our models can detect that and alert the moderation team that they should take a closer look at the suspicious listing. Even though the architecture we designed worked well for many cases, after some time we realized that there are still some drawbacks. The asynchronous communication made it a lot easier to scale up and down and deal with surges during peak hours. However, that often resulted in a backlog in the queues. That’s not the best situation for cases when we need real-time predictions: since queues have the FIFO semantics, the old messages are processed first, and the new messages have to wait. This was a serious limitation, especially for moderation purposes: we can’t wait to catch up with the backlog only to find out that 15 minutes ago a listing with a gun went live on our platform. Additionally, the system became quite complex and it was easy for things to get out of control: there were too many moving parts and if something was falling apart, it was difficult to hold everything together. In our next post, we’ll talk about how we addressed these issues and redesigned our model serving platform. tech.olx.com To know more about our images, we can keep some metadata about them. To extract this metadata, use deep learning. AWS SageMaker is a great service for training models. We need to provide it with data, and it takes care of setting up the environment and training models. It’s easy to deploy models with AWS SageMaker, but we found it expensive and eventually replaced it with our own serving components deployed with Kubernetes. Making a system asynchronous makes it easier to scale the system up and down and gracefully react to peaks in the traffic. The easiest way of integrating the service with S3 is by subscribing by S3 event notifications. There are limitations in our design — especially our decision to do everything asynchronously sometimes resulted in delays in processing images. This post is partly based on the talk “Image classification at scale” presented at PyData Berlin Meetup on 20.03.2019 ( slides ). We operate a network of online trading platforms in over 40… 756 Thanks to Augusto César Dias . AWS Deep Learning Machine Learning TensorFlow 756 claps 756 Written by Lead Data Scientist at OLX Group. https://www.linkedin.com/in/agrigorev/ https://twitter.com/Al_Grigor We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Lead Data Scientist at OLX Group. https://www.linkedin.com/in/agrigorev/ https://twitter.com/Al_Grigor We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-04-24"},
{"website": "Olx", "title": "the evolution of image model serving at olx part 2", "author": ["Alexey Grigorev"], "link": "https://tech.olx.com/the-evolution-of-image-model-serving-at-olx-part-2-34a8a8aa85f0", "abstract": "About OLX Group The users of OLX upload tens of millions of images every day. At this scale, applying deep learning is quite challenging. This is the second post of the series. In the first post, we covered our metadata service, the process of training the models and our initial infrastructure for serving these models. tech.olx.com We concluded the previous post by saying that even though our system worked quite well in many cases, there were some drawbacks that we had to solve. In this post, we will briefly go over these drawbacks and then show how we addressed them. In particular, we will cover: Simplifying our architecture and making it synchronous Saving the predictions to AWS Athena to make the analysis easier Creating an MXNet Serving component Adding two new models: artificial text detection and NSFW detection Let’s first start with a recap of our previous post and the architecture of our metadata service. The purpose of the metadata service is to keep the metadata about each image we have on our platform. After a few iterations, this is the architecture of the service we came up with: First, the metadata service checks if we already have the results in the database. If it’s not the case, it adds the requests to the model queues. The model components (image category or image quality) get messages in batches. For each message, a component gets the image from S3, resizes it and does all the other required pre-processing steps. Then the pre-processed data is sent to the actual model. The model replies with predictions, which are put to the “results” queue. The metadata service pulls the results and saves them in the database. Once we process all the images, we inform the client that the results are ready. The client makes another GET request to fetch the predictions. There’s a second flow: we run the predictions for all the newly uploaded images. For that, we use S3 event notifications. While asynchronous design made it easier to autoscale, we felt that our system became unnecessarily complex and inconvenient. The asynchronous nature also became a serious limitation. In some cases, we couldn’t wait for the system to work through the backlog during peaks of traffic. In content moderation, if somebody is trying to sell a gun, even a minute of delay is catastrophic. We also weren’t happy about MMS — the solution we used for serving MXNet models. It accepts one image at a time per each request, but we wanted to process our images in batches to improve throughput. Let’s see how we addressed these problems. To work around these limitations, we decided to do a major redesign of our service. By then, our metadata service looked like this: For the first step, we took a look at each model component (image category and image quality) and started factoring out the common logic from these components into a new one. We started with the code for getting the images from our image hosting (based on AWS S3), and called the new component “X”: Now each image is retrieved from S3 only once and then sent to each service for pre-processing. With this change, the individual model services become redundant: keeping them only for pre-processing of images no longer makes sense. That’s why we moved this logic to X as well and removed the services altogether: At this point, we no longer needed the metadata service: X could do everything itself. So we could remove all the queues as well as the metadata service. For simplification, let’s also remove the queue with S3 Put events for now — we’ll bring it back later: Since we weren’t satisfied with MMS and wanted to change the way we serve MXNet models, let’s remove MMS for now from the diagram as well. “X” isn’t a very descriptive name. Because the purpose of this service was to serve image models, we called it “IMS”: Image Model Service. After moving things around and removing some of the components, this is what we were left with: It looks quite simple — and in retrospect, it seems this is what we should have had from the start. Of course, it’s not the final design and we need to add a few things to make it more useful. First, we need to cache the results: we don’t want to re-do calculations for the same image multiple times. Previously, we used a MySQL database for that, but it turned out problematic to maintain with our scale. This is why we decided to use a managed key-value storage for keeping the predictions — and chose AWS DynamoDB for that. When doing it, we discovered a nice feature in AWS S3: each file in S3 has an eTag — the MD5 hash of a file. This is great for dealing with duplicates: once we calculate predictions for a file, we can recognize all its duplicates and won’t need to repeat the calculations for them. So, when making a prediction, we: First, send a HEAD request to AWS S3 to get the eTag of an image Then, use the hash to look up the predictions in the cache If the results are already there, return them Otherwise, trigger the prediction pipeline and save the results to the cache at the end There was a small downside of this simplicity: our service became synchronous, so it was more difficult to configure it to scale up and down to deal with peaks of traffic. But eventually, we tuned it and started serving the models to all the production traffic. The clients of our service were quite happy: they no longer needed to worry about callbacks, they simply could send a request and get a response with predictions. However, we had a different type of users of our service: data analysts. This time we wanted to make it very easy for them to access the predictions. We used AWS Athena for that: for each image, we save the results to S3. Once the results are there, we can use SQL in Athena to query these results. This approach is scalable and easy to maintain: we can run analytical queries without touching the production databases and worrying about managing Athena ourselves. To have the results available in Athena as fast as possible, we used the same trick as previously: S3 Event Notifications . The idea is to trigger the prediction pipeline, get the results and then same them in S3, so analysts can access them. When a user uploads an image, the image hosting puts it to the S3 bucket with other images. Immediately after that, S3 sends an ObjectCreated:Put event to an SQS queue. We have a special component — “S3 Events Listener” — that gets messages from the queue and asks IMS to run predictions for these images. It does it in exactly the same way as other clients of IMS. The response from IMS is saved to a stream with results. We use AWS Kinesis for it. Eventually, the results from the stream are saved to an S3 bucket. When it happens, the results become queryable through Athena. With this setup, we could bring the delay between the time an image is uploaded to the time the predictions are available in Athena down to less than five minutes. Using a Kinesis stream gave a good side effect: now any other consumer could listen to the stream with predictions — and use these predictions without putting extra load on IMS. So far we talked about only about one model: the category prediction one which we deployed using TF-Serving: At some point, we needed to add another model. In some cases, OLX rules forbid putting contact information on images, that’s why we needed a model for detecting images with artificially added text. With our architecture, it was quite easy to add new models: Create a model for detecting artificial text in Keras Convert the model into the TF-Serving format Add a special class for pre-processing images in IMS And that was all! After a few days of work, we had two models: Not long after that, we needed to add a third one — this time, in MXNet. The third model we needed to add was an NSFW detector: it was detecting images with explicit nudity and other not safe for work content. This model was trained using Apache MXNet. We weren’t really happy about the MXNet Model Serving library: it could only accept one image at a time and there was no simple way of doing batching, so we needed to find a different solution. Unlike TensorFlow, MXNet’s core is quite lightweight, so, in principle, it’s possible to put it inside a web framework and serve models from there. The only concern was the data exchange format: how could we efficiently exchange data between IMS and the serving component? The first idea was to do it in the same way as TF-Serving and use protobuf, but it looked too complicated. The solution was simple: compressed NumPy arrays! Get a batch of images, pre-process them and convert them into a NumPy array Convert the array to binary representation and gzip it Send the gzipped representation to the serving component Uncompress and reconstruct the NumPy array, convert it to NDArray (a special array class from MXNet), apply the model, send back the predictions Once we had the MXNet serving component in place, it was quite easy to integrate this model: again, we only needed to add a special class for handling the pre-processing of images for this new model. With just a few steps, we could easily integrate a third model to IMS. This is the system we use now to serve our image models to tens of millions of images every day. Making our asynchronous system synchronous dramatically simplified the design, at the cost of being more difficult to tune to adjust for peaks in traffic. Each file in AWS S3 has an eTag, which is the MD5 hash of this file. We can avoid unnecessary calculation by using the eTag as the key when saving predictions to our database. AWS Athena is a scalable and easy to maintain way to analyze the predictions without affecting the production databases Saving the predictions to a Kinesis stream makes it easy to share the predictions with any interested consumer and takes some load off the main service. Because MXNet’s core is quite light, it’s possible to put it inside a web service and use compressed NumPy arrays as the data exchange format. Adding more models is quite simple: all we need to do is to add a special class for doing image pre-processing. We hope you enjoyed this series of posts about our system for serving deep learning models. Thank you for reading! We operate a network of online trading platforms in over 40… 276 Thanks to Augusto César Dias and JB Lorenzo . Technology Software Engineering Data Science Deep Learning Machine Learning 276 claps 276 Written by Lead Data Scientist at OLX Group. https://www.linkedin.com/in/agrigorev/ https://twitter.com/Al_Grigor We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Lead Data Scientist at OLX Group. https://www.linkedin.com/in/agrigorev/ https://twitter.com/Al_Grigor We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-04-24"},
{"website": "Olx", "title": "detecting image duplicates at olx scale", "author": ["Alexey Grigorev"], "link": "https://tech.olx.com/detecting-image-duplicates-at-olx-scale-7f59e4b6aef4", "abstract": "About OLX Group Duplicated content creates many issues in online platforms, and this is especially true for online classifieds, where duplicates require special attention. This is a second post of the duplicate detection series. Previously, we described the problems with duplicated content and suggested to use a content moderation for solving them. We also outlined a generic approach for candidate detection: the two-step framework. tech.olx.com In this post, we’ll talk about implementation details and we’ll focus on images. Building a system that can deal with processing millions of images daily is one of the most difficult parts of building a duplicate detection system, that’s why we cover this part in detail. In particular, this post talks about: Why images are important for detecting duplicates and how they can be used; Perceptual image hashes and how to use them for duplicate detection; Implementing a system for detecting image duplicates with AWS and Elasticsearch . Like in the previous post, we talk about a generic approach to duplicate detection, and the system described here doesn’t necessarily 100% reflect what we use at OLX. We do this on purpose, so this material cannot be used for bypassing our moderation system and harming our users. On the other hand, the post gives enough information to build your own duplicate detection system. Before we start, let’s quickly go over the main idea from the previous post: the two-step framework for duplicate detection. The framework consists of two steps: candidate selection step and candidate scoring step. Candidate selection step. At this step, we find candidate duplicates: listings that are likely to be duplicates. For that we use domain knowledge and look at listings published in the same category, from the same city; and listings published by the same author, from the same IP or the same device. Candidate scoring step. At this step, we use machine learning for scoring the candidates from the previous step to get the actual duplicates. We covered simple features that we can use: the basic features like the difference in price, geographical distance and whether two listings are published from the same IP; and more complex features that involve calculating text similarity between the two titles or the two descriptions. With these basic features and text similarities, we can build a model that does quite well for the second step. However, we didn’t talk about one particular type of content: images. In online classifieds, this is one of the most important aspects of a listing, which is why we need to use them in the duplicate detection model. We’ll see how to do it in the next section. Images play an important role in online classifieds, so we need to extract the information we have there in order to have a good model. One of the possible ways of doing it is by using image hashes: we extract hashes from images and use them for determining how similar two images are. There are two types of hashes for images: cryptographic and perceptual. The cryptographic hashes, like MD5, are general-purpose hashes: they work for all files, not just images. They are typically used for detecting if a file was manipulated with or not, so hashes like MD5 are used as a checksum. Even the smallest modification in the file — for example, a one-bit change — results in a completely different hash. For example, we can take an image of a car and compute its MD5 hash: If we modify it slightly by adding a small white rectangle at the corner, we’ll have a completely different MD5 hash: Using MD5 hashes for duplicate detection is possible, and quite beneficial, but as we see, it only tells us if two ads have exactly the same byte-by-byte images — and nothing more. Perceptual hashes are different: they are designed specifically for images. When an image is modified slightly, the hash shouldn’t change significantly, and often it won’t change at all. There are a few such hashes: ahash, dhash, phash, and whash. You can read more about these hashes here . If we take the same images as previously and calculate the dhashes for them, they’ll be the same for both images: In this particular case, dhash isn’t sensitive to adding a small white rectangle in the corner, so the resulting hash doesn’t change. There are other modifications that shouldn’t affect the hash: Resize of the image; Saving in a different format, e.g. JPEG with a different compression level or WEBP; Small modifications of the image, e.g. adding invisible noise, or a small watermark. Unfortunately, there are some modifications that break these simple hashes. For example, cropping a part of the image, rotating the image, or reflecting it does affect the hash. For catching such modifications other methods, like neural networks, should be used. The basic perceptual hashes are quite simple to implement. Let’s take a look at how to compute dhash. Suppose we want to compute a 64-bit hash. For that, we first need to resize the image to the size of 8x9 pixels. What it effectively does is dividing the image into 8x9 blocks and calculating the average pixel value in the block: Next, we take each column of this array and subtract it from the column next to it. That is, we calculate the difference between the 2nd column and the 1st, the 3rd and 2nd, 4th and 3rd and so on. After doing that, we have an 8x8 array with differences: Now we look only at the sign of the difference: for each value, if it’s positive, we replace it with TRUE, and if it’s negative, we replace it with FALSE. This way we get an 8x8 array with boolean values: Finally, we convert it into ones and zeros and treat each row of the array as an 8-bit integer. The first row is “10010100”, which is 148 when converted from binary to decimal, or “94” in hex. We repeat it for each row, and this way, we get eight 8-bit numbers in hex. Finally, we put all the hexes together to get the final hash: in this case, it’s “94088af86c038327”. It’s a heuristic, but it’s quite simple and efficient to compute. Other hashes, like phash, work similarly, but phash first applies the Fourier transformation to the image, and whash uses wavelets. When we have hashes, we can use them for comparing the images of two listings. To check how different two hashes are, we can calculate the number of positions where the bits of the hashes don’t match. This is called “hamming distance” — the number of different bits in two binary arrays. We can compute the Hamming distances between all the images and this will give us a set of good features to a machine learning model. For example, suppose we have two hashes, “94088af86c038327” and “94088af86c038328”. These hashes are different only in the last character: it’s “7” for the first hash and “8” for the second. The hamming distance between these hashes is 4 bits: when we compare the binary representation of the hashes, only 4 bits are different: This is a very useful feature for the machine learning model, but hashes are also quite useful for the first step: the candidate selection step. If two things have the same or similar image, they are excellent duplicate candidates and we definitely need to include such listings to the candidate set. In the next section, we will see how to implement it. Let’s consider one part of the duplicate detection system: the part that computes hashes for all the images and, given an image, lets us find all other ads that contain this image. It’s quite a complex part of the flow, that’s why we cover it in detail. We call it “image index”. The image index is a system that indexes hashes of images. It’s a web service that can answer the following questions: Given the image, what are the other images that have the same hash? What are the images that have a similar hash? There are many possible options to choose from when deciding for the storage layer of this system: traditional RDBs like MySQL or PostgreSQL, key-value storages like Redis or DynamoDB, or full-text search engines like Elasticsearch. We believe that for this system Elasticsearch is the best option. Elasticsearch uses Lucene, which is an implementation of inverted index — a special data structure often used for search. In direct index, we provide an ID of a document and get back the content of this document. For example, retrieving the stored hash of an image by its ID is a lookup in direct index: we give the ID and get back the hash. The inverted index does the opposite: given the hash, we can find all the images that have this hash. This is exactly what we need for our system! In addition, on top of being able to find images by exact hash match, Elasticsearch also allows finding images with similar hash. To calculate the hashes and ingest them into Elasticsearch, we can use a simple pipeline: First, an image is uploaded to our internal image hosting. The hosting uses AWS S3 as the storage: all the images are stored there. In S3 it’s possible to subscribe to all the upload events: every time a file is put to the S3 bucket, it can create an event and send it to a specified queue. We can simply listen to such events by consuming from this queue. AWS Lambda is a great option for processing the events from the queue. The lambda gets the newly uploaded image from the bucket and computes all the hashes: MD5 and all the perceptual hashes. After that, the hashes go to another queue, and, eventually, to Elasticsearch: The reason we need to use two queues instead of one is to make sure we don’t overload Elasticsearch with inserts when there are sudden spikes in traffic. By adding another queue in the pipeline, we can control the pace of ingesting data and make sure we don’t kill our database with writes. Using AWS Lambda for calculating hashes is a great option with many advantages: it takes care of scaling up and down automatically — with no involvement from the engineering team at all. This way we can process up to 10 million images per day, which peaks at 230 images per second. In the previous post , we covered the content moderation system and described some of its components, including the duplicate detection system. With Image Index, the entire moderation flow may look this way: First, a seller creates a listing; all images are uploaded to S3; The images go through the Image Index pipeline, we calculate hashes and put them to Elasticsearch; When the listing reaches the automatic moderation system, we already have all the hashes computed and stored in Image Index; The automatic moderation system via the duplicate detection component talks to Image Index (and other components) to get duplicate candidates and scores them; The system makes a decision to accept or reject the listing, and in cases when the system is not sure, the listing goes to manual moderation; The moderators make the final decision and the decision is stored in logs — to be able to feed it back into the system for improving the quality of predictions in the future. The easiest way to incorporate the information from images to the model is by using hashes. The hamming distance between two perceptual hashes tells us how different two images are. Image hashes can also be useful at the candidate selection step: if two listings share the same image, they are quite likely to be duplicates. We can use Elasticsearch to build a system for quickly finding images with the same hash: Elasticsearch uses inverted index, which is quite suitable for this type of query. To build the hash calculation and ingestion pipeline, we can listen to S3 events. This way, every time an image is put to S3, we immediately calculate its hashes and put them to our storage. AWS Lambda is a great platform for deploying the hash calculation component: it scales up and down with no involvement from the development team. We should save the decisions made by moderators to feedback into the system in order to improve its performance in the future. We hope you find this article useful and can now apply the knowledge to get started on building your own duplicate detection system. This post is based on the materials from the talk “Fighting fraud: finding duplicates at scale”, presented at PyData Berlin 2019 and Highload++ 2019 in Moscow ( slides ). If you liked the story, please help to spread the word about it. We operate a network of online trading platforms in over 40… 304 Thanks to JB Lorenzo . Machine Learning AWS Elasticsearch Duplicate Content 304 claps 304 Written by Lead Data Scientist at OLX Group. https://www.linkedin.com/in/agrigorev/ https://twitter.com/Al_Grigor We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Lead Data Scientist at OLX Group. https://www.linkedin.com/in/agrigorev/ https://twitter.com/Al_Grigor We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-02-07"},
{"website": "Olx", "title": "a two step framework for duplicate detection", "author": ["Alexey Grigorev"], "link": "https://tech.olx.com/a-two-step-framework-for-duplicate-detection-fbbe4c905480", "abstract": "About OLX Group Duplicated content is often an issue in many online platforms, especially in classifieds, where it requires special attention. Duplicates cause many problems, including spam and fraud. In the first case, users create multiple copies of a listing in order to get more attention and increase their chances of selling an item. This spams the platform and makes it more difficult for other users to browse through the catalog of items. The second problem is more serious: fraudsters can create fake listings by copying existing content and trick honest users into giving away their money. In this post, we propose a way to address this problem. In particular, this post covers: User-generated content, the problems it causes, and how content moderation systems can solve these problems; The two-step framework for approaching the duplicate detection problem; Using simple heuristics for finding duplicate candidates at the first step of the framework; Using machine learning to find actual duplicates as the second step. Let’s start. User-generated content is the content created by the users of a website. For example, posts on a social network (e.g. Facebook, Twitter), questions and answers on online forums or listings in online classifieds — all fall into the category of user-generated content. When uncontrolled, user-generated content may cause many problems. For example, consider a case of online classifieds: a user wants to sell an item and they create a listing. However, the listing doesn’t get any attention, so they decide to create the same listing multiple times. Or people may try to sell something that is illegal, inappropriate or violating the terms of service: drugs, guns and other prohibited content. In all these cases such listings should not be allowed to go live. When users create duplicates — multiple listings of the same item — it spams the platform and makes it difficult for other people to look for items on the platform. But apart from being annoying, duplicates can also be harmful. Consider the following scenario: a seller wants to sell a cellphone, so they go to an online classifieds website and create a listing. In the meantime, a fraudster creates a copy of the listing, posing as a genuine seller. However, when contacted by buyers, the fraudster asks for a deposit and then disappears with the money. To solve this problem, we should be more selective about what we allow people to post on the website. For that, we need to introduce a content moderation step: when published, the content should first go through a moderation system and only then go live. Overall, a content moderation system may look like the following: When a user posts a listing, it first goes to the automatic moderation system. This system makes an automatic decision whether to accept the listing and let it go live, or reject it. In cases when the system is not sure, it can ask people: the moderators. In such cases, the listing goes to the moderation panel where moderators review the content of the listing and make the final decision. The automatic moderation system may consist of many components, and typically some of these components use machine learning when making their decisions. The duplicate detection system may be one of such components — and this is the component we will cover in this post. Let’s start with the theoretical foundation for building such a system: the duplicate detection framework. One of the possible ways of approaching the duplicate detection problem is a simple framework with two steps: Candidate selections steps. This is the step where we try to find the most likely duplicate candidates. Candidate scoring steps. At this step, we get the candidates and use machine learning to select the listings that are real duplicates. Let’s look at each step in more detail. The first step in the duplicate detection framework is the candidate selection step. When we want to detect if a listing has duplicates, we need to compare it with all the other listings from our database. Typically there are quite a few records there, so it’s simply not feasible to compare the listing with everything that we have. For example, imagine that our database contains only 1000 records. If we want to detect duplicates in this database, we’ll need to compare every record with all the others, resulting in almost half a million comparisons. Doing something like that for olx.in , olx.pl or olx.ua is simply impossible — there are many millions of active listings on these platforms! To make it more efficient, we need to narrow down the set of listings for comparison to only the most likely candidate duplicates. There are many ways to narrow it down. For that we can use: Heuristics based on domain knowledge; Information retrieval techniques; Approximate KNN techniques. While information retrieval and approximate KNN each deserve a blog post of their own, the heuristics are quite simple: we use our domain knowledge and common sense to come up with the best set of candidates. For example, consider a listing from olx.in : We can use the information about the listing to find candidate duplicates: for example, we can select other ads in the same category and location (city/district). Also, we can use the information about the seller and check the other listings from the same seller; or the listings posted from the same device or from the same IP address. Once we identify the possible duplicate candidates, we can move to the next step: candidate scoring. At the candidate scoring step, we take all the candidate duplicates we identified previously, and score them to select the listings that are actual duplicates. For that we can use machine learning: we can build a model that takes in a pair of listings and outputs the probability that the listings are duplicates. If the probability is above a certain threshold, we treat the pair as a real duplicate. To be able to use machine learning we need data. We already have the candidates from the previous step, but we’re missing one important part: the labels. That is, for each pair we need to know if it’s a duplicate or not. Typically, to get this information, we need to involve people in the process and ask them. So we can present each pair we have and ask our annotators to label them as “Duplicate” or “Not duplicate”. If there’s already a content moderation tool, then we can use the moderation panel and ask our moderators to help us. Alternatively, it’s possible to use crowdsourcing platforms — like Amazon Mechanical Turk or Yandex Toloka — for getting this data. Once we collected the dataset, we can start using machine learning. First, we encode the target, so “Duplicate” becomes “1” and “Not duplicate” becomes “0”. Then the next step is doing feature engineering: we create features that capture the degree of similarity between the listings of a pair. Once we have the features, we can train a model! What features can we use? We can start with the simplest ones, like The absolute and relative difference in price between the listings; The geographical distance between the two listings; Whether the listings were published from the same IP; Whether the listings are in the same category; And so on. Using simple features like that is already good enough and can give a relatively good baseline. When Avito (the largest Russian classifieds website) ran their Duplicate Ad Detection competition on Kaggle, the first baselines contained only simple features like the ones above. The textual content of two listings is quite important when determining if two listings are duplicates. That’s why we need to add some features that capture the similarity between titles or descriptions. For example, The cosine similarity between titles ( Bag of words ); The cosine similarity between descriptions ( Bag of words ); Word2Vec similarity between titles; between descriptions. “Bag of words” is the simplest approach that lets us encode any text as a vector. To do it, we first take all possible words that we have in all our texts and then create a large table. The column of this table are the words, and the rows are the texts. If a text contains a word, the corresponding cell gets the number of times the word occurs in the text. This gives us a simple way of encoding texts as vectors. Once we have vectors, we can measure how similar they are using the angle between these two vectors. This is exactly what cosine similarity gives us: it’s the cosine of the angle between two document vectors. It’s 0 if two documents are not similar at all — they have no words in common, and it’s 1 if two vectors are exactly the same. With the basic features and text similarities, we can already get quite a good model. When it comes to online classifieds, we must also include images: this is one of the most important aspects of a listing. The easiest way of doing it is by using perceptual image hashes. We cover image hashes in the next post of the series. In this post, you’ll learn how perceptual hashes work and how to implement a system for detecting duplicates based on hashes. tech.olx.com Having a content moderation system is important for making sure the content quality on the platform is up to high standards. A duplicate detection system is one of the many components of the content moderation system. To approach the problem of finding duplicates, use the two-step framework: first, detect candidates and then narrow down the candidates to the actual duplicates. In the first step, we find candidates using domain knowledge and simple heuristics. In the second step, we use machine learning to find the actual duplicates among the candidates. Simple features such as the difference in price and distance and whether the listings were published from the same IP or device are already good enough for a start. Adding text similarity features based on a bag of words will further help model performance. We hope you find this article useful and can now apply the knowledge to get started on building your own duplicate detection system. This post is based on the materials from the talk “Duplicates… Duplicates everywhere”, presented at AI tonight meetup in Kiev and Berlin Machine Learning meetup ( slides ). If you liked the story, please help to spread the word about it. We operate a network of online trading platforms in over 40… 240 Thanks to Andreas Merentitis and JB Lorenzo . Machine Learning AWS Elasticsearch Duplicate Content Moderation 240 claps 240 Written by Lead Data Scientist at OLX Group. https://www.linkedin.com/in/agrigorev/ https://twitter.com/Al_Grigor We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Lead Data Scientist at OLX Group. https://www.linkedin.com/in/agrigorev/ https://twitter.com/Al_Grigor We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-04-20"},
{"website": "Olx", "title": "modular architecture in ios", "author": ["Oleh Kudinov"], "link": "https://tech.olx.com/modular-architecture-in-ios-c1a1e3bff8e9", "abstract": "About OLX Group Modular Architecture is very popular topic in software engineering. As monolith application grows it becomes less and less maintainable and there is the need to split it in separate modules. On backend it is microservices and in Web is micro frontends . In this article we show how it works in iOS . In the previous article , we have seen how to create an App using Clean Architecture + MVVM . Here we show how to improve your project by decoupling your app into isolated modules (e.g. NetworkingService, TrackingService, ChatFeature, PaymentsFeature… ). Module isolations can help teams to work with these modules rapidly and independently. Monolith is not bad when we are using good architecture like Clean Architecture + MVVM . Still, the app can become too big to be a monolith. And there is a need to make faster builds, reusable modules as frameworks, and isolated modules so people can work easily in separate cross-functional teams. Big companies usually have their apps separated into dozens of modules. This year we managed to split our multi-millions users’ monolith OLX app into 12 modules. And now we always create a new module for new big enough features. We call it Modular Architecture and it is also applicable to other platforms(e.g. Android or cross-platform like ReactNative or Flutter). In this architecture the App is separated into totally isolated Features modules which depend on Shared and Core modules. In this article, we show how to separate the monolith App into isolated modules: Networking Service and Movies Search Feature . Networking Service module is configured inside the App and injected into Movies Search Feature : And at the end of the article we will see how it can be scaled to big size, where every module has its own Clean Architecture + MVVM, Domain and DIContainer : Note : detail description of this graph is below in the section: How Modular Architecture Scale Modular Programming is a software design technique that emphasizes separating the functionality of a program into independent, interchangeable modules, such that each contains everything necessary to execute only one aspect of the desired functionality. We use here CocoaPods as a dependency manager to split the App in isolated modules. CocoaPods is a powerful dependency management system that makes the frameworks integration very easy and convenient. Other dependency managers which we could use are Swift Package Manager or Carthage. We create a new feature module when the feature is large enough to be called or considered as a product on its OWN. And it can be developed in isolation by cross-functional team. Every module will have its own Architecture. Each team decides which architecture fits the best for their module to develop(e.g. Clean Architecture +MVVM , Redux..) and Domain. It means that all Domain Entities of the module will be fetched from API inside this module and mapped here. Note : when we want to share some Domain Entities like User we create CommonDomain module. Every isolated module feature will have its own Dependency Injection Container to have one entry point where we can see all dependencies and injections of the module. Every module will have an example/demo project to develop it in isolation fastly without compiling the whole app. Every module's Tests also will be running fast without the host app. They will be with the module source code. Important to mention that the modules are local , which means that they are located in the same repo( Monorepo ) as the main App , inside the folder with the name DevPods . We only make them remote, in a separate repo, when we want to share them with another project. This makes sense until a new second project needs to use this module then it can be very easily moved to its own repo. As a general rule, we try to minimize the use of 3rd part framework in this Architecture too. If a module needs to depend on 3rd party framework, we try to isolate this use by using wrappers and limit the use of this dependency only to one module. We explain this on an example inside the section bellow: Initial App Scaling -Adding Authentication Module with 3rd party framework Note: In this article module and framework have equivalent meaning. because the way of creating a new module in Xcode is by creating a new framework. And DIContrainer — is a Dependency Injection Container. Example App of a feature module — means it is Demo App of this feature module and it is used to develop the feature. Builds times are faster. Changing one feature module will not affect other modules. And the app will compile faster by recompiling only the changed module. Compiling a big monolith app is much much slower than compiling isolating part(both clean and incremental builds). For example, our clean build time of all app is: 4 minutes vs Payments module example/demo app: 1 minute. Development time is faster. The improvement is not only in compiling speed but also in accessing the screen that we are developing. For example, during module development from example project, you can easily open this screen as the initial screen in the app launch. You do not need to get through all the screens as it happens usually when we are developing in the main app. Isolation of Change . When developing in modules there is clear responsibility of the area of code in the project, and when doing merge requests it is easy to see what module is affected. Tests running in seconds because they still will run without host app, they will be in Pod of the module. Modules can depend on each other(without circular dependencies ) and on 3rd party frameworks. (e.g dependency A<->B is not allowed) A ->B->C means that module A which imports B will have access also to C When creating a new module and this module depends on other feature that was not yet extracted into a separate module, we delegate this functionality to the main App using delegation or closures. For example, if the Delivery module needs to show chat for a user, we can create delegate func openChat(withUserId:itemId:onView:) inside Delivery module, and implement it inside main App and inject it into Delivery module. On the other hand , if the Feature already exists in a separate module, we just configure it inside the main App and injected it into the module which we are separating. Here we will separate the monolith App into Service and Feature, totally isolated modules. The monolith App is in this repo . Before moving Movie Search Feature into a module, first, we need to move Networking Service into a separate module, because from this feature we will need to fetch movie items using Networking Service. It will be configured with the base URL and API key inside App and injected into the Movie Search Feature module(using DIContainer) . First, we set up our project with pod init and pod install commands. Then we create the folder with the name DevPods inside the project folder. And inside this folder, we run pod lib create Networking command, which will create the module with the example project which is used to develop this module. After moving all module's code from the main app and configuring Podfile and module . podspec file (and running pod install ), we have as a result an additional schema Networking-Example inside the main App workspace, which is used to develop this Networking module. Now Networking can be developed separately by selecting schema: Here we can see how module files are automatically copied into the app workspace Pod project by CocoaPods when command pod install is run: Important : Detailed explanation of module creation can be found here: Steps for module creation with videos . Steps can be linked from readme.md file so every developer could easily create a new module. Note: In case if we want now to share this Networking with more people and between many different projects is easy to share it in remote repository: https://github.com/kudoleh/SENetworking Same as we did for Networking service, we run pod lib create MoviesSearch inside DevPods folder, which creates a new module with an example project to develop this module. We move all module related files into this module. After moving all module’s code from the main app and configuring Podfile and module . podspec file (and running pod install ), we have as a result an additional schema MoviesSearch-Example inside the main App workspace, which is used to develop this module(on next picture). Note : this feature already has Clean Architecture and DIContainer, if not we would add it to the feature before moving it. Now Movies search can be developed separately by selecting schema: Here we can see how module files are automatically copied into the app workspace Pod project by CocoaPods when command pod install is run: Inside App DIContainer we configure Networking with base URL and API key, and inject it inside Movies Search module: Important : Detailed explanation of module creation can be found here: Steps for module creation with videos . Steps can be linked from readme.md file so every developer could easily create a new module. Note: modules are local. They are located in the same repo( Monorepo ) as the main app because we do not share this module yet with another project. Localizable.string can be located inside the main app. All modules by default use the main apps bundle where these translations are located. If they are needed during development for an example project of a module, we can reference this file. It is nice to see at one point what dependencies have a module and what function method it provides(to provide a front-facing interface, Facade pattern). We create Module struct and move here module Dependencies. Project Source : https://github.com/kudoleh/iOS-Modular-Architecture . Nowadays many apps need to authenticate. So we add here Authentication Module. Here we will see how we can use 3rd party framework( Alamofire for authentication ) from our local module(Authentication), and how we can limit the use of this 3rd party framework to one module only. In Authentication.podspec file we add s.dependency 'Alamofire' The Authentication module contains code to do requests through Alamofire( SessionManager ) which provides the functionality of authentification . (It has RequestAdapter and RequestRetrier where we add access token and refresh it) Note : For not exposing the use of 3rd party framework(Alamofire) to other modules or to the main app we create AuthNetworkRequest wrapper . It is a wrapper around DataRequest(Alamofire class). Because when another module calls the func authRequest(:completion:) -> DataRequest , it depends on DataRequest class from Alamofire. This way we limit dependency on Alamofire to one module only( Authentication ). Also, we omitted here the implementation of Authentication configuration(with auth base URL) and delegating user authorization to the main app(when login is needed). In the heart of Networking module, we use Network Session Manager protocol to request data. We will make Authentication module(Auth Networking Session Manager) to conform to this protocol : All requests are adapted inside the Authentication module, the token is added there, and if it fails it refresh the token and retries the request. This way Networking module does not know anything about Authentication. It makes sense because adding token and refreshing it, is an Authentication module concern. And no other module in the system should know about it (no Networking service, neither feature module). Authentication Conformance to Networking and injection into Networking: Note : In order to not depend from Authentication module on Networking module, we make AuthNetworkSessionManager protocol to conform to NetworkingSessionManager and we inject it into NetworkingService. Project Source : https://github.com/kudoleh/iOS-Modular-Architecture Here we describe an example of how modular architecture can scale: In this graph we have the following modules: Configuration module : Contains configuration for the whole app. Parameters like base URLs and feature flags for every country and every environment (stage or production) are stored in here plist. For example, the main app uses the base URL from the Configuration module to setup Networking Service and inject it into all features and services. It is used also in every feature module example to easily configure services used by the feature. Networking Service: Simple wrapper around Network API request, it is configurable with parameters(e.g. base URL). It also maps decodable data. Authentication Service : Authenticates all networking requests by adding and refreshing an access token. It depends on 3rd party framework Alamofire . Note : No other feature module has a dependency on this module, only the main App to configure it with the auth URL and inject it into Networking Service, by conforming to protocols of Networking. From all feature modules we only neet to use Networking Service to request data. Tracking Service : Tracking service that makes tracking easy from all modules. It exposes only one func track(event:attributes:) . Inside the module, it has the implementation of all trackings. As the Networking module, it is configured in the main app and injected in all modules. Account Feature : Management of User Account (e.g. sign up, log in or update passwords). Utils module : To share common utility functions and extensions. UIComponents module : to share Common UI components and extensions related to UI. Themes module : To share common images, colours and font. Note : For simplification, it can be part of UIComponents module to have less modules. Feature module: Module with big enough feature and its own Clean Architecture, Domain and DIContainer . It depends on Networking Service to fetch data from the network and map it into domain entities. A feature can depend on other features. For example, we might need to open chat with a user from the Delivery module. (section above: Module Dependencies rules ) Common Domain (Core) module : To share common Use Cases and Domain Entities (e.g User entity with UserId and it's func isLogged, Money entity with its operations). Due to the fact that this module contains also Repositories implementation used by the Use Cases, it needs to depend on Networking module. It also can depend on Cache (3rd party framework to cache items) or on images cache Kingfisher used by ImagesRepo. Also it can depend on Utility module with common functionality. Note : When we have two Apps, we can share between them shared modules (e.g. Networking, Tracking, Utils, UIComponents or shared feature). https://github.com/kudoleh/iOS-Modular-Architecture App modularisation is successfully used at fintech company Revolut with >70 iOS engineers. It is used to break down our monolith app into totally isolated parts. It makes our development easier and faster, teams can work rapidly and independently Even if your app is not big yet, it can become very big fast in the near future and it is much easier to start the app modularisation already now. We create a module for big enough features. With its own domain and architecture(e.g. Clean Architecture +MVVM or Redux ). We isolate the use of 3rd party frameworks as much as we can, so there is no change in other modules when framework changes. The modular architecture is equally applicable regardless of dependency management tools or platforms (e.g. Android or cross-platform like ReactNative or Flutter ). Pros of using modular architecture: Faster build times (both clean and incremental) Faster locating code in domain-related logic inside a module Good decoupling and separation of concerns Easier to do code reviews . It is easy to see what changed in an isolated module Creating components separately is faster, also testing them in an example app and later integrating with the main app Easier for new developers to onboard and create new teams that will work in an isolated module domain It is important to keep all modules' example projects always buildable. We can use tools like Travis CI + Fastlane Every isolated module feature have its own Dependency Injection Container and Module Dependencies to see all dependencies it needs in one point Module can be built and shared as compiled framework Unit Tests of a module are running super fast because they run without the host app (no simulator needed), and they are inside the module’s pod We operate a network of online trading platforms in over 40… 946 10 Thanks to JB Lorenzo . iOS Modular Architecture Swift Software Architecture Clean Architecture 946 claps 946 10 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-11-15"},
{"website": "Olx", "title": "refactoring your code across packages repositories", "author": ["Ivan Yarych"], "link": "https://tech.olx.com/refactoring-your-code-across-packages-repositories-194b4ec69c71", "abstract": "About OLX Group Modern IDEs allow you to do lots of refactoring regularly. It’s so easy that you can always make rather big changes to your code and IDE will just handle it for you. But that’s all true when you refactor inside a single project/repository. And what you do when your project spans across multiple GIT repositories? A new feature for our Node JS project was going nicely thanks to Domain Driven Design approach. Also IDE plays big role in that because of all kind of refactoring actions that make all changes a breeze. One day though that changed. Our Domain was nice but need arose to reuse it across different services. The most obvious option was to move the domain to separate npm package and import it in services that need it. And this is where we started to have trouble. So no more fun: renaming method name and all it’s usages is no more automatic. You need to handle it some other way. But there seems no easy way to do that. PhpStorm (which is basically WebStorm with DB support, who needs PHP anyway?) can still find all the usages of the method, but it won’t allow you rename it. The only option you have is to do all the renames manually. And synchronize that between different packages… Have you encountered such a situation? Do you have some advice how to deal with that? Ideally IDE should be able to allow you to do some refactoring in such a case. I would think about renaming a method in class like a column rename in DB table. In code it would be probably very similar: Rename a method in package, but leave the old method’s signature intact and make it only call a new one Mark old method deprecated and state which method you need to use In service that uses your package IDE should allow you to do the switch between methods with the same signature — so that you can switch all your uses to new method. Remove old method in package when nobody is using it anymore (that’s more or less easy when you are in charge on all the repositories) That way it would be really useful and still shouldn’t be very hard to implement. What do you think? Here is an example project with all the steps. Main project: Package: If you were in the same project you would just rename the method name in place and all it’s usages. But if you are in different packages you would do everything in steps. Add new method, copy the content of old one there, make a deprecated alias for the old method name: 2. Replace all the usages of the method in main project: 3. Remove the old deprecated method: In this particular case it’s easy to replace the usage manually. But it’s of course not always like this and IDE’s ability to do so would be really helpful. If we go further we can add more of that kind of refactoring, i. e.: moving a file/class in directory structure — again make a “symlink” from the old one to new one and later remove change method signature — add an override to method which will support new and old signatures, change client code, remove And list can go on. The key thing here is ability to switch implementations. Is that possible? It would be really nice to have in IDEs. Please let me know what do you think about this approach or maybe there is different option that I am not aware of? About me: I am a Software Engineer working on Fixly ’s backend using TypeScript and Domain Driven Design approach. We operate a network of online trading platforms in over 40… Thanks to JB Lorenzo . JavaScript Typescript Ddd Refactoring Written by Senior Software Engineer, NodeJS/TypeScript, @Fixly We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Software Engineer, NodeJS/TypeScript, @Fixly We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-12-06"},
{"website": "Olx", "title": "improving solr performance", "author": ["Yael Lorenzo"], "link": "https://tech.olx.com/improving-solr-performance-f4202d28b72d", "abstract": "About OLX Group This is a short story of how we managed to overcome the stability and performance issues of our Search and Relevance stack. I’ve had the great pleasure of working with the Personalization and Relevance Team during the last 10 months. We are in charge of providing “personalized and relevant content to the user” based on rankings and machine learning. We do it through a set of microservices that provide three public endpoints, the Home Feed, Search and Related items API. I remember that a few months after joining the team the next challenge was to be able to provide excellent services for larger key countries. The goal was to keep it with the flawless performance and stability we already had with smaller countries. We are using SolrCloud (v 7.7) within AWS on Openshift using Zookeeper. At the time of writing, we are proud to mention that the API serves ~150K requests per minute and sends ~210K updates per hour to Solr in our largest region. After deploying Solr in our largest Market we then had to test it. We did it with an internal tool for stress tests and we could roughly get the desired traffic. We trusted that Solr was well configured so that the team worked on improving performance on the clients and setting higher timeouts against Solr. Finally we agreed we could handle the traffic with some looseness. The services were responding with acceptable response times, Solr clients were doing pretty good until they started opening some circuit breakers due to timeouts. The timeouts were generated by apparently random issues with Solr replicas taking too long to respond and these problems became more often affecting the front-end clients without information to show. Below are some of the problems we faced: High ratio of replicas going into recovering and taking too long to recover Errors in replicas not reaching the leader because they were too busy Leaders being under too much load (both from index, queries and replicas syncing) which prevented them from functioning correctly and led to shard crashes Doubts on the “Index / Update Service” because reducing its traffic to Solr stops the replicas going down or into recovering mode Full garbage collector running often (old and young generation). SearchExecutor thread running on top of CPU, as well as Garbage Collector SearchExecutor thread throwing exception on caches warm up (LRUCache.warm) Response Time increasing from ~30 ms to ~1500 ms Finding that IOPS reaches 100% on some Solr EBS volumes As part of the analysis, we came out with the following topics Apache Solr is a widely used search and ranking engine, very well thought and designed with Lucene under the hood (shared also with ElasticSearch). Lucene is the engine behind all the calculation and makes the magic for rankings and Faceting. Is it possible to do the math for Lucene and check the settings? I can share an approximate result based on a lot of documentation and forum readings, however its configuration is not as heavy as math for Solr. It is possible to tune Lucene, only if you are willing to sacrifice the structure of your document. Is it really worth the effort? No, you will find more info as you read further. Let’s say we have around 10 M documents. And let’s say the average document size is 2 kb. Initially your disk space is going to take at least this: Having multiple shards for one collection does not necessarily result in a more resilient Solr. By the time we have an issue with one shard and the other shards could respond anyway, the time response or blocker would be the slowest shard. When we have multiple shards we divide the total number of documents by the shard count. This reduces the cache and disk size and improves index process. Is it possible that we have an overkill index / update process? Given the result of our experience, it is not overkilling. I will leave the analysis of this question for another post. Otherwise this is going to be too extensive. We have reached ~210K updates per hour (peak traffic) on our key markets. The only job Apache Zookeeper has in this environment is keeping the cluster state available for all the nodes as accurate as possible. One common issue if the replicas are recovering too frequently is that the cluster state might be out of sync from Zookeeper . This will produce inconsistent states among the running replicas and the one trying to recover ends up in a long loop that might last hours. Zookeeper is very stable and it may fail only due to network resources, or better said the lack of it. One of the most driving factors for Solr performance is RAM. Solr requires sufficient memory for the Java Heap and the free memory for the OS disk cache. It is strongly recommended that Solr runs on a 64-bit Java because 32-bit Java is limited to a 2GB heap, which can result in artificial limitations that don’t exist with a larger heap (discussed in a later section of this post). Let’s take a quick look at how Solr uses memory. First of all, Solr works with two types of memory: heap memory and direct memory . Direct memory is used to cache blocks read from the file system (similar to file system cache in Linux). Solr uses direct memory to cache data read from disks, mostly index, to improve performance. As it is exposed, a large portion of heap memory is used by multiple caches. The JVM heap size needs to match Solr heap requirement estimation plus more for buffering purposes. This difference of heap and OS memory setup gives the environment some room to accommodate sporadic memory usage spike, such as background merging or expensive queries, and allows JVM to perform GC efficiently. For example, setting 18Gb of heap in a 28Gb RAM computer. Let’s remember the equation that we have been improving for Solr and the most relevant fields for memory tuning are the following: While the explanation of the following is long and complex, to build another post, I still would like to share the math that we have been working on. We started with our own calculator at the beginning of the problem solving, just to realize later similar concerns shared by the community online. Furthermore, we made certain of enabling the Garbage Collector correctly in JVM Args when starting Solr. We tuned the caches from evidence found in Solr admin panel as follows: The queryResultCache has 0.01 of hitratio The filterCache has 0.43 of hitratio The documentCache has 0.01 of hitratio Using New Relic we could check memory and GC activity on the instances and noticed that the NR agent was opening frequently its circuit breaker (light red vertical bars) due to memory threshold: 20%; and Garbage collection CPU threshold: 10%. This behaviour is a clear evidence of free memory problems on the instance. We could also monitor some of the high CPU instances processes, finding out that ~99% of heap was taken while the searcherExecutor thread was using 100% of CPU. Using JMX and JConsole we reached the exception that contained: …org.apache.solr.search.LRUCache.warm(LRUCache.java:299) … as part of the stack trace. The above exception is related to the cache settings sizes and warmups. The first idea to have search results for the front-end clients is to always have Solr replicas still alive to respond queries in case the cluster is unstable as a consequence of having replicas in recovering, or even gone state. Solr 7 has introduced new ways of synchronizing data between leaders and their replicas: NRT replicas: old way of handling replication in SolrCloud. TLOG replicas: which uses the transaction log and binary replication. PULL replicas: which only replicates from leader and uses binary replication. Long story short, NRT replicas can perform the three most important tasks, index , search and lead. On the other hand, TLOG replicas will handle index in a slightly different way, search and also lead. The differentiation factor is in the PULL replicas, which will only serve queries with search . By applying this configuration we could guarantee that as long as the shard has a leader, the PULL replicas would respond back, resulting in a huge reliability increase. Also, this kind of replicas don’t go into recovering as often as the ones handling the indexing process. We still faced issues while the index service was on full capacity, resulting in TLog replicas going into recovering . Based on the question Do we have enough RAM for the quantity of documents? , we decided to experiment. The initial concern was why do we have these values configured in “units” of documents as follows: According to equations shared previously, and considering that we have 7 Million of documents, the estimated RAM is ~ 3800 Gb. But, let’s say, we have 5 shards, then we would be working with ~1.4 Million of documents per shard that impact directly on the replica. We can estimate that, with that shard configuration, the needed RAM is ~ 3420 Gb. That does not produce a radical change, so we moved on. From cache evidence , we can see that there is only one cache being used best, the filterCache. The tested solution was the following: With previous configuration of caches we obtained the following results: The queryResultCache had 0.01 of hitratio The filterCache had 0.99 of hitratio The documentCache had 0.02 of hitratio In this section we can see the Garbage Collector metrics provided by New Relic. We don’t have Old generation activity which normally lead New Relic agent to open it’s circuit breaker (memory exhaustion). We got amazing results also in disk activity, regarding indexing with tremendous drops too. One of the services that accesses Solr showed a considerable drop in its response time and error rate in New Relic. One of the cons of a multi-shard schema is that if any replica is compromised, the shard leader is going to take more time than its peers to answer. This leads to the worst time response among the shards as Solr would wait for all the shards to answer before providing the final response. In order to mitigate the above issue and considering the previously described results, we decided to start reducing the number of nodes and shards gradually, which impacted in lowering the internal replication factor . After weeks of investigation, testing and tuning, we have not only got rid of the problems exposed initially, but also we have improved the performance by reducing latency, reduced management complexity by setting less shards and less replicas, gained trust with the index/update service working on full capacity and helped the company to reduce expenses by using almost the half of the AWS EC2 instances. I want to give special thanks to the team that made this possible and also SRE and AWS TAM for their guide and support. We operate a network of online trading platforms in over 40… 260 Thanks to Sebastian Venditti and Juan Bautista Blasco . Elasticsearch Solr Cloud Solr Performance Memory Optimization 260 claps 260 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-01-07"},
{"website": "Olx", "title": "running spark on kubernetes a fully functional example and why it makes sense for olx", "author": ["Rafael Felix Correa"], "link": "https://tech.olx.com/running-spark-on-kubernetes-a-fully-functional-example-and-why-it-makes-sense-for-olx-d56b6a61fcbe", "abstract": "About OLX Group Spark supports submitting jobs natively to a k8s scheduler since version 2.3.0 , but even with its nice documentation you still have to piece some parts together to get it working properly. In this article, I intend to provide a local working example and explain why it made perfect sense for OLX to go this way. Handling data is at the heart of what we do to deliver the best possible experience to our customers around the world (to understand how important this is for us, you can have a look at every data-related article written in the OLX Tech Blog ). And handling data at a global company scale can be quite challenging, which pushes us to seek ways to improve on how we do it on a continuous basis. Apache Spark plays a fundamental role in that space, being the most pervasive data processing technology inside OLX. Another technology at the core of OLX engineering is kubernetes , the de-facto standard platform for running most of our apps. K8s brings us many benefits, such as alignment (even regarding vocabulary , which is often overlooked but very important when dealing with multiple teams and applications), operational excellence, eases standardisation over monitoring, alerting, logging and auto-scaling (as cluster-wide solutions often offload SREs from having to think about every single of these aspects for each new application). Given our current expertise in k8s, all we needed was “a little push” to start running spark there as well. And this push came in form of: Cost savings, as many teams often deploy their own Spark clusters using Amazon EMR , which abstracts them from dealing with the complexity of provisioning and bootstrapping at the expense of a per-instance, per-second fee . Naturally, such costs can grow quite high on large datasets; and Serving https://kylin.apache.org/ properly, yet not risking it becoming unmanageable in the future (we didn't want to trade EMR costs over operational costs that a more complex solution would require). Spark supports submitting jobs natively to a k8s scheduler since version 2.3.0 . Yet, companies are only starting to embrace running spark on kubernetes (understandable, as the documentation still states that k8s scheduling support is experimental). By the way, if you haven't done so, go ahead and check https://spark.apache.org/docs/latest/running-on-kubernetes.html for a comprehensive guide about the concepts and configuration options. At OLX, we decided to take it for a spin and piece everything together so you can test is easily, as this could lead to massive cost savings in the future. These are the requirements for running spark on k8s: spark >= 2.3.0; a docker registry to host the spark images built by the docker-image-tool.sh script; a k8s cluster (for local tests, minikube will do) and tooling around it (such as kubectl and helm ); [Recommended] if not running a local test, you'll want cluster-autoscaler for enabling auto scaling nodes only when jobs are actually submitted; [Recommended] if running in AWS, you'll want something like kube2iam to assign proper permissions to the pods created by spark-submit; a sample spark job for testing TL;DR : check https://github.com/olxgroup-oss/spark-on-k8s , we've put together the same steps you'll see in a repo with a Makefile to simplify downloading and installing all the tooling you'll need locally, without messing with your environment. For the long version, keep reading. First, let's download and unpack spark in your local directory: At the time this article was written, we faced an issue that prevents spark from launching pods in recent kubernetes versions . The fix is merged already, but not yet released. For overcoming it without building spark on your own, we need to replace the kubernetes-related jars in $SPARK_HOME/jars/ with newer versions: Now, let's start minikube (this might take a few minutes if doing it for the first time): And deploy a docker-registry on it using helm: Before continuing, you now must add your newly created registry into the insecure registries list of your docker daemon. As an example. if you're using Docker for Mac this is under Preferences -> Daemon: Replace 192.168.99.105 with the result from: And restart the docker daemon. Once the docker daemon is back, you're finally ready to build the spark docker images and push them to your registry! Here's how you do it: Now it's time to run a sample spark job! You can run either from your machine (outside the cluster) or from within a pod inside the cluster. This is fairly straightforward: While the spark job is running, try opening a new terminal to check the pods running: If everything went well, you should see a message in the end that states “Pi is roughly 3.14…” If you want to run spark-submit from within a pod, you'll have to grant the pod access to the k8s API. This is done by creating a Role with the permissions and attaching it to the pod through a service account : Save this as a yaml file, and apply it with kubectl apply -f. Then, launch your pod using serviceAccountName: spark (this will link the pod you're running with the role you just created) For more information or how role-based access control works in kubernetes, check https://kubernetes.io/docs/reference/access-authn-authz/rbac/ . On cluster mode, the driver pod is not the same that originally ran spark-submit. This helps you, given that driver pods need to be reachable from the executor and this is taken care of (which is not true for client mode). However, if your application relies on spark-submit output for logging/making decisions whether the job finished successfully or not, this won’t work ( kylin relies on such output, as an example). On client mode, the documentation suggests creating a headless service for executors reaching out the spark driver. We found out this setup is not the best if you have more than one pod running spark-submit at the same time (the executors might reach back to the wrong pod, which will cause intermittent job failures). The way out was to set the pod’s FQDN in the $SPARK_HOME/conf/spark-defaults.conf, so each executor resolves their calling pod instead of any (that change allowed us to run multiple spark-submit jobs in parallel). If you rely on the performance of spark on top of HDFS, one of the key performance features is Data locality , in other words the capability to schedule jobs as close as possible to the HDFS blocks that need to be read. Such capability is lost when deploying in kubernetes currently. Make sure to benchmark the actual impact this causes in your case before investing too much too early. One significant limitation we found was the lack of support for k8s tolerations . This prevents you from running spark jobs on specialised nodes exclusively (such as spot instances in AWS , or GPU-powered nodes). For that reason, we decided to patch Apache Spark and add this feature (currently available here: https://github.com/apache/spark/pull/26505 ), since it allows us to reduce costs significantly. EDIT: spark 3.0.0-preview supports setting pod templates, which can be used to set tolerations among other configurations. Given it's already there for next major release, there's no reason for the spark maintainers to merge this PR in branch-2.4. Check https://spark.apache.org/docs/3.0.0-preview/running-on-kubernetes.html#pod-template for more information. When designing a spark cluster from scratch, using k8s may not currently be the most optimal way from a performance point of view. However, it doesn't seem the lack of data locality will be an issue for much longer. Initiatives such as https://github.com/GoogleCloudPlatform/spark-on-k8s-operator (although beta, it's currently under heavy development) should eventually address this. If you're already running applications in k8s (with a properly setup cluster, which comprises cluster-autoscaler and kube2iam at least), you'd be surprised by how stable running spark on k8s is. And, in OLX case, it means zero extra maintenance and support, as everything can be managed by leveraging the same operational standards we've already mastered. We operate a network of online trading platforms in over 40… 131 4 Docker Kubernetes Spark AWS Olx 131 claps 131 4 Written by Senior Site Reliability Engineer @ OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Site Reliability Engineer @ OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-05-19"},
{"website": "Olx", "title": "publishing android applicationunit test report on sonarqube part 2", "author": ["Jatin Juneja"], "link": "https://tech.olx.com/publishing-android-applicationunit-test-report-on-sonarqube-part-2-4592c1d296e7", "abstract": "About OLX Group This is the second part of the series. If you’ve not read the first one, you can start from below. tech.olx.com Let us begin by understanding a few terms and what are they used for before diving in detail. Code coverage is a software metric used to measure how many lines of our code are executed during the unit and automated tests . It helps you to measure the efficiency of test implementation It offers a quantitative measurement. It defines the degree to which the source code has been tested. Unit Tests are typically automated tests written and run by software developers to ensure that a section of an application (known as the “unit”) meets its design and behaves as intended. In other words, Unit Tests are lines of code written to test the actual logic written. During Unit Testing, all of the dependencies of the code are provided by some mocking frameworks such as Mockito. Improve and maintain product quality with less QA’s manual effort Finding software bugs early Increase confidence when shipping Perform routine checks Easier refactoring of old code Automation testing is a Software testing technique to test and compare the actual outcome with the expected outcome. This can be achieved by writing test scripts or using any automation testing tool. Test automation is used to automate repetitive tasks and other testing tasks which are difficult to perform manually. Reduce time for executing test cases Increase the productivity of your development process Early bug detection, save cost on software maintenance Quickly found and fix the bugs on implementation Ensure the quality of software Part 1- SonarQube Integration in Android Application Part 2- Publishing Android ApplicationUnit Test Report on SonarQube (you’re here) Android development experience (SDK, library usage, gradle, etc.) Part 1- SonarQube Integration in Android Application What is JaCoCo? Integrating JaCoCo in Android Application Generating Report with Code Coverage Sonar Analyzer does not run your tests or generate reports.SonarQube uses Jacoco to import pre-generated test reports to publish on Sonar Server. Now its time to publish the Android Application Unit Test report on Sonar Server. JaCoCo is a free code coverage library for Java. You need to update the build.gradle file to integrate JaCoCo. a. In Project’s build.gradle file, add JaCoCo plugin at top:- b. Add classpath dependencies within buildscript (buildsccript -> dependencies) c. After Step 1 and 2, hit “Sync Now” d. Configure JaCoCo by adding the following:- e. Add an action for task “Test” f. Under build.gradle -> android -> buildTypes, add: g. Under build.gradle -> android -> testOptions, add: h. Update the SonarQube properties, by adding : i. Now your updated SonarQube property will look like:- j. Replace PROJECT-NAME and PROJECT-KEY with the name of your Android Application k. Hit “Sync Now” That’s all….. you have successfully configured JaCoCo in your Android Application. As discussed, SonarQube imports the test reports pre-generated to publish on Sonar Server. You need to run your tests first before publishing reports. a. Run the Tests written for Android Application b. Run gradle command for SonarQube The updated Sonar Dashboard will look like:- Before going into the integration part of SonarQube CI Pipeline, first, let’s understand the CI Pipeline. Continuous Integration (CI) is a development practice that requires developers to integrate code into a shared repository several times a day. Each check-in is then verified by an automated build, allowing teams to detect problems early. Code control, code versioning Automated Testing, pre-deploy, post-deploy testing. Defects are detected & fixed sooner CI reducing the risk & Overheads Prevent wrong commits, missing file & resource while committing code Build automation Deployment Automation Let’s dive into the code to see how you can integrate SonarQube in Gitlab CI pipeline. You need to update the .gitlab-ci.yml file like below:- Replace SONAR_HOST_URL and SONAR_LOGIN_KEY with the actual Sonar Url and Login Key. I hope you enjoyed the post and able to set up and publish the report. Don’t forget to clap your hands on both the pit stops We operate a network of online trading platforms in over 40… 261 3 Thanks to JB Lorenzo . Android Sonarqube Code Quality Code Coverage Code Review 261 claps 261 3 Written by Learn -> Implement -> Share -> Repeat We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Learn -> Implement -> Share -> Repeat We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-12-09"},
{"website": "Olx", "title": "amping olx", "author": ["Leonardo Pittelli"], "link": "https://tech.olx.com/amping-olx-c124cc673399", "abstract": "About OLX Group This post intends to tell the story of having the huge opportunity to develop a new frontend, an AMP-compatible website, for a big and global company. First of all, let me explain what AMP is. The official site defines it as “a simple and robust format to ensure your website is fast, user-first, and makes money. AMP provides long-term success for your web strategy with distribution across popular platforms and reduced operating and development costs.” And as a key benefit, they say that “web page speed improves the user experience and core business metrics. AMP pages load near instantly, enabling you to offer a consistently fast experience across all devices and platforms that link to AMP Pages including Google, Bing, LinkedIn and more. These performance gains often translate into improvements in the numbers that matter, such as time spent on a page, return visits and CTRs.” So, long story short, the pages will load faster. Way, way faster. When we started to define the project, we had to take some key decisions. Our first key decision was where to start . We wanted to find an incremental path, to start measuring the impact and be sure we were really improving the experience for our users. One of the problems you have, when you go AMP, is that you lose control over which version your users will see when coming through Google (A.K.A. your organic traffic). In other words, you can’t choose if a user will get the AMP or non-AMP site. If you have a linked AMP version, your users will get it by default and no A/B testing is possible (or almost not possible, because you can create an AdWords campaign ). With that in mind, we had to choose carefully where to start from. Analyzing our organic traffic, we found that most of it lands on listing pages and, given that we have less listing URLs than item URLs, it’s easier to have more traffic on fewer pages, which helps to measure and make conclusions. So, as you’re probably imagining, we chose to start from AMPing listing pages. Once we decided the first URL to do the AMP on, we had to decide how to do it . Our actual website is not AMP compatible, because it’s a PWA built in React and the AMP restriction of not using custom Javascript was enough to make us take the decision of building a separate new app . As we wanted a quick proof of concept, we found it simpler to build a new app rather than refactor our PWA to be AMP compatible. So, we started a Node application serving AMP-compatible templates and listening to all the URLs with the /amp/ prefix. Your biggest challenge when you build a new frontend is to keep it up to date . Even though we were building a new version of our website, we wanted to offer an experience as similar as possible to our PWA version in order to not bias the metrics and make them comparable. This challenge is even more daunting in a company that is continuously looking at how to improve its product. We needed everyone being aware that a new frontend was born and any changes on the UI (or any new feature released on the original web app) had to be replicated in AMP too. Remember, we wanted to have almost the same experience in both web versions. How did we do that? Communication is the key. Telling everybody what AMP is and what we were planning to do, during our internal technical open talks. Letting everybody know that we were working on it, during our monthly update meeting. Asking every web developer to let us know when a new UI change is being cooked. Once we had the proof of concept ready, it was the time to deploy it and let Google know we were AMP compatible. Before choosing a country, we tested on a few URLs and found some random problems. Our CDN was randomly injecting a monitoring JS script and we had a bug on an image being served through HTTP instead of HTTPS (but only in certain cases). Both situations made our site to randomly return non-AMP valid HTML. Advise: create a cron querying your AMP site and validate the result with amphtml-validator node package. It’ll save you a lot of headaches! Google search console will show you the errors (if you have it) but sometimes the error messages are not clear enough to understand what’s happening. After fixing those random (and a bit hard to find) issues we chose to start with the site of one of our markets. We set up a dashboard with a lot of performance metrics and comparisons between our AMP and our non-AMP sites and it was time to decide how to A/B test it. Analyzing our tracking data, we divided our traffic into two groups with similar behavior in terms of conversion rates and other metrics. Finally, we “AMPed” one of those groups and started receiving traffic on our AMP site for it. And since you might be wondering if it was worth it, I’ll share some preliminary results with you: 74% faster load time 50% lower bounce rate 46% more pages per session As you can see, the results are very promising! If we continue in this way, the next steps will probably be to add more countries, more pages and more features to our AMP site. All the metrics are looking really good and there’s nothing we can see to say we should stop considering AMP. Probably, adding more countries will be the easiest way to validate it in other markets without a big effort. And, after that, if it’s working well there too, we should consider the effort of starting the same process with our detail pages. We operate a network of online trading platforms in over 40… 116 Thanks to Sebastian Venditti and Maryna Cherniavska . Web Development Amp SEO 116 claps 116 Written by Software Engineer. Fan of web technologies. Google Developer Expert for Web and Cloud. FullStack JS Dev / Chapter Lead Web @olxtecharg. Prev: PWA TL @garbarino We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Software Engineer. Fan of web technologies. Google Developer Expert for Web and Cloud. FullStack JS Dev / Chapter Lead Web @olxtecharg. Prev: PWA TL @garbarino We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-09-18"},
{"website": "Olx", "title": "lessons learnt from organisation level migrations revamps massive changes", "author": ["Jasjit Singh"], "link": "https://tech.olx.com/lessons-learnt-from-organisation-level-migrations-revamps-massive-changes-9c4527c7db50", "abstract": "About OLX Group If someone said migrations are tough, try migrating 8 countries to a new platform within 3 months. Let me take you through this journey of a lifetime and the key lessons that I learnt on my way.. To give a little bit of context, LATAM countries in OLX were operating on a legacy system and we wanted to upgrade them to the global platform in order to realise the economy of scale and reduce the time to market for product rollouts. There could be many ways to achieve the same or better results but the secret sauce remains the passion to make things happen. Personally, I felt this passion while working closely with the teams working on the migration across hubs and with the LATAM market representatives. This shared passion helped us in finding the way when we couldn’t clearly see things. I also saw similar patterns in similar activities like revamps & migrations and hence thought to document them. Here are the key lessons: Lesson 1: Communication, Communication, Communication We soon realised that in order to get everyone aligned to a common goal, bring out risks and get things done, we need a rhythm of communication that brings out an honest assessment that everyone can relate to. We used newsletters, operational calls, publishing minutes of squad — business interactions to discover and communicate things before it’s too late. We phased this with an initial focus on product/technology — business squad meetings to align on scope & answer open questions, issues, concerns and bring out gaps and later with abrupt situational meetings. The decisions were communicated in the weekly newsletters so that we can drive alignment & set expectations quickly. Lesson 2: Don’t fix things that are not broken It is very easy to get carried away and change everything to make things ‘right’ but the fact remains that more disruptions mean losing more time to manage these disruptions. We already had a process that worked well to track squad-level progress . QA was already raising issues and ensuring that teams were focusing on the bugs/issues. The migration team was already doing a terrific job in managing the overall migration activities. Squads were already passionately working to resolve issues/blockers or complete scope. Hence, we avoided new tools, new frameworks to improve for things which were already working for us. Instead, We focused on facilitating decision making with markets, bringing out things that need immediate attention, facilitating squad- squad communication to accelerate things. Lesson 3: Build Trust & a Safety net for everyone to feel comfortable in vulnerability The biggest mistake that someone could do in managing complex projects is to blame /shame people who are already working really hard but it’s easy to do that given the dependencies, slow progress and/or a feeling of being powerless when overwhelmed by situations. We felt that if we were to get this done, we all need to feel the butterflies in our stomachs and yet be absolutely honest with each other without any fear. It should be absolutely fine to say, Oops.. this can’t be done, Oops, we can’t make it to the promised time/commitment but the key was to bring this out as early as possible. In such situations, we used “ It’s ok, how can I help? ” I am happy that we did a lot of this during the journey. This helped us appreciate the complexity and reduce risk by thinking out of the box or take help from other squads or re-assess risk by sorting dependencies quickly and in the process being more open with each other. Lesson 4: Greet, Meet, Facilitate and Get Out! Give space to build long term relationship When I look back, one thing that crossed my mind early was that how can I facilitate this complex migration without being a bottleneck myself. It’s easy to fall into the ‘I will get things done’ mode and then get overwhelmed when the floodgates open near the migration. Right from the start in the product/tech — business squad meetings, we made sure that we pointed out these are icebreaker meetings and now stakeholders knew each other enough to be able to ping /call each other without a facilitator in the picture. Of course, if they were stuck somewhere, they knew where to find the facilitator( ahem..ahem ). This placed the onus on to the teams (product- business) to sort things out and make it work. We obviously followed up regularly and did observe often one party was waiting on the other, while the other didn’t realise the other side is blocked. This is where we optimised by cutting a short turnaround time. Another relationship that blossomed from strength to strength was with the market. Carlos Andres Rodriguez Morales was an amazing person to work with. Honest and open communication between us helped to bring out key issues in a timely fashion. Lesson 5: Clear decision-making matrix In complex situations, it’s very easy to move in circles when it comes to decision making but with each pass, the team loses time, effort, energy and the will to make this happen. We took a conscious call to document minutes to ensure we don’t go back on any of the decisions and we were moving forward every time the stakeholders met. The attendance of the meetings was always such that a decision could be made basis the known and unknown. We made sure at least 80–90% of decisions were taken in these product- business meetings rather than escalating them and then waiting for decisions. The contentious ones were brought up to the management and we got the decisions taken. This ensured that there was no bad blood as everyone was trying to do their job. We learnt here, decision making is not about compromises, it’s about understanding what’s best for users and can be done in a limited timeframe, meanwhile having a long term view so that the experience could be at least at par at the time of migration and then be improved over time. Lesson 6: Situational squads for special needs Multi-service framework like in OLX has an inherent dynamic of building silos around services. For example, someone in Seller experience isn’t too aware of the nitty-gritty in the Buyer experience and vice versa. Unfortunately, things are not as elegant when we need experience changes that span across squads, especially since the user is agnostic to this complexity and looks at the product holistically. During this journey, we pulled up people from different squads and loosely formed functional/situational teams who could work together on blockers to bring overall experiences live. The best thing about them was that they knew their squad stuff really well but now could also appreciate the overall picture. As always there were challenges but this almost happened organically for things like multi-phone number, CRM integration etc. Lesson 7: Transitioning from meeting heavy culture into asynchronous communication For the first wave of scheduled product/tech — business meetings where the scope was closed, we did marathon meetings (3 hours a day) to close the scope and we were able to cover scope in the first two-three weeks. During this period, the tech team worked to get the environment ready. Both these steps were crucial to moving quickly in Oct/Nov. Post that, we did something offbeat; instead of waiting for scheduled meetings, we started pulling in people in meetings to resolve issues as and when we observed them. These were abrupt situational meetings where we pulled in people ad hoc to solve issues right there and then. This comes with a cost and is not sustainable but it was the need of the hour. Around the migration time, these ad hoc meetings gave way to asynchronous sync ups on slack. I was amazed by the power of just checking in and asking people how they were? if they were blocked somewhere and how can I help? Also, this mode of communication worked well for inter hub communication when the time difference was more than 8 hours! Lesson 8: Monthly, Weekly, Daily Goals basis Risk Assessments We took our risks seriously and made sure we broke them into goals or a plan of action that can milestones to de-risk them could be tracked daily, weekly or monthly depending on the risk itself. For us, it was Ok to have big risks as long as there was a plan to address them. This also helped us in sharing our vulnerabilities and seek help early. Lesson 9: Bug bash and the final push! The devil is in the details and these details can make or break a migration. The biggest risk to the migration in the later phases was ensuring the stability of 8 markets. We did a bug bash where we focussed on testing variants of platform and countries to ensure all the critical flows are experienced by the teams. This enabled us to get a first-hand view of what we will be delivering to our customers. This not only brought about 20 blockers and about 200 bugs/improvements almost before the migration but also helped us understand the whole journey that the customers go through to be successful. Additionally, Like in previous migration, the team organised crowdsourced testing through Applause. These initiatives helped us to get a better assessment of where we are and what we need to do to migrate. For me, this was crucial else we would be shooting in the dark when it comes to migration dates and our comfort feeling to a ‘Go’ decision. Lesson 10: Enjoy and have fun! It’s very easy to get overwhelmed It’s ok to fail, it’s ok to be vulnerable, it’s ok to appreciate the complexity but at the end it’s about being happy and taking pride in what we do. At least for me, it was a journey where we went through a rollercoaster ride almost every day. It’s too early to say if we were successful or not but no one can take away the bonding & friendships across the globe while making this a reality. This sure was one hell of a ride. I don’t drink but now I sure know what a hangover feels like.. I would like to thank everyone involved in making through this rollercoaster ride together and in style! We operate a network of online trading platforms in over 40… 19 Startup Product Management Program Management Platform 19 claps 19 Written by Product, Users and the magic.. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Product, Users and the magic.. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-12-11"},
{"website": "Olx", "title": "a story about gender diversity", "author": ["Corina Dumanovschi"], "link": "https://tech.olx.com/a-story-about-gender-diversity-5253fc0be98e", "abstract": "About OLX Group This is the #2 article from a series related to diversity & inclusion ( see the first one here ). How it came to be? Ayelen and I came together and wanted to talk about this topic a lot more, in the context of OLX Group but not limited to it. One way to start a conversation with everyone in the group was to post an article on our blog, this blog. However, we then realized that this could instead be a series of posts to which more people can contribute to. DISCLAIMER: This series is based on our own experiences and personal opinion. We do not want to restrict or dictate. That being said, let’s start with what diversity & inclusion means. Diversity is used to refer to the various differences between people, ranging from race, ethnicity, gender, sexual orientation, age, social class, physical ability or attributes to religious or ethical values system, national origin, and political beliefs. Inclusion is the art of welcoming and embracing all these differences. In this article, I will be focusing on gender diversity as something that I have more experience with. If you are wondering about the relevance of gender diversity and its correlation to a company’s performance, I recommend you take a look at this research and summary article . My Story One of my favourite things about Berlin is the diversity of people in it. It is a special environment where different cultures come together to live and work. Everyone who has moved here has brought their hometown and/or home country culture with them. It is also one of the aspects that creates quite a bit of friction. How much do we really know about one another? How truly open are we to understanding each other? On a personal level, I’ve had many uncomfortable moments throughout the years. Discussing all of them openly is not easy and sometimes feels like a minefield. I’ve been angry, frustrated, annoyed; I felt powerless, I felt enraged, I felt an inexplicable energy and motivation to fight against what was and/or felt unfair — in a nutshell, yes, I have been emotional about it and no, this doesn’t make me weak or less equipped to do my job . It is one of the things that I as a woman in tech have experienced. A few years ago, in a previous job, when I wanted to take the next step in my career, I hit a wall. I was told that I should focus on my family and my home, that advancing in my career should not be a priority. It was one of the most crucial meetings I’ve ever had and it became an inflection point for me. I could see how my male colleagues were being treated, encouraged and the opportunities they got to grow and it became clear to me afterwards that I was not getting the same support, coaching, nor feedback, and that it was intentional. Doubt and impostor syndrome kicked in. Was I not good enough? What was missing from my education and skill set? What should I be learning? I want more, but am I capable enough? Am I being overly ambitious? Looking back, as bad as it sounds, I appreciate the honesty of that moment. It shook me to the core. As mentioned above I questioned a lot of things, however in the end I became more observant, more mindful and a hell of a lot more determined. I decided that this was not acceptable, that I need to grow stronger and learn how to deal with these things on the spot and afterwards, that I should go find a way to, if not prevent, at least reduce the number of times women need to hear this directly or indirectly. That trigger gave me an extra energy boost and the next time I found myself in a problematic situation, I was better equipped to deal with it. Learning about the different issues and different ways of addressing these issues has become a constant component of my daily life. You are reading this post today because of this journey. Education is crucial Continuous learning and reflecting on the topic have made me realise it was frustrating that we were not using education extensively enough to promote equality , promote situations where both partners are equally involved in everything related to their household, family, etc., and are also encouraged to pursue a career. It’s even more worrisome when children are not encouraged in the same manner and we see the discrimination starting so early; from certain colour schemes like blue vs pink, to what activities are encouraged to try out and how they are presented to them. You can read more about how education gradually shapes the gender imbalance here , how family, school and society affect boys’ and girls’ performance at school here , and one of my favourite resources from the UN Chronicle on Education as the Pathway towards Gender Equality . This has long term ripple effects in various industries and it leads to a very low percentage of women in them, especially in leadership roles. In tech we can see people coming together in smaller groups to support each other — which is amazing, don’t get me wrong, however it is not enough. It is not enough for smaller groups to fight the battle and organise amongst themselves. We need to involve everyone, we need to bring different people in the conversation and we need to call out issues as they are and have those conversations openly. As women in tech, we need each other, we need to learn from and empower one another, however we should not be doing this in isolation. If we want to have more women at the table, then we need to bring the men and the people who do not identify as either or at the same table. We need to pull one another up and support each other. We need to change our collective mindset and the way education works in order to remove barriers. I dream of a future where people, whatever job they are doing, are *just* people. No need for gender, race, ethnicity or anything else to be a factor or an attribute. No more telling kids, consciously and unconsciously, that math is for one gender and social sciences is for another one. This is the power of inclusion: understanding our conscious and unconscious biases and calling ourselves out when we are acting or saying something that originates from them. Let’s learn to recognise them so we can be better human beings, the kind that treat each other with respect. It is not easy. We as people tend to get defensive when someone calls us out on something or challenges us. We go into protection mode and, in the moment, our survival instinct will trump any rational call for stopping and thinking about it. Diversity & Inclusion need you — yes, you! “The data suggests that for diversity to work, workers have to buy into the value of diversity, not just hear some rules about it. Diversity creates positive benefits when people believe in its intrinsic value. They can’t just see gender inclusion as an obligation.” Change happens when each of us play our part in it, so I want to ask you to answer a few questions for yourselves: Are you encouraging, promoting or hiring someone because they are or look like you? Why is that? Are you aware of your own unconscious biases? If you look closer, you will see that having different people around creates a healthy dose of friction. Challenging conversations arise and better business outcomes are achieved. How so? Because people learn to listen and process, which leads to everyone rethinking concepts and becoming more open for change and/or alternatives. Do you want to become a better person/manager/colleague? Listen to the people around you and don’t always look for mini MEs- especially the managers. You don’t need to agree with everyone around you, but you owe it to them to listen and understand where their views are coming from. You don’t need to convince one another of the best one out of 2 or 3. It’s a big step forward to accept that the answers don’t have to be identical, that we can all live and work together, independently of our beliefs, gender, etc. You need to speak up when things happen to you or around you if you want to make things better. It comes down to trust, respect and owning the fact that each one of us makes a difference. At OLX Group we’re addressing Gender Diversity. Here is a message from the CEO: Gender diversity @ OLX Group . Thanks to Kathleen Sharp , Herman Maritz , Floriane Gramlich and Maryna Cherniavska . We operate a network of online trading platforms in over 40… 140 Thanks to Herman Maritz and Maryna Cherniavska . Diversity Inclusion Women In Tech Product Management Women In Product 140 claps 140 Written by Product Person, Geek, Avid Skier, Feminist, She/her We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Product Person, Geek, Avid Skier, Feminist, She/her We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-12"},
{"website": "Olx", "title": "clean architecture and mvvm on ios", "author": ["Oleh Kudinov"], "link": "https://tech.olx.com/clean-architecture-and-mvvm-on-ios-c9d167d9f5b3", "abstract": "About OLX Group When we develop software it is important to not only use design patterns, but also architectural patterns. There are many different architectural patterns in Software Engineering. In mobile software engineering, the most widely used are MVVM, Clean Architecture and Redux patterns. In this article, we will show on a working example project how architectural patterns MVVM and Clean Architecture can be applied in an iOS app. If you are also interested in learning about Redux, check out this book: Advanced iOS App Architecture . More information about Clean Architecture . As we can see in Clean Architecture graph, we have different layers in the application. The main rule is not to have dependencies from inner layers to outers layers . The arrows pointing from outside to inside is the Dependency rule . There can only be dependencies from outer layer inward. After grouping all layers we have: Presentation, Domain and Data layers. Domain Layer (Business logic) is the inner-most part of the onion (without dependencies to other layers, it is totally isolated). It contains Entities(Business Models), Use Cases, and Repository Interfaces. This layer could be potentially reused within different projects. Such separation allows for not using the host app within the test target because no dependencies (also 3rd party) are needed — this makes the Domain Use Cases tests take just a few seconds. Note: Domain Layer should not include anything from other layers(e.g Presentation — UIKit or SwiftUI or Data Layer — Mapping Codable ) The reason that good architecture is centered around Use Cases is so that architects can safely describe the structures that support those Use cases without committing to frameworks, tools, and environment. It is called Screaming Architecture . Presentation Layer contains UI (UIViewControllers or SwiftUI Views). Views are coordinated by ViewModels (Presenters) which execute one or many Use Cases. Presentation Layer depends only on the Domain Layer . Data Layer contains Repository Implementations and one or many Data Sources. Repositories are responsible for coordinating data from different Data Sources. Data Source can be Remote or Local (for example persistent database). Data Layer depends only on the Domain Layer . In this layer, we can also add mapping of Network JSON Data (e.g. Decodable conformance ) to Domain Models. On the graph below every component from each layer is represented along with Dependency Direction and also the Data flow (Request/Response). We can see the Dependency Inversion point where we use Repository interfaces(protocols). Each layer’s explanation will be based on the example project mentioned at the beginning of the article. 1. View (UI) calls method from ViewModel (Presenter). 2. ViewModel executes Use Case . 3. Use Case combines data from User and Repositorie s. 4. Each Repository returns data from a Remote Data (Network), Persistent DB Storage Source or In-memory Data (Remote or Cached). 5. Information flows back to the View (UI) where we display the list of items. Presentation Layer -> Domain Layer <- Data Repositories Layer Presentation Layer (MVVM) = ViewModels(Presenters) + Views(UI) Domain Layer = Entities + Use Cases + Repositories Interfaces Data Repositories Layer = Repositories Implementations + API(Network) + Persistence DB Inside the example project you can find Domain Layer . It contains Entities , SearchMovies UseCase which searches movies and stores recent successful queries. Also, it contains Data Repositories Interfaces which are needed for Dependency Inversion. Note : Another way to create Use Cases is to use UseCase protocol with start() function and all use cases implementations will conform to this protocol. One of use cases in the example project follows this approach: FetchRecentMovieQueriesUseCase . Use Cases are also called Interactors Note : A UseCase can depend on other UseCases Presentation Layer contains MoviesList ViewModel with items that are observed from the MoviesList View . MoviesList ViewModel does not import UIKit. Because keeping the ViewModel clean from any UI frameworks like UIKit, SwiftUI or WatchKit will allow for easy reuse and refactor. For example in future the Views refactor from UIKit to SwiftUI will be much easier, because the ViewModel will not need to change. Note: We use interfaces MoviesList ViewModelInput and MoviesList ViewModelOutput to make MoviesList View Controller testable, by mocking ViewModel easily( example ). Also, we have MoviesListViewModel Actions closures, which tells to MoviesSearch FlowCoordinator when to present another views. When action closure is called coordinator will present movie details screen. We use a struct to group actions because we can add later easily more actions if needed. Presentation Layer also contains MoviesList View Controller which is bound to data (items) of MoviesList ViewModel . UI cannot have access to business logic or application logic (Business Models and UseCases), only ViewModels can do it. This is the separation of concerns . We cannot pass business models directly to the View (UI). This why we are mapping Business Models into ViewModel inside ViewModel and pass them to the View. We also add a search event call from the View to ViewModel to start searching movies: Note: We observe items and reload view when they change. We use here a simple Observable , which is explained in MVVM section below. We also assign function showMovieDetails (movie:) to Actions of MoviesList ViewModel inside MoviesSearch FlowCoordinator , to present movie details screens from flow coordinator: Note: We use Flow Coordinator for presentation logic, to reduce View Controllers’ size and responsibility . We have strong reference to Flow (with action closures, self functions) to keep Flow not deallocated while is needed. With this approach, we easily can use different views with the same ViewModel without modifying it. We could just check if iOS 13.0 is available and then create a SwiftUI View instead of UIKit and bind it to the same ViewModel otherwise we create UIKit View. In the example project I also added SwiftUI example for MoviesQueriesSuggestionsList . A t least Xcode 11 Beta is required. Data Layer contains DefaultMovies Repository. It conforms to interfaces defined inside Domain Layer ( Dependency Inversion ). We also add here the mapping of JSON data( Decodable conformance ) and CoreData Entities to Domain Models. Note: Data Transfer Objects DTO is used as intermediate object for mapping from JSON response into Domain. Also if we want to cache endpoint response we would store Data Transfer Objects in persistent storage by mapping them into Persistent objects(e.g. DTO -> NSManagedObject). In general Data Repositories can be injected with API Data Service and with Persistent Data Storage. Data Repository works with these two dependencies to return data. The rule is to first ask persistent storage for cached data output (NSManagedObject are mapped into Domain via DTO object, and retrieved in cached data closure ). Then to call API Data Service which will return the latest updated data. Then Persistent Storage is updated with this latest data (DTOs are mapped into Persistent Objects and saved). And then DTO is mapped into Domain and retrieved in updated data/completion closure . This way users will see the data instantaneously. Even if there is no internet connection, users still will see the latest data from Persistent Storage. example The storage and API can be replaced by totally different implementations (from CoreData to Realm for example). While all the rest layers of the app will not be affected by this change, this is because DB is a detail. It is a wrapper around network framework, it can be Alamofire (or another framework). It can be configured with network parameters (for example base URL). It also supports defining endpoints and contains data mapping methods (using Decodable). Note : You can read more here: https://github.com/kudoleh/SENetworking The Model-View-ViewModel pattern (MVVM) provides a clean separation of concerns between the UI and Domain. When used together with Clean Architecture it can help to separate concerns between Presentation and UI Layers. Different view implementations can be used with the same ViewModel. For example, you can use CarsAroundList View and CarsAroundMap View and use CarsAround ViewModel for both. You can also implement one View UIKit and another View with SwiftUI. It is important to remember to not import UIKit, WatchKit and SwiftUI inside your ViewModel. This way it could be easily reused in other platforms if needed. Data Binding between View and ViewModel can be done for example with closures, delegates or observables (e.g. RxSwift). Combine and SwiftUI also can be used but only if your minimum supported iOS system is 13. The View has a direct relationship to ViewModel and notifies it whenever an event inside View happens. From ViewModel, there is no direct reference to View (only Data Binding) In this example, we will use a simple combination of Closure and didSet to avoid third-party dependencies: Note : This is a very simplified version of Observable, to see the full implementation with multiple observers and observer removal: Observable . For convenience it calls observer’s block on main thread because it is used by Presentation Layer which contains UI. An example of data binding from ViewController: Note : Accessing viewModel from observing closure is not allowed, it causes a retain cycle(memory leak). You can access viewModel only with self: self?.viewModel. An example of data binding on TableViewCell (Reusable Cell): Note : We have to unbind if the view is reusable (e.g. UITableViewCell) MVVM Templates can be found here ViewModel of one MVVM(screen) communicates with another ViewModel of another MVVM(screen) using delegation pattern: For example, we have ItemsList ViewModel and ItemEdit ViewModel . Then we create a protocol ItemEdit ViewModelDelegate with method ItemEditViewModelDidEditItem(item). And we make it conform to this protocol: extension ListItemsViewModel: ItemEditViewModel Delegate Note: We can also name Delegates in this case as Responders: ItemEditViewModel Responder Another way to communicate is by using closures which are assigned or injected by FlowCoordinator. In the example project we can see how MoviesListViewModel uses action closure showMovieQueriesSuggestions to show the MoviesQueriesSuggestions View . It also passes parameter ( _ didSelect: MovieQuery) -> Void so it can be called back from that View . The communication is connected inside MoviesSearch FlowCoordinator : Now each layer (Domain, Presentation, UI, Data, Infrastructure Network) of the example app can be easily separated into separate frameworks. Then you can include these frameworks into your main app by using CocoaPods. You can see this working example here . Note: you will need to delete ExampleMVVM.xcworkspace and run pod install to generate a new one, because of a permission issue. Dependency injection is a technique whereby one object supplies the dependencies of another object. DIContainer in your application is the central unit of all injections. Using dependencies factory protocols One of the options is to declare a dependencies protocol that delegates the creation of dependency to DIContainer . To do this we need to define MoviesSearchFlowCoordinator Dependencies protocol and make your MoviesScene DIContainer to conform to this protocol, and then inject it into the MoviesSearch FlowCoordinator that needs this injection to create and present MoviesList View Controller. Here are the steps: Using closures Another option is to use closures. To do it we need to declare closure inside the class that needs an injection and then we inject this closure. For example: github.com Clean Architecture + MVVM is successfully used at fintech company Revolut with >70 iOS engineers. Advanced iOS App Architecture The Clean Architecture The Clean Code The most used architectural patterns in mobile development are Clean Architecture(Layered), MVVM, and Redux. MVVM and Clean Architecture can be used separately of course, but MVVM provides separation of concerns only inside the Presentation Layer, whereas Clean Architecture splits your code into modular layers that can be easily tested, reused , and understood . It is important to not skip creation of a Use Case, even if the Use Case does nothing else besides calling Repository. This way, your architecture will be self-explanatory when a new developer sees your Use cases. Although this should be useful as a starting point, there are no silver bullets . You pick the architecture that fulfills your needs in the project. Clean Architecture works really good with (Test Driven Development) TDD. This architecture makes the project testable and layers can be replaced easily (UI and Data). Domain-Driven Design (DDD) also works very well with Clean Architecture (CA). In software development there are more different architectures interesting to know: The 5 Patterns You Need to Know More software engineering best practices: Do not write code without tests (try TDD) Do continuous refactoring Do not over-engineer and be pragmatic Avoid using third-party framework dependencies in your project as much as you can How can you improve your project by decoupling your app into totally isolated modules (e.g. NetworkingService, TrackingService, ChatFeature, PaymentFeature… )? And how can all teams work with these modules rapidly and independently 🤔? Check out the tech article about Modular Architecture: modularisation of the app tech.olx.com We operate a network of online trading platforms in over 40… 2.5K 11 Thanks to JB Lorenzo and Aleksander Lorenc . iOS Mvvm Swift Clean Architecture Software Architecture 2.5K claps 2.5K 11 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-10-08"},
{"website": "Olx", "title": "webpack from zero to hero", "author": ["Rubens Pinheiro Gonçalves Cavalcante"], "link": "https://tech.olx.com/webpack-from-zero-to-hero-10a7dc26c74", "abstract": "About OLX Group This article is part of the Webpack from Zero to Hero series , for more background or for the index you can check the “ Chapter 0: History ”. Previous — Chapter 4: Dynamic Imports and Code Splitting In the last chapter you learned how to add dynamic imports to your app and play around how Webpack does code-splitting. In this last chapter, we’ll add React and react-router to the app and do routing based code splitting . Let’s start! As the job to transpile the code isn’t in Webpack but in BabelJS’ hands, let’s make it read and transpile JSX . And add it to .babelrc : Now that all is set, let’s install React as a dependency : But wait! Webpack rule must be changed. Remember it only sends files having names ending with .js to Babel? Let’s change the regex to accept .jsx too ( .jsx are files which will render our React components ): The ?x means that it is optionally present in a match. This means that it matches both .js and .jsx . To finish, let’s make Webpack be able to resolve all .jsx without the necessity of explicitly specifying this extension on imports. On webpack config we add to the default extension values the .jsx : Now instead of doing: You can import without explicitly saying the extension : 🤓 — “Why don’t you make images, styles and media extensions to be automatically resolved too?” My personal view is, if it’s an asset it should be explicitly stated that it’s an asset , and we can easily determine it from extensions. Another problem to avoid here is naming collision , like a style file with the same name as the component, or having the same image but with different extensions like .png and .webp , etc. React Development bundle is way bigger than the production one. To tell React which one we want to use, we need to provide it to Node.js using the NODE_ENV variable. Since Webpack doesn’t use NODE_ENV anymore though, we can use the mode parameter to provide this value to React. Let’s include the Define Plugin on webpack.config.js : And add it to the plugins section: Now we can use React development/production builds accordingly. Throughout the chapters, we were using a string template to create our HTML, but it’s time to convert it into a React Component. First let’s rename our index.js to index.jsx , and replace the content to use a React component: Now we have exactly the same app from the other chapters, but with React. 🎉 Now that we have vendors code, we need to worry about another thing, the size of the main bundle and possible duplications. If you run yarn analyse you’ll see that a big slice of your app is basically React . Luckily for us, Webpack opens some optimization setup to us. Normally I split my apps into two bundles, first and third party code. Let’s add a configuration for this on webpack.config.js : Running yarn analyse again, you’ll see the vendors.js bundle, with anything that comes from node_modules in it. After the tweak to keep our bundle split between source and vendors, let’s setup react-router and start to create some routing in our app. First let’s install it: Then let’s enable the history API on webpack-dev-server , in the package.json change the script “ start:dev ” to: Let’s create 4 modules with some dummy components inside the modules/ directory: Each page only changes the Page ${number} and is named after it - Page-${number}.jsx . Now let’s get rid of the content in index.jsx and replace it with our routes: Ok, nothing new in the code from above. Just some static defined routes, right? But now, with React.lazy and <Suspense> , we’re capable to defer the loading of a component. For that we just need to use the previously seen dynamic import and convert the static routes to lazy routes . Let’s change the module imports code for this: If you open the network tab in your browser, you can see the page loading only after clicking the link . Other thing to notice is, Webpack caches any dynamic imported module already requested, so if you click on a previous visited link, no network requests are made again. Let’s say the Page-4 is very important to your business, and you don’t want any delay if a user clicks on it. Let’s use what we already learned in the previous chapter to prefetch this page : And in the <head> of the page you will find something like: As you can see with yarn analyse , the pages were split into 4 small chunks, one of them being our prefetched module importantModule.js You can keep up this strategy not only for routes, but also for sub-routes — depending on their sizes, it can be a smart choice! F inally we finished a production-ready web app setup. It was quite a long journey of whole five chapters, but I hope that it was totally worth it for you! If you liked this article, please don’t mind giving it some claps 👏, subscribe to OLX Tech Blog for more updates and share it to your friends! Your support is very important to us! See you! We operate a network of online trading platforms in over 40… 150 React Webpack Learning Reactjs JavaScript 150 claps 150 Written by Front End Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Front End Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-22"},
{"website": "Olx", "title": "mixing swiftui combine olx", "author": ["Aleksander Lorenc"], "link": "https://tech.olx.com/mixing-swiftui-combine-olx-636c4f3c4162", "abstract": "About OLX Group The dust after this year’s WWDC has not yet settled, and we’re still discovering many things that Apple introduced to us — Sign-In with Apple, Combine , SwiftUI , Catalina, Catalyst… The list could literally cover the whole screen. What got me, and probably every other iOS developer excited the most is, of course, SwiftUI — the declarative framework for creating views using code. Surprisingly, just a week before it was released, I gave a talk about creating views in code for our mobile team. While I saw many advantages in creating views with code, I couldn’t ignore its flaws — lots of code to set up constraints (I prefer not to use external libraries unless necessary, just simple helpers for my own purposes), no live previews, steeper learning curve. Now, you could imagine how I felt when I heard and saw what SwiftUI is capable of. Hyped up, right after the keynote I immediately rushed to download Catalina and the new Xcode — my MacBook has suddenly become a battleground and I didn’t care too much about what would happen to it. So I opened up the documentation and tutorials and started testing, testing and once more testing. Different abominations of the layouts and lots of compilation errors. But I finally got some understanding of how it works. So I spent the following next days watching the next WWDC videos (that took some time, and no, not all of them), checking Twitter every minute for news from other developers, accustoming to the new ways of creating layouts, and waiting for Combine. So an idea was born in my head — I decided to create a simple clone of the OLX app using SwiftUI and Combine. And I started doing just that. I found a boilerplate on GitHub which seemed to fit my requirements — basic extension for URLSession, and that’s pretty much the only thing I needed. Having played around with SwiftUI a bit, I chose to advance with View Models for the backing logic, and the Repository pattern ( example ) acting as a bridge between the API Client and the View Models. The repositories made use of a custom API Client I wrote and within them transformed the sequences to my desired outputs — for example, errors to empty arrays. My first impression of Combine is that it is very similar to RxSwift (there’s even a cheat sheet from RxSwift to Combine already), and if you knew RxSwift — getting to know Combine would be very easy, and vice versa after learning Combine. Overall — I really do recommend getting to know some kind of FRP if you haven’t already. Combine seems to be quite an easy framework to use. In comparison to RxSwift, it seems to have its advantages and disadvantages. On one hand, you can assign the value of a KVO-compliant property from a publisher (there’s already a Merge Request of the same feature on the way for RxSwift — quick reaction!), on the other, chaining operators results in a very long result type which you need to .eraseToAnyPublisher() to get a generic AnyPublisher<ResultType, Error> type — what RxSwift offers out of the box when dealing with Observables only. I felt the naming of Combine’s operators was a bit better, for example what is flatMapLatest in RxSwift, is switchToLatest in Combine — that makes more sense to me. What I’m still missing is traits like Maybe , Completable (stares at Apple). This is the outcome of what I have learned so far, and most importantly what is possible to do up to date in SwiftUI. The example fetches a list of 40 offers from OLX and displays them in a list. The images are lazy loaded upon appearing: each with a title, location, price, and a fixed date. This entire project took me around 12 hours to code, with breaks, with analyzing, with calling for help. While coding I tried to preserve good quality, making sure the code is easy to read and everything else follows good practices. Download this example project here . It was stripped off of any real API calls, so no hacking for you, but the simple API client with Combine support is left in place, so you can do whatever you feel like with it :) You just need Xcode 11 beta to run it, no need for macOS Catalina. SwiftUI is no doubt a revolution in how you create views. It’s easy, it’s fast to create. But it’s a beta. And while my heart is already in love, my brain says it’s not yet time. The demos, the first impression — it all makes you really excited, I’m a bit of an FRP fan so this fits right into my programming style. Unfortunately, as of now, there are still too many limitations. Too many pieces of the puzzle are missing to make this the right choice for your production app; no wonder — it’s still a beta! So, what is it that is missing? For example: You can’t set navigation bar/tab bar colors. You can’t set a TabbedView ’s item title & icon at the same time. You can’t build a SwiftUI project to Mac yet. Combine is not yet fully integrated in Foundation. The Foundation integration for the Combine framework is unavailable. The following Foundation and Grand Central Dispatch integrations with Combine are unavailable: KeyValueObserving, NotificationCenter, RunLoop, OperationQueue, Timer, URLSession, DispatchQueue, JSONEncoder, JSONDecoder, PropertyListEncoder, PropertyListDecoder, and the @Published property wrapper. (51241500) There’s no way to watch the Lists ’ offset (therefore no way to implement infinite scroll). No SwiftUI way for a collection view (apart from hosting UIKit ). No way to change small details like separatorStyle in UITableView. I’m not sure there is a way to inject protocols as environment objects yet. We’ll get back to this in September at the Keynote… ;) If you want to get in touch, catch me on Twitter or by mail aleksander.lorenc@olx.pl https://developer.apple.com/videos/play/wwdc2019/721 https://developer.apple.com/videos/play/wwdc2019/204/ https://www.hackingwithswift.com/quick-start/swiftui/ https://developer.apple.com/documentation/swiftui/scrollview https://developer.apple.com/tutorials/swiftui/ https://github.com/ra1028/SwiftUI-Combine/tree/master/SwiftUI-Combine-Example https://medium.com/gett-engineering/rxswift-to-apples-combine-cheat-sheet-e9ce32b14c5b We operate a network of online trading platforms in over 40… 225 2 iOS WWDC Swift UI Reactive Programming 225 claps 225 2 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-23"},
{"website": "Olx", "title": "hold the door hodor a hackathon project story", "author": ["Maryna Cherniavska"], "link": "https://tech.olx.com/hold-the-door-hodor-a-hackathon-project-story-28e62ce7abc9", "abstract": "About OLX Group Once upon a time, our company had a hackathon. W ell to be exact, this is a recurrent event that happens once or twice per year. The event is called a Hack & Learn Week , and from the very name, you can draw a conclusion that if you are not in the mood for hacking, you can learn . The conclusion would be correct. All week long, workshops and talks are also taking place in the office, given by the employees willing to share their knowledge. Almost everyone pitches in: either you’re hacking, learning, giving a talk, being one of the organisers, acting as a technical consultant ready to jump in and help the teams in some particular knowledge area, or an infra person helping the speakers and the teams with their setup. There’s also a yoga session and some massages thrown into the mix, so as you can imagine, the week is pretty colourful. There’s of course an option to just continue working, and some teams do just that, because of deadlines etc. However, you would be missing out not to use this opportunity to do something different . T he limitations for hack projects? None. You can do anything, either related to work, or completely out of the blue. Most teams try to do something helpful for the business, but that isn’t a requirement. The main requirement is to have fun. During the previous hackathon I wasn’t hacking. I was learning on my own, having picked some technologies that I wanted to try, and did a small pet project with them. However, I looked around and saw how much more the teams managed. The effectiveness of people working together was really obvious. So I decided, this time I am joining the team. O ne of the ideas that’s drawn my attention was a project about a smart doorbell. Our office space at the time wasn’t quite connected, and there was a challenge with one of the floors not being accessible from the entrance used by two other floors. It was’t convenient, because you needed to walk around the building to the other entrance to access that third floor. And there was even a door from the third floor to the entrance other floors shared: the problem is that door didn’t open with our key cards from the outside. It could only be opened from inside, because the building management refused to add one more security camera. So, people who didn’t want to take a long walk were just signalling in a Slack channel to the ones already on the floor, to open the door for them. A nyway, I was interested in the project, because I was one of those working on that third floor. So I jumped and proposed the name Hodor for it, because, obviously, something that solves the door problems just must be called Hodor, right?… The project initiator, Kathleen Sharp , received the name with enthusiasm. And so we had a name and two people on the team. Later, after giving the pitch, we were joined by another backend engineer, Barbara Dimitrova . The project idea was quite simple: a smart doorbell that would be switched on by someone going to the third floor . Then in the third floor, the signal would come up . We decided that the sound signal was too disruptive and we’d go with the light. So, someone pushed the button before going to the third floor; the participating rooms have lights that start flashing. Someone then pushes the acknowledge button, so that the lights change to green , letting other potential helpers know that someone’s handling the situation. Then he or she goes and opens the door. Easy peasy. W hat wasn’t easy is that only one of us, Kathleen, had any experience with hardware. Barbara was a pure software developer, same as me. We never had to work with anything remotely reminiscent of programming microcontrollers. Luckily, Kathleen once did a fun project with a friend who was deep into “this stuff” and she picked up some knowledge, which she was willing to share. And we were willing to learn. We were very deliberate that our intention was to do as little work-like stuff as possible. We love the work, but we really wanted to delve into something completely unknown, completely different, and just completely fun. Hodor was perfect for it. We started with some assorted hardware that we thought might help: pre-ordered some WiFi Arduino boards (WeMos D1 mini) , some DB1 switches , LED strips and LED pins. We also found an article about powering a light on a T-shirt with an Arduino board and because that article was the perfect example for us, we ordered two NeoPixel rings : with 16 and 24 LEDs. Apart from the WeMos boards, we had a Raspberry Pi in case we would need something more powerful; and a couple of Amazon Dash buttons which we thought we might hack. And of course we had cables, breadboards and a soldering iron which Kathleen borrowed. No one except her ever touched a soldering iron before. No one even thought to. We also decided to keep the Slack integration, since we wanted people to have options for how to send or acknowledge the signal. As in the article here , our main backend would be the Slack bot passing the commands to and from. Our first iteration was the blink test . H aving made sure the board is working, we defined that we needed three pieces of code. One would manage the signal light, another would be connected to the KNOCK button, and yet one more — to the ACK button. The KNOCK and ACK would be similar, except that they would send different commands to the Slack bot. The signal light code would open a socket and watch the Slack messages. On a KNOCK, it would start flashing the light; on an ACK, it would stop. Since there might not be anyone watching the lights, the KNOCK should also have had a timeout, after which it would stop. Initiating the commands from Slack would look like this: Where @hodor is addressing the Slack bot we created. We also made a webhook for it. The bot token is used to listen on the web socket; the hook URL is used to send knockknock and ack commands when needed. The code for Arduino could either be written in C++, which most of us haven’t touched since the uni, or in Python. However, most of the examples found online are using C++, and we didn’t want to spend too much time investigating how to set up Python with Arduino and converting the examples. So we also went with C++. So the first component, the knock code, looked like this. As you can see, there’s just two methods: setup() and loop() , those are similar for all the Arduino sketches (this is how scripts are called). The setup() in our case establishes the connection to the WiFi, which we need to send the signal. The loop() is performed in an infinite cycle and it tries to detect the button being pressed. When the button state changes and the signal comes in, we’re simply POSTing a small piece of JSON text with the knockknock command to the Slack webhook URL. The ack code is pretty similar, only the JSON sent to the webhook differs a bit: it just sends another command, the ack command. T he “smartest” piece of code here is the signal light code; in our case, the light is the NEOPixel 12-LED ring. This is the code responsible for reading the Slack commands and passing them on to the LEDs to turn the signal light on or off. As mentioned before, mainly we just adopted the code from this article , which explained things in great detail. T he code is pretty verbose here, but it is mainly because of the repetitive code that draws different colors. Perhaps it might have been refactored, but we were pressed for time and just needed it to work — it was a hackathon after all! The structure of the code is still the same: in setup() we establish the connection, in loop() we try to connect to the Slack web socket and assign the event handler. When the event comes in, webSocketEvent() handler is called. If the event is a message (called WSType_Text here), then processSlackMessage() is called. That processSlackMessage() code checks for the ack or knockknock commands and in case of a knock , sets the state accordingly. You need to notice that it doesn’t call the knockknock() method directly, though it does call acknowledge() if needed. The idea is, the knockknock() method should be called on repeat until someone acknowledges the knock. That’s why it is resolved through the state variable, which is checked and acted upon in the loop() instead. The ack , unlike knock , doesn’t need to happen on repeat: it just flashes the green light three times and after that, it is done. For the coding, we had to use the Arduino IDE . It helps you connect to the boards and upload the sketches. The IDE is not bad, though of course it is much more bare-bones than the full-fledged IDEs like IntelliJ IDEA and Visual Studio Code. There’s also a web IDE for Arduino , but we haven’t tried it, so I can’t say how good it is. However, you still can code in your preferred IDE — the Arduino .io files are just .cpp files after all, — and then use the Arduino IDE to upload to the board. There’s also Arduino plugins for CLion and for VSCode . However, at least for the VSCode plugin, the Arduino IDE still needs to be installed, and the CLion plugin reportedly has a lot of limitations. We just went with the approach of editing in VSCode as .cpp and then getting the code into the Arduino IDE, but I think if one worked with it on a regular basis, one could find a better setup. A fter the code was ready and working, my partners in crime also spent some time filming a video for the presentation: in hackathon, sometimes it is more about how the project is presented, than about the actual implementation. The pitch is important! We had a lot of fun building this project and learned a lot in the process. And actually, there’s a lot more you could do with Arduino boards. Look at the list of libraries — they might give you some ideas! We also won a small prize for innovation! However, the most important thing was to just play with different tech, work with people you don’t normally work with, try the tools you’re not accustomed to, and generally get out of your shell a little. Useful links: Article that contains a step-by-step for building a Slack bot with Arduino and NeoPixel ring that we used as a base ; Arduino IDE ; Arduino Libraries; Getting started with WeMos D1 on Mac OSX ; How to use a breadboard … and everything else you can find! Shout out again to Kathleen Sharp and Barbara Dimitrova for being a great hackathon team and also helping with this article! The article is also reposted to my personal blog here . We operate a network of online trading platforms in over 40… 32 1 Thanks to Kathleen Sharp . Arduino Wemosd1mini Slackbot Neopixel Cplusplus 32 claps 32 1 Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-29"},
{"website": "Olx", "title": "review lead dev london 2019", "author": ["Ayelen Chavez"], "link": "https://tech.olx.com/review-lead-dev-london-2019-87cf701dde96", "abstract": "About OLX Group I had the benefit, opportunity, luck, and pleasure of attending the Lead Dev London 2019 last week and I couldn't be more excited and grateful. That's the reason why I feel obliged to share my review for the conference with all of you. Spoiler alert: It was marvellous! (read it with an English accent) The conference lasted 2 days, there were 28 speakers, only one track*, almost 1400 attendees, 22 sponsors, and A LOT of tweets. #LeadDevLondon * track is a term for (bigger) conferences that are separated into several thematic areas and have (usually) dedicated chairs and program committees for each track . Several of the talks were given in a flashtalks format, meaning they would last only 10 minutes each. Even though I bet delivering an idea or a message in 10 minutes is a lot of work, the speakers did a great job doing so. Most talks that I personally loved were flashtalks. They were efficient, effective and impactful. Also, the format was so good that it got me thinking about why we cannot have ONLY this kind of talks, instead of some 30 minutes ones. Not only were there some talks about the topic, Diversity & Inclusion, but the audience and the speakers reflected and represented the significance of this topic. I’ve never seen a group of speakers so diverse and it surprised me that women were the majority (my first time ever). Also, the entire event was very inclusive and accessible. They provided real-time audio captioning for all talks and indicated several times that there were gender neutral toilets in the venue. I learned, among other things, that writing hashtags with #CamelCase would make them more accessible ( tweet here ). You had the possibility to customize your badge with a rainbow strap, stickers to indicate your gender identity and more. You could even opt-out to be in the conference media. All these carefully designed details collectively attributed to an inclusive and delightful event and I personally found them inspiring. Even though the agenda was pretty packed, there was only one big auditorium where all the talks were presented one after the other. We had planned coffee and lunch breaks, but that was it. Being able to avoid the pressure of choosing between multiple tracks plus my FOMO (if I chose one track, and the talk is not as hip, I will start feeling anxious about the other cool talks I’m missing), this new (for me) format was super helpful. There was no room or time for Q&A, but there were office hours to make sure you had a way to ask questions. To be honest, office hours would be useful for you only if you didn’t need to eat, have a coffee or go quickly to the toilet to be ready for the next round of talks! It was crazy! Because I know we are all busy, and if I list all the talks as a MUST watch you are not going to watch any, I forced myself to select the talks I liked the most and also impacted me BIG TIME. After that, I listed some in a SHOULD watch session, even though I think you are going to miss some really cool talks and messages if you don't watch them. By the way, the organizers will share the photos and talk videos very soon, and they'll collect links to slides and resources on the conference site here . Navigating team friction by Lara Hogan I related to Lara 's talk since lately I’ve been reading a lot about how our brain works, particularly around habits. Finding out she was speaking the same language and about the same topic I was so into was surprising for me in a cool* way. *I felt super cool with myself but I didn't write any bestselling book or have any of her impressive presentation skills… Give 10%, get 110% by Kate Beard Kate ’s explanation of how people with different realities need to use their 10% time at work and why it’s important to provide that time as part of our working hours, was my biggest A-ha! moment . She was referring to what some companies call 20% Time, FedEx Day, Passion Time, Genius Hour, ShipIt , etc. My A-ha moment was that, in order to be fair and stop privileging the same group of people to grow in their career in tech, we need to give this space at work. Because the reality for a lot of us is that we either need to take care of someone at home, we have kids, we have other responsibilities or, as she simply put it, we have a life. She said, and I quote “It’s ok to have a life outside your job” and “Faster development + Still getting free time = 😸”. I couldn't agree more. Facilitation Techniques 202 by Neha Batra Neha provided the next level of tips and tricks of how for facilitating discussions. She listed tools, indicated how to organize the meeting's content and she even shared her complete Amazon order for essential materials to make every facilitated discussion a success. I'm surely going to use some of them in my next brainstorming meeting, retrospective or postmortem. 12/10, Excellent doggo: the power of positive transformation by Heidi Waterhouse Heidi was funny since the beginning of her talk. Most of us agree with the fact that: \"Very few managers know how to get people to do something they don’t want to do\". She quickly came to the conclusion that understanding that people are motivated by different things is crucial to be able to talk to them more effectively. She concluded by saying that inspirational leadership can only get us so far, and we can do the rest with hard work, consistency, and compassion. Yes, I know… I'm not good at summarizing this talk. So I challenge you to watch it! The reality of testing in the real world by Angie Jones The story of how Angie wanted so bad to test the Machine Learning model that her company was about to release as part of their main service, that she ended up learning a lot about how AI and ML work. She rejected the common notion that AI is this omniscient black box that doesn’t require testing and, as a result, she spotted a bada*s bug challenging the idea that we should blindly trust AI. Inclusion starts with an I by Dora Militaru An inspirational talk about why topics such as Diversity & Inclusion are everyone’s responsibility. She explained why we need to shut up and start acting now. Dora was super energized, direct and called out a lot of bulls**t that we usually hear from a lot of corporations which claim to care about Diversity & Inclusion . How long is a piece of string: the key to solving the conundrum of software estimation by Jonathan Rigby The story behind why estimations are difficult and we shouldn’t try to estimate everything but trust each other. Some of the speakers recommended books to further read about certain topics. Here you can find the complete list I collected. Drive by Daniel Pink Accelerate by Gene Kim and Nicole Forsgren You can negotiate anything by Herb Cohen Mindset by Carol Dweck Radical Candor by Kim Scott Making work visible by Dominica DeGrandis I loved the conference. Content-wise, most of the talks were helpful and interesting, and the flashtalk added the efficiency and effectiveness that every busy tech lead loves and needs (love eeeeverywhere). The event was well organized and accessible to everyone. The presenter, Meri Williams was really good, empathetic, funny, engaged with the audience, and nice with all the speakers (she basically hugged everyone arriving and leaving the stage). I had a great time and I felt super energized by these amazing speakers. I cannot wait to come back to work and apply some of what I’ve learned in these 2 great days! We operate a network of online trading platforms in over 40… 68 Conference Leadership Leadership Development Lead Dev Review 68 claps 68 Written by Engineering manager, nerd, bigger sister, crossfitter, dance-lover, bad writer, good writers follower ;) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Engineering manager, nerd, bigger sister, crossfitter, dance-lover, bad writer, good writers follower ;) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-21"},
{"website": "Olx", "title": "android jetpack benchmark json parsers performance", "author": ["Mootaz Ltaief"], "link": "https://tech.olx.com/android-jetpack-benchmark-json-parsers-performance-1e76031a296b", "abstract": "About OLX Group Over time, our OLX Android app kept getting bigger and bigger in size due to new features and bloated resources. So we decided to tackle this issue and reduce the app size. For that, we converted PNG’s to WebP and used the new app bundle distribution format. The outcome was amazing, we got the size down from 23 MB to 11 MB. One thing we noticed after analyzing a release build, is that Jackson is taking 1.1 MB of space and this started a discussion about using an alternative library. Besides the library size, one aspect of evaluating alternative libraries is performance. And this discussion happened during the same time Google released the Benchmark library. We are going to compare the performance of the commonly used serialization libraries for Android and their size footprint. Gson : 2.8.5 Moshi (Using code-generation): 1.80 Jackson : 2.9.9 Kotlinx.Serialization : 0.11.0 The sample data used for the benchmark is generated randomly. We have three sample JSON files, that varies in complexity and size. As the title of this story implies, we are going to use the new J etpack Benchmark library The Benchmark library was an internal tool for Google for many years, and now it’s getting out for developers. The current version (at the time of writing) is 1.0.0-alpha02 , so we might expect breaking changes. We can add the Benchmark library manually, or use the Android Studio template. To enable the Android Studio template for benchmarking, check this link . The benchmark library injects a tiny activity and keeps it in the foreground (unless you have your own activity to test). This is in order to guarantee that you can use all cores with minimal interference in rendering. When we create a benchmark module via the Android Studio template we can notice that in the AndroidManifest file, the debugging is disabled. Debugging is enabled by default for tests, and it’s great because it allows us to connect a debugger and look for correctness. But it’s not so great for benchmarks, it can make the code run between 0~80% slower. The evaluation is done on a Huawei Y7 with Android 7.0 This lower-end device represents a considerable part of our userbase smartphones. And any change in the app performance can be felt by this part of users. The benchmarks time unit is in nanoseconds. or visually as a chart: This is a release APK analysis after Proguard and R8 have been applied. As we can see, Jackson is the winner when it comes to performance, but the library is 1.1 MB in size!!! and over 10k methods count. Kotlinx Serialization comes second and not far from Jackson and with smaller size. Also, it has native support for Kotlin and Kotlin-Multiplatform (Android, iOS, and Javascript). Good old Gson is not far from the race and the library is the second smallest library. Moshi is quite interesting, the footprint of the library is super small (82KB in size and only 597 methods count). But the library was the slowest even when used with code generation, especially with a complex JSON payload. For small simple JSON responses, all the libraries were almost at the same level. But when the response gets bigger and more complex, we start to see the difference. The difference is not big for recent high-end devices but it is for lower-end devices, especially if the app relies on the backend to display data. Source code can be found below. github.com Also, you can check the library introduction at Google IO 2019 We operate a network of online trading platforms in over 40… 126 3 Thanks to JB Lorenzo . Android Android App Development Performance Jetpack Json 126 claps 126 3 Written by Android Engineer at OLX Group, Berlin We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Android Engineer at OLX Group, Berlin We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-24"},
{"website": "Olx", "title": "webpack from zero to hero", "author": ["Rubens Pinheiro Gonçalves Cavalcante"], "link": "https://tech.olx.com/webpack-from-zero-to-hero-5540b6d620ec", "abstract": "About OLX Group This article is part of the Webpack from Zero to Hero series. The next chapters are: Chapter 1: Getting Started Chapter 2: Tidying Up Webpack Chapter 3: Everything is a Module Chapter 4: Dynamic Imports and Code Splitting Chapter 5 (final): Route Based Code Splitting with React Please follow the blog to see even more quality content published! Disclaimer: All the points of view and experiences are from me, the author, and it may miss some information because I haven’t worked with all tech stacks of each epoch of my time, but rather with one most common work stack I’ll be using as a default. A fter a successful Webpack workshop reuniting frontend developers from the Berlin (Germany), Lisboa (Portugal) and Poznań (Poland) OLX offices, and after receiving a good feedback, I got the necessary push to create a series of articles (starting from this one) about bringing an application to life while setting up Webpack in the most simple way possible . But before going full technical, I wanted to share some history about how we got here . This is important to both new JS developers and to more experienced developers willing to do a “backtracking” on their career and look back at the front-end development tools that have become milestones. Those tools gave us independence and the voice to speak as the real engineers we are. JavaScript was always considered a “support” language and was never taken very seriously. Back in 2007 , when I started doing software development, I used to work with PHP and JS. We didn’t have the terms backend or frontend then, all of use were just “web developers”. Don’t get me wrong, I don’t have an issue with the term. At that time tech jobs didn’t have so many branches because they were following what we also had at that time - everything together in a single web application. Since that time, I really enjoyed working with the “holy triad” - HTML/CSS/JS. It was the “tableless” era, jQuery and AJAX golden age, it was a revolution ! “Why do we need to refresh the whole page just to update a small part of this screen?” Few years passed, the Web Applications were becoming bigger and the backend guys were converting everything to a service . This was the new trend, — you just do Ajax now, you rely on our REST API , no more posting forms or UI on backend. While before I was 40% a backend and 60% a frontend developer, now I was almost full-time JS developer, no more focusing on CodeIgniter , Symfony , Django or Flask . The first JS frameworks started to appear, proposing to go to SPA (single page applications) and bringing the MVC pattern, something that we were used to see only on backend now also on frontend. I started to work with Backbone JS , but I heard a lot about KnockoutJS too. Together with jQuery UI and Twitter Bootstrap we created several web applications, now much more rich and powerful than before. Then we started to naturally use the terms frontend and backend , because of this new way to build web applications where we had people working full time on both parts. 👔 — “So many third party styles and scripts, this is out of control! This is blocking the page and our users are always complaining that our app feels too slow!” The applications where growing… too much! OK we need some way to ship this in a smart way, like requiring it only when necessary, and including something that’s common for backend guys: Continuous Integration ! With this situation, after some research, we found out RequireJS and its nice idea of loading JS in an asynchronous way ( AMD — Asynchronous Module Definition ), never blocking a page with numerous <script> tags. There was only a single entry point script which required all the other scripts, downloading then through Ajax and parsing them internally, based on the format: But hey, we needed a way to ship this too, and minify all this stuff that was interconnected but should have been maintained separately, so the old concat and minify would not work for us. That’s when we found GruntJS . We started to create some scripts with the new kid on the block called Node.js (and now just Node), started to take advantage of LESS inside of Bootstrap and watch for changes on such files. We were able to do all what we needed with them, but the projects grew and it started to be really painful to run the builds in watch mode , and we were coming back to the monolith days, waiting for the backend to pre-process everything to render the pages. Grunt became slow, RequireJS syntax was kinda dirty and Backbone was not enough. Something had to be done. So came the natural conversion to Gulp which was able to run multiple tasks in an asynchronous way, and way faster than Grunt did. The new new kid on the block was AngularJS , promising a mind blowing way to keep your UI always updated, replicating the models (data) and making you forms data (or any kind of input) being automatically applied on your models. It was the MVVM pattern. Also, a tool called Browserify came with the promise of module resolution, something we had on Node.js but didn’t have in browsers, and without the nasty syntax AMD had. The syntax was simple: use the “ require()” from CommonJS . It didn’t take long for applications using AngularJS to start suffering from performance issues. Of course, because the tool was born from some designers prototype but then became a framework! To sync up model with the view and vice-versa, it used a heavy algorithm to check for updates on both sides: the infamous Dirty Checking , registering watchers for pieces of data/views, which were prone to grow fast and stay in memory, causing severe memory leaks. And then this guy came kicking at the door: It came with the idea of one way data flow (as opposed to Angular’s two-way street) and Virtual DOM algorithm, performing way better than Angular. It introduced a new syntax too — one called JSX . Instead of manually adding polyfills , we used BabelJS to do this job, and since Babel was a transpiler , we were able to input JSX and translate it to JS too. 🎉 Finally, with all these tools we arrived to the bundlers era: Is like a fusion of module resolvers and task managers. It assumes responsibilities of what RequireJS or Browserify did, taking care of modules resolution, but also assumes some responsibilities of task managers like Grunt or Gulp , watching the files and parsing different types of files - other than JavaScript. 💡 Webpack is a module bundler, and currently one of the most famous out there (among others, like Parcel and Rollup ). It doesn’t only take care of resolution of modules but, with the help of the loaders, parses different kinds of files and treats them equally as JS modules. We normally use it along with ECMA script imports , but it also gives support to CommonJS require and AMD require/define , and is capable of exporting all of them using the UMD (Universal Module Definition) format. No , because it doesn’t do 100% of the “task management” work of the tools like Gulp or Grunt. But now we can use simple tools to do the job. In this series we’re going to use npm scripts as it’s a simple solution and covers everything that Webpack don’t. H ope you liked this short introduction and all the history that led to the module bundlers generation. See you in the next chapter ! We operate a network of online trading platforms in over 40… 1.7K 5 JavaScript Webpack Technology Nodejs Frontend 1.7K claps 1.7K 5 Written by Front End Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Front End Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-25"},
{"website": "Olx", "title": "building an aws serverless ml pipeline with step functions", "author": ["Rafael Felix Correa"], "link": "https://tech.olx.com/building-an-aws-serverless-ml-pipeline-with-step-functions-b39feed12bab", "abstract": "About OLX Group At OLX Group , we strive to serve customers in many markets (like India, Pakistan, Indonesia, Brazil, Poland, and Ukraine) with the best online classifieds experience possible. Obviously, this leads to a series of challenges, one of them being: How to consistently deliver meaningful recommendations, e.g. categories to visit, given markets have so many differences among them? This is where Machine Learning comes to the rescue. Warning: this article does not intend to compare AWS Step Functions with other workflow processing engines (e.g. Airflow). Please refer to this article for a detailed comparison among several of these tools. It’s not a secret that OLX works heavily on top of AWS , thus Sagemaker is currently the first stop when problems arise. Often, the process is spinning up a notebook instance on a development account, and start testing a combination of models and hyper-parameters against a subset of the data. A typical starting point is the Sagemaker examples Github repository, which is pretty comprehensive and helps Data Scientists to spin up an initial version quickly. When you have selected a model for the first implementation with real-world data, fetched in a continuous way, that's when things get more complicated. Sagemaker won’t cover the ETL part of the process (out of its scope), neither controlling which steps should execute and when. For that, you’ll need to piece together other AWS services to get the job done in a scalable and maintainable way. And by scalable, I mean not just the amount of data but also being able to use more than one ML model (after all, you often want to benchmark and pick the best). After experimenting with several combinations of AWS services, we came up with the following solution proposal: Storage for input, output, and temporary steps data: Amazon S3 . ETL (to fetch and prepare the input data as well as output data in the correct location and format): AWS Glue (Athena can’t export to Parquet natively as of the day this article was written). ML Model training and Batch Transformation: Amazon Sagemaker . Custom triggering logic with proper input parameters: a combination of Cloudwatch Events , Glue ETL triggers , and AWS Lambda : Although aligned with the current set of best practices for serverless applications in AWS , once we deployed the pipeline we quickly realised: Tracking which job belonged to each execution was painful (tagging could help, but you’d have to implement it yourself). It was hard to assess the current state (again, you’d need to implement something perhaps in DynamoDB to save the executions current state). It needed extra tooling/scripts to assess the total execution time (as there are multiple sources to check, like Sagemaker and Glue) as well as to run backfill/reprocessing tasks with past dates. Coordination between training and transforming was tricky, as the triggering logic was spread across Glue ETL triggers, Cloudwatch Events, and two lambdas. These problems ended up harming the experience of whoever needs to interact with it, as troubleshooting failed runs was painful compared to other platforms like Airflow. To tackle the issues described above, we decided to try out Step Functions. Simply put, AWS Step Functions is a general purpose workflow management tool. A more comprehensive definition can be found in their AWS service page . Although not focused on data workloads, the fact you can coordinate long-running tasks controlling/watching execution details/transitions from a single interface only paying by state transition was exactly what we needed. The final solution diagram looks much simpler, as AWS Step Functions coordinates the sequence of execution and the triggering logic was moved to a single place: Create a triggering lambda function. Reason : you can programatically ensure input validation and safe defaults before the state machine is called. Otherwise, you’ll potentially spend unnecessary dollars running jobs with wrong parameters. 2. Stick with AWS Step Functions native integrations. Reason : Service integrations will save you from writing lambdas to fire up a Glue ETL Job or a Sagemaker Training Job, and provide you full control from the Step Functions console or API (e.g. StopExecution will stop any synchronous step like arn:aws:states:::glue.startJobRun.sync ). 3. Use Lambda for performing string interpolation and formatting. Reason : You might run into a scenario — as we did — in which you need to combine your input parameters into a single string (e.g. s3://my-bucket/model/market ). Unfortunately, as of today, you cannot do this purely with ASL as it doesn’t support mixing JsonPath expressions with strings (this information is not explicitly documented anywhere, but I got it after talking to AWS Support). To overcome this, you can code a very dumb lambda function that does this for you and returns it to the next steps. Example: The lambda function code: ResultPath and JsonPath are your best friends. Reason : Step Functions sends the output of a previous state as the input of the following state by default. Although a reasonable behaviour most of the time, often you want to access the input arguments from a middle-stage step, which won’t be possible. To overcome this, you can encapsulate the results produced in one step using the ResultPath option. Example: For more information: https://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html AWS Step Functions turned out to be a good fit for the data pipeline use case as it comprises long-running steps. It’s a very handy way of overcoming the lambda execution time limit (and it’s also cheaper to pay state transitions than a “long-running” lambda function). Although not a requirement, it helped us overcome a reasonable amount of shortcomings of a serverless data pipeline in AWS. There's no magic though: you still have to implement any flexibility you require (like reprocessing from a past date or skipping part of the flow), as it’s meant for covering general workflow use cases. Amazon State Language spec: https://states-language.net/spec.html Inspiration: https://github.com/aws-samples/aws-etl-orchestrator and https://epsagon.com/blog/hitchhikers-guide-to-aws-step-functions/ We operate a network of online trading platforms in over 40… 670 2 Thanks to Andreas Merentitis and JB Lorenzo . AWS Serverless Data Science Machine Learning Data Pipeline 670 claps 670 2 Written by Senior Site Reliability Engineer @ OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Site Reliability Engineer @ OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-04-15"},
{"website": "Olx", "title": "android evolution at olx", "author": ["Martín Moreno"], "link": "https://tech.olx.com/android-evolution-at-olx-52a5493f7b7f", "abstract": "About OLX Group At OLX Panamera we operate in a wide range of countries, spread over several continents. South Africa, Pakistan and India, for example, are some of our biggest markets. They alone add up to 15 million users a month. These users not only search for products or publish new ones, they also interact with each other while doing so. Our chat serves around 2 billion messages a month (and that’s only India) which is huge for any platform. The users are also diverse, they have different cultures, different needs, and of course, different hardware. Our app runs in an enormous range of devices, from the classic Samsung phones to the Xiaomi, from the Oppo to the … Batmobile. Yeap, we’ve crashed the Batmobile (sorry Batman) With this variety in mind, we’ve built a special team to attend the Performance & Growth of the platform. It is in charge of working with the whole chapter to reduce APK size, network consumption and stabilizing the newly introduced Android Vitals. As a team, we’ve set the ambitious goal to go below 9 mb for the APK size on our release to India. We found some quick wins while refactoring the existing code, removing unused code and migrating to a new backend API (that would also made our app consume less network data). The migration of the API wasn’t an easy task, it took us almost 2 months, but we were able to reduce almost 4mb of the size of the APK. Most of it code and unused resources. Then we removed some fonts that we were not using, improved our Proguard rules and configured Gradle properly to remove unused resources on build-time, including also languages that the app didn’t support from third party libraries. Again almost 4mb were saved with this new batch of initiatives. In The final push we migrated all of our drawings and icons to vector graphics. With this final change we were able to over-meet our goal reaching 8.12MB apk size. For the Network usage we found that the main responsible were the images in the listing and the item details. Our images api has some good features that allowed us to convert from Jpeg to WebP format and resizing images server side on demand. WebP is way more aggressive in compression without losing much quality: By viewing the images side by side you might notice some small details on the new version, but after doing some user testing and checking the numbers after releasing we learned that it didn’t matter for the user, and the overall experience was greatly enhanced with smoother scrolling and faster load times. We’ve also optimized our request to match a set of scaled down sizes based on the device screen dimensions to minimize the size of the downloaded images. In total, the reduction for the listing images was of 71%, item page images 66%, and gallery images 22%. A Google Playstore analysis show that more than 60% of the 5-stars ratings are related to performance and UX while more than 30% of the 1-stars reviews are related to ANRs and Crashes. With this in mind is that Google decided to create Android vitals to measure the quality of applications. In Vital’s dashboard we can check 15 different metrics that give us an overall picture of the health of our application. Of those metrics there are four that are considered core: Crashes , ANRs (Application Not Responding) , Excessive Wakeups and Stuck partial wake locks . Crash rate is the most used metric by the industry as a quality measure, since is one of the easiest to measure, understand the impact and to and fix. For Panamera we solved lots of our problems with crashes by moving to Clean Architecture and standardizing the way we consume data from the remote APIs. We started with 95% crash free users a year back, and we are now in 99.6% ANRs is another important metric since, like the crashes, the OS notifies the user when the application is not responding in a timely way. This could mean that we are doing a lot of work in the main thread resulting in a “freeze” experience for the user. It’s a tricky one, since it could happen on low-end devices only, but if you have a high number of ANRs it’s usually a hint that something could be improved. For example we had good figures for ANRs in South Africa but after we went live in Pakistan where the low end devices share was higher we noticed some opportunities to further improve our architecture. We were relying too much on libraries that required initialization code during the app startup and didn’t work properly when combined, so we moved away from some of them, and implemented lazy initialization for some other. Both “Excessive Wakeups” and “Stuck partial wake locks” are related to battery consumption, they are usually related to location updates and background services. We had some issues with this metric in South Africa, specifically in the location service. While the location is important in some flows, it’s only required once and no further update is required. Android has a service to request locations that continuously request location updates with a set of given parameters and you need to manually turn it off when you are done using it. We used to have several implementations of this in the different flows so we rewrote the whole location implementation as a single entry point that asks for a location update with the best accuracy and a time-out of 15 seconds to turn off completely the GPS to avoid battery drainage. After doing that we didn’t have any more issues with battery related metrics. We are still working on improving the remaining vital figures, so look forward to the next post from the Performance and Growth team regarding the App Startup Time and Frozen Frames improvements in the near future. Chat often opted as one of the most safe and secure channels for buyers and sellers to connect with each other. Chat technology that we adopted was based on XMPP protocol (Ejabberd). The first version of chat android client released had only xmpp based connection with backend. While this was super fast in real time message/presence sending, it was heavy on network chatter for loading historical data. We re-architected the network component and added HTTP based communication for data-optimised chat history fetch. This helped us reduced chat loading time for large accounts under 10 secs. XMPP and HTTP interfaces together gave real-time rich and performant chat experience. Unlike other components in the product, Chat UI was too dynamic (realtime messages, presence updates, user typing status and more). This made the conventional sqlite controlled UI implementation jerky and hard to manage. Also, cursors are a pain to manage and error prone. Using Google Architectural components, specifically ROOM, made our life a lot easier. Receiving updates directly from the DB as objects made seamless UI updates. Our current architecture follows the Fernando Cejas 2014 proposal that builds upon Uncle bob’s Clean Architecture . After a few years of use we found out that some of the practices proposed there were of great use, but also found some we needed to change. In particular, the layering of the app in App/Domain/Data proved to be of great use, but we needed to go deeper into how we modeled our business logic, as it was getting overly complex to represent in a clean way. In this road we’ve found Interaction Driven Design. IDD proposal is to model the user interactions modeling them as “actions” and, as it’s built on top of Domain Driven Design, it makes use of the classic DDD tactical patterns. In OLX domain, a good example is the Contact User action. The action is in charge of retrieving the data from a users repository, checking that the user making the contact is logged in (via a domain service) and making the actual call via an infrastructure service. Testing is a way too big subject for this post, but IDD proposes the outside-in method of test driven design, which proved helpful when trying to design our domain in a concise way. Designing our action thinking first in how they are going to be consumed is a nice way of achieving that. On top of all this, Kotlin is pushing in as the new go-to language for android, and with reason. Not only is way less verbose than java 7/8, and provides more features (null safety, immutability tools), it also provides an alternative to classic threading, the coroutines . The impact is so big, that it even questions the necessity for Rx in the project. Our problem with Rx is not only the complexity it involves, but also that messes up the ubiquitous language we are trying to express in code. We are definitely going to write a post on this subject in the future. This post was written in conjunction with @sahil.jain_5980 and Marcos Navarro , big thanks to them for recollecting the history of the platform. Also, special thanks to Sandro Mancuso for reviewing this article. We operate a network of online trading platforms in over 40… 595 Android Interaction Driven Design Domain Driven Design Performance Apk Size 595 claps 595 Written by Android Chapter Lead @OLX Arg We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Android Chapter Lead @OLX Arg We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-04-30"},
{"website": "Olx", "title": "webpack from zero to hero", "author": ["Rubens Pinheiro Gonçalves Cavalcante"], "link": "https://tech.olx.com/webpack-from-zero-to-hero-60673ea906aa", "abstract": "About OLX Group This article is part of the Webpack from Zero to Hero series , for more background or for the index you can check the “ Chapter 0: History ”. Next - Chapter 2: Tidying Up Webpack . Disclaimer: During this series I’ll be running Yarn, but if you like NPM better it is totally fine too 😄. In this article we’re going to start from an empty directory and proceed to build an application with dependencies, producing a simple bundle using Webpack with just a few lines of setup! Open the terminal, create your project directory (or clone a empty repository) and create a new package: This will create your npm environment. Then create a simple file with some JS inside of it on src/index.js : To install Webpack runtime and client we’ll need to do: Now you’re ready to go. Just run: If you’re running with npm > v5 you can use npx webpack. And the result will be something like this: Note that webpack will warn you about not providing the mode and it’s therefore assuming “production”, later we’ll understand what this means. Now we can see the output at dist/main.js , and this is the bundle! 😎 😡 — “Hold your horses partner! I don’t have a clue what’s happening here!” Wait a minute, let me explain. Following Bundlers like Parcel , Webpack 4 is now designed with this in mind. You see that we’re able to run without any configuration file, and you probably have noticed that it assumed two things: - The entry file being src/index.js - The output being dist/main.js And more, when it warned about the “ The ‘mode’ option has not been set” , it’s because it assumes a different set of options when you’re bundling for production and development modes. Let’s see what happens when we run with different modes. Open the dist/main.js file when Webpack assumed “production” mode. Now run: And check dist/main.js again. You’ll see the non-uglified unoptimized version of the bundle. At the start you can see the webpack runtime code, the one responsible for the module resolutions (imports/exports), and then your wrapped code: Now move the “hello()” function to another file: hello.js : And import it in index: Now run the dev build again and check the result on the bundle dist/main.js. To make our life simple, let’s move these build commands to the package.json file. Add this section to it: Now we’re able to run the build with just: or If you’re using NPM you need to do npm run [script-name]. 😠 — “When I checked the bundle, all ES6 code was still there! It’s not transpiling!” Now is my time to say hold your 🐴 🐴, because as I explained in Chapter 0: History , Webpack acts only as a bundler. Its work consists only of module resolution and piping these modules through consumers through loaders . The one responsible to transpile from ES6/ES7 (or ES2015+ if you prefer) to ES5 is BabelJS : In this chapter we started from an empty directory and created a project with NPM dependencies and scripts already running webpack builds with just a few steps. But it is still a raw setup and we have to support more browsers than just the latest ES2015+ supporting ones. But let’s see how to do it properly in the next chapter . See you there! We operate a network of online trading platforms in over 40… 523 2 JavaScript Webpack Technology Nodejs Frontend 523 claps 523 2 Written by Front End Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Front End Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-06"},
{"website": "Olx", "title": "will it scale lets talk about that", "author": ["Facundo Viale"], "link": "https://tech.olx.com/will-it-scale-lets-talk-about-that-3039392edbf0", "abstract": "About OLX Group At OLX we are going through a lot of technical challenges under the hood. The biggest one being our brand new platform. An entire re-architecture based on micro-services, which has already been rolled out in several countries, like Pakistan and South Africa . Before those migrations, one of our biggest concerns was; How well is our platform going to perform? That’s why, before rolling out the platform in a new country, our end goal was to understand what to expect from it in terms of scalability, responsiveness and stability under a production workload. This wasn’t comparing apples with apples, because the new platform has new features and behaves differently in most of our key flows. Testing the platform is very important to fine-tune the components, prevent crashing during the release and be sure that we aren’t over-provisioning or under-provisioning the platform. Luckily there is a whole area of study called “software performance testing”. First of all, it’s very important to determine; What do you want to measure? Because there are different ways in which you can test your system to see different measurements. So, before we start defining the tests, what do we want to know?: Given the current infrastructure setup, how many users can we handle? Can the new platform handle the number of users the old platform is currently handling? How many resources do we need for that? Compare the two systems to verify the new one performs equally or better, in all flows. Which parts of the system or workloads cause the system to perform badly. It’s important to notice I’m talking in terms of Users and not Reqs/Sec, given that the new platform could generate different amount of requests for similar actions in the old platform. I’m going to go into more detail at the end of this post. With all this in mind, we decided to use 4 types of performance testing to do the checks and gather the measurements we need. Some things to keep in mind: Use a good monitoring tool during your tests like Prometheus. In our case, we used New Relic. Don’t rely on average measurements, use always percentiles. We usually look at the 95th percentile. If you are testing several instances, you need to separate the different measurements in charts per instance like; CPU, memory or GC. Be aware of cold starts. Sometimes, when you run your tests, your application is idle. This means connection pools might also be in idle, and if you just deployed the application, the VM probably haven’t optimized the code yet. In this scenario, you are most likely going experience spiky starts. Let’s go back to types of performance testing: Normally used to understand the upper limits of capacity within the system. This kind of test is done to determine the system’s robustness in terms of extreme load and helps to determine if the system will perform sufficiently if the current load goes well above the expected maximum. With this type of test, our goal was trying to find the upper limit of our platform. The idea wasn’t to see how our systems respond but rather to see when they start to fail. Because we use OpenShift, most of our applications/services support horizontal auto-scaling. This means that when we hit a configured limit in CPU or memory usage, a new container is added to the current amount of instances. From our experience and context, in this type of tests, applications fail because of: Incorrect configuration of the GC/Memory. Incorrect configuration of the connection pools/thread pools. Memory leaks. Incorrect configuration of stateful services behind this application, e.g.; databases. Our general approach to finding these things is to build a workload based on incremental iterations. We start with a small load, which increments every fixed amount of time. During this time we monitor CPU, Memory, GC and response time. One possible fail scenario is when you have the following services schema; A -> B -> C, and C takes to much to respond, so B or A are accumulating reqs. This could end-up with B or A out of memory or without connections/threads in their pools. For those cases, using any back-pressure solution avoids this situation. In this case, A or B were close to failing not because of the load but a poor design. This type of testing works by suddenly increasing or decreasing the load generated by a very large number of users, and observing the behaviour of the system. The goal is to determine whether performance will suffer, the system will fail, or it will be able to handle dramatic changes in load. This type of test is very useful for measuring how your platform is going to react in scenarios where you cannot gradually rollout your application. We called this a Big Bang Deploy, you go from nothing to full load in an instant. Another scenario is when you have an event in your platform like Cyber Mondays, Black Fridays or a massive push notification. In those cases, a huge volume of users is going to hit your platform in a short period of time. For specific known dates, it’s common to over-provision the infrastructure one day before. But for other scenarios you rely on the responsiveness of your relasticity, e.g.; How much time does it take to create a new pod in your OpenShift/K8s cluster? A possible fail scenario could be when it takes too much time to allocate a new pod, meanwhile, current pods are handling the extra load. If this extra load is too high, the current pods could start failing because they hit memory/CPU limits, causing a chain reaction. If one fails, the rest are going to be handling an even bigger load. Usually done to determine if the system can sustain the continuous expected load. During soak tests, memory utilization is monitored to detect potential leaks. Also important, but often overlooked is performance degradation. It essentially involves applying a significant load to a system for an extended, significant period of time. Soak testing is a very good tool to test asynchronous systems and our platform heavily relies on event-buses with asynchronous communications. So, for us, it’s really important to check message delivery and message order. The idea is to introduce a load, similar to what we expect to have in production, to verify that at the end we didn’t lose any messages and all the expected asynchronous actions were completed. Sometimes, inconsistent states show up after prolonged periods of time. A load test is usually conducted to understand the behaviour of the system under a specific expected load. This test will give out the response times of all the important business critical transactions. The important part of this type of test is to check that the responsiveness of the system keeps constant until the point of overload. In some aspects it’s similar to a stress test, the difference is; in a stress test you are trying to find the upper limit, and here the limit is not very important. What’s important is to see if your application scales well, and for that purpose, if your application is stateless, you usually don’t need to test several instances of your application. Testing just one instance should be enough. Sometimes a bad configuration of the GC, thread pools, connection pools or even locking can lead to increments of response time under heavy loads. You usually look for constant response times until the point you reach 100% of CPU/memory usage. If not, it’s a sign of a bottleneck inside your application. As I mentioned before when you build your performance test setup, it’s important to take into consideration how your users behave and your APIs works in order to define the type of workload for your tests. There are two main types of approach: Closed system model: New requests are only triggered by the completion of previous ones. In this type of model, it is assumed that there a fixed number of users and each one waits for the responses before making new requests. Open system model: New requests arrive independently of completions. Each user is assumed to make a request to the system, wait to receive the response, and then leave. The number of users queued or running at the system at any time may range from zero to infinity. The differentiating feature of an open system is that a request completion does not trigger a new request: a new request is only triggered by a new user arrival. Knowing this, we can divide load-testing tools into the following categories: Virtual Users: It’s a thread-based approach trying to simulate N users working together. Each thread will wait for a response before sending another request, so the request rate depends on the response rate. Request Per Second: It’s a hit-based approach trying to produce N hits per second. It will attempt to create a defined request rate, regardless of the response times, meaning there is no feedback. Open versus closed: A cautionary tale Bianca Schroeder, Adam Wierman and Mor Harchol-Balter Proceedings of NSDI, 2006. For our platform migration, given most of the flow works differently, only in a few scenarios we can use a Req/Sec workload ( Open Model ). In most of the cases, the only constant that we know is the amount of users we currently have. So, using a Closed model is going to fit better for our tests. We could potentially use an Open model for every case, but we are going to end up throwing an educated guess of numbers of expected load. Also, we need to be careful about setting a very high limit. Oversizing the infrastructure is expensive and sometimes not that easy to shrink, e.g.: changing the node type of your database without downtime. Software performance testing should be under every engineer’s radar. It’s very useful to categorize the scenarios and understand; What to measure? and the right approach to do it. Otherwise, you could potentially end-up wasting time and resources on building a test bigger than your needs. We operate a network of online trading platforms in over 40… 451 Thanks to Martín Di Doménico . Performance Testing 451 claps 451 Written by Follower of the blasphemous Scala programing language and the dark arts of JavaScript We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Follower of the blasphemous Scala programing language and the dark arts of JavaScript We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-12-14"},
{"website": "Olx", "title": "how to get a different las vegas experience aws re invent 2018", "author": ["Jochem van Grondelle"], "link": "https://tech.olx.com/how-to-get-a-different-las-vegas-experience-aws-re-invent-2018-11bfdf93498c", "abstract": "About OLX Group Gambling, partying and getting married to a stranger: that is what most people end up doing while visiting Las Vegas. But not last week — when Amazon Web Services hosted their annual global tech conference Re:Invent 2018 . Thanks to a countless number of amazing sessions and partner meetings, this was the ultimate time and place to see Las Vegas from another side. OLX Group was represented by a balanced mix of 25 people from different regions (Europe, Latam, India, MEA), roles (engineers and managers from software, infra to data) and different teams of our organisation (product, tech and finance), resulting in a great division of learnings across the wide AWS range of products. Most of OLX’s enormous infrastructure runs with nearly all available AWS products on AWS infrastructure, so this was the perfect opportunity to not only learn more using deep-dive sessions, but also meet with our partners and internal AWS product teams in so-called EBC sessions. We can recommend everyone to go next year — and looking forward to the summary of next year’s nominees to visit Re:Invent. If you want to know more about OLX’s opportunities and contribute towards our huge AWS stack, visit our Career site at https://www.olxgroup.com/. Together with all OLX participants, we made a summary of all major announcements (new products or major features) and compiled a list of the highlights, major blog posts and recordings that are worthwhile watching, depending on your own interests. So, to the summary! New product announcements summary — https://aws.amazon.com/blogs/aws/aws-previews-and-pre-announcements-at-reinvent-2018-andy-jassy-keynote/ Full summary by AWS — https://aws.amazon.com/new/reinvent/ Keynote Andy Jassy — https://www.youtube.com/watch?v=ZOIkOnW640A Personalize — https://www.youtube.com/watch?v=9tArgQrJBzE&t=0s&index=55&list=PLhr1KZpdzukeXZMkUXh2kGlWvzXakRh-a Private Marketplace — https://www.youtube.com/watch?v=niVsU9Tcp6E&t=0s&index=57&list=PLhr1KZpdzukeXZMkUXh2kGlWvzXakRh-a Textract — https://www.youtube.com/watch?v=PHX7q4pMGbo&t=0s&index=60&list=PLhr1KZpdzukeXZMkUXh2kGlWvzXakRh-a Sagemaker Ground Truth — https://www.youtube.com/watch?v=gjiozYXHKc8&t=0s&index=61&list=PLhr1KZpdzukeXZMkUXh2kGlWvzXakRh-a Timestream — https://www.youtube.com/watch?v=oTPpIyXoE3k&t=0s&index=62&list=PLhr1KZpdzukeXZMkUXh2kGlWvzXakRh-a Forecast — https://www.youtube.com/watch?v=_-2E8iDEqFY&t=0s&index=63&list=PLhr1KZpdzukeXZMkUXh2kGlWvzXakRh-a Amazon Sagemaker RL — https://www.youtube.com/watch?v=6skqe2IuI34&t=0s&index=64&list=PLhr1KZpdzukeXZMkUXh2kGlWvzXakRh-a Sagemaker Neo — https://www.youtube.com/watch?v=RjhrugELYW8&t=0s&index=68&list=PLhr1KZpdzukeXZMkUXh2kGlWvzXakRh-a Amazon Elastic Inference — https://www.youtube.com/watch?v=dZ5FLzOIQF0&t=0s&index=65&list=PLhr1KZpdzukeXZMkUXh2kGlWvzXakRh-a Lake Formation — https://www.youtube.com/watch?v=uVF73MXYay8&t=0s&index=66&list=PLhr1KZpdzukeXZMkUXh2kGlWvzXakRh-a Managed Kafka — aka AMS. Managed Kafka (including Zookeeper) cluster. Public preview. Info: https://aws.amazon.com/msk/ Keynote — https://www.youtube.com/watch?v=femopq3JWJg Aurora: 08:20 DynamoDB: 21:08 Redshift: 46:00 What’s new — https://www.youtube.com/watch?v=2WG01wJIGSQ Deep Dive MySQL — https://www.youtube.com/watch?v=U42mC_iKSBg Deep Dive Postgres — https://www.youtube.com/watch?v=3PshvYmTv9M Aurora Serverless — https://www.youtube.com/watch?v=4DqNk7ZTYjA Announced: Plan for Aurora PostgreSQL serverless EDP automatic calculation live starting in January (we currently have a semi-automatic). Provided beta testing possibility to automatic allocation of RI purchase across multiple accounts. The possibility of umbrella agreements and separate billing. New Cost Explorer tool that will have RI proposals, separated by services and accounts. IAM view on Cost Explorer (to be split by teams if necessary). What’s new: https://aws.amazon.com/containers/new/ Announcements: New service AWS App mesh (preview): https://aws.amazon.com/app-mesh/ Support for dynamic admission controller on EKS (istio w/ automated sidecar injection): https://aws.amazon.com/about-aws/whats-new/2018/10/amazon-eks-enables-support-for-kubernetes-dynamic-admission-cont/ Since August 2018 EKS supports horizontal pod autoscaler: https://aws.amazon.com/about-aws/whats-new/2018/08/introducing-amazon-eks-platform-version-2/ AWS Cloudmap is managed service discovery: https://aws.amazon.com/about-aws/whats-new/2018/11/aws-fargate-and-amazon-ecs-now-integrate-with-aws-cloud-map/ ALB ingress controller is now officially supported: https://aws.amazon.com/blogs/opensource/kubernetes-ingress-aws-alb-ingress-controller/ Amazon EKS now supports additional VPC CIDR blocks: https://aws.amazon.com/about-aws/whats-new/2018/10/amazon-eks-now-supports-additional-vpc-cidr-blocks/ Useful resources shared in sessions: Comparison between IAM credential management solutions inside kubernetes (including kube2iam): https://docs.google.com/document/d/1rn-v2TNH9k4Oz-VuaueP77ANE5p-5Ua89obK2JaArfg/edit?usp=drivesdk (shorter version presented at the conference: https://image.slidesharecdn.com/mastering-kubernetes-on-aws-c-be20775a-4386-499e-afa8-f3a814ab90ce-203832051-181128051531/95/mastering-kubernetes-on-aws-con301r1-aws-reinvent-2018-30-638.jpg?cb=1543382148 ) Logging architecture suggested for EKS: EFK (elasticsearch, fluentd, kibana) or cloudwatch logs instead of elasticsearch: https://image.slidesharecdn.com/mastering-kubernetes-on-aws-c-be20775a-4386-499e-afa8-f3a814ab90ce-203832051-181128051531/95/mastering-kubernetes-on-aws-con301r1-aws-reinvent-2018-33-638.jpg?cb=1543382148 Highlights: Announcing Amazon DynamoDB Support for Transactions Introducing DynamoDB On-Demand All videos — https://aws.amazon.com/blogs/database/amazon-dynamodb-related-content-from-aws-reinvent-2018/ Announced: A1 class with Amazon’s ARM CPU — https://aws.amazon.com/about-aws/whats-new/2018/11/introducing-amazon-ec2-a1-instances/ C5n class featuring 100Gbps — https://aws.amazon.com/about-aws/whats-new/2018/11/introducing-amazon-ec2-c5n-instances/ What’s new in EC2, Containers & Serverless — https://www.youtube.com/watch?v=cb0KvqGjXRE Interesting session about network-level optimization on EC2 (also covers the evolution of network throughput through previous instance generations until the current ones): https://www.youtube.com/watch?v=DWiwuYtIgu0 Deep dive — https://www.youtube.com/watch?v=QxcB53mL_oA Amazon Managed Streaming for Kafka — https://www.youtube.com/watch?v=zhsVfsykBHc Under the Hood — https://www.youtube.com/watch?v=QdzV04T_kec Recent updates: Layers (sharing common project between functions) Ruby support Custom runtimes (any executable implementing API) Application load balancer support (exec function from ALB) — https://aws.amazon.com/about-aws/whats-new/2018/11/alb-can-now-invoke-lambda-functions-to-serve-https-requests/ More: https://aws.amazon.com/about-aws/whats-new/2018/11/aws-lambda-now-supports-custom-runtimes-and-layers/ A good wrap up of the announcements in one article: https://www.businesswire.com/news/home/20181128005660/en/Amazon-Web-Services-Announces-13-New-Machine https://aws.amazon.com/about-aws/whats-new/2018/11/amazon-translate-now-supports-customized-translations/ Useful resources: Very good intro level session about deep learning: Deep Learning for Developers: An Introduction, Featuring Samsung SDS (AIM301-R): https://www.youtube.com/watch?v=4SJBcZQX8uA Running tensorflow on AWS Sagemaker (including a few cost saving tips, like running local training and the pipe mode, which allows to stream large amounts of data from S3 into the training instances): https://www.youtube.com/watch?v=uu-VKHlv0_E Amazon ML Solutions lab, an offering to work together to fine tune our needs to what AWS has to offer (comprises not just services, but education and training as well): https://aws.amazon.com/ml-solutions-lab/ Some interesting services (some were announced at reinvent, some were already available): Elastic Inference — https://aws.amazon.com/machine-learning/elastic-inference/ (available, can be useful for saving inference costs on both EC2 and sagemaker — https://aws.amazon.com/blogs/aws/amazon-elastic-inference-gpu-powered-deep-learning-inference-acceleration/ ) Comprehend — https://aws.amazon.com/comprehend/ (available, roughly NLP as a service) Rekognition — https://aws.amazon.com/rekognition/ (available, image/video analysis, can be used for moderation and face/object/sentiment detection) Textract — https://aws.amazon.com/textract/?hp=r (in preview, roughly OCR as a service) Personalize — https://aws.amazon.com/personalize/?hp=r (in preview, roughly recommendations as a service) Forecast — https://aws.amazon.com/forecast/?hp=r (in preview, can be useful for financial/budget forecasting) Sagemaker — https://aws.amazon.com/sagemaker/groundtruth/?hp=r (available, useful for “automating” dataset labeling — higher label confidence is fully automated, lower confidence is sent to a human workforce for manual labeling — for later usage in training) Inferentia — https://aws.amazon.com/machine-learning/inferentia/?hp=r (ASIC board supporting TensorFlow, Apache MXNet, and PyTorch deep learning frameworks, as well as models that use the ONNX format.) Links to ML hands-on workshops performed at reinvent: Sagemaker Keras text classification https://github.com/aws-samples/amazon-sagemaker-keras-text-classification Darwaishx Media Library https://github.com/darwaishx/media-library This one is awesome! Comprises even a moderation example using Lambda step functions with Amazon Rekognition. What’s new / Coming up — https://youtu.be/MS1zleTPmA0?t=1020 Coming up soon: Elastic resize Automatic concurrency scaling for burst of user activity (read cluster replica) Longer term — also enables to read from one cluster to another cluster OLX is in Early Preview Unload to Parquet (expected Jan 2019) Automatic distribution Dynamic/Automatic WLM Auto analyse / auto vacuum Spectrum caching E.g. query count of events for the last 30 days (eq. 30 partitions). The next day if you would run the same query, it only calculates the new partitions, and caches the previous results Stored procedures (PL/SQL) Encrypt existing cluster with KMS on demand Integration with Lake Formation Recent updates: https://forums.aws.amazon.com/forum.jspa?forumID=155 https://docs.aws.amazon.com/redshift/latest/dg/doc-history.html https://docs.aws.amazon.com/redshift/latest/mgmt/document-history.html AWS Security Hub Introduction — https://www.youtube.com/watch?v=TdT8ds_C8Gs Useful resources: Interesting case by Intel, about moving a data stream pipeline from traditional kafka, akka and hadoop to fully serverless (kinesis, lambda, api gateway, cognito, nlb, fargate, s3) — https://www.youtube.com/watch?v=eCrxOA5XSgQ (from minute 26 on) Reference application on serverless monitoring — https://github.com/aws-samples/aws-iot-core-acmebots-monitoring What’s new in AWS Storage (S3, EFS, EBS, …) — https://www.youtube.com/watch?v=gidUa4lJd9Y AWS Transit Gateway & Transit VPCs, Ref Arch for Many VPCs — https://www.youtube.com/watch?v=ar6sLmJ45xs Happy learning! We operate a network of online trading platforms in over 40… 90 AWS Reinvent Redshift Ec2 Amazon Web Services 90 claps 90 Written by Data Engineering Manager @ OLX Europe We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Data Engineering Manager @ OLX Europe We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-12-06"},
{"website": "Olx", "title": "selecting a data storage how house m d would do that", "author": ["Maryna Cherniavska"], "link": "https://tech.olx.com/selecting-a-data-storage-how-house-m-d-would-do-that-4ae5a1f18544", "abstract": "About OLX Group This article takes its origin from a presentation, which I prepared for the OLX Product and Tech conference, that has taken place in Berlin in September 2018. I took the title slide for my featured image. The reason I chose this topic for my presentation is that we had a bit of a struggle, doing this for a new service. We had a lot of freedom in database selection. And as a rule, having choices is good because, well, you have options. But imagine that you want to select between three sorts of ice cream. You have chocolate, vanilla and strawberry — it is probably easy right? But what if I tell you that you have not three, but thirty sorts of ice cream to choose from? Our current database choices are in the second category. How the hell do you make a selection if legion is their name? This is where Uncle Bob comes to help. Uncle Bob, or Robert Martin, is a writer and a software engineer and one of the Agile Manifesto authors. He wrote Clean Code, Clean Coder, Clean Architecture and a few other books. His thoughts on the matter: database is an implementation detail. What it means is that users don’t care about how the data is stored or fetched. They don’t care how you query the data. They only care how the data is presented to them. Therefore, while you are doing the POC, or the MVP, or even in the later stages of the application development, the decision about the data storage can be delayed. How does one do that? Easy. By hiding the implementation behind the abstraction. Every developer heard this rule: program to the interface. The interface is the specification of WHAT you are going to do; the implementation is the HOW. So, when you define an interface, you declare WHAT it does and also the inputs and the outputs. The rest is up to the implementation. When you program like this, it means that changing the data storage solution that’s hidden behind the interface is just a question of replacing one implementation with another. Most often it will be just a few classes. The rest of the code can stay untouched. In this paradigm, the first implementation you pick should just be something you can implement as quickly and easily as possible. You can keep your data in flat files, in memory storage, anything. So, how Dr.House, or House M.D., fits into the picture? To explain that, we need to know the way he works. Dr. House is a diagnostician from a TV series. He is supposed to be brilliant because he can find out what is happening to patients with complicated diseases. And if you watch at least a few episodes of House MD, you will see his main method. Throw stuff at the wall and see if it sticks. Basically, what he does is he has a hypothesis about the patient, and he tries to prove it by giving the patient medicine that should work if this hypothesis is correct. If the patient gets better, then all is good. If the patient gets worse, he tries the next hypothesis. And he iterates. So, with the help of Robert Martin, we saw that the database implementations can be replaced. This means that the process of selecting the database can also be iterative. You try something, you see if it fits. If it does, you leave it as is. If it doesn’t, then you discard it and try something else. To go through the process, you might need a set of criteria. These are some example criteria you might have. Capability — how big a data this solution can work with; Query language — SQL is something a lot of people are familiar with; AWS compatibility — because we are mostly backed by the AWS stack; Development effort — how difficult it is to integrate this database into a Spring Boot application which we were sure to go with; Infra effort — how difficult it is to set up this data storage; Limitations — what the solution can’t do (and we need). Now we come to the practical example: the metadata service. The service we were implementing was supposed to be able to extract, store and provide the metadata about the files, kept in the system. Mostly those files are images. The system currently holds 1 billion files and more are upload daily, so it grows quite quickly. Each file can have 10–20 properties, which adds one more order of magnitude to the metadata. Key-value data storage. Fully AWS-managed, automatically scaled and backed up. Very fast reads due to in-memory cache. Dynamo DB allows a maximum of 5 global secondary indexes and 5 local secondary indexes per table. Primary key can be a partition key or partition key + sort key. You can query on partition key or partition key + sort key, but not sort key only (or, it can be done with a table scan). Why we discarded this option: we need to be able to query the data on different combinations of attributes. Dynamo DB allows the data to be queried on non-key attributes with the help of secondary indexes, but the secondary indexes are basically also tables, and they have the same limitations, that is, no more than 2 key attributes per index (partition + sort key). This leads to ugly workarounds when you need to query for more than 1 or 2 attributes: like creating extra columns that are a result of concatenation of the values of the columns you need to query on. This is not pretty, not flexible and not easy to support, because if the data was already there and you discovered the need for such a query, then you need to retrofit the data with the scripts. Since our service was only in its initial stages and we were still not sure how the data will be used, we decided that we don’t want this complexity. Self-managed — no ready-made AWS solution. Optimized for big volumes and fast writes. SQL-like queries (CQL). Used by big players: Netflix, Hulu, Instagram, Ebay… Cassandra is also a noSQL, key-value storage. Thus, it has a lot of the same characteristics as DynamoDB. And the main idea is, you need to design the schema very carefully. The articles about Cassandra often say “Cassandra table is a query”. Most of us come from a SQL world. Table is a table, it has rows and columns. Query is a statement you use to read some data from a table based on a few conditions. So, how can a table be a query? The answer is simple: it can’t. It is just an expression. What this expression means is is that data in Cassandra is best arranged as one query per table, and data is repeated amongst many tables, a process known as denormalization. Cassandra tables have primary keys which can be composite. First part of a PK is always the partition key (can contain more than 1 attribute). The rest are clustering attributes, they determine the order of data within a partition. The data should be queried by the key attributes in the same order as they are specified in the key. So, if you have keys 1–4 in the PK, then you can query by key 1, key 1 + key 2, key 1 + 2 + 3, key 1 + 2 + 3 + 4, but not key 1 + 3 or 2 + 4 or even 2 + 3 (because the first is a partitioning one and to query without a partitioning key you need to allow filtering). Cassandra allows secondary indexes but they are best not used. Reason is, data is physically arranged by the partition key, so all the secondary indexes are local to the partition. Which means all of them should be scanned to get the data and then the results merged together. This is why secondary indexes aren’t efficient. Another characteristic of Cassandra is that it’s pretty opinionated in how it wants to be used . So for example, where another database would just let you run a less-then-efficient query and deal with the performance problems, Cassandra will just will raise an error. So, the main reasons we didn’t go Cassandra were complicated infrastructure (completely self-managed solution) and complicated schema design. AWS-managed, automatically scalable, automated backups, point-in-time restore. We already use it for other services. It is MySQL and PostgreSQL compatible. Simple SQL for querying. One big advantage: As with all relational databases, we can provide a normalized schema and think about which queries we need later. The main reason why it was not our first choice is, as we are already using it for another service, we know that on our data size, some queries don’t perform that well. And potentially, the metadata storage would have much more data. On the other hand: 1 — we are only working to get one type of metadata into the API right now, so it won’t be that huge. 2 — initially, we will only have that data for one category of the files. 3 — and that data will not have to be extracted for all the images of the category, but just for some of them. This means that we can apply the YAGNI principle right now: you ain’t gonna need it . Choose the simplest solution that satisfies current use cases. And as we said before — hide it behind an interface to maybe change later. Takeaway of this whole article is that is is possible to delay the final database selection and so make it less critical and maybe a bit less painful. Article originally published on my personal blog at https://mchernyavska.wordpress.com/2018/11/03/selecting-a-data-storage-how-house-m-d-would-do-that/ We operate a network of online trading platforms in over 40… 18 Programming Database Design 18 claps 18 Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-11-06"},
{"website": "Olx", "title": "understanding language through search", "author": ["Vaibhav Sharma"], "link": "https://tech.olx.com/understanding-language-through-search-78ea37be7ac8", "abstract": "About OLX Group Have you ever wondered how e-commerce platforms know that you might also be interested in a ‘Couch’ when you are searching for a ‘Sofa’ or that when you search for an ‘iFone’ it can figure out that what you actually want are ‘iPhones’? This blog post will take you through how we at OLX built a solution to address these problems. We are the biggest internet company you have never heard of ! The OLX Group is a network of leading classifieds platforms in 45 markets, encompassing such brands as OLX, Avito, Letgo, and Dubizzle. We connect communities and allow them to buy, sell or exchange new or used goods and services through the creation of a hyper-local digital marketplace. On our marketplace users can find almost everything they can think of including phones, pets, real estate, and services on our platform; you name it, we have it. Unlike other e-commerce platforms, every item on our platform is unique, we have no stock, no warehouses only people’s garages. Our marketplaces experience more than 1.7 billion monthly visits, 37 billion monthly page views and 54 million listings each month. But how do you serve millions of users searching over billions of unique items and ensure that the best item is returned to each one of them? How do you make a person selling couches meet a buyer searching for sofas? Our solution was to leverage the power of Machine Learning in order to make our search more powerful! Our objective was to develop a language agnostic solution to improve our search that can be easily deployed worldwide. To start with we chose the country of South Africa as our testbed. We had the following market characteristics in mind while designing our solution: There are two primary languages on our platform, English and Afrikaans. Local dialects and wide ranges in education levels meant that many different words and spellings of their words could be used to refer to an item. Our users have a wide variety of intents when coming to the site, some are searching for inspiration while others have a specific item in mind. We had to make sure that our improved search takes into account the constraints mentioned above, and at the same time provides the user with a better search experience. The first milestone in our journey was to incorporate state of the art machine learning techniques in our search platform while making sure that the solution is valid for South African market, and we indeed improved the user search experience by engaging more users per day and reducing the null searches (searches with zero results) by 27% . To provide our users with the items they are looking for, we had to be one step ahead. The search algorithm had to be more than just matching the user queries with items as that might not always include enough results. In order to solve this, we adopted an approach to find synonyms, meaning queries that generate highly similar results and that can be used to provide better results for the user queries through simple text matching. Our problem was then reduced to ‘Expand’ a search query to its multiple synonyms and use these synonyms along with the original query to generate results. The problem of query expansion has been explored many times in the past. However, our plan was to adopt an approach that was simple, fast, language agnostic and gave us the ability to iterate fast and see results. This led us to use the technique of Query2vec . Initial steps We had at our disposal a lot of anonymized data, available from our users’ interactions with our Marketplace such as search behavior and browsing patterns. We grouped our users’ activity into sessions based on what events occurred temporally close to each other. The typical user session consists of the following: Searching for an item( ex: Car). If it does not give desired results, refining the search query and making it more specific (Ex: Toyota corolla green). Checking out multiple ad results for the search query. Engage in conversation with the seller for the product of choice. A demo session for the purpose of understanding can be thought of like the one below: Intuition Looking at these sessions, one can easily realize that ‘toyota car for swop’ ad is a valid result for ‘old cars’ as user showed interest in the same. They are related as they belong to the same session. Similarly, ‘old vehicles’ and ‘old cars’ are related and have similar meaning even when it is hard to identify the similarity based on the text. For the purpose of query expansion/synonym generation, we saw that it was easy to identify searches that are very similar in meaning if they occur in the same context in a session. Approach The approach adopted to go from searches with same context in sessions to synonyms was to create n-dimensional embeddings of each search query such that the embeddings capture the semantic relationship among queries. This helped us find queries that occur in the same context by searching for the ones that are similar in the embedding space. The process followed to do the same is summarized below: Collect sessions of query chains: For the purpose of generating synonyms, not every searched query is important. There are a lot of absurd queries that occur only once. To keep the volume of data limited and guarantee that we obtain high quality embeddings for every query, we decided to take the queries that were quite frequent. Our analysis led us to use only those queries that occurred more than an empirical threshold in our data. Some examples of real sessions that we had in our data are shown below: Rest of the queries that had the frequency less than the empirical threshold were handled separately and would be referred to as tail queries. The embeddings for the tail queries were obtained using the embeddings of head queries (queries used for training). Our dataset had head queries that made up 25% of all unique queries and 75% of all queries. The next step was to create n-dimensional embeddings of search queries and this process in implementation is handled via a training technique similar to the training of Word2vec . We chose the skip-gram version of Word2vec for our problem and the sessions of query chains were fed as sentences to the model. The skip gram version takes training samples that have target word as input and it learns to predict the context as shown below: In order to inspect the learned embeddings ,we visualize the embeddings using t-SNE for dimensionality reduction. As expected, the queries that occur in same context have embeddings that are similar. We used cosine similarity as a similarity measure and similar queries have less cosine distance. For example, ‘sofa’ and ‘couch’ are semantically similar and are closer in the visualized embedding space. Model details: Multiple models were trained using Tensorflow and Gensim libraries. The models were selected based on the accuracy and the time it took to train them. Following are the details of the model that we finally selected for our problem: Number of epochs : 500 Embedding size : 50 Window size : 2 Sampling with 5 negative samples. For generating synonyms of a query cosine similarity was used. For any query, we just find its similarity score with other embeddings in the same space and then the closest neighbors are chosen as synonyms. Only the synonyms that have a similarity score greater than a chosen threshold, were selected as closest neighbors. Not everything that trains well, performs well! After continuous training for days, the training NCE loss of our model was quite low and the model produced accurate synonyms for our validation set. However, the initial deployed solution resulted in some users finding ‘shoes’ when searching for ‘clothes’. We found out that for our model it was hard to identify that ‘shoes’ and ‘clothes’ are not synonyms. At that moment, we realized the need of a good evaluation process for our model. Evaluation/Choosing model The process of evaluation was to create a set of evaluation pairs wherein each query had 5 different synonyms and those synonyms were later judged manually to check their quality. The queries to generate evaluation pairs, chosen based on different parameters including recall and popularity, consisted of the following: High-frequency queries (Being sure that we do well on obvious things). Some randomly sampled queries. Low recall queries (Make sure we do well on null queries). Low frequency queries (queries that model didn't see often). Frequency-based subset- 50 queries with an average frequency. After creating the extensive evaluation set, the next steps were to check the output of different models on the evaluation queries. The task would have been tedious had it not been for the query-synonym evaluations done manually. Manual Evaluations A baseline model was selected to create query-synonym pairs for each query from the evaluation set. For each query five synonyms with different similarity scores( scores ranging from 0.9 to 0.4) were considered from the baseline model in order to create evaluation pairs. This made our evaluation set to be an extensive and exhaustive list of around 5K query-synonym pairs. This set was then used to create a form for Amazon Mechanical Turk , wherein the users were asked to assign a label to each query-synonym pair as follows: Same queries/ Label 2 : The synonym and original query mean the same thing. The two can be presented to the user in the same list. Ex: ‘Couch’ and ‘Sofa’. Related queries / Label 1 : The synonym and original query are related but still different. The user would not want them in the same list. ie: ‘toyota’ is related to ‘toyota corolla’. However, ‘Hyundai engine’ is not related to ‘Toyota tyres’ (even though they are car parts). At least one model, brand, etc., has to match. Not related / Label 0 : Both synonym and original query are totally unrelated and belong to a different category. Which model is best? After assigning labels to each query-synonym, the way to evaluate models was to analyze the similarities that different models assign to these pairs. For pairs with label 2, the ideal model should assign a high similarity score whereas for label 0 the similarity score should be low. The more the difference in the similarity score for label 0 and label 2 by a model, the better it is. Along with the model with user searches, we also had the information like Id and category of the item that user clicked on. As the searches that lead to the similar items are usually similar, we planned to use item information for creating sessions in one of the variants of our model. Adding category information along with the Id of items made up another variant and gave improvements over our baseline model with only searches. Below are the similarity distribution graphs for three models: Baseline model, Model trained on data with items, Model trained on data with items and their category information. It can be seen that the data with more information about the items searched is better able to separate Label 2 and Label 0 pairs. We also went ahead to analyze the average similarity score that different model assigned to each label. The difference between the average similarity score for each label was taken to be an important metric in choosing the best model. This metric for our models is shown below: As the model with category and items data was able to differentiate better in the labels. Hence, this model was chosen to generate synonyms for our use case. The results were in line with what we expected, the synonyms generated by our model and the original query pointed to the same thing in the real world. As shown below, we try to suggest different options of vapes for the people trying to quit smoking and switch to e-cigarettes! For our animal loving users who are searching for ‘Krimpvarkie’ ( Afrikaans for hedgehog) in South Africa, we return the results for Hedgehog. Our model was able to, without human intervention, learn the Afrikaans and English words for common searches solely through analyzing user search behavior. The model provides synonyms for searches that actually point to the same items without applying any NLP on search tokens or extracting any information from the item images. Through our model, we can also understand incomplete queries that would otherwise require an autocomplete to generate results. The A/B test result indicated that not only were we showing more results to our users but also the relevant ones. This resulted in an increase in key performance metric and a significant improvement in the number of searches with more than 60 results. Null Search rate : decreased by 27% Key performance metric: increased by 14% 60+ item search results: increased by 22% This work would not have been possible without Vladan Radosavljevic , Colin Smith and Johannes Braun who made sure that the project keeps moving forward and is delivered to our users. Special thanks to the team- Mariano Semelman , Manish Saraswat and the Personalization & relevance team at OLX, that actually worked on this project and made it a success. Now, that we have laid down the foundation of the project and have seen significant improvements with it. We have lots of interesting tasks to explore and learn. Some of the potential tasks that we might want to pick for research in the next iteration would be: Including browsed locations into sessions in order to have related locations, and find items in a particular location. Using similarity score for ranking ads. A solution to handle tail queries (queries without embeddings), this would probably be covered in the next part of the blog. We operate a network of online trading platforms in over 40… 1K 2 Machine Learning Query Understanding Word2vec 1K claps 1K 2 Written by Data Science @OLX, Polyglot, Practicing the dark arts of programming in the muggle world. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Data Science @OLX, Polyglot, Practicing the dark arts of programming in the muggle world. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-11-05"},
{"website": "Olx", "title": "migrating an android sdk to kotlin in 8h", "author": ["Cristiano Madeira"], "link": "https://tech.olx.com/migrating-an-android-sdk-to-kotlin-in-8h-42a610bcedc5", "abstract": "About OLX Group Here at OLX, we’re always experimenting and trying to improve our way of working. This time we’ve decided to do our very own version of the FedEx Day or ShipIt day with members of different teams in one of our tech hubs. But, instead of a 24h event, we wanted to see what we could come up with in our regular business hours. There was no contest. Because everybody wins! And since developing SDKs for mobile platforms is the main focus of some of the developers participating in this hackathon, we decided to show our ❤️ for Kotlin by migrating one of our java projects to the beloved programming language. The story begins with two developers with basic Kotlin knowledge working on this migration. We chose medium size project with 173 KB, 5104 LoCs, and structured using Clean Architecture with a different module for each layer. Pure Java module containing classes for entities and use cases . Android module containing classes related to network and database . Android module containing the public methods of the SDK. Also, all layers had unit tests, and the data layer had some integration tests for the database. Our goal was to migrate 100% until the end of the day using the Android Studio’s J2K conversion feature and doing the necessary improvements. Due to time constraints, we decided to keep it simple and not go wild with the refactoring. When you start migrating, the first question that comes to mind is where I should start? Our answer to this question was, let’s start low, small and simple . Since we planned to convert the whole project at once, we didn’t have to deal with all the Java interoperability hassle. Therefore, the best strategy to avoid that was to start from the lower layers of the architecture and go up through the upper layers. In our case, the sequence was Domain layer > Data Layer > Presentation Layer. This way we had only Java code calling Kotlin until the end of the migration. When you’re learning a new language, it’s easier if you start migrating the small classes first. This way you can better follow the language documentation and start applying the basics. In our case, we started with entities, interfaces, enums, constants and util classes. If you start migrating the most complex classes first, it can be overwhelming and error-prone. So it’s better to start simple and move on to the complex ones once you feel more comfortable with the language. You don’t have to this manually, Android Studio can do this for you. Go to Tools > Kotlin > Configure Kotlin in Project . Android Studio can automatically convert the files for you if you go to Code > Convert Java File to Kotlin file . This feature does a pretty good job, but the conversion is not very idiomatic and you can end up with some verbose Kotlin code. You don’t have to convert the files one by one. It’s possible to select several files and convert them all at once, though I don’t recommend doing that because you’re going to get lost in all the issues that will come up. I do recommend converting file by file or a small group of files that are closely related. If you have for example an entity class with multiple constructors: The J2K converter will give you: This doesn’t look good. You can make this look much better using primary constructors and default parameter values : This way you can also choose to omit the parameters that already have a default value when you instantiate this class: Or even use one of the optional parameters with named arguments : But there is still some room for improvement in our example. Since this is a class that only holds data, Kotlin has a special type of class called data class . For this type of classes, the compiler automatically generates all the necessary methods to deal with data like equals() , hashCode() , toString() and copy() . Then our example becomes: But there is still one thing missing for our example to be perfect. Kotlin aims to eliminate all Null Pointer Exceptions (NPEs) from your code by making types Non-Null by default. You can still make your types nullable using the ? and !! operators. But if you want your application to be safer, you're going to want to remove as many as possible nullable types. Depending on the size and complexity of our code this can be a lot of work, but be brave. We were able to remove 90% of nullable objects from our SDK. Now our example is finally complete: After the conversion, some of the Java APIs still remains in your code. One good example is the use of Java Collections API . Meaning, all your ArrayLists will still be there. But you don’t have to stick with that, because Kotlin has a much powerful Collections API , and by using it you can not only save a few lines of code, but also make your code much more readable and elegant. For example, if you want to filter the even numbers from a list in Java, the code will be something like this: By using Kotlin Collections API, you can do the same with: You can also make your tests more readable by using lower case and backticks for test method names. If you’re using Mockito , after converting your tests you will probably see some of them failing. Those are the two issues we faced: That happens because classes in Kotlin are final by default and Mockito can’t mock a final class. You can fix that by either changing the visibility of the class that is being mocked to open or mocking the interface that this class is implementing. That happens because myMethodName() only takes Non-Null types parameters and if you check any() and anyObject() implementation, they both return null. You can fix that by using this class provided by Google in the Android Architecture repository , MockK or Mockito Kotlin . You can also choose to change the method's parameters to nullable. But please, don't do that! By the end of the day, we had our SDK fully functional with all tests passing. 🎉 Before the migration our SDK's code base had 5104 LoCs, now we have 4263 LoCs. Not bad. 🙂 Before the migration the total size of our SDK's .aar files was 173 KB, now is 264 KB. That’s due to the overhead of both Kotlin runtime and stdlib. Considering all the advantages of Kotlin over Java and the amount of development time that Kotlin will save us from now on. I would say that having 91 more KB is a fair trade-off. 🤔 There is still more room for improvement if we take more advantage of Kotlin’s features. Challenge accepted! 💪 Taking a day off to do a Hackathon event is definitely worth it. You get to either try something new or do that improvement in your product that always wanted but never had time. And of course, have tons of fun and learnings in the process. We’re doing this again for sure! 😃 Special thanks to JB Lorenzo for being my partner in crime in this adventure and to Ayelen Chavez , Remerico Cruz , Roberto Santana and Marcos Ferreira for changing our way of work with this amazing Hackathon. We operate a network of online trading platforms in over 40… 232 Kotlin Android Sdk Migration 232 claps 232 Written by Mobile Software Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Mobile Software Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-11-13"},
{"website": "Olx", "title": "type microservice node js flow", "author": ["Facundo Viale"], "link": "https://tech.olx.com/type-microservice-node-js-flow-1cbdbb436123", "abstract": "About OLX Group N ode.js is a great platform for developing micro-services, specially if they are I/O intensive. However, evolving and growing a JavaScript codebase is notoriously challenging. For a lot of JavaScript developers this could sound a little bit strange, but I come from other languages like Scala and Java and there is something that I really miss; Types. This isn’t something new, nowadays Flow and TypeScript are getting very popular and both are supersets of JavaScript. We also have some big and popular projects using these supersets; VSCode and Angular are written in TypeScript and React in Flow. In the paper To type or not to type: quantifying detectable bugs in JavaScript , researchers found: On 19/08/2015, there were 3,910,969 closed bug reports in JavaScript projects on GitHub. We use this number to approximate the population. We set the confidence level and confidence interval to be 95% and 5%, respectively. The result shows that a sample of 384 bugs is sufficient for the experiment, which we rounded to 400 for convenience. Of the 400 public bugs we assessed, Flow successfully detects 59 of them. Running the binomial test on the results shows that, at the confidence level of 95%, the true percentage of detectable bugs for Flow falls into [11.5%, 18.5%] with mean 15%. All of this pushed us to start adopting types not only in the front-end but also in the back-end. Also, the fact that intelligent code completion and basic refactoring efforts (e.g., rename a symbol) are reliable, makes a huge impact on the process of writing and especially refactoring code. Like I mention before, Flow is a superset of JavaScript that gives you advantages like: Optional static typing : It’s easy to migrate an existing Node.js project without the need to re-write the whole project. Pure ECMAScript: Unlike TypeScript, Flow doesn’t introduce any custom feature to JS. It’s just types on top of JS. Here is a simple example of an application using Express and Flow: If you are interested in the cool features of Flow I recommend: Maybe Type Tuple Type Union Type Intersection Type Type Variance Unknown Property Access in Conditionals Opaque Type Aliases https://github.com/olx-global/TNT TNT it’s a tool for less boilerplate and an easy way to bootstrap a Node.js service with Flow. It works in a similar way than create-react-app , we provide an “app creation component” (still in progress), a custom preset and a “scripts” tool, with the common tasks; start, build, etc … The “tnt-scripts” works in a similar way to “react-scripts”. package.json The “build” task generates the application bundle in; build/app.js Underneath “tnt-scripts” use rollup.js with Babel 7 , jest and a custom preset with some very useful plugins, like: Class properties Decorators Object rest spread Optional chaining Dynamic Import Export namespace from You can also can add more features, if you want, by just adding plugins and/or presets in the .babelrc. We are going to be releasing version 1.0.0 version very soon! We operate a network of online trading platforms in over 40… 113 JavaScript Flow Typescript Microservices Expressjs 113 claps 113 Written by Follower of the blasphemous Scala programing language and the dark arts of JavaScript We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Follower of the blasphemous Scala programing language and the dark arts of JavaScript We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-10-19"},
{"website": "Olx", "title": "qualifying image quality part 1 cropped images", "author": ["Akash Gupta"], "link": "https://tech.olx.com/qualifying-image-quality-part-1-cropped-images-27bd7c3ef949", "abstract": "About OLX Group OLX is a platform where buyers meet sellers. Sellers post advertisements of the stuff which they want to sell and the buyers contact them if they find their ads interesting. An advertisement from a seller has content about the item on sale like its images, its price, description and title. The title and description tell more about the condition and variant of the product, the price tells how good a deal it is, but it is the images which tell how attractive the item is and how much attention it deserves. Thousands of Ads are posted on OLX each day. Most of the ads have a cover image which is usually the first thing a customer notices about the advertisement. Images play an important role in classifieds as they influence whether the customer would want to go to the ad-detail page as well as whether he or she would want to interact with the seller or not. Let’s say our average Joe posts an advertisement on OLX to sell his old car. He fixes a price and uploads some images for the car. However, his images of the car look like this : He waits for the buyers to come and connect with him. However, they don’t and the few buyers he gets are offering much less than his expectation. Our innocent Joe does not have any idea why there are no takers for his offering. Apparently, the buyers see the image and ignore it and don’t even try to connect with him. A small research conducted internally which studied the effect of various ad- related parameters on clicks found out that: Users are 20% less likely to go to the ad detail page if the cover image of the advertisement does not contain the item in full or has poor brightness/contrast. Here are some examples of poorly cropped images : And here are some examples of non-cropped images: In our quest to keep solving our customers’ problems, we started to work on the said problem and providing the feedback on image quality to our innocent sellers. After all, more ad clicks mean better chances of selling and getting the best price for your item. Happy seller means happy OLX. To tackle the first problem of detecting whether the full object is in the image or not, i.e. whether the image has been wrongly cropped or not, we need a smart solution able to identify the main object being pictured, as well as its precise location in the frame . Quite a lot of work has been done on object detection and localization where multiple objects in the image can be identified. RCNN , Fast-RCNN , Faster-RCNN are bounding box proposal networks which propose many bounding boxes in an image and try to classify the objects in the boxes. They learn to minimise the error so well that the box fits the object precisely and the object is correctly classified. Mask-RCNN takes it a step forward by trying to detect the boundaries around objects. We have used object detection models in combination with image processing algorithms and traditional machine learning models to get to the final output. We built the initial version of our algorithm around cars as that is the most important category for us at OLX. We go through a pictorial step by step run of our algorithm on one of the more complicated examples. We start with a colour image of a car. We first get the image through a Mask-RCNN model to get the approximate masks of all the objects present in the image. Mask-RCNN is a state-of-the-art object localization model which is used to localize the objects in an image and it also tries to form the masks around those objects. Underneath it uses Convolution Neural Networks to classify the objects and form the boundaries. We used a pre-trained Mask-RCNN model on the COCO-dataset . Let’s see MaskRCNN in action on our image. One of the issues with MaskRCNN is that the masks are not pixel perfect especially near the edges . Our use case requires finding exact position/pixels of the car so that is a big drawback. In this example just taking the masks from the MaskRCNN would have also led us to believe that the car is well within the boundaries, but it is not quite true. Next, we try to find the exact boundaries of the objects. For this, we use Deeplab which is a state-of-the-art image segmentation model which produces pixel level classification. It classifies each pixel into one of the object classes using information from the surrounding pixels as well as the overall image. The result is a pixel-perfect segmentation map of the image . Again, we used a pre-trained network on the COCO-dataset . Let’s see Deeplab in action on our image. The only drawback of using this model is that it does not differentiate between multiple objects of the same class, i.e. two cars standing next to each other would be catered by the same mask. So when multiple cars are present in a frame, we will not get the mask of the main car being photographed. To harness the best parts of both models and to overcome the drawbacks, we combined these models. The MaskRCNN model is used to calculate the areas of the multiple cars in the image and the car with the largest area is labelled as the main car while others are labelled as background cars. The masks of background cars, as obtained from MaskRCNN model are added together to form the complete background car mask. The background car mask is subtracted from the image segmentation mask obtained from the deeplab model. Now we are left with an almost perfect mask of the main car with some connected tails and disconnected islands left from the subtraction. To remove these we apply gaussian blur to the image, which basically averages each pixel by its neighbours helping in disconnecting the fine tails. Then we find connected components in the Mask and the largest connected component is retained leaving out the smaller ones. The final output is the mask of the car without any deformations and capturing the boundaries correctly. Now as we are done with calculating the mask of the car, we need to use it to identify whether its a cropped car or not. To do this, we use machine learning to avoid having rules such as: If distance from left edge > 10 pixel => Good Car, else Bad Car The rules have maintenance issues as well as imposing a hard limit can result in erroneous predictions in case of small inaccuracies from the previous mask making model. So we prepare a dataset of hand labelled cropped and un-cropped images and features to identify the cropped images from the un-cropped ones. We don’t need a big dataset here, as we extract only a few features whose parameters we need to learn. The Features we extract are: Distance of rightmost mask pixel from the right edge, and similar distances from left, top and bottom. Sum of the pixels on each edge in the mask The output variable is whether the car is cropped or not . We trained a GradientBoostedDecisionTree model to learn the differences between the cropped and non-cropped images in the feature space . GradientBoostedDecisionTree method is an ensemble method which forms multiple decision trees iteratively each improving on the errors of the previous iterations. We had prepared a dataset of 2400 images for training, 600 for validation and 400 for testing. We trained the model on the training dataset and used the validation dataset for fine tuning the hyperparameters. Finally, the test dataset was used to evaluate the performance of the algorithm. The performance was judged on the basis of overall accuracy, i.e. the ratio of the number of correct predictions to the total number of predictions, as well as the AUC which represents how well the predictions separate the positive and the negative classes. The overall performance of the model on test dataset was 95% accurate with AUC of 98.3% implying a good class separation. The probability of the image being a non-cropped image is output by the classifier and a probability below 0.5 signifies it is a cropped image. When we run our example image through the model, we get a score of 0.1 which signifies a cropped image. We can also have a look at which features contributed towards this decision by using the eli5 library. The feature importances tell us that the distance from left edge, as well as the crop on left edge, is leading the image to be a cropped one. This way we can be more specific in our communications to customers telling them where and how much they got it wrong. The probability and the feature importances are requested by the application which further communicates with the customers about the results. We didn’t stop at building such algorithm which fits our use-case perfectly. We thought, what if a single network can be trained to tell whether an image is cropped or not. A well-known way to achieve good results in custom classification tasks is to take a pre-trained network architecture and fine-tune its last few layers. This way the model does not need to be trained from start and it can retain its already perfected edge detection and basic feature extractions from the initial layers. Only the last few layers which specialise at the task at hand are retrained. We ran a small experiment in which we trained a network having input as the original image and the output as the predicted class — cropped or non-cropped image. We used a pre-trained Inception architecture on the Imagenet dataset as the base and fine-tuned the last layers to our target. However, the performance was not satisfactory on this size of the dataset. The max accuracy achieved was 89% using this architecture because we are probably overfitting learning other things rather than the crops on the edges. Another thing we tried was using RGB image + B&W Mask as a 4 channel input to an Alexnet architecture and training from scratch. However, the max accuracy achieved was 91% . Perhaps some further work using more complex networks and more data could help us achieve results comparable to the original algorithm. Another challenge which arises using such an arrangement is when we want to extend for more categories, we will need to train more networks for other categories and label more data, while we can use the pre-trained ones in the original algorithm we described. We wanted to enhance seller experience by getting them more buyers and for that, we built a crop detection algorithm which tells the cropped images apart from the non-cropped ones. The algorithm uses a combination of deep learning based object detection models, image processing algorithms and machine learning models on the extracted feature set. The object detection models find the masks for the images followed by refinement from the image processing algorithms. Finally, features are extracted from the masks which are used by a machine-learned classifier to make its decision on the image being cropped or not. The overall classification accuracy achieved is 95% with an AUC of 98.3% which tells a good class separation by the predicted probabilities. The predicted probability of the input image being cropped or not is output by the algorithm, along with the edges and amount of cropping which is used by the client to inform the user about a possible reason for the lower number of contacts. I would like to thank the team - Mohit Sharma , Vladan Radosavljevic and Udit Srivastava . We operate a network of online trading platforms in over 40… 177 Machine Learning Artificial Intelligence Image Processing Classifieds Data Science 177 claps 177 Written by Data Scientist @OLX We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Data Scientist @OLX We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-05"},
{"website": "Olx", "title": "fast prototypes with flutter kotlin native", "author": ["JB Lorenzo"], "link": "https://tech.olx.com/fast-prototypes-with-flutter-kotlin-native-d7ce5cfeb5f1", "abstract": "About OLX Group Tasked with making the app for our upcoming internal OLX Group conference and having a lot already on our plate, my team was thinking of how to reduce the time for us to make an app for both iOS and Android. The first thing that came to my mind was using Kotlin, as it has a way to compile on other platforms. Not everything went as planned, but (spoiler alert) we made it in time, and we learned a lot! This is a story about how we made the OLX Group Product & Tech Conference app in record time. There’s been a lot of effort by companies like Google and Facebook lately regarding cross platform development. Flutter and Kotlin/Native are frameworks that came out of some of those efforts. Flutter, developed by Google, was announced back in 2017. It uses Dart as the programming language. Flutter supports compiling to both Android and iOS using one code base written in Dart. Flutter compiles natively instead of using Web Views inside the apps. However it does use its own UI components as opposed to using the native platform specific ones, e.g. UIView in iOS, Fragments and ViewGroups in Android. Flutter also natively supports Material Design and Material Theming in its UI components. We had no experience in Dart, we didn’t feel we had time to learn a new language (we have other priorities as well). We really didn’t start looking into using Flutter until we realized we don’t have much time left. Going back to the story, we started with using Kotlin/Native, and developed some mock data and logic with it to start: Kotlin/Native is developed by JetBrains, the same company that created Kotlin. If you haven’t heard of Kotlin, you might not be using the internet lately. It’s basically a simpler replacement for Java, and as a plus, works seamlessly with Java. Kotlin/Native allows having common code written in Kotlin and cross-compiles to support other platforms that don’t support Kotlin natively, like iOS. We asked our designer for some quick designs for the conference app, and she quickly used Material theming to generate a nice looking design. There is a plugin for Sketch to generate material-theming-compatible designs, that saved us a lot of time and effort. Material Theming allows creating designs fast and supports both iOS and Android Initially we looked for ways to implement the UI separately for Android and iOS. There is Material Theming support for both platforms but, we were still going to have to write UIs. We thought that if we can use Flutter for the UI, we only had to have one UI code base and we could continue to use the Kotlin logic that we already started with. Since Flutter was new to us, we needed to be sure if it would work with the existing code that we have. It seemed that no one had tried using both Flutter and Kotlin/Native. We planned to have an architecture represented by the image below. This architecture reduced the amount of platform specific code that needs to be written, and also reduces the amount of Dart code since we can isolate most of the logic in the common Kotlin code. We could reduce a lot of the platform specific code using Kotlin/Native and Flutter’s Dart. Although it might be possible to write most of the common code in Dart, we are more familiar with Kotlin, so we used more of that. Both of these frameworks have certain limitations that made using them not as easy as it can be. Limitation 1: Kotlin/Native only supports pure Kotlin classes for the common code. For example you would want to parse an integer into a java.util.Date class or use that class itself, you would not be able to use it, as the java.util.Date class requires JVM. One way to deal with this is to implement the methods in the platform code (Android and iOS), and `expect` that method from Kotlin/Native. Limitation 2: Kotlin also uses Companion objects for static methods, that are translated into a totally new object for iOS. One way to get around this is making the static methods available as instance methods or to make the class method a global function. Limitation 3: Kotlin/Native only supports compiling to arm64 code on iOS. The latest devices have this CPU architecture but older devices will still have armv7. It does compile to x86 and x86_64 though, so you can also run it on the simulator. To set the architecture support, just go to Build Settings and remove the other architectures aside from arm64 on valid architectures. There are also limitations on the communication between the common Kotlin code, Flutter code and platform specific code. Flutter can communicate with platform specific code using channels, passing around a method name and a set of parameters. The parameters are limited to native classes such as map, list, string, int and similar. Things get a little bit messy when passing these objects around, so to handle this, serialization to a custom object is used. However both Flutter and Kotlin/Native have limited support for serialization, so for now, when serialization is not natively supported, we use the native classes directly. The key takeaways from our experience are: Flutter is awesome for fast prototypes Flutter and Kotlin/Native are still in beta but is usable already though with some limitations Kotlin/Native works well to reduce the amount of platform specific code Some glue code needs to be written to pass around/serialize objects We made our app with only around a week of planning and a week and a half of developing. We spent around an hour a day or so with four devs and a designer while still juggling our normal work. We had a lot of fun building an app with Flutter & Kotlin, and hope that you’ll also give these two technologies a try. You can read more about Kotlin/Native on these links: https://kotlinlang.org/docs/reference/multiplatform.html https://kotlinlang.org/docs/reference/native-overview.html And read/watch about Flutter here: https://flutter.io/docs/ MDC-103 Flutter: Material Theming with Color, Shape, Elevation, and … Code beautiful UI with Flutter and Material Design (Google I/O ‘18) Thanks Team for all the efforts 🚀🚀🚀 Karen Banzon , Rem Cruz , Ayelen Chavez , Cristiano Madeira , and Caio Moritz Ronchi . You are awesome. Thanks to Herman Maritz for editing and reviewing. Presented in Devfest Romania 2018 — Nov 16 European Women in Tech 2018 — Nov 30 Devfest Warsaw 2018 — Dec 1 Droid Kaigi 2019 — Feb 8 Tweakers 2019 — Feb 14 AppDevCon 2019 — Mar 15 Droidcon Boston 2019 — April 9 We operate a network of online trading platforms in over 40… 3.9K 20 Kotlin Flutter Mobile iOS Android 3.9K claps 3.9K 20 Written by Engineering Manager at OLX Group, Berlin. He/Him. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Engineering Manager at OLX Group, Berlin. He/Him. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-04-13"},
{"website": "Olx", "title": "making mistakes productive using the five whys", "author": ["Zef Hemel"], "link": "https://tech.olx.com/making-mistakes-productive-using-the-five-whys-bacad905b535", "abstract": "About OLX Group Most companies have some rules or guidelines in place for whenever an “incident” happens, be it an outage or something else that impacts customers. Some companies call them incident reports, others root cause analyses. Sadly, too often, producing these types of reports is seen as a chore — essentially punishment for standing somewhere close to a failing system. They end up writing reports that point just enough fingers in other directions to get away with it. “It’s bad enough that we had to work until 3am to fix it, now we have to write a report about it too?” While this attitude towards incident analysis is very understandable at a human level, we believe it’s the wrong way to look at it. At OLX we focus heavily on learning . And by far the best opportunities to learn are right after the shit hits the proverbial fan. At our company, incidents happen, as they do happen everywhere. We have people on call that can generally resolve issues quickly. Then, the next day, after the dust has settled, all people involved are tasked with answering four questions: What happened? Why did it happen? What can we learn from it? What can we do to prevent such issues in the future? To achieve this we used various techniques over time. Recently, we’ve been moving more towards using Toyota’s “Five Whys” technique. Not all our packs are applying this yet — but we’re seeing a lot of value already. The goals of a Five Whys session: Identify the root cause of an incident . At first sight this root cause may seem obvious to everybody, but more often than not, a well-conducted five whys session leads to surprising results. Learn from mistakes . Incidents happen, and they should be treated as a learning moment. Share the learning by sharing the report widely. Notably, the goal of this session is not the report. Even in the hypothetical case where nobody were to read it, the method should have increased insight into deeper issues in the tech, product, sometimes even organizational culture and valuable action items. The goal of the session is also not to point fingers. Yes, somewhere along the line people made mistakes, but this is bound to happen — our goal is to create an environment, a system, that minimizes the impact of such human mistakes. Here is probably my favorite quote on this topic that perfectly captures what the goals is of the Five Whys: “Let’s plan for a future where we’re all as stupid as we are today.” — Dan Milstein Before we get to the recipe for conducting a five whys (which is, in a sense, shockingly unsurprising) let’s start with some basic rules. The meeting happens after every major incident, no exceptions. Especially if the incident happened before (or even even occurs regularly). “We don’t need a five whys, we know the root cause” is not a reason to skip. All people involved are present in one room (real or virtual) discussing the issue synchronously . The meeting happens within a few days of the incident. It can happen the same day, but there’s value in having it after the heat of the moment — allowing some time for reflection. Action items must be SMART — specific, measurable, attainable, relevant, and time-based. Also, they should be assigned to a specific person (not a team). Action items must be prioritized proportionally to the severity of the incident. For instance, if the incident was that some color of a button changed accidentally — prioritizing end-to-end tests that verify color of DOM elements is probably not proportional. Here’s the basic formula: Start with the problem statement. Write it down on a whiteboard or Google Doc. Ask the “why?” question. No productive “why?” question to ask? Really? goto 5 . Answer the “why” question. goto 2 Done While traditionally the line of asking “why?” is linear, we have seen good results trying asking multiple different “why?” questions at each level. So not plainly “why?” but various more specific whys, for instance: Why did this occur? (root cause) Why did we only find out when people complained about it on twitter? (monitoring) Why did it take us 5 hours to find the root cause? (mean-time-to-recovery) Why did this problem have the impact that it had? (impact minimization) This approach tends to result in “why forests” — but that’s ok. It’s the job of the session facilitator to figure out which “branches” can lead to productive results. You may wonder: what’s up with the five aspect of “five whys”? It originates from is the general heuristic that asking “why?” five times should get you to the root cause. If you ask it once or twice, you’re probably just scratching the surface of an issue. Beyond that you tend to get into deeper cultural, organizational or systems level problems — and this is where the big wins can be made. To learn more about the five whys and best practices, here are three must-read articles: Eric Ries’ introduction on Five Whys and its history Eric Ries’ article on how to conduct successful Five Whys sessions Dan Milstein on his lessons learned after conducting 250 Whys If you have a feeling that “stuff keeps breaking over and over” in your organization, have a serious look at the Five Whys. Find somebody internally who’s already familiar with them or passionate to learn, or hire a trained facilitator. Then, start building that improvement muscle. Mistakes will always be made, the best thing you can do is to make those mistakes productive. We operate a network of online trading platforms in over 40… 127 Design Support Productivity Incident Management 127 claps 127 Written by Picking my battles. Director of Engineering at OLX. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Picking my battles. Director of Engineering at OLX. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-04"},
{"website": "Olx", "title": "privacy protection in on line classifieds using yolo to blur the car license plates", "author": ["Andreas Merentitis"], "link": "https://tech.olx.com/privacy-protection-in-on-line-classifieds-using-yolo-to-blur-the-car-license-plates-42e88c4d66f0", "abstract": "About OLX Group A platform like OLX that connects buyers and sellers in more than 40 countries faces many challenges, several of which are related to trust and safety. In this domain, apart from the more standard challenges like preventing fraudulent types of behaviour in our platform, preserving user privacy is something that we take very seriously at OLX. One particular instance for that comes from the car verticals market and specifically from the use case of users selling their cars and providing some photos of the car they are selling. These photos are typically taken from different angles and often include the license plate of the car. This can be used to search in various databases and potentially poses a privacy violation for the individual who is selling the car in question. For this reason, the listings that contain cars are post-processed in order to blur the car numbers. This is a time consuming step that is better left to be done by algorithms rather than humans. In order to address this issue we developed a model that detects the part of the car that contains the license plate and then proceeds to blur the respective part of the image. Such a model has to address a number of challenges: the images that users submit are of different quality, different resolution, taken under various light conditions, etc. Furthermore, the object in question (car) can have different orientations in the image and the license plate can be fully or partially visible in the image. In order to address these challenges, we use as basis for the neural network the famous YOLO architecture. The name YOLO is the acronym of the words You Only Look Once (perhaps not quite what you expected). When it was introduced (by J. Redmon, S. Divvala, R. Girshick, and A. Farhadi in the paper “You Only Look Once: Unified, Real-Time Object Detection” ), it was a completely novel approach for end to end object detection: instead of re-purposing classification as a way to address object detection, YOLO frames detection as a regression problem with spatially separated bounding boxes and associated class probabilities . Using this formulation, a single neural network can be used to predict the bounding boxes and class probabilities directly from the full image in one step evaluation. Since the entire detection pipeline is a single network, it can consequently be optimized end-to-end directly on detection performance. Following the success and broad adoption of YOLO several implementations for popular libraries quickly became available, as well as modified versions and guidelines on how to train with your own data . The basic architecture is using a resize step, a series of convolution steps and non-max suppression. The network predicts multiple bounding boxes in one run, as well as the class probabilities for those boxes. The overall architecture is presented in the schematic below. The fact that the network process works end-to-end offers several benefits over traditional methods of object detection that are based on multi-step pipelines (e.g, first providing region proposals to generate bounding boxes, followed by classification and finally aggregation). One key benefit is of course speed, and this is a main advantage also for our use case given the sheer amount of data that are generated in some of our markets. The second advantage is that YOLO sees the full image during both training and test time so it encodes contextual information that would otherwise would not be available . This is the main reason it typically outperforms previous methods, since it it less likely to make mistakes about the image background. Finally, YOLO has also proven more generalizable to new inputs or slightly modified domains, which is also an important requirement for our use case given the potential difference between the conditions that the photos in the training and test set are taken. Looking a bit more under the hood, it’s interesting to understand how predictions are made. The system divides the input image into a S × S grid. If the center of a candidate object falls into a grid cell, that grid cell is the one assigned with the task of detecting the object. Each grid cell predicts B bounding boxes and confidence scores for those boxes. These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks it is about the objects dimensions. Formally YOLO defines confidence as: If no object exists in that cell, the confidence scores should be zero. Otherwise the desired outcome is for the confidence score to be equal to the Intersection Over Union ( IOU ) between the predicted box and the ground truth. Each bounding box consists of 5 predictions: x, y, w, h, and confidence. The (x, y) coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the entire image, while the confidence prediction represents the IOU that exists between the predicted box and any ground truth box. The loss in YOLO adds three different components (localization, confidence and classification losses together). Essentially we are asking the model to predict the correct object class, to detect where the center of the detected object resides and also to detect the bounding box for that object and we penalize the model in case these predictions are not correct. In addition, the loss is formulated in a way that it takes into account that small deviations in large boxes matter less than in small boxes (this takes the form of predicting the square root of the bounding box width and height instead of the width and height directly). A sample prediction of the YOLO model and the result of blurring the license plate is presented in the image below: Blurring the location of the license plate can then take place: We can see that the license plate is detected correctly, although it is slightly shifted. This is a common pattern in YOLO, which typically is more likely to make small localization errors rather than completely miss the object in question. One workaround for our use case is the blurring of a slightly larger area than the one predicted to be the license plate. Trying the newer versions of YOLO3 is also something to consider in the future. Specifically, YOLO2 adds anchor boxes with some of the most frequently occurring shapes (called priors in the YOLO2 paper). In additional to anchor boxes, YOLO3 also adds the ability to detect objects in three different levels of resolution, which alleviates the small object detection problem often. It is however also slower, which is a trade-off that should be considered based on the requirements of the use case. We operate a network of online trading platforms in over 40… 123 Machine Learning Privacy Yolo 123 claps 123 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-05"},
{"website": "Olx", "title": "kdd london 2018", "author": ["Cristian Martinez"], "link": "https://tech.olx.com/kdd-london-2018-bd1ea240a103", "abstract": "About OLX Group L ast week, part of our Data Science Team went to London to participate in Knowledge Discovery and Data Mining (KDD) conference. The KDD gathers data scientists from all around the world and is arguably the top conference in the field. This year, the conference achieved a record number of attendees, publications, and sponsorship with more than 3300 participants from 99 countries, over USD$1.2M in sponsorship and around 1500 submissions. As part of the program, this year they included a considerable number of events regarding Deep Learning including a full Deep Learning Day with very interesting keynotes and several workshops, hands-on tutorials, and conventional tutorials. We want to share some of our experiences and insights. During the Deep Learning Day, we attended the keynotes session in the ICC Auditorium. The idea of this session was to provide a clear, wide overview of recent developments in deep learning, including emerging topics deserving of more attention, such as graph convolutional neural networks or computational optimal transport. These are some of the most relevant keynotes: Building the Software 2.0 Stack In this keynote the Director of AI at Tesla introduced the idea of Software 2.0, i.e. instead of designing an explicit algorithm to solve a problem (Software 1.0) curate a large, varied and clean dataset, which indirectly will influence the code . Ultimately, the code is generated by an optimization, commonly in the form of neural network training . He explained the idea and showed how this approach was being used in the field of image recognition, in which currently more and more of the code is generated by algorithms, instead of having a lot of handcrafted features and rules that are difficult to maintain and just cover a small portion of the problem space. He also mentioned, that currently a great amount of Tesla’s autonomous driving technology is running Software 2.0. They spend a lot of time curating the datasets and generating new samples of unseen cases. Then, they use Deep Learning models that optimize different metrics depending on the task. The resulting artifact (the code) is what Karpathy calls Software 2.0 and you can think about it as a “fill-in-the-blanks programming”. The video of the keynote is yet not available but you can watch this video from another event where Karapathy talks about the same idea. The Natural Language Decathlon: Multitask Learning as Question Answering Richard Socher, Chief Scientist at Salesforce, presented the Natural Language Decathlon (decaNLP), a challenge that spans a number of tasks like machine translation, question answering, sentiment analysis, and summarization. The central idea is to frame all those challenges as a single question answering task over a context so it is possible to train a single, general-purpose question answering model that solves them all. He also presented a multitask question answering network (MQAN), a deep architecture that jointly learns all the tasks without any task-specific modules or parameters, and showed how this architecture provides a mean to do transfer learning, domain adaptation and even zero-shot learning for natural language processing tasks. For more information, you can read the paper or go through this very interesting blog post . Software for Deep Learning: an Overview of PyTorch and Others Soumith Chintala from Facebook talked about PyTorch and gave an overview of its different modules. He showed how to easily create and load tensors in a numpy array fashion but with GPU support, using the modules torch and torch.utils and how to define and train deep neural network architectures using the modules torch.nn and torch.optim . Finally, he explained the torch.autograd module for automatic differentiation. Thanks to its imperative execution model, PyTorch also allows users to debug their models and to use any Python package to interact with it. If you want to see some PyTorch code in action you can check this excellent tutorial on building deep recommender systems. Paper submissions were divided into two tracks: Research and Applied Data Science (ADS). The paper on Adversarial Attacks on Neural Networks for Graph Data from the Technical University of Munich received the best paper award, meanwhile XiaoIce Band: A Melody and Arrangement Generation Framework for Pop Music was selected as the best student paper for the Research track. On the other hand, in the ADS track, the paper on Real-time Personalization using Embeddings for Search Ranking at Airbnb from Airbnb’s Search Ranking team was selected as the best paper and ActiveRemediation: The Search for Lead Pipes in Flint, Michigan the best student paper. The conference was a great opportunity to get updated with the latest trends in the field and get to know what data scientists are working on in academia as well as in industry. The event was huge, with different tracks running at the same time and external events organized by companies like Pinterest, Netflix, Airbnb, and Facebook. Overall it was an excellent experience and we strongly recommend attending future editions. Active and transfer learning at scale with R and Python: Github repo . Feature Extraction and Summarization with Sequence to Sequence Learning: Github repo , website . Deep Learning with Keras: Github repo . Building Custom Deep Recommendation Engines: Github repo , website . Deep Learning and Natural Language Processing with Apache MxNet Gluon: Github repo , website . Introduction to Reinforcement Learning with Ray: Github repo . Beyond Graph Mining — Higher-Order Data Analytics for Temporal Network Data: Github repo , website . GOAI — Accelerating the Scalable Data Science Environment with GPU-enabled Python: Github repo . We operate a network of online trading platforms in over 40… 489 Machine Learning 489 claps 489 Written by Data Scientist @ OLX We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Data Scientist @ OLX We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-09-10"},
{"website": "Olx", "title": "from postgres to elasticsearch an olx location service thriller", "author": ["Olivier Korver"], "link": "https://tech.olx.com/from-postgres-to-elasticsearch-an-olx-location-service-thriller-f853aef021c6", "abstract": "About OLX Group I 've recently had the immense pleasure of starting as a Site Reliability Engineer for the Core Services Team, specifically Sphere, which is our in-house-built location taxonomy service. At the time of writing, we are proud to mention that the API does about ~1.5 billion location requests/month, a number that will only increase exponentially with countries like India and Pakistan soon integrating our service endpoint. It was only in my second month of employment as such, that with a hefty, maybe even unhealthy dose of impostor’s syndrome I took the now infamous JIRA ticket “Validation of Migration to Elasticsearch” upon me. This blog post aims to tell the story of my teams’ and my journey, in migrating the storage component of this service, from Postgres to Elasticsearch… Where have I heard this before… Admittedly, we weren’t the first doing this.. A previous employer of mine engaged in a migration from PostgreSQL to “the other Lucene based” search platform, SOLR. I am not sure on the state of this migration at this point, but when I left, the migration had been ongoing for two years, and it was still incomplete. Of course, the scale & use case was a completely different one: the company aimed to boost the speed of its analytics platform, heavily relying on tweet ingestion from the Twitter stream. At the time, the widespread belief was that PostgreSQL would offer no such room for scalability, and even though SOLR showed performance improvements, the company had a hard time scaling the latter out in a reliable fashion. I already had a decent amount of confirmation bias, and therefore questioned if this migration wasn’t history repeating itself. PostgreSQL, the problem First of all, let me start out by saying that we ❤️ Postgres! It is the Swiss Army Knife of databases. If you have any experience with storing location data, you’ll indubitably be familiar with PostGIS, and know that it is crazy fast. How fast? Supposedly, about 10 x times faster than Elasticsearch .. So, it would seem, at least regarding performance, that PostGIS is the clear winner. This was, however, only less than half the story, and we hadn’t done any benchmarks of our own, however more on the benchmarking later on in this story… Due to my initial doubt about this migration, I did some fact finding to see what needs had to be catered for, and found that we needed to overcome the following challenges: Scaling out inside of AWS RDS. Part of the problem with Postgres (which in all fairness isn’t specific to Postgres itself), was the fact that this was running inside of RDS. We felt too constrained by the pricing model of Amazon’s managed Database Service, as well as the duration of many of the basic operations required when needing to scale out. A much faster, less black-boxed, and therefore more efficient approach, was doing this ourselves on our self-managed kubernetes cluster. We already had a dedicated Elasticsearch Cluster for our API, for which we could increase resources (CPU & mem limits), as we please. The Autocomplete Location endpoint of our API, already relied entirely on Elasticsearch, and because of it, our pipeline process had become unnecessarily painful & complicated to execute & manage: 1.) build a dataset for a specific country, 2.) load the dataset it into Postgres 3.) index location data from Postgres into Elasticsearch. Besides, inside of our API, we needed to maintain separate controllers & DAO layers, just to deal with the different backends. We wanted to manage “just one backend store”. Elasticsearch, the solution? As time went by, and more opinions and needs were gathered, it became quite clear that the logical path forward, was indeed Elasticsearch: Fast, based so far on the experience of running our autocomplete on this platform. Reduction of complexity by sticking to one backend store. Scaling out is a straightforward operation, partially because of Elastic’s great out-of-the-box clustering, including rack awareness that allows fault-tolerant, highly available deployment to kubernetes: We can easily shard one country dataset per index, enabling us to version, release and rollback this data individually. Offer N+3 level-redundancy with Elasticsearch replicas. Getting to work — the Proof of Concept With the proof of concept, we focused on validating the following points that we weren’t sure about: How difficult would it be to get the same query results from Elasticsearch that you’d usually get from Postgres? How well does Elasticsearch perform under load and / high cardinality of requests? What performance does it offer in comparison to the already very fast Postgres? The above methods were chosen at the time of validation, as getLocationById and getLocationByCoords in the top 5 of most requested LocationController endpoints (data that we gathered from our APM dashboards in New Relic). Even though getLocationShapeById doesn’t fall into this category, it could provide us with an interesting benchmarking scenario; many locations have complex shapes that translate into large request objects. Because of the flat structure of a document Elasticsearch (one shape is a document object not requiring additional CPU cycles to render said shape), we thus expected this to be an improvement. Testing set-up In order to have a fair comparison of results, we used the following testing set-up: You’ll see that the resources used, and the platform on which the benchmarks are performed; are identical. This to ensure that during testing, we are comparing apples to apples. As moving away from RDS is a certainty, performing the tests on the platform where our Elasticsearch cluster is already running, is a straightforward choice. Benchmark results Findings after testing with mixes of Czech Republic, Portugal, and Argentina datasets: Elasticsearch is faster for location id and reverse geocode lookup. Most of the time, the response times for elasticsearch within location id stay below 20 ms. For location id lookup, the elasticsearch endpoint mean is below 3 ms, and the 95th and 99th percentiles barely ever make it above 10ms. Postgres’ mean is 5 ms, but the 95th and 99th percentile show a lot of variation (the curve isn’t very stable and seems to be trending upwards), ranging between 150 and 250 ms. For reverse geocode lookup, a very similar picture is seen. ES reverse geocode endpoints’ mean is around 12 ms, with 95th and 99th percentiles hovering between 24 and 50 ms, and being relatively stable. Postgres shows a mean of 39 ms, with 95th and 99th percentiles ranging between 199 and 399 ms. Elasticsearch shape lookup on first sight, appears to be slower. However, the complexity of the shape we store in elasticsearch is higher than the one we store in postgres. Mean for PG shape endpoint is 3 ms, versus 6 ms for ES. 95th and 99th percentile range between 5 and 60 milliseconds. We did a few other benchmarks where we scaled out vertically our Elasticsearch staging cluster, in order to see whether there was any difference: There is a very small, negligible difference; and if you were paying attention, you'll see that the Postgres response times are also lower, which could be attributed to external factors. Based on both the challenges with Postgres and the solutions offered by Elasticsearch, we jointly decided to commit to this migration. We were aware that there were a few challenges, which at the time had shortlisted to the following: Elasticsearch tooling was not as mature as that of Postgres’ (at this time of writing, there is no ArcGIS visualisation plugin for Elasticsearch, for example) Elasticsearch DSL & templating not as straightforward as having a database “schema” and traditional SQL queries. Migration As we were feeling quite confident about the migration, due to decent benchmark and stability testing results, during our planning meeting on Monday, March 26th of the year 2018, we decided to commit to the migration in full, convinced it could be done in “just two weeks”… What follows next is a timeline of the dramatic events that unfolded after committing to the migration…. 26/27 March —A discussion unfolds on the details of implementation, of the layer that will be accessing the Elasticsearch backend, without it affecting other layers of the application, as well as how to structure the Elasticsearch-related code. The decision is made to create a decorator around the LocationRepository that will forward calls either to the Postgres-based implementation or the subsequently added Elasticsearch Client. 28 March — 6 April — The team is implementing all existing queries against ElasticSearch, periodically pushing changes to the staging environment… During this time our staging Kubernetes cluster doesn’t provide the usual stability, due to an ongoing migration of k8s from 1.4 to 1.9. Also, the team is seeking to find stable ground, to increase confidence in the design of the Elasticsearch schema. 6 April midday (an oh shit!.. day) — All queries have been migrated to Elasticsearch, deployed to staging aaaaaand….. observed response times in NewRelic look.. abysmal: observed 95%-tile of response times for all endpoints is close to 100ms. Ouch…. 9 April — 9 April — Panic, uncertainty, disdain and all negative feelings in the world accumulate onto the team, and as we’re thunderstruck by a sense of collective incompetence, we make it a priority to come to a decision by the end of the week on whether to still commit to a migration of ElasticSearch at all… 9 April — 13 April — The team is working on multiple fronts to reduce the response times of the ES-based version. ElasticSearch in staging is given more resources and moved to kubernetes workers with more resources, hoping to improve Quality of Service. Extra debugging is also done with VisualVM to understand what is happening inside of the Elasticsearch nodes. The application code is instrumented to report on response times of each query individually, and the latter identified quite some bottlenecks that are discovered and fixed: moved away from storing the location shape in the _source field; stopped asking for a subset of fields from _source to avoid extra unnecessary CPU cycles on the Elasticsearch-side moved from JSON to Smile for API-to-Elasticsearch intercommunication; Elasticsearch schema & the worst performing queries are reworked for better performance However, aside from the above providing us with visible, incremental improvements; the situation still looked quite bleak, and in comparison to our trusted Postgres backend, the latter was still significantly faster. 13 April — the last resort — We've basically reached our deadline decision moment, but we're still not confident about deploying this to production. As a last \"out of sheer desperation\" move, we migrate Postgres on Staging back to our Kubernetes cluster. This, in order to to remove the RDS-factor from the equation: we want to see if there is any contention we missed on the new staging cluster and see the performance of Postgres in Docker / k8s benchmarked against Elasticsearch again… Performance is actually very similar in both cases… Even though we can’t reasonably explain the lousy performance of neither Postgres or Elasticsearch on Staging, we’re more confident now that Elasticsearch is by no means any slower than Postgres. We decide to proceed with the migration. 16–19 April —At the start of this week, the team makes it a priority to move Elasticsearch to production. We further consolidate on the stability and consistency of the new back-end. The twitter-developed Diffy tool is used to compare Sphere responses between our Staging and Production environments, enabling us to identify discrepancies in the responses of the old against the new backend. More Elasticsearch code is cleaned up and refactored, and several minor incremental performance improvements are measured on staging.. 19 April — Doomsday? We deploy to production, we hold our breaths…. And…. Phew… It looks like our POC was valid after all.. The perceived response time distribution, for average, median and 95th percentile, is quite low (between 8 and 15ms), and our 99th percentile response times show that the outliers are a lot less pronounced, making response times generally more reliable and predictable. In other words… The result is a massive sigh of relief, both for me personally, and my incredible team that had fought blood, sweat and tears to pull this off. We are now running this on production for two months, and are more than happy that we committed to the migration, even though in the middle of it we had quite a few unexpected challenges to overcome. Aside from the API speed improvements, adding new Elasticsearch replicas is now minutes of work, and as soon as we have all our kubernetes clusters upgraded, we can automate this through the horizontal pod autoscaler. Furthermore, our disaster recovery process, thanks to the Elasticsearch s3 repository plugin , has been significantly shortened — in case of a complete loss of our dataset, we can restore it in less than 2 minutes. Lessons Even though we were all happy with the result, we felt that we had some learning points to take with us, for any potential large-scale projects in the future: Feature flags : in our team, we do trunk-based development , and we ❤️ the bits out of it. However, because we were accumulating changes in our staging environment, we were blocking other required feature-releases from going to production. This challenge would have been easy to overcome, by implementing feature flags, which enable Elasticsearch per API method, from the start. We did implement these feature flags, but only reactively, at a later stage, to unblock us from going to production. Contention : as we discovered later, most of the problems that we encountered during our first trial deployments on staging, were in large part due to maintenance performed on the aforementioned kubernetes staging cluster. Also here, having feature flags would have helped us at least partially, in combination with the necessary metrics per endpoint in Prometheus, where we would be able to do a one to one comparison of the API endpoint per backend, on production. Timelines : clearly, our “two weeks” plan was overly confident and did not account for failure or issues that could block us during the migration. We need to do a better job of estimating, which hangs together more tightly with scoping the migration. We should have scoped the project from the start, and have clearer milestones per project. Clearer definitions of done : while we committed to the migration, we didn’t scope it out all the way, and were also not clear as to what exactly needed to be done within those two weeks. In the end, the initial migration as it was completed, could be shortlisted to “move all Sphere API methods to the Elasticsearch backend”. What was not immediately part of it, however, but of the same vital importance, as well as part of the original reasons to move, was redoing the data pipeline. You guessed it again if you were paying attention.. we didn’t refactor it, and at the time of writing this blog post, this hasn’t happened yet..! Ahead Pipeline : we're currently evaluating Argo , a container-native workflow engine, which will allow us, amongst others, to template country dataset releases. Cluster : we're currently running Elasticsearch 5.6.4. We need to evaluate how difficult it would be for us to migrate to Elasticsearch 6.x, as 7.x is already ahead. Further performance improvements from 6.x are expected thanks to \"Index Sorting\". Scaling out : where it comes to CPU & memory resources needing to be scaled horizontally, this will mainly focus on enabling our horizontal pod autoscaler in kubernetes, so that we can scale up and down on demand, depending on traffic patterns. Regarding storage, the current data footprint is relatively small (around 2 GiB excluding replicas), but this is considering that we don’t yet have country datasets for Pakistan & India. Even so, we currently use the default EBS volumes attached to our kubernetes workers, mainly because we don’t care about any of this data persisting; our API is not write-enabled, we can restore all data from S3 with the press of a button. However, we need to start worrying about potential disk IO not causing contention on other applications that require it, so at some point, Elasticsearch will have its own dedicated EBS volumes. Development : on the short term, we are looking at indexing our location dataset directly to Elasticsearch. Currently, we still load our dataset into Postgres, and index it from there. Additionally, we will consolidate a lot of our older autocomplete-related code, to ensure this has the same abstraction layers as our LocationRepository. Furthermore, we need to make sure that additional queries we implement have the same performance benchmark as our current ones. At the time of migration, we did not consider the Scroll API , as we don't yet have queries that return 10,000 items. All in all, we are now in a happy place with Elasticsearch, where we have made both significant performance and reliability improvements. As we aim to write more on the evolution of our shiny new backend-storage solution in the coming time, please keep a close eye on the OLX Tech Hub Berlin blog! We operate a network of online trading platforms in over 40… 365 Elasticsearch Postgres Migration 365 claps 365 Written by Site Reliability Engineer - OLX / Naspers Classifieds We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Site Reliability Engineer - OLX / Naspers Classifieds We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-26"},
{"website": "Olx", "title": "let the code rule your process", "author": ["Fernando Otero"], "link": "https://tech.olx.com/let-the-code-rule-your-process-65fef2fa4a0a", "abstract": "About OLX Group H ating Jira was how I grew up as a developer; when I was junior I thought it was pure bureaucracy created by my manager to justify his job. But as I grew older I realized that I needed a way to track what was being done and more importantly: when things went into production. I still think that manually transition tickets is a pain in the a** and this is why I want my team to use JIRA only for sprint planning and communication of the details of the work. If the development team has to manually advance tickets through the process or take the time to manually specify the exact version that the ticket was merged into — errors will propagate through the board and the tickets will be forgotten. One common situation that I have had to deal with throughout my career is when some random guy comes running at me yelling: “We just run the numbers and it seems like after [date] there was a drop in [variable]! ” and my response is commonly “that’s 5 weeks ago, a lot of things went to production”. Hopefully you are organized and you will know what was deployed when and your investigation will be pretty simple. However, in my experience tickets often get assigned to the wrong version, release dates are incorrect and a multitude of other factors all confound the truth. Before jumping into how we do it, let me give some context about my team’s process and stack: We use feature branches, develop and master. When commits get merged to master they go to production. Our APIs are only consumed by our projects and and our code is not reused by any other team so even though versioning is important at an API level, it’s not that important from a library perspective. We use docker in production so we want to track which image has which code, and ideally you also want to know which pipeline was executed to create each version. The first thing to debug a problem is to know what is causing it and most importantly why it was written that way (it might actually be a “ feature ”). In order to do this we have a commit hook that appends the branch name to every ticket: Now this is kind of useless if we don’t name our branches with a ticket ID, so all our branches must have a ticket ID (ie : XXX-1234_some_description). With this information, if we find the line that produced the bug, we can use git blame on it and check the ticket purpose. Ideally every single change has a ticket but: we hate logging into jira and filling in all the mandatory fields . To simplify this we also have a bash script that uses Jira’s REST API. Suppose you want to refactor something and there’s no ticket for that, then you can do: That command will create a Jira ticket with the given summary, it will also assign predefined labels and fields that the company needs and create a git branch for you. It will also switch you to that branch and transition the ticket to “In Progress” , without needing to login into jira you are ready to code. You can find the code for the hook and pbt at https://github.com/olx-global/easy-flow Once your code is ready we create a Pull-Request, this action automatically transitions the ticket to “In Code Review” , and after it gets approved and merged the ticket automatically transitions to “Ready to Test” . All this is done with normal Github <-> Jira integration (webhooks). When we merge to master it triggers a deployment to Staging, and, if everything goes OK, we can click “push to prod”; our Docker image gets deployed to production; a new Jira version is created; all tickets merged since last deployment are tagged with a version (using Jira’s REST API); we create a tag in git with the version; and finally we have a release completed with little human involvement. So the first step is getting the git id of your previous version: for that you have at least 2 options, use git rev-list -n 1 TAG or use Github’s REST API. I use the REST API because our pipeline does git fetch --no-tags 😞 In order to get all the tickets merged you can do: This will give you a nice list of XXX-123, XXX-434, XXX-4555 that is easy to parse with groovy and then we can assign all those tickets the version created. A word about versioning: you can use a file with the release version, we use the build number. As said before, our services are mostly used by us and they are not exposed as libraries, so using the build version helps tracking which build generated which docker image and which tickets got into that image. Finally, Special thanks to our devops whose assistance was instrumental in getting everything set up and running smoothly. We operate a network of online trading platforms in over 40… 273 Git Jira Wow Process 273 claps 273 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-02"},
{"website": "Olx", "title": "probabilistic programming primer", "author": ["Seydou Dia"], "link": "https://tech.olx.com/probabilistic-programming-primer-55b88ef4cc0a", "abstract": "About OLX Group The goals of this series is to discover what is probabilistic programming and how it can be used to solve real world and business challenges. We will see its applications in analytics, inference and data driven product experimentations. Probabilistic programming refers to programs that fit probability model and the primitives of the language can be stochastic ; e.g. probability distribution. Thanks to those primitives we are able to express programs that deal with uncertainty and information . This post is the first of the series and we will introduce probability distribution and likelihood. If you want to know why probabilistic programming is exciting let me point you to this post by Beau Cronin ; my favourite quote from his post is, […] it’s clearly cool, but why does it matter? Probabilistic programming will unlock narrative explanations of data, one of the holy grails of business analytics and the unsung hero of scientific persuasion. People think in terms of stories — thus the unreasonable power of the anecdote to drive decision-making, well-founded or not. But existing analytics largely fails to provide this kind of story; instead, numbers seemingly appear out of thin air, with little of the causal context that humans prefer when weighing their options. The first time I read this, I was definitely excited about the subject and promised myself to get up to speed. So if, like me, you are excited and want to learn more, I hope this series will give you the intuition about the mechanism behind probabilistic programming, as well as some useful applications . In this first section we will talk about the concept of probability . For our purpose we define probability as a way to quantify our degree of belief in some events. For example, we think that the probability it rains tomorrow is 60%, or 0.6 . A probability is always a number between 0 and 1 . Moreover the sum of the probabilities over all outcomes is always 1–– or 100% . If the probability it rains tomorrow is 60% then we imply that the probability of not raining is 40% . The same applies when there is more than 2 outcomes, for example there is 3 possible outcomes in a football game for a given team: draw / loose / win . If we were asked about our belief in a match we could say something like : 10% chance to win, 35% chance to loose and, by deduction, 55% chance of a draw. A probability distribution combines the possible outcomes and the belief we put on them in one view. If we take the rain example, the probability distribution looks like this: The probability distribution for the football match example looks like this: Let's look at another example, say I have a fair dice; if I am interested in the probability to get a specific number the probability distribution looks like this: What we can read from the previous distribution is that the numbers from 1 to 6 have the same probability of occurrence — that is expected, we are dealing with a fair dice after all. So far we've seen example of distribution for discrete events: YES / NO for tomorrow rain, [1, 2, 3, 4, 5, 6] for the dice and 3 possible outcomes for the football game etc. However distributions also apply to continuous values, for example I might want to predict the temperature for tomorrow; in such case a continuous distribution is convenient. This is quite similar to what we have seen so far, the only difference is that the outcome we try to predict, the temperatures, is continuous: it can take value like 25.57ºC , etc. Previously we saw that the probabilities add up to 1 (or 100% ), so how do we do that in the continuous case? The answer is in calculus–– the construct that allows us to “sum” continuous function is called the integral . So returning to my example, this is how I express the distribution summing up to 1: In calculus functions are the constructs used to represent continuous value, in the formula above I’ve used the function named p to represent the probability on the continuous range of temperatures. Ok, it looks scary, but bear with me, don't close that tab just yet! Integrals are not really relevant for our purpose, I just mention them for the sake of completeness. Before closing this section let me share the most famous probability distribution of all time: the Normal distribution. In the previous section we saw that functions are used to represent a continuous distribution . In this section we will focus on a special family of functions called Beta functions . A function from the Beta family can be used to represent any continuous variable with values going from 0 to 1 . There are many things that take value between 0 and 1 : proportions, percentage, conversion rate, probability itself. A Beta function has 2 parameters, let's note them a and b . The only thing you need to know is that changing a and b has the effect to change the shape of the Beta function . This is what make the Beta function so powerful, by tweaking a and b you can design a function that matches closely your model of belief. Let's say I want a probability distribution for a conversion rate of 0.5 , or a fair coin ( 50% chance to get heads), then Beta function shown in the next figure is a very good candidate. In a similar way, if I want to model a conversion rate of 20% I can use the following: Finally, if I have no idea what the most probable conversion rate is, then from my perspective all values are equally probable; I will go with the uniform Beta function, by setting a=b=1. So putting everything together, the following plots sum up the shape of the Beta function for various value of a and b: The key takeaway of this section is that Beta functions are very useful to build probability distribution for continuous variable in the 0 to 1 range. However, another amazing property of the Beta functions is , they help simplify computation. You remember those scary integrals shown earlier, — well, thanks to Beta functions we can get rid of them. In this section we discussed: probability and probability distribution to model our belief on some events, the Beta function as a tool to model any continuous variable in the 0 to 1 range. So far we have dealt with probability , and uncertainty in general; but then at some point what you tried to predict actually happen (or not), your predictions face reality; in other words you see the data! Now you need to capture the information gained from the seen data — that is the goal of the next section. The second construct we'll see is called likelihood . Basically if we put belief in probability for future events, then likelihood captures information from new data, once those events actually take place. We use probability before we see the data and likelihood after . Let's say before seeing the data we have a belief on a set of outcomes — a probability distribution ; then after data has been seen, likelihood is the construct that allow us to tell which of our initial beliefs best explain the data we are seeing. Likelihoods are inherently comparative; only by comparing likelihoods they become interpretable. Unlike probabilities that provide an absolute measure of belief, likelihood is a relative measure. Let’s see likelihood in action. Say we have a coin, and we think it is a fair coin meaning the probability of heads coming up is 50% . Now we flip the coin 10 times and say we get the following sequence: H-T-H-H-T-H-H-T-H-H , so 7 heads for 3 tails. The likelihood for that data is a classic of statistics, it is called the Binomial distribution , and for our dataset it looks like this: where: p is the probability to get head h is the number of heads and t for tail (in our case h=7 and t=3 ) It's important to see that this formula is very simple, it's just the multiplication of the probability of occurrence of each data points — p for the heads, 1-p for the tails. If we replace h and t by what we get in our dataset, then the formula is: and the plot of the likelihood looks like: The graph above can be interpreted as follows: according to the data, the most likely probability is 0.7 (among all the possibilities) — which makes sense as we have 7 heads out of 10 trials. In this sections we have introduced the concept of likelihood . Likelihood can be seen as a construct to capture information from data. An important point to remember is that likelihood is inherently comparative. Contrary to probability, the absolute value for likelihood does not make sense per se; only when used to compare assumptions. For 2 given hypotheses the one with higher likelihood is most likely to explain the observed data. Maximum likelihood is a major concept in classic statistics. In the previous section we've seen that before seeing any data, we've put belief on events using probability distribution . Having seen the data, we use likelihood to capture new information gained from the data. A natural thing to do once you have new information is to update your initial belief; that is achieved simply by multiplying the probability distribution with the likelihood . Going back to the coin example, — before tossing the coin our assumption was that the coin is fair. Let's use a Beta function centered at 0.5 to model that belief: Let's toss the coin 10 times, say we get 7 heads out of 10 tosses, that results in the following likelihood: The multiplication of probability distribution and likelihood is not as easy as it sounds, it can actually be quite an intensive operation involving calculus and integrals. But it turns out the Binomial likelihood and the Beta function play really nice from a computational point of view. In fact, the result of that multiplication gives back a Beta distribution , thus frees us from a lot of of hassle, like having to deal with calculus and, God forbid, integrals. So, coming back to our main subject: how to update our initial belief in the face of new information? Our initial belief is captured in the probability distribution : a Beta function centered around 0.5 . And the new information is in the Binomial likelihood. The multiplication of both looks like this: The result of this multiplication is a probability — we are using the symbol for proportionality instead of equality because we are missing a normalisation step to make the result a probability . For our case you can just consider the previous equation as an equality. So plotting everything will give us the following: In the previous graph the orange line is the result of the multiplication. In other words, the blue line is our initial belief and the orange line is the new belief once we take into account new data. Initially we assumed a fair dice and our belief was at 50%, but taking into account new data has \"shifted\" that belief to 70%. Probabilistic programming gives us the ability to iterate and continuously update our belief as new data becomes available. We could, for example, write a program like the following: In the previous snippet we see that we started with a prior belief and no data. As we get new data, we keep updating our belief by multiplying it with the resulting likelihood . Each iteration is an update step, and return basically a Beta function , which in turn is updated at the following iteration, and so on. If we plot the intermediate graphs resulting from the previous snippet we'd get something like: In the previous graph, iterations refer to coin tossing or more data; as we get more data as the we converge toward the true probability of head: 70% . If you replace the coin and the probability of heads by, say, a conversion rate, you can start to see how powerful this approach can be when it comes to analytics and business in general. We will see some applications in upcoming posts. If you have followed this so far, you already understand enough about probabilistic programming to starting exploring on your own. This is just a start though, the idea here was first to give an intuition about probabilistic programming ; we will see in the follow-up post how we can apply what we learned here to answer business questions, run inference, analyse a/b testing and experimentation in general. This post is a primer on probabilistic programming . We started with an introduction of the concept of probability and probability distribution . Probability is very useful to quantify uncertainty , on the other hand likelihood captures information from data. Likelihood is not a probability although it is very similar to it. We use likelihood for comparison purpose, to compare the hypothesis that is more likely to explain the data we are seeing. The process we described in this post is the basis of a well defined way of doing statistics and is called Bayesian statistics . The idea is to start with a prior belief and then using new data to build likelihood ; the multiplication of the prior and the likelihood will yield the posterior after normalisation–– posterior is a probability just like the prior . The treatment of Bayesian statistics is beyond the scope of this post, but hopefully you have a basic understanding of how it works. In this post we also introduced the Beta function as a very useful probability distribution for prior . We showed that the Beta function has a really nice relation with the Binomial likelihood; in terms of Bayesian statistics we say the Beta function is the conjugate prior of the Binomial likelihood . I hope this post will develop your curiosity about probabilistic programming and make you want to explore more. If you want to see some applications of what we learned so far, stay tuned for future posts. Thank you for reading! I would like thank Andreas Merentitis and Maryna Cherniavska for the revision and proofreading work. We operate a network of online trading platforms in over 40… 102 1 Thanks to Andreas Merentitis and Maryna Cherniavska . Statistics Probabilistic Programming Probability Probabilistic 102 claps 102 1 Written by data / software / cloud / science : @olx We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by data / software / cloud / science : @olx We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-27"},
{"website": "Olx", "title": "the importance of intent sensitive locations in classifieds", "author": ["Camila Santos Matos de Freitas Ribeiro"], "link": "https://tech.olx.com/the-importance-of-intent-sensitive-locations-in-classifieds-fc31826f35c", "abstract": "About OLX Group I have to confess that when I explain to the outside world that I am a product manager for “locations” in a classifieds website, this sounds quite weird. Why are locations important, if they are a problem that is already “solved”? That’s a common misconception that I would love to deconstruct. The fact that many applications display a digital map with a location icon marking an object’s position doesn’t mean that they have “solved” the locations problem — it only means that they can pinpoint latitude and longitude coordinates! Being able to whip up scrambled eggs doesn’t make someone a cook ;-) Locations are, however, a source of very dense and rich information which can remotely mimic a person’s visual and sensorial experience with regards to local surroundings, and this is not yet often explored enough. Classifieds platforms can benefit a lot from it, and deepening the user experience related to locations is what, proudly, my team’s job consists of. We are known in the OLX world as Sphere, the service in Berlin’s Tech Hub which is responsible for serving locations globally. But how can locations impact the business of classifieds ? Part of the reason why buyers love to buy stuff via classifieds is that they can look for items in areas in which they are willing to go to and meet the seller to make the exchange. This involves a balance between local trade and distance trade: how far convenient areas are depends a lot on the item type — people sometimes travel up to 500 km to find the perfect car, but they might be willing to only go around the corner for a used bicycle seat. Also, distance is very relative — what actually matters is how quickly and easily you can cover it, so the ability to search by commuting time can also play an important role in helping buyers find relevant items. Once buyer and seller agree on the price of the item and decide to exchange, where should the transaction be executed? Finding a suitable and safe place to trade is also a task for locations. Meetup points in open and public places can be suggested via chat, for example, considering how far buyer and seller are willing to travel. If buyer and seller are ready to move, how should they arrive at the desired meetup point? Guiding both parties to the meetup point via multimodal directions, while possibly updating one party on how long it will take for the other to get there, ensures that people willing to meet will actually find each other. A bright green jacket might also help :-) And, more specifically, for real estate classifieds : When buyers want to find for a property to buy or rent, several questions pop up: is the neighbourhood safe? Are there schools around? How far is the supermarket? Do I have easy access to parks and leisure areas? Does it look nice? There are various ways in which locations can contribute to enlarge the picture of the actual life in an area: display of local crime rates, mapping and distribution of relevant points of interest per category, distance of those points to the property in question and street views are a couple of those possibilities. Through locations, a classifieds portal can become the buyer’s eyes and ears when looking for a new home. With commuting time playing such a huge role in people’s overall happiness , it’s no surprise that the actual commute duration is an important decision factor when choosing a new home. Being “commute friendly” is a relevant attribute to a property, besides all of its other intrinsic qualities — especially in large and crowded cities in which traffic is slow and public transportation is not optimal. The ability to 1) find properties to rent or buy within a desired commuting time of your workplace or your kid’s school, and 2) figure out how far a perfect property is from your relevant points of interest is crucial in assessing which is one place’s actual subjective value in comparison to other places. As you can see, there’s much more to the locations experience than just maps ;-) Locations are, actually, an essential part of the success of classifieds, and a polished locations experience can be a huge contributor for the starting and the conclusion of transactions, so that everybody can win! We operate a network of online trading platforms in over 40… 194 1 Real Estate Classifieds Location Product Management Transactions 194 claps 194 1 Written by Senior Product Manager in Berlin, in love with technology, design, languages, traveling, literature, photography. camilafreitasribeiro.com We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Product Manager in Berlin, in love with technology, design, languages, traveling, literature, photography. camilafreitasribeiro.com We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-23"},
{"website": "Olx", "title": "how i learnt to love coding infrastructure", "author": ["Matthew Byng-Maddick"], "link": "https://tech.olx.com/how-i-learnt-to-love-coding-infrastructure-d275355666f5", "abstract": "About OLX Group I’m a Site Reliability Engineer, and I keep hearing about this idea of “Infrastructure as Code.” And then, when I look at the “code” that people talk about, it’s not in a Turing-complete language, I sometimes can’t write tests against it, and sometimes the tools to run it lack a certain je ne sais quoi . It’s always better to work on problems that actually affect you and haven’t been really solved yet, and all of the solutions for the problem that I decided to tackle felt a bit inadequate. As part of my day job, I manage 3 Openshift Origin clusters, and right now, to configure these, there are at least 430 separate objects (represented as in its Kubernetes parent by yaml files) for the applications running on these clusters. These include secrets, configurations, privilege information, external routes, cluster internal load-balancing and, of course, actual software deployments. I want to keep track of what’s supposed to be on these clusters, be able to rebuild them if necessary, and build out the base infrastructure for new clusters (I’ve already had to do this, and this tooling made it a lot easier). Some people, at this point will say “ Helm Charts !” but I started my career in the late 1990s and early 2000s, and I thought we had learned the hard way (regex substitutions of XML in Perl, anyone?) not to try and do textual substitutions of structured data. Here, though, we seem to like the idea of textual substitutions in YAML, but it’s very easy to suddenly treat a string like structured data in YAML ( eg . your string is “false” or “true” or a number or starts with ‘[’ or ‘{’ or ‘!’), and YAML string representations (particularly in the case of a multi-line string) turn out to be somewhat varied and non-trivial. Similarly, we’ve long had an internal implementation of something similar, using ERB (the embedded ruby templating so enamoured in Puppet) and magic ( qv . direnv ) commands to wrap kubectl or oc, which do the ERB expansion in place. This latter has the problem that we can’t easily see what’s going to end up in the cluster configuration, editing can be a bit “guess and hope”. The core problem, though, is that when there is an error somewhere in the templating, the error messages from the Kubernetes / Openshift masters can be unhelpful in figuring out where, and what type is incorrect — there’s no type-checking of the resulting content before it gets sent to the masters. So, how can we do better? Introducing rubiks , our tool for generating all of the hundreds of YAML files we need to configure our cluster. The rubiks model is to do infrastructure as (real) code ( see here for example ). You write python, filling out the objects, and the rubiks compiler turns this into the (type-checked) YAML files. Because of the type-safety, rubiks can also do translations of other objects into the correct form for the output to be read by the cluster. On every run, we generate the complete set of objects for update in our cluster, allowing us to carry through these links. Rubiks acts on a git repository (we also use this property to allow us to run rubiks from anywhere inside the repository and still sanely act relative to the repository root), allowing us to refactor cleanly (by checking that the output didn’t change), and conversely to use git to help figure out what changed on the output from a change to the input, and thus what changes we should push out to our clusters (something that normally happens by an ‘ oc replace -f <file> ’). So why is this better? Isn’t it more complicated to write code? The answer turns out to not be simple. Make your Domain Specific Language ( “DSL” ) not powerful enough, and you’ll end up writing code to auto-generate it (as anyone who ever built a Nagios config of sufficient size will testify, as well as all the people currently thinking to do this with Terraform’s HCL). Similarly, make it too powerful but arcane, and you need a configuration generator too ( qv . sendmail.cf and the m4 macros). Puppet has been through this. While its language isn’t (and explicitly aims not to be) Turing-complete, it’s had to add (finite) looping and branching constructs to actually make itself useful. At the same time, Chef , in the same space, doesn’t try to protect the user from themselves, and runs the recipe in plain Ruby, injecting special names ( “symbols” ) for internal functions and objects into the namespace of the program to refer to its resources as native Ruby. Using a base language that many Site Reliability people know and use already (and in our industry, the number of languages necessary does seem to be increasing), should help here. In writing this tool, I chose Python because I understand the Python constructs much better than Ruby. The use of Python’s strong-but-dynamic typing helps here, too, as it enables much of the type safety and type mapping. I was also inspired quite significantly by the configerator tool at Facebook although this was written without any direct reference to any of that code. In terms of python, I don’t use pip or virtualenv or anything else. This will run on a python2.7 and a python3 (>3.5, I think) cleanly from just a checkout and making sure the binary (or a symlink to it) is on your path. This is (in my experience) a good start from the tools we want to try before we commit to using them. But my files aren’t “*.py”, and what’s with the whole “import_python()” thing? We could try to solve this problem with just plain Python (or any other language), after all, general purpose languages are just that. This solution would have value, it works in editors without any additional configuration. But general purpose languages also have downsides, they’re general purpose, not tailored to the readability of a specific problem domain. Boilerplate impedes readability, and specifically it can hide the code that you might want to highlight. The job of the code written for rubiks is to write out a cluster configuration, not to be a general purpose python program. Instead, the rubiks tool acts as a compiler, running the “special” python (and choosing which files to run), which, like the ruby in Chef, injects a bunch of symbols that facilitate writing easy code to generate the configurations, and attempt to reduce boilerplate. There’s another trick we can do here too, we can run these python files more than once, in several different contexts (particularly, the output cluster), generating subtly different results for each one. This means, that, for example, scc.ekube (in our examples) isn’t quite what it appears, it’s actually run twice, and there are two separate modules held in memory, with the name “scc” and with the exported symbol “scc”, once for “staging” and once for “production” . This is, of course, the behaviour that we want, because we don’t want a service account added on staging in one of our other files to alter the production output. And then to answer the import_python question, we can now carry across the cluster information correctly, making sure staging always imports staging and production imports production (or whatever your clusters are called), but also it allows us to have different (and complementary) behaviours with filename extensions. Python’s own import keyword is ultimately tied to files with a .py extension, and the search path. In python2, that means you need __init.py__ files everywhere (which are ugly). Having import_python as a function isn’t quite as clean as a real import keyword, but with the utility of the more intelligent import it’s probably the closest we can get. None of this suggests that rubiks is a finished product yet. We’re using it internally at OLX to help manage some of our Openshift clusters, and it has definitely proved its utility already, allowing us to define our configuration in the most appropriate ways and give us lots of flexibility and some better readability, and it’s been a massive time saver for making changes already. The initial conversion took time, although the tool has since acquired a parser and a code generator, which allows you to take your current clusters and generate a relatively ugly, but correct object. Some objects have proved more useful than others, eg . while rubiks supports “User” types, this turns out not to be useful because of the way Openshift handles authentication and the creation of the bindings. Similarly the rather ugly “Namespace” handling was designed fairly early in the life of the project; in Openshift we should more likely be using a “ProjectRequest” and a “Project” type (possible future features to come!). We also found that where things were missing (because I was often too lazy to traverse oc explain or to write the code for the full object before we required it), that we have sometimes needed to write the Kubernetes / Openshift object-handling in parallel to the DSL code that was going to use it. We’ve made it Open Source because we believe that this is a progression on the “Infrastructure as Code” ideal, particularly in the space of managing Kubernetes or Openshift clusters. It definitely doesn’t (yet?) have the package management features of Helm, but in terms of guiding you to write consistent and correct configurations, we hope that this is something worth your time. That said, it has some advantages for us over something like Helm, in that we have an nginx proxy in our clusters behind the main Openshift haproxy “router”. We do this to be able to customise the authentication based on IP address and do more intelligent routing and handling of google OAuth. Rubiks allows us to more clearly auto-generate the appropriate nginx config based on what services are available. We also run prometheus in-cluster for monitoring the cluster, and this allows us to do application monitoring in a more sane way too. Please play with it , and feel free to use our examples , and please let us know what you think, and, as always, patches (or pull-requests or whatever) welcome! We operate a network of online trading platforms in over 40… 164 1 Docker Infrastructure Kubernetes Infrastructure As Code Yaml 164 claps 164 1 Written by Site Reliability Engineer, OLX in Berlin We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Site Reliability Engineer, OLX in Berlin We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-03-24"},
{"website": "Olx", "title": "out of sight out of mind or how to be productive when working remotely", "author": ["Maryna Cherniavska"], "link": "https://tech.olx.com/out-of-sight-out-of-mind-or-how-to-be-productive-when-working-remotely-3164806d6bd0", "abstract": "About OLX Group We live in a world where technology seemingly lessens the need for actual office space. In IT industry especially, there seems to be less and less sense to actually come to work. At some point, telecommuting was all the rage, to the extent that a lot of big software companies offered unlimited remote work possibilities as one of the main perks. A few years back though this trend shifted a little bit. First Yahoo shut down its remote work back in 2013–2014, then this year, IBM did . Amazon planned to hire a lot of remote workers in 2017 , but with the catch of those position being rather low-paid. However, there are companies like Trello who actively promote remote work and whose entire or almost entire team consists of remote workers. They have recently published a whole guide on the topic , and this is what I would like to review here and compare to my own experiences. Trello argues that hiring remote workers allow them to have better quality of the workforce, since they have a much bigger pool to draw upon. They also have more diversity, which brings more creativity and different perspective. (They don’t say it but I suspect that the financial considerations might also play a role — after all, there are countries where programming talent comes cheaper, without any significant loss in quality). I have worked with my team remotely for two years. I didn’t start as a remote worker though, I just relocated a few years after I was hired, but didn’t want to leave the company. We had a very small team and one of our developers preferred working from home all the time. Then, we hired one more developer in another city, and the team became even more distributed. The last half year (or a little longer) I was also leading this team, so this quite challenging task of making a remote team effective fell to me at that time. What challenges have we faced? Obviously, there was a problem with communication. When you work in the office, you can just come up to the person and ask him a question. When all you have is a chat, you can send a message… and wait. And wait. And wait some more, wondering where the hell the team member in question got to. There was a problem with hours. The team itself had only 1 hour difference in timezones, but the client was 10 hours off and was taking a very active part in the meetings and discussions. Basically, we didn’t have a manager, we worked directly with the client — a CEO of the company, selling the software which we produced. There was a problem with team building. People just don’t feel as a real team when they can’t celebrate events together or do some out-of-the-office things together from time to time. (By the way, there’s a quote in the document referenced above that talks about it: “Oh, and we all get to eat cake. Everyone gets a cake delivered to their place on their birthday.” — Michael Pryor, co-founder of Trello. ) Personal problems/factors might also arise, making it more difficult to work from home. For example, it is difficult to concentrate on your work when there’s a sofa and a TV set close to you (or if, for that matter, you have a screaming baby in the room). You may find it difficult to keep working hours in check, since the whole point of working from home seems to be flexibility. It becomes easier to allow yourself to oversleep or to work late. You might feel out of the loop and disconnected. You might feel like everything happens without you. It is relevant for the cases when only a part of the team consists of remote workers. So, there seems to be two sides to the challenges of remote work: first, how to keep the team productive as a unit; second, how to be productive yourself. Let’s look at the personal side first. When I worked from home, I found that the following things helped most: Have a dedicated space. It is perfect if you have a separate room for a home office, however in our cramped European space it isn’t always possible. You should still try and find at least a corner where you can set up your desk, external screen, headset, printer, fax, whatever you might need. Try not to do work in other parts of your house. The Trello manual also mentions this one, but calls it “Never work from bed”. Needless to say, your office space should be quiet (unless you prefer to work with music). No screaming kids, barking dogs, bothersome relatives allowed. It is your work sanctuary. If you have no way to ensure that, better find a coworking space. Dress up. Never, ever work in your pyjamas or loungewear. This makes the borders between your work and home activity way too blurred and you less concentrated. You don’t have to wear anything special, the jeans and a jersey will do, but these should be the ones you’d wear to go outside. (Chances are, if you are working from home, your profession doesn’t have a strict dresscode to follow.) Set a schedule. Same as with your space and your looks, your time should be structured and separated from your “at-home” hours. You may not follow the schedule to the minute because, as always, life happens and you might need to run an errand, go to the doctor, etc — same as you might have to do while working at the office. But you should strive to keep your work hours and leisure hours firmly separated. The Trello manual also mentions this one in myth #4 . If you do these simple things, you will give you brain a clear and distinct way to switch from work-related stuff from just being at home. You put on your work clothes, you sit at your desk — you are in the work mode. Five (or six) o’clock, you get out of your office space, change into loungewear and go start some dinner — because you are now “at home”. The brain makes this transition much more easily, when allowed some explicit factors to base on. So, this was how you can help yourself be more productive. How, then, can you help your team keep together and make things come true, as if you weren’t just some individuals sitting in their rooms thousands or miles apart? This is what we found to work for us: Video chat is king. You should conduct all your status meetings and the other meetings using a video call. You should see the faces of the people you work with, their expressions. The non-verbal signs are just as important as the verbal ones. Do not gather your in-office people in the conference room while plugging others in as video talking heads. This makes the meeting experience terrible for the people calling in. The Trello manual talks about it in detail, and states that if you have even one person calling in on a video, then all the meeting should be conducted over a video call . I would say that this is very true, but there are still some situations where “most people in the meeting room, some people calling in” might be useful. For example, if this is a talk someone gives at the office, and the people calling in are list listening and not expected to take an active part in the discussion. Be accounted for and make all team members learn to do this. You don’t have to be present at the screen every minute of your time, same as you don’t have to be glued to your chair at the office. But since you are not visible, people have no way of knowing where you are and what you are doing, whether you ignore them or just nipped out for a cup of coffee. (And it is sometimes very easy to assume that the other person is there and just won’t look at his messages.) So, when you step off the computer, change your status to let people know where you are and — this is very important — when you are coming back, or post a message to the shared chat. Try not to make your status message sound like “Back in 15 minutes” because people might have no idea when it was changed — rather, if you do it as a status message, specify the concrete time you’ll be back. (If you send a chat message though this form is acceptable, since the messenger with tag the message with the start time.) Try to have regular team-building events. If at all possible, the company should provide for the budget for the team to meet from time to time, either to have some fun together or to have regular planning/review meetings, or both. In Parse.ly , where the whole team is also distributed, they have an annual outing, when the whole team travels to some remote location (which may be in tropics) to get some sun, drink some cocktails and in between, organize some face-to-face meeting time. It also helps to do something extra for the team members. For example, if the team members have a shared birthdays list and chip in to send everyone a present for his or her birthday. This help feel as if people really cared. These are all things that we did and found working. The Trello document also gives some more tips that we haven’t tried, either because we haven’t thought of them or they weren’t relevant for us. For example: “Mr. Rogers chat” — random grooping of team members to just chat on video. This helps to make people better acquainted with each other if, for example, the team is really big and not everyone is working (or has worked) with each other. “Town Hall” — a company-wide forum of questions, discussions and presentations, occuring once a month (instead of a traditional quarterly review done by one of the big bosses). People in the Town Hall talk about proirities, company values, anniversaries, and new hires are introduced. Remote team offsites — flying the team over to some exotic location to work and communicate for a few days or a week. Expensive, but might be worth it. But all this is only working when you accept the remote culture, which mainly consists of the following principles: Remote workers can and will work as effective (and may be more so, because they are able to work in a peaceful and quiet environment, as opposed to the crazy open space at the office) as the in-office people. Remote workers are not slackers and are not just napping there behind their remote screens. It is possible to achieve the team feeling and synergy with the contemporary communication tools. The remote workers don’t have to prove that they really work. They don’t have to stay online every minute of every day. They should have the same hours as in-office people do, notwithstanding that they save time on a commute. If the team can accept that remote work might make it more effective (or at least not less so), then they can conquer the challenges related to it and be both productive and satisfied with their work and their life. At the very least, it is worth a try. Or more than a try. Originally posted on my personal blog here. We operate a network of online trading platforms in over 40… 47 Remote Working Productivity Software Engineering Olx 47 claps 47 Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-09"},
{"website": "Olx", "title": "monitoring as a service a modular system for microservice architecture", "author": ["Vladimir Kolobaev"], "link": "https://tech.olx.com/monitoring-as-a-service-a-modular-system-for-microservice-architecture-e53bcc144879", "abstract": "About OLX Group In addition to the all-in-one code, our project is supported by dozens of microservices. Each of them needs to be monitored. Having all them monitored by DevOps engineers is hardly possible. We have developed a monitoring system operating as a service for developers. They can on their own configure metrics in the monitoring system, use them, build metrics-based dashboards, set up alerts triggered by thresholds. The only thing that DevOps engineers have to provide is the infrastructure and documentation. This blog post is a transcript of my presentation at our RIT++ section . We have received multiple inquiries about a text version of the presentations made at RIT++. If you attended the conference or watched the video, you will hardly discover anything new. Otherwise, enjoy this blog post. I’ll tell you how we arrived at this solution, how it works, and how we plan to update it. How did we arrive at the existing monitoring system? In order to answer this question, we need to go back to 2015. Here’s how it looked back then: We had 24 nodes responsible for monitoring. There is a whole bunch of crons, scripts, and daemons of all types that somehow monitor something, send messages, perform other functions. We realized that the further down that road we went, the more unsustainable the system would grow. It did not make sense developing the system — it was simply too cumbersome. We decided to select those monitoring elements that we will keep and develop, and those that will be dropped. 19 elements were selected to be kept. Those included only Graphites, aggregators, and Grafana as the dashboard. But what will the new system look like? Like this: We have a metrics repository — Graphites on fast SSD disks and metrics aggregators. Also, Grafana for displaying the dashboards and Moira for the alerting function. We also wanted to develop a system for finding anomalies. Here’s what our plans looked like in 2015. But we had to develop not only the infrastructure and the service itself, but also its documentation. We developed a corporate standard and called it Monitoring 2.0. System requirements were like this: • 24/7 availability, • metrics storage interval = 10 seconds, • structured storage of metrics and dashboards, • SLA > 99.99%, • collection of event metrics via UDP (!). We needed UDP, because we have heavy traffic and multiple events generated by metrics. If they are all immediately stored in Graphite, the repository will crash. We also chose first level prefixes for all metrics. Each of the prefixes has some property. We have metrics for servers, networks, containers, resources, apps, and so on. There is a clear, strict, typified filtering approach, where we keep the first level metrics and drop the others. That’s how we saw the system in 2015. What does it look like today? First of all, we monitor apps — our PHP code, apps, and microservices — in short, everything our developers code. All apps send metrics via UDP to Brubeck aggregator (statsd, rewritten in C). It proved to be the fastest in synthetic tests. Brubecks sends the aggregated metrics to Graphite via TCP. It has a special type of metrics — timers. They are extremely convenient. For example, for every user connection to a service, you send the response time metric to Brubeck. Even with a million responses, the aggregator generates as few as 10 metrics. You have the number of visitors, the maximum, minimum, and average response time, the median value, and the 4 percentiles. Then the data is transferred to Graphite and we see it all live. We also aggregate the hardware and software metrics, system metrics, and our legacy monitoring system Munin (we used it until 2015). We collect all this with the C daemon CollectD (it has a whole bundle of plugins embedded, can interrogate any resources of the host system it is installed onto, and you only need to specify in the configuration where the data should be written) and send the data to Graphite. It also supports python plugins and shell scripts, so you can develop custom solutions: CollectD will collect the data from a local or remote host (let’s assume that there is a Curl) and send it to Graphite. Then, all the collected metrics will be sent to Carbon-c-relay. It is the Carbon Relay solution by Graphite, modified in C. It is a router that collects all the metrics that we send from our aggregators and routes them to the nodes. When routing, it checks the validity of the metrics. First, they must match the prefix layout shown above and, second, they must be valid for Graphite. Otherwise, they are dropped. Then Carbon-c-relay sends the metrics to the Graphite cluster. As the primary metrics repository, we use Carbon-cache modified in Go. Because of its multithreading capability, Go-carbon is much more powerful than Carbon-cache. It receives the data and writes it to disks using the whisper package (standard package, written in python). To read the data from our repositories, we use the Graphite API. It is much faster than the standard Graphite WEB. What happens to the data next? The data is sent to Grafana. As the main data source, we use our Graphite clusters, and we have Grafana as a web interface for displaying metrics and building dashboards. For each of their services, developers build its own dashboard. Then they plot graphs showing metrics taken from their apps. In addition to Grafana, we also have SLAM. This is a python daemon for calculating SLA based on the data from Graphite. As I said, we have several dozen microservices, each of which has its specific requirements. Using SLAM, we check the documentation, compare it with Graphite’s data, and assess whether the availability level of our services meets the specifications. Alerting is the next step. It is built using a powerful system — Moira. It is autonomous because it has its own Graphite under the hood. It was developed by the SKB Kontur team, written in Python and Go, and is 100% open source. Moira receives the same stream that goes into Graphites. If, for some reason, the repository is down, the alerting function will still work. We deployed Moira in Kubernetes and as a primary database it uses a cluster of Redis servers. As a result, we have a fault-tolerant system. It compares the metrics stream with the list of triggers: if there are no mentions in it, it drops the metric. So it is capable of processing gigabytes of metrics per minute. We also added a corporate LDAP, with the help of which any user of the corporate system can set up notifications for existing (or new) triggers. Since Moira contains Graphite, it supports all its functions. So, first you choose a line and copy it into Grafana. Check how the data is displayed in the graphs. And then you copy the same line to Moira. Set up limits and now you have an alert. To do all this, you do not need any special skills. Moira can send alerts by SMS, email, to Jira, Slack, etc. It also supports the execution of custom scripts. When it is triggered and is subscribed to a custom script or binary, it launched the binary and sends JSON to the binary’s stdin. And your program has to parse it. It is up to you what to do with the JSON. Send it to Telegram, open Tasks in Jira, or do whatever you want. For the alerting function, we also use our proprietary solution — Imagotag. We adapted to our needs the panel usually used for electronic price tags in stores. We use it to display Moira triggers. It indicates their status and time. Some of our developers have unsubscribed from notification by Slack and email in favor of this dashboard. And since we are a future-oriented business, we also use this system to monitor Kubernetes. We added it to the system using Heapster, which we installed in the cluster to collect the data and send it to Graphite. The resulting layout looks like this: Here is a list of links to the components that we used to do this. They are all open source. Graphite: • go-carbon: github.com/lomik/go-carbon • whisper: github.com/graphite-project/whisper • graphite-api: github.com/brutasse/graphite-api Carbon-c-relay: github.com/grobian/carbon-c-relay Brubeck: github.com/github/brubeck Collectd: collectd.org Moira: github.com/moira-alert Grafana: grafana.com Heapster: github.com/kubernetes/heapster Here are some performance statistics of our system. Aggregator (brubeck) Number of metrics: ~ 300,000/sec Interval for sending metrics to Graphite: 30 sec Server resource usage: ~ 6% CPU (here we mean fully-featured servers); ~ 1 Gb RAM; ~ 3 Mbps LAN Graphite (go-carbon) Number of metrics: ~ 1,600,000/min Metrics refresh interval: 30 sec Metrics storage pattern: 30sec 35d, 5min 90d, 10min 365d (gives an idea of how the service performs over an extended period of time) Server resource usage: ~ 10% CPU; ~ 20 Gb RAM; ~ 30 Mbps LAN Flexibility We at Avito greatly appreciate the flexibility of our monitoring service. Why is it actually so flexible? First, its components are interchangeable — both the components themselves and their versions. Second, it is highly supportable. Since the entire project is built on open source solutions, you can yourself edit the code, make changes, implement functions not available off the shelf. We use quite common stacks, mostly Go and Python, so it’s quite easy to implement. Here is an example of a real-life problem. A metric in Graphite is a file. It has a name. Filename = metric name. And it has a path. In Linux, filenames are limited to 255 characters. Here come some guys (our internal customers) from the database team. They say: “We want to monitor our SQL queries. And they are not 255 characters, but 8 MB each. We want them displayed in Grafana, see the query’s parameters, or even better, see the top rating of the queries. Would be great if it is displayed in real time. And ideally, they should be integrated into the alerting function.” We set up a Redis server and, using our Collectd-plugins that connect to Postgres and get the data from there, send the metrics to Graphite. But we replace the name of the metric with hashes. The same hash is sent to Redis as the key and the entire SQL query as the value. The only thing left is to enable Grafana to connect to Redis and get the data. We open the Graphite API, since it is the main interface for the interaction between all the monitoring components and Graphite, and enter a new function called aliasByHash () — from Grafana, we get the name of the metric and enter it in the Redis query as the key, in return we get the value of the key, which is our “SQL query.” Thus, we have displayed in Grafana a SQL query, which in theory cannot be displayed there, along with its statistics (calls, rows, total_time, …) Availability. Our monitoring service is available 24/7 from any app and any code. If you have access to the repositories, you can write data to the service. Language doesn’t matter, solutions don’t matter. You only need to know how to open a socket, upload a metric, and then close the socket. Reliability. All components are fault-tolerant and perform well under our loads. Low entry barriers. In order to use this system, you do not need to know programming languages ​​and queries in Grafana. You simply open your app, set up a socket that will send metrics to Graphite, close it, open Grafana, create dashboards, and monitor your metrics through notifications via Moira. Independence. All this can be done independently, without the involvement of DevOps engineers. And this is a clear advantage, because you can start monitoring your project right away, without having to ask anyone for help — either to get started or to make changes. All listed below is not just abstract thoughts, but actual goals, towards which first steps have been made. Anomaly detector. We want to set up a service that will connect to our Graphite repositories and check every metric by different algorithms. We have algorithms that we want to view, we have the data, we know how to handle the data. Metadata. We have many services, and they change over time, and so do those who support and use them. Maintaining the documentation manually is not an option. Therefore metadata are now being built into our microservices. Metadata specify who developed the service, the languages it supports, SLA requirements, recipients of notifications and their addresses. When the service is deployed, all data entities are created independently. As a result, you have two links — one to the triggers, the other one to the dashboards in Grafana. Monitoring for everyone. We believe that every developer should use this system. In that case, you can always understand where your traffic is, what happens to it, where problems and bottlenecks are. If, for example, something has crashed your service, you will discover that not when your customer service agent calls you, but from an alert and will be able to immediately open the logs and check what happened. High performance. Our project is constantly growing, and today it processes nearly 2,000,000 metrics values per minute. A year ago, the figure was 500,000. Meanwhile, we are still growing, and it means that, after a while, Graphite (whisper) will start overloading the disk subsystem. As I said, the monitoring system is quite universal due to the interchangeability of its components. Some have chosen to support and expand their infrastructure specifically for Graphite, but we decided to go the other way — to use ClickHouse as a repository for our metrics. The transition is almost complete, and very soon I’ll describe in more detail how that was done — what the challenges were and how we overcame them, how the migration process went; I will describe the harness components and their configurations. Hope you enjoyed reading this. Your questions are welcome and I’ll do my best to answer them here or in the upcoming posts. If anyone has experience building a similar monitoring system or switching to Clickhouse in a similar situation — share it in the comments. We operate a network of online trading platforms in over 40… 479 4 DevOps Avito Monitoring Microservices Olx 479 claps 479 4 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-10-03"},
{"website": "Olx", "title": "paparazzo open source media picker for ios apps part ii", "author": ["Andrey Yutkin"], "link": "https://tech.olx.com/paparazzo-open-source-media-picker-for-ios-apps-part-ii-b2b58ee1d686", "abstract": "About OLX Group In Part I we discussed the options we reviewed when implementing custom camera in Avito iOS app and shared some implementation details of the final solution. Now it’s time to continue the story. Our photo picker can deal with photos coming from three different sources: ● Photos taken with the camera are kept in the app’s folder on disk. ● User can select photos from the their photo library. ● When editing an existing ad, photos can come from the web. Rather than having three separate entities for each of these cases, we wanted to have a single one to avoid ugly code branching when manipulating photos and to protect the code from changes, should some new image source be added. We have identified four typical actions performed on an image: ● The most common one is, obviously, displaying the image in UI. Ideally, we would prefer not to keep a full-size photo as large as 3000×4000 pixels in memory if we just need a small thumbnail. ● Next is receiving the original image — for example, in order to send it to a server or save it to disk. Again, in this case we want as low memory consumption as possible, so a compressed NSData representation of an image would be sufficient — we don’t want to spend system resources on converting, say, a JPEG picture into a bitmap. ● Sometimes it’s useful to know image size, and getting the size should be optimized to avoid (if possible) loading the image into memory for this purpose only. Often, the size can be obtained from the image metadata or from a server that can send it as a separate property in the JSON next to the image URL. ● Finally, if the image is being downloaded from the web, but at some point we understand that we no longer need it (for example, we have closed the screen where it should have appeared), it would be nice to be able to cancel the download. Since the image may not be available locally at the time we need it, the API in the first three situations should be asynchronous. To display image in UI without loading redundant data into memory, we need to find out what size of it we need. For this purpose, we need to know the size of the area it will be displayed in and how we want to use it: whether we want to fill it with the image entirely or it’s okay to sacrifice some of its parts so that there is no empty space left inside (similar to UIViewContentMode.aspectFit and .aspectFill). Since the API must be asynchronous, we need a handler where we’ll be setting UIImageView.image property. It may also happen that we need to download a photo from the web, and we already have a smaller sized version of the same image cached locally. It turns out that if we display this smaller version in the view while the original image is downloading, the user gets the impression that the download is faster. That’s why we need ‘deliveryMode’ parameter — by setting it to ‘progressive’ we indicate that we do not mind lower-quality versions of the requested picture, and the handler can be called multiple times as the quality improves. ‘best’ means that we want the handler to be called only once passing us the best version of the picture. As a result, the image request method with the described parameters may look something like this: We can simplify it by combining the first three parameters into a structure. It allows us to add other parameters in the future without changing the method signature. The resulting code still needs improvement. First, the handler’s closure parameter clearly indicates the UIImage type, while we wanted to get rid of UIKit dependency to make this method compatible with platforms other than iOS. So UIImage has to be replaced with something that may be subsequently converted to UIImage. There is a type that meets this criterion and is present on both iOS and macOS — CGImage. This led us to declaring InitializableWithCGImage protocol: Fortunately, UIImage and NSImage already have these initializers, so all we have to do is to add blank extensions, formally indicating their conformance to the protocol. After replacing UIImage with this protocol, we end up with the following method signature: Finally, we want to ensure the possibility of canceling the request. To do this, we add return value of type ImageRequestId to ‘requestImage’ method, that will give us the opportunity to identify the request. There is only one minor modification left. Earlier, I said that if deliveryMode is set to ‘progressive’, the handler may be called multiple times. Ideally, it should be possible to be able to figure out within the handler whether it has been called with the final or intermediate image version. Therefore, we will call it with ImageRequestResult structure as the parameter. In addition to the image itself, this structure will contain other useful information about the result of the request. This is the final version of the method for displaying the image in UI. Three other methods are simple, two of them are essentially asynchronous getters. Now we have ImageSource protocol, perfectly suited for use as a model of our photo picker, and the only thing left is to implement it for three possible cases: photos from the built-in storage, from the web, and from the user’s photo gallery. Starting with iOS 8, access to the photo library is provided by Photos.framework. The library itself is represented by PHPhotoLibrary instance, and photos in it are represented by PHAsset objects. To get photo representation that can be displayed in UI, we use PHImageManager to convert PHAsset to UIImage. The method behind this transformation has the following signature: As you can see, it looks very similar to ‘requestImage’ method in our own ImageSource protocol. That’s because the first implementation of ImageSource was basically a PHAsset wrapper, so we largely relied on that signature. Unfortunately, while working with PHImageManager, we encountered some problems. The first problem became obvious when we were working on the task of displaying photos in collection view. PHImageManager doesn’t guarantee that resultHandler will be called when the request is canceled. Sometimes it will be called, but image will be nil. We wanted to simplify the client code so that it did not have to figure out what had happened. Thus, a strict set of rules for calling resultHandler was developed, one of which stated that resultHandler must not be called after canceling the request. The solution to this problem was fairly simple. PHImageManager’s resultHandler is given two input parameters — the first one is UIImage, and the second one is ‘info’ dictionary containing useful information. This information includes a flag allowing to determine whether the request has been canceled. But this flag may not be there if the request was canceled after this resultHandler call was added to the queue. So we had to store an array of canceled requestIds and search for the request in it. The second problem came when we were working with a photo from iCloud, and we needed to show the activity indicator during the download. The only possibility to track the download is to set ‘progressHandler’ closure in PHImageRequestOptions object, which is then passed to PHImageManager when the image is requested. We needed to track only when download started and finished, so we added two closures to our own request parameters structure. And if in case of starting we simply activated onDownloadStart at the first call of progressHandler, onDownloadFinish required a bit more work. If we were lucky and progressHandler reported that the picture download was 100% complete, which corresponds to the value ‘progress == 1’, we called onDownloadFinish at this point. However, the trick is that it may not happen, and the last call of progressHandler is at a progress value of less than 100%. In this case, we try to guess whether the download is complete or not inside resultHandler. The ‘info’ dictionary, which we receive in this callback, contains IsDegraded flag indicating whether we have received the final or an intermediate version of the image. Here it is logical to assume that the download is complete either if we have canceled the request or if the final version of the picture has been received. You can explore the implementation of ImageSource for photos from disk and from the web in Paparazzo repository . Our media picker has already caught the attention of iOS developers, including global websites . They report that our photo picker has excellent performance, elegant and straightforward implementation. You are welcome to try it out, test, discuss. Avito team is always happy to answer your questions. We operate a network of online trading platforms in over 40… 1 Swift iOS Open Source 1 clap 1 Written by Senior iOS developer @ Avito We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior iOS developer @ Avito We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-06-12"},
{"website": "Olx", "title": "api monitoring with new relic and elk stack", "author": ["Jędrzej Józefowicz"], "link": "https://tech.olx.com/api-monitoring-with-new-relic-and-elk-stack-6847fc0112cf", "abstract": "About OLX Group If you ever had to deal with supporting partners using your API, you know just how difficult it might be to track down a bug. The client makes a complaint, you try to replicate the situation and… it works just fine. You need more details so the e-mail thread continues, time passes, the client gets frustrated and so on and so forth. Have you ever tried to find a specific problematic client request among millions of other API calls? We had this problem at OLX EU and we looked for a solution tailored to our needs. It had to be configurable, hosted locally and it had to be free. The ELK acronym stands for Elasticsearch, Logstash and Kibana. What we basically do is that we log all information pertaining to the customer’s request. And I do mean ALL information — the URL that clients use, the sent payload and the API request output (for unsuccessful requests). What are the perks of this solution? You see exactly what your API client sent your way in just a few seconds. No need for lengthy e-mails and countless questions, and it all happens instantly! Just select the correct date range and use any unique identifier (at OLX we use either the internal advert id or the client’s id). Everything you might need shows up in a matter of seconds. Obviously Kibana can do a lot more than that, everything depends on your specific requirements. You need charts based on the logged requests? Simply go to “Visualise” section and visualize what you want! I will focus solely on the application side of the configuration. For this purpose let us assume that there is an already running machine with ELK. First of all you have to configure Logstash so that it takes in incoming messages and passes them to ElasticSearch. The application file could look as follows: Logstash is told to look out for incoming connections on UDP port 5000 and to add the field “type” with the “api-requests” value to each request. The following section specifies ElasticSearch index prefix and states that the message is in JSON format. We use UDP to log data to Logstash as we want to avoid any issues related to logging. In general Logstash is a fast and stable service but we don’t want our users to experience any delays or see error pages in case of network issues or crashes of the logging machine. This makes our solution reliable — we have logs which are entirely transparent for users. We use Monolog with a ready-made plugin for Logstash. The only thing we had to add was a simple custom Monolog Handler that connects to Logstash and push data. Besides that, you obviously need some logic in order to prepare an event to log. We decided to log everything — GET and POST data, headers and all data relating to the API clients. Sensitive data such as access tokens and passwords was the only thing that was excluded from logging. There is one catch: the UDP protocol has a limit of 65kB of data, which means that you need to handle this manually (truncate the data or even skip such a record) or otherwise language errors will occur. The extensive logging provides us with a lot of data — a couple of gigabytes of data in ES daily. There is a script cleaning ES from indexes older than 14 days that uses the simple ES REST API. And that’s it! It is that simple to get a tool that is easy to use and enables you to browse through almost all API requests. It is a great help in everyday work. When will you know that API integration has failed? Waiting for the client to call is one way to go about it, but what if you want to be proactive? Here enters the New Relic. (If you never have seen NewRelic in action, I strongly recommend to take a quick tour here ). I will skip basic installation and configuration, as there are many sources that will explain how to do it. Let’s focus on the application side. First of all we had to separate the API part of the traffic from the rest of the application. Even if API is still a part of the application the statistics need to be separated. We were able to achieve this extraction by explicitly calling newrelic module functions in the code responsible for API. Once it’s been released, all of the API requests started to accumulate in the application so we experienced traffic, performance and error analysis (together with the remaining NewRelic features). However this only covers the technical side of monitoring, while we still need more business monitoring. A couple of months ago New Relic has introduced “New Relic Insights” — a powerful data analysis tool based on the data gathered by Newrelic agents. Even with a new tool there already is a lot of data to be analyzed. For that reason we created a dashboard monitoring traffic of errors separated by HTTP code — 4xx errors have a different meaning than 5xx, don’t they? But… what if you want to monitor one particular API client? NR allows you to tailor requests (transactions) with custom parameters. As all of our endpoints require OAuth authorization, we added the information about the client id from the token to NewRelic. It was that easy — with a single parameter we gained access to “personalized” statistics of requests. The way you will use the data is entirely up to you and your needs. Alright, all of the above pertains to passive monitoring. But let’s do some active tests. Let’s assume you want to be sure that your API main entry point (OAuth authorization) works fine — if the client provides correct credentials, he/she will receive a valid access token. This is the point where “Synthetics” comes to the rescue. In simple words, it’s a kind of scripted monitoring. You write a script and assertions, pick locations across the world that you want to run it from and… wait. That’s it. In our case, I wrote a simple script that uses OAuth authorization endpoint and checks results. If a token is valid — it’s a pass. If not — error is triggered and NR sends a notification to Slack. That is all. Myself, I used it out of curiosity. We don’t write any more scripts here because we have other types of tests. However, it might be an interesting option for some of you. With this kind of metrics it’s easy to create a custom check. The error rate is too high? Not a problem, let NewRelic send a Slack notification. OAuth authorisation returns 503 instead of tokens? Send a notification, let the people see that there is something wrong. The use of ELK stack allows us to quickly respond to issues reported to us by consumers. Now, it is enough to copy the unique advert ID (or any other type of unique data the client provides you with) and paste it to Kibana in order to get the full history of requests regarding the specific item in a blink of an eye. Additionally, New Relic allows us to take a look at API as a whole. You can see the detailed performance, stability and errors. Depending on your needs you can also access other data. You decide what to do with New Relic. This is the very beginning of our journey of monitoring with NewRelic and there are still many ideas to be tested. However, I am confident saying that API debugging has never been easier than it is now :-) We operate a network of online trading platforms in over 40… 22 1 Elasticsearch API PHP 22 claps 22 1 Written by Software Developer @ OLX EU We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Software Developer @ OLX EU We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-02-20"},
{"website": "Olx", "title": "how we changed our payment service provider over an a b test via haproxy", "author": ["Rafay Aleem"], "link": "https://tech.olx.com/how-we-changed-our-payment-service-provider-over-an-a-b-test-via-haproxy-13f722dc0b86", "abstract": "About OLX Group Imagine you have to rewrite an existing web service to move to a new payment gateway (PSP or payment service provider) due to various business use cases. Your first thought might be to replace old with the new one in its entirety and roll that out. That is a naive approach especially when you are working with payment gateways which have their own SLAs, agreements with acquiring banks, risk and fraud detection softwares, etc. which makes this process riskier in terms of conversions, revenue, customer retention and eventually business. In this blog post, we discuss the approach that we took to mitigate risks while switching payment gateways and why it was so important. Our old payment service is written in Python 2 and is highly coupled with the old payment gateway (payment service provider). When we first dissected the problem, we thought that integrating a brand new payment gateway within the same flows, urls and Python Bottle views would be trivial. Once we started working on the first POC, we realized that we were already generating a lot of spaghetti code as API flows for these two payment gateways were entirely different. While the old payment gateway API design had to rely heavily on Redis and Gevent to optimize user and payment flows, there was absolutely no reason to re-introduce that dependency with APIs from the new one. A/B Tests Are Good As Long As They Are Simple A/B tests are very powerful in making product decisions. At dubizzle , these tests have traditionally been more oriented towards the user facing components such as page flows, page components, placements and products. However, these tests complicate things when you want to test underlying systems that are highly dependent on each other. While we were still working on our POC, we realized that the A/B test we wanted to perform should not be done using tools such as Optimizely even if we somehow managed to integrate the new gateway within the same views and user flows. Here is why: Introducing third-party hosted JavaScript on a payment service that can potentially capture credit card details is a huge security risk. Going the Optimizely route would have required us to implement some A/B test logic on each and every view of the web service to ensure that we direct requests across all views for a transaction to the correct payment gateway based on cookie. This could be something like the following on every view and is definitely not neat: Debugging two different payment gateways that are using same namespaces such as Redis, UUID generation and logging would definitely fireback and we would be scratching our heads in no time. Note that once a user lands in bucket A (web service talking to old payment gateway), that web service needs to ensure that payment flow for that user continues on the same payment gateway on which it was first started. For instance, web service should never initiate transaction on old payment gateway and try to finalize it on the new one. This problem can be resolved using sticky sessions which are supported by Optimizely and HAProxy both. Once we had some idea around the problem we were trying to deal with, we decided to write a new payment service integrated with the new gateway and compare the performance through a 50–50 split A/B test. The whole A/B test had to fulfill at least the following: Look and feel of card details page for new service should exactly match the old one to ensure that we do not skew any conversions. Conversions are considered as card “Submit” events in this case. No internal backend tasks or API calls should be able to increase the rate of transaction failure in new service as compared to the old one. This would mean that most of the user flows should essentially work in the same way as the old one. Since Optimizely was out of the equation, we decided to leverage HAProxy for running the test. HAProxy is a powerful layer 4 and layer 7 load balancer with an extensive set of features. One such feature is its ability to enable cookie based persistence in a backend which happens on layer 7. We configured our HAProxy to have three backends: Main backend Old service backend New service backend To give you a taste, here is an example of our HAProxy backends: You might be thinking why we had static backends? This would become clear once we come to HAProxy frontend so lets discuss the main backend first. We went for weighted round robin approach for load balancing requests between old-service-old-pg and new-service-new-pg . This makes sense for an A/B test where you just have to split the traffic to A and B buckets with the caveat that any request that lands on bucket A should never land on bucket B for that session duration. We neatly achieved this through HAProxy cookie directive which is very powerful. With our assumption that any user who initiates a transaction would complete it within 2 hours, we told HAProxy to discard a session cookie after 2 hours and generate a new one based on what round robin decides for that request. This solved two very strong use cases for us: Gave us the flexibility to tweak A/B test weights to propagate to all users in 2 hours without breaking any sessions or user flows. Increased the probability of a user attempting transactions on both services during the lifetime of the A/B test. This helped us in identifying issues where one gateway would reject the same credit card that was accepted by another one. We were literally amazed to see how things can break in bigger contexts where multiple cascaded systems spanned across continents are involved! There is a small vulnerability in our setup that you probably wouldn’t have noticed yet. Consider the following diagram: A user starts a transaction on the old payment service at 45 min mark after receiving the cookie. He then proceeds to 3-D Secure page of the bank at 1hr 59th minute and is redirected back to our success url after the 2hr mark. Since HAProxy is configured with cookie maxlife of 2hrs, it will discard the session cookie and try to insert a new one on redirect to success url. If we are unlucky enough, round robin might tie the new session to the new payment service which would not know how to handle a success redirect that was configured by the old service. In our case, we chose to ignore this problem because we have seen earlier that users who initiate a transaction would usually complete it under two hours. But can you imagine the severity of this problem if we had set cookie maxlife to 1 min, for example? Lets come back to our discussion around why we had old_service and new_service backends along with main . HAProxy is usually configured with a frontend proxy that handles all ACLs (Access Control Lists), which in our case was configured like this: These webhooks and endpoints that you see in the configuration are specific views to handle requests originating from the external gateways. Since old and new services cannot speak to each other’s payment gateway, putting them under a round robin LB would mean 400s or 404s for as much as 100% of the requests. Also, since these requests are coming from payment gateways, there is no need for any sort of persistence because these do not incorporate flows spanned across different views (All requests are touch and go with a 200). The best place to fix this problem was on LB and we did this by defining ACLs to control request flow to appropriate application servers through old_service and new_service static backends. To put it in simple words, this would be a conventional conversation between payment gateway originated request and HAProxy that would eventually hit the appropriate application servers: Payment Gateway: Hey, I am an internal request from old payment gateway and I need to tell you that I have successfully received the payment. HAProxy: Hey, I recognize you! You should pass through door old_service to reach your destination application. Destination Application: Hey, I know how to process you! Lets activate the order for this user. I will give you the number 200 in return! With all conditions in place, how would you test such a complicated A/B test? HAProxy provides you with very precise logs to debug such complicated systems. Lets get through some examples from our actual A/B test logs to understand this. Notice flags NI , VU and VN for each request made for the same order id. These flags give a lot of information on how persistence was handled by client, the server and by HAProxy and are one of the most important indicators you would look at while testing and debugging. Quoting HAProxy docs here : — : Persistence cookie is not enabled. This is the case where the request path is webhook-new and it doesn’t makes sense to put it under the A/B test. NI : No cookie was provided by the client, one was inserted in the response. This typically happens for first requests from every user in “insert” mode, which makes it an easy way to count real users. This is where round robin would decide the test bucket for the user. VU : A cookie was provided by the client, with a last visit date which is not completely up-to-date, so an updated cookie was provided in response. This can also happen if there was no date at all, or if there was a date but the “maxidle” parameter was not set, so that the cookie can be switched to unlimited time. VN : A cookie was provided by the client, none was inserted in the response. This happens for most responses for which the client has already got a cookie. This is how HAProxy finds the current bucket for the user and directs him to the correct backend. Notice that once HAProxy decides server new-service-new-pg and sets a cookie, all subsequent requests from that user are directed to new-service-new-pg through the main backend. For every thing else that matches one of the ACLs, HAProxy directs the request without setting any cookie. This also ensures that A/B test results are not skewed by HTTP calls from computers rather than humans. Note that DNS and Edge Tier are common across all our micro services. All the A/B test magic happens after the traffic passes through HAProxy load balancers. While this has worked for us perfectly, there are more complicated load balancing use cases which could potentially require a lot more parameters than just session persistence. Shopify has covered this in an excellent blog post here by leveraging Nginx and OpenResty. This A/B test was a major team effort across the dubizzle infrastructure team and product engineering. Thanks for everyone involved! Hope you enjoyed this post. Feel free to add comments or ask questions. You can also reach me out on my twitter handle: mrafayaleem . We operate a network of online trading platforms in over 40… 13 Payments Python Software Development Software Architecture Software 13 claps 13 Written by Data Engineer at PointClickCare. Based in Toronto. Music aficionado who likes playing guitar and is an Eric Clapton fan. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Data Engineer at PointClickCare. Based in Toronto. Music aficionado who likes playing guitar and is an Eric Clapton fan. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-13"},
{"website": "Olx", "title": "test automation patterns not only page object", "author": ["Pawel Maciejewski"], "link": "https://tech.olx.com/test-automation-patterns-not-only-page-object-55ff25238e3b", "abstract": "About OLX Group Software development patterns that will help you create better automated tests. The first thing that comes to mind if you think about patterns in test automation is Page Object pattern. Almost all know it and almost all use it, but it’s not the only software development pattern, that you can use in your test automation suites making them flexible and maintainable. In my article, I would like to focus on following subjects. I would like to show how to extend Page Object pattern and design your pages using factories and entities in your tests in order to facilitate your assertions and make them clear and meaningful. Beneath that, I would like to present how to benefit from command and builders in your test code. As you may know from the Greek Mythology Trojan Horse was ‘hero’ in The Trojan War tale. After ten years of city’s siege Greeks decided to construct a huge wooden horse and hid their men inside. Troops pretended to sail away and Trojans pulled the horse into their city. It was Sinon who pretended to have deserted the Greeks and while kept captive, told the Trojans that the giant wooden horse the Greeks had left behind was intended as a gift to the gods to ensure their safe voyage home. Going back from the ancient Greece to OLX code. In our case Trojan is our way into the app and Sinon is our helper. Trojan is our own testing API. It is simple REST API that is enabled on non-production environments. As it is internal there is no authorisation too. It allows us to create or generate test data in our test suites. Our approach is to create test data right before the test so that it is the test itself that knows what data needs to be generated. Sinon is abstraction layer over Trojan API. It allows easy usage of methods in the tests. This approach is not common, people usually store their test data as fixtures or raw SQLs that are injected into the environment before test execution. What is better in data generation via API is that you don’t need to maintain and store test data. Gathering data and preparing your environment to test is just part of the scenario — the test is responsible for preparing it for itself. When using that kind of API you usually operate on some database abstraction layer so you are not dependant on database schema or any configuration of its server. When generating, even partially, test data you also have greater and wider coverage, but you must be cautious as it can lead to nondeterministic and flaky test scenarios. Also, notice the last line of Sinon’s call for creating a new user. It returns User’s entity which is part of app domain and it should be common in the entire lifecycle. Builders, Factories, Entities Let’s take following example. It is simple API test that at first generates user and then requests it via API. Notice that Sinon actually returns an entity. Also, the API class is designed to return the same entity. It contains common data, but what is crucial here is that also entity identity is the same. Both our entities contain domain identity, i.e. they are the same section of application domain. Application and test operate in the same domain so we can make the assertion on that identity. Besides kind of philosophical gains that approach gives us also some technical benefits. Asserting actual object instead of values one by one gives us clear test code and clear assertion output in case of failure. What we need to be aware is that any additional value added in expected or actual object will cause a test to fail as we expect identity. Another way of dealing with entities and making them shareable is using Factory Pattern. With that approach, you can make the same structure from different sources. The entity from the JSON or from the array may contain different structure, but the data and actual entity is the same. While the simple factory is useful while handling arrays or JSONs (so mostly API testing), UI testing may need to be handled differently. In the example below, you can see how I handled building entities based on data from application UI. First, you need to gather data from application. Then you pass data into builder class that is responsible for creating an appropriate instance with given data. When you write tests against a web page, you need to refer to elements within that web page in order to click links and determine what’s displayed. However, if you write tests that manipulate the HTML elements directly your tests will be brittle to changes in the UI. A page object wraps an HTML page, or fragment, with an application-specific API, allowing you to manipulate page elements without digging around in the HTML. Martin Fowler, https://martinfowler.com/bliki/PageObject.html Most of the libraries that implement Page Object Pattern focus on the external interface of page — its methods that allow interaction with the page. But what is usually omitted is efficient page elements definition. Let’s take a look at example of page object from Selenium’s documentation: As you can see ids are hardcoded in the method and you cannot reuse the locator or element itself. What you can we do with it? I started with the first step — instead of defining the elements on the fly in the code let’s extract them as class instances. Then we can define a type of each element: For more simplicity just initialise elements in the constructor and finally, you get: What you get is clean and transparent page object following Object Oriented Design Principles. Elements that you use on your page should be designed carefully and if possible inherit the same interfaces. I’ll cover that later, but just note that it can be really helpful in your pages and tests design. As an example let’s take base and text field elements. First is abstract the class that contains methods for common elements actions. click() method can be used in each element so there is no need to define and implement it separately for each element. getValue() implementation may differ so it’s only defined as abstract method — this way it will serve as an interface and without knowledge about element we can manipulate with a page and try to get value for element provided. TextField element implements methods with specific implementation or specific methods for that element. TextField element implements methods with specific implementation or specific methods for that element. setValue() cannot be defined as abstract in BaseElement as not all UI elements may give you the possibility to set actual value — let’s take Label or Button. Above I explained factory pattern usage in a creation of test entities. Now I would like to show how that approach can be used for pages. Imagine you have several versions of the app and you do not want to create separate scenarios for each version. You can define only one scenario and in the context of the scenario, in the example, it is the country that determines a version of the page, you return suitable page. The only thing that you need to take care is to the create common interface for each page. Another case is with page elements. It may happen that elements differ between different versions of the same page. With the usage of extended page object pattern, you can define different elements in your page factory. If they implement the same interface it’s transparent for page calls. As a result, your test code is clear and simple — for all cases, you can use the same page’s method. It’s the scenario context that determines actual page implementation. Factory, Builder, or Entity Abstraction patterns are widely known in the software development world but usually, are not introduced in the test development. It does not require much more technical skills than regular test implementation and can help you make your tests maintainable and reliable. We operate a network of online trading platforms in over 40… 7 1 Testing Software Development PHP 7 claps 7 1 Written by Test Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Test Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-02-24"},
{"website": "Olx", "title": "migration from objective c to swift a case study", "author": ["Dominique Stranz"], "link": "https://tech.olx.com/migration-from-objective-c-to-swift-a-case-study-c56a99becbd", "abstract": "About OLX Group Let’s imagine that we have a mature iOS app under active development. At some stage one of the team members comes up with a suggestion: “Why don’t we migrate to Swift?” Obviously you know that suspending feature development for 3 months and rewriting the entire app almost from scratch is not an option. 3 months ago at OLX we have embarked on a journey of migration. On the way we’ve encountered countless challenges and problems and now we feel that we want to share some tips and good practices that you may find helpful during future migrations. First you need to identify the parts of the code you want to start with. The process will be smooth and easy if you just follow the rules: File by file migration is the most effective way to go. Start with files without subclasses — you cannot subclass a Swift class in Objective-C. Remember: Objective-C will not be able to translate certain Swift-specific features developer.apple.com If there are parts of the code that you don’t want to migrate just yet (e.g. in-house pod) it is worth considering adopting Nullability to Objective C code. You will avoid ending up with a Swift code with implicitly unwrapped optionals. Each part that would work as nil in Objective-C will crash in Swift (accessed without necessary nil-check). Mix-and-match functionality makes it easy to choose which features and functionality to implement in Swift, and which to leave in Objective-C. Interoperability makes it possible to integrate those features back into Objective-C code with no hassle. This nice quote comes from a book that was already mentioned before: “Using Swift with Cocoa and Objective-C”. In the real world however, sometimes it works more like this: Only classes and types included in the automatically generated Swift header file will be accessible in Objective-C code. They absolutely have to be stored in *.m files. If both Swift and Objective-C classes are in the application project, use: If both Swift and Objective-C classes are in the same Pod project, use: If Swift file is in another Pod, use: (with DEFINES_MODULES and use_frameworks! enabled) In Objective-C header files only use forward declarations for @class ClassName and @protocol ProtocolName . Importing a Swift header file there will leave you with a circular reference. Protip: ⌘ + click on Swift class name to see its generated header You have already migrated the first class but fail to see it in Objective-C code? The property is not visible? The mixed projects have some limitations and you need to respect certain rules: In order to be visible in Objective-C all classes should inherit from NSObject In Objective-C primitive types cannot be nullable, so properties with Int? , Int! , Float? , … will not be visible Some names are already used in NSObject , like description: String Swift classes have objc_subclassing_restricted property, so they cannot be subclassed in Objective-C Methods with NS_REQUIRES_NIL_TERMINATION will not be visible In Objective-C enum can only have Integer type, so all of our fancy Swift enums with associated values or String type will not be visible. Additionally, enum visible in Objective-C should have the @objc prefix. Unfortunately NSCoder is based on Objective-C runtime and as such will not see properties, which are not exposed to Objective-C (like enum: String , etc…). Instead, you will end up with a runtime error: “this class is not key value coding-compliant for the key status”. How can we deal with it? It is possible to map property value for type known by NSCoder. In this example we are converting enum: String to it’s rawValue. Representing all error reasons in Swift enums along with their associated values is a very convenient and recommended way to go. It works well, unless we use these errors in Swift: But when we try to figure out the failure reason in Objective-C it comes out… empty: Fortunately in Swift 3 we are able to use CustomNSError protocol that describes the error type -provides a domain, code, and user-info dictionary. Then in Objective-C we will have: Before the release remember to double check all string interpolations in order not to end up with Optional(\"2000 PLN\") visible for user. In Swift 3.1 this raises a warning: String interpolation produces a debug description for an optional value; did you mean to make this explicit? Before that make sure that you have unwrapped all Optionals. A very long time ago, back in the day when nobody even thought about Swift we have implemented Push Notifications with a solution that proved quite popular at the time: stackoverflow.com We quickly realized the extent of our mistake when AppDelegate was migrated to Swift and all of the new tokens in the database were equal to “32bytes” string. Why did that happen? Because Swift Data description is equal to “ N bytes ”. How to parse it properly? Iterate through data bytes array and concatenate as hex strings or… use deviceToken.toHexString() from Marcin Krzyżanowski’s CryptoSwift library. After 3 months we have migrated about 40% of our code to Swift. It hasn’t been an easy ride but one that is absolutely worth it. Still, there are many drawbacks in Xcode 8 Swift support (slower code completion or disabled refactoring to name a few), but Swift makes up for it by enabling us to use powerful enums, protocol extensions and lazy variables. We are definitely ready to embrace all of the new features proposed by the Swift community! We operate a network of online trading platforms in over 40… 24 1 Swift iOS Objective C Olx Xcode 24 claps 24 1 Written by iOS Developer @ Allegro We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by iOS Developer @ Allegro We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-02-13"},
{"website": "Olx", "title": "clean architecture at olx europe", "author": ["Dawid Mazurek"], "link": "https://tech.olx.com/clean-architecture-at-olx-europe-f0501c37f09d", "abstract": "About OLX Group Post is about how our domain code is structured It is a complicated process to build a large application destined for many markets. Local conditions may vary in many aspects: legal regulations or other cultural, technological, financial and functional differences. Such difficulties couple application to regional context, where logic is depending on this on many levels. In a well-designed system, the local context is fully separated from the application code. This can be achieved in many ways, but at OLX Europe we decided to apply Clean Architecture and to extract all differences between versions of application to a local platform level— according to the Ports & Adapters architecture. This architecture contains a base system (port) and other systems that communicate with it — adapted to local requirements (adapter). A good example of such port is REST API, where the mobile app, desktop client and the CLI scripts could be proper adapters. System does not have to be limited to a single architecture. Many of them are complementary — we can use them simultaneously in same components. It may also be convenient to apply different architectures to individual elements of the system. Some features may benefit from using Layered Architecture , other from Event Driven Architecture — everything depends on application requirements. We call such approach Feature-Driven-Architecture where we can try out new solutions for single functionalities without interfering into the system as a whole. Clean Architecture has been promoted by Uncle Bob as a proper solution to a problem of treating the database as a heart of the application. In the Clean Architecture, the database is an implementation detail, I/O used to exchange data. When writing a business code, you don’t have to worry about a precise data structure and the way data will be stored. Such decisions can be done later, often saving lot of time, because application requirements can change during development. It is a good practice to use CQRS, what means to separate the write-model from the read-model. In short, in such case you use one object to query the system and another one to interact with it (change its status). We have solved it by implementing the Command design pattern which we use in a similar form in two versions: QueryObjects and CommandObjects. Example: This is a query, which hides the how an operation is actually performed. More detailed implementation can look as following: Here you can see a class witch name specifies exactly what it does. This is how we have built all business logic use cases. This is entry point to domain logic from the perspective of platform code. One class is destined to perform only one operation. Information on how the data is delivered is irrelevant at this stage. You can assume that the repository contains a mechanism that will deliver all the valid objects you need. For passing domain values we are using value objects. Those are elements containing own validation, what prevents from create a value object with incorrect value. As a result, we don’t have to maintain validation in a rest of application. Value objects have immutable state. This means that the internal value of a created object cannot be changed. As a result, the same object can be used safely in various elements of the system without having to worry about it being modified by reference from another context. There may exist public methods enabling value modifications, however they do not modify the value physically, but only return a new object with a new value. Value object that allow creation of a new value: The repository is responsible for the exchange of data contained in domain objects between business cases and data sources. It offers methods for saving and retrieving data. The repository hides the implementation of building objects and the way the objects are sent to and collected from data sources. It may also entirely encapsulate the logic of entities by hiding them behind the entity aggregate. Full implementation: Here you can see several other elements that reduce responsibilities of the repository. The gateway is an element that provides the repository with data structures, while the repository role is to return their representation in the objective form. There is 2-way communication and it always use simple data structures organised in arrays. It allows to separate domain layer from infrastructure layer. The repository expects injection of a class that implements the gateway interface. The class may be any source of data, e.g. database client, API, file storage, cache layer. The sources may also be mixed, e.g. a cache layer-based gateway can be based on any further gateway implementation. A sample gateway interface can look as follows: It specifies methods used for communication between the repository and a data source. A sample implementation of a gateway that retrieves data from the database: The names of tables/columns are stored in a class that aggregates them in the const values. It allow to keep high code readability and makes it possible to freely change the names of sources and attributes in just one place without introducing the same changes in other parts of the code. The gateway can be implemented in any way, because only its interface is coupled with the domain logic. Every implementation is closely related to the particular use case in a given application, so it can be coupled with the application and its framework. An example of such coupling is using the bundled database access layer or cache mechanisms. When implementing the gateway for a clearly defined purpose (e.g. related to the database), you don’t have to worry about cache. Cache should be additional layer, layer that can be implemented by creating a proxy that caches access to the gateway. With this approach, the cache layer can be applied to other gateways — no separate cache has to be created for every type of data source. Cache can take the form of proxy access to the database, API, text files or even to the next cache layer. Example: Such proxy makes it possible to add a cache wherever needed. A queried element is searched in cache. If it is not found, the search is transferred to the next gateway. It is optional to use the proxy. Cases of using gateway in the repository may look as follows: Repository with a caching gateway: Repository without caching: Building repositories in all use cases manually is hard and prone to errors, this is why it is worth to implement a factory, which would always build a valid result object. The best option is to use a dependency container that enables dynamic creation of dependencies for services that depend on such repositories. Another advantage of this approach is the ability to change behaviour of the existing code, without modified it, just by changing dependency injection container configuration. Another element of the repository is the entity factory. It encapsulates from repository the way the objects are created. It limits the amount of code in the repository and makes it possible to use it outside of the domain. Sample factory: In the example above, the factory has 2 methods. The create method creates a single instance, while the createClone method clones the entity prototype. If you need to create only one instance in the repository, use the first method. However, if you operate a collection of entities, use the prototype option to cut the costs of initiating every new object. The last repository dependency is the hydrator. The hydrator is a class that mediates in communication between the gateway and the repository. It serves two responsibilities. The first one is entity hydration, that is connecting an instance with data that represent that instance. The second one is a reverse process, that is extraction — separating a data structure from the entity. When using the hydrator, it is not possible to create an entity that contains incorrect data — the hydrator uses value objects that apply their own validation when setting the data. Therefore, it is worth using it also outside the domain, where entities are created manually. An output element from the repository can be either entity or aggregate of entities. To make it simple, an aggregate is a collection of entities that can be used to perform an operation, such as adding new elements or calculating the sum. It is a layer that enables interactions with entities, without knowing anything about the interfaces of individual entities and without having to perform low-level operations. It is a convenient way to add a new logic to the result provided by the repository, to clean application layer from low-level operations even more. Proper implementation of use cases inside the application is a major problem. In legacy systems, a change of code in one place often leads to uncontrolled changes in another place that depends in one way or another on the place where the change was initiated. Such problems can be greatly limited by separating services from their logic. Proper implementation of use cases inside the application is a major problem. In obsolete systems, a change of code in one place often leads to uncontrolled changes in another place that depends in one way or another on the place where the change was initiated. The issue can be solved by extracting logic to services. A proper place to use services are controllers and dependencies on other services. Service use in the controller: The example shows that you do not need to create dependencies on your own. You can use all the above elements to perform a domain operation on the controller level. Also, you don’t have to worry about many elements that are the weak points of the legacy application, like global contexts or database support, and about the way data structures are named and how they look like. From the business logic perspective, all of these are just implementation details that could change, while the business logic remains same. Clean Architecture is an architecture that presents a very flexible approach to application development, due to the fact that every element can be replaced with another one and the use of a design pattern makes it possible to add new functions without modifying the existing code. The database system is no longer the key element of an app. Now, the main part is the business logic. Separation of a business logic from the framework gives vast possibilities of migrating application to different framework. It also makes easier to implement and test new tools. Well designed application structure should be self-explanatory. When reading a file structure, you should be able to define the purpose of the app. Unfortunately, it is more often that the only information that you can deduct at first glance is the framework on which the application is based. Use cases, that are queries and commands, specify everything what application can do. This architecture is very useful during refactoring old systems. It is easy to separate its domains and restructure the app in small iterations. Sticking to SOLID rules used in examples above make it easy to achieve 100% test coverage of the business logic. If you have found the above text interesting, join the discussion and see mine examples of implementations on Github: PHP7 implementation https://github.com/DawidMazurek/php7-cleanarchitecture-demo ECMAScript 6 implementation https://github.com/DawidMazurek/ecmascript6-cleanarchitecture-demo I gave talk about this topic during PHPCON 2016. Slides from my presentation: https://speakerdeck.com/dawidmazurek/clean-architecture-w-olx-1 As soon as recording of talk is published, i will attach it here. We operate a network of online trading platforms in over 40… 101 1 Software Architecture PHP Clean Architecture Php7 Software Development 101 claps 101 1 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-02-06"},
{"website": "Olx", "title": "automation is not enough start automating automation", "author": ["Diego Garber"], "link": "https://tech.olx.com/automation-is-not-enough-start-automating-automation-eb91be327b8d", "abstract": "About OLX Group It all started on my holiday break, since I was in charge of creating deployment pipelines for new components, I was pretty used to the routine, I even had it documented and all: Copy one example YAML spec file and adapt it for the new component Add the new component to my Ansible inventory Copy & paste init scripts from another similar component Create a new Jenkins job (also copy & paste it from a similar component) Add a new line to my jenkins config-package-creation job Commit & Push Test, fail, correct and fail a few more times until done! Many of you might be familiarized with some of this steps, some of us use Jenkins , others Bamboo ; for scripting some use Ansible , others Salt , bash or whatever, but we ALL copy & paste things from well working examples and re-adapt them accordingly . And I think, I speak for all of us, when I say that once you start copy-pasting too much, you start feeling an itch on some anti-pattern gland of our geeky body. The thing is, I might have ignored or overlooked this itch before my break, since all these steps never took me more than a day or two to get my new pipeline up and running, and it was a fair trade off since we didn’t get as many new components requests to consider this a problem. BUT, I left off, for my family holidays, pretty confident everything was going to be A-OK!. Well, kind of.. on my return I found myself with a couple of criticisms from my peers telling me that the whole process was too complicated, too many steps, etc… I have to admit I didn’t take it very well at first, I mean, what was soooo complicated? but then it hit me, someone else might not share the same background, have the same context and no matter how clear the steps were documented, It was a pain! More so in an increasingly larger and complex infrastructure. Now I found myself with the opportunity to improve something, and rather than throwing more documentation at it, doing it the Devops way: Automating!. So the first thing to do was getting rid of all that copy+pasting stuff, and the best way I know is refactoring, I won’t bore you with all the details of my own implementation, but basically once you manually perform a task more than 5, 6 times you can easily pinpoint the repetitive stuff, and then you consolidate it in a single place. Ansible is very good at this, so good that I could even remove most of my group_vars files. Both the refactoring part and the re-engineering were facilitated thanks to one simple but essential concept for automation: Convention ! and that’s why my new motto is always Convention over Configuration . It took me some time but I finally got to a stage in my company where everybody is convinced that getting too creative is very nice for advertising people, but for us it should almost be banned! OK! ok, not that much, but conventions save as a lot of time and give us a framework where we can feel safe that following some simple rules, things will work, and without so much effort. If we could guarantee that the same component name will apply for: Code Repository Config Repository Instance name and tags (AWS) DEB Package name Jenkins jobs We could save lot of time and lines of code. So I did, and never looked back ever since!! The new “automated automation” script, is basically a Jenkins job that commits a couple of lines to a couple of repos and hacks the very same Jenkins it’s running on, since it needs to add a new commit hook to another job, it's kinda nasty but it works! Another approach we took for creating the Building part of the Pipeline was using a Jenkins plugin called “Job Generator”, which is very useful when you need to replicate a model job, we use it as a job template pipeline and saves us a lot of effort. This is just one example, but it illustrates another big concept, Automation is the new Documentation , at least for us DevOps. It's always funner to do and easier to execute: a Jenkins job than a Documentation page. From backups, file encryptions, cache flushes, or even DNS configurations... you name it! In the age of the big microservices hype and cloud-based infrastructures this kind of automation becomes more and more important, it’s no longer acceptable to have a big overhead for new services deployment. When you work in a company where you can have dozens (our case) or even hundreds (like Netflix, Airbnb, etc..) of services, doing things manually simply does NOT scale anymore, even the creation of automated processes should be laid in the hands of automation. Did you like what you just read? Recommend this story (by pressing the ❤ button) so other people can read it! You can also react on Twitter: @diegarber or @ OlxTech We operate a network of online trading platforms in over 40… 25 1 Thanks to Santiago Hollmann and Cristian Fabián Elena . Cloud Computing Software Development Automation DevOps Software Engineering 25 claps 25 1 Written by System Engineer at OLX, passionate about technologies, sustainability and sports. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by System Engineer at OLX, passionate about technologies, sustainability and sports. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-13"},
{"website": "Olx", "title": "5 good practices i follow when i code using git", "author": ["Santiago Hollmann"], "link": "https://tech.olx.com/5-good-practices-i-follow-when-i-code-using-git-71120b57c0f5", "abstract": "About OLX Group Nowadays using Git is almost a rule and of course, tools like GitHub , GitLab and Bitbucket are almost a standard. To me, it really doesn’t matter the size of the project that I am coding (it could be for my current job, a freelance one or my personal apps), I always use Git . I think that habit is like a cane to walk the road to perfection. That’s why I not only use Git but also always follow some good practices that I have learned. Now it is time to share them. This practice allows me to make much more atomic changes, not getting distracted, identify branches easily and of course, translate all these advantages into pull requests afterwards. Do you remember the checkpoints on the games you play? Every time you screw it up you are able to start again from your last checkpoint. Commits to me are the same thing. Just started coding a feature? Make the initial commit. Improved a little bit the feature? Make a commit. Wanna try a different approach? Commit what you have done until that moment and then refactor whatever you want. By doing this, I avoid having comments, messy and commented code and things that I will need to clean later but more importantly I can always go back to stable states of my code. I always use descriptive titles for my commits. Also, if I have been working for a while and/or have touched many classes, I write a short title and then summarize the changes with bullet points. Another good practice related to this is to merge all related commits into one to avoid having a lot of noise among all the commit messages. Note: these are two stories that talk entirely about the importance of commit messages: The Secret to Great Commit Messages and Light of a Thousand Commits . It doesn’t matter if I work with other people or on my own, I find great to use pull requests every time I want to merge my code with my base branches. By doing it I can detect issues, wrong code style, commented lines, mistakes, things to improve, etc. I usually use at least two different base branches: develop and master. Master is the code base that reflects the latest product released to the users. This is very handful when you have to release hot-fixes on production because you just create the fix branch from master. Develop, on the other hand, is the base branch where I merge all my feature branches or things that I am coding that were not released to the users yet. After I integrate everything with develop, do the proper testing and release based on that code, I merge develop into master. Did you like what you read? Recommend this story (by clicking the ❤ button) so other people can see it! You can also react on Twitter @santihollmann We operate a network of online trading platforms in over 40… 352 5 Thanks to Lu Acevedo . Github Software Development Mobile App Development Android App Development Git 352 claps 352 5 Written by Android developer at Eventbrite. Passionate about mobile development, open source, flying gliders and Taekwondo. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Android developer at Eventbrite. Passionate about mobile development, open source, flying gliders and Taekwondo. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-09-26"},
{"website": "Olx", "title": "dr pushlove or how i learned to stop worrying and love the deploy", "author": ["Diego Garber"], "link": "https://tech.olx.com/dr-pushlove-or-how-i-learned-to-stop-worrying-and-love-the-deploy-b820128b74f3", "abstract": "About OLX Group Here at OLX Latam we have quite a classic story about how we release code, we came from quite a long way (of fuck-ups) until we finally achieved THE .com’s Nirvana of Continuous Delivery. Just a couple of years ago we were struggling with physical servers, requesting them with weeks of anticipation and carving them from the ground up like an old craftsman, we used SVN and deployed a PHP monolith manually to every environment, and now [SPOILER ALERT] we let our developers deploy their microservices at will with a push of a button to as many instances on our AWS cloud as the traffic demands . This might not sound to many of you as such a big deal, but during the article I’ll try to share with you some details that might be useful, or at least amusing, to some. Our key tool for the whole deployment machinery is Jenkins, nothing new under the sun, is like the glue that binds all the parts together, the symphony’s Director , orchestrating all the instruments, such as the Ansible scripts that run deploys in our servers, or our Github Repositories that trigger builds and start the pipelines. Jenkins is like that old reliable pal you can always trust but is not that fun to hangout with anymore, the UI looks a little bit retro, kind of watching an old TV show like Friends, it will always make you smile but the punch line never surprises you anymore, right? Well they have lately released the 2.0 version which comes with some nice new features , but the only thing that will truly make you feel you are watching something like Game of Thrones is Blue Ocean but it’s still in Alpha by the time I wrote this article. Jenkins is like that old reliable pal you can always trust but is not that fun to hangout with anymore We actually have two different Jenkins instances, one for Build and Testing, aka Devs playground, and another for Deployments and infrastructure concerns, SysOps playground . The first one uses Docker to build technologically different components the same way (Java, NodeJS, PHP, etc), running an image to build and test inside it, like: The Sysop Jenkins has actual access to all environments and runs the Ansible scripts over them. Something like: You can find some other Jenkins fun stuff in my previous article . Another important feature of our deployment pipelines is our deployment unit, aka what we install, some will deploy a tar file, others a git tag or even a Docker image, we use DEB packages . We find this very convenient since is the easiest, most natural (actually native ) thing to install on Linux Debian based servers. Without any extra third party tool, we get a very powerful set of command line tooling like dpkg, apt-*, etc.. which can do or tell you all about your installed packages. To create such packages we use a couple of very nice gems called fpm and deb-s3 , the first to create the package, the 2nd to upload it and all its meta-files to an AWS S3 bucket of yours. So yes!, we also use S3 as our package repository, providing an endless amount of historic versions to deploy or rollback to, whenever we need. The last instrument in our tooling orchestra would be our own Deployment Application: Octopush . We developed, and open sourced, it in order to give more visibility and empowerment to devs, it allows them to follow up releases, check versions and trigger LIVE deployments (and rollbacks) on demand. It helps out tracking all pipeline steps and historical rollouts. It’s a simple but powerful tool. What Octopush does is mostly offering a UI layer over Jenkins, in our case equally on both our instances, because it completely relies on it to execute (and show resulting) builds, tests and deployments. It also provides an API for other tools to control and consume deployment behaviour and data. We also developed an integration with Github and JIRA , the first to provide single-sign-on and provide links to the repo and versioning commits. The second so that it creates tickets automatically for LIVE deployments. One particularity about our Workflow is that we trigger the Build-Test-Deployment process on each git tag, instead of single commits. It has it cons and pros, but it’s what actually works for us. Anyway, on each tag we trigger a Build & Test process, it’s positive outcome must be a new and deployed package on our Testing environment. And right after that, we trigger acceptance tests on the that same environment. The deployment process integrates with some other external tools of our infrastructure like: Graphite and New Relic : to mark beginning and ending of each deployment. Sensu : disables/enables alerts upon deployment and also check whether an alert has triggered and requires a rollback. Kibana : provides customized links to error logs upon failure. This is just the tip of the iceberg, on following articles we will get into more detail on the build process, the packages, rollbacks and canary releases, auto-scaling, etc… depending on your feedback and interest I might favor one topic over the other. ;) Did you like what you just read? Recommend this story (by pressing the ❤ button) so other people can read it. You can also react on Twitter: @diegarber . We operate a network of online trading platforms in over 40… 13 Thanks to Santiago Hollmann . Docker DevOps Continuous Delivery Continuous Integration Cloud Computing 13 claps 13 Written by System Engineer at OLX, passionate about technologies, sustainability and sports. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by System Engineer at OLX, passionate about technologies, sustainability and sports. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-16"},
{"website": "Olx", "title": "jvm profiling in kubernetes with java flight recorder", "author": ["Abhinav Rohatgi"], "link": "https://tech.olx.com/jvm-profiling-in-kubernetes-with-java-flight-recorder-b39a6181a99c", "abstract": "About OLX Group At OLX Autos, we run a lot of JVM workloads in our Kubernetes cluster. A lot of our applications are developed using Java or Kotlin. We, at times, encounter performance issues with these workloads — be it issues around thread contention or frequent major GC cycles causing undesired autoscaling of applications and unnecessary resource usage. We use NewRelic as our primary Application Performance Monitoring (APM) tool, and though NewRelic helps us find whether an application performance issue is actually caused by JVM, it falls short if we want to drill down to find the actual cause within JVM. This is where we need the detailed JVM profiling. There are plenty of JVM profiling tools available and most of them are geared towards profiling local applications. Although some of them are suitable for production usage, getting them to work in a dynamic environment like Kubernetes is a real challenge. In this post, we’ll discuss our approach to profiling JVM applications running in Kubernetes. JVM Profiling is the process of performing application diagnostics to figure out any performance, memory, or I/O-related bottlenecks that could be plaguing the applications that run on JVMs. The JVM Profiler monitors the java byte-code constructs and operations at the JVM level. These constructs and operations include object creation, iterative executions (including recursive calls), method executions, thread executions, and garbage collections. It then generates relevant metrics against the same and sends them to a reporter, the reporter can be either a file, stream or a downstream application. Profilers usually consist of two components: The first component gathers the metrics/events from a running application and logs them to a reporter (generally a file, stream, or downstream application). In a JVM profiler, this component generally looks at the JVM-specific events such as thread samples (which show where the program spends its time), lock profiles, and garbage collection details. The second component uses the metrics/events from the reporter and generates relevant visualizations- these can be time trends, flame graphs, etc. which help the developers in analyzing the applications. We evaluated a few options that were commonly used across the industry to perform JVM application profiling, a few notable mentions are: Async Profiler JProfiler VisualVM YourKit Java Flight Recorder (JFR) Low overhead: The profiler should be suitable for continuous profiling in production. Low TCO: We did not want to spend big bucks on it. Detailed profiling and visualization: The profiler should be able to capture detailed metrics and allow for visualizations. Based on our evaluation, we chose Java Flight Recorder (JFR). Primary reasons for choosing JFR: It facilitates the profiling applications that run locally as well as in production. Profile applications retrospectively as well as live applications. Minimal performance overhead due to the use of internal buffers that limit Disk I/O operations. Reduces total cost of ownership as it’s freely available with JDK. More extensible and community-driven as this is a community-maintained open-source project. JVM profilers are typically used with applications running locally, even though they may not generate the same level of useful insights that you might get when profiling an application running under actual production load. This is because most of them introduce a performance overhead of their own, so using them with an application running in production is usually met with hesitation and apprehension. Even if a profiler is low-overhead and suitable for production use (like JFR), using it in a dynamic environment such as Kubernetes has its own set of challenges. Let’s take a look at different ways we can profile applications running in production and the associated challenges when that environment is Kubernetes: Most JVM profilers support profiling remote applications such as those running in production. They run locally and connect to the remote process via mechanisms like JMX to collect and visualize profiling data. This is not straightforward in Kubernetes as application pods are not directly exposed to the outside world. It needs to be done via a host of workarounds which can be tricky and may not be aligned with infra maintenance standards. In this approach, the profiling data is collected on the remote server by running a profiling agent along with the application. This data can then be fetched and visualized using the profiler’s UI application. In the case of JFR, it collects profiling events from the running application and writes them to a jfr file. The profiling data in jfr file can be visualized using Java Mission Control (JMC) . This is again quite challenging with Kubernetes. Application pods in Kubernetes are ephemeral in nature. Running pods are frequently terminated as part of the deployment or downscaling activities. If we enable profiling in a pod and store profiling data locally, as soon as the pod is terminated, the profiling data will be lost. So we need to figure out a way in which the profiling data can persist and remain available even after the pod has been terminated. Even if we can have a pod running long enough for profiling, the storage space on these pods can become a concern. Application pods are generally stateless in nature and hence do not have high storage requirements. This can lead to a shortage of storage space when it comes to keeping our profiling data on the application pods. Even if we have the required storage space available, SSHing into the pods to copy profiling data is cumbersome and not a great security posture. We need to make the profiling data available to developers easily without requiring Ops intervention or production SSH access. To overcome the challenges that exist in the Kubernetes environment w.r.t JVM Profiling, we came up with an approach whereby we baked the steps of profiling the application into the docker files of our applications. This way we enable profiling of these applications when these docker containers are deployed into our production environment as application pods. The applications then start reporting the profiling data into the small file chunks ( jfr files) locally onto the filesystem of the application pods. These chunks are synced to Amazon S3 and deleted from the local filesystem. This way we limit the storage requirements on the application pods. The jfr file chunks can be retrieved and stitched together for any arbitrary time period using an in-house tool called Springboard. The consolidated jfr file can be visualized in JDK Mission Control in the developer’s local machines for retrospectively analyzing the profiled data of these applications. Below you can find the high-level architecture of the approach discussed above and a more detailed explanation of the various stages involved, Enabling the profiling for an application is on-demand and part of the CI/CD pipeline. It can be done by setting the right pipeline variable when required. By default, the application is deployed without profiling enabled. This is achieved by modifying the Application Docker file and embedding certain steps into the same that are only executed when the feature flag for profiling is enabled in the pipeline. This way the application docker image that is built has the necessary steps required for its profiling baked into it. Once this application is deployed, the steps that are baked in perform 2 basic tasks: Task 1: Enable Runtime argument to enable application profiling at startup. Task 2: Setup the Amazon S3 sync script to sync JFR files with the FS A few things to note in these tasks: In the first task, the runtime argument for JFR has the 2 attributes maxchunksize and memorysize. These attributes enable us to create chunks of a defined size. It also has another attribute maxage that defines the maximum duration of disk data. These parameters help us to rotate and only keep a set number of file chunks each with a unique name on the disk so that we do not bloat up the disk space on the application container. In the second task, s3_sync.sh is a custom script we’ve created to copy over the JFR file chunks to a defined location in Amazon S3. The script runs periodically defined by a cron job set up on the container. These JFR files that are synced to Amazon S3 can then later be retrieved using an in-house tooling application called SpringBoard. This application requires the developer to specify a time range and the ID of the application pod that they want to analyze. The application then creates a job in the background to do the following tasks sequentially: Fetch the JFR Files from Amazon S3 by filtering them based on the pod ID and time range provided by the developer. Stitch the JFR Files using the jfr utility to create a single JFR File. Upload the stitched JFR File back to Amazon S3. Update the Download URL for the same as the output of the job. This way once the job is complete and the download URL is available to the developer, they can then download that JFR file over to their local system for further analysis. Once we have a recorded and stitched JFR file, we can import that into Java Mission Control (JMC). JMC will read the time series data and generate insights and graphs that can help you dig out the performance gremlins (if any). It also generates a nifty summary report that indicates if there are any performance bottlenecks that it has automatically detected. While we have a system in place, there is certainly a lot of scope for improvements in terms of how we have integrated it with our applications. First, we want to completely decouple the activity of syncing the JFR files from the application using a sidecar container. The sidecar can be dynamically attached to the application pod using the Kubernetes admission controller. This will avoid the applications having to change their setup to enable profiling. The separate container would also help us manage resources better. Secondly, we have planned to look at the recent developments that have been made to the Java Flight Recorder to now be able to perform JFR Event Streaming since Java 14. We are looking at how we can leverage this to move away from a file-based approach altogether. These are certainly exciting times for someone looking into this space with the fact that Java Flight Recorder is now open source, it makes it all the more exciting to contribute towards the development of a tool that certainly provides so much value. https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm#JFRUH170 https://jaxlondon.com/wp-content/uploads/2016/10/Java-Flight-Recorder-Production-Time-Profiling-On-Demand-Ola-Westin.pdf https://www.infoq.com/presentations/monitoring-jdk-jfr/ Thanks to Vikas Kumar for his valuable inputs. Thanks to Anmol Krishan Sachdeva and Sahil Ajmani for their help during the implementation. We operate a network of online trading platforms in over 40… 284 Jvm Profiling Application Performance Jfr Kubernetes Gitlab 284 claps 284 Written by Information Retrieval | Backend Engineering ( https://www.linkedin.com/in/abhinav-rohatgi ) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Information Retrieval | Backend Engineering ( https://www.linkedin.com/in/abhinav-rohatgi ) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2021-04-15"},
{"website": "Olx", "title": "sonarqube integration in android application part 1", "author": ["Jatin Juneja"], "link": "https://tech.olx.com/sonarqube-integration-in-android-application-part-1-37041b0379", "abstract": "About OLX Group Delivering high-quality code is the topmost priority of all the developers. To ensure this, developers try to make code that is clear, maintainable in the long run, scalable, refactored from time to time, etc. But the need for improvement is always there as some parameters might get ignored while developing or at the time of peer code review. To overcome this situation and deliver high-quality code we integrate quality check tools like SonarQube, Lint, FindBugs, PMD, etc. that can do an Automatic code review with every release. Automated code-review tool checks source code for compliance with a predefined set of rules or best practices. It will drastically reduce the time and effort while ensuring better overall quality and performance for large complex applications It offers the ability to identify common vulnerabilities before an application is released or implemented Tracks bugs and vulnerabilities Gate-keeper if a new vulnerability is introduced Keeps track of a large number of bugs What is SonarQube? Why an automatic code review tool required? Why use SonarQube? Getting Sonar Local Server up and running Integrating SonarQube in Android Application Publishing Android Application reports on Sonar Server Android development experience (SDK, library usage, gradle etc.) Part 1- SonarQube Integration in Android Application (you’re here) Part 2- Publishing Android ApplicationUnit Test Report on SonarQube As per the official documentation, “ SonarQube is an automatic code review tool to detect bugs, vulnerabilities and code smell in your code” . It empowers developers to write cleaner and safer code and detects the overall health of the platform. SonarQube offers code-quality management by suggesting what is wrong and helps you put it right It provides a clean dashboard to address bugs, coding rules, test coverage, API documentation, code duplication, complexity, and many more things It gives you the snapshot of today’s code quality as well as tells you what went wrong and what’s likely to go wrong in future Other code quality tools focus mainly on bugs and complexity but Sonar covers 7 sections of code quality:- Architecture and design, unit tests, duplicated code, potential bugs, complex code, coding standards, and comments So, what all things we require to view our SonarQube reports:- a. Sonar Server (for publishing reports) b. Android Application (for SonarQube integration) a. We need to first install Docker in our local machine first before installing SonarQube. For Docker installation, visit “ https://hub.docker.com/editions/community/docker-ce-desktop-mac ”. Login/Create Account to download Docker b. Once you have installed Docker, its time for SonarQube installation c. Open terminal in Mac(Command + Space) and type “ docker pull sonarqube:7.5-community ” and press ENTER d. “ docker ps -a ”, press ENTER ( this will give the list of containers running within Docker, there should be none if you have done SonarQube Docker installation for the first time) e. “ docker run -d — name sonarqube -p 9000:9000 sonarqube:7.5-community ”, press ENTER f. “ docker ps -a ”, press ENTER (now it should give you one row with SonarQube running) g. Now, the SonarQube should be up and running. To test, visit “ http://localhost:9000/ ” h. Login with credentials Username — “admin” and Password — “admin” i. Generate a token for your Android Application by providing a name for your token j. Save the token. You will need it later in configuring Android Application for running SonarQube a. In Project’s build.gradle file, add Sonar plugin at top:- b. Add classpath dependencies within buildscript (buildsccript -> dependencies) c. After Step 1 and 2, hit “Sync Now” d. SonarQube is added in Android Application, its time to do the basic configuration for SonarQube. Replace PROJECT-NAME and PROJECT-KEY with the name of your Android Application P.S.- if any file within the module needs to be excluded, you should mention it in “sonar.exclusions”. e. Hit “Sync Now” That's it. You have integrated SonarQube in the Android App. a. To generate a report, we need to run a Gradle command:- ./gradlew sonarqube -Dsonar.host.url=http://localhost:9000/ -Dsonar.login= $REPLACE_WITH_GENERATED_TOKEN b. You can see the execution of the command in Terminal:- c. Visit “ http://localhost:9000/projects ” after the build is successful https://en.wikipedia.org/wiki/Automated_code_review https://docs.sonarqube.org/latest/ https://docs.sonarqube.org/latest/analysis/scan/sonarscanner-for-gradle/ https://vizteck.com/blog/benefits-using-sonarqube/ On the next pit stop, we’ll learn about Publishing Android ApplicationUnit Test Report on SonarQube. medium.com We operate a network of online trading platforms in over 40… 331 3 Thanks to JB Lorenzo and Rem Cruz . Android Code Quality Sonarqube Code Coverage Code Review 331 claps 331 3 Written by Learn -> Implement -> Share -> Repeat We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Learn -> Implement -> Share -> Repeat We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-10-22"},
{"website": "Olx", "title": "diversity needs you", "author": ["Ayelen Chavez"], "link": "https://tech.olx.com/diversity-needs-you-a0c34286d7b3", "abstract": "About OLX Group This is the #1 article from a future series related to diversity & inclusion . How it came to be? Corina and I came together and wanted to talk about this topic a lot more, in the context of OLX Group but not limited to it. A way to start the conversation with everyone in the group was to post an article on our blog, this blog. However, we realized that instead this could be a series of posts to which more people can contribute to. DISCLAIMER: This series is based on our own experiences and personal opinion. We do not want to restrict or dictate. That being said, let’s start with what diversity & inclusion means. Diversity is used to refer to the various differences between people, ranging from race, ethnicity, gender, sexual orientation, age, social class, physical ability or attributes to religious or ethical values system, national origin, and political beliefs. Inclusion is the art of welcoming and embracing all these differences. At university, I studied computer science, being one of the few women in every class. I started working quite early in my career, and this pattern repeated at work. I was usually the only female engineer in the team or even in the whole organization. However, I was lucky enough to (almost) never feel exclusion, or being out of place, because of my gender. The very lack of tech female role models during my childhood or even when I started my career didn’t stop me. I’m not saying it was easy either: when the typical stereotype of a software engineer is a big bearded man, you don’t feel too inspired to pursue a career in tech. I was clever enough to ignore this and grow a lot without being aware most of my classmates and colleagues were male. I felt like an equal. That feeling of equality changed when I became an engineering manager. There weren't many female role models around either. Challenges and expectations were different. I started feeling the pressure of being a female leader managing all kind of different backgrounds and nationalities. I was fortunate to have great managers and mentors (male ones) who would push my limits, help me reach my full potential and create new opportunities, (probably) escaping their own unconscious biases. I also had the pleasure to work with multi-cultural, multi-background, multi-everything team members that would hear me out and respect me as a person regardless of our differences. They would even call my bullshit out, with respect, whenever needed. And oh boy (or girl, or better dude), that made me grow a lot! I’m aware that lately the term diversity seems to be a buzzword that everyone repeats in conferences, job interviews, surveys and all kinds of meetings and presentations. And gender diversity seems to be the one type getting most of the attention. Maybe because we clearly lack it in the tech industry and among our C-suite executives, or maybe just because we need to start somewhere… As a female engineer, I feel the responsibility to be that role model I didn’t have back then. I feel the responsibility to highlight the fact that history, culture or simply luck were not fair with female technologists so we do need to make an extra effort to bring more women to our interview pool, be active in \"women in tech\" conferences, or meetups, or coach women that followed a different career in the past and want to start a new one as an engineer now. I feel responsible for little girls that are missing a role model and I feel the need to spread the word around parents, so they can inspire their children to pursue any career they want regardless of their gender or gender identity. And I feel responsible to clarify that diversity & inclusion goes beyond gender. Instead of limiting the discussion to gender, we need to start talking about other realities . Let’s stop talking about male and female and start talking about people! Diversity means different things to us according to the lifestyle we subscribe to or long for. To achieve this kind of lifestyle, we need it to be reflected and embraced at work. People who want to devote more time to their personal development. Diversity in this context could be a person who wants to work part time to study something new. It's about going for a run in the middle of your working hours, it's staying at home waiting for your furniture delivery to arrive, it's going to a doctor's appointment when it seems more convenient for you, it's having the possibility to leave the office early to start a new hobby. People who want some location flexibility. Diversity is also a person who works remotely 3 months a year to see their family who lives 20 hours away from work. The possibility to stay at home when the public transport system is not having a good day. It's being able to stay at home since your doctor's office is near home and far from your work place. It's about having your manager's support to work from your holiday's location so you can stay for a longer while at the place you are visiting. People who want to adjust their workflow because of changes in their life. Diversity is a new dad who wants to stay home to be with his newborn. It’s a woman who wants to start a family, and wants to share that with her colleagues without feeling judged. It's also a person who wants to adopt a dog and have enough time to be part of its life. Diversity is about that plan you gave up a hundred times because it does not align with your current work schedule. And it is much more than that. It's as diversified as we are. This kind of change takes generations to materialize. It needs help from all of us at home, at the bar while drinking a beer with friends, in meetings where a sexist joke should be called out and in family dinners or events where the possibility of talking about a future career should be independent of gender. Yes, we do have room for improvement, and this kind of change takes time. But we need to embrace already that it’s up to all of us to share the message, for the sake of generations yet to come. I can promise you it’s going to be a long way until we live in a world where everyone is treated equally. The good news is that we are already walking in that direction. We are making history, so let’s enjoy it! At OLX Group we’re addressing Gender Diversity and it is one of the company priorities. Here is a message from the CEO: Gender diversity @ OLX Group . We operate a network of online trading platforms in over 40… 178 Thanks to Kathleen Sharp and Maryna Cherniavska . Diversity Inclusion Women In Tech Engineering Engineering Mangement 178 claps 178 Written by Engineering manager, nerd, bigger sister, crossfitter, dance-lover, bad writer, good writers follower ;) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Engineering manager, nerd, bigger sister, crossfitter, dance-lover, bad writer, good writers follower ;) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-01"},
{"website": "Olx", "title": "webpack from zero to hero", "author": ["Rubens Pinheiro Gonçalves Cavalcante"], "link": "https://tech.olx.com/webpack-from-zero-to-hero-f64924e4d06", "abstract": "About OLX Group This article is part of the Webpack from Zero to Hero series , for more background or for the index you can check the Chapter 0: History . Previous - Chapter 3: Everything is a Module . Next - Chapter 5: Route Based Code Splitting with React . S ometimes you want to make your app more flexible, but responsive design is not enough and your UI needs not only a different look and feel but another behavior on different platforms. You may want to swap business logic at runtime , depending on your user actions. Or your app became too big and you may just want to load things only when necessary. For all this, we have a solution, and the most important thing is an ECMA proposal ! Webpack supports it (of course with some differences, same as with the official static import) and adds a nice behavior to it: code splitting. The syntax is pretty straightforward, we have a function that returns a Promise. Let’s say we have a dumb module called module-1.js : To import it dynamically we do something like this: But there’s a problem if you try to run it: ⚠️ Update: If you’re using Babel v7.5 and you’re using @babel/preset-env you don’t need to do this next step, dynamic import syntax is already added to preset-env by default now. The problem is because dynamic imports is only an ECMA proposal , there’s a good chance that this will be a reserved syntax of JavaScript, so BabelJS will “complain” when parsing it. In fact, if you remove the babel-loader rule, you can see that Webpack works fine with it: But hey, if you look up the error, Babel gives you this tip : “Add @babel/plugin-syntax-dynamic-import (https://git.io/vb4Sv) to the ‘plugins’ section of your Babel config to enable parsing.” So let’s install it: …and enable it in .babelrc : Enabling babel-loader rule again and running yarn start:dev , now all works fine! If you check the build logs, you’re going to see this: That 1.js means Webpack created a “chunk” and included our dynamic imported module inside of it. If you run the dev server and put a breakpoint right on the `import()` you will not see the 1.js on the network tab, but as soon you release the breakpoint, the request to fetch it is done. When the project grows, you probably will find it necessary to check how your bundle ended up, and how to optimize it: “ what if I do this tweak ?”. But not everyone is a Hackerman and is able to read this output extracting all the information from it. However, Webpack has plenty of options for visualizing the final output and making it more readable. The one that I like most is Webpack Bundle Analyser . Let’s check how it works. First, install it: Add this to the npm scripts on package.json : There are two things happening here: Command composition: the analyse script will result in: We’re giving an environment variable to Webpack, and we’ll be able to access it within the configuration. Now, let’s setup our webpack.config.js and use the given --env.analyse parameter on it: Now just run the npm script and the bundle analyzer will automatically open a new tab in your default browser: You’re going to see this: So now we can see that Webpack splits your code by default when you do a dynamic import, but let’s go further and play around it. Let’s create a new module called module-2.js which exports only this function: You can see that there’s a pattern for our module names, where every module is marked by its number. This way it is possible to use an expression to import them: This will output the default functions of both modules 1 and 2 as a result: And if you analyze the bundle, you will see that Webpack automatically creates a context based on your expression : You see the power of functional programming — we’re basically transforming a set of numbers to their respective module default export function outputs! As long our modules keep the same signature we can play around like this. This opens a lot of new possibilities, for example as we said before — loading UI components depending on the platform (or browser), taking different actions after the user chooses or inputs some value into your app, etc. Most of the time you will want to defer the module loading so that you could use the module only when it is necessary. There are some techniques to do so, let’s check them. 👀 Let’s create a file called lazy-one.js that exports a string: Now at the end of index.js, paste this snippet: We have just appended a button with a dynamic import on the “click” event. Loading the page and looking at the network tab shows us that it loads the page, and we can see it requesting the two chunks from before: And then, after clicking the button: As Webpack adds all this behavior to dynamic imports, it would be nice if we had some control over it, right? But we do! 🎉 Webpack provides a nice way to configure these imports through comments following a certain pattern. 😕 — “Hey, but the name 2.js isn’t really helping to know what we have inside it, can we rename it?” Yep! Let’s name it after myAwesomeLazyModule : This will result in: Let’s say our lazy-module is super important and we don’t want a delay on the button content change after the user clicks it. We can choose to preload this resource as we’re pretty sure it will be used on the current page: This will make webpack add a `<link rel=”preload”>` at the start of the page, making this resource to be loaded (but not executed) before the `main.js`. This is pretty useful for small resources where users have a `high latency network`, causing a big delay on the execution of small chunks not because of it’s size, but because of the HTTP protocol process (being even worse on HTTPS because of the “handshake”). “This will give a little load time boost since it only needs one round-trip instead of two.” — Webpack Docs 😖 — “What!? What’s the difference between preload and prefetch ? It looks similar, but prefetch will not load before the bundle/chunk that requests this resource, it will load after on the browser idle time . This makes it perfect for resources that will very probably be used in future . For instance, you have the data (through analytics) that the majority of unsubscribed users who visit your “products” page will with a high probability access the “subscribe” page. Since you’re a smart developer, you set a prefetch import on “products”, making the browser fetch and cache this resource in its spare time: As you can see, it works similar to the preload, activating it just by adding the “magic comment” above. And here is a tip (works for preload too): if you have multiple prefetch/preload resources, you can define their priority: You may be asking yourself: I have a 100 of those modules-${ id } and as I’m requesting them on the page directly, I don’t want to request 100 separate files, since most of my users will suffer because of high latency . For that we also can control how Webpack resolves and splits the chunks for dynamic imports, using the webpackMode magic comment. From the docs : “lazy” (default) : Generates a lazy-loadable chunk for each import()ed module. “lazy-once” : Generates a single lazy-loadable chunk that can satisfy all calls to import(). The chunk will be fetched on the first call to import(), and subsequent calls to import() will use the same network response. Note that this only makes sense in the case of a partially dynamic statement, e.g. import(`./locales/${language}.json`), where there are multiple module paths that could potentially be requested. “eager” : Generates no extra chunk. All modules are included in the current chunk and no additional network requests are made. A Promise is still returned but is already resolved. In contrast to a static import, the module isn’t executed until the call to import() is made. “weak” : Tries to load the module if the module function has already been loaded in some other way (i. e. another chunk imported it or a script containing the module was loaded). A Promise is still returned but, only successfully resolves if the chunks are already on the client. If the module is not available, the Promise is rejected. A network request will never be performed. This is useful for universal rendering when required chunks are always manually served in initial requests (embedded within the page), but not in cases where app navigation will trigger an import not initially served. Let’s try out lazy-once on our dynamic import with an expression: And we see the result: You see, all chunks which were separate are now in the same common chunk, making it easy for the application, as it loads all modules with one single request. If instead, we use the “ eager ”, all these modules will be included inside the one which is importing them, in our case main.js . W rapping up, in this chapter we went deep into Webpack and used the dynamic-imports, tweaking the way the code is being split in the process (while visualizing everything with the bundle analyzer). We’re almost done and the only thing left to us is to take advantage of what we learned in this chapter while using React and the new lazy/Suspense API. See you in the next chapter ! We operate a network of online trading platforms in over 40… 392 2 JavaScript Webpack Nodejs Frontend Technology 392 claps 392 2 Written by Front End Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Front End Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-12-06"},
{"website": "Olx", "title": "how to train a dragon or can a software developer become an sre part 2", "author": ["Maryna Cherniavska"], "link": "https://tech.olx.com/how-to-train-a-dragon-or-can-a-software-developer-become-an-sre-part-2-44a294f1de8", "abstract": "About OLX Group Read the first part here to find out how it all began. Meanwhile, let’s continue where I left off. A. has strong opinions on stuff we’re doing wrong at OLX (on the SRE side). Like, reinventing our own wheels for stuff and adding manual hacks that those wheels require. Also, stuff that complicates life along with simplifying it. Like having our own DSL which of course isn’t parsable by IDEs, therefore making code unreadable — it is really difficult to see what comes from where and the IDE goes crazy and underlines everything in red. However, these custom tools also exist for a reason. Standard tools like Helm makes one type complicated command lines to do things, there’s a lot of repetition, and the commands syntax usually isn’t like any programming language. Therefore, a lot depends on SREs, who are humans and therefore make mistakes — like typos — which are really hard to catch while debugging. The custom tools attempt to express infra in terms of code, with objects and types that allow to use the compiler to catch errors faster and easier. My reading day. We decided I jumped too quickly into the practical things and need to back it up with some theory. Reading about Terraform, I see that it’s intended to not only configure resources, but provision them. It can create all the types of resources that it has providers for. It has providers for AWS, therefore it is possible to create all the AWS entities like instances, roles, policies, RDS, clusters etc. For Kubernetes, one can provision pods, create deployments, secrets, routes, config maps and stuff like that. However, we aren’t using the Kubernetes provider; we’re using OpenShift, or rather, an open source tool built internally called Rubiks that creates those same entities — pods, create deployments, secrets, routes, config maps and stuff like that — or to be more exact, it creates OpenShift specifications for all that. This is one of the internal tools created by an OLX SRE Matthew Byng-Maddick . The reasoning behind it is explained this article . Still doing mostly Terraform and very little OpenShift, so I still am not very advanced in creating instances or setting up new apps. Another SRE said that he was very happy I was trying the SRE training because he was hoping to see more devs come into the SRE field. Reason is, because devs would want to write code and therefore push for creating more internal tools and solutions . Taking into account what I previously wrote about the wheels being reinvented… it is probably not a bad thing, but it is something that’s hard for me to come to terms with. I like tools. Hell, I use tools all the time, for everything. However, having too many internal tools, or even wrappers around standard tools, is something that comes with downsides. One of my colleagues was a Google contractor for a while; he told me that he had a great experience while working with Google, but there was something he was worried about: Google was using a huge amount of internally sourced tools and frameworks. They were great… if you intended to work with Google forever. But if you changed the job afterwards, this knowledge wouldn’t help much. This is of course not a company’s concern, as no company is really interested in how successful an engineer should be, once he or she leaves. It might be a concern for some of the (perhaps less experienced) engineers and SREs who want to gain useful experience on the job. On the other hand, one can argue that no acquired knowledge is really useless. Things might be done a bit differently somewhere else, but the underlying principle will stay the same. Yet another downside is support. It’s fine when you’re in a company with thousands of engineers who are using and supporting/revising the tool. But it makes it much harder in smaller companies — there’s a danger of the tool going stale, or the engineer(s) driving it leaving the company and nobody taking ownership. This could possibly be less of a problem if the tooling system wasn’t supported by only the creators, but by all the engineers using it. However this is where it becomes really complicated. No team will want to include “tooling support” in their OKRs — everyone has their own priorities. People will work on a thing in two cases: if they have allotted time for this within their workday; or if they are real fans of the tooling and will gladly say “free time is overrated” and do this on weekends and evenings. But even if there was a lot of these enthusiasts… life happens. Families and kids come into play. Learning becomes secondary. Perhaps this could also be remedied by the tools being open sourced, but then they would have to be really good (and really generic!) to get traction and community support. I suspect it’s difficult to make a “one-tool-fits-all” solution. The more generic the tool is, the less valuable it might be for the specific company tasks. With great power, comes great responsibility. But not just that. Sometimes, with great power comes something else: the feeling of superiority. I sense this around me in SRE meetings sometimes. Like some people really seem to think they can do no wrong. Others , however, are considering an SRE job to be something like a… nurse, as opposed to a doctor? (Yes I do like to watch medical TV shows, so what? We all have our little weaknesses.) Someone who’s doing technically complicated things, but still does kind of a supporting job and not the “rockstar” job. My “Yoda” SRE tells me that “I want to be a developer when I grow up”, meaning that for him, it’ll be a next step in the career. But some SREs really seem to draw the line between “us” and “them”, and “us” are inherently… better? Smarter? More “low-level”? I am also wondering whether an overtime pay comes into this. I understand that due to it, a junior SRE can in theory receive a higher compensation than even a senior developer. Can extra income at least partially responsible for the elitism? In general, however, this is more subtle than it sounds. I think this is more of an undercurrent, which I only feel due to currently not really belonging to any of the worlds. Which brings me to another topic, developers on call as an ongoing dispute. Seems like this is a dispute that’s been going on for a long time: should developers go on call? Should they not? At OLX, there’s no universal rule, not by default. The teams are free to experiment and try out different ways of working, which I think is a good thing. For example, in my previous team, we decided on devs being on call on a daily basis, but with a rotating schedule: each day it’s someone else. However, the dev on call didn’t have all the infra access (so any incident could still be escalated to an SRE) and wasn’t doing the on-call at the off-hours (therefore not eligible for overtime pay). My husband is also a developer in another company but in his team, all the developers go on call for a week at a time, 24/7. It means two things. First, they have all the power . Every dev has permissions to manage not just the code, but all the related infrastructure. Second, they get paged by PagerDuty, sometimes during the night, and receive appropriate extra compensation. So, they have all this power and responsibility. The other side of this is, SREs are a bit… let’s say, redundant, at least for their team. SREs are still doing their job, they just aren’t constantly being distracted by the routine failures, which means sometimes they do have periods of time when they seemingly do no work. But any incidents could also still be escalated to them, and they have a better global view of things, though less of a local one. I am a little bit on the fence about this. Practically speaking, I would like the developers to have all the possible permissions and the possibility to fix everything. That would give them more responsibility for the code, and more visibility into the architecture. But on the other side, not everyone can afford to go on call 24/7. That’s something I’d like people to be able to choose for themselves, however SREs can’t really pick and choose, so would it be fair? There’s one more side to being an SRE/DevOps, and that is being on the front line, but not on the front page. When the project will get its next release out successfully, the product manager, the engineering manager, the developers will likely get most of the credit. An SRE will probably not get anything more than “an honourable mention”. Perhaps this feeling of “us” and “them” is partially born from this too? I am not the one to say, and anyway, this feeling I have is very subjective. This article has come to be pretty opinionated. I think it was a really unique situation though. Perhaps for someone the questions here will have different answers. There will be one more article in this series, one with the conclusion and the takeaways. Stay with me. Well if you are bored, don’t, but otherwise do. This article has been also reposted to my personal blog. We operate a network of online trading platforms in over 40… 154 1 Thanks to Herman Maritz and Herman Maritz . Sre Development DevOps Thoughts Thoughts And Feelings 154 claps 154 1 Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-22"},
{"website": "Olx", "title": "how to train a dragon or can a software developer become an sre part 1", "author": ["Maryna Cherniavska"], "link": "https://tech.olx.com/how-to-train-a-dragon-or-can-a-software-developer-become-an-sre-part-1-9def010ea9f5", "abstract": "About OLX Group T his is a story about a rather unusual experiment, which our company ran with me as a (willing) guinea pig, to try and retrain a software developer as an SRE. SREs (or DevOps, and there’s a controversy on whether it’s the same job or not) are a hot item right now, I think maybe even more so than data scientists (well I don’t have stats in hand to confirm it, that’s rather a one-sided view). Anyway, our company was desperately searching for SREs, and then the bright idea came to one head. We have all those devs, and they are all technical people too, right? And they work with infrastructure too, only a bit on the other side, but at least they have some idea about it, right? And maybe retraining a senior developer would actually be easier and less costly than training a junior SRE? However the idea came to be born, born it was, and moreover, it was implemented. (Spoiler alert: due to unforeseen circumstances, the experiment ran shorter than expected.) But the initial idea was that the best training is hands-on, and that to make sense, it should happen along a 2–3 months period, and that the chosen software developer was to be given as an apprentice (call it a trainee if you wish, but I actually like the word apprentice here) to an actual SRE, working on one of the teams. That chosen software developer was me , a backend programmer with about 15 years of experience, currently working with Java and Kotlin. And this was how the story began. Some disclaimers first. To protect people’s sensitivity, I won’t give them their real names. Let them be called A., B., C. etc. in order of appearance. I understand it might make the story somewhat artificial, but these are my real thoughts, and they are about real people. So… bear with me. Anyway, I am the author. I have the power. Another thing I must note is, the company made no assumptions that I will switch to be an SRE after the training. I was free to go back to development if I felt it suited me better. But in case I would say I found my vocation, the company was ready to embrace me in a new position. S o, first day as an apprentice. I come to work and my “master” SRE, A., is already there. Turns out he comes to work before 8am, which I can’t possibly match since my commute alone takes almost an hour, and I also have breakfast at home. Oh well. I have stopped trying to impress people with my working hours long ago because I think it is not a good metric of your performance, but still in cases like this, I feel inadequate. Whatever. A. starts to help me onboard by sending links to the Wiki pages I need to read, and at the same time we go through the tools I need, tools I already have installed, repos I need to check out etc. What’s good is that I have had some exposure to the tools: I have used Terraform (very little, true, but I know how the TF file looks at least), I already have my GPG key set up and A. only needs to add it to some projects and repos for me to be able to use them; I have an idea of what git and git-crypt is used for and though I am not fluent in console, I can do basic stuff. Of course A. is working in the console with the speed of light, and of course he has his console split into 4 parts, each performing its own task. And of course the font is so tiny it is unreadable for me, but he quickly corrects it without me needing to say anything. I suppose he noticed that my console and app fonts are scaled almost to the maximum. One more reason for me to feel inadequate: I can’t consume so much information at once. It is easier for me to use tabs in the console instead of splitting the windows, and it is also much easier for me to read stuff that doesn’t make me strain my eyes. But the fact that A. works like this isn’t SRE-specific: a lot of developers I know do the same. When I recall that most of them wear either glasses or contact lenses and I still don’t use any, I feel a bit better, but still not a lot. A. also seems to be using Visual Studio Code as a main IDE to edit stuff. He’s happy that I use it also, — what I don’t mention is that I actually am a newbie in that as well. My IDE of choice is IntelliJ IDEA . It is perfect for Java and Kotlin, which are my main specialty. But IDEA is actually pretty slow when loading large projects, and it is also not very helpful with the terraform syntax, so lately I tried VSCode as a replacement for the configuration-only projects. But whereas I know my way around IDEA pretty well, as in — I remember a lot of standard shortcuts and have also set up some of my own, — I am basically still trying to blunder my way around VSCode . Do you ever mistype stuff when someone is sitting behind your shoulder? A. is great and very patient, but I feel like I misspell the simplest commands (I challenge you to mistype git — but that is something I actually managed!). I attend the standup and sit through weekly planning where I predictably don’t understand much. Not that the concepts are unfamiliar, but I just realise I have very little idea what this team is actually doing — what are their current goals, challenges etc. The team also seems to be following the sprint workflow which we don’t. I realise that I will probably have a much worse meeting-to-coding ratio that I’ve had previously. By the end of the day, A. basically has hardly left my side. We have resolved a production JIRA to increase some read/write limits on the AWS Dynamo DB , which required a couple of line changes in the code containing the Terraform configuration and a few console commands to stage and apply the change of config. It wasn’t exactly difficult, but I hope I will just be able to remember it all tomorrow. My head aches because all the new information I’ve consumed is threatening to squeeze out of my ears. Mercifully, it is time for my master SRE to go home. I stay to type up my notes about the day and add some links that he shared with me to Kotlink (which is a tool Illia Sorokoumov invented and I swear by because I can’t possibly remember all the browser links I need. And no, Chrome Bookmarks aren’t the same). And then I go out into the freezing Berlin night. This is not going to be easy. But this is going to be manageable. Especially if my master SRE will continue to keep his cool. I wonder what he’s thinking though. He probably might not share this opinion. S econd day, more of the same. I find that I remember most of the stuff we did with Terraform but almost nothing about OpenShift which we also did a little. Makes sense, because I used Terraform at least a bit, but the OpenShift is completely new. I really need to do a dump of console commands and keep them as a reference. I start to think that this will help a lot with my goal to get to be an architect at some point. Already, I see many more opportunities to think about the architecture and not the actual application logic. Why? As a developer, I think I just always knew in theory that I needed to think about the architecture, but somehow I never actually tried to find holes in it. I could think of a basic design that satisfies the requirements but not about its cost or performance with live traffic — bottlenecks, interactions with other systems. And also, I was too little exposed to the actual behind-the-scenes setup and it was just easier to leave it to someone else because I either didn’t have access, or didn’t know how. SRE B. after a meeting: “If you have any trouble understanding any stuff in SRE meetings, I can always stay with you and explain whatever you need.”. Me: “How much time do you have?…”. A. asked me whether I was excited to think of being on call at some point. No, not really… I think this ad-hoc thing is what puts me off most in the SRE job. This is something we don’t think much about, but a lot of SRE work is putting out fires. The best SRE is the one that doesn’t allow many fires to happen, of course. But it’s not realistic to think that none will happen. Which means that sometimes you will still be fixing something in a hurry, because a disgruntled engineering manager is breathing down your neck because something just broke in production. And even if it’s not production and/or there’s no fire breathing manager, your flow will still be broken. These notes are to be continued in the next article . Hope you found them interesting! If yes, let me know in the comments and I will continue. If not, then also let me know in the comments… and maybe I won’t! (The article also posted to my personal blog here .) We operate a network of online trading platforms in over 40… 125 Programming Software Development Software Engineering Experience Sre 125 claps 125 Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-22"},
{"website": "Olx", "title": "moving to microservices", "author": ["Facundo Viale"], "link": "https://tech.olx.com/moving-to-microservices-66d00a30bad0", "abstract": "About OLX Group When we started the OLX Panamera Project we had a big PHP monolith in charge of everything: search, feeds, posting, listings, users, configuration, internationalization, monetization, etc … It was very hard to move fast in that context, with around 20 developers making changes every day. Deploys to production were a nightmare, having to track bugs in between hundreds or thousands of lines changed. The code was so big that almost no one knew what was going on in a simple flow. Teams had different priorities and schedules for developing features, making it really hard to coordinate everything in a single release. There is a lot of mixed opinions around microservices; from beneficial to maddening. I always saw microservices vs monoliths as a trade-off where you can mitigate some of the negative things in any of the cases. Here is where the context, seniority and culture of the company come into play to decide which one is right for you. We decided to go with microservices, fully aware of the potential problems of this approach: Coordinating services interaction and distributed state. Handling APIs and versions. Increased complexity for SREs. Networking complexity. Technology consensus. Quality standards. Resources handling (load-balancers, cache or DBs). Having taken this into consideration, the first step to move to microservices was to build a plan on how to make developers’ life easier. Because in the end, microservices are going to let us move faster, isolate responsibilities and scale better. First of all, as I already mentioned, it’s very important to understand this kind of changes are not only just technical but also cultural. When you come from a monolith, moving to microservices is a huge paradigm change. So you need to build a new culture revolving around how a team should handle and code their services. This kind of transition is always incremental because it’s hard to build and prepare everything beforehand. That’s why it’s important to analyze and prioritize the ground base for starting. Without going in too much detail, in our case, having AWS, OpenShift and GitLab was our ground base. Not everything is tech, which is why it’s also very important to have a group of engineers with experience in microservices. It doesn’t matter if the group is small because their main role is to spread the culture, team pollination and encourage cross-team discussions. In a monolith context, it’s very common to split responsibilities; developers code, SREs deploy and QA/Automation test everything, each of these types in their own team. When you start scaling the number microservices, it’s difficult for SREs and QA/Automation to keep up with all the changes and releases. In our case, with the monolith, we had release cycles and 1 or 2 deploys to production per week. With microservices, you have dozens of services and you probably end up having multiple releases to production every day. For dealing with this situation, we shift the culture to give more ownership to the developers. A service is not just code, it's everything; deploy, DBs, caches, monitoring, alarms, automation, tests, etc. Some of these things can be easily automated, like CI, deploys and handling DB/cache resources, others are a change in responsibility. Does this mean we don’t need SREs or QAs? No, this means we only need them in special cases or for support. They should focus more on abstract complexities for the developers and help them adopt technologies or methodologies. When all the developers are working in the same code base, it’s much easier to control what's going on. Mainly because all the devs are familiar with the code, every PR usually is seen by 2 or 3 devs and big refactors require coordination and have everyone on board. But in microservices, both teams and the codebases tend to be small. Sometimes they are isolated from the work of others. Without some kind of coordination, services can end-up diverging from each other in unexpected ways , like API patterns, log format, project structure, quality standards, code format, etc. The first and most important step is to have everyone on board with this. Sometimes people think that by standardization, you mean that things can’t be changed and that you have set up strict rules. The main driver is that any developer should feel familiar with any service. As the company evolves, both technology and standards do as well. Going back to the main topics, let's talk a little bit about each other: APIs: It’s very common in a lot of companies to have an API standard. It’s very useful for defining things like path patterns, paginations, internal headers, payload format, authentication, etc. Code Format: Another common topic in companies, there are several examples of this, like Google Checkstyle or Airbnb ESLint . In our case, we went a little bit further by using hadolint for our Dockefiles. Project Structure: This is not only how to name files or folders hierarchy, but also basic tools for every project, like Using Java 11 and project should use SpotBugs, CheckStyle, JUnit 5, Spring Webflux, etc. Quality: We don’t like the idea of imposing % of coverage, but the idea is always to have as much as you can. We prefer to have a centralized dashboard to see the state of the platform. In our case we use SonarQube . But coverage is only the tip of the iceberg in terms of quality. There are several other tools we encourage to be used by teams - mutation testing, Static Application Security Testing (SAST), dependency scanning etc. Performance Testing: In every platform, it’s very important that every part can scale at the expected levels, which means every new service must pass through a performance test battery before reaching production. Because this topic is quite long, you can see my other post about this. For us, tooling means anything that makes the life of the developers a little bit easier. When developers start building things outside the monolith, they find out that now they are in charge of things that were managed by one or more other guys. One example of this is deploying. In a monolith, usually the SREs take care of the job, but when you have dozens of microservices, it’s hard for them to keep up with releases or requirements. Of course, a solution could be hiring more SREs, but some of these tasks can be easily automated. In our case, we rely on GitLab and we built several reusable and customizable modules for running the CI/CD life-cycle of the services, like: Compiling and running integration tests depending on the language. Master branches deploy to the staging environment. Production releases with manual intervention. Per region release. Generating and sending release notes based on the commits when a service goes to production. Publish Swagger files as static pages using spectacle . Create AWS resources base on the terraform files in the project. Generate and publish docker images to our internal registry. Other examples of useful tools for teams are: Seed Project: This is creating our own“ create-react-app ” or maven archetypes , to bootstrap a service. This is an easy way to reduce the “time to production” in new services. The idea is to have a one-line command to generate a service flavor; Java with Spring service, Node with Express service, internal back-office with React, etc. With all the required libraries and configurations to make a push to master and have the application up and running. Terraform Modules: Creating AWS resources can sometimes be a little bit complex and verbose. In many cases, you need to define security groups, cloud watch alarms, iam roles, iam policies, route53 records, etc. To simplify all of this, we created several modules with our common recipe for creating an Aurora cluster, DynamoDB tables, etc. We reduce tens of lines of terraform to less than 20, with very straight forward fields. We also develop other types of modules, like Grafana dashboards. These are very useful for standardizing how we monitor AWS resources. Performance Testing: To simplify this task, we build a seed project for deploying an ephemeral job to run a clustered workload using Gatling . We also have several examples of how to use tcp proxies to simulate specific network conditions in our environments. One of the biggest concerns with microservices is how developers are going to handle this without having experience, and even more if they are also switching to others technologies. In our case teams moved from PHP to Java/Node. To share an idea of our initial state, we had several teams working mostly on a particular domain. Ideally, Team A is working on Domain A and team B was working on Domain B, but in reality, it’s more complex. Sometimes, domains are delimited by a fuzzy line from a business perspective. Other times, the Team didn’t build a clear line for splitting responsibilities and the domain can end-up being owned by multiple teams. The first step is to analyze each domain, understand their dependencies and sometimes it’s even necessary to transform grey areas into proper domains. The second step is to build a strategy to support teams in moving out of the monolith. Like I mentioned before, probably not all members have experience working with microservices in other technologies or owning the deployments and infrastructure. To solve this, we built a Support Team with Senior Engineers with a background in Java or Node and microservices. The idea is to embed some of those engineers in the teams to help them with their current work in the old code and splitting the logic in a new service. Collaboration between both parts is the key to success. Once the new service is able to take over the old code, one of the members of the Support Team is going to remain permanently as a member of the Team. From the technical side, the Support Team spent some time building some tools to make this transition smoother: seed projects, tools, terraform modules and also some migration tools. I already talked about most of them, but in the case of migrations tools, we built a proxy to be able to perform progressive shifting of the traffic by percentage and country. Also, we started to implement request mirroring to see how the application works with real traffic but without impacting the end user. Our journey isn’t complete, and we are far from an ideal scenario. The truth is, platforms are constantly evolving and improving and you need to keep up with that. At this point, we realize that our efforts were worthwhile; teams are autonomous and independent. We move faster and don’t have incidents during releases to production. This is an ongoing process so we improve daily and iterate around ownership, standards and tooling, as part of the process. Hopefully, in the near future things are going to be even better. We operate a network of online trading platforms in over 40… 424 Microservices Monolithic Monolith Distributed Systems 424 claps 424 Written by Follower of the blasphemous Scala programing language and the dark arts of JavaScript We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Follower of the blasphemous Scala programing language and the dark arts of JavaScript We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-05"},
{"website": "Olx", "title": "ranking ads with machine learning", "author": ["Chandan Routray"], "link": "https://tech.olx.com/ranking-ads-with-machine-learning-ee03d7734bf4", "abstract": "About OLX Group OLX helps people upgrade their lives by making it super easy for anyone to buy or sell almost anything through our platforms. We operate in more than 40 countries with varied demographics, different languages and usage patterns. Search is an integral part of any e-commerce website and one of the most important user journey towards the conversion. At OLX, search generates about 80% of total conversions and we strive to deliver the best search experience to the user. In this blog, we are going to talk about how we improved search results rankings using machine learning and we hope this could serve as a starting point for many companies who are starting their journey in search rankings. The search functionality on any platform usually works in the following way, user enters a query(search string), might apply some filters like brand of the product, price range etc. This information is explicit and usually is not relaxed, so for e.g. if the user is looking for “iphone” in Cape Town, inside the “Cell Phones” category, we end up with pseudo query like: This query hits our search engine and depending upon the query and filters we get back anywhere from a few to over ten thousands of relevant results, this is what we call “ Recall” of the query(You can read more about how we improve the recall here ). Now, in what order should we present this to our user as all of them match the user query, this is what we called the “ Ranking ” problem, i.e. given a list of items and user query, how should we rank the list of items such that we have the most relevant results at the top. Usually a machine learning project is structured in these following steps: Defining a baseline approach : Deciding a decent baseline that your ML solution should outperform. It could be a previous solution to the problem or a solution based on business intuition Formalising the ML problem : Transforming the business problem into a ML problem for e.g. whether its a classification or regression, defining the target variable, the objective function, losses etc. Dataset and Labeling: Generating a labeled(if required) dataset for training and evaluation. Feature Engineering and Modelling: Creating the right features for training the model, cross validation, hyper-parameter tuning etc. Serving: Building infrastructure to serve the ML solution that meets certain performance requirements Evaluation and Experimentation : Defining metrics for evaluating the model offline as well as online in the A/B testing phase Lets go step by step through each of these as we move ahead. In online classifieds, one of the important factors for conversion are: Proximity of the item: Measure of closeness between user’s selected location and item’s location Freshness of the item: Measure of how recently the item was listed on the platform. These are also some of the sorting options that a user gets on our platform. We know from our past experience, that ranking by proximity or freshness is better than ranking randomly. So, an easy and intuitive step that we did in the past is to linearly combine these two factors to create a weighted score which we used to rank the items: Now the question arises, how did we get the weights ⍺ and β. A simple approach would be to launch multiple A/B tests to see which weights are performing the best, a simple metric like Conversions per Search can be used to measure the performance. Great! so why do we need machine learning if we can do this ? There are multiple drawbacks of this approach like what if there are some combinations of weights that we didn’t explore, or can the score be better represented with some non-linear function of the factors, or what if we want to add other factors. And also, A/B test usually takes days and with many factors this could take forever to get decent weights. This already tuned weighted formula served as our baseline and also the starting point. For the first version, we decided to include some new factors and learn these weights with historical search log data. In a broad way, a ranking or learning to rank problem can be formulated as, There are three popular approaches that are used across academia and industry to convert this to a learning problem , they are classified depending on how many items they consider in their loss function: Point-wise approach : The algorithm learns by measuring loss over a single item, which is usually the difference in the actual relevance score and the predicted relevance score of the item. It reduces the ranking problem to classification or regression problem. Pair-wise approach : The algorithm learns by measuring loss over a pair of items, which is the difference in the actual relative order and the predicted relative order of the two items. It reduces ranking to a pairwise classification problem. List - wise approach : The algorithm learn by directly minimising the IR measures like NDCG or MAP. In practice pair-wise methods have been found better than point-wise methods because predicting relative order is closer to the nature of ranking than predicting class label or relevance score independently. Though a big upside in point-wise ranking is that you can get started with your favourite classification or regression algorithm with no modification at all. That being said, feel free to try different approaches offline first and decide which suits you best. Here forward, we are going to show how we used point-wise method as our first approach towards solving the ranking problem. Now, let us take a look on how did we get the labeled dataset. There are two ways to create training data. The first one is by human judgments, in which human judges are asked to choose more relevant item given a pair of items and the query context, but to get a dataset of substantial size it could be very costly. In the second approach, the one that we used, past search log data is used to create relevance labels. For example, suppose for a search query, we presented the user with 100 items, out of which user scrolled up to the first 8 items and interacted with them. We convert these interactions to relevance labels, for e.g. an impression has a relevance score 0 and click has 1. These labels serve as target for the classification problem, later during prediction time, the class probability of the relevant class(in the above example click ) is used as the ranking score. In case you have multiple relevance levels you can use methods like McRank , Prank etc. to convert class probabilities into ranking score. We have various signals on our platform that can be used to determine relevance of a particular item given a search context, for e.g. Features that provide proxy for the performance of the item like its CTR during the last day or the last week Features that provide information about the activity of the item’s seller on the platform like number of items posted in the last week Personalisation features like what was the price of the item that the user last bought, within a product category Prioritising which features to use is very important as making the same features available online during the prediction time is a challenging task, which can be be very time consuming and can involve a lot of engineering work. A good approach would be to explore all the features offline first, by recreating them from the historical data and selecting the features which are providing the best uplift in the performance with least effort to make them available during the prediction time. Also, even before exploring all the features, a good exercise could be to use the features that are already available during the request time or are present in the search infrastructure. This will let you know early on, if you are optimising the right objective function offline or if the assumptions that you have made during the labelling step are okay, plus as the system matures it gets hard to debug. For us, the final training dataset looks something like this: Choosing the right evaluation metric is very critical and it should take into consideration various factors such as layout of the results page, business impact etc. There are many options available and you can pick one or multiple. We chose NDCG@K as our evaluation metric for measuring the offline performance of the algorithm, K can be decided based on how many items does the user actually see in the search results on average. For e.g. if the user rarely goes to page 2 of your search results, then you can pick K as the number of the items you show on page 1, as you would really want the relevant item to show up in page 1 of the search result. To measure the performance online, we can look at metrics like conversions/search or any other key performance metric related to discoverability in search. For the first version of the ranking model, we used the features that we were already using in the hand-tuned formula but this time learned these weights by training a logistic regression model on the labeled dataset. These features were already being calculated online, so no separate infrastructure was required for testing, plus training a simple logistic regression helps us to quickly validate the coefficients whether they align with the business intuition or not. Another big advantage of using logistic regression model is that the learned coefficients can directly be plugged into the ranking function of the existing search engine so that we don’t need to build additional ML serving infrastructure to test this model. You can also take advantage of plugins provided for ranking by popular search engines like Solr and ElasticSearch . With this minimal effort we were able to increase the key performance metric for search by 10%. These initial results validated our training process and laid down the foundation for the project. To keep the blogpost as an introduction towards ranking, we deliberately didn’t cover certain aspects of the problem like the position-bias , improvements provided by pair-wise methods, designing a feature service and high performant ML serving architecture. We plan to cover these topics in future. Special thanks to Cristian Martinez , Mouloud Lounaci , Andres Pipicello, Colin Smith , Jorge Corigliano, Vladan Radosavljevic , Data Science and Personalisation & Relevance team at OLX. Libraries XGBoost: https://github.com/dmlc/xgboost/tree/master/demo/rank LightGMB: https://github.com/Microsoft/LightGBM/tree/master/examples/lambdarank CatBoost: https://github.com/catboost/catboost/tree/master/catboost/benchmarks/ranking TensorFlow Ranking: https://github.com/tensorflow/ranking Search Engine Plugins Solr: https://github.com/airalcorn2/Solr-LTR ElasticSearch: https://elasticsearch-learning-to-rank.readthedocs.io/en/latest/ Evaluation Metrics NDCG: http://proceedings.mlr.press/v30/Wang13.pdf ERR: http://olivier.chapelle.cc/pub/err.pdf We operate a network of online trading platforms in over 40… 1.1K 2 Thanks to Mouloud Lounaci and Vaibhav Sharma . Machine Learning Search Search Ranking Learning To Rank 1.1K claps 1.1K 2 Written by Machine Learning, Search and Personalisation @ OLX We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Machine Learning, Search and Personalisation @ OLX We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-05-14"},
{"website": "Olx", "title": "webpack from zero to hero", "author": ["Rubens Pinheiro Gonçalves Cavalcante"], "link": "https://tech.olx.com/webpack-from-zero-to-hero-cf1b77b852c9", "abstract": "About OLX Group This article is part of the Webpack from Zero to Hero series , for more background or for the index you can check the Chapter 0: History . Previous - Chapter 2: Tidying Up Webpack Next - Chapter 4: Dynamic Imports and Code Splitting R ight now we achieved a really simple application using Webpack and ES2015+ transpilation, but real apps are more than that, they have CSS, Images, custom Fonts and all sorts of different assets. The good thing is, in your code Webpack will treat then equally as ES Modules , as long you have the proper loader to them. But calm down, we started with keeping Webpack config simple , and we’re going to stick to that! We’re going to learn how to setup more loaders, combine and pass options to them. Now is the time to load assets. So, create a directory called assets in the root of the project and save any image you like there, and then you can follow the examples. The file-loader treats your image as any asset and gives it a url path when importing it: If you run this without installing the file loader first though, Webpack will complain: Let’s install it then and add to our configuration file: Append this snippet to your webpack.config.js in the module.rules section: Translating the regex : we are searching for any file ending with .gif, .png, .jpg/.jpeg or .svg , all being case insensitive (the `/i` flag in the end). Now if you run it again, webpack will successfully execute the build. Using image-webpack-loader + file-loader will provide us with some nice optimization to our image files. It’s also easy to add: And add the rule after the file-loader one: It has some image compressors already enabled by default: mozjpeg — Compress JPEG images optipng — Compress PNG images pngquant — Compress PNG images svgo — Compress SVG images gifsicle — Compress GIF images Similar to images, we can set a rule for video and audio to be processed by the file-loader. Just add the rule to the webpack.config.js in the config.rules section: 😕 — “Wait there’s already a rule for file-loader, how this is going to work?” That’s true, but we can add many config rules for different kinds of files. We’re just not adding the media files together with the rule that matches images because, on the last section, we chained it with the webpack-image-loader and it will probably break when reading an audio/video file. Now you can import audio files, same as you do with images ( file-loader will behave the same, providing the source url for you): Let’s play with that and add the following to the end of the index.js file: You can use any audio file, I’m using this one , but beware, the audio is quite loud 😆. Don’t play it at work! Let’s add Sass support to our app! Same as before, we need the proper loaders and the sass parser : 😱 — “Whawhawhaaaat? Why so many dependencies?” Each one of this dependencies has a role to play: sass-loader + node-sass : receive your Sass files and output CSS css-loader : turns it into a JS module style-loader : gets the CSS module and inserts it into the page inside a <style></style> tag Adding this to the config: Now let’s play around. Create a style.scss file and paste this code into it: Now add an import in index.js file: Each import you do will create a new <style></style> entry in the header of the file. This can quickly go out of control, so you may want just to join all the files together, minify and provide as a single optimized file. For this we have a solution, which is the mini-css-extract-plugin . To install is simple: Let’s add it to our production builds, in the webpack.config.js , on top of it add the require call: Then add the loader to the scss rule, but only for mode production , otherwise let’s stay with the style-loader (and let’s take this opportunity to add source-map support too): And finally, we do the same for the plugins section: Running the build, we’ll see the style bundle being output: As you see, the config already started to grow. But the advantage is, it’s a Node.js file and we can split it into functions! And require it in webpack.config.js : Now you can see that is way cleaner, and you know where the module rules are because the filename follows the same name of the config schema. Remember what we learned in Chapter 2 , a clean config sparks joy ✨ ! B asically Webpack loaders and plugins mostly follow the format we saw in this chapter. There are many loaders out there , some from the Webpack core team , some created by the open source community ❤️, and even one that’s developed by me , webpack-chrome-extension-reloader (check it out 😉). Let’s end here and go deeper in the next chapter . We will learn not only how to setup dynamic imports on our application, but play around with all Webpack code splitting strategies. See you in the next chapter! We operate a network of online trading platforms in over 40… 469 4 JavaScript Webpack Technology Nodejs Frontend 469 claps 469 4 Written by Front End Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Front End Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-07-25"},
{"website": "Olx", "title": "how to handle coordination in a multi hub project and succeed", "author": ["Alan Marcello"], "link": "https://tech.olx.com/how-to-handle-coordination-in-a-multi-hub-project-and-succeed-d63f72434378", "abstract": "About OLX Group As Engineering Manager for OLX Monetization Team in Buenos Aires, I have been working for the last year in a project that required the joint effort of three squads in different hubs: one in Berlin, one in New Delhi and one in Buenos Aires. As you may imagine, this kind of setup has a lot of challenges we had to overcome to reach our final goal: release new Monetization Architecture in South Africa, Pakistan and India. I want to share our journey with you and the lessons learned through the process. Being able to sell extra features to our sellers is a project that integrates four flows: Feature Selection : where the users are able to select which features they want to buy. Payment: integrating multiple payment providers such as credit cards, over-the-counter methods, etc. Post-Sale: where the feature bought is applied. Billing Information and Invoice List: where the users provide information needed for invoice generation and can check their generated invoices. At the beginning, the complete flow was handled only by the Monetization Team in Buenos Aires (MBT) with the following architecture: Having everything centralized in one team simplified the decision-making processes and reduced the coordination effort as everything happened in the same building. This architecture was released successfully in LetGo Argentina on June 2017. However, this setup had some drawbacks: Invoices and Finance Reports were not covered by this solution (this means that the Billing Information flow was not available at this moment). Solution was not PCI compliant. Payment User Experience was driven by 3rd party Payment Provider. As one service had two responsibilities, a release in Feature Selection flow affected Post-Sale flow and vice versa. One team had multiple responsibilities/focuses. Local solution even when OLX was moving towards a global platform. To address those issues, OLX decided to start a process to redefine ownership and responsibilities within this architecture. Our first step was to separate responsibilities for Payment flow. For that purpose, the company created a specific team in Berlin to handle everything related to Payments globally (I will refer to them as PT). PT has developed two new services: a new PCI compliant Payment Service in charge of Payment Flow and a gateway to Payment Providers; and an invoice gateway to a 3rd party system responsible for invoicing and reporting. As a consequence, “Billing Information and Invoice List” flow was created by MBT and original Payment Gateway developed in Buenos Aires was replaced by PT’s two new services. This schema was first released as an MVP (Minimum Valuable Product) in OLX South Africa on May 2018. Our second step was to extract “Post-Sale” flow’s responsibilities to a new micro-service. The task of building this new micro-service was in charge of Monetization Team in India (MIT). India market is huge and so it’s the number of monetized ads. This drove the decision to build a product and technical team that could be closer to the market. The main challenge at this step was to rethink most of the interactions between services so that they would be able to run seamlessly for the end users. This schema (actually, our current one) was released in OLX South Africa on September 2018, in OLX Pakistan on October 2018 and in OLX India on February 2019. To summarize, here is our current setup: “Payment” flow is owned by PT, “Post-Sale” flow, by MIT and “Billing Information and Invoice List” flow, by MBT. “Feature Selection” flow is a special case as it is owned by both MBT and MIT. MBT’s next step is to do knowledge transfer of “Feature Selection” to MIT to simplify the setup even more. Now that you have more clarity about the context, let’s move into the main issues faced and what actions were taken to overcome them. To explain the challenges we had, I chose to focus on the “Second Step” of our journey which was the hardest one: three teams had to split the “Feature Selection” flow from the “Post-Sale” flow. The first challenge we faced was to define a clear scope for the three hubs and to make that scope (along with the estimations to achieve it) visible to all the stakeholders. No wonder since this is the cornerstone of any project. One important difference to our original setup was that there were three Product/Engineering Managers instead of one deciding over the same flow so the decision-making process was more complex as the three teams had to be aligned. To achieve this, two actions were run in parallel: Product Managers did a Feature Gap Analysis between MVP in South Africa and what Old India platform had. So from a Product perspective they re-evaluated the current flows and built a list of requirements based on differences, iterations over features and users’ feedback. Engineering Managers had discussions on how to execute the split from a technical standpoint to get the list of technical tasks that were needed to achieve the goal. Also, technical debt was included as part of this list. Once those lists were created, both Product and Engineering Managers began a process to prioritize the list of features/technical tasks and get a High-Level Roadmap , that included both dependencies and estimations. This sessions were held remotely vía Zoom Meetings. Once the High-Level Roadmap was shared and approved by the stakeholders, Product and Engineering Managers from the three teams arranged to meet in India to discuss further. This trip had the following objectives: Team Building (something vital for a project this big) Define the details for the tasks identified in the High-Level Roadmap. Have sessions with India Business Teams to have more insight about their needs, challenge the proposed solution in a different context and identify any problem or opportunity that might be overlooked. From this sessions, there were some minor adjustments to our original definition. Each team returned to their homes with a clear understanding of the tasks and their assignations. But not everything was as good as it seemed at that moment. One of the results of these meetings was the assignations from each team at Frontend and Backend level. As I said above when talking about the Project Overview, “Feature Selection” flow was owned by both MBT and MIT. MBT was assigned with “Feature Selection” entry points and MIT was assigned with “Feature Selection” page at Frontend level. But there was something we missed out until it was a real problem: for Backend, the assignations were the opposite. MIT’s Frontend needed information for MBT’s “Feature Selection” service (already up and running in South Africa) and MBT’s Frontend needed information from “Post-Sale” service that MIT was building as part of this project. This created a dependency between teams that considering the timezone difference generated some frustration in team members. We decided to invest time in improving the project’s documentation to allow both Frontend teams to integrate seamlessly. The second challenge we faced was the time difference (especially between Buenos Aires and New Delhi which is 8:30 hours). If we consider strict work shift from 9am to 6pm, there is only an overlap of 30 minutes between teams. This time would only be helpful to catch up on open topics for Buenos Aires to follow-up during our day or maybe discuss something very specific. So we had to do something to increase the amount of interactions between teams. We decided to move work shifts (1 hour before for MBT and 1:30 hour after for MIT). This simple move allowed to increase the overlap between MBT and MIT in 2:30 hours providing a total of 3 hours. More than enough time to analyze/troubleshoot issues together, follow-up project progress and so on. Considering we had three teams in different timezones, we needed to find a way to keep the information flowing. This step was crucial as communication was our key to succeed . Our first action was to create Slack Channels with different purposes. We created one channel with both Product and Development of the three teams to centralize the communications. We also had a channel specific for management that included Engineering and Product Managers. In both channels, people were encouraged to work in an asynchronous way: any issue or concern should be reported in the channels so any other member (either from his/her own team or from another one) could help to troubleshoot or clarify if the issue was something expected. The rationale behind was to avoid the overhead of creating tickets for potential issues that were working as designed. Our second action was to create a spreadsheet with the list of dependencies with two purposes: have clarity on dependencies between teams and the expectations in terms of dates and also improve our follow-up process. From a management perspective, along with the Slack channel mentioned above, we had set up a follow-up meeting between the Product and Engineering Managers of the three teams twice a week where we analyzed the dependency list and any open topics that may have arised. This was really helpful as it was a good way to be aligned when dealing with issues that affected more than one hub and also it was a way to provide/get continuous feedback. Along with this, we had also setup a meeting between MBT and MIT teams to provide a quick report (either written or by conference call) with a status on issues fixed, issues in progress, issues that needed follow-up. For this, we held a daily meeting between Engineering Managers. Really close to the idea of a daily standup but only at management level to simplify the amount of attendees. This project had a complex setup. But it became easier as we fine tuned two vital aspects: communication and well-defined processes . They were key aspects towards reaching our objectives on time, which was crucial since the flow we owned was critical for the platform. We learned from the experience of our journey to do the necessary tweaks/adjustments to provide a way of working (WoW) where the three teams felt comfortable. This WoW helped us to keep the information flowing, to improve our workflow and the most important thing: to become one team . I would like to hear more about your own experiences in this kind of projects. Feel free to share the pain points that you might faced in different projects and strategies you used to overcome them. I would like to say thank you to: my wife for her support and patience while working on this; Maria Ciminieri , for her important help and support to complete this post; German Scoglio , for encouraging me to write this when I came with the idea; Patricio Rocca Hughet , for his suggestions along the writing process. Last but not least, I want to say thank you to the members of three teams that made this project possible with great success. If you want to join us, you can check OLX open positions in https://www.olxgroup.com/careers We operate a network of online trading platforms in over 40… 255 Leadership Collaboration Management 255 claps 255 Written by Engineering Manager at OLX Group. Father, I love technology and sports (Independiente and Miami Dolphins) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Engineering Manager at OLX Group. Father, I love technology and sports (Independiente and Miami Dolphins) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-04-26"},
{"website": "Olx", "title": "webpack from zero to hero", "author": ["Rubens Pinheiro Gonçalves Cavalcante"], "link": "https://tech.olx.com/webpack-from-zero-to-hero-1e02cb42ab7b", "abstract": "About OLX Group This article is part of the Webpack from Zero to Hero series , for more background or for the index, check the “ Chapter 0: History ”. Previous - Chapter 1: Getting Started with the Basics Next - Chapter 3: Everything is a Module L ast chapter we saw the need for BabelJS to transpile our code, and the way to make Webpack pass files to other parsers like BabelJS, which is through loaders. But until now, we were running Webpack with no configuration file. In this chapter we’re going to create our very first Webpack configuration, learn how to use a loader and set up our local development server . Let’s start! First we need to install the dependencies: Babel Core : it has all logic necessary for transformations and also some polyfills; Babel Preset Env : it is able to choose the right transformations/polyfills depending on the target browser list ; Babel Loader : it will be responsible for receiving the input file from Webpack and passing it through BabelJS. First let’s setup the Babel to use the preset-env. Create a file called .babelrc with this content: And set a browser list range on package.json : Note: I’m creating a pretty generic query here. For production apps, always check analytics to properly choose your target browsers! Let’s see how many browsers will be targeted with this query: As we don’t want to install browserslist just for a single run, we will directly use it through npx. The output will be ( from the time of this article publication ): So one of the baselines for transpiling/polyfilling will probably be Internet Explorer 11 (and its mobile version) . As I said before, don’t go for queries which are too generic, instead build the list based on usage data from your target audience. Now we just need to “ tell Webpack” that all JS files should pass through Babel. Let’s create a webpack.config.js file on the project root directory and add this code: Webpack config is just a NodeJS module, exporting the configuration object . 😖 - “ Hey, I don’t understand regular expressions, can you explain that?” Right behind ya⚡️! The expression above should just match all files ending with .js : We need to escape the “ . ” from .js , because in regex lingo it is used as a mask for “any character” and we don’t want it, we want the actual period char; Then we set the “ $ ”, stating that the matching should end right after .js , so we don’t mismatch things like .json . Be happy, you’re a regex master now 💪 ! Some will say to put all babel and browserslist configuration inside the Webpack configuration, but in my point of view, both Babel and browserslist configuration tend to stay the same (size-wise) , while Webpack config tends to grow , so the key to keep it organized is to make it as modular as possible. Like with any normal source code (remember that the Webpack configuration is a Node.js file!), if you see it’s growing, assuming too many responsibilities and repeating the code, you should break it up! For any app/site development, we need to create a dev environment, where we can test and see the updates right away. And since we haven’t seen our actual code running in the browser yet, I think is time already, let’s go and do it! As you may know or have heard about, Webpack has a pretty nice tool called webpack-dev-server , where you can simulate an HTTP server on your machine integrated with a hot module reloading feature. It is pretty nice as the browser reloads every time a compilation is triggered, and you don’t need to be reloading the page manually every time you do a change to your code. We will install both the webpack dev server and the plugin to generate an index.html for us: On the webpack config we add the plugin to the “plugins” section: Tip of the day: If you don’t want to output an index.html on the production builds, we can skip it by checking the webpack argv.mode : Some explanation for the code above: Webpack accepts both an Object or a Function as configuration. When you provide it as a function, it will inject the env and the argv as parameters: env : everything the client (webpack-cli) receives under the env param comes as an env object property, e.g.: argv : all the arguments given to webpack config that are part of the configuration schema, e.g.: As we’re starting simple there’s no need yet to create two configuration files, one for development and another for production, so let’s stick to simplicity. Now is time to run the server, which accepts the same arguments that the webpack client does (plus some additional ones). Let’s remove the “build:dev” in package.json and change to: Let’s test it! And you’re going to see something like this: Now open the page at http://localhost:8080/ , open the dev tools on the console tab, and you’ll see this: If you click on the link right after the console.log result on the dev tools console tab, you’re going to be forwarded to the sources panel, and you’re going to see something interesting: This is the transpiled code by Babel. But how can I check my actual code? Enter the source maps! ⚡️ Source maps are something that will map your actual source to the final bundled source, letting you use breakpoints and see the actual code lines on stack traces in case of exceptions. To enable them, just add this to webpack.config.js : Stop and run the dev server again and check the source on the console tab link, and this time you’re going to see the actual source code! N ow that we have everything up and running on our development server, we have paved a way to add more loaders and parse all kinds of files. But let’s read about that in the next chapter - see you there! We operate a network of online trading platforms in over 40… 370 3 JavaScript Webpack Nodejs Technology Frontend 370 claps 370 3 Written by Front End Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Front End Engineer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-06-13"},
{"website": "Olx", "title": "plantuml diagrams as code", "author": ["Maryna Cherniavska"], "link": "https://tech.olx.com/plantuml-diagrams-as-code-93773b394cd9", "abstract": "About OLX Group Everything “as code” is all the rage now. What can we represent as code except for the programs? First of all, infrastructure as code is gaining popularity — it is enough to see the Google Trends graph for it to see that it is steadily climbing year by year. Terraform , OpenShift , CloudFormation , Helm , Puppet and many other tools are the representatives of this trend. However, this article deals with something else entirely: diagrams as code. Why do it? Code has a few advantages over, well, diagrams: It is readable. At least good code is supposed to be. A lot of people absorb written information better than anything else, despite that saying about one picture being better than a thousand words. It is compact. A text file size is usually times and times smaller than any picture. And is much easier therefore to store in the repository. Version control. You can keep pictures under version control, however, they are binary files, and the changes are therefore obfuscated. If you change the picture in a repo, people will not know what the change was about, until they check out the repo and have a look at the picture. The diff itself won’t be much help at all. It is easy. It is much easier to type “Service A uses Service B” than draw those boxes on a diagram, label them, connect them with arrows etc. Especially for people who might be, let’s say, artistically challenged. It turns out, however, that there’s a tool that allow you to have a best of both worlds. And this tool is PlantUML . PlantUML allows to basically write text which is automatically transformed into the diagrams. It has its own pretty simple DSL and allows for a lot of the types of UML diagrams: Sequence diagrams; Usecase diagrams; Class diagrams; Activity diagrams; Component diagrams; State diagrams; Object diagrams; Deployment diagrams; Timing diagram. Also, it supports some non-UML diagrams which are pretty cool, for example the Wireframe diagrams for UI design, which seems a really interesting concept. How to use PlantUML? Actually, in a hundred ways. It can be installed locally as a separate tool or as a plugin to basically anything (Wikis, forums, text editors, IDEs and what not, check the link and chances are, you will find at least several alternatives that you’re already using). As my tool of choice is IntelliJ IDEA, this is the plugin I use. Let’s try a sequence diagram, because it’s the one that usually gives me a lot of headache. (All those swimlanes and blocks that need to be aligned, don’t get me started.) We’re designing an automated restaurant order system (no waiter, just a tablet to order with — know what I mean?) and need a bird’s view of the basic flow. We have a client who orders from the menu, an inventory against which the order is checked, and a feedback system to be able to correct the order. And we’ll put some queues in to make the process asynchronous (just because we are cool). How will it look? Approximately like this. We can clearly see that we have one actor — Client , four participants MenuService , InventoryService and two queues for requests and responses — and a database to keep track of all this. The IDE plugin instantly transforms the code into this picture: What can I do with it? I can export it into a picture and show to anyone. Also, I can use the online demo server and just copy and paste the whole code I have into the textbox there and click Submit . The demo server will return a URL to the generated diagram: http://www.plantuml.com/plantuml/png/bP71QiCm38RFzLE8xXmeEuPIkgOxxB33khs0PgiiuDYPR1S6VVWfyKhID1HsACYgykd_zMz3H8wfiGfrz0oLhSb5m00LF30PrShVjQvRbEuhkVHEVAermNeTShNPNCqSI8Y9fJpGTvB7MwBKZSiZJOTVt8cShFtv3pMqtWMQOWr6_CH0i1ncOV_NN7Q6zZkbAYWsOnibU3R4SFJUm95z0_6Be941DJOh7ytxAe5Shm0uTvKwdy_zJzsOA-vWR1qYEK1dX8JcWeCUVXyrkKa9WNfAxa7q6J9ITfLZKSoJHggY7t2PDJdEVYTRHHy1CNiUfgK-eQqDTF6C5A2T2J_84z1D66enMLQLb-jaOQr5tM_Iw3HPg2CrGIrkezQUcNJi8iwnAxA1eF66R5u-Q9tPEhXwNAswf384dZ-U53cJtQcjbAcnlm00 This URL can be used to get the picture into your project readme file, confluence wiki or just any web page. The interesting thing about it is that a picture itself isn’t stored on the demo server, because all the information is already encoded into the URL. So, just the URL is stored. I think this tool is great to play with and explore. And these “diagrams” are great to store under source control, because all the changes are immediately readable by just scrolling to a diff. And it goes so much faster than drawing and repositioning all those blocks and swimlanes. If you like the idea, by all means go and try the tool on your own! What I’ve shown here is just a very basic example, but I think one can do a lot with it. The website also has a FAQ to help people with some issues that may arise (I experienced none with the IDE plugin, but this tool has so many integrations which I haven’t tried). Not all of us are artists, but the great thing is, not all of us have to be. The article is also published on my personal blog here . We operate a network of online trading platforms in over 40… 6 1 Uml Design Code Design Process 6 claps 6 1 Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-08"},
{"website": "Olx", "title": "coreml image quality observation on ios", "author": ["Raman Soni"], "link": "https://tech.olx.com/coreml-image-quality-observation-on-ios-4e02b25285a1", "abstract": "About OLX Group In Earlier Series of blogs, we talked about why image quality prediction is important for our platform and how we have solved this problem with deep convolutional networks using transfer learning of Pre-Train MobileNet Model. In this part, we’ll talk about how we have used machine learning model in iOS for image prediction. First Approach: Using TensorFlow TensorFlow is an open-source library for dataflow and differentiable programming across a range of tasks. It also contains a symbolic math library useful for machine learning applications such as neural networks . It was developed by the Google Brain team. One of the simplest ways to integrate TensorFlow is via CocoaPods . The prerequisite for using TensorFlow is having a TensorFlow-compatible deep learning model. In the previous blog , we had trained our model using Keras ( which creates a model in .h5 format). However, .h5 is incompatible with TensorFlow, so we first converted the Keras model to a TensorFlow-compatible one(.pb format). Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow , CNTK or Theano . It was developed with a focus on enabling fast experimentation. To convert Keras model to TensorFlow-compatible format, we used existing functions provided by Tensor Prepare Model Tool . For mobile deployment, this tool first freezes all variables and graph nodes to optimize graph size (as we cannot train machine learning model on real devices so we don’t need variable graphs and can save some memory space here). Then this tool saves the constant graphs and weights into a binary Protobuf file (Protobuf will do another round of size optimization as it has a reduced boilerplate code). While freezing, the tool also applies node pruning which removes nodes with no contribution to the output Tensor. Following diagram shows the conversion process. For more information, you can refer here . You can find the conversion script here . Now we had everything set up for injecting machine learning in our application. Unfortunately, when we finished our implementation, we realized that using TensorFlow had increased the app binary size by an extra 11 MB. On deep-diving the issue, we found that TensorFlow iOS framework is ~10 MB in size. That’s too much, and we do not like that at all! So, we brainstormed some more, and voila, onto our second approach! Pros: Its responsive build allows easy visualization of every aspect of a graph. Backed by Google, TensorFlow has the advantage of the seamless performance, quick updates and frequent new releases with new features. When it comes to distributed computing, it is easily identifiable on both CPU and GPU. It can be customized and is open-source. It offers advance-support for queues, asynchronous computation, and threads. Its auto differentiation capabilities benefit gradient-based machine learning algorithms. This allows you to compute several derivatives of values in relation to other values, thus leading to a graph extension. Cons: To operate, it necessitates an advanced understanding of machine learning, calculus & linear algebra. Does not support OpenCL (Open Computing Language) GPU memory conflicts with Theano ( a Python library that permits defining, optimizing, and evaluating mathematical expressions, involving multi-dimensional arrays effectively ), when imported in the same scope. The TensorFlow iOS framework is 10MB in size — a considerable disadvantage when you want to keep your app size optimized. Second Approach: CoreML CoreML is a new machine learning framework introduced by Apple in 2017 — it’s great for newbies just starting. One can use this framework to build more intelligent Siri, Camera, and QuickType features. CoreML provides blazing fast performance with easy integration enabling developers to add intelligent features with just a few lines of code. CoreML comes bundled with iOS, thereby no extra space required for machine learning. This perfectly suits our requirement! The first step of integration is to convert Keras model to CoreML-compatible format (.mlmodel). Big thanks to Apple that they provide support tool ( CoreMLTool ) for such conversion. Conversion script below: The second step is to add this newly created model (.mlmodel) into the application source code. Again a big thanks to Apple for an easy-to-integrate interface. All we have to do is import the model into the project and Xcode will do the rest. Here’s how it looks when imported. The illustration above shows: Information about the model Model type and its expected inputs and outputs Following flow-chart [ Fig 3 ] illustrates the model integration process: To reiterate, our purpose for machine learning here is to predict image quality. This is where we need Apple’s Vision framework. Vision works with CoreML to apply classification models to images and preprocess those images — making machine learning tasks easier and more reliable. We’re all set to start coding and predict image quality. Create Vision Model from CoreML Model — Vision model helps you to set up all the image processing pipeline along with helping in preprocessing, rescaling, cropping, detecting rectangles/barcodes/faces and much more. 2. Create Vision Request for image quality prediction — an image analysis request that uses a Core ML model to process images. From the above illustration ( Fig 2 ), model evaluation output is a multi-array (array of image quality scores). In quality prediction, output array contains only one result (quality score) for an input image. 3. Result handling Since we expect a single result, our code extracts the first value from output array and uses it is as VNCoreMLFeatureValueObservation ( VNCoreMLFeatureValueObservation is the type of observation which results from image-analysis whose role is prediction rather than a classification or image-to-image processing). All Vision requests use GPU computation (on a high priority background thread) so the result has to be passed to the main thread. 4. Perform vision request — In our use case, image quality prediction happens when the user chooses an image while posting an ad [ Fig 4 ]. Pros: Very easy to integrate. Not restricted to deep learning only — can be used for logistic regression, decision trees, and other classical machine learning models. Comes with a handy converter tool that supports several different training packages (Keras, Caffe, scikit-learn, and others). Cons: CoreML only supports a limited number of model types. If your model is trained to do something CoreML does not support, then you cannot use CoreML. The conversion tools currently support only a few training packages. A notable omission is TensorFlow — arguably the most popular machine learning tool out there. You can write your own converters, but this isn’t a job for a novice. TensorFlow is not supported because it’s a low-level package for making general computational graphs, whereas CoreML works at a higher level of abstraction. No flexibility & little control — the Core ML API is very basic in that it only lets you load a model and run it. There is no way to add customizations to your models. Available on devices supporting iOS 11+ only. In summary, through our journey of model conversion to model integration, we learned that using TensorFlow for machine learning in mobile may increase your app size quite a bit, making it the less favorable approach. On the flipside, using CoreML keeps things simple and easy to integrate. During the process, we also learned how Keras model can be converted and deployed to different high-level machine learning frameworks. Eventually, this entire exercise makes our app experience more customer-centric . How? Image quality prediction helps our users [sellers] understand and improve the quality of their ads, leading to more interested buyers, leading to a better deal, leading to a happy user . References: www.tensorflow.org github.com github.com developer.apple.com github.com We operate a network of online trading platforms in over 40… 192 Thanks to Ashwani Bhargava and Akash Gupta . Machine Learning iOS iOS App Development Data Science Image Processing 192 claps 192 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-02-25"},
{"website": "Olx", "title": "what do you need to know to be a successful data scientist", "author": ["Viacheslav Dubrov"], "link": "https://tech.olx.com/what-do-you-need-to-know-to-be-a-successful-data-scientist-78823200899c", "abstract": "About OLX Group Disclaimer: This is my very subjective point of view on what you need to know to be successful as a Data Scientist. All statements presented here are based on my personal experience in data science workflow. D ay to day work of a Data Scientist in the industry requires to not only know theoretical aspects of the machine learning algorithms for making correct model choices and use Scikit Learn methods “fit” and “predict” to build that model. In Applied Data Science, the main goal is delivering a service and providing results to your customers. To make this possible, Data Scientists have to be aware of other topics which help them deliver the required results. Below is the list of topics necessary to build successful data science services. First of all, we can divide data science workflow into 4 main stages. ETL pipeline ML pipeline Productization CI/CD Statistics Machine Learning theory Programming knowledge and code writing skills The first two components require little explanation, as they are classic DS/ML topics. Therefore, the discussion will focus more on the third part. Programming knowledge is depending on the type of a position, but in general it means knowing the basics of two programming concepts ( OOP and Functional ), Data Structures , Python (you could choose dark R path, of course), with an adjacent knowledge of the most important and popular libraries for manipulating data and building models ( pandas , numpy , scikit learn ). Knowledge of some additional basic programming language (Java, C++, Scala) will only help you. By code writing skills , we mostly refer to the ability to write clean, readable, maintainable code. This makes it much easier for the team to work together with a data scientist in supporting the code. ETL basics Relational databases / SQL NoSQL / MapReduce / Spark Schedulers (Airflow) For building reliable, scalable and maintainable systems, DS specialists have to know all aspects to which they should pay attention while accessing, manipulating and loading data. They also need to know general relational database building principles and SQL syntax for writing effective queries, understand NoSQL databases foundations, MapReduce paradigm, and Apache Spark framework. As to the schedulers, we have put them into this stage, but they also intersect with other stages (Productization, CI/CD). The most popular and flexible is Airflow . Production code / Testing Microservice Architecture / Docker Hosting (RestAPI basics / Flask / Tensorflow Serving) As for any software product, if the code goes into production it should be readable and maintainable. And for providing this a Data Scientist should (at least in general) know about basic testing principles (unit tests, acceptance tests, system tests) and tools (unittest, pytest, unittest.mock). After finishing their project, Data Scientists have to create a service from their model and code. Microservices architecture is everywhere right now and without the knowledge of Docker containers , you cannot deliver applications. The model should not be just a script, it should have endpoints, that is why hosting is in this list. It includes RestAPI basics , Flask (alternatively you can choose Django/ Tornado), Load balancing ( Gunicorn ). In addition, there is Tensorflow Serving . Bash Makefile version-control system (git) deploy tools / gitlab CI DVC Bash is an integral part of the entire data science workflow, but we have added it in this part because it has the most significance in CI/CD . Bash and Makefiles are a very important part of local deployment and automation process. Of course you can’t participate in any projects right now without VCS knowledge. But automatization tools such as gitlab CI also may come in handy during development. Since Data Scientist works with data, the use of data VCS like DVC can greatly facilitate the workflow. Cloud Services (AWS/ Azure/ Google cloud) With the increase in popularity of “cloud” and “serverless” architectures, companies integrate their solutions with Cloud Architectures. We didn’t put Cloud Services into a specific category, because every service provider ( Amazon , Microsoft , Google ) has its own solution for every stage. For example, AWS (which we’re working with) has Sagemaker and Recommender Tool in the ML part, and you always can build your specific solution with EC2, Auto Scaling, Load Balancer, VPC and other technologies and tools which you could use for improving reliability, scalability, and maintainability. In the ETL part, AWS has a lot of stuff like SQS (queries), EMR (Spark, Hadoop), Glue (ETL jobs), Athena, etc. We recommend that Data Scientists spend some time to familiarise themselves with one of the cloud services. This is by no means an exhaustive list, and it can be always adopted for your specific domain. However, this represents my own experience of areas and tools that you need to be at least familiar with, if not proficient in. Have fun! A journey of a thousand miles begins with just a single step. We operate a network of online trading platforms in over 40… 363 1 Thanks to Andreas Merentitis . Data Science Machine Learning Python AWS Microservices 363 claps 363 1 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-25"},
{"website": "Olx", "title": "qualifying image quality part 2", "author": ["Mohit Sharma"], "link": "https://tech.olx.com/qualifying-image-quality-part-2-55b2479fb8a8", "abstract": "About OLX Group In the first part of our series of blogs, we talked about the importance of images on our platform, the impact of image cropping on quality and how we quantified it. In this second part, we measure quality in terms of other aspects such as brightness, contrast, sharpness of the image etc. We iterated through a number of models to measure image quality score. Our final and most scalable solution was a deep convolutional network that scores images based on quality. We also made sure the network was light enough to keep computation cost low, and finally deployed the model on-device using TensorFlow Lite. To rehash a bit on how our platform works, people post advertisements of stuff they want to sell and other people contact them if they find their ads interesting. Among the first and the most prominent things a buyer notices about the ad is the image of the item. And so, the quality of that image determines how many buyers click on that ad or contact the sellers. Now when a new buyer lands on the platform and is welcomed by images like these the buyer might not want to explore these ads any further. However, if the buyer sees images like these the buyer is much more likely to click on the ad to find out more. This is a problem for both buyers and sellers since sellers are missing out on potential buyers and buyers are missing out on good deals only because the pictures were poorly taken. This makes it extremely important for us to quantify image quality so we can identify poor quality pictures and then either reach out to sellers asking them to retake the picture or possibly fix it ourselves. The challenge in assessing image quality is that judgement of aesthetics tends to be very subjective in nature. We tried to tackle it in the following ways: Attempt 1: NIMA We looked at pre-existing models that try to address this problem such as Neural Image Assessment Model(NIMA) by Google. The model comprises of a deep CNN that is trained to predict which images a typical user would rate as looking good (technically) or attractive (aesthetically). On applying the model directly to our data, we found out that for most of our images, the model gave scores that lied in a very narrow range and hence it wasn’t of much use to us directly. We believe this is due to the difference in the nature of the training data for NIMA and our images. Attempt 2: Feature Engineering and Modelling To counter this, we tried to identify certain attributes of images that intuitively relate to image quality and created features to capture those attributes. We started off with some obvious features such as brightness and contrast of the image. These metrics give us a macro idea of the picture but aren’t very useful just by themselves. We needed to determine if the brightness and contrast were consistent across the image or if there were any local pockets of extreme brightness/darkness. This is how we solved it: We take an image and break it down into foreground (Object) and background. We use the MASK RCNN mentioned in the first blog to achieve this. Another important aspect of good pictures is spatial consistency of light, i.e. no corners or parts of the image which are extremely dark or too bright. To do this, we further broke both the foreground and background images down into four quadrants and compared the brightness and contrast metrics of object and background in each quadrant. When there is a large difference between object brightness/contrast and background brightness/contrast, it tells us the object is different from the background and therefore distinctly visible. We added another feature to compare the brightness of the darkest quadrant with the brightest quadrant. We included other metrics such as the number of completely white pixels and completely dark pixels since they are good indicators of image quality. The NIMA score for these images was also used. We hand labelled more than 1k images as good or bad to create our training data and trained a GradientBoostedDecisionTree model to learn the differences between the good quality and bad quality images in the feature space . The trained model had an AUC score of 0.9 on test data. But since this solution might not apply as well to other object categories with different characteristics such as for smaller objects with consistent indoor lighting etc., we then tried to create a more scalable solution not based on engineered features. Attempt 3: Deep Learning Model Transfer learning is the domain of machine learning where a model developed for one task is reused as the starting point for a model on another task. In this instance, we used the MobileNet architecture pre-trained on the popular ImageNet dataset for object classification tasks. MobileNet is lighter in comparison to other conventional CNN’s. This makes the network ideal for deployment on platforms with lower compute capacity like smartphones. We chose it since on-device deployment was our ultimate objective. We needed to build a Ground Truth Dataset for training and leveraged the power of Amazon Mechanical Turk to crowdsource the creation of our training data. The labelling process involved getting each image labelled as good or bad by 5 different observers. Since label was either a 0 or 1, an image could have a quality score ranging from 0 to 5. We had over 5k images labelled from M-Turk which formed our training data. So now we have the images and the labels and we need a deep network trained on the images to predict the labels. Convolutional neural networks have shown great results in image classification tasks. Model Details We used the MobileNet V2 architecture trained on ImageNet Dataset as the starting point of our model. It is a deep neural network having many convolutional layers followed by a fully connected layer at the end. It makes use of Depthwise Separable Convolutions instead of standard convolutions. The standard convolution operation has the effect of filtering features based on the convolutional kernels and combining features in order to produce a new representation. The filtering and combination steps can be split into two steps via the use of depthwise convolutions making the process much cheaper. Since our dataset is similar to the ImageNet dataset in terms of the classes (cars, trucks) and it is only our labels which are different, we do not need to re-train whole of the network. We only need to remove the last fully connected dense layers and add our own fully connected layers which can be trained keeping all the convolutional layers fixed. We train the network to predict the score of an image (0–5). The model had a root-mean-squared-error (rmse) of 1.4 on the training set and 1.9 on the validation set. Some of the scores given by model: Deployment We have 2 options to deploy the model: Server Deployment In-App Packaging We chose to go with in-app packaging over server deployment so that we can do the predictions at real time when a user is choosing images to post. Also, you don’t have problems with scaling and maintaining servers when you want to expand to more users. We used the TensorFlow Lite (TFLite) platform to deploy the model in-app. The model’s weights were quantised (reduced to int8 format from float) to reduce the size of the model file. After quantising the weights and doing optimisations around the size of the network, we have a model file which is less than 450 kb which is light enough to ship with the app. Add to that the TFLite libraries which require less than 1 MB which help in keeping the app lightweight. While selecting pictures to upload for his product to sell, stars are shown over images indicating their quality according to the model. To summarise, we wanted to measure image quality for which we used NIMA, then tried handcrafted features, and finally did some transfer learning on a pre-trained MobileNet. We then deployed the model on our Android app using TensorFlow Lite. I would like to thank the team — Akash Gupta , Vladan Radosavljevic , Udit Srivastava and Nicolas Quartieri We operate a network of online trading platforms in over 40… 527 1 Thanks to Vladan Radosavljevic and Akash Gupta . Machine Learning Image Quality Computer Vision Deep Learning TensorFlow 527 claps 527 1 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-02"},
{"website": "Olx", "title": "replicating mysql tables on s3 as parquet danadb", "author": ["Shafi Rasulov"], "link": "https://tech.olx.com/replicating-mysql-tables-on-s3-as-parquet-danadb-5cf53ce2369b", "abstract": "About OLX Group At OLX big data team, we are responsible for maintaining the data lake, which includes 300+ TBs of compressed data on S3. Big portion of these is user behaviour data, which contains information about user interaction with our mobile apps and the web app. The data stored in data lake is in compressed json format. We also provide the same data in parquet format, which is much faster to run reports and analysis on the data lake directly. Different teams query the data lake mainly with AWS Athena and Presto . The second source of the data lake is operational data, which are basically change logs of tables in MySQL databases. OLX is currently operating with 17 brands in 40 countries all over the world, and almost each brand and country has its own database and tables. As big data team, we are responsible for capturing the changes in the databases and write the change logs into the data lake to make global business intelligence, personalisation, communication etc. teams keep their databases in sync with operational databases. Thus, they get the change logs from data lake and apply them to their local databases (i.e Redshift) to get current snapshot of the tables. Moreover, a lot of users connect to MySQL replica databases, to query the tables. But, that puts a lot of additional load on the replicas, and it is impossible to do deeper analysis like joining MySQL tables to the user behaviour data on the data lake. Because almost all analysis require joining operational tables to user behaviour data, and because of the data size is huge it is needed to have very powerful analytical database clusters to handle that amount of load. To make customers use the data lake directly we were missing the second part — the operational tables. Thus, we started to think about how to provide the snapshot of the MySQL tables on S3 as columnar format (parquet). Basically, our stakeholders should be able to query the tables with Athena, Presto etc.(figure 1) as it is in MySQL databases (with ~1 hour latency). The main challenge is that the files on S3 are immutable. So, even to update a single row, the whole data file must be overwritten. The second challenge is the data file format must be parquet, to make it possible to query by all query engines like Athena, Presto, Hive etc. The job starts with capturing the changes from MySQL databases. We developed a project called Lazarus , which reads the MySQL binlog , and adds two additional fields to the table data — operation_timestamp and operation_type (figure 2). Operation timestamp is the timestamp of the system when the records were read from binlog. For binlog we use row-based logging. Because we use single thread to read the binlog, the operation_timestamp always provide consistent sorting of the operations. Operation type indicates the DML type of the operation, which could be one of — insert, update or delete. Lazarus writes the change logs every 5 minutes as micro batch in json format to S3 data lake. Whenever a new file arrives to S3, a lambda function triggers and writes the file path to a kinesis stream. Then a consumer reads the paths of new files, makes mini batches of 30 minutes and puts the message to SQS queue. The kinesis stream has only one shard, which streams the file names of all tables. By creating mini batches and sending to SQS, processing of different tables becomes asynchronous to each other. Cluster of workers reads the items from SQS queue and processes them independently. Workers are spark executors, such that each executor receives an item from SQS and processes it. Processing an item basically means reading the files included in the mini batch in parallel, converting the json records to avro and sending the avro records to DanaDB client (figure 4). Because the records have consistent operation_timestamp field, we can safely process the files in parallel. To be able to write data to DanaDB, it is required to create the table beforehand. DanaDB keeps metadata about the table like the schema of the table, key columns, partition columns and number of partitions. DanaDB client library partitions, sorts, deduplicates and writes records to S3 as parquet format (figure 5). Firstly, DanaDB client gets partition number of the record by applying a hash function (murmur3) to the partition column. Partition column by default is the same as the key column, but in some cases it is preferable to use a different column. After finding the partition number, the record is sent to the corresponding in-memory partition writer. In-memory partition writer keeps the records sorted by key and operation timestamp. We use ConcurrentSkipListMap for in memory store. To reduce the memory usage and to keep as many records in memory as possible, the avro records are stored as serialised form — as byte array. As it is seen in figure 6, in-memory store is basically serialised avro records sorted by key (timestamp column omitted in the picture). From the memory store the data is flushed to S3 in parquet format, sorted by key (figure 7). While records are written to S3, two new fields are added to the records — rowid and version (file_id). Rowid is sequence number and version is a uuid which is same for all records in a file. Usage of rowid and version will be explained later in the post. We only talk about writing the change log data so far. Well, how the readers will see only the latest version of the records (figure 8)? Because we only append new files to the S3 table location, we need to find the latest version of the records as efficiently as possible. So, let’s review what we have so far: Parquet files sorted by key A key in a file is unique Each record in a file has unique rowid. All records in a file has same file_id, which is unique among data files. One idea is to create an index file which will keep key/pointer relationships in a separate file. But because some of our tables has more than a billion records, the size of the index will also be quite big. To keep the flag indication whether a record in a file is latest or not we use bitmap array structure. Thus, for each file there is a separate boolean array which contains an element for each rowid in the file (figure 9). Because we don’t keep the keys in the bitmap file, the size of the bitmap is very tiny (a few KBs) even for very big tables. The small size of the bitmap file makes it possible for query engines to easily load it and keep it in memory. Moreover, updating and overwriting the bitmap file is very fast and efficient. We have data files (parquet) and bitmap files. Now, we need to combine them to get the latest version of the records. Users query the tables by views (figure 10) created on top of data and bitmap files. But how to keep the bitmap file sync with the data files? It is responsibility of DanaDB server. The higher view of the DanaDB architecture is shown in figure 10. The advantage of this architecture is that writers (workers) and readers (Athena, Presto, Hive) do not need a database in the middle. Writers and readers use their own system resources to do their respective jobs. DanaDB server has two responsibilities: repairing bitmap file and running compaction jobs. Because the number of versions of records will keep increasing with time, the size of the data will increase accordingly. Thus, we need to maintain the files. To keep the number of files and number of versions of the records under control, DanaDB server runs compaction jobs periodically. To understand how bitmap file is repaired and compaction applied, first of all we need to understand how to read latest version of records. Because the data files are already sorted by key, we need to apply merge part of the sort-merge join . I will show an example of the compaction process. Lets assume we have two data files, and we need to compact these files into one file. We read one record from each file (figure 12), compare the key of the records and write the record with smaller key to output. The record from the file on the left has smaller key, so it is sent to output, and we jump to the next record in that file (figure 13). The record has smaller key than the record on the right, so it is sent to output. Then, we jump again to the next record in the left file. But, in that case the keys are equal. Thus, we compare the timestamp of the records. Record with bigger timestamp means newer version of the record, so it is written to output, and we skip the other one (figure 14). In this way, we read the latest versions of the records from two files, create a new file and drop the two files (figure 15). Please note that there is only one array in the bitmap file and all elements of the array are “True”. Because files are sorted by key, merging sorted data sets requires constant memory, which make it possible to run massively parallel jobs with limited cluster resources. To repair the bitmap, data files are read in the same way as we read for compaction, but with one very small difference. Because we are only interested in the columns — key, timestamp, rowid and version — we don’t need to read whole files. Moreover, because our data files are in columnar format, by reading just these 4 columns we skip reading a big portion of the data files, which makes the read process much faster. DanaDB re-constructs bitmap files in each repair job. Right now, we replicate 600+ tables using the DanaDB, which means 15 million CRUD operation per hour . We use 20 r4.xlarge nodes to replicate these tables, keep the bitmaps updated and run the compactions. Current latency is ~1 hour, which means changes happening in MySQL are reflected in our tables in S3 in less than 1 hour. We built table/view definitions on top of the replicated MySQL tables in Glue catalog, and stakeholders have all the data ready for their consumption. Users that connects to MySQL replica databases will start querying the tables directly from S3, and access to replica databases will be revoked. That will decrease the pressure on the replicas, and moreover, users can run more complex analysis on top of parquet data. Additionally, big and expensive redshift clusters that are used to run analytic queries will be scaled down , because we will not need to keep lots of historical data in the cluster anymore. We operate a network of online trading platforms in over 40… 706 5 Big Data MySQL S3 Danadb Data Lake 706 claps 706 5 Written by Senior Big Data engineer at OLX Group | https://twitter.com/azsefi We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Big Data engineer at OLX Group | https://twitter.com/azsefi We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-08-12"},
{"website": "Olx", "title": "ejabberd ec2 to openshift", "author": ["Nikhil Sharma"], "link": "https://tech.olx.com/ejabberd-ec2-to-openshift-c16305650446", "abstract": "About OLX Group We are living in an era where Docker has revolutionised the way we build and deploy an application. From developers to team lead, from SysAdmin to Product Manager, everyone’s eager to accelerate the product delivery cycle. Docker came as a solution of shipping your code as an image & thereby ensuring that it runs seamlessly across all computing environments. How do we containerise an application? If we jot down the basic steps, it looks more or less like this. Create a base image as per application requirements. Create a deployment pipeline that deploys latest code using base image. Once final image is ready (with latest code), deploy containers (or pods) in a container orchestration platform. Looks pretty simple, right? What could go wrong with this? This article addresses the journey of containerising Ejabberd Application & deploying Ejabberd Pods on Openshift, and a million things that could & have gone wrong. First step towards the Openshift migration was to containerise our application that was running on EC2. We’ve created a base docker image that downloads and installs Ejabberd from source code. Once we were ready with the docker image, we had launched an Openshift cluster with 3 Openshift Master nodes, 3 ETCd cluster nodes & 3 Infra Nodes. Now that we’ve collected all the ingredients required for migration, all we needed to do is combine them. In order to deploy a HTTP/HTTPS application on Openshift, we needed to follow the steps as mentioned below. Deploy pods on Openshift (pod is the term that Openshift/Kubernetes uses which implies a collection of containers). Create a service and map it to the application pod. (Service Endpoints are like virtual ELB that distribute traffic to pods behind it, like what AWS ELB does). Create a Route mapped to service created above. Map your application DNS to your Openshift Router ELB. Voila, your application is deployed now on Openshift & ready to cater to user requests. However, we couldn’t use the same series of steps for Ejabberd, as Openshift routes only support HTTP/HTTPS & TLS, whereas, Ejabberd “is an XMPP application”. We had to look up for an alternative solution, and after a bit of research we went for NodePort which is one of the few alternatives provided by Openshift. Once Ejabberd is up and running on Openshift, it was time to call in our testing warriors & begin their testing spree. They did everything from functional to stress testing. Surprisingly, the results were a lot better than our expectations. Apart from some minor glitches, all the tests passed smoothly. Moving ahead with the QA sign off, we created a plan for migration to implement on D Day. After weeks of hard work, D Day came, the whole team was excited about migration and pumped up with confidence that nothing could go wrong (all thanks to our stupendous testing results). What did we have to do? Just a DNS switch from our EC2 Servers to Openshift Cluster, and if anything went wrong, it would be a simple rollback strategy (rollback DNS switch). We did a DNS switch early in the morning & started monitoring business metrics (like messages sent, login failed). As per business metrics, everything went smoothly, no hiccups, no glitches. [But if everything would have gone smoothly in the very first attempt, I probably wouldn’t have been writing this blog.] After running smoothly for a few hours, cracks started to appear and as the day progressed, they worsened. Even after going through all rounds of testing prior to migration, we certainly missed something. The question is, what? The moment we started getting traffic, AWS CloudWatch metrics were showing that out of 3 backend instances, 2 were unhealthy. However, in ELB instances tab we could see that all 3 instances were in service. How come all 3 instances are in service, when 2 out of 3 backend instances are unhealthy? After analysis by an AWS support team, they concluded that only one of the ELB instance was able to make connections with all three of our backend instances. Other 2 ELB instance wasn’t able to make connections with backend instances. Since our ELB’s have Cross-Zone Load Balancing enabled, it was showing all instances in service in ELB instances tab. Sounds weird? How come one ELB instance could make connections with all 3 backend instances while other 2 ELB instances could not. Apparently coupling of NodePort with AWS ELB doesn’t work well under high traffic ( at least not with Openshift Version 3.5 ). As a result of which, even after having 3 ELB instances only one was able to make a connection with all 3 backend instances. It was clear that we couldn’t use NodePort , so we’ve explored other alternative solutions and after a bit of research we went with HostPort . Unlike NodePort, HostPort doesn’t acts as a virtual load balancer over pods. Instead it binds your application to a specific port of EC2 instance & let ELB do the load balancing of traffic. As this is a major change, once again we did a whole round of testing. This time we closely monitored the healthy host count metric too. After receiving a green signal from our QA, we planned for another attempt of migration. We had learnt from our failures from last attempt, and it was the time to implement those learnings. We did a DNS switch hoping not to rollback this time, but we don’t always get what we hope for, right? Within a few minutes post DNS switch, we realised that we failed as our application went down and we had to rollback (again). We were scratching our heads wondering why instead of progressing, we failed even quicker than last time. Now to make another attempt, we really had to double crosscheck that we’ve closed all loopholes. Prior to every attempt of migration, we’ve done a thorough load testing, but still we failed. Apparently, we were doing something wrong, but what? We retraced our steps & jotted down list of all the things we missed or did wrong. Simulate Exact User Behaviour: We created a test scenario for load test, however in earlier attempts, we failed to simulate exact user behaviour (we learnt this the hard way). Monitor Everything: We were so focussed on Ejabberd metrics (like messages sent, authentication stats, etc), that we missed certain key metrics of AWS, like Spillover Count, Surge Queue length. Make every packet count: We have to keep a close eye on all the packets flowing, as health of Ejabberd cluster is primarily based on whether all the packets are flowing seamlessly. D-Day 3 After implementing the learnings above in our load testing, we realised that we missed two major things. Ejabberd creates a lot of chatter on the network . It creates too much of network noise even for internode communication. HostPort uses docker bridge network for containers communication. For every user request, Ejabberd is creating so much of network noise that docker bridge is unable to handle it & it starts denying most of the packets, then boom! The application goes down. We did a network packet analysis flowing through docker bridge during a load test on our staging environment to test our hypothesis. We observed around ~ 20% of packet loss even in low traffic. Ejabberd doesn’t like even the thought of sharing its network resources, it demands all of the EC2 network resources to the container running in it. We switched from docker bridge to docker host network, which also limits us to run only one container per EC2 server. These learnings gave us enough confidence to proceed with another attempt. We switched from Openshift HostPort to HostNetwork. We ran one Ejabberd Container per EC2 server & allocated all the underlying host resources to that container We rolled out above changes & did a DNS switch (keeping our fingers crossed). Finally, after two consecutive failures, we got the sweet taste of success in our third attempt. And as they say, third time’s the charm . All this pain, but why? By now you must’ve been wondering, why did we go through all this pain? Why didn’t we let Ejabberd run on EC2? Maturity of a product is directly proportional to maturity of its deployment pipeline. In order to deliver the best user experience, it is necessary for a product to evolve to cater to dynamically changing user requirements. For instance an IT giant that we all know, Amazon, deploys code every 11.6 seconds, while Netflix deploys code thousands of times per day. This isn’t possible without a bulletproof deployment pipeline. At OLX, customer is our highest priority, and to fulfil this obligatory norm we fathom their demands and make changes according to them, as we also learn from users’ feedback. Containerising our application & deploying it on OpenShift gave us a smooth Deployment Pipeline, which gives us the luxury to deploy code at ease. This migration didn’t only accelerate our development Cycle, it also helped in reducing the number of outages. We operate a network of online trading platforms in over 40… 368 3 Ejabberd Openshift Docker Containers Containerization 368 claps 368 3 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-10-01"},
{"website": "Olx", "title": "how our mobile team does ci cd for sdks to save time", "author": ["Remerico Cruz"], "link": "https://tech.olx.com/how-our-mobile-team-does-ci-cd-for-sdks-to-save-time-5811239986f7", "abstract": "About OLX Group Our team makes SDKs for all the teams to use. One of the challenges of our work is to make our SDK generation and distribution more efficient and more reliable. This was achieved by applying continuous integration in our code. We moved from different tools to using gitlab for our pipelines. Gitlab was used since all the team’s repositories were moved from GitHub to it. Currently, we are maintaining 9 different SDKs across two platforms (5 on Android and 4 on iOS), and maintaining all of their pipelines is no small task. Pipeline Before Initially, releasing a new version of our SDKs involves the following manual tasks: Create a new Git tag for the release Edit the CHANGELOG.md in the project repository Press the ‘Publish’ button in our Gitlab CI Send an email to Google Groups account about the new release, including the list of changes Edit Confluence wiki with the latest SDK version Move all Jira tickets included in the release to ‘Done’ We release a new version every week or every other week, meaning that doing these set of tasks easily consume a significant amount of our time. Also, as our project pipelines have individual code instances, synchronizing changes in the pipeline was very tedious and error-prone. Pipeline After After making changes to automate more things in our pipeline, we also moved to a centralised repository for the common scripts that we are using (e.g. sending a message to slack, getting the code coverage, etc.). Centralised Scripts Move all our scripts in one repository Use Git submodules Reducing the redundant code and synchronizing them is one of the problems we wanted to solve. Using git submodules is one of the ways to synchronize changes across your repositories although it has some limitations. It only updates to the revision that was specified when you committed it. We manually specified with a fetch+rebase command on the submodule folder to solve this. Changelog Automation To automate our changelog, the pipeline scripts depend on four data sources: Project version For iOS, the project version is stored inside the podspec file: For Android, it is stored as a set of variables inside the gradle.properties file: We use a Python script that will automatically parse these and acquire the project version based on the platform. 2. Git commit messages / branch names In order to associate Jira tickets with our codebase, we follow a convention of naming Git branches based on Jira ticket IDs. This allows the pipeline script to see which Jira tickets are committed and merged by querying git log . 3. Jira ticket title Each Jira ticket will be included as an entry in the changelog file. Therefore, it is important to name the tickets with a descriptive title. 4. Latest Git tag/release The script will use the last git tag as the starting point where it will look for commit logs up until the latest commit. It is therefore important to generate a new git tag after each new release. Using these data, the script will perform the following operations: Fetch all git commit messages after the latest created git tag. The script will assume that these commits have not been released yet. Parse each commit message using Python and look for matching Jira ticket ID pattern using regular expression. From the sample git log above, the script will find two matching Jira tickets, PROJ-1557 and PROJ-1558 . Using the acquired set of ticket IDs, fetch the corresponding ticket titles from Jira via API . The ticket title will be used as the entry in the changelog. Now that we have our list of Jira tickets and their respective titles, we can now send our email! To make formatting easier, we use an HTML template with all the variables inside curly braces. Feel free to customise your own email template, as well as add more variables: We will now read the template file and substitute the variables into actual data: After we’ve generated our email JSON payload, we will use a service called Zapier to send emails via HTTP webhook: Now that we’ve successfully sent out our changelog email, we have a couple more tasks to do, including updating our Changelog markdown and finally creating a new Git tag. Insert the list of Jira tickets into the CHANGELOG.md file via Python script and push the changes via Gitlab API: The script above will insert the following entry into the CHANGELOG.md file: We’re almost done! We can finally create a new git tag using the project version: Now that we’re finished with our release, we can now move our Jira tickets to ‘Done’: Summary Automating our release related tasks can save us a lot of energy when releasing SDKs. Here are some of the important points that we covered. Things that take up our time are manual but can be automated: tagging releases, sending release, emails, updating the changelog, moving tickets to done Automated solutions for these include using common scripts that are reusable for all our projects Standardized build yml files make it easier to sync changes across our projects Acknowledgements to our wonderful Mobile team Ayelen Chavez , Cristiano Madeira and JB Lorenzo for making this effort possible. We operate a network of online trading platforms in over 40… 294 1 Git Ci Deployment Deployment Pipelines Deployment Automation 294 claps 294 1 Written by Software Developer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Software Developer at OLX Group We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-10-02"},
{"website": "Olx", "title": "tableau how to moving axis label from bottom to top", "author": ["Marija Lukic"], "link": "https://tech.olx.com/tableau-how-to-moving-axis-label-from-bottom-to-top-65ba627cfa53", "abstract": "About OLX Group Tableau is business intelligence software that helps people see and understand their data. Everyone who is analyzing data knows it; it is a de-facto industry standard. However, sometimes it is not very obvious how to do something quite simple in it. For example, sometimes you want to customize labels for axis or headers, move it up or down or even hide titles in Tableau. Let’s see how to go about it! When creating new worksheet in Tableau, fields are placed on the rows and columns. If the field is discrete, a header for the field is created. If the field is continuous, an axis is created. In this article, moving of axis title will be tackled, and you best ally is Dual axis. This example is using a simple data set of daily volume for messages that were sent to users. I have the following KPI’s on dashboard: Number of messages sent in last 24 hour ( #Sent ) 2. Weekly movement — number of messages sent in last 24 hour compare with same time period week before ( % Weekly Movement Sent ) Steps to follow: Add two other container measures, % Weekly movement container next to the first % Weekly movement container , and another # Sent Container next to the first # Sent Container . 2. I will format them the same way that I formatted the first two containers. 3. Click on the second measure and check Dual axis . Now I have titles on the top and bottom of the chart. 4. Click on measure title and go to edit axis. On a Tick Marks tab put ticks to none . Change the title on the General tab. Set an empty title for bottom measures and a real title for top measures. Do this for every measure. Final result looks like the picture below. Process is repeated for %Weekly Movement Open rate and % Open rate columns . Now it is done and you have your title on top! We operate a network of online trading platforms in over 40… 70 2 Tableau Data Visualization 70 claps 70 2 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-06"},
{"website": "Olx", "title": "olx chat platform part 1", "author": ["Satish Chandra"], "link": "https://tech.olx.com/olx-chat-platform-part-1-47d80bf49dc2", "abstract": "About OLX Group The perception about OLX is that it is just a classified platform while the rightful way of looking at our business is through the definition of a marketplace. Communication plays a major role in a marketplace not just as a way of trading but also establishing trust between the trading parties, where issues like privacy and user safety might crop up. The Buyer and Seller communicate multiple times via different channels. While the traditional channels like Call and SMS are helpful, they also come up with additional constraints like user privacy. Not many would know but User to User real-time chat is the most commonly used communication channel in every region/country where we have enabled this worldwide. “Real-time” chat has become a de-facto standard in the chat systems. Systems like Whatsapp has set new benchmarks in user experience while providing nearly 100% uptime. The days of the inbox as a method for chat are long gone, and users are accustomed to receiving their responses quickly in real time conversation just like real world. In this blog series I will take you through the journey of how we built, the challenges we faced, the solutions and the lessons we learned while we tried to scale our real-time chat service for OLX. Let us start the series by explaining some recurrent terms. XMPP stands for eXtensive Messaging and Presence Protocol. It is a communication protocol based on XML. It is a comprehensive chat protocol which covers nearly all aspects of the chat systems. It enables the near-real-time exchange of structured yet extensible data between any two or more network entities. Deriving its base from XML, it was designed to be extensible from the basics. It covers semantics for Real-time messaging Relay of presence information Contact list maintenance (also called rosters) Ejabberd is a battle-tested chat server implementing XMPP which was written in Erlang by Alexey Shchepin in 2006 and since has seen numerous deployments and customizations by some major Tech companies like Whatsapp, Blizzard (League of Legends), Facebook etc. Ejabberd is highly scalable, extensible and overall a marvelous piece of software written. Ejabberd is extensible via modules, which can provide support for additional capabilities which it accomplishes via hooks. In addition, modules can provide support for extensions of the XMPP protocol, such as MUC , HTTP polling, Publish-Subscribe, and gathering statistics via XMPP. Erlang is a functional language invented at Ericsson. It was built to be used as a language for telephone switches. Even now 90% of the Internet traffic is routed through Erlang nodes. From the ground up it’s build to be fault tolerant. All the requirements for a highly loaded chat system like distribution, fault tolerance, soft real-time and high availability is baked into the language itself. It comes pre-bundled with a framework which makes writing scalable systems extremely easy. Erlang is easily one of the best language for highly concurrent systems like a chat-server. Ejabberd server works in a cluster and is connected to the SQL server using Amazon’s RDS service. We also use Redis for session management. This architecture worked best for low traffic, since each of the components like MySQL and Redis are proven technologies. A typical message might go through following events to name a few: Message sent by a user process Metric collected for the sent message for our monitoring systems Send the message details to our analytics service Store message for future retrieval by the user Send notification if the user is offline Send the message to receivers process This works quite good but also creates a lot of bottlenecks. The entire chat server gets dependent on a lot of external services adding up extra latency. The Problems We support multi-device authentication and therefore our users are free to access their accounts from any of their devices. This poses a big problem for us because every time the users log in to a new device they have to fetch all the messages using the archive call. This creates a lot of contention on the network between both the Cluster and SQL. Also we need to do some processing on the received messages, like analytics and building our notification pipeline. Degradation in the performance of MySQL or the external notification system might lead to situations where the system might get stressed because of the increase in queue length of processing of such messages. A lot of chat activity like contact list management was dependent series of DB transactions. These transactions were necessary for ACID guarantees, but they had their own overhead. Frequently we would see our SQL server reaching up to 80% of CPU usage. It became imperative for us to separate these concerns due to these above-mentioned problems. We devised a new architecture for taking all these extra steps from the message broker and just focus on what it does best i.e. the transfer of messages from one user to another. The problems we faced led us to decide a new architecture for augmenting Ejabberd’s functionality. We tried to offload all the secondary activities, which did not require soft real time guarantees to a separate system. We built a secondary consumer service with Nodejs backed by Cassandra as a data source for processing messages. We decided to use Kafka as a data bus, through which Nodejs would consume the messages published to Kafka. It would then take all the secondary steps like sending notification for the offline messages, sending a message to our analytics service, storing messages for later retrieval etc. This separation was a huge win for us. Storage Improvements We were able to ditch our message storage backend from MySQL to Cassandra. MySQL was performing good at low scale, but later it had become a bottleneck due to a huge number of writes and reads being processed by a single MySQL server. Cassandra provided high availability and eventual consistency out of the box. It also has a very high write performance as compared to SQL, which suited our use case really well. I/O Improvements We also offloaded retrieval of messages to our HTTP API’s. This gave us two advantages: low I/O in Ejabberd nodes. the guarantee of high availability, since reading could be done from each of the Cassandra nodes. Suffice to say message retrieval would never again be a bottleneck in our systems. Astute readers might see that the images have a lot of differences namely Redis cluster and MySQL master/slave. I would elaborate on these in the next blog post. We wanted to decouple our systems for specific needs. We decided to let the message brokering system excel at what it does best and offloaded all the other service calls to our background NodeJs consumers. As a result we arrived at a completely different architecture then what we started with. This was also a huge win in our uptime, because we could easily add other functionalities without disrupting our chat systems. This also allowed us to scale the chat infrastructure independently depending on the demand of services. This was an introductory blog. In the coming blogs, I would describe the further problems that we faced during scaling of the Ejabberd service and how we solved each one of them. Next in Series — The Scaled Step! We operate a network of online trading platforms in over 40… 320 4 Marketplaces Chat Scaling Technology 320 claps 320 4 Written by Software Developer@OLX/Naspers Classifieds, Scaling chat systems We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Software Developer@OLX/Naspers Classifieds, Scaling chat systems We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-08"},
{"website": "Olx", "title": "gdpr not just an opportunity to be gdpr compliant", "author": ["Joseph Boston"], "link": "https://tech.olx.com/gdpr-not-just-an-opportunity-to-be-gdpr-compliant-cc7ef7377ba0", "abstract": "About OLX Group In 2016, the EU adopted the GDPR replacing the 1995 Data Protection directive. Apart from GPDR being a large chunk of work that organisations have to do to be legally compliant, it also emphasizes an opportunity to think a little bit about how your users react to change in data protection regulations. So how did we, as a team, approach a common GDPR compliancy issue? Well, of course there are many ways… In the next paragraph, I will address a small example of how GDPR compliancy affects your users experience. In our team we use a simple “5 whys” approach to think about why a user might want to unsubscribe from a message (ability to unsubscribe from messages which are not directly related to your core service is a GDPR compliancy requirement): We started from the end user perspective. 1. ‘I do not want to receive messages’ allowed us to dig deeper into the most sensitive case for our user, and after all the whys the flow ended with the outcome of a user not wanting to receive the messages that 4. ‘do not relate to what they are experiencing’ and also user being 5. ‘not always able to look at messages on their phone’ . By the fifth ‘why’ we had came to the conclusion that this change we were making towards our GDPR efforts was not just about user data protection and privacy but also about giving our users a relevant experience and choice of how they wish to receive the message. Here is the end result: So in short , remember this: GDPR compliancy is not just a check box but also a wake-up call to start thinking about how you can give your users a relevant experience by letting them choose how (and whether at all) they receive their messages. To read more about this: GDPR EDPS elected, Toyota’s five whys , EDPS . We operate a network of online trading platforms in over 40… 16 Thanks to Maryna Cherniavska . Privacy Gdpr Eu Gdpr 16 claps 16 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-07-06"},
{"website": "Olx", "title": "tableau trend indicators up or down arrows on particular columns", "author": ["Marija Lukic"], "link": "https://tech.olx.com/tableau-trend-indicators-up-or-down-arrows-on-particular-columns-c02e92875a68", "abstract": "About OLX Group It’s very common for business users to want to see KPIs and trends in the same view. These give them a sense for the overall direction of their product and also highlight the most meaningful numbers to them. One of the most powerful trend indicators is a coloured triangle or arrow pointing up or down next to a number. Tableau can only add them to all column in a view. But what to do if we don't want them next to every column? I often see people solve this by creating separate worksheets in Tableau, but with this post, I’m going to show you how to combine them into a single view. This example is using a simple data set of daily volume for messages that were sent to users. I have the following KPI’s on dashboard: 1. Number of messages sent in the last 24 hours (#Sent) 2. Weekly movement — number of messages sent in the last 24 hours compared to the same time period one week before (% Weekly Movement Sent) 3. Number of opened messages compared with a number of sent messages (% Open Rate) 4. Weekly movement — open rate in the last 24 hours compared with the same time period one week before (% Weekly Movement Open Rate) This technique is also commonly used for Year over Year (YOY) and Month over Month (MOM) trend. In this article I will explain how to create the table above, isolate measures and put arrow indicators next to them. Very important thing to emphasise here is: It’s not a table, it’ a chart! Here’s the final solution, with details on how to create the view above. Steps: Select the Rows shelf and add a add country name. 2. Create three calculated fields, one for sent messages for the last 24 hours (I am reading this from data source) — field name is #Sent, one for messages sent 7 days ago — field name is #7 days ago Sent. And the third field is weekly movement (% Weekly Movement Sent). Remember that I am comparing the number of messages sent in the last 24 hours with the same time period 7 days ago. Business request is to know whether we are sending more or less messages comparing with the same time frame from the previous week. 3. Measures that will be shown on report are % Weekly movement sent and number of sent messages. 4. Create fake “container “ measure (with value 1), this measure will be a placeholder. I will edit the shape and colour on container measures, and put real measure value as text. 5. Place container measure on the Columns shelf. Use Minimum as an aggregate function. 6. On Marks card change the chart type from Automatic to Shape . 7. Create a calculated field that will indicate whether to use an up or down arrow. 8. Add this new calculation (Arrows Sent) on a shape mark card on your “container “ measure and edit the shapes. 9. Add this new calculation (Arrows Sent) on a colour mark card on your “container “ measure and edit the colours. 10. Add original value (calculated field % Weekly Movement Sent) as text on Label Mark card. 11. Now I have a chart that look like this one. Next I will add real value for sent in the Container for sent value. 12. On Marks card change the chart type from Automatic to Text . 13. Add original value (calculated field # Sent) as text on Text Marks card. I now have a table with two measures and only one of them has an indicator arrow next to it. We can further tidy this view up a bit by setting the column titles on top and removing the tick marks, as described in a previous How-To . Now we have the view that matches our requirements! We operate a network of online trading platforms in over 40… 121 2 Data Visualization Tableau 121 claps 121 2 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-08-10"},
{"website": "Olx", "title": "new in town how to change code without fear", "author": ["Facundo Viale"], "link": "https://tech.olx.com/new-in-town-how-to-change-code-without-fear-14579cd991c0", "abstract": "About OLX Group As a Software Engineer I hear all the time things like “I don’t want to touch that code, because I don’t know if I’m going to break something else”. Sometimes this is because the code is very complex or very old or making a refactor is a bigger risk than a benefit. Funny thing is, this happens even if the application has a reasonable amount of tests. This is a very common situation, especially when you join a new company. Everything is new, there most likely are some legacy projects, projects that really need a refactor, or just very big ones. People, like me, want to start coding as soon as possible, making changes without a full understanding of the entirety of the project. I’m very confident of my abilities, but I always prefer to trust something more than my gut feeling. If the project has tests, awesome, but how do you know those tests have good quality? Luckily, there’s a way… Was originally proposed by Richard Lipton as a student in 1971. The first implementation of a mutation testing tool was by Timothy Budd as part of his PhD work (titled Mutation Analysis) in 1980 from Yale University. Mutation testing is another testing tool, like coverage, but instead of checking what lines are covered by the test, it checks how well the tests are covered. To do this the tool runs the tests over and over again, but in each run different types of errors are seeded into your code (mutations). If your tests fail then the mutation is killed, if they pass then the mutation lived. The quality of your tests can be gauged from the percentage of mutations killed. The idea is that every test should be strong enough to fail to any unexpected behavior, state or error. Mutation testing is based on a hypothesis called the coupling effect. The coupling effect asserts that simple faults can cascade or couple to form other emergent faults. Here is an example: With the a “Conditionals Boundary Mutator”, the mutator replaces the relational operators <, <=, >, >=. The code will be mutated to: If your tests aren’t checking the boundaries between a and b , the tests are going to pass and the mutation will live. This tell us, if someone makes a change in the conditions of the if (because the engineer didn’t understand the logic behind the code), they are going to introduce a bug that no test is catching. If a 100 percent of the mutations were killed, we can safely say that we can make any change in the code or refactor knowing that any potential bug we could introduce is going to be catched by the tests. This depends heavily on the implementation, there isn’t a single tool for any language. In most of the cases the tool requires to have their own language interpreter or re-write the AST to change how the code behaves. Here are some of the implementations: JS — Stryker Java — Pitest PHP — Infection Python — Cosmic Ray Scala — scalamu Rust — mutagen Because not all platforms are equal, not all available mutations are in them. Some of my favourite mutations are: Null returns Mutator: Replaces return values with null. This is not very useful in language like Scala or Rust, but is really useful in JS, Java, etc … Languages that heavily rely in null checking or null isn’t catched by the type checking. Empty returns Mutator: Replaces return values with an ‘empty’ value. For example: an empty string in case of a string or an empty array in case of an array. Void Method Call Mutator: This mutator removes method calls to void methods. For example: will be change to: In functional programming languages this is forbidden because it’s pure side-effect. But in some languages is a common practice because there are initialization methods, background tasks or some kind of logging/tracing methods. Mutation testing is a very useful tool. It’s non intrusive and most of the time it’s just a plugin or cli command, so you don’t need to do something to your code for it to support mutation testing. There is a huge benefit in using mutation testing because it gives us information about the quality of our tests. This is very useful for the health of the application and for everyone new in the team as a way to give some robustness to the code. In my personal experience, it’s very rare to find a project with 100 percent score in mutation testing. I’m usually very confident working in a project with an 80 percent score or more. One with lower than 50 percent could present risk and I strongly suggest writing more tests or trying to cover more cases before making any changes to the code. Checking the score and improving it should be always part of the testing acceptance, like with coverage. We operate a network of online trading platforms in over 40… 681 JavaScript Java Scala Rust Unit Testing 681 claps 681 Written by Follower of the blasphemous Scala programing language and the dark arts of JavaScript We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Follower of the blasphemous Scala programing language and the dark arts of JavaScript We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-05-29"},
{"website": "Olx", "title": "predicting and understanding advertisement liquidity in classifieds", "author": ["Andreas Merentitis"], "link": "https://tech.olx.com/predicting-and-understanding-advertisement-liquidity-in-classifieds-d65f60e4a158", "abstract": "About OLX Group A platform like OLX that connects buyers and sellers in more than 40 countries faces many challenges that are to some extent similar but also somewhat different to online retail. One of these challenges has to do with the user experience when navigating the platform and the recommendations shown to them, the results when doing searches, etc. In a typical retail setting the stock is relatively stable, while in our case every item offered for sale is potentially a new one and we have many millions of them at any time, which exacerbates the cold start and long tail problems faced by any recommendation system. As part of the solution for a good user navigation and browsing experience, it is useful to have a good estimate if a specific advertisement has been already sold so that we don’t show it again in the recommendation or search output. This is a probabilistic time-series prediction problem. Another important aspect connected to the previous case is identifying what is that makes some advertisements much more liquid (easy to sell) than others. For this particular case, understanding how the model is making decisions is really important as the outcome can be provided to the sellers in order to improve the liquidity of their advertisements. For the reminder of we will focus on this specific liquidity prediction problem, predicting if an item is sold 15 days after its entry in the system, and we will use XGboost and eli5 for modelling and explaining the predictions respectively. XGboost is a well known library for “boosting”, the process of iteratively adding models in an ensemble of models that target the remaining error (pseudo-residuals). These “week learners” are simple models and are only good at dealing with specific parts of the problem space on their own, but can significantly reduce bias while controlling variance (giving a good model in the process) due to the iterative fitting approach followed in constructing this type of ensemble. The data we have available for this problem include textual data (the title and textual description of the original advertisement, plus any chat interactions of the seller and potential buyers), as well as categorical and numeric data (the category of the advertisement, the brand and model of the item, the price, number buyers/sellers interactions for each day after the entry, etc.). The data sample we are using here is a relatively small part of the data from some countries and categories only, so in many of its properties it is not representative of the entire item collection. Nevertheless, let’s start with some basic data munging. The histogram of the day that an item was sold is shown above. We can easily see that most items are sold in the first days after the respective advertisement is placed, but there are still significant sales happening a month later as well. With respect to the day an advertisement is added to the platform, we can see that there is a peak on weekends, but other days are roughly at the same level. Finally, with respect to the hour an advertisement is add to the platform, we can see in the figure below that there is a peak around lunch time, and a second peak after work hours. One way to capture more complicated relations is to use the pairplot functionality of the seaborn library. In this case we will get the combinations of scatterplots for the selected columns, while in the primary diagonal we can plot something different, like the respective univariate distributions. We can see that the number of buyers interaction in the first day is a strong predictor if an item will be sold early or late. We can also see that category id is very important predictor as well, as some categories in general tend to be much more liquid than others. Now that we are done with the basic data munging we can proceed to make a model, using the XGboost library. Using a hyperparameter optimization framework we can find out the hyperparameters that work best for this data. Since we are interested also on the output confidence of the prediction itself, it is typically a good idea to use a value for min_child_weight that is equal or larger than 10 (given that we don’t loose in predictive performance) as the probabilities will tend to be more calibrated. The ranking of the features from the XGboost model is a first step in understanding what information the model in using in order to make decisions. Although feature ranking from tree ensembles can be biased (favoring for example continuous or categorical features with many levels over binary or categorical features with few levels) and in addition if features are highly correlated the effect can be split between them in non-uniform way, this is already a good indication for many purposes. However, a feature ranking is a property of the model and the dataset, while we want to understand how decisions are made at the level of each individual sample. For many types of model (including tree ensembles like the one we used here) this is possible by using dedicated libraries that explain what the model does at prediction time. Using eli5 we get an explanation of how this instance was handled internally by the model, together with the most features that where the most important positive and negative influences for this specific sample. Just for better understanding, L1 is the top level category of the product (used for grouping similar products), num_seller_message_day is the number of seller messages sent till a given day, and number_buyers_day is the total number of buyers expressing interest till the given day. The way we interpret the output is that for this this particular instance (ad) the features that played the prominent role in deciding if it was liquid or not are listed here in order of decreasing contribution. As we can see the sample was classified as being not liquid and there was some pull down from the text properties (description length, etc), the activity of the seller (perhaps he did not respond to the buyers?) as well as the price of the item, which we can use to provide guidance to the seller for improving the advertisement. Special thanks to Christian Martinez, Akash Gupta, and Carmine Paolino, for developing and improving different versions of the core liquidity prediction model. We operate a network of online trading platforms in over 40… 330 1 Machine Learning Olx Advertisement Online Advertising 330 claps 330 1 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-02-15"},
{"website": "Olx", "title": "chat for classifieds really", "author": ["Praful Poddar"], "link": "https://tech.olx.com/chat-for-classifieds-really-99ed6d6d6f0d", "abstract": "About OLX Group The title largely summarizes the interest level some time back within OLX when we started to think of chat. These were the obvious industry trends telling us to make chat part of a classifieds experience: customers need of a messaging platform and comfort with using messaging apps day-day protecting customer privacy by hiding phone numbers having a communication medium which can be asynchronous, and stays on record Our critics said: classified users don’t have the time to open chats & respond it will be too much effort users eventually have to talk and meet in person so what is the need why reinvent the wheel, make them use WhatsApp etc. Chat has been a challenging area where we have been making a couple of strides, and this article shares a bit on our journey thus far, although it is not anywhere close to complete. As the technology & product teams at OLX, we are constantly aiming to build industry benchmarks experiences that help buyers & sellers succeed on our platform. This is what chat is aiming to do too. Could it? We tried to answer this with 3 simple questions… This question was simpler to answer, but that does not mean it was something easy to develop. Bear in mind this was the first time we were introducing another communication medium on our platforms. But the answer came quickly and in black & white when we launched Chat on our India App a year back. We saw the numbers go something like this: Pretty great huh! Just to make sure it was value adding we checked our liquidity trends. (Liquidity is the measure of how many sellers are successful on our platform). This is what we saw: It worked to give us our first answer, so… Customer feedback, both positive & negative, came swiftly after our launch: Customers appreciated chat, since its a non-intrusive, privacy protecting, asynchronous & familiar way of communicating Even though some are more familiar with WhatsApp, some users would prefer to keep discussions related to OLX on OLX itself There is still a lot of effort for selling / buying Not to mention that for OLX, chat makes sure our customers stay on OLX (increasing our DAU/MAU) and gives us so much information into the buyer/seller conversation… All of which can be leveraged to identify further customer pain points. This led us to our next question. It was beginning to get exciting now! We wanted to make the chat experience custom to classifieds and add value for our customers. One strategy that never fails is to understand what our users want. That is the fundamental way that we approach product development, which is illustrated below: Taking a step by step approach with the above strategy in mind, we set out to answer the 2nd question. Illustrating a few examples of some things we did: Calls & chats Post our chat launch we saw that users chat & call interchangeably and often with the same person. But the communication streams were totally disconnected. As a customer I will never know who called for my ad on OLX (unless of course I save the contact and track it in my phone call list). Picture our John Doe being in the situation below John wants to to track all his communications easily in one place so he can follow up easily to close deals. He also wants an attach an identity to the flashing number. So to help John, we went implemented a way for him to know all the folks he has interacted with whether through call or chat within the OLX app. So when someone now calls, it will look like this. This is has been live for a while now and we are seeing a lot of satisfied John Does on our platform. Customers are starting conversations on call and then moving to follow up on chat, which indicates that this feature makes it easier for them to track trades. 2. Price discussions One other thing John engages a lot in is price related discussions. This is a core part of most classified deal discussions. So why not make it easier for John to do this, which is why we made the ‘Make an offer’ flow you see illustrated below. This lets John now share his offer price with one tap. 3. Messaging platform expectations: While we were customizing the experience to classifieds as much as possible, we also wanted to make sure we are not making John relearn a lot. Simultaneously we also needed to fulfil his expectations from a messaging platform. Couple of examples in this area were: Search to make it intuitive for customers to find conversations they were looking for Voice messages to enable our emerging market customers who sees voice as a growing need, not to mention the high usage of Whatsapp voice messages by our users All of this had given us our 2nd answer With all the stuff done earlier, the one clear win for us was that chat started to become the preferred way of communication replacing direct calling, confirming that tuning it to classifieds helped deliver user value. We now wanted to get our hands dirty with something more complex, and which is why we picked a hard third question. We started with our fundamental strategy — let’s ask our customers. One big need for John that we identified — ‘Please lessen the effort required to close deals’. And another one — ‘Sometimes I am not sure how to proceed.’ Before jumping into execution we strengthened our solution concepts with: User testing: Conducted in 3 countries in parallel. Here is a quick pic showcasing one of them. This gave us a bunch of findings which we incorporated to our solutions. Data inputs: To understand opportunity areas. Here’ an example our analysis and modeling exercise. This was aimed to understand the key topics that users talk about as the conversation progresses. The results are illustrated below: As intuitive as it might sound, lots of conversations start with negotiations. Propensity to discuss meeting also increases as conversations mature. (Note that we have masked the topics to maintain data confidentiality). Using this we can help users more contextually. Now combining inputs for our user need, user testing & data analysis we have built, what we call internally, Intelli-Chat — an intelligent engine that helps users contextually close deals faster. Here are some of the cool things Intelli-chat does: Makes it easier for buyers & sellers to start & respond to conversations Help sellers in in ads management. (This works well for us as well, as more unavailable ads are removed from the platform, the more chances of buyer success) Inform customers of important updates The first version of Intelli-chat is Live in our South Africa market and we are seeing some awesome results — more than 50% of users adopt to the suggestions that we show, and majority of their messages are being sent through these suggestions vs. being typed. Question 3 answered! With this ‘much more…’ we already thinking our next question(s): Can we make the experience better for our business users? Can we increase trust, reduce fraud through chat? How can we make the experience more intuitive for different categories These are just some of the initial areas we are beginning to think of. Stay tuned to get more updates on our subsequent phases to enable seamless buy-sell transactions on OLX. We operate a network of online trading platforms in over 40… 152 1 Messaging Chat Marketplaces Product Management User Experience 152 claps 152 1 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-04-03"},
{"website": "Olx", "title": "microservices an organisational architecture and a journey part i", "author": ["André Aleixo"], "link": "https://tech.olx.com/microservices-an-organisational-architecture-and-a-journey-part-i-42f32457783a", "abstract": "About OLX Group Microservices Architecture has became one of the newest trendy words in tech. Specially in tech giants and, for quite a while now, in every single conference that I’ve attended, I’ve heard the word on and on which means that there are many companies adopting the concept. After extensively reading several authors, people like Sam Newman , Martin Fowler or Peter Bourgon (who I think addresses this quite pragmatically), after hearing dozens of talks, reading a bunch of opinionated articles from a bunch of people and actually starting to implement those in the company that I work for, I now have a genuine understanding of what a microservices architecture is and most important, what it is for. I hope that by the end of this article you see the benefits of using them, as I did over the past months. Such an architecture is a clear evolution of several others that brings small new pieces to the puzzle. The nature of problems that this architecture will address is extremely important. As I see it, a microservice architecture is meant to solve organisational problems. Applying it will generate a lot of technical problems . Believe me, it will. As I often say, technology does not fix problems. People do. So we need to cater for that. Arrange teams and set mindsets around this idea. In the end, technology will serve as a toolset to do the job but first, someone needs to identify what that is. What I often see is people assuming that the size of their teams are perfectly ok. People just need to keep releasing things throughout a development process. What this architecture aims for is to mitigate certain risks. Things like blockers that get bigger and bigger as teams grows. The more people work in a single monolithic codebase the more blockers are likely to come up, which then translate to communication and alignment overhead. Long story short, it slows down release capacity. People can perceive this as a “normal” thing to happen to an organisation just because it’s growing. Teams are growing, so it’s normal to have more meetings to align on objectives and dependencies. As I see it, that is totally wrong. You need to adapt (change your organisation from left to right) fast enough to keep your delivery capacity as high as possible. Now, being fast and continuously delivering is something that companies like Spotify, Netflix or Soundcloud (considered the pioneers of microservices architecture) are really good at. That is what we, foreign people to the process, can perceive. You can attend to talks and hear them talking about results — which are great by the way — but ultimately you’re not there during the transformation process and their own journey . That’s at the core of applying such as architecture and you need to walk your walk. Be inspired by others but walk your walk. Test, iterate and fix it. What in fact is a microservice architecture? After studying this topic for a while, I think that there are three main domains we can use to classify microservices architecture: Size, Data and Operations. I don’t mean the number of lines of code to create a new service, neither the number of Github repositories, not even the number of packages that I could use. It’s about the size of the team that supports it. As Martin Fowler stated: Team of half-a-dozen would support half-a-dozen services. Here you can see it happening already. The very first thing to assess about how to use microservices is to see how you have your teams structured. How big are they and what kind of enablement do you have in place to support a fast delivery cycle. This does not have much to do with technology. Remember what we talked about? Technology does not fix problems. People do. So it is in people that you should focus. Actionable conclusion about Sizing : Think carefully about how small your teams should be and what is a good ownership pattern for your teams/services, who (teams) owns what (services), so no one gets blocked and everyone can collaborate effectively. Knowing why you’re building software is important. Understanding your use cases is also important. We can describe microservices architecture as the art of knowing your problem domain. It’s easy. You can’t find a solution to a problem if you only know part of it . In order to see the whole you need to deep dive into your monolith problem domain(s). Iterate and identify your bounded contexts and separate model representations. In other words, each bounded context should have its own logical storage. Avoiding a shared database is halfway through getting microservices right. As Sam Newman said: A microservice implements a single bounded context Actionable conclusion about Data : Identify seams in your code (classes, namespaces, packages, etc.) and in your shared database (tables, constraint keys, etc.). Suggestion would be to use Domain Driven Design while assessing your use cases to help you identify the transactional boundaries. People often start addressing microservices architecture from a technology perspective and that has to do with fair concerns around what needs to be run and where at a given point in time. This means that each team, owning and supporting a dozen of services, should design, implement and deliver to a production environment without any kind of dependencies . If you can’t achieve this, you can’t achieve a microservices architecture. Actionable conclusion about Operation : Invest in having proper automation pipelines, so that every single artefact can be considered as a release candidate that can reach production without any drawbacks. Also invest time to consider effective service ownership and contribution models between teams/services. Successful microservice architecture transformations are very much focused on people and organisations and not only on the latest technologies for backend services. Assessing your organisation is essential to decide. Once you’re convinced and you want to start the journey, don’t look back. This will not be your silver bullet but can be a hell of a cannonball to move forward. It will take time and energy, but it will happen. We operate a network of online trading platforms in over 40… 212 Microservices Architecture Software Architecture Microservice Architecture Adopting Microservices 212 claps 212 Written by Portuguese Dad, webaholic, Head of Architecture at OLX We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Portuguese Dad, webaholic, Head of Architecture at OLX We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-10-09"},
{"website": "Olx", "title": "beautiful mind maps", "author": ["Maryna Cherniavska"], "link": "https://tech.olx.com/beautiful-mind-maps-9c4382025c52", "abstract": "About OLX Group This post is about an instrument allowing your mind to become better organized. And therefore hopefully more beautiful. This instrument is called mind maps . (Disclaimer: This is a rewrite of the article on my blog which was written two years ago but wasn’t ever published on Medium, which is why I wanted to share it here, but improved and extended.) When I heard the term mind maps before it sounded complicated. When I googled the term and looked at the images, they were even more complicated. But once explained by someone who used them in practice, mind maps turned out to be rather simple and useful. How can mind maps be used in software development? Well, they can help you eat the elephant piece by piece . For example, you can split the task into subtasks, and them into more subtasks, and then mark the progress on each small part as you go. You can drill down into the tasks as you go, expand them further and add/delete what comes to mind, essentially making notes that will not be lost and that are more or less structured. One might say, wait, but we got JIRA. It has tasks, and subtasks, and estimates, and time spent, and time left. It’s got it all! And here is where one would be both right and wrong. JIRA will do all that, but it won’t allow you to see the whole picture. You’ll have to drill down into tasks to see what’s left and how much of it. After the second jump you’d probably be hopelessly lost. Mind maps, however, allow you to get into as much or as little detail as you want, and all the picture would still be obvious at a glance! Let us start with a real-life-like example. Suppose you want to add some file uploading and sharing capabilities to your web application. You might start with something like this: This map has more questions than answers and most of the actions are just sketched, not planned in detail. You always know which part of the task you are on, though, because you put a red person icon on it. You also know which parts are done (and green), which are not (the red ones), and which should be investigated further (question marks). There’s many more markers and additions that can be used: assignees, estimated and actual dates etc. — you might be planning not just for yourself, but for the team working on the same task. Usually I don’t use too many icons though, because if I do, it becomes increasingly difficult to read the map. So I just have a limited set of most-used icons. I also sometimes use a minus icon — for the things that were planned initially but didn’t make a cut and were decided to go to the “won’t fix” or “won’t implement” group. I am unwilling to just delete these from the picture, because one might save some time at a later stage and decide to include one of those outliers after all. As the task progresses, the picture becomes more detailed: You can delete the leaf parts from the branches that are completely green, if you find that the diagram becomes big and unruly. Another good thing about the mind maps is, they allow to make quick notes about the feature — you might mark them with the “information” or “attention”, whatever fits best. When you come to implementing these features, you might expand the notes into their own full-fledged branches after you make decisions about them, or just leave them as is if no actions are needed there. So, the mind maps turned out to be pretty simple after all, and in my experience very useful. They help you see the current state of things as a whole and always know where you are right now. At the very least, they are worth a try. And it just shows that one doesn’t have to be afraid of complicated terms. They might not be so scary after all. Originally published at mchernyavska.wordpress.com on May 31, 2015. We operate a network of online trading platforms in over 40… 24 1 Productivity Olx Software Engineering Software Development Mindmap 24 claps 24 1 Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-25"},
{"website": "Olx", "title": "olx payments powered by kotlin", "author": ["Benjamin Schneider"], "link": "https://tech.olx.com/olx-payments-powered-by-kotlin-95eb77bafacb", "abstract": "About OLX Group At our Tech Hub in Berlin, we build best-in-class shared services that are used by OLX apps and websites around the globe. One of the more recent services is our Global Payments Service aka Plutus. Payments is tricky business. OLX does operate in many countries, in which credit cards are not a very common payment instrument. Instead we are looking at SMS payments, cash deposits at the nearest gas station or some very niche online payment methods, that most people have never heard of. But even beyond enabling our customers to pay, there are many more obstacles to overcome when you want to accept payments as a business. You have to be able to offer refunds , need to know when exactly the generated revenue should be written into the books, should have risk checks in place, to reduce losses and fraud. And wherever you are offering card payments, you have to do a lot of effort on your infrastructue, processes and application security side to be compliant with the so called PCI Data Security Standard . Given that huge complexity, we wanted to relieve every of our apps and websites from the burdens of implementing payments themselves, and rather encapsulate all of that inside an easy-to-use service. When we started thinking about the technology stack for Plutus, one thing we were certain about from the beginning was that we need a robust and battle-proven foundation. After all, no one likes to lose money and our customers trust us that we handle their financial data according to the highest standards. This quickly led us to the Java Virtual Machine — a platform that we felt was very mature, offered tons of enterprise tools and frameworks and we already had very good experiences with in Berlin. At the same time, looking at Java , we felt that it just has not aged very well. Even with the latest features of version 8 and now the upcoming 9, it can’t be considered a modern language. It is still clunky to use, very verbose, functional programming concepts are concealed behind an inconvenient Stream API and the famous NullPointerException hides behind every corner… We though it is about time for something new, something fresh . But we still wanted to build on top of the JVM . Call us insane, or maybe brave. In the end it was probably something in between, but we decided to give Kotlin a try. For those of you who have not heard of Kotlin yet — and they are getting fewer by the minute — Kotlin is a rather new programming language by JetBrains , the creators of IntelliJ IDEA and other awesome products. We made this decision before Google announced to offically support Kotlin as an Android programming language , so we did not have quite the confidence we got today back then. But what was convincing was the language itself. It offers some very well thought-through features, like built-in null-safety or immutable collections . Or the type inference , that makes you write way less code which in turn enables Kotlin to be as expressive as Java can never be. Functional programming is a breeze, you can call map() or filter() directly on a collection, without going through the Stream API. Lambdas are very well integrated and Kotlin comes with a fantastic set of standard library functions that makes every-day programming tasks so enjoyable! Interestingly, even experienced Java engineers who came into our team and spent a bit of time with Kotlin, stated after just a couple of weeks that they prefer Kotlin over Java and are likely to stick with Kotlin, if they so can. What better approval could we have been asking for when we started this great experiment? In the end — besides the language — we don’t do much different to how other JVM applications work. We don’t use that many Kotlin specific libraries but instead use proven Java frameworks, and Kotlin enables us to seamlessly call their APIs without any additional effort. Kotlin even made us better programmers. Remember how I mentioned earlier that all types in Kotlin are non-nullable by default? That means if you see a String type in your code, you can be sure that it will never be null . In fact, you will get a compiler error if you ever try to put null in there. If you really want a nullable type, you have to append a ? to the type, so String becomes String? and is now nullable. Likewise the compiler will give you an error if you try to dereference a String? without explicitly verifying that it is not null . Now whenever we see ? s in our code, we treat it as a code smell, we realise that we should be refactoring those bits to not rely on nullables at all (unless in some very special cases, where we consider it to be a valid scenario). In the end, Kotlin has proven itself to be a really good choice for us. We really do enjoy programming with it and we firmly believe that happy coders generate better code. After all, we spend 8+ hours every day in front of our computer screens, why not having a bit of fun while we are at it? We’d strongly recommend every JVM developer to check out Kotlin. Play a bit around with it , you don’t need to go as far as we have and using it in production right away. But even if you choose to, Kotlin interoperates so well with Java, you can just start with one class or maybe a unit test, and then gradually move on in your own speed. And after all, we — the Plutus team — hope to be able to build on that foundation and deliver great and secure payment experiences to you! We operate a network of online trading platforms in over 40… 123 3 Kotlin JVM Software Development 123 claps 123 3 Written by Head of Payments Engineering at OLX We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Head of Payments Engineering at OLX We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-05"},
{"website": "Olx", "title": "create your own aws rabbitmq cluster the dubizzle way", "author": ["Ibrahim AbdelFattah"], "link": "https://tech.olx.com/create-your-own-aws-rabbitmq-cluster-the-dubizzle-way-b4c632cb47d6", "abstract": "About OLX Group A group of software engineers and SREs have been recently assigned the task of creating a stable RabbitMQ cluster running on production in dubizzle. We heavily rely on celery, and with RabbitMQ-based-docker-running-images failing almost every week — especially over weekends — we decided to take some time, do some research and come up with a healthy robust running cluster to support the massive number of tasks held by celery. A brief description of RabbitMQ, as per pivotal’s definition of the system — pivotal is the company that owns RabbitMQ now — RabbitMQ is the most widely deployed open source message broker. It has been used by most of the internet giants for different purposes. At dubizzle, and in our journey into microservices, we have chosen RabbitMQ to be our communication bus over other alternatives like Kafka / Kinesis , VerneMQ and even considering at sometime writing our own. Because of the amazing flexibility RabbitMQ provides and that once deployed you should never consider revisiting except for some minor maintenance tasks to be executed. It also comes with a lot of out of the box features that would facilitate working with it on a daily basis like the nice Admin interface and a RESTful API. RabbitMQ also support lots of programming language with wide variety of protocols, clients and libraries. We rely on AWS for almost 99% of our services either internal or exposed. Other than some third party integrations, We are considered as one of the largest accounts in the region working with Amazon. We used to have a docker based RabbitMQ cluster using some open source images from docker hub, which for some reasons failed to serve our needs. Facing lots of down times every now and then. That affected us badly. So, instead of relying on docker images, we decided to go on bare EC2 machines implementing the RMQ cluster from scratch. After a lot of investigation in the features that comes with RabbitMQ plugins, we decided to implement our own cluster using EC2 launch configurations — using CloudConfig -, Auto-Scaling groups and RMQ Auto-clustering plugin which comes with AWS support. So, by end of this blog post, we should have a working RabbitMQ Cluster of 3 nodes on AWS with fault tolerance and auto clustering of new nodes if any of the old nodes fail. On the following link GitHub Gist , we will keep the code that we will be using as a launch configuration which will then be used for the Auto Scaling group — later to be addressed in the post — on AWS. The Gist has comments that explains almost every step along the way, so please feel free to leave a comment asking about any part that might need more explanation. You should change the password values on lines 71 and 76 to something secure. On your AWS Account, go to EC2, from the left sidebar menu, click on Launch Configuration then click on Create launch configuration , the following screen should appear, from which choose the AMI that might suit you, for this tutorial, we will be working with Ubuntu Server and the latest LTS supported version at the time of writing this blog post it’s 16.04. So, beside the distro entry, click Select . On the next screen, you will have to choose the instance type based on the load RabbitMQ is expected to handle. We have tested multiple versions with this, but finally we chose t2.medium considering load will be moderate and consumers should always be running. For the sake of this tutorial, we will keep the default chosen option which is t2.micro. Then click Next: Configure details in the bottom. To continue to the next step, we need to setup a new IAM role for the cluster, which will allow communication between RabbitMQ cluster nodes. For this tutorial, we will be calling it rabbitmq-cluster-iam-role , with the following policy: Launch configuration details: ** Assign the name of the launch configuration in the next screen, for now we will call it RabbitMQ Cluster . ** In the IAM role , we will choose the IAM role — we created from the previous step rabbitmq-cluster-iam-role — from the select box. ** You can also allow CloudWatch monitoring on that form. ** Click on Advanced Details and a new form will appear on the same page in which you will paste the script we linked for earlier in the User data text box. ** In the IP Address Type, we will choose not to have public IP addresses for the cluster instances as we will be adding a load balancer for that cluster which can be used to access the cluster either publicly or privately. ** Click Next: Add Storage RabbitMQ writes messages to disk if they were not consumed after some period of time from memory, so unless you change the configuration to keep messages in memory or have consumers always running, you will need somehow a good amount of storage for the nodes. Depending on what you would favor, choose the amount that you feel would be appropriate. We will be choosing 10 GB. Then click on Next: Configure Security Group You will experience failures in the cluster if messages filled up your disk space that’s why it’s always good to make sure your consumers are healthy and consuming messages in a steady manner. For this step, we will be creating a security group which will allow communication between the RabbitMQ cluster nodes for clustering and sync. And also for us to connect to any node separately or through the load balancer via console or UI interface. As per RabbitMQ’s documentation we will need to allow the following ports: ** 22 : for ssh connections ** 4369 : epmd , a peer discovery service used by RabbitMQ nodes and CLI tools ** 5672 : used by AMQP 0–9–1 and 1.0 clients without and with TLS ** 15672 : HTTP API clients and rabbitmqadmin (only if the management plugin is enabled) ** 25672 : used by Erlang distribution for inter-node and CLI tools communication and is allocated from a dynamic range (limited to a single port by default, computed as AMQP port + 20000). Note that that this step is based on your AWS configuration, but mainly you will have to enable communication from only subnets that should access these nodes through the mentioned ports. That’s why how you create security groups is not covered in this tutorial. Click Review Review your launch configuration details and if everything looks fine, just press Create launch configuration from the bottom of the page. A new popup will now be shown from which you can choose which key pair you will need to use to connect to these instances. Either create a new key pair or use a pre-created one. Make sure you have saved it somewhere secure and then press Create launch configuration. Now, we will create a new auto scaling group for the recently created launch configuration. We will press on Create an Auto Scaling group using this launch configuration , the following window will appear: We add the group name, set the number of instance to 3 in the Group size field and choose the network and subnet, again this is totally based on your AWS account configuration. Press on Configure Tags from above, in there we will add one tag which will set the node name when it’s being created to easily identify and manage the nodes from EC2 later on. Then click on Review. On the review window, if everything is ok, click on Create Auto Scaling group button in the bottom of the page. You should see the following window. Now, let’s create a load balancer which will then be used to access the cluster. From the left menu, click on Load Balancers, then click on the Create Load Balancer button. Choose the Classic Load Balancer and click Continue. Choose a load balancer name, for now we will be using RabbitMQ Cluster LB , choose which VPC the load balancer should reside in and based on if you want it to be a public on private load balancer, check the Create an internal load balancer checkbox. Make sure you add the listening ports 5672, 15672 as per the following screenshot. Choose subnets based on the VPC and Assign Security Groups This step is to choose the security group for the load balancer, which might be totally different than the security group of the nodes created earlier. Make sure for which security group you will be choosing, it’s allowing access to the load balancer via the ports 5672, and 15672. On the Configure Health Check Tab, you can define the health check and how is it performed. Default settings should be fine. Keep clicking next till you reach the Review page and click Create . Now we need to attach our load balancer to the auto scaling group, so from the left menu, click on auto scaling group and choose the RabbitMQ Cluster ASG entry then Click on Edit . In the load balancer field, search for and select the newly create rabbitmq-cluster-lb. Then click Save . Now we should have a running RabbitMQ AWS Based Cluster with a Load Balancer balancing between all its nodes. Based on if you chosen the load balancer to be public or private go directly to its URL or tunnel to it, you should be prompted to login, you can use the admin:admin as username:password from the launch configuration code from the github gist. After login, you should see your cluster nodes healthy, up and running. Go on, try deleting one of the nodes from EC2 and see a new one almost instantly coming up and joining the cluster. When we first tested this with one of our VPCs on AWS, we had a problem with name resolution on the cluster nodes, we got in touch with the AWS Team and they notified us that name resolution of non standard TLD will not work in VPCs created before October 2016. So after a couple of hours, trying to debug this, and after we created the ticket with AWS we had to create a new VPC that would allow nodes to resolve each other. We also found another problem with non-standard private IP ranges with clusters created before October 2016, if so, make sure you have a standard IP range within your VPC. This article was originally published on dubizzle’s tech blog (Boiler Room) , We will keep updating this part on the way, we know that enhances can be added, so keep checking this section for updates, and for sure let us know if you tried that and found any problems so we can enhance this guide. We operate a network of online trading platforms in over 40… 82 3 AWS Olx Rabbitmq Software Development Infrastructure 82 claps 82 3 Written by Another World Change seeker, with great motivation and a lot of gonna come true dreams! I usually write about technology, development and software engineering.. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Another World Change seeker, with great motivation and a lot of gonna come true dreams! I usually write about technology, development and software engineering.. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-18"},
{"website": "Olx", "title": "developers ecosystem in 2017 jetbrains survey", "author": ["Maryna Cherniavska"], "link": "https://tech.olx.com/developers-ecosystem-in-2017-jetbrains-survey-2e0e76b717", "abstract": "About OLX Group Recently JetBrains conducted an extended survey about the developers ecosystem, that is, what languages/frameworks the developers are using, in which companies they work (by size), what is the demographical situation etc. The survey obviously mainly covers the users of JetBrains products, so it is not the whole dev ecosystem, but it is interesting still. Such surveys usually help to understand which technologies are currently on the rise and which maybe deserve more attention that you gave them. This survey, however, lacks a few things. First, dynamics: it doesn’t have any previous results. How can you see the trends if you don’t have anything to compare with the current figures? Second, I for one would be really interested to see what is the dynamics for the JVM languages has been — it is obvious for example that Kotlin is actively promoted by JetBrains , it was seen in a previous annual report for the 2016. But how does it affect Java? What happens with Clojure and Scala in the meantime? etc. However, there’s still a lot of info in that report, and there are some key takeaways . As to programming languages, no surprises here. Java and JavaScript still take the lead, and probably will for quite some time yet. Python, however, is close and the potential adoption rate is actually highest (12% are going to take it up), whereas only 5% of the developers are going to go into Java. This too is no surprise though because of the still increasing popularity of Data Science and Machine Learning, where Python takes the lead. One of the interesting black horses there might be the Go language , whose intended adoption rate exceeds its current popularity. So, it might be worth a look if you want to learn something new and interesting. It is even questionable whether it can be considered a black horse at all, what with being developed by a Google team and having quite an impressive list of users . Clojure looks more like an outsider here, with a 2% current and 2% planned adoption rate. However, from my personal (and admittedly biased) experience, Clojure is a language of geniuses… well, that or really, really smart people. So, if you aspire to be one of them, try to wrap your mind around it. It might be a daunting task, but those who turn to that particular dark side are usually people who have the potential to rise high in the world of programming. OK, let us look in more detail at one more thing that is very important: testing . Turns out that 55% of the developers write unit tests, yay! However, 16% use them but don’t write them. Which is, to tell the truth, a little confusing. I wonder how it works, exactly. Do they perhaps have their own special people on the team who do the “dirty work”? One has to wonder. However, what is more alarming is the fact that 29% of the developers do not use or write unit tests. Like, zero unit tests. Whaaaat? Really? You should, people. Please make sure that you do, by next year. It is an important part of Continuous Integration, as Martin Fowler will explain . There’s much more information in that JetBrains report and I wouldn’t want to stop on everything, or it would be a really long article. Just go and read the original! However, there’s one more thing that made me a little apprehensive: the demographics . And this time I am not talking about the gender , I am talking about the age. The majority of developers are still young (under 30), which is a little depressing (for me as a non-junior), but that is also not new. However, we see that 33% of developers are between 30 and 40 years old, and then after 40, the percentage drops to 9%. And that is, I confess, a little scary, especially as I myself am moving to the wrong end of that scale. What happens to those 30-something developers after the big 40, pray? Do they fall off the edge of the earth? Do they make a million dollars (or a few) and happily retire? (Now, that would be a comforting thought… if only I could believe it.) Are they all fired because they can’t fit into the hipster vibe any longer? (Whaaat? You remember the floppy disks? OK thx bye grandpa…) Or do they suddenly all become managers and leads and stop coding?… The survey doesn’t really give an answer to that question. (Come on, JetBrains, you know you really could do better there.) I suppose I’ll have to find out for myself at some point… but I am warning you, whoever wants me out of there just because I turn that scary age, they are going to have to drag me out of there kicking and screaming. Because that is the path I chose for myself, once. And I am walking it. As indeed we all are, some slower, some faster, some running, some crawling. And some are flying. We operate a network of online trading platforms in over 40… 13 Jetbrains Framework Programming Java 13 claps 13 Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior Java Engineer at Elastic (You Know, For Search) We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-09-05"},
{"website": "Olx", "title": "paparazzo open source media picker for ios apps part i", "author": ["Andrey Yutkin"], "link": "https://tech.olx.com/paparazzo-open-source-media-picker-for-ios-apps-part-i-c22c95a2c9fc", "abstract": "About OLX Group A while ago we were faced with the task of radically redesigning the process of publishing ads via Avito iOS app. The result had to be a tool that would make this process quick and convenient for the user. Obviously, the buyer prefers to see what he is going to pay for. Therefore, giving the seller the capability to easily add and edit photos was one of our top priorities. Here you can read how we achieved this. Running a bit ahead of the story, our media picker is called Paparazzo and it’s available on Github . In this article, we’ll discuss the technical details of capturing camera image and displaying it in several UIViews at the same time. The simplest way to implement camera in iOS app is to use UIImagePickerController. Its functionality is quite restricted, that imposes certain limitations: ● It can operate either in camera mode or in gallery mode, while we needed a hybrid mode. ● It has to be presented modally, because it’s actually a UINavigationController that’s not allowed to be embedded in another UINavigationController, while our camera should’ve been one of the steps in a linear sequence of screens. ● UIImagePickerController produces UIImage, which is an uncompressed version of an image stored in memory and takes up a lot of space. When working with UIImagePickerController in my previous projects, I’ve noticed that on slow devices like iPhone 4 (and sometimes even iPhone 5) an app sometimes crashed due to memory warning even before delegate method that returned image to a client code was called. We wanted to avoid inefficient use of RAM in order to prevent such problems. We also explored some of the available open source solutions, but they fell short of our requirements. Some lacked the necessary features, such as image crop or rotate, others allowed selecting only one photo, without supporting a filmstrip showing selected photos. All in all, the user flow implemented in those components did not suit us. Therefore, we decided to implement our own camera from scratch, so that we could make quick improvements in the future. Another way to implement camera is to use the low-level framework called AVFoundation. It allows to get the most out of the photo, video, and audio recording and playback capabilities offered by iOS. The central object in AVFoundation is AVCaptureSession, which coordinates the flow of data from a capture source to a client. In order to do something useful, though, it needs some inputs that actually provide data to the session. Inputs are represented by instances of AVCaptureDeviceInput, and they originate from AVCaptureDevice, which is a physical device, such as the front camera, rear camera, or microphone. In addition to the media input, we also have to define the output. For this purpose, there are one or more AVCaptureOutputs on the other side of AVCaptureSession. Examples of outputs include AVCaptureStillImageOutput (for photos) and AVCaptureMovieFileOutput (for videos). Each output can receive its content from one or more inputs (for example, AVCaptureMovieFileOutput can receive both video from a camera and audio from the microphone). Inputs and outputs are connected by one or more AVCaptureConnection instances, so if, for example, we need to record video without audio, we wouldn’t establish a connection between AVCaptureMovieFileOutput and the microphone input. Configuring AVCaptureSession is quite easy. First, we need to get a list of devices that support video capture. After that, we find the rear camera, checking the value of the ‘position’ parameter of each device. Finally, we initialize AVCaptureDeviceInput passing it the camera object as a parameter. Setting up an output is even easier: we simply create AVCaptureStillImageOutput and set the codec to be used during the capture. Now that we have both the input and output, we can proceed to creating AVCaptureSession ‘sessionPreset’ property allows you to set quality, bitrate and other output parameters. Apple provides 14 default presets which are suitable for the majority of common tasks. However if you’re not satisfied with the result, you can set specific properties on the AVCaptureDevice instance representing the physical capture device itself. Before adding inputs and outputs to the session, you must check whether it’s possible to do so by calling canAddInput and canAddOutput methods, otherwise the app may crash. Then we call startRunning() on the session to start transferring data from inputs to outputs. Note that startRunning() is a blocking call, and its execution may take some time, so it’s recommended that you configure the session in the background to be sure that the main thread is not blocked. AVFoundation also includes AVCaptureVideoPreviewLayer class. It’s a CALayer subclass that’s initialized with an instance of AVCaptureSession and displays the preview from the camera. We need to simply add this layer to the right place, and everything works automagically. What could possibly go wrong? However, in our case it turned out not to be as straightforward as it seemed at first glance. As you have probably noticed, in the lower right corner of the screen, in addition to the icons of the selected photos, there is one more icon with a live preview of the camera, which duplicates the main preview. It was not included in the original design. When it appeared as a result of further development of the UI, we thought that it was just a matter of adding one more AVCaptureVideoPreviewLayer and linking it to the existing AVCaptureSession. But we were wrong. A single AVCaptureSession can send output to a single AVCaptureVideoPreviewLayer only. Then we tried creating two sessions, with each of them sending its output to its own AVCaptureVideoPreviewLayer. But that did not work either. As soon as the second session starts, the first one is immediately terminated. Further research led us to a solution. In addition to CaptureStillImageOutput, that out initial setup included, we had to add a new output of class AVCaptureVideoDataOutput. This output has a delegate, which, in turn, has a method that provides us with each camera frame, making it possible to do anything with it, including manual rendering. The output provides frames in the form of CMSampleBuffer objects. Data represented by this object can be efficiently rendered by a GPU using OpenGL as well as Apple’s own low-level framework Metal, that was introduced recently. According to Apple, Metal can be up to 10x faster than OpenGL ES, but it’s supported only on iPhone 5s and newer devices. So we settled on OpenGL. In order to manually render camera frames you must implement captureOutput(_:didOutputSampleBuffer:from:) method of AVCaptureVideoDataOutputSampleBufferDelegate protocol: A bit later you will see that almost all the rendering is performed by Core Image. However Core Image cannot operate directly on CMSampleBuffer object, it needs CVImageBuffer, so we transform one object into another. Note that at this stage we check ‘isInBackground’ flag. No OpenGL calls can be made when the app is in the background, otherwise the system will immediately terminate it. I’ll show how to avoid this later, but for now just remember this flag. Then we iterate the views which will be displaying the preview, and pass them the resulting imageBuffer. All these views are instances of our class GLKViewSubclass. This class, as you have probably guessed by name, is a GLKView subclass. Like any other GLKView, this view is initialized with an OpenGL ES context. What sets it apart from the others is a Core Image context, which will perform the rendering of image buffer’s content. The implementation of the draw(_ :) method is shown below. ‘drawableBounds’ method simply converts CGRect specified in points to a pixel CGRect, because Core Image deals with pixels and does not know whether the display is Retina or not. ‘sourceRect’ method returns a rectangle corresponding to a fragment of the frame being displayed that fits into the view, given its aspect ratio. In other words, if the frame has an aspect ratio of 3:4, but the view is a square, this method will return a central part of the frame. As I said, OpenGL calls cannot be made when the app is in the background. To guarantee this, we need to handle UIApplicationWillResignActive and UIApplicationDidBecomeActive events. Both notifications are sent on the main thread, but messages from the AVCaptureSession delegate are delivered on a background thread. To ensure that no rendering occurs after the first handler has been executed, you need to synchronously switch to the delegate’s queue, call glFinish(), and set ‘isInBackground’ flag, that the output delegate will check before frame rendering. The second event should also be handled on the delegate’s thread, but this can be done asynchronously, because in this case the delay is not critical and will not have any negative consequences. To sum it all up, the general implementation is as follows: CaptureSession sends the camera frame to VideoDataOutput, which forwards it to its delegate, and the delegate, in turn, sends the data to the respective views, unless the app is in the background, and finally, the views render the frame themselves. Now the problem of displaying the camera preview is solved. In this article, we have discussed using AVFoundation to capture the image from the camera and displaying it in several UIViews simultaneously. In the forthcoming article we will focus on the abstraction, which we introduced to handle photos from different sources (such as disk, network or user’s photo library) identically. We operate a network of online trading platforms in over 40… 15 1 iOS Olx Mobile App Development Mobile App Developers 15 claps 15 1 Written by Senior iOS developer @ Avito We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Senior iOS developer @ Avito We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-05-24"},
{"website": "Olx", "title": "generic approach to ios ui tests in swift", "author": ["Krzysztof Kempa"], "link": "https://tech.olx.com/generic-approach-to-ios-ui-tests-in-swift-f32cd77d10ef", "abstract": "About OLX Group UI Tests are a crucial element in the process of assuring quality in a big project. Testing the product as a black box reflects, to a certain extent, the app end-user’s behaviour. We are not testing individual methods, classes or interfaces but the product as a whole. Such an approach allows us to pinpoint errors that are difficult to find at a first glance and that result from an incorrect cooperation between respective components of the application. Even though the tests are not a part of the production code, it is sure worth taking a closer look at their design. It allows you to quickly react to the dynamically changing requirements, especially if the app in question is made of tens of thousands of different views. We use the popular Page-based design where Page instances are the abstract of respective views in the application. If you use Swift it is a good idea to switch from inheritance to Protocol Oriented Programming so that instead of creating Page subclasses you accomplish the same by using protocol and deliver the default implementation in its extension. Each Page instance needs a unique identifier and a BaseTest parameter in constructor. BaseTest derives from KIFTestCase class because we use the KIF framework in our tests. It should be emphasized that it is an implicitly unwrapped optional type test. We chose it because there is no point in setting any default value in init() , as every Page needs to have access to test instance. This is the only way that gives access to the tester object. If we want to deliver default implementation of a parameter constructor we need to make sure that Page has a constructor without parameters and its empty implementation needs to be explicitly added in every struct/class that conforms to protocol. This stems from the fact that the compiler needs to be sure that all properties of the object that conform to protocol have already been initialized. The default implementation of the protocol includes the constructor. In the constructor we always call a method waiting for the view. This means that each time you create a Page instance, you wait until the view shows up in the application. Thanks to that solution it isn’t necessary to explicitly check whether or not the view showed up every time you create a Page instance. Property tester is only a helper in order to shorten the access route to the tester object from BaseTest class. Below you will find the implementation of the user profile view — the AccountPage . We implement property pageIdentifier — same identifier should be set as accessibilityLabel for view in the appropriate view controller. We add store property test and empty implementation of init() method. Open() method executes tap on the last tab of the TabBar and opens the appropriate tab. Logout() method taps log out and waits until the log in button shows up on the screen. In the original version of our tests Page instances had open() method. It included a set of instructions that referred to a given view in the application. This approach turned out to be quite restricted in terms of usage. Let’s use the log in screen as an example. As you can see, there is more than one way to access the log in screen. How do we implement open() method of the LoginPage ? It’s impossible to do universally because the tester does not control which login screen will be displayed. It’s not problematic as long as you want to test the functionality of logging in itself, but when the test case has to log in the user in a specific context, ex. while posting an ad, it becomes an issue. Page instances should include only the methods, which define actions possible to execute in a given view or transitions to other views. In such a case you will need to introduce an additional object that would be the abstract of the main application controller (in our case that would be TabBar controller) called Application and remove open() method from Page protocol. I will now demonstrate how the application log in test would look like if we used the previously described approach. Below you see application screens showing up one by one in our scenario. These are respectively: Application , AccountPage , LoginOptionsPage , LoginWithEmailPage . The body of testLogin() method could be the following Everything works smoothly, but it seems obvious that there is a lot of unnecessary code being written. So what I decided to do was to bring an interesting concept to life — I chained multiple method calls together. In order to do that I assumed that each action has to return some kind of a Page instance. OpenLoginOptions() method returns LoginOptionsPage . Parallely: openLoginWithEmail() method returns LoginWithEmailPage but fillEmailField() and fillPasswordField() methods return LoginWithEmailPage because they do not cause transition to a different view. We also need to add openAccount() method to the Application that returns AccountPage . This kind of refactor makes our test look a lot better. However, the above-described approach has a weak spot. Let’s use submit() method as an example. At first glance you’d say that it should return AccountPage because once logged in the user is referred to that view. But what if you want to create a test that checks whether or not the application works well when incorrect credentials are provided? It cannot return AccountPage type of instance because the user should stay with the LoginWithEmailPage view with additional information about the log in error. We could potentially write a new method — submitWithWrongCredentials() , that returns AccountPage , but we’d soon see that this approach leads to an exponential growth in methods’ number covering all possible options. Same thing applies to Application class. OpenAccount() method always returns AccountPage which is correct only if nobody has ever been referred to a different view in this tab. Summing up, every time the result of an action is difficult to foresee and we are not 100% sure about the following view, we cannot just return a specific type that implements Page protocol. One thing we are certain of is that a Page instance will be returned, but we can only be sure about its specifics when we know what we expect when writing the test. And here Swift comes to save the day with its amazing property of generic programming. Thanks to generic programming we can write a generalized implementation of ex. submit() method, which activates the log in button and then returns Page that we expect to see. Let’s now take a look at the possible shape of submit() method and Application class using generic approach. We can easily create 2 log in tests — one with correct and one with incorrect credentials. The person writing the test has to define the type of generic Page being returned by casting the result of a function call — either using the as operator or assigning the result to an explicitly typed variable. If the method should return a generic Page it may also return a different result from a method returning same generic parameter. Simple, clear and flexible. Developers often refrain from creating tests because they find it dull and monotonous. I wanted to show that this field could be a great area to look for interesting solutions and to simply have fun with the opportunities that Swift provides. It could make test creation fun and easy and help you introduce changes to keep up with the development without giving you a headache. We operate a network of online trading platforms in over 40… 68 1 Swift iOS Ui Testing Mobile App Development 68 claps 68 1 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-27"},
{"website": "Olx", "title": "vertica anchor modeling grow your mycelium", "author": ["Nikolai Golov"], "link": "https://tech.olx.com/vertica-anchor-modeling-grow-your-mycelium-68d623f795e5", "abstract": "About OLX Group Hi. My name is Nikolay Golov, I’m a data warehouse architect of Avito , biggest classified site of Russia, and third largest classified site in the world (after Kraigslist of USA and 58.com of China). 4 years ago, in the middle of 2013, I started implementation of Vertica based Data Warehouse for all existing and possible data of Avito. My Date Warehouse design is based on the Anchor Modeling technique . Back in 2013, this modeling technique choice was, in many respects, a leap of faith. Now, after almost 4 years, I can say that it was a hit. My earlier papers listen quite a number of arguments in favor of Anchor Modeling. Those arguments still apply, though have somewhat paled. Today, the key argument in favor of Anchor Modeling + Vertica is the almost unlimited expansion capabilities. The growth in data volume, velocity, and variety — aka 3V (Volume, Velocity, Variety) — characterizing Big Data can be easily accommodated. Imagine that your data storage is a small mycelium. It starts with a single spore, and then begins to grow, sheathing tree roots meter by meter until you have a multiton monster… that can’t stop growing… NEVER STOP GROWING. Imagine yourself a storage architect. You are fortunate if you can early on estimate the approximate amounts of data, systems to be integrated, data analysis algorithms. Then you can choose the optimal data model and processing platform for your situation. And it is not necessarily the case that Vertica + Anchor Modeling is the optimal solution. Anchor is a noun, a real-world phenomenon. It can be a product, a user, or a payment. Accordingly, every noun should have its own table. An Anchor table must store ONLY the surrogate key (in Vertica, the best key is int ) and several technical fields. Conceptually, we need Anchor for only one task — to upload each unique product/user/payment only once. To avoid repeated uploading and to record when and from which system the original item came. That’s it. To understand how the problem of identifying the product/user/payment is solved, let’s move on to the second entity. Attribute is a table for storing the object’s properties (attributes). Attributes include product name, user’s login and birth date, amount of payment. If the object has only one property, there is only one Attribute table. If the object has ten properties (first name, last name, birth date, gender, registered address, etc.), there are ten Attribute tables. Very straightforward. May be hard to grasp at first, because the sheer number of tables may be scary — but it’s very logical. Each Attribute table stores a surrogate object key, which is reference to the respective Anchor, field for the attribute value, and, optionally, date for historical records and technical fields. Accordingly, the Attribute table storing the name (Name) of the customer (Customer) should be called S_Customer_Name and contain fields Customer_id (surrogate key), Name (attribute value), and Actual_date (date for SC2 historical records). As you can see, the name of the table and the names of all its fields are very clearly defined by their respective content (the customer’s name). What does Vertica offer?… It’s simple: all Attribute tables of the same Anchor must be identical segmented — by the hash of the surrogate key, sorted by the surrogate key and by the historical date. A simple rule, observing which guarantees that all join operations between the Attribute tables of the same Anchor will be MERGE JOINS , i.e. the most efficient join type in Vertica. Similarly, that segmentation guarantees optimal window functions required for handling ETL operations with SC2 historical records on same date. In the previous section, an approach to object identification was announced: a user data string has been received — how can one figure out whether the user already exists in the Anchor or is a new one? Naturally, the answer to this question lies in the attributes. The main advantage of Anchor Modeling is the possibility of using some attributes (full name) first, and then switching to others (full name + taxpayer ID no.). And this takes into account the history. Tie is a table storing relationships between objects. For example, a table storing whether the customer is a citizen of a particular country. Accordingly, the table should contain a surrogate key of the left object (customer_id), of the right object (country_id) and, if necessary, dates of historical records and technical fields. Vertica adds the following feature — the Tie table must be built using two projections: segmented by the left surrogate and segmented by the right surrogate. In that way, at least one of the table’s joins is a MERGE JOIN. An important feature in terms of modeling: Anchor Modeling differs from Data Vault in that Data Vault allows connecting data (satellites) to a link, while in Anchor Modeling, the data (Attribute) can only be connected to the Anchor, not to the Tie (must NEVER be connected to the Tie). This at first glance excessive restriction enables more accurate modeling of the real world. For example, the traditional link to properties in Data Vault is a sales transaction to the customer, where the property is the sales amount. Anchor Modeling makes us think a little and realize that the sales transaction is not a real-world object, but an abstraction. A real-world object is a sales receipt (paper slip) containing number, date, etc. Accordingly, in Anchor Modeling the described example uses three Anchors — Customer, Receipt, Goods; and two Ties — Customer-Receipt and Receipt-Goods. (Here, an attentive reader will notice that even the illustration given in the beginning of this section is not quite appropriate. Citizenship is evidenced by a specific document (passport), and this type of data can be more appropriately modeled using passport as an Anchor). The first time you read about Anchor Modeling, you are scared. Afraid of getting lost in the tables. That fear is justified, but the important thing is not to let it stop you. The extreeme right graph shows the growth rate of the number of tables of each type at Avito over 4 years (top: Anchor + Attribute + Tie total). Recall the first graph in this post — in late 2016, Avito storage contained data from more than 29 source systems. As you can see, this means tons of tables. But not so scary. We can see that there is a surge in the number of tables in the early phase, and then, through the increasing reuse of existing tables, the pace of growth slows down. The spectacular surge in the number of tables in late 2016 was due to an unusually large number of new systems being connected and demonstrates that, despite the size of the system, it is still capable of expansion. The second reason to be afraid of too many tables is the complexities involved in third-party analysis. I will explain how to deal with this concern in my next post. PS. About the Anchor Modeling methodology: if you want to master it, you can try following online course of Lars Ronnback — http://anchor.teachable.com/courses/enrolled/124660 . We operate a network of online trading platforms in over 40… 8 1 Data Warehouse Software Architecture Software Development Database 8 claps 8 1 Written by Data warehouse architect at Avito.ru We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Data warehouse architect at Avito.ru We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-04-13"},
{"website": "Olx", "title": "may the code review be with you", "author": ["Egor Tolstoy"], "link": "https://tech.olx.com/may-the-code-review-be-with-you-3407955e4c19", "abstract": "About OLX Group Code review can be a real pain for a team that begins implementation. You are sure to fall into many traps: review may take more time than coding; location of the brackets can cause deadly holywars; and disputes on whether or not the branch can be merged into the master before the team approves it could last forever. I’ve put together a number of techniques that will help you smoothen the adaptation process quite a bit — at least, they did turn out to be helpful to me. This material is a brief summary of my experience gained through several years of my work in large mobile development teams. For the most part, it relates to mobile development, which had some influence on my examples and references. To begin with, I will outline briefly the goals of the code review. It is these goals and their relative priority that I check before recommending a certain technique. Decreasing the bus factor by ensuring that each component of the code has been seen and analysed by more than one pair of eyes, and the knowledge does not belong to one specific developer only. When several people who are responsible for various work tasks and possess different knowledge about the product look at each other’s code, they can spot certain patterns or suggest reusing one of the pre-existing solutions, or to put the current solution into a separate library. Each team member has some knowledge they can share. Someone is really good at algorithms; someone knows a lot about metaprogramming; and there is also someone who is a real MVC Jedi. Having a chance to look at each other’s solutions and ask questions offers a great opportunity to raise the overall technical level of the team as a whole. The most obvious goal is to detect errors of different severity. This is the first thing your manager thinks about when you suggest that they allocate a certain period of time for code review. Every programmer has their own opinion about how to do coding. Tabs and blanks, file structure of the project — lack of consistency can make code reading and analysis a lot more difficult. Let’s get to business now. Teams can be very different. One company can have thirty people in one department working on one and the same project, while in another, one developer single-handedly supports dozens of applications. The first case is probably the most common one. It is a team with a few members, let’s say, up to four, who all work on the same project. This is where we normally see Gif Flow thriving; the tasks get developed independently in separate branches and then get merged in Develop when complete. The best fit for this case would be a model where one needs the approval of all team members before merging the branch with the task to the main work branch. This gives maximum benefits for all code review goals mentioned in Section 1. Let’s scale up the previous situation and imagine a large outsourcing company having several small teams similar to the one described above, and each of them is working on their own small project. In cases like this, the hierarchical structure usually becomes more complicated. In addition to the lead developers, each team also has a team lead or a tech lead who is responsible for several such projects at once. The basic approach remains unchanged — reviews are conducted for all new tasks and require the entire project team, but the final responsibility for all technical decisions rests with the lead developer. Besides, important reviews must also involve the lead. This way the lead will know what’s going on with the project and step in when his/her help is needed to solve a problem. Another activity can be quite useful. Periodic cross-project reviews when the developer learns about the other projects proved highly efficient. These reviews help everyone be generally aware of the technologies and approaches used, and the areas of expertise of the project participants. There may be different approaches to cross-project reviews. It could be a list of all projects sorted by time, or every developer can inspect other projects periodically. Such reviews are conducted mainly for information purposes, and the approval of an external reviewer is not required to close the task. The person reviewing the code learns from other teams. Another case is a large team with all the members working on one large-scale project. It is reasonable to set a certain limit to the number of reviewers approving a branch to be merged. This number can depend on the size of the team, but 2 to 4 people are usually enough. The big question is how to choose reviewers — they can be selected randomly (whoever responds first) or designated by the review author. This process can be automated in order to avoid disputes and ensure there is no bias. For example, the code owner can be identified based on the history of the revision control system or of the code review system. Single Developer The last case is also the bitterest one. The developer is working alone in the team. First of all, the goal of collective code ownership is no longer applicable — no group, no ownership. The rest of the goals also become rather senseless. However, an external look at the code will help find some errors and increase the technical level in general. In this case, the developer needs to turn to the open community. I can recommend you three channels. 1. Various chats and communities of developers My personal russian favorites are Cocoa Developers Club Slack Chat and iOS Good Talks Telegram Chat . You may also look at iOS Developers HQ for English speaking developers. 2. Meetings of developers For example, Peer Lab that is held all over the world every week. 3. Joint work with a colleague from another function You can involve a colleague who works on development for a different platform — Android, Web or Backend. But be careful — not everyone is prepared or willing to focus on issues from a different development field. The main issue here is the flow state. Pulling a programmer out of the flow state can be quite destructive. Just imagine: you are neck-deep in the development of your feature, building castles of abstractions in your mind, when suddenly your colleague gives you a shy tap on the shoulder asking to take a look at his code. Nothing good can come out of a situation like this — the developer can no longer do their work normally and is too biased and annoyed to review the work of others. A good solution here is a schedule. Each developer in the team decides the time of the day when they are able to review. The schedule of the whole team can be kept in one table, so that one could use it as a guidance when choosing reviewers for themselves or calculating the time required for review of the task. For example, I have always allocated for it an hour of my time in the morning, right when I come in for work — this helped me get into the swing of things. Of course, there are cases when this mechanism fails — some reviews really need to be done as quickly as possible. The most important thing is not to make a habit out of situations like this, but to stick to the routine order. I am sure you came across some situations like this — after the task has been implemented, it turns out it had to be done in an absolutely different way. Code review is not the best place to argue about the global architectural decisions. The code is already finished, the task is almost done, and recoding from zero will be too costly. Such things cannot be postponed; they need to be addressed beforehand. A good option is to conduct an architecture review and defend your component before the team in words or using a diagram. Of course, there are cases when a brilliant idea comes to someone as late as at the point of review. If the suggestion is really valuable, document it in the issue tracker and never return to it. You can invite the project tech lead and developers who share an imminent interest in the implementation of the task, or simply any team member who wants to participate. There is an interesting paradox — the less code is provided for review, the more comments the reviewers leave. This rule remains true for virtually any team. Just a couple of lines of code can provoke broad discussions and heated debates. But as soon as you increase the scope of changes up to several thousands of lines, the number of comments drops to one or two and remains unchanged. This practice is very simple — don’t submit too many changes for review at once. This requirement correlates with the standard rules for the breakdown of tasks — each task should take no longer than one work day to complete. If too much code is submitted at once, the reviewers will lose focus quite easily, and many issues will simply be overlooked. Another requirement would also fit in here nicely — take away all the information noise that distracts the reviewers from the real meaning of changes. The authors can spot the majority of code errors themselves. That’s why the developer should look through the code themselves once or twice before asking colleagues to review their work. This simple procedure will help you save a lot of time and nerve and improve the relationships with your co-workers. I have already mentioned that when you conduct code review on a permanent basis, it becomes difficult to focus on the actual meaning of changes. One way to help reviewers maintain their focus is to provide a detailed description of the meaning of changes. I usually use the following template: Review name Brief description of changes Things that require special attention Linked tasks and materials Check lists are cool. Use them for code review as well. Here are some examples of questions from such a check list: “Is there a code that has been commented-out?”, “Was the business logic covered by any tests?” “Were all the errors processed?”, “Are all the constants in one place?” Try to memorize the questions that the team members ask during the review and put them on the list. Do not let this list grow too long. Think about the ways to automate each checkpoint on your list — given enough efforts, you are most likely to cover the majority of the needs. A great number of tools that can help with this task — linters, static analysis tools, and duplicated code detection tools. Implementation of every process should go hand in hand with collection of metrics describing its efficiency and with analysis of such metrics. The process is not an end in itself. Code review is no exception. It is important to understand to what extent it helps the team to do their work. A good option is to look at the goals of the code review. You can use the ones I listed in the first section as a template. One way to measure efficiency is to use the Goal-Question-Metric approach. It includes three stages — identify the goal you want to measure; list the questions that will help you understand the extent to which the goal has been achieved; and determine the metrics that help find answers to the questions. Let’s consider an example. First, we identify the goal. Say we want to “Increase code reuse.” You can use several goals at a time and measure them independently from each other. Now, let’s develop questions that will help us understand whether or not we are on the way to achieve the goal. In our case, the following questions seem reasonable: “Are we using shared libraries in the project?”, “Did we put the visual styles into a separate component?”, “Is there a lot of duplicated code?” The last thing — let’s determine the metrics that will assist us in answering the questions. In our case, it will be the number of shared libraries, the number of locations with hard-coded styles, and the share of duplicated code. It is quite easy to use this framework. We check the necessary metrics before implementing the process, determine the desired outcome, and once in a while, on a monthly or quarterly basis, repeat all the measurements again. If this can be automated, then it’s even cooler. After some time we analyse the outcome and compare the changes in metrics to judge the efficiency of the process. But why is that metric really good? Imagine that instead of working on suggestions on code reuse developers had spent most of their time engaged in holy wars about blanks and tabs — with such reviews, the key indicators would have been unlikely to increase a lot; the desired outcome would not have been achieved; and we would not be able to understand that there was something wrong with the process. Besides, it’s useful to collect some other metrics that can tell us what problems we faced using the process. Inspection rate — tells us how quickly the review was done, Defect rate — describes the number of defects detected per hour of review Defect density — refers to the number of defects found per code line. If we collect these data for each review, we will be able to adequately assess the level of process engagement for each team member. A number of code review systems — for instance Crucible from Atlassian — automatically collect these data. As I have mentioned in the introduction, this article is a brief summary of my presentation where I also talk about various process automation tools, team regulations and rules of conduct, and some other code review aspects. Examples of decent reviews: Wire , Artsy , Kickstarter . iOS Developers HQ . Peer Lab Official Website and Peer Lab Moscow Telegram Chat . About Goal-Question-Metric We operate a network of online trading platforms in over 40… 127 Software Development Software Code Review 127 claps 127 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-07-14"},
{"website": "Olx", "title": "live mysql schema changes on amazon rds with percona a walkthrough", "author": ["Rafay Aleem"], "link": "https://tech.olx.com/live-mysql-schema-changes-on-amazon-rds-with-percona-a-walkthrough-27e46222f1b4", "abstract": "About OLX Group At dubizzle OLX, we recently reached a roadblock on a project where we had to change schema on around 130 tables in our production database with master/slave replication. The simplest possible solution that could have been was to recreate same tables with new schemas, copy the data and update our application to point to the new ones. This couldn’t have been made possible without a significant downtime for the whole website which wasn’t acceptable. So, we leveraged Percona Toolkit for MySQL to do the job and we would like to share our learnings in this post. Percona Toolkit is a collection of open source command line tools to perform a variety of MySQL database tasks. Reconsider the problem that we had in question and how we could have solved it manually. Recreate around 130 tables with updated schema. Take the website down or put it in read only mode to ensure zero data discrepancy between old and new tables. Copy everything from old tables to the new ones. Rebuild foreign keys. Rename new and old tables or update the app to point to the new ones. Redeploy the website. The other solution was to setup a complex set of triggers, copy/read/write operations and have a rollback plan. All of this sounds very risky to manage on your own without expecting any downtime and this is where Percona Toolkit fits in to do the job for you. Percona Toolkit has a bunch of tools for various tasks. The one we are going to discuss here in this post is pt-online-schema-change . Simply testing the tool on your local machine before doing it on production is no plan at all. You have to have a similar replica of production at your disposal to test how the change would perform. For this reason, we experimented pt-online-schema-change on a production like set up which we call the staging environment. Making sure that each and every parameter/configuration on the staging database is exactly the same as production is a very crucial part of testing how the migration would perform in production. A note about AWS RDS MySQL instances: AWS RDS doesn’t give you SUPER privileges on your MySQL instances so you would have to play around and tweak your DB Parameter Group. The most important parameter in context of this post is the log_bin_trust_function_creators . The default value for this in most of the cases is 0 (disabled). Quoting from MySQL docs here: This variable applies when binary logging is enabled. It controls whether stored function creators can be trusted not to create stored functions that will cause unsafe events to be written to the binary log. If set to 0 (the default), users are not permitted to create or alter stored functions unless they have the SUPER privilege in addition to the CREATE ROUTINE or ALTER ROUTINE privilege. A setting of 0 also enforces the restriction that a function must be declared with the DETERMINISTIC characteristic, or with the READS SQL DATA or NO SQL characteristic. If the variable is set to 1, MySQL does not enforce these restrictions on stored function creation. This variable also applies to trigger creation. Since pt-online-schema-change cannot get SUPER privilege on RDS instances, log_bin_trust_function_creators has to be enabled for trigger creation and other routines. You can read more about it at MySQL docs . Earlier, we discussed how one of the solutions could be to setup a complex set of triggers and copy/read/write operations to perform schema changes. Internally, pt-online-schema-change tool works in pretty much the same way. Lets look at an overview of what it does for that. Creates an empty copy of the old table with the new schema. Creates triggers on the old tables to update corresponding rows in the new ones. Copy all the records from old tables to the new one. Rebuild relationships. Swap old/new tables by doing atomic RENAME TABLE operation. Drop the old tables (default behavior). Note: The operations listed are not necessarily performed in the same order. A typical pt-online-schema-change command for performing schema change would look something like this: Note: Some of the description has been copied verbatim from pt-online-schema-change docs for brevity. dry-run: Create and alter the new table, but do not create triggers, copy data, or replace the original table. A safe mechanism to see what actual migration might look like. Note that it cannot reproduce an exact production like migration scenario. Think of it as a mock test. nocheck-replication-filters: Abort if any replication filter is set on any server. The tool looks for server options that filter replication, such as binlog_ignore_db and replicate_do_db . If it finds any such filters, it aborts with an error. If the replicas are configured with any filtering options, you should be careful not to modify any databases or tables that exist on the master and not the replicas, because it could cause replication to fail. For more information on replication rules, see http://dev.mysql.com/doc/en/replication-rules.html target . The default values for this option is yes so if you don’t intend to use it, make sure that there are no replication filters on your slave. Replication filters are rules to make decisions about whether to execute or ignore statements. For your slave, these rules define which statements received from the master needs to be executed or ignored. Now consider a case where you have a filter on your slave that says don’t execute any ALTER statement for table_a . When you do the schema change for table_a on master, the slave would never see that change. Eventually, this could cause the replication to fail after the RENAME operation. For this reason, the default value for this is yes . If you decide to change it to no , make yourself aware of the replication filters that you have on your slaves before you get into a situation where replication starts failing for one of your tables. recursion-method: This specifies the methods that the tool uses to find slave hosts. Methods are as follows: However, for various reasons, your RDS instance might be configured to not give up the correct slave host information to pt-online-schema-change as this was the case with our setup. The most concrete way to do this is to create a DSN (Data Source Name) table for pt-online-schema-change to read information from. For this, create a table with the following structure: After that, you can specify your slave hosts by creating entries for them like this: By specifying recursion-method=\"dsn=D=mydatabase,t=dsns\" , you are telling percona to find slaves in a table called dsns in database mydatabase . alter-foreign-keys-method: This is only required if you have child tables that reference the tables that are going to be changed as part of the schema change. The recommended method is rebuild_constraints which uses ALTER TABLE on child tables to drop and re-add foreign key references that reference the new table. The other two riskier options are drop_swap and none and if you happen to use them, please make sure that you know the intricate details. You can read about them in pt-online-schema-change docs . chunk-size: The tool performs copy operation in small chunks of data (a chunk is a collection of rows). This option governs the number of rows that are selected for the copy operation and overriding the default value would disable the dynamic chunk size adjustment behavior. This option does not work with tables without primary keys or unique indexes because these are used for chunking the tables. During the schema change, there could be a situation where the number of rows in a chunk on one of your replicas is more than what is there on master. This could happen because of replica lag and you would have to adjust your chunk-size for that. Note that a larger chunk size means more load on your database. Following are some of the resources to look into how Percona Toolkit handles chunking. Related options are: chunk-size-limit , chunk-time and alter-foreign-keys-method . Additional Resources: How Percona Toolkit divides tables into chunks Chunk Size Command alter: The schema modification that you want to apply. You can read more on rest of the options here: pt-online-schema-change docs Prepare a full fledged replica (call it staging) of your production environment with exact master/slave replication. Make sure that your staging is sufficiently populated with data. Creating artificial load on your staging databases when testing live schema changes is highly recommended. You can use tools such as Selenium and JMeter to do that. Make sure that DB Parameter Group in AWS RDS on production and staging are same and log_bin_trust_function_creators is set to 1 . Prepare a set of scripts to do dry runs and actual schema changes. This may include a list of tables and a shell script which would run pt-online-schema-change and store the logs for each run. Make sure that you store logs for each and every run (staging and production both). Do dry runs on staging and production to see the expected outcome. Have someone from your DevOps team to help you out in case if things start falling apart during the production migration. Choose an appropriate time do the migration. Ideally, this should be the least busiest time for your website. Take backups of production before the migration in case you have to restore. Here is a sample log from one of our migrations. Resolution: Increase chunk size pt-online-schema-change from Percona worked exceptionally well with us. The migration was tested very well on our staging environment and it worked in pretty much the same way on production. Helpful commands: To get list of tables with particular column: To find number of connections: To find if some table is locked: To find all foreign keys on a table: Percona toolkit docs (2.2+) pt-online-schema-change docs Other resources: MySQL Replication Rules MySQL Show Slave Hosts Percona Table Checksum Check and Fix MySQL Replication Inconsistencies Notes: * The version of pt-online-schema-change used for these migrations was 2.2.14 * This blogpost was also published here and here by the same author. We operate a network of online trading platforms in over 40… 34 MySQL Programming Amazon Infrastructure 34 claps 34 Written by Data Engineer at PointClickCare. Based in Toronto. Music aficionado who likes playing guitar and is an Eric Clapton fan. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by Data Engineer at PointClickCare. Based in Toronto. Music aficionado who likes playing guitar and is an Eric Clapton fan. We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-03-06"},
{"website": "Olx", "title": "olx test automation code architecture for several platform versions", "author": ["Filip Słomski"], "link": "https://tech.olx.com/olx-test-automation-code-architecture-for-several-platform-versions-ae9b734f948b", "abstract": "About OLX Group OLX Europe— 10 countries, milions of ads & users — probably most of you recognise the brand. Whole Continuous Integration/Continuous Deployment idea along with detailed UI tests started to take shape in January 2016. Ten months later there we are with fully working Jenkins pipeline with 67 jobs designed for 8 countries, triggers based on commits, github pull requests and timers. In this article I want to share the ideas we had, the code practices we follow and the challenges we encountered during our journey toward CI/CD. It is very similar to the presentation we recently gave at the biggest testing conference in Poland (Testwarez) and on few other occasions. We decided to use Gherkin + Behat + PHP + Page object pattern. Why those technologies? The whole OLX site is based on PHP so writing tests in the same language seemed like a good idea. Should the developers ever feel the desire to write some tests it would be much easier for them to do this, moreover I have had some previous commercial test automation PHP experience and I knew those are the technologies that can give us the expected result. Finally the last reason is trivial… the Athena test framework (which grows as an automation platform and is open sourced https://athena-oss.github.io/ ) supported only PHP at the very beginning. In long term run one might wonder if the extra gherkin abstraction layer was really necessary, but on the other hand it gives much more readable scenarios and that is a big advantage. In our code repository — we try to adapt and follow most of the best practices used by developers in order to make our code more reliable and to constantly improve it. We do code reviews of more critical changes, we try to apply all the good programming and test automation patterns. Page object pattern, factory pattern, mappings, DRY, KISS, SOLID, correct classes responsibilities, storing consts in different files, those are only some of the practices we use everyday. We often rewrite parts of the code to make it even more maintainable and easier to read. Coming from the OLX tradition of naming based on Greek mythology we took part in the creation process of Trojan — platform API that can create any data or basically perform all the desired actions on platform database. So basically what we do is we call the Trojan API via Sinon class and we get the desired result or data in a JSON response. Depending on the request the JSON may contain complex data like new advert details, user information or category trees or very simple responses like single id or just the status message of the action performed. We implement the endpoints on our own in the platform code using Storage, SQL syntax and sometimes queues or services already implemented in the main platform in order to achieve our goals. To make it a bit more clear the main examples of our Trojan usage are: generating random or specific users, ads, categories, setting up different platform configurations, triggering payment queues, moderation tools or even simulating whole actions in order to speed up the test process. As an example[I want to test buying promotions]: I send an api request to create a user. As a response I get a json with user email, password and id. As a next step I send an api request to create a new ad and pass user_id as a parameter. Finally I can send another request to api with the user_id and number in order to add credits to his account. This allows me to perform the task via UI without any issues. In the company we create platforms for 10 countries, however we test eight of them, because the other two are still growing markets without too many users. The base platform and layout is the same but there are differences. Besides obvious ones like language or database there are some other more or less subtle discrepancies. A lot of the categories and subcategories differ amongst countries, many features and views are only introduced for some platforms, finally even the element selectors in HTML are often different. For obvious reasons this present some challenge to get to know those differences and design the tests so that most of them work for each country without many changes and without code duplication. Our goal was to cover all key features for all the countries and in case of new functionalities to be able to easily adapt tests for new countries when the new feature is released. In order to manage the gherkin scenarios we decided to use one of behat mechanisms — tagging. All scenarios are marked stating whether they should be run for one, several or all supported countries. We also used tagging in order to differentiate between Jenkins job and platform features. In order to achieve that we specified tagging rules in the behat.yml for each of our testing jobs. Another solution we implemented was the Page Object Factory Pattern. Let’s assume the following scenario: We want to write new test for adding new advert in a specific category and we want to have it for all countries. However for each country there are different parameters and components required for this specific category and the common ones have different selectors based on database ids - so basically each country would require its own set of selectors. How do you create a neat page object class for something like this ? Unless you want to have plenty of mostly unused elements and very unreadable code you may want to use factories for this. The main idea is to feed the page object with elements with proper selector and appropriate type. For Romania choosing the Garden & House category may result in the appearance of input field with a specific selector while in Poland the same category may show a dropdown with a completely different selector. So in order to support both that cases we need to pass to page object an associative array where the element name is the key, and value contains an object of a proper element type with a valid selector. Of course the returned array depends on the provided country code. Another problem we came upon was writing the tests using a specific category. Categories for each country have different ids in database and the selectors are based on them. In this case the solution was pretty straightforward — we write a single scenario and then use a different class to map the category name to its id basing on the country for which the test is being run. Yet another solution we had to provide is differentiating between basic platform options like payment providers or advert promotions. Of course the easiest solution would be to get it directly from database, however in some cases these information are hidden so deep or not stored within db so we had to provide several classes that return configurations based on countries, similar to the one below: Another goal of ours was to achieve stable and fast tests. As you probably know UI tests are usually quite slow due to the need to actually perform every single action on the page as the user would do it, wait for the page load etc. Having that in mind two solutions instantly come to mind: clean code, parallelism and performing some repetitive action via API. Parallelism will be described in details in another article regarding infrastructure, few things worth saying is that you have to have it in mind while designing the whole solution. Tests can’t use the same users or adverts, make any permanent changes in configuration or in any other way interact with other scenarios. Each tests should be isolated as much as possible — create its own data and ideally clean after itself. Of course in some cases the scenarios may have to be run in sequence (if they modify configuration that can affect other test results) but those situations should be kept to a minimum. However what we try to avoid at all cost is using direct waits (sleep) in our code. Not only does it not provide stability but it also unnecessarily prolong the test run. Active wait is much better option in this case(more about that in a moment). A great solution when it comes to speeding up the tests is using the power of API. When it comes to some repetitive actions like creating an advert, registering a user, setting configurations it’s enough if we test it once via UI. Doing the same thing for the 100th time is just wasting resources. So whenever it’s possible we try to use Trojan API endpoints to speed up some scenarios. We use generated users, we create ads, set configurations using Trojan, create new user packets and we try to focus on the actual user flow and tested functionality. Obviously some of the important features have to be tested end-to-end and in this case there is possibility to do it via API. A question coming to mind when reading this may be: “Is the same user action done via API as reliable as if it was actually done by the user via UI?”. I know there are different opinions regarding that subject but I believe that reproducing the exact entry in database(often in the same way as the platform does it) and using API that is connected to the actual platform code reduce the risks to an acceptable value, especially when you look at the time advantages of this solution. Having said that here are the timetables for our testing jobs. UI tests and selenium are often connected with instability. In my experience I have seen huge suites of selenium tests that were really unstable or had a lot of outdated / failing tests. I guess I don’t need to say that the consequences of having such unreliable test suites are huge. Every test run requires additional manual verification? Someone has just added a new test, but it fails the whole job every second run? There are several bugs discovered by tests but the developers have no time to fix them? Probably many of the automated testers have been in position where they had to ask themselves those questions. Personally I don’t believe that an ideal testing situation is possible during a big project with a huge UI test suite, but you can always get as close to that ideal situation as possible. We took several measures both process-wise and code-wise in order to achieve that. I’ll start with the base question: “Why do selenium tests fail randomly?”. There are many reasons like: page loading is too slow, some script on the page was not loaded in time, the changes have not yet been saved in the database, some backend queue have not yet processed the given data and that’s only the tip of the iceberg. However looking at those reasons there seem to be a common denominator. Something has not been done in time and the action/assertion has to be repeated until we achieve the proper result. Of course we can’t keep repeating the action forever, because in case of an actual bug the test would never end, so a proper timeout has to be introduced. So we implemented this mechanism - known as active wait on several code levels. As the method name suggests this code handles repeating the actions within the method every 0.3s until it ends up in success(returns true) or until the given timeout is reached. We introduced this method in several places in order to increase stability: getting element from the page, checking if given element is (not) visible, handling dropdowns, any slower or packed up with scripts components, finally in most of the assertions when we expect some data to be visible on the page. This has given us a much better stability in tests without slowing them down. The key is that we don’t wait any particular number of seconds, but we proceed as soon as the method ends up in success. The only moment when we actually have to wait longer is when the action keeps failing and the timeout is reached, but that is to be expected. However handling it in the code is only one of the solutions we had to introduce. In order to keep our Jenkins jobs stable we also created another job called staging-job which is kind of a “purgatory” for tests. All the unstable tests go there for investigation as soon as we discover that they tend to occasionally fail without any good reason. Because of this we manage to keep our main jobs in good shape and we investigate the unstable tests as soon as we have time without disrupting the whole flow. We also introduced the possibility to test pull requests before the code is merged. Our solution enable the developers to have automatic feedback for each pull request and they can trigger other types of tests on demand just by typing a proper comment. An important matter are the code bugs which in spite of all the precautions sometimes manage to find their way to the master repository. Of course we monitor the situation constantly using test monitors that show us all of our jobs and we sound the alarm whenever tests turn red in order to avoid that situations. However sometimes a bug is introduced, and before we investigate the actual cause the developer start to work on another task. In this case it is crucial that the bug is reported and fixed as soon as possible. Allowing situation like this can very quickly lead to a moment where the tests are failing because of bugs and have to be manually checked in every single run in order to make sure no new bugs have been introduced. The developers can no longer trust in their results and can’t verify their pull requests easily which results in even bigger chance of introducing another bug. Finally some of the tests may become useless because they may leave some of the functionalities untested because of a blocking bug. Generally this is a very bad situation and can easily escalate to many severe issues if is not handled properly and in timely manner. In this case automation is the key. You have to enable the developers the opportunity to test their code before it is merged to the main repository and coming from experience: it has to work fast and it has be mandatory. You can’t just rely on the fact that the developers will run the tests every single time, sooner or later someone will decide that this change can’t break any functionalities and will not test it. On the other hand you also can’t introduce mandatory tests for each pull request if they last for several hours or if they are unstable and give no clear result. That’s why we try to keep our tests as fast and stable and possible and we expect the developers to respect our work and test their changes in order to avoid spending whole day on investigation and pointing out a particular person who introduced the bug. We’re only working and improving the testing process for 10 months so there is obviously much room for improvement. We still deal with some issues like infrastructure and resources issues which affect both test stability and performance, we still have not figured out everything process-wise as there is always a clash between being able to release fast and being able to release with good quality after all tests have passed. We still have several plans for the future in order to achieve the Continuous Delivery idea, should we manage to introduce it and gain feedback I will make sure to share it via the OLX blog. We operate a network of online trading platforms in over 40… 16 Testing Continuous Integration Continuous Deployment PHP Selenium 16 claps 16 Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Written by We operate a network of online trading platforms in over 40 countries under market-leading brands that are used by over 300 million people every month to buy and sell almost anything, creating win-win exchanges for people, their communities and the environment. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2017-01-23"}
]