[
{"website": "Kogan", "title": "Management Command Aliases", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/tag/productivity", "abstract": "", "date": "2019-05-07"},
{"website": "Kogan", "title": "Optimising Webpack Build Performance", "author": ["Simon Willcock", "Simon Knox"], "link": "https://devblog.kogan.com/blog/tag/webpack", "abstract": "", "date": "2020-09-15"},
{"website": "Kogan", "title": "Custom Relationships In Django", "author": ["Alec McGavin"], "link": "https://devblog.kogan.com/blog/tag/sql", "abstract": "", "date": "2021-04-27"},
{"website": "Kogan", "title": "All Hands on Deck - Kubernetes Hackday  (Part 1)", "author": ["Rizki K"], "link": "https://devblog.kogan.com/blog/tag/docker", "abstract": "", "date": "2018-11-22"},
{"website": "Kogan", "title": "Making Heroku Subdirectories Easier", "author": ["Jake Barber"], "link": "https://devblog.kogan.com/blog/tag/Review+Apps", "abstract": "", "date": "2019-08-07"},
{"website": "Kogan", "title": "Zoe's First Month", "author": ["Zoe Zhan", "Jake Barber", "Goran Stefkovski", "Claire Pitchford"], "link": "https://devblog.kogan.com/blog/tag/Process", "abstract": "", "date": "2021-02-11"},
{"website": "Kogan", "title": "How to Transform your Team's Communication with Stakeholders", "author": ["Goran Stefkovski", "Claire Pitchford"], "link": "https://devblog.kogan.com/blog/tag/Trello", "abstract": "", "date": "2016-02-10"},
{"website": "Kogan", "title": "Django on Kubernetes", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/tag/operations", "abstract": "", "date": "2019-01-07"},
{"website": "Kogan", "title": "How to Transform your Team's Communication with Stakeholders", "author": ["Goran Stefkovski", "Claire Pitchford"], "link": "https://devblog.kogan.com/blog/tag/Wall", "abstract": "", "date": "2016-02-10"},
{"website": "Kogan", "title": "Django on Kubernetes", "author": ["Josh Smeaton", "Rizki K"], "link": "https://devblog.kogan.com/blog/tag/kubernetes", "abstract": "", "date": "2019-01-07"},
{"website": "Kogan", "title": "Zoe's First Month", "author": ["Zoe Zhan", "Jake Barber", "Simon Willcock", "Jake Barber", "Jake Barber"], "link": "https://devblog.kogan.com/blog/category/Culture", "abstract": "", "date": "2021-02-11"},
{"website": "Kogan", "title": "Optimising Webpack Build Performance", "author": ["Simon Willcock", "Jake Barber", "Simon Willcock", "Josh Smeaton", "Jake Barber", "Goran Stefkovski", "Simon Knox", "Dylan Leigh", "Choon Ken Ding", "Sean Kozer"], "link": "https://devblog.kogan.com/blog/category/React", "abstract": "", "date": "2020-09-15"},
{"website": "Kogan", "title": "Making Heroku Subdirectories Easier", "author": ["Jake Barber", "Goran Stefkovski", "Claire Pitchford", "Claire Pitchford"], "link": "https://devblog.kogan.com/blog/category/Process", "abstract": "", "date": "2019-08-07"},
{"website": "Kogan", "title": "Custom Relationships In Django", "author": ["Alec McGavin", "Josh Smeaton", "Jake Barber", "Josh Smeaton", "Josh Smeaton", "Josh Smeaton", "Josh Smeaton", "Josh Smeaton", "Josh Smeaton", "Michael Cooper", "Nam Ngo", "Dylan Leigh"], "link": "https://devblog.kogan.com/blog/category/Django", "abstract": "", "date": "2021-04-27"},
{"website": "Kogan", "title": "June Hackday - Deployment Traffic Lights (Part 2)", "author": ["Alec McGavin", "Simon Willcock", "Jake Barber", "Jake Barber", "Jake Barber", "Goran Stefkovski"], "link": "https://devblog.kogan.com/blog/category/Events+%26+Community", "abstract": "", "date": "2018-09-14"},
{"website": "Kogan", "title": "React JS Melbourne - February Meetup", "author": ["Jake Barber", "Goran Stefkovski", "Simon Knox", "Choon Ken Ding", "Sean Kozer", "Goran Stefkovski"], "link": "https://devblog.kogan.com/blog/tag/Community", "abstract": "", "date": "2018-03-02"},
{"website": "Kogan", "title": "Webpack Your Things", "author": ["Simon Knox"], "link": "https://devblog.kogan.com/blog/tag/cubes", "abstract": "", "date": "2015-05-20"},
{"website": "Kogan", "title": "Making Heroku Subdirectories Easier", "author": ["Jake Barber", "Josh Smeaton", "Josh Smeaton", "Rizki K"], "link": "https://devblog.kogan.com/blog/tag/infrastructure", "abstract": "", "date": "2019-08-07"},
{"website": "Kogan", "title": "A Smarter, local-memory Django cache backend.", "author": ["Josh Smeaton", "Dylan Leigh", "Dylan Leigh", "Dylan Leigh", "Goran Stefkovski"], "link": "https://devblog.kogan.com/blog/tag/Django", "abstract": "", "date": "2017-07-28"},
{"website": "Kogan", "title": "Kogame (Koh-Gah-Mi) - A real time game in Django", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/tag/channels", "abstract": "", "date": "2018-06-11"},
{"website": "Kogan", "title": "A Smarter, local-memory Django cache backend.", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/tag/Caching", "abstract": "", "date": "2017-07-28"},
{"website": "Kogan", "title": "Faster Django Tests by Disabling Signals", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/tag/signals", "abstract": "", "date": "2018-08-23"},
{"website": "Kogan", "title": "Hacking the WiFi Spy Tank - Part 3", "author": ["Michael Cooper", "Michael Cooper", "Michael Cooper"], "link": "https://devblog.kogan.com/blog/tag/Warranty+Void", "abstract": "", "date": "2017-02-15"},
{"website": "Kogan", "title": "Custom Relationships In Django", "author": ["Alec McGavin", "Josh Smeaton", "Josh Smeaton", "Josh Smeaton", "Josh Smeaton", "Josh Smeaton", "Josh Smeaton"], "link": "https://devblog.kogan.com/blog/tag/django", "abstract": "", "date": "2021-04-27"},
{"website": "Kogan", "title": "Continuously Improving our Process - Retrospectives", "author": ["Claire Pitchford"], "link": "https://devblog.kogan.com/blog/tag/Retrospective", "abstract": "", "date": "2015-01-27"},
{"website": "Kogan", "title": "Optimising Webpack Build Performance", "author": ["Simon Willcock", "Simon Willcock", "Jake Barber", "Jake Barber", "Goran Stefkovski", "Simon Knox", "Dylan Leigh", "Choon Ken Ding", "Sean Kozer"], "link": "https://devblog.kogan.com/blog/tag/React", "abstract": "", "date": "2020-09-15"},
{"website": "Kogan", "title": "Monitoring Celery Queue Length with RabbitMQ", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/tag/monitoring", "abstract": "", "date": "2019-06-05"},
{"website": "Kogan", "title": "Django Test Splitting on Circle CI", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/tag/testing", "abstract": "", "date": "2020-06-01"},
{"website": "Kogan", "title": "June Hackday - Lifx Smart Tiles (Part 3)", "author": ["Jake Barber", "Alec McGavin", "Simon Willcock", "Josh Smeaton", "Sean Kozer", "Claire Pitchford"], "link": "https://devblog.kogan.com/blog/tag/Hack+Day", "abstract": "", "date": "2018-09-28"},
{"website": "Kogan", "title": "Catches when Expecting Exceptions in Django Unit Tests", "author": ["Dylan Leigh", "Dylan Leigh"], "link": "https://devblog.kogan.com/blog/tag/Unit+Tests", "abstract": "", "date": "2015-08-26"},
{"website": "Kogan", "title": "Making Heroku Subdirectories Easier", "author": ["Jake Barber"], "link": "https://devblog.kogan.com/blog/tag/Heroku", "abstract": "", "date": "2019-08-07"},
{"website": "Kogan", "title": "Django Test Splitting on Circle CI", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/tag/ci", "abstract": "", "date": "2020-06-01"},
{"website": "Kogan", "title": "Making Heroku Subdirectories Easier", "author": ["Jake Barber", "Josh Smeaton"], "link": "https://devblog.kogan.com/blog/tag/development", "abstract": "", "date": "2019-08-07"},
{"website": "Kogan", "title": "Faster Django Tests by Disabling Signals", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/tag/python", "abstract": "", "date": "2018-08-23"},
{"website": "Kogan", "title": "All Hands on Deck - Kubernetes Hackday  (Part 1)", "author": ["Rizki K"], "link": "https://devblog.kogan.com/blog/tag/deployment", "abstract": "", "date": "2018-11-22"},
{"website": "Kogan", "title": "Catches when Expecting Exceptions in Django Unit Tests", "author": ["Dylan Leigh", "Dylan Leigh"], "link": "https://devblog.kogan.com/blog/tag/Testing", "abstract": "", "date": "2015-08-26"},
{"website": "Kogan", "title": "Custom Relationships In Django", "author": ["Alec McGavin"], "link": "https://devblog.kogan.com/blog/tag/join", "abstract": "", "date": "2021-04-27"},
{"website": "Kogan", "title": "Zoe's First Month", "author": ["Zoe Zhan", "Jake Barber", "Alec McGavin", "Simon Willcock", "Jake Barber", "Jake Barber"], "link": "https://devblog.kogan.com/blog/tag/Culture", "abstract": "", "date": "2021-02-11"},
{"website": "Kogan", "title": "How to Transform your Team's Communication with Stakeholders", "author": ["Goran Stefkovski", "Claire Pitchford", "Claire Pitchford"], "link": "https://devblog.kogan.com/blog/tag/Agile", "abstract": "", "date": "2016-02-10"},
{"website": "Kogan", "title": "Catches when Expecting Exceptions in Django Unit Tests", "author": ["Dylan Leigh"], "link": "https://devblog.kogan.com/blog/tag/Database", "abstract": "", "date": "2015-08-26"},
{"website": "Kogan", "title": "Django on Kubernetes", "author": ["Josh Smeaton", "Rizki K", "Jake Barber", "Alec McGavin", "Simon Willcock", "Josh Smeaton", "Sean Kozer"], "link": "https://devblog.kogan.com/blog/category/Hack+Days", "abstract": "", "date": "2019-01-07"},
{"website": "Kogan", "title": "Custom Relationships In Django", "author": ["Alec McGavin"], "link": "https://devblog.kogan.com/blog/custom-relationships-in-django", "abstract": "Before working with Django at Kogan I used SQLAlchemy. One of the many features I liked about SQLAlchemy was you had the freedom to join tables on any clause. This is especially useful for when you have a not-quite-normal schema and the data almost matches. The trick is to use the undocumented ForeignObject (the base class of ForeignKey ) which allows for more flexibility when joining. Using the ForeignObject on its own will attempt to create a new column in the database which we don't want. Setting private_only=True will let us use a \"virtual column\"! Here's a snippet we use to set up joins: class Relationship(models.ForeignObject):\n        \"\"\"\n        Create a django link between models on a field where a foreign key isn't used.    \n        This class allows that link to be realised through a proper relationship,\n        allowing prefetches and select_related.\n        \"\"\"\n    \n        def __init__(self, model, from_fields, to_fields, **kwargs):\n            super().__init__(\n                model,\n                on_delete=models.DO_NOTHING,\n                from_fields=from_fields,\n                to_fields=to_fields,\n                null=True,\n                blank=True,\n                **kwargs,\n            )\n    \n        def contribute_to_class(self, cls, name, private_only=False, **kwargs):\n            # override the default to always make it private\n            # this ensures that no additional columns are created\n            super().contribute_to_class(cls, name, private_only=True, **kwargs) This keeps the original column intact (so your production code won’t need a huge refactor) and allows you to prefetch or select_related other attributes off the referenced table. Here’s a hypothetical situation where you might use this. Say you’re working on a legacy system (because you’d never make these mistakes now!). You’ve got a table customers which has customer_number which was generated by an external system. You’ve also got another table sales which uses customer_number as a foreign key. Unfortunately, customer_number might reference something that doesn’t exist in the customers table as it was dated a long time ago. CREATE TABLE customers (\n customer_number VARCHAR(100) NOT NULL UNIQUE,\n name VARCHAR(100)\n);\n\nCREATE TABLE sales (\nid int NOT NULL,\ncustomer_number VARCHAR(100) NOT NULL,\nPRIMARY KEY (id)\n);\n\nINSERT INTO customers (customer_number, name) VALUES (\"CUST01\", \"Steven\"), (\"CUST02\", \"Amy\");\n\nINSERT INTO sales (id, customer_number) VALUES (1, \"CUST01\"), (2, \"EXT-01\"); How could you perform a join on these? Let’s start with raw SQL. SELECT id, name FROM sales LEFT JOIN customers ON sales.customer_number = customers.customer_number; In SQLAlchemy, you could use a custom join condition: q = session.query(Sales).join(Customer, Sales.customer_number == Customer.customer_number) In Django, you can now do this: Class Sale(models.Model):\n    customer_number = models.CharField(max_length=100)\n    customer_reference = Relationship(\"Customer\", from_fields=[\"customer_number\"], to_fields=[\"customer_number\"])\n\nSale.objects.values_list(\"id\", \"customer_reference__name\") We've found this to be very useful for our own legacy schemas by being able to optimise blocks where multiple queries were previously necessary.", "date": "2021-04-27"},
{"website": "Kogan", "title": "Zoe's First Month", "author": ["Zoe Zhan"], "link": "https://devblog.kogan.com/blog/zoes-first-month", "abstract": "Hellooo! Nice to ‘meet’ you! I am so excited to share my application process, and first month’s experience at Kogan with all of you. Hopefully it can give you an idea of what you can expect during your application process, and what your first month will look like as a software engineer at Kogan.com. The whole application process for a software engineer role was incredibly pleasant, fast and smooth. It took less than 3 weeks in total from submitting my application to receiving an offer. First of all, I emailed Kogan.com to express my great interest and after that: Coding challenge: You will be sent a problem statement to solve within 2 days. You can choose any programming languages you prefer so it is a good time to show off your coding skills and style! Phone interview: With the CTO and Director of Technology Projects Online Coding & Lightning Talk: During this session, you will be asked to code with some of the senior developers. In Addition, you will be given a chance to present on any tech topic that you are passionate about for around 8 minutes. This was the most interesting and impressive part for me! Overall, this interview takes around 60 minutes. Receive an offer! Since joining Kogan.com, I’ve found it to be a super attractive place to work. We are keen to get things done! Imagine you are working for a household name in Australia, and your work goes to production everyday that impacts people’s online shopping experience. I still remember the satisfaction achieved after I shipped my first front end change within 24 hours after the first day with Kogan.com. The team here is very quick moving and no day is the same - this can be one of the biggest challenges for new joiners. However, the team is a group of talented and motivated engineers who are eager to help you. You are encouraged to ask for help when you need, rather than sitting alone behind your screen. Whenever you ask a tech question within the group, you will not only have different solutions shared with you, but are also constantly learning various tech best practices. This supportive atmosphere really helps me to understand my task and to get familiar with the project structure. In my short time here so far, I have already worked on several Kogan.com changes. These include adding new functionality to the customer’s order history page and improving filtering search results on mobile web. More recently, I’ve also made improvements to our personalised discount email template and started my backend development journey. This is definitely one of the most rewarding parts for me. At Kogan.com, you will have many opportunities to work across the frontend, backend and infrastructure. Trust me, it is one of the best places to develop your software engineering career! I’m looking forward to joining in our great social activities like hack-days, regular tech talks in the IT team and our next React Meetup (watch this space!). Zoe’s work from home set up", "date": "2021-02-11"},
{"website": "Kogan", "title": "Optimising Webpack Build Performance", "author": ["Simon Willcock"], "link": "https://devblog.kogan.com/blog/optimising-webpack-build-performance", "abstract": "At Kogan.com we use Webpack to bundle our React app. which powers the frontend for our Django backend application. One way we ensure that our website loads fast and is responsive for our customers is using isomorphic rendering to have the HTML of the page pre-rendered, ready for React to hydrate when it loads in the browser. This is also crucial for SEO purposes to ensure that search engines can easily crawl the pages of our site. To allow us to do this, we create two builds of our React app, one to be loaded in the browser and one for our node service to server render. As our codebase grows in size and complexity, the performance of our webpack builds - both development and production - has suffered. Throwing hardware at the problem is one option, but this isn’t so easy for our local developer environments. We suspected we had some room to improve in our build configuration that could give us some quick wins. Due to the number and complexity of our builds, we already had a number of optimisations in place. These included: parallelising our six builds with parallel-webpack and happypack, picking faster source maps, using the Webpack DLL plugin , and disabling some plugins for server render. Various ideas on potential bottlenecks and improvements had been thrown around, but without any measurements of the current build internals, they were hard to validate. Profiling We first attempted to profile the build using the ProfilingPlugin that comes built-in to webpack. This is as simple as adding a plugin to your webpack configuration, running your build and dropping the file produced into the Chrome Devtools Performance profiler. Unfortunately, it seems that our builds were a bit too large and complex and the Devtools profiler crashed whenever we tried to load the file produced by the plugin. We also used the Webpack Bundle Analyzer to visualise the size of each chunk being produced in our bundle to make sure we didn't have any issues in our code structure that was making the build output larger than it needed to be. However, this didn't surface much actionable information as we look at this fairly regularly and we understand the existing pain-points that are mostly tied to legacy libraries still in use that we are actively working to remove. Finally, we used the Speed Measurement Plugin which proved to be the most useful for our needs. It provides timing information for each loader and plugin in use to quickly see where the most time is being spent in your webpack builds, allowing you to better target your optimisations. It is worth noting that the HappyPack plugin made it harder to interpret the results, but it did still work. Below is an example of SMP’s output for a full build on my local machine: A sample of the output from the Speed Measure Plugin Optimisations Our goal was to improve build speeds for both the development and production builds, as both have impacts on our team efficiency (and frustration levels!). We made a number of changes in an effort to reduce our build times and have jotted down our results below so you can learn what worked and what didn’t. Minifying code is expensive As you can see from the screenshot above, the UglifyJsPlugin is taking a large percentage of our total build time. We already run this plugin with the compress: false , parallel: true and cache: true options , so we weren’t able to find any improvements here this time unfortunately. Type check only once We build server and client bundles in parallel and the builds are very similar. Aside from the entry points and some initialisation code for the client bundle, they are compiling the same React apps. We set ts-loader to run with transpileOnly: true for development builds of the server bundle so that we would only be running type checking on the client build, while keeping 99% of the Typescript coverage. This saved us around 30 seconds when building the server bundle. Don’t extract CSS unless you need to We swapped to use style-loader instead of the MiniCssExtractPlugin loader for the client bundle, as we don’t need a CSS file to be generated like we do for the server builds. Avoiding the need to extract CSS into a single file saved us about 3s (2%) on our build time. Node Sass to Dart Sass Changing to the dart-sass package from node-sass didn’t end up providing any performance improvements to our build time, but it did reduce our yarn install time. This is because Dart can be compiled down to JS and so it does not need to be built for the environment being installed on, unlike the node-sass implementation. Avoid generating expensive hashes For development builds, we tried to remove any locations that were generating content hashes either for uglification or cache busting, as this was not important in a development environment. \nWe changed the localIdentName in the css-loader options to remove the hash output, as well as removing the HashedModuleIdsPlugin altogether. Scale back threading We use parallel webpack to produce multiple webpack builds at the same time and take advantage of all available cores. We were also using HappyPack which enables webpack loaders to use multiple threads as well. However, running both of these at the same time ended up being slower overall as all the cores were already in use and so we were paying the price of HappyPack’s overhead with no benefit. We ended up removing this altogether, however this would likely be beneficial if you were only building a single webpack build. Cache everything you can The HardSourceWebpackPlugin was the biggest contributor to our final performance improvements. This plugin caches the output of each module to the file system so that subsequent webpack builds can re-use the previous run’s work and massively cut down the build time. NB: the HardSourceWebpackPlugin is currently incompatible with the SpeedMeasureWebpackPlugin, so make sure to remove one before adding the other!  Using the HardSourceWebpackPlugin with a warm cache resulted in a 60% decrease in our build times  Final Results After combining each of the changes above, we saw slight improvements to our first build time with a roughly 5% decrease duration for builds with no cache. However, given we use a warm cache for production builds, the hard source plugin cut our build times from 152 down to 52 seconds ! This was a huge win, and it took only minor changes to our docker build configuration to ensure the HardSourceWebpackPlugin cache persisted between our production docker builds so that it was almost always warm. A sample of our final build times The plugin has its risks as you need to ensure that all external factors of your build are taken into account when it is determining if the cache needs to be updated, but it worked out of the box for our fairly complex setup and we haven’t run into any issues after using it in production for over a month. If you also love finding performance wins and efficiency gains in your tooling and code, check out our careers page to learn more about our team at Kogan.com.", "date": "2020-09-15"},
{"website": "Kogan", "title": "Django on CloudRun", "author": ["Alec McGavin"], "link": "https://devblog.kogan.com/blog/django-on-cloudrun", "abstract": "We've recently discovered the wonders of a CloudRun stack here at Kogan. As a fast moving business we often need to quickly spin up new projects to demonstrate new ideas. We previously used Heroku, but for low-traffic or internal sites we didn't need the application to be running constantly: we're looking for a serverless solution! We came up with a checklist: Django Our team knows Django already Postgres Something fully managed, we don't want to spend our time configuring Asynchronous/Long-Running Tasks Need to process data while keeping responses fast Need to schedule tasks at regular intervals CI/CD We need to move fast! Google Cloud Platform (GCP) had most of the answers: CloudRun , a \"serverless\" cloud container that operates off Docker images, for the Django requirement; CloudSQL , a fully managed RDBMS, for the Postgres requirement; and CloudBuild for CICD. We're missing something to run our offline tasks! We've previously relied on Celery to handle our scheduling of long running tasks. However, Celery needs infrastructure: message brokers, worker pools, and a beat. That's a lot to consider when scaling up, and it also won't scale down to zero. Wouldn't it be nice to be able to scale our workers and web traffic in the same way? The solution we came up with was to define tasks as http endpoints. We could then scale both web and task traffic by increasing the number of containers, and everything is executing in the same environment. Enter CloudTask and CloudScheduler .\nCloudTask is a task queueing service managing retry-on-failure, exponential backoff, and rate limiting. CloudScheduler allows you to define tasks to be sent on a cron-like schedule. Both of these services define tasks as http payloads - perfect! Any alternatives?\nWe’ve previously attempted to use Cloud Pub/Sub as a lower level task queue, but we found that it was difficult to get any visibility into running tasks. In comparison, CloudTask gives you a nice dashboard of metrics where you can view tasks in progress, what the execution rate is, and how many tasks have failed. We built a library to interface with CloudTask and CloudScheduler: django-cloudtask . We made the API simple and similar to Celery. Tasks are defined by a decorator and may be called both synchronously and asynchronously. For example: from django_cloudtask import register_task\n\n@register_task\ndef process_data(report_pk, days_ago=1):\n    # tasks support both args and kwargs\n    for record in Report.objects.filter(pk=report_pk).generate_report(days_ago):\n        process(record)\n\n\n@register_task(schedule=\"15 14 * * *\", should_retry=False)\ndef schedule_reports():\n    # tasks can operate on a cron schedule\n    for pk in Report.objects.filter(enabled=True).values_list(\"pk\", flat=True):\n        process_data.enqueue(pk) More docs and examples are available in the repo! We also built a template, django-cloudrun-cookiecutter , which can kickstart a Django project ready to be deployed to CloudRun. We took a lot of inspiration from Google's demo Django app, Unicodex , a recommended read to understand what all the steps are doing. We’re very happy with our new stack and are already using the above repos in our production systems. There are still improvements to be made, such as more task managing defined in code, but we’re getting there.", "date": "2020-12-22"},
{"website": "Kogan", "title": "React JS Melbourne - August Meetup", "author": ["Jake Barber"], "link": "https://devblog.kogan.com/blog/react-js-melbourne-august-meetup-1", "abstract": "Last Thursday we had the React Meetup once again!  A quick summary of talks and relevant links are set out below: Three speakers covered these topics: Ben Teese talked on how to implement component/data colocation with Apollo Client and React - Slides Alec McGavin spoke about what really happens when a React component re-renders - Repo Basarat Ali Syed showed us the GLS layout system and how to make development at Australia Post a breeze - Link Interest in attending a meetup? Join us next quarter! - https://www.meetup.com/React-Melbourne/", "date": "2019-09-03"},
{"website": "Kogan", "title": "Django Test Splitting on Circle CI", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/django-test-splitting-on-circleci", "abstract": "One of the most important things you can do for your development teams' productivity is to shorten the feedback loop during development. This applies to getting feedback from customers or stakeholders to ensure you're building the right thing, as much as it does to testing the code you're writing to ensure no bugs have crept into your change set. Today we're focusing on the Development - Test feedback loop. Developing a change and running your regression test suite to validate that change should be as fast as possible. We've used three different systems to run our unit tests over the last few years. Beginning with a self-hosted Jenkins instance, transitioning to Travis CI, and finally arriving at Circle CI . We have several different applications at Kogan.com, but today I'm only considering the test suite for our biggest application, the one driving the shopping website. Slow Tests Our test suite currently clocks in at 3610 tests. A year ago, before we moved to Circle CI, this was probably closer to 3000 tests. And those 3000 tests were taking upwards of 25 minutes to run from start to finish. 25 minutes is not a short feedback loop for a change. Often, we'd have a change that was passing all tests and was about ready to ship in the daily deployment, but received some last minute feedback that required a small adjustment. That adjustment would be made, but waiting upwards of 25 minutes for that change to validate would mean missing that deploy window, or holding up the deploy while the test suite did its' thing. So it'd just get merged, before the tests had ran, because the changes would be low that a bug had crept in. We shipped most of our outages in this way. There came a point where the team gathered and agreed to do what we could to get the test suite running in under 5 minutes. A 5 times improvement is massively ambitious, but we wanted the threshold to be \"push your changes, grab a coffee, verify all tests have passed\". 5 minutes is enough to do some admin work, write up some docs, check your email, and remain somewhat in context. 25 minutes breaks your context every time. Faster Tests There were a few strategies we used to get the runtime of our test suite down, beginning with profiling test durations and aggressively optimising the slowest tests, or deleting them if they had little value. A few strategies we employed: - Reduce the amount of test data required for a test\n- Eliminating any network calls that should have been isolated\n- Trying to reduce or eliminate any SQL queries we could\n- Move test data setup to setUpTestData which is class based rather than test based The one strategy that really looked interesting was a built in Django feature for running the test suite in parallel . Runs tests in separate parallel processes. Since modern processors have multiple cores, this allows running tests significantly faster. But, there is a catch: Each process gets its own database. You must ensure that different test cases don’t access the same resources. For instance, test cases that touch the filesystem should create a temporary directory for their own use. While Django will ensure that each parallel process gets its' own isolated database, it does nothing to ensure that any other shared resources are also isolated. We had both Redis and Elasticsearch in our project, and there was no mechanism to ensure that we could isolate those dependencies. NOTE Django could assist here in a fairly basic way. When it spins up parallel processes, it could give each process an integer as an environment variable that the application could then use to do its own isolation. Example: TEST_PROCESS=3\nCACHE_URL=f\"redis//host:port/{os.environ['TEST_PROCESS']}\" Test Splitting We had been chatting to the team at Circle CI, and they were convinced that they could help us get the runtime of our test suite to under 5 minutes. By using a combination of bigger instance types and test splitting we would be able to meet our goals. Circle CI approach test splitting by partitioning the test suite, and running each partition on completely isolated environments. If you want to run with a parallelism of 4, then you get 4 instances, 4 databases, 4 redis nodes, and 4 elasticsearch instances. Each set of services is run within docker, using a familiar yaml format that most CI services have converged on. Where Circle CI is different from other systems is their CLI tool will split your test suite for you, in a deterministic way, and distribute your test suite evenly over the number of executors you've declared. Most interestingly, is the test suite can be split by timing data so that each of your executors should have a fairly consistent run time. And, true to their word, we were able to get our test suite run time down to under 5 minutes. As timing information changes, sometimes executors will unbalance, but that resolves itself over time. Caveats Test splitting comes with some downsides. In particular, we haven't figured out an easy way to aggregate 4 separate code coverage outputs into a single output. Each of the 4 coverage reports shows about 30% coverage, but aggregated would be > 80%. It's also more expensive if that wasn't obvious. Circle CI uses a credits system for billing. Running 4 processes uses more than 4x the credits, as you're now waiting for 4 separate environments to provision before you can actually run your tests. Considering the costs of a developer waiting for 25 minutes, that cost is certainly worth it. You can also gate test runs by other quality checks first so you aren't burning credits for no reason. The config It wasn't as straightforward as we'd hoped to get test splitting running correctly. There weren't a tonne of resources available for splitting Django tests considering Circle had just migrated to their v2 test scripts. There was some back and forth, but we arrived at a config that worked perfectly for us. I'll share it in it's entirety, with private details removed, so that it might help others in the future. version: 2.1\nworkflows:\n  version: 2\n  test:\n    jobs:\n      - test-python\njobs:\n  test-python:\n    docker:\n      - image: kogancom/circleci-py36-node\n      - image: circleci/redis\n      - image: elasticsearch\n        name: elasticsearch\n      - image: rabbitmq\n      - image: circleci/postgres\n        environment:\n          POSTGRES_USER: postgres\n    resource_class: medium+\n    parallelism: 4\n    steps:\n      - checkout\n      - restore_cache:\n          keys:\n            - pip-36-cache-{{ checksum \"requirements/production.txt\" }}-{{ checksum \"requirements/develop.txt\"}}\n      - run:\n          name: Install System Packages\n          command: |\n            sudo apt update\n            sudo apt install -y libyajl2 postgresql-client libpq-dev\n      - run:\n          name: Install Python Packages\n          command: pip install --user --no-warn-script-location -r requirements/develop.txt\n      - save_cache:\n          key: pip-36-cache-{{ checksum \"requirements/production.txt\" }}-{{ checksum \"requirements/develop.txt\"}}\n          paths:\n            - /home/circleci/.cache/pip\n      - run:\n          name: Prepare Postgres\n          command: |\n            psql -h localhost -p 5432 -U postgres -d template1 -c \"create extension citext\"\n            psql -h localhost -p 5432 -U postgres -d template1 -c \"create extension hstore\"\n            psql -h localhost -p 5432 -U postgres -c 'create database xxx;'\n      - run:\n          # pip install --user installs scripts to ~/.local/bin\n          name: Setup Path\n          command: echo 'export PATH=/home/circleci/.local/bin:$PATH' >> $BASH_ENV\n      - run:\n          name: PyTest\n          no_output_timeout: 8m\n          environment:\n            DJANGO_SETTINGS_MODULE: xxx.settings.test\n            TEST_NO_MIGRATE: '1'\n            BOTO_CONFIG: /dev/null\n            DATABASE_HOST: localhost\n            DATABASE_NAME: xxx\n            DATABASE_USER: postgres\n            DATABASE_PASSWORD: xxx\n            REDIS_HOST: localhost\n            BROKER_URL: 'pyamqp://xxx:yyy@localhost:5672//'\n          command: |\n            cd myapp\n            mkdir test-reports\n            TESTFILES=$(circleci tests glob \"**/tests/**/test*.py\" \"**/tests.py\" | circleci tests split --split-by=timings)\n            # coverage disabled until we can find a way to aggregate parallel cov reports\n            # --cov=apps --cov-report html:test-reports/coverage\n            pytest  --rootdir=\".\" --tb=native --durations=20 --nomigrations --create-db --log-level=ERROR -v --junitxml=test-reports/python/junit.xml --timeout=20 $TESTFILES\n      - store_test_results:\n          path: myapp/test-reports What else can you do? Running your test suite in parallel across multiple nodes is great at reducing the feedback loop for developers, but it does nothing to decrease the total runtime of your test suite. Indeed, it takes longer in absolute CPU time as N environments now need to be provisioned. To reduce your costs and get an even tighter feedback loop, you have to optimise your actual tests too. I've covered some of those methods above, but if you're looking for real, practical advice on speeding up your tests, Adam Johnson, a Django contributor has just released a book titled Speed Up Your Django Tests . Come work for us And if optimising test suites to increase developer productivity and happiness is interesting to you, hit us up on our careers page ! We'd love to chat.", "date": "2020-06-01"},
{"website": "Kogan", "title": "Making Heroku Subdirectories Easier", "author": ["Jake Barber"], "link": "https://devblog.kogan.com/blog/making-heroku-subdirectories-easier", "abstract": "To keep your codebase clean, it helps to have a separation of concerns. Splitting your codebase into a backend and frontend directory a great way to do this. Unfortunately Heroku buildpacks do not detect nested applications in a repo. There are two ways to overcome this: Option 1 - Use a third party subdir module (heroku-buildpack-subdir) Use a Third party buildpack that supports applications in project subdirectories. There’s a few out there, but I recommend heroku-buildpack-subdir as it has the most stars. Most only support 1 folder depth, but that should be all you need. Add heroku-buildpack-subdir to your heroku app’s settings dashboard. Dashboard -> Settings -> Buildpacks Once done, Create a new file called .buildpacks and add it to your project root. frontend=https://github.com/heroku/heroku-buildpack-nodejs.git\nbackend=https://github.com/heroku/heroku-buildpack-python.git Heroku will now install the correct buildpacks for the backend and frontend directory next time you deploy! Note: You can do the same thing for Review Apps by adding heroku-buildpack-subdir to your app.json. Option 2 - Ensure your backend and frontend entry points are available in the project root Most application’s entry points can be moved around, so why not do that for a django/node application? Say you have the following project structure: backend/\n    example_app/\n    Manage.py\n    Pipfile\nfrontend/\n    src/\n        index.jsx\n    webpack.config.js\n    package.json If you deploy this right now, Heroku will fail because it doesn’t know what kind of project it’s looking at. To fix this, move the Django project’s Pipfile and the Node project’s package.json to the root directory and configure to use nested project assets. Like so: backend/\n    example_app/\n    Manage.py\nfrontend/\n    src/\n        Index.jsx\n    webpack.config.js\npackage.json\nPipfile The benefit of this approach is having no extra buildpack configuration in Heroku. For this reason I’d recommend Option 2 over Option 1. Using one of these methods will enable you to use multiple buildpacks with your Heroku app!", "date": "2019-08-07"},
{"website": "Kogan", "title": "Monitoring Celery Queue Length with RabbitMQ", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/celery-queue-length", "abstract": "Earlier this week Matthew Schinckel wrote a post about how he monitors Celery queue sizes with a Redis backend. RabbitMQ is also a popular backend for Celery, and it took us a long time to get good visibility into our task queues and backlogs. So we'd like to share our solution for monitoring RabbitMQ celery queues for others that might be in a similar situation. Do read Matts post first. I'm going to assume you've done that, and just talk about how this script is different. Link to the full code! The trick to getting queue length details is to run the script from the RabbitMQ host itself, where rabbitmqctl is installed and allowed to speak to the cluster. We do this with Cron. First, get the distinct virtual hosts in your cluster. You might just have one, but we have several. VHOSTS=$(/usr/sbin/rabbitmqctl list_vhosts -q | grep -v \"/\" | xargs -n1 | sort -u) Next, define the stats to fetch from RabbitMQ: STATS2READ=( messages_ready messages_unacknowledged consumers ) Finally, loop over the virtual hosts and statistics, and push the metrics to Cloudwatch (or send them to New Relic!): for VHOST in ${VHOSTS[@]}; do\n    for STATISTIC in ${STATS2READ[@]}; do\n        MDATA=''\n        while read -r VALUE QUEUE ; do\n            MDATA+=\"MetricName=$STATISTIC,Value=$VALUE,Unit=Count,Dimensions=[{Name=Queue,Value=$QUEUE},{Name=InstanceId,Value=$EC2ID},{Name=VHost,Value=$VHOST}] \"\n        done < <(/usr/sbin/rabbitmqctl list_queues -p \"$VHOST\" \"$STATISTIC\" name | grep -Ev \"pidbox|celeryev|Listing|done\")\n        /usr/local/bin/aws cloudwatch put-metric-data --endpoint-url $ENDPOINT --namespace $NS --region ap-southeast-2 --metric-data $MDATA\n    done\ndone Use Grafana to visualise the metrics from CloudWatch, and get pretty graphs!", "date": "2019-06-05"},
{"website": "Kogan", "title": "Management Command Aliases", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/management-command-aliases", "abstract": "A quick minor efficiency tip for today that'll make your life at the Django command line incrementally better. You're using django-extensions in your project,\nright? If not, go and rectify that problem immediately. The best feature™ that django-extensions provides is the shell_plus command, which\nimports all of your models and the most common django modules. If you've got ipython installed (you should!) the shell will launch within an ipython shell. You know the worst thing about shell_plus ? The name. That's a long name to type out into your\nshell, especially since it's snake_case , and I type\nthe command approximately 15,000 times per day. Wouldn't it be nice to alias shell_plus to something like sh ? Well, now you can! Create a new file under one of your apps (I chose the \"core\" app) commands subfolders called sh.py and add the following code: # core/management/commands/sh.py\n\"\"\"\nA shortcut for shell_plus\n\"\"\"\nfrom django_extensions.management.commands.shell_plus import Command And now you too can enter a shell_plus session with the much nicer command sh . $ ./manage.py sh Why not a bash/zsh/whatever alias? Because docker, really. We could mount everyones bash or zsh profile. Or we could\njust rename a command and still access all of the help, arguments, and command\nline switches previously available. I like that. Bonus Section Setup your own extra default imports, so you'll never have to import timedelta ever again! SHELL_PLUS_POST_IMPORTS = [\n    (\"sales.enums\", \"*\"), \n    (\"datetime\", \"timedelta\")\n]", "date": "2019-05-07"},
{"website": "Kogan", "title": "Django on Kubernetes", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/django-on-kubernetes", "abstract": "This is part 2 of our Kubernetes hackday series. You can find Part 1 here which goes over\nhow we spent the day and what the goals and motivations are. For part 2, we're going to delve into the architecture required for running a Django\napplication on Kubernetes, as well as some of the tooling we used to assist us. This post will assume some knowledge with deploying and operating a production\nweb application. I'm not going to spend much time going over the terminology that\nKubernetes uses either. What I hope to do in this post is present enough information\nto kickstart your own migration. Kubernetes is big - and knowing what to research now\nand what to put off until later is really tricky. I'm also going to describe this deployment in terms of Amazon EKS rather than Google GKE simply because I was on the EKS team,\nand most of our applications are already on AWS. Disclaimer We're not yet running any production apps on Kubernetes, but we've done\na lot of the analysis, and are comfortable with the high level design. If you're\nlooking to migrate yourself, feel free to use this design as a jump start, but know there'll be a lot more detail to deal with. Current Django Architecture Before discussing where we're going, it's helpful to know where we are. When\ndeploying a new Django application, we usually have the following components. Application server(s) (Gunicorn) fronted by Nginx Worker server(s) (Celery) Scheduler server (Celery beat + shell, single instance) Database (Postgres) Cache (Redis usually, sometimes Memcache) Message Server for Celery (Redis or RabbitMQ) Static content pushed to and served from S3 A CDN in front of the static content on S3 The scheduler server is a single instance that we use for our cron tasks, and\nfor production engineers to ssh to when they need to do production analysis. It's\nincredibly important for our workflows to maintain this capability when moving to\nKubernetes. The Cluster EKS requires you to setup the VPC and Subnets that'll be used to host the master\nnodes of the cluster. Thankfully, AWS provide CloudFormation templates that do\nexactly this. The Getting Started section is required reading, and will walk you through the rather complicated\nprocess for setting up a cluster, authentication, the initial worker nodes, and\nconfiguring kubectl so\nyou can operate the cluster from the command line. This setup is one area where EKS lags well behind the GKE experience. Creating a\nfully operational kubernetes cluster in the GCP console is literally a few clicks. Minikube Minikube is a single-node local\nKubernetes deployment that you can run on your laptop. If you want to experiment\nwith Kubernetes but don't want the hassle of spinning up a production-grade cluster\nin the cloud, Minikube is an excellent option. Because EKS took some time to get operational, the rest of the team members worked\nexclusively on Minikube until the final hour. Minikube has some limitations, but\nwas excellent for a development environment. Kubernetes Design The very first task we set out to do was to come up with an architecture, in\nKubernetes terms, that would support our application and our desired workflows. The\nfinal design ended up somewhat different from the initial one based on our learnings,\nbut it gave us a shared language and allowed us to partition the tasks properly. There are actually two designs. The staging architecture differs from the production\narchitecture. We can have 10s of staging environments at any time, so being able to\neasily bring up and destroy environments is highly desirable. For this reason, our staging design does not use cloud services like RDS or\nElastiCache, which would require coordinating deployments between Kubernetes\nand various cloud provider SDKs. Instead, we deploy Postgres and Redis within\nthe cluster. Note: We love RDS. Properly configuring, backing up, monitoring, and updating\nservices like Postgres and Redis isn't trivial. We trust cloud providers to do it\nmuch better than our small team can, especially on Kubernetes where our experience\nis, well, very little. But if our Staging database deployment crashes, it's not a\ncritical event. The only real difference between the two environments is that the Services either point externally to a managed data store, or internally to a datastore. The\napplications themselves remain blissfully unaware. Deployments, Containers, and Services, OH MY You may have heard of Pods or Services but don't really understand how all of these\nconcepts fit together. The docs are dense, and knowing where to begin learning is\nhalf the battle. Let me give a quick run down of the Kubernetes objects we're\nusing here. Namespaces let you group together related components so they can communicate. Namespaces aren't\n(on their own) a security mechanism. Use namespaces to segment apps you trust, or\ndifferent environments for a single app. Deployments are like an auto-scaling group for a Pod . A Pod has (usually) a\nsingle container, but may have multiple containers if they are co-dependent .\nContainers within a pod share a volume and can communicate directly . Services are\nbasically TCP load balancers. They are capable of a lot more, but for our purposes,\nthat's what we're using them for. ConfigMaps can be environment variables or entire files that are injected into a Pod. Secrets like passwords\nare injected into the cluster, and then made available to Deployments at runtime. I encourage you to read all of the Concepts documentation in time. But do so after you have a handle on the above types first.\nThere's lots you can ignore as implementation details you won't have to deal with\ndirectly. Application Design Unless you're already following Continuous Delivery it's likely that your application is going to need to change in some way to be\ncompatible with a Kubernetes deployment. Here are a few things that we needed\nto change. Static Files Deploy them to S3? Bundle a copy inside each container? Use Nginx? This choice\ncan be particularly contentious. Here's my advice. Use Whitenoise so that Django is in\ncharge of serving static content, then put a CDN in front of it. If, for some\nreason, you don't trust Whitenoise then use nginx within the same Pod to serve\nthe files, and still put a CDN infront of it. Shared Storage Maybe you're using an NFS mount for uploaded media? Stop doing that. Use an object\nstorage service (like S3) instead. That said, you'll still need to tell Kubernetes what type of storage engine it\nshould use for local volumes. Before Kubernetes 1.11 (EKS), you will need to explicitly\nconfigure a storage class. See Storage Classes documentation for setting this up. Config Files You might have config files for services like New Relic or similar. Deploy these\nwith ConfigMaps instead. Secrets / Config Variables Use Secrets for sensitive material or ConfigMaps otherwise. Consume variables\nfrom environment variables within the app. Dockerfiles You might already have Dockerfiles for development or some other environment. My\nadvice is to have a separate Dockerfile optimised for development, and another\noptimised for Production. Perhaps you need auto-reloading and NPM in development? Great! Don't ruin your\nproduction builds and artifacts with cruft that you don't need. Reduce the size\nand build time aggressively for production. In our design, we have an app server, a celery worker, and a celery beat. Use the\nsame Dockerfile for all 3 servers, but change the Entrypoint so the right service\nis started. We don't want to be building 3 slightly different containers. Deploying to Kubernetes We have a good idea what objects we require. Our application is ready. How does\nit all come together? With YAML. Specifically, having Helm Charts produce YAML. Helm Charts are packages. You can use Helm to deploy Charts onto Kubernetes. Helm\nCharts represent fully running Applications like Postgres, or our own custom\nDjango application. Helm Charts can have dependencies . They may also have configuration, and run\ntime options that change how an application can be deployed. For example, you may\ndeploy a chart with a STAGING flag set, that additionally deploys a local Postgres\nrather than relying on hosted RDS. Sound familiar? You will spend a lot of time designing your Helm Chart yaml files. They are basically\ngolang Templates that will render Kubernetes compatible YAML for each object in our\ndiagram above. We hope to be able to abstract the chart enough where we can deploy\narbitary Django applications based on an image name. Monitoring and Operations We've covered the the concepts required for setting up a cluster and deploying the application, but it's harder to find information on monitoring your application and operating it in production. I'll briefly touch on some of those facets now. Logging Centralised logging is critical for any large application, but doubly so for one running within Kubernetes. You can't just hop onto a node and tail log files, and doing any kind of post mortem analysis on dying nodes becomes impossible. The standard wisdom is to run a logging daemon within each Deployment or Node that takes your logs and sends them to the logging service of your choice. There are helm charts for major logging services that you can reuse. Logging daemons typically run as DaemonSets , which is another Kubernetes object, but one you probably won't interact with directly. Monitoring Prometheus is the standard monitoring tool used within Kubernetes clusters. Your application (and services) can export metrics that Prometheus will then gather. We'll continue to use New Relic for our applications though, as it gives a lot of great insight into your Django app. Within your web/celery startup script, run it with newrelic-admin as normal. Deploy your newrelic.ini as a ConfigMap. SSL If you're using AWS Certificate Manager with your Application Load Balancer, then SSL is mostly handled for you. There are also Helm charts for deploying LetsEncrypt into your Kubernetes cluster. DNS This one is a bit trickier, and I'm not sure we've fully landed on a solution at this time. Amazon Route 53 is the managed DNS service, but what is going to provision a new zone for an instance of our application? There are tools like Pulumi that allow you to program your infrastructure with nice Kubernetes integration. It may make sense to have Pulumi provision the hosted zone, and then deploy your helm chart into Kubernetes. Auto Scaling There are two systems that you're going to need to autoscale now. The nodes that the cluster is running on, and the number of application instances (Pods/Deployments) you want to run. Helm cluster-autoscaler claims to support scaling the worker nodes. While Kubernetes has support for Horizontal Pod Autoscaler for scaling your application. Summary I hope the information presented above is useful and answers some questions you might have already had about moving to Kubernetes. We found that working our way through and understanding the concepts behind Kubernetes has lead to a lot more productive conversations about what a migration might look like, and the effort required to make that migration. A full migration is definitely on the cards for this year, and we'll be documenting our journey as we go. If there are any corrections needed above, please let us know in the comments. We'd also love to hear about your experiences with your own migration and what some of the sticking points were. And, as always, if infrastructure is your thing and you think you can help us with the migration to Kubernetes, We're Hiring!", "date": "2019-01-07"},
{"website": "Kogan", "title": "June Hackday - Team KASX (Part 1)", "author": ["Simon Willcock"], "link": "https://devblog.kogan.com/blog/june-hackday-team-kasx", "abstract": "Last June we had a Hackday at Kogan.com! This Hackday’s focus on displaying information and providing useful alerts using hardware and software. Teams were asked to express something they find interesting through one of the available mediums. In this 3 part series Simon, Alec & Jake will talk a little bit about how each team went. Team KASX Our team was given the task of creating an app using React Native for a mobile device that could be used to display information such as Kogan’s stock price. This would then be attached behind a cutout in a poster that is displayed in one of the office meeting rooms. Below is the mockup we created for our idea - the poster cutout is in the tram route display, so space/size would be an issue! Planning Before beginning, we needed to choose a platform and framework. The first was an easy decision; we chose to build an Android app, as our team involved a mix of Mac and Linux users and so we would not all be able to have a full development setup for iOS development. This was the first time any of our team had used React Native so we decided to use the starter package ‘ create-react-native-app ’ (CRNA) suggested by Facebook team in the React Native documentation . Splitting up the tasks We had a team of 4 and broke the work into 3 areas to avoid stepping on each others toes too much. Visual layout and style - getting the look and feel of the app to match the required positioning and style of the poster Data Feed - Finding suitable APIs that we could consume for different screens Tooling and Deployment - Getting the build environment working and figuring out how to deploy a RN app Create React Native App + Expo The underlying platform used by any app based on CRNA is a tool called Expo . This is a toolchain that simplifies some of the React Native workflows such as a shared configuration file for both mobile platforms, easy code push, remote live reload and debugging during development for devices not on the same network. Using CRNA and Expo made it very easy to get a hello world app up and running. Our visual virtuoso soon had an app running with placeholder data that looked vaguely like our simple mockup. We also found and integrated a dot-matrix font to give us a more authentic text style However we quickly began to encounter some niggling issues as we added functionality to the app. The worst of these were related to the live reload. We found that as we made changes to the app, it was not always obvious that the app had updated correctly. The tooling was telling us that it had updated, but when we checked the physical or virtual test device, we often found it had not actually changed. This grew more frustrating as the hours passed; we found ourselves not knowing if a change didn't work due to bugs in the code or due to the app not reloading; we were constantly force closing the app and restarting it. Despite the issues, the real benefit of Expo for our hackday challenge was the build and deployment process. When you start an Expo build, it is actually created remotely and stored in S3 (which Expo uses for code-push and other features). Once an Expo account had been created, it was simply a matter of logging in and starting the build using the CLI exp login -u myusername -p mysupersecretpassword exp build:android For our purposes, we didn’t need to publish the app to an app store, so we downloaded it manually from S3 using wget and pushed to a connected device manually using adb wget https://exp-shall-app-assets.s3-us-west-1.amazonaws.com/.apk -O my-great-app.apk adb install my-great-app.apk This was quite a smooth process thanks to our deployment doyen , who documented and tested this, ensuring that we were well positioned to showcase our app at the end of the day. Berk! Feed Me! While the front-end team toiled away, our designated data dictator began searching for a suitable API for that was publicly accessible and free. We found an NPM package asx-data but given the simplicity of our requirements ended up grabbing the underlying endpoint https://www.asx.com.au/asx/1/share/ and using it in a simple fetch call. This returned all of the information we needed, such as last price and change %, previous day price and change %, etc. This API was integrated into a simple React container component that fetched the data, updated state and then polled for changes at a slow interval. componentDidMount() { // This will also set the initial data this.getStockDataAndUpdate() this.interval = setInterval(() => this.getStockDataAndUpdate(), POLL_TIME) } getStockDataAndUpdate = () => { getStockData() .then(res => res.json()) .then((data) => { this.setState({ price: data.last_price, changeInPercent: data.change_in_percent, previousDayPercentage_change: data.previous_day_percentage_change, changePrice: data.change_price, }) }).catch((err) => { console.log(err) }) } Once this was up and running, our data feed fiend began to tackle a practical stretch-goal set by the team: Displaying the time until the next 109 tram departure from Montague St stop, a popular transport route for many Kogan employees. As it happens, Yarra Trams have their own publicly accessible API that has quite a lot of data available and was perfect for our needs. const TRAM_SCHEDULE_API =  ‘https://yarratrams.com.au/base/tramTrackerController/TramInfoAjaxRequest’ const TRAM_STOP = 2709 export function getTramTimeAway(){ return fetch(TRAM_SCHEDULE_API, { body: `StopId=${TRAM_STOP}&Route=&LowFloorOnly=False`, headers: { 'content-type': 'application/x-www-form-urlencoded', }, method: 'POST', }) } Outcome By the end of the day, we had been able to produce an Android app that rotated between screens displaying live ASX pricing data for the Kogan stock ticker (KGN) and upcoming 109 tram departure times. We were even able to showcase this on a real device. Our remaining follow-up tasks from the day were: Get another copy of the chosen photo blown up to the right scale and dimensions Mount the selected device to the back of the photo frame Hang it a nearby meeting room for all to see!", "date": "2018-09-14"},
{"website": "Kogan", "title": "All Hands on Deck - Kubernetes Hackday  (Part 1)", "author": ["Rizki K"], "link": "https://devblog.kogan.com/blog/all-hands-on-deck-kubernetes-hackday-part-1", "abstract": "Intro On the 5th of November 2018, the IT team at Kogan.com started another hackday. This time, we set our goal on learning Kubernetes. We wanted to answer the question; how exactly can we leverage container orchestration to make our deployment process faster and more efficient? In order to achieve our goal, we set out to deploy one of our major apps that controls customer subscription preferences with Kubernetes on two different cloud providers, Google Cloud Platform (GCP GKE) and Amazon Web Services (AWS EKS). By doing this, we hoped we could understand the pros and cons of each platform, while learning the intricacies of Kubernetes deployment at the same time. This will be a two part blog series. The first part will be a short overview of our motivation, goals, as well as how the day actually unfolded. The second part will be more technical, focus on the approaches of the two teams, and discuss the pros and cons of each platform. Motivation In order to understand our motivation, it is useful to have a general idea of our deployment process here at Kogan. Everyday, we have a set time where we do a daily deploy of our major apps. The deployment pipeline consists of a fairly expensive build step, storing the artefact, and then pushing the artifact to provisioned servers. Those servers are mostly provisioned with a combination of autoscaled CloudFormation and Salt. While this existing process is sufficient in most cases, we are still facing some outstanding issues that we would like to improve. Autoscaling takes too long, so we aren't able to react to changes in traffic patterns as quickly as we'd like. Secondly, the deployment process can take up to 15 minutes. One solution for this is to introduce docker into production as it will help us with deployment speed and standardise our process further. The number one tool for container orchestration at the moment is Kubernetes, and that's where it comes into play. The word Kubernetes had been floating around the office for a while, but few of the engineers could say they understood it, let alone experimented with it. Infrastructure can be a bit of a black box for some people, so we took this opportunity to learn together and get everyone somewhat across how deployment works at Kogan. Goal & Planning Considering that we would like to learn the pros and cons of both GCP and AWS platforms we set up two teams of 6 to 7 developers. Each team was responsible for deploying the app to their assigned platform in a manner suitable for a staging or UAT environment. With this in mind we wrote down a series of infrastructure related tasks on what constitutes deploying\" the app. We separated tasks into major components, like setting up a cluster, deploying the web app, and deploying dependent services. We began with an outline for the day, and then settled in to install all the tools we would require. After completing a short tutorial with minikube we split up into our respective teams to put together a proposed architecture diagram which we'd present to each other before any real hacking began. In the middle of architecture discussion Hackday The initial challenges were understanding the fundamental concepts and terminology surrounding Kubernetes. What is a Pod? What is a Service? What are the difference between them? Where does a Deployment come in? As the day unfolded and each team had their infrastructure running, we met some new challenges. The first was a difficulty with making the application Kubernetes \"ready\", which involved new configuration, variables, and strategies for managing static content. Then there were difficulties with learning new tools, like Helm, for composing a Kubernetes deployment. We found that examples and documentation could be lacking or outdated. There was a big jump in translating our existing docker-compose configuration to something that Kubernetes would understand e.g passing secrets to apps automatically, and getting the application to communicate with the Database. There were also some difficulties in getting a cluster running. The GCP team, as expected, had an easier time considering that they were able to run Kubernetes controller out of the box on GCP. The AWS team spent quite some time configuring the VPC and other AWS services to accommodate a Kubernetes cluster. Due to these challenges, at the end of the day, we had a situation where the GCP team had managed to setup their cluster, but had difficulties composing the configuration necessary to get the application running. The AWS team had managed to successfully get the apps deployed locally with Minikube but more of a hard time in translating that configuration to the AWS cluster. Conclusions Even though we did not manage to completely deploy our app from end-to-end with Kubernetes, we had gained a significant amount of experience as a team. Everyone was involved in the nitty gritty details of configuring Kubernetes, and even at various levels, every team member was aware of Kubernetes basic concepts and tooling. The hackday was a giant leap forward on our long term goal of converting our production apps to running on Kubernetes. We also acquired valuable information on the pros and cons of each cloud platform. Overall, we were confident to say that the goals set out originally had been achieved by the end of the day. Stay tuned for part 2 where we will discuss the technical advantages and disadvantages of both GCP and AWS platform, and what we have learned as a whole from a devops standpoint. Team GCP in the middle of demo Team AWS in the middle of discussion Doughnuts accompanying this hackday", "date": "2018-11-22"},
{"website": "Kogan", "title": "React JS Melbourne - August Meetup", "author": ["Jake Barber"], "link": "https://devblog.kogan.com/blog/react-js-melbourne-august-meetup", "abstract": "Yesterday we had our third react meetup of the year! The night went quite well, with the pizza disappearing in a swift 15 minutes - so get in early :o Three speakers spoke about three different topics: Vladimir Zotov spoke about different approaches & optimisations to rendering React. James Adams talked on how you can combine Recompose and High Order Components (HOCs) together to get some excellent reusability. Damon Smith used crowd debugging to modify a React Web + Native codebase in an interactive coding session. Check out the repo here . Think React might interest you? Come along to the meetup next quarter", "date": "2018-08-31"},
{"website": "Kogan", "title": "June Hackday - Lifx Smart Tiles (Part 3)", "author": ["Jake Barber"], "link": "https://devblog.kogan.com/blog/june-hackday-lifx-smart-tiles-part-3", "abstract": "Last June we had a Hackday at Kogan.com! This Hackday’s focus on displaying information and providing useful alerts using hardware and software. Teams were asked to express something they find interesting through one of the available mediums. In this third and final part of the series Jake will talk a bit about Team LifxnChill. Team LifxnChill Our team was given a set of 5 smart Lifx Tiles . These nifty light panels are 8x8 LED grids of dissipated lights. Five tiles can be chained together, arranged and animated. They’re programmable using an API ( docs here ). Each tile cannot be programmed separately. These have great potential for expressing all kinds of stuff. Brainstorming Ideas proposed included the following: Use github webhooks to do something with the lights when a commit is pushed, a pull request is merged/closed, or a deviant force pushes. Standup Glow - Pulse when standup kicks off Stretch goal - animations Plan of attack 1. Write experimental commands 2. Create endpoints that use the commands 42. Create lambda functions for other stuff We never got around to writing up steps 3 to 41. Steps 1, 2, and 42 were enough to get started! The API Two APIs were available; LAN and HTTP: HTTP - Send off basic requests, such as pulses, brightness and cycles. Authenticated by an oauth token. This was low entry and really fun to see ideas come together. Team members could POST requests and see the result immediately. curl -X POST \"https://api.lifx.com/v1/lights/all/effects/breathe\" \\     -H \"Authorization: Bearer YOUR_APP_TOKEN\" \\     -d 'period=2' \\     -d 'cycles=5' \\     -d 'color=green' Keep me POSTed - Lifx Tile LAN - Limited to, you guessed it, the Local Area Network. The lower latency LAN API allows calls to map each individual light rather than a whole tile. This meant you could use animations. We initially tried this with an existing package called photons-core but opted for HTTP for reasons we’ll later explain. Problems faced The LAN API was looking promising, until we discovered the complexity involved in getting the tiles running. Remember we only had one day here, so the focus had to be on getting something out. Using a local network also made it difficult for a team member working remotely to participate. With these factors in mind we opted for the HTTP API. Getting into it While developing with the tiles, we discovered often API calls were not coming through. We suspected it was to do with throttling, but cumulatively the team’s usage was nothing should have triggered it. It turned out the Tiles had a bug: WHEN all Lifx Tiles are off AND a cURL request is sent Expected Result: All tiles animate according to the options sent Actual Result: The first master tile ignites, but it’s daisy-chained titles do not When all tiles are off, you can’t power them all on with a single request (Which was incredibly frustrating). As a proof of concept everything worked, but a dealbreaker for day-to-day usage. At this point I decided to post on Lifx’s forums seeking an answer. Not long after they posted a firmware update and voilà! The Tiles became usable. Outcomes We now have a proof of concept standup reminder, an Orange/Red/Green status integrated with our jenkins pipeline and a glow each time a commit is made. In the future we’d like to move these actions over the LAN API with endpoints that our pipeline can hit, allowing the use of animations.", "date": "2018-09-28"},
{"website": "Kogan", "title": "Faster Django Tests by Disabling Signals", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/disable-signal-receivers-in-your-django-tests", "abstract": "Django signals are a form of Inversion Of Control that developers can use to trigger changes based (usually) on updates to models. The canonical example is automatically creating a UserProfile when a User instance is created: from django.conf import settings\nfrom django.db.models.signals import post_save\nfrom django.dispatch import receiver\n\n@receiver(post_save, sender=settings.AUTH_USER_MODEL)\ndef post_save_receiver(sender, instance, created, **kwargs):\n    UserProfile.objects.create(user=instance, ...) Signals are useful for connecting your own behaviour to events that you might not have control over, such as those generated by the framework or libary code. In general, you should prefer other methods like overriding model save methods if you have the ability, as it makes code easier to reason about. Signal receivers can sometimes be much more complex than creating a single object, and there can be many receivers for any given event, which multiply the time it takes to perform simple actions. Tests and signals Often you won't actually need your signals to execute when you're running your test suite, especially if you're creating and deleting many thousands of model instances. Disconnecting signals is tricky though, especially if the disconnect/reconnect logic can be stacked. An easier, but more invasive method for suspending signal receivers is to check a global setting, and return early. from django.conf import settings\n\n@receiver(post_save, sender=MyModel)\ndef mymodel_post_save(sender, **kwargs):\n    if settings.DEBUG:\n        return\n    work() This has two drawbacks. Firstly, it's messy, and you need to remember to add the check to each receiver. Secondly, it depends on a specific setting that you might need to have turned off when running your tests, which makes it harder to test the actual receiver. Selectively disabling signals We can take this most recent idea of checking a setting, and wrap it up nicely in our own decorator. When running your tests, you can override the SUSPEND_SIGNALS setting per test method or class. Here's a gist that does just that import functools\n\nfrom django.conf import settings\nfrom django.dispatch import receiver\n\ndef suspendingreceiver(signal, **decorator_kwargs):\n    def our_wrapper(func):\n        @receiver(signal, **decorator_kwargs)\n        @functools.wraps(func)\n        def fake_receiver(sender, **kwargs):\n            if settings.SUSPEND_SIGNALS:\n                return\n            return func(sender, **kwargs)\n        return fake_receiver\n    return our_wrapper And using this decorator in your test suite is straightforward: @override_settings(SUSPEND_SIGNALS=True)\nclass MyTestCase(TestCase):\n    def test_method(self):\n        Model.objects.create()  # post_save_receiver won't execute\n\n@suspendingreceiver(post_save, sender=MyModel)\ndef mymodel_post_save(sender, **kwargs):\n    work() And just like that, we can skip signal receivers in our test suite as we like!", "date": "2018-08-23"},
{"website": "Kogan", "title": "June Hackday - Deployment Traffic Lights (Part 2)", "author": ["Alec McGavin"], "link": "https://devblog.kogan.com/blog/june-hackday-deployment-traffic-lights", "abstract": "Last June we had a Hackday at Kogan.com! This Hackday’s focus on displaying information and providing useful alerts using hardware and software. Teams were asked to express something they find interesting through one of the available mediums. In the second part of this series Alec will talk a bit about Deployment Traffic Lights. Deployment Traffic Lights Our team was to build a system to monitor the status of our deploys. We were given a handful of wifi-enabled Arduinos , a light stick, a relay board and some jumper leads. Our plan was to poll Jenkins, and then turn on the lights according to the following matrix: New build starting: display amber light. Build in critical deployment step: flash amber light. Build finished successfully: display green light. Build finished in error state: display red light. Getting started The first step for this project was to get our development environments under control. With half of us running the newer MacBooks with only USB-C ports, the biggest challenge was installing and configuring a serial interface to upload our programs. While we were tinkering away, our IoT expert, Michael, was wiring up the lights to the relay board. Team division Once we were up and running, we broke down the tasks into 3 groups: Jenkins connectivity - ensuring a safe connection between the Arduino and production Jenkins Light management - building a set of functions to change light colours Integrating both sides - The glue between Jenkins and the light Problems As we were programming, we ran into a number of problems. Firstly, how to “acknowledge” a failed build? If a build failed or was cancelled before a critical step, we want to be able to flip the light back to green to reduce anxiety around the office. We decided that the best way forward was to have a novelty sized button that would mark a build as “OK”. This also meant that now our code was going to have to keep track of the individual build numbers (as the Jenkins polling was always retrieving the most recent build). Next, concurrency. An Arduino is, for all intents and purposes, single threaded. Now that we’ve got some user input (a button), we need to make sure that that the button push event would be handled in a timely manner. In multithreaded environments, there’d normally be a thread for UI events, and then a pool of background workers to perform non-interacting, long running tasks. In this case, the UI would be the button, and the background tasks would be HTTP requests and light changing. We came up with 3 options to solve this problem: Implement asynchronous http. Much like JavaScript, an asynchronous model seemed like a really good solution. Use interrupts. An interrupt pauses execution, executes another function, and then returns back to the original paused position. If the interrupt function is small enough, this would fake asynchronous behaviour. Hold down the button until something happens. The lazy method. Keep the signal high until the program has processed it. As this was a hackday and we were pressed for time, we quickly crossed off asynchronous http. Interrupts would be the ideal solution, but we decided to go with the lazy option 3. If we had time at the end we would fix it. We decided that we’d also not poll Jenkins on every loop, to give time for the psuedo UI loop to “breathe”. Finally, secrets. We had sensitive data that we wanted to load onto the Arduino, specifically the WiFi password and a Jenkins token. Since the permissions for the Jenkins token would be very limited, we weren’t too concerned about leaking this. As for the WiFi password, we were too pressed for time to come up with a good solution, so we baked it in to our repository (don’t look!). Outcome At the end of the day we didn’t have much to show. As it turns out, the WiFi module on the ESP32s isn’t that strong (or not that compatible with the office WiFi), and we had a lot of issues maintaining connectivity (even shutting the door would weaken the signal too much!). We did have connectivity to Jenkins, and the lights did change with the build status. The button was abandoned, and so were flashing lights (flashing lights meant asynchronous problems). We were left with green (everything OK), yellow (deploying), and red (failed).", "date": "2018-09-14"},
{"website": "Kogan", "title": "Kogame (Koh-Gah-Mi) - A real time game in Django", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/kogame-django-channels", "abstract": "Here at Kogan we do a hack day every 2 months. The organisers will come up with\na theme for the day, we'll split off into teams, and get to building. For our March hackday this year we decided to build a multiplayer game using Django Channels . We've been keeping\nan eye on channels over the time, and thought with the release of channels 2.0 that\nit was the right time to dive in and get some practical experience. The organisers\ndidn't want to do yet-another-chat-system implementation, so they decided to make \nthings a bit more interesting, and look at writing a real-time game. We split up into two teams. The first team decided to build a top down zombie\nshooter based on pygame using a tiled map with the\ndesign of our office. The second team decided to build a multiplayer version of\nthe popular mobile game Snake #Nokia_phones). This\npost is about the second project, which we affectionately named Snek . Note:\n    We know that Snake isn't exactly a real-time game, but we wanted to focus more on\n    how channels worked, rather than the mechanics of the game itself. This will be a fairly technical analysis of the project, the choices we made, and about how channels\nworks in a semi-production like manner. We made the decision to open-source the project in the interests\nof providing a real life example to the community on how the various pieces fit together. Introducing https://github.com/kogan/kogame-snek/ . I've also just published the\ngame to heroku at https://kogame-snek.herokuapp.com/ so you can actually load it\nup and play. High Level Architecture Tile based games like Snake aren't very complex conceptually. There's a 2-dimensional\narray to represent the game board. Within each element of that array is some kind\nof object, like a snake segment, a piece of food, or a wall. It can also be empty,\nrepresenting a tile that can be moved into. Each snake can be represented as a list of coordinates within the grid. Food can\nbe represented as a single coordinate within the grid. At any given moment, the entire game can be represented in roughly the following\nway: {\n    \"dimensions\": [24, 24],\n    \"players\": [\n        {\n            \"name\": \"username-1\", \n            \"snake\": [(0, 0), (0, 1), (0, 2)], \n            \"direction\": \"RIGHT\"\n        },\n        ...\n    ],\n    \"food\": [(14, 9), (20, 18)],\n    \"tick\": 143\n} After each \"tick\" (a tick represents the next game state) the game recalculates the next position\nof each snake, if any of the snakes collided and are now out of the game, if a piece of food was eaten,\netc. It will then render the next game state, and deliver that to each game participant. We use React.js a lot at Kogan, so we figured we'd represent the game state\nas a grid of divs on the page. setState could then be used to update the board each time it received\na new state from the server. For a snake to change direction, the client would send the server a new\ndirection, which would be used during the next game tick. In a real game, you likely would not use a\ngrid of divs that was updated roughly 5 times per second. It has very bad performance, resulting in\nmany missed \"frames\". Again, we were mostly focussed on learning how channels worked, so this was an\nacceptable solution for now . In the future, we'll look into using Canvas . Given we had a good idea of the game data format, and how we'd represent that on the frontend, we then\nhad to figure out a few things. How do we update the client in an efficient manner? How can the client update the server to change direction? How do we run the game engine on the server so that it is shared between multiple clients? These are exactly the kind of things that django channels allows us to do. Enter Channels The primary purpose of channels is to enable WebSockets in your Django project. WebSockets are an advanced technology that makes it possible to open an interactive communication session between the user's browser and a server. With this API, you can send messages to a server and receive event-driven responses without having to poll the server for a reply. HTTP is based around the request/response model. The client sends a request to the server, and the\nserver responds with the data. There is no facility in HTTP (HTTP 1.x) for the server to send data\nto the client when the server is ready. A WebSocket is a persistent connection that a client can make to a server. Since the connection is\nalways open, the server can send data to the client whenever it likes. That data might be new HTML,\nor a JSON structure representing the state of a game. It is then up to the client to interpret each\nmessage and react (hehe) accordingly. The client may also send messages to the server, which the \nserver would also need to interpret and process, depending on the data being sent. Channels provides an application server that can maintain these persistent websocket connections, but it\ncan also handle the traditional HTTP requests and responses. It's called daphne and replaces the function of gunicorn or uwsgi in your deployment. Note: You still can use gunicorn or uwsgi for your regular HTTP traffic if you like. Another thing that channels does is provide the concept of groups , which allow multiple\nwebsocket clients to communicate with each other over a lightweight pub-sub channel (hence the name). There\nare a few different layers that channels can use, but the most mature and easiest to deploy layer is\nprobably channels_redis . Syncing game state Each client in a channels deployment is represented as an instance of a Consumer . The actual instance persists\nfor as long as the client remains connected. For a websocket, a client will connect() , the server will accept() , and then the server can either .send() data to the client, or receive() data from\nthe client. You can follow along in the next section with the PlayerConsumer class. Given what we know about channels, let's go back to our 3 issues and see how we can solve them. 1. How do we update the client in an efficient manner? All clients connect, via websockets, to a channels group. When the game has a new state ready, it publishes\nthe state to the group, which every client receives. The client can then use that new state to update its UI. async def connect(self):\n    self.group_name = \"snek_game\"\n    # Join a common group with all other players\n    await self.channel_layer.group_add(self.group_name, self.channel_name)\n    await self.accept() # Send game data to group after a Tick is processed\nasync def game_update(self, event):\n    # Send message to WebSocket\n    state = event[\"state\"]\n    await self.send(json.dumps(state)) 2. How can the client update the server to change direction? All the clients are already connected via websockets, so the client publishes the new direction over\nthe websocket. The below code shows how the server must decode and interpret the message it's receiving,\nas the client can either join a game or publish a new direction . # Receive message from Websocket\nasync def receive(self, text_data=None, bytes_data=None):\n    content = json.loads(text_data)\n    msg_type = content[\"type\"]\n    msg = content[\"msg\"]\n    if msg_type == \"direction\":\n        return await self.direction(msg)\n    elif msg_type == \"join\":\n        return await self.join(msg) 3. How do we run the game engine on the server so that it is shared between multiple clients? Hmm, so this question isn't easy to answer given the information we already know. Let's dive a bit deeper. Running the game engine Game engines like this typically run in one big infinite loop. In fact, this is exactly how we chose to run our game loop: def run(self) -> None:\n    while True:\n        self.state = self.tick()\n        self.broadcast_state(self.state)\n        time.sleep(self.tick_rate) The game calculates the next state with tick() , it then publishes or broadcasts that state in some way, and then goes to sleep for some small amount of time. It'll\nthen wake up and do it all again. Now, if you forget about wanting this to interact with a webserver, you'd probably create a thread\nor process, and run it indefinitely. Indeed, that's what we do: class GameEngine(threading.Thread):\n    def run(self) -> None:\n        ... But what starts this thread? And how do we interact with that thread via our websocket\nclients? The answer is, we create a new consumer! Consumers can join groups and interact with each other over\nthe channel layer (redis). But aren't consumers websocket clients? Nope. Consumers are classes that\ncommunicate with each other over a channel layer. There are many different kinds of consumers. We used\nan AsyncWebsocketConsumer for the client, but we can use a SyncConsumer that's running our infinite\nloop game engine, and have the two communicate over the channel layer! class GameConsumer(SyncConsumer):\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Created on demand when the first player joins.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.group_name = \"snek_game\"\n        self.engine = GameEngine(self.group_name)\n        # Runs the engine in a new thread\n        self.engine.start()\n\n    def player_new(self, event):\n        self.engine.join_queue(event[\"player\"])\n\n    def player_direction(self, event):\n        direction = event.get(\"direction\", \"UP\")\n        self.engine.set_player_direction(event[\"player\"], direction) And the engine is able to send data back to the group by asking for the currently\nactive channel layer: from channels.layers import get_channel_layer\n\ndef broadcast_state(self, state: State) -> None:\n    state_json = state.render()\n    channel_layer = get_channel_layer()\n    async_to_sync(channel_layer.group_send)(\n        self.group_name, {\"type\": \"game_update\", \"state\": state_json}\n    ) Ok, so we have an engine running infinitely. It is started by a consumer, so has\naccess to the consumers channel layer. But what runs the consumer?! (I'm finally getting to it). Workers Have you used celery before? It runs as a separate service\nalongside your normal Django application server. It can receive and process tasks from a queue. Well it's the same concept with Channels workers. Channels can run workers , which run on top of channels, and\nhas access to the groups required to communicate back with the websocket consumers. We start a worker like so: $ python manage.py runworker game_engine The game_engine argument refers to the channel name in your routing.py application = ProtocolTypeRouter(\n    {\n        \"websocket\": SessionMiddlewareStack(URLRouter([url(r\"^ws/game/$\", PlayerConsumer)])),\n        \"channel\": ChannelNameRouter({\"game_engine\": GameConsumer}),  # THIS RIGHT HERE\n    }\n) So the worker process runs separately and creates the GameConsumer which then starts the GameEngine in an infinite loop. The PlayerConsumer websocket consumers can then publish\ndata to the GameConsumer : async def direction(self, msg: dict):\n    await self.channel_layer.send(\n        \"game_engine\",\n        {\n            \"type\": \"player.direction\", \n            \"player\": self.username, \n            \"direction\": msg[\"direction\"]\n        },\n    ) And the GameConsumer can receive that message ( player.direction is converted to the method player_direction on the receiver): def player_direction(self, event):\n    direction = event.get(\"direction\", \"UP\")\n    self.engine.set_player_direction(event[\"player\"], direction) Something to note is that there are actually two channels groups in play here. The first group is the snek_game group, which all of the players are connected to, and what the game publishes the state\nto. The second group is the game_engine group, which is a channel dedicated to sending player join\nand player direction messages from the player to the game engine. The following (simplified) diagram shows all of the components involved, and sample messages that are\nsent between them. Running the application There are a few resources out there that describe how to deploy a channels application, but unfortunately\nmany of them are out of date with version 2 of channels. For example, a worker service is no longer required\nif you just want to handle websockets. Daphne can now do it all. I'm not going to touch on optimum configuration for a service at scale. With more complex architecture\ncomes more complex operations and failure modes. Each websocket server can only maintain a finite number\nof clients, since they are persistent, so you'll need to think about how you'll scale your services for \nany real deployment. For our deployment, we chose Heroku. They have excellent resources for deploying Django applications so\nI'm only going to call out where the docs differ. Here are some high level config tasks before getting\ninto the harder parts. Create the app within the heroku dashboard Add a node.js buildpack with a custom npm script for building webpack: \"scripts\": {\n    \"heroku-postbuild\": \"webpack --mode production --config=webpack.config.js --optimize-minimize\"\n  }, Add a python buildpack as the second buildpack (it needs webpack to compile into static folder) Add a redis and postgres resource, so that $DATABASE_URL and $REDIS_URL is available to our app Ok, onto the fun stuff. You're going to need to customise the Procfile to\nproperly run daphne and our game engine worker : # Procfile\nrelease: python manage.py migrate --noinput\nweb: daphne kogame.asgi:application --port $PORT --bind 0.0.0.0\nworker: python manage.py runworker game_engine -v2 Heroku will provide the $PORT environment variable when starting the application. We need two services\nto run. The web service will be started automatically by Heroku. The worker service, which will run\nour game engine, will need to be manually scaled after the initial deployment. See that we've told daphne to load an asgi application? It looks just like a normal wsgi file, but \nallows channels to setup properly and load the routing.py definition: # kogame/asgi.py\nimport os\nimport django\nfrom channels.routing import get_default_application\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"kogame.settings\")\ndjango.setup()\napplication = get_default_application() That's it, provided you've followed the Heroku documentation up until we started messing with daphne. Checkout kogame/settings.py if you need some inspiration with your own configuration. Let's actually run this thing: $ heroku login\n$ heroku git:remote -a <app-name>\n$ git push heroku master\n$ heroku -a ps:scale worker=1:free -a <app-name>  # start the worker Thoughts on Channels We had a lot of fun building this game, but we also ran into a lot of hurdles along the way. There aren't\nmany tutorials around the web that deviate very far from a toy chat implementation. The good tutorials out\nthere were mostly out of date when we began. The documentation is mostly good but is still missing some of\nthe nuances that you need to know (like the type message argument mapping to a method on the message receiver .) Once we had the architecture setup and the code written though, everything worked as advertised. It hides so\nvery much of the complexity of a distributed system that, once you have a good grip on the role of each component,\nit becomes easy to add additional behaviour. I personally think there is a big open space for tooling on top of channels, such as specialised Consumers,\nmessage validation libraries, and group interfaces. Once more and more people begin using channels in\nproduction applications, patterns will emerge, as will the libraries to address and enforce them. It's an exciting time to be working with Django! And remember, if hackdays sound like your kind of fun,\nand Django is your cup of tea, we're hiring !", "date": "2018-06-11"},
{"website": "Kogan", "title": "React JS Melbourne - May Meetup", "author": ["Jake Barber"], "link": "https://devblog.kogan.com/blog/react-js-melbourne-may-meetup", "abstract": "The React meetup was on again at Kogan HQ last Tuesday! Four presenters covered three exciting topics; including the use of React in non-profit organisations, understanding the new Context API and scaling a React team and their tools from 2 to 20 devs. Due to the large waitlist for this quarter's event, we trialled a live feed via Google hangouts which went quite well! The event attracted 200 RSVP’s who consumed many burritos throughout the evening. Want to come along next time? Tune in to our Meetup Page . Links Harnessing the Power of Volunteers for Community Benefit - Mike King & Matt Wiseman (Back2Bikes) Github repo - https://github.com/Back2bikes/attendance-app Understanding the New Context API - Mair Swartz Github repo - https://github.com/Mair/react-meetup-context-api", "date": "2018-05-25"},
{"website": "Kogan", "title": "React JS Melbourne - February Meetup", "author": ["Jake Barber"], "link": "https://devblog.kogan.com/blog/react-js-melbourne-february-meetup", "abstract": "Recently we hosted the quarterly React Meetup at Kogan HQ! We heard valuable tips and insights from the presenters on the night from topics touching on bootstrapping and setting up your application, creating a living style guide, testing and the much loved Typescript! Check out some photos and links to the presentations below! The event attracted 250 RSVP’s and a large waiting list. Bas talking about how React + Typescript are used at Auspost. Storybook basics with Simon. Links Connecting the Dots - Scaling code and tooling up a react application for production - Jay Jamero (Software Engineer at Kogan) Slides - https://drive.google.com/file/d/1MSs_EAl8zOjVXkd80RcpX2d3iQwK0V7J/view Wallaby and react - How to make react testing super easy - Mair Swartz (Full Stack Software Engineer at Essential Energy) Github repo - https://github.com/Mair/react-meetup-wallaby/ Information on the talk topics and comments are available on the MeetUp event page: https://www.meetup.com/React-Melbourne/events/247502413/", "date": "2018-03-02"},
{"website": "Kogan", "title": "A Smarter, local-memory Django cache backend.", "author": ["Josh Smeaton"], "link": "https://devblog.kogan.com/blog/a-smarter-local-memory-django-cache-backend", "abstract": "Caching is one of the best strategies you can use when tuning the performance of your website. A lot of work goes into minimising the number of database queries for any given page, and making the queries that do hit the database efficient. But the best query is the one that's never executed at all. There are many different types of cache: Browser caches that cache static assets in a users browser. Proxy caches that cache web pages and static assets in data centers or edge nodes. Application caches that cache the result of expensive calculations in the application. Django's cache framework provides an interface that backends can implement to act as application caches. A number of backends are provided out of the box: Memcached (recommended) Database Filesystem Local memory Dummy django-redis is also extremely popular as a cache backend, and comes with a bunch of useful features. The Benefits of Caching Application caches in Django are used for storing rendered partial templates, fully rendered pages, and even the result of an expensive database aggregation. The common theme is that the thing being stored is generally expensive to calculate compared to the cost of retrieving the data from a cache. For example, rendering a navigation bar might require 3 database queries, a nested loop, and some non-trivial if-else statements. That might take \"only\" 30ms, but if your time budget is 100ms then you haven't got time for much else. Rather than paying the 30ms cost every time a user views a page, you might store that fragment in a cache, and return it the next time a user views a page. Getting the value from cache might cost 3ms, which would be a substantial time saving. Caching Pitfalls There are only two hard things in computer science - cache invalidation, naming things, and off by one errors. Cache invalidation is one of the harder problems to overcome, because it requires: that you know that something cached has changed, and; that you have a mechanism for clearing that cached something For this reason, cached values are often coupled with an expiry, called a TTL (time to live). When the time expires, the value is either purged from the cache, or the value is cleared on the next access. Another, sometimes overlooked, problem is managing the size of a cache. This is especially of concern if you're storing very large strings like templates, or calculated properties for a large number of objects. Memory can fill up, the process managing the cache can die, and the cache goes away. Being unable to access the cache can often mean your application server returning errors to users. Overhead Caching isn't free. The most robust cache solutions are implemented as servers, sometimes on the same server, but often a network hop away. You might end up paying a cost of up to a few milliseconds requesting data from a server over the network. Then there's the problem of storing complex objects. Strings and numbers are easy to store in a separate system. Lists and class instances not so much. To store and recreate complex data types, we need to serialize and deserialize data which comes with an overhead of time and memory. Django will use the pickle module for serialization. Local-Memory Caching By default Django will use the LocMemCache backend, which stores all key-values in the memory of the application. It's basically a per-process thread-safe dictionary, and is not recommended for production because it's not particularly memory efficient. Because the LocMemCache backend is per-process, there can be many slightly different versions of the cache across nodes and worker processes. Benefits: Avoids network overhead Great for access patterns that hit a small number of keys hundreds or thousands of times in a short period Pitfalls: Still incurs serialization overhead Non-deterministic eviction strategy at max size Overuse can cause memory problems for the application server Cache invalidation is only local to that process Better Local-Memory Caching We had found many places in our code base that were storing values in what were, effectively, global variables. Each location had a slightly different way of storing this data but the purpose was always to avoid expensive re-calculations. A lot of this data didn't make sense to store in our primary Redis cache. It was either highly local to the process (large offline celery tasks), network call overhead would eliminate any value of caching, or the particular value was recalled so often that eliminating the network overhead resulted in significant savings. Still, these solutions were all poor re-implementations of a local memory cache to avoid the pitfalls of the local memory cache backend. If we were able to design a local memory cache without those limitations, we'd gain a useful reusable caching component. We began by taking the existing LocMemCache backend and changing the behaviour we weren't interested in. Eliminating Pitfalls The first limitation was the easiest to overcome. Since the cache is in-process complex objects can be stored in a simple dictionary without any serialization. All calls to pickle were dropped. The default eviction strategy is to drop a large percentage of key-values upon reaching the cache max size. Unfortunately this strategy could evict really useful keys while leaving untouched keys to exist forever. It also requires iterating through the entire key space each time an eviction is performed which is an O(N) operation. A better eviction strategy is a Least Recently Used (LRU) strategy. LRU works by keeping all keys ordered according to the last access time. When key 123 is retrieved it's moved to the front of the queue. If a new key 321 is added it's moved to the front of the queue. When the cache reaches its max size then the value at the very back of the queue is removed. Fresh and frequently accessed keys are kept, while old unused keys are expired. The easiest way of introducing LRU behaviour to the cache was to change the caching dictionary to an implementation of an LRU dictionary. lru-dict was chosen as a fast C-implementation but other implementations could be used if CPython is not the target platform. Handling time based expiration (TTL) needed to change once LRU expiration was implemented, as we no longer had information about which keys were expiring. Storing the key and expiration date in a separate dictionary no longer made sense. Instead, we chose to store the expiration alongside the cached value, which we check on each access: lru_dict[key] = (value, expiration) The final change was to remove key name validation. Memcache does not allow specific characters in its keys so Django will validate key names to ensure applications will work across all cache backends. Validating key names is again an O(N) operation based on the length of the key which can add up to significant CPU overhead. This got us about a 17% increase in throughput. There is no good way of preventing unintended memory growth of a local in-memory cache. A small number of keys can still be used to store huge lists. Trying to memory inspect every value that enters the cache is non-optimal, and not the easiest thing to do in Python anyway. We've approached the memory growth problem with the following strategy: Default to a small number of max keys (50 - 100). Clearly document the limitations of the cache, and when it's useful. Don't make it the default cache! Using the local memory cache must be opt-in. By making this cache opt-in, we're able to identify good candidates and apply targeted optimisation, without fear of a developer accidentally spraying large querysets into it. There is no good global cache invalidation solution that we were willing to implement. The tradeoff we made was to have relatively short TTLs, and to only add data that can be read stale. The graphs below show the timing differences (lower is better) between LocMemCache , the first iteration of our LRU cache, and a second version of our LRU cache that eliminated key validation. View fullsize Set operations get extremely slow for LocMemCache as lots of evictions start to occur due to key iteration. Results This project began because a particular group of queries had begun to really impact our database. We determined that preloading all instances of a particular type and performing calculations in Python would be a lot more efficient than performing calculations in the database. Because these values are used by “fat models” it made sense to have a service layer returning the data rather than each model instance fetching or generating it. Fetching data over the network would have been extremely costly for the number of times our models required values from the cache. Eliminating serialization was also a large performance win. Pickling lists of model instances isn't fast. We measured a massive improvement to the CPU utilization of our database after deployment. Can you spot it? We'll continue to selectively add small sets of stable data to the in-memory cache to take even more load off the database. We're considering any data fetched by context processors first. Another big one for us are the store objects, which are used everywhere, and change on the order of months or years. Monitoring application memory usage is now a lot more important, and we'll be keeping a close eye on those particular metrics as we scale up this cache. Show me the code! We’ve released the LRU cache backend on github at https://github.com/kogan/django-lrucache-backend and published to pypi . There’s also some performance and tuning tools along with analysis in the benchmarking folder . The benchmarking tools were adapted from the awesome python-diskcache project. View code on Github As mentioned before, there is a dependency on lru-dict but any dict-like LRU implementation should be able to be used with little modification. Happy Caching!", "date": "2017-07-28"},
{"website": "Kogan", "title": "Team Outing", "author": ["Jake Barber"], "link": "https://devblog.kogan.com/blog/dev-team-outing", "abstract": "What do you do when you finish a card? Recently the Kogan dev team had an outing! It’s always great to get out of the office once and awhile right? We went to Fitzroy for a graffiti spray painting class. ./spraypaint.py The instructors showed the basics of spray painting then let us go nuts. We were given a large variety of spray paint colours. There were notepads to scribble brainstorm ideas too, a lot of which ended up like this . Our session was also outfitted with burgers, potato gems and drinks from Rockwell and Sons. Dev team represent (⌐▨_▨) This was done with the awesome people at Work Shop. You can find out more about them here . Interested in joining a team who work and play? We are hiring!", "date": "2017-11-20"},
{"website": "Kogan", "title": "Jake's First Month", "author": ["Jake Barber"], "link": "https://devblog.kogan.com/blog/jakes-first-month-working-at-kogan", "abstract": "Hi there! I’m Jake, a Graduate Software Engineer at Kogan.com. I’ve recently finished a Software Engineering degree and have been working here for one month now. This post will share a little bit about my experience here & what you can expect from the application process. Kogan.com was a very attractive place to work for me. I had heard good things from previous employed devs & the React Melbourne meetup. My goal was to find a workplace that fostered learning & provide opportunities to work with the latest web tech using best practices which Kogan represented. I also believed in the company’s vision after undertaking an internship at a tech startup in Shanghai, China involving the development of private label products. I emailed Kogan.com asking if there were any job opportunities. The application process went something like this: Coding challenge to solve in approximately 1 week in any language you like. I suggest you pick something you’re strong at, not just because it’s used at Kogan. i.e. if you’re good at Ruby, use it! Phone interview. Onsite technical interview (or if you’re far, far away Skype) & lightning talk. This goes for around 1 hour. You will be asked to work through a coding problem & answer using code and pseudocode. One or more senior developers will be there to guide you through. Receive an offer. This process was incredibly fast, with less than 3 weeks from application to offer. The Kogan office is awesome. It’s hard to go wrong with casual clothes, ping pong & arcade machines. There’s also yoga on Thursday mornings :O Something unusual in an organisation of this size is being able to choose which operating system you’re using. Additionally you can opt for extra accessories such as a standing desk too (they are actually pretty sweet). As a graduate, it can be a little overwhelming at times understanding how to solve a problem. Luckily I have the support of other team members at all times, who are happy to help out & guide me towards the best solution. I feel extremely privileged for my role at Kogan.com & am looking forward to what the future holds!", "date": "2017-07-31"},
{"website": "Kogan", "title": "Hacking the WiFi Spy Tank - Part 3", "author": ["Michael Cooper"], "link": "https://devblog.kogan.com/blog/hacking-the-wifi-spy-tank-part-3", "abstract": "In part 2 , I covered how to get into the router configuration, and now we are going to build on that to connect to your own WiFi network. This is particularly exciting because you could connect multiple tanks to the one WiFi network and control them all from a computer. NOTE : This is tricky, and if you mess it up, or miss a step, you will brick the tank and need to factory reset and try again.\nInstructions on factory resetting can be found in part 2. Step one: Change to client mode. On the left tree menu, select Wireless -> Basic In there, change the mode selector from \"AP\" to \"Client\". Press save changes, and in the next page make sure you select \"Reboot Later\" ! View fullsize Change the \"Mode\" to \"Client\" View fullsize Make sure you \"Reboot Later\", otherwise you will be locked out! Step Two: DHCP Once its on your home network the IP address will not match the correct subnet, so you will need to tell it to get a different IP. Go under \"TCP/IP Settings\" -> \"LAN Interface\" and change the \"DHCP\" setting to Client . Press \"Apply Changes\" and dont forget to select \"Reboot later\" again. View fullsize Change the IP to DHCP. Step three: Join Now go to Wireless -> Site Survey\nFind your network SSID in the list, select it using the radio selector on the right, then press \"Next\" View fullsize Select your wifi network using the radio selector on the right. In the next page, enter the PSK of your wifi network. View fullsize Enter your PSK here Press connect and it will start connecting! NOTE: The page will never load, as the network has changed now. Step four: Reboot The tank has now joined your network, but still hasnt restarted to change the DHCP settings, so it has the wrong IP. Restart the tank by using the power switch underneath. Turn it off and back on. Once it has started again, go to your home router's configuration and find the IP address of the tank. That's it! This will work with as many tanks as you want, so you could drive them all at the same time using your computer.", "date": "2017-02-15"},
{"website": "Kogan", "title": "React JS Melbourne - June Meetup & Video", "author": ["Goran Stefkovski"], "link": "https://devblog.kogan.com/blog/reactjs-june", "abstract": "We're loving the growth React JS is having in Melbourne and it's great to see the React Melbourne Meetup group's developers are driving adoption of React and it's associated patterns and tools across small and large organisations! Check out the video of the June MeetUp! A quick g'day from the ReactJS Meetup regularly held at Kogan.com HQ. The React Melbourne group has seen huge growth numbers and we had 200 RSVPs plus a substantial waitlist at the last MeetUp - so get in quick on the next one! In order to share the experience with those that were on the waitlist or couldn't make it - we also made a Live Stream recording of the night: https://www.facebook.com/kogan/videos/10157047065925484/ ... be warned, it doesn't flow as smoothly as the YouTube clip above ;) Slide Links Using Typescript with React  - by Mair Swartz, Team Leader at Oakton https://github.com/Mair/Typescript-and-React-demo SVGs in React - by Ken Ding (REA) https://docs.google.com/presentation/d/1jZhPKjCDq9_MDHslibsVyw05wQ2eOod84bk97dH9mn0/pub?start=false&loop=false&delayms=5000 Information on the talk topics and comments are available on the MeetUp event page: http://www.meetup.com/React-Melbourne/events/231550744/", "date": "2016-07-13"},
{"website": "Kogan", "title": "React March Meetup Recap", "author": ["Simon Knox"], "link": "https://devblog.kogan.com/blog/react-march-meetup-recap", "abstract": "Last night we hosted the largest-ever React Melbourne Meetup ! Talks included: Short intro to React & Flux - Nick Farrell React migration methods, made easy - Me Converting Stateful components to Redux - Cam Jackson React + Typescript - Basarat Ali Syded There was a great range of content and complexity across the talks, with some good discussions during the breaks. I gave an overview of how we have iteratively converted Kogan.com to React, in the most non-breaking way possible. Slides are embedded below, or available here . Interested? If you'd like to learn more about this we are hiring ! Or if you are interested in speaking at the next Meetup, get in touch . You may even get a free hat.", "date": "2016-03-31"},
{"website": "Kogan", "title": "Peter's First Day", "author": ["Peter Boin"], "link": "https://devblog.kogan.com/blog/peters-first-day", "abstract": "My name is Peter Boin, and this week was my first working with Kogan as a Software Engineer. I’ve come from almost a decade of work in the Automotive industry, testing ABS/ESC applications, and coding for Body Control Modules. This is quite a change for me, but so far I’ve been very impressed. The interview process was seamless, friendly, and respectful. I particularly enjoyed coding being a strong part of the interview process, it clearly indicated that that’s what I’d be hired for. The team at Kogan.com were very interested in what my passions were, even though they had little to do with web development or design (specifically). I see now that they could clearly see that my passions were not too different to those that drive the dedicated and talented engineers already here. My first day was started with what you’d expect, a brief tour around the facilities; spacious offices, break room, and of course, the development area. It was good to see that everybody was very busy, but still considerate and friendly. I was also asked what hardware I would prefer to use, Windows, Mac, or Linux. “Linux!” I cried (literally), but that’s just me. This particular luxury of choice is something my last industry lacked, so I was particularly excited about working with my prefered operating system. At 10am sharp we had a “stand-up”, which is a variation on Agile’s SCRUM. This is a fast-paced and focussed agenda of the plan for the day. Then without delay, I got my first task, pinned up on the wall, to add a feature to a django product update script. They said it wouldn’t be easy, but I said that’s exactly why I’ll take it. Peter gesturing meaningfully, Simon deeply understanding I spent the rest of the day installing my OS, editors, gaining access to the myriad of systems required to operate, and start learning, fast. And of course, I had my choice of Kogan “swag” to take home with me on the first day. My choice: a hoodie, and 2 t-shirts. The rest of the week has been a blast. It’s been very technically challenging, which is exactly why I chose to come here. I’ve integrated the work required for my first task, and I believe it’s ready to go, but I missed my window by minutes! So close, and yet… It’ll be launched on Monday. It’s also been personally challenging, because I’ve left so much behind. So many friends. I also need to travel a lot further, which I’m not used to yet. But I’m here late on a Friday because I’m enjoying it; writing this blog to recommend you to join us.", "date": "2016-05-30"},
{"website": "Kogan", "title": "A Dashing Radiator", "author": ["Michael Cooper"], "link": "https://devblog.kogan.com/blog/a-dashing-radiator", "abstract": "Our information radiator or dashboard is a powerful tool for our technology team. It is used to give a quick visual representation of the current state of things, and an anchor point for the team’s most important statistics and state. We are never short of lots of data, so without this dashboard we would need to create a habit of seeking out information from various sources, like New Relic, Sentry or Google Analytics. Our radiator also creates conversations with people outside the team about the metrics that matter the most to us. Tool Here at Kogan.com, we are using the framework \" Dashing \" by the people at Shopify to power our dashboard, and we have it deployed on Heroku . Dashing was our tool of choice as it has a decent amount of community contributed widgets to pull data from all different sources, like Google Analytics and New Relic. Red/Green To make our radiator easy to read and to draw our attention when it’s needed, we opted for a red/green visual system. If a widget is green it is good, if it turns red this indicates to us something that needs attention. This also means even if you are too far away to read the details of the widgets, you can still tell the overall state. The standard widgets don’t do this out of the box, so we modified them. Here are some of our modified widgets: Meter Widget You can grab the code here or install it with: $ dashing install e81a30b2ededb434062c Time Since Last Widget At Kogan, we like to deploy often, and this widget lets us know how long its been since we last deployed. You can grab the code here or install it with: $ dashing install 656f8dc218d3e11d238a Security A concern we had with Dashing jobs was that it encourages people to hard-code the credentials of your third party sources into the ruby code, which is not good if you want to put the code in source control for others to see. To combat this, we put the credentials in environment variables and changed the code to use Env['VARIABLE_NAME'] notation to get them in the code. This allows you to distribute a sourceable .sh script to those who need it, while keeping the main code base behind the radiator open to anyone who might be curious to see how it works. Before: Trello.configure do |config|  \n  config.developer_public_key = 'THE_DEVELOPER_KEY'\n  config.member_token = 'THE_MEMBER_TOKEN'\nend After: Trello.configure do |config|  \n  config.developer_public_key = ENV['TRELLO_DEV_PUB_KEY']\n  config.member_token = ENV['TRELLO_MEMBER_TOKEN']\nend Like the sound of how we work? Check out our Careers Page !", "date": "2015-03-03"},
{"website": "Kogan", "title": "Template Illogic: from Django and Handlebars to React", "author": ["Dylan Leigh"], "link": "https://devblog.kogan.com/blog/template-django-handlebars-react", "abstract": "When the current incarnation of Kogan.com was developed, most pages were rendered server-side with Django templates; client-side rendering used Handlebars.js templates. As well as the effort (and inevitable mismatching content bugs!) caused by maintaining two templates for some parts of the site, the syntax often became awkward and cumbersome. As Kogan.com expanded with new features and into new markets, both templates grew more complex. For example, if you wanted a page to use \"Zip code\" in one country and \"Postcode\" in the others, one could do this: {% if country.name == 'United States' %}  Zip code\n     {% else %} Postcode\n     {% endif %} This is just the \"obvious\" way to choose which word to use; there are more appropriate ways of doing this (using the translation features ) as well as less appropriate methods: {{ country.name == 'United States'|yesno:\"Zip code,Postcode\" }} Unfortunately this isn't so straightforward in Handlebars, as there are no operators to allow comparison of variables - control flow structures such as if/else will only operate directly on the boolean value of a variable. This is a deliberate design decision by Handlebars to enforce the separation between logic and presentation. Mustache (the templating language Handlebars is based on) is even stricter - there are no explicit control flow statements other than loops. Therefore, it is necessary to pass a value which can be treated as a boolean to Handlebars: {{#if country.isUSA}} Zip code\n    {{else}} Postcode\n    {{/if}} Or just take all the logic out of the template and pass the content itself: {{ country.prefixOfTermUsedInThisCountryForPostalCodesIncludingSpaceCharacterIfSeparateWord }}code Handlebars can be extended with arbitrary functions, so some such as #ifEquals have been written to allow equality comparison. Other helpers such as #ifCond can work with any conditional operators, but they have no matching else statement, so it is necessary to use two opposite operators: {{#ifCond country.name '==' 'United States'}}\n        Zip code\n    {{/ifCond}}\n    {{#ifCond country.name '!=' 'United States'}}\n        Postcode\n    {{/ifCond}} One of the benefits of our migration to React.js is the melding of Javascript with the template system into the creature known as JSX . Although at first glance this tramples all over the separation of logic and presentation, the logic is restricted to only presentation-related decisions due to the way React enforces rendering from a meticulously defined state. Keeping the front end code and the template within the same file also means it is easier to figure out what is going on - and much easier for programmers who have less front-end experience to use. Compared to our previous hierarchy of .js and .handlebars files, we have found React is a more readable and maintainable solution for the whole team - most of the time: ${storeCode === 'us' ? 'Zip ' : 'Post'}code", "date": "2016-03-30"},
{"website": "Kogan", "title": "Kogan Django Heros - Supporting the Django Fellowship Program", "author": ["Goran Stefkovski"], "link": "https://devblog.kogan.com/blog/kogan-django-heroes", "abstract": "We're happy to announce that Kogan.com has donated $1,000.00 to the Django Fellowship project.  The Django Fellowship is project where paid contractors are engaged to manage some of the administrative and community management tasks of the Django project to support rapid development of Django itself.  — djangoproject.com It's no secret the Kogan.com tech team are fans of Django ( we've previously dontated $10,000 ). We use django because we believe it has a well thought out structure, attention to security best practice as well as its ability to assist developers write features rapidly ... meaning our customers get a better shopping experience :) We think it's a great idea and encourage other corporate Django users to donate to the program .", "date": "2015-02-07"},
{"website": "Kogan", "title": "How to Transform your Team's Communication with Stakeholders", "author": ["Goran Stefkovski"], "link": "https://devblog.kogan.com/blog/transform-your-stakeholder-comms", "abstract": "TLDR. Displaying your work and backlog on a physical wall makes it accessible to those around you. However, in order to effectively achieve this goal – you need to keep them groomed and well presented. Easily identify the progress of your initiatives by using colour coded cards. A great idea is to maintaining initiative streams with their own colours. We even match them to the colours available in Trello’s labels. For added bonus, our cards ‘pop’ out more because we use a black wall. Days in Progress for a task help identify unhealthy cards – we have a health meter element on our cards that visually assist identifying cards which have gone past our 1-4 days in progress range. THE WALL The dev team at Kogan.com follows the Agile project methodology. One of the big benefits of the Agile process is it’s ability to effectively expose the progress of tasks and initiatives to business stakeholders. At Kogan.com our dev team and other business teams are collocated and we primarily use a physical wall to monitor and follow progress. The physical wall is backed by a virtual wall in Trello, which is used to collaborate on the requirements, tests and other data relating to the card. Shortly after the day we switched to using a physical wall (long ago now), I noticed an interesting change in behaviour at the office. I would often see people stop in front of the wall in order to look at the work we were doing. Claire, our Product Manager would also often engage with them and we found that each stakeholder was quite good at prioritising their interests amongst work the team was doing for other stakeholders. They were also amicable as to when the item they requested was estimated for delivery. Now don’t get me wrong, communicating tasks and progress was not a new concept at Kogan.com, we always had our digital tools available for stakeholders to view – but what was stark in this observation was that it was not nearly as effective as our physical wall. Nobody ever viewed these tools or dashboards – probably because they aren’t fun to view, you need to remember to visit them on your browser, you probably forgot you username and password, and when you look at them – you usually ignore tasks that you are digitally linked as a stakeholder on. .. our online tool was not nearly as effective in communicating prioritiy and work in progress to stakeholders compared to the physical wall. THE SECRET So what was the killer feature this paper technology possessed? The physical wall had presence – people would walk past it multiple times a day – and all they needed to do was stop for a moment and glance to get an overview of what our team was doing and how their projects were tracking. But presence alone was just one part of the magic – the other part – was the presentation of our wall. I’ve seen many card walls in the past and quite often they live on scrawny cards, post-it notes and biro markings. They’re just too difficult to read. Our wall has large cards, colour coded and sharpie etchings! Take a look below and compare the difference: Compare pictures of hand written notes I found on the internet (left) with the Kogan.com cards (right). We love our wall so much, we thought we’d share some secrets that make the wall work so well for us. Location, Location, Location! The old real estate mantra equally applies to your wall! You need a big space which is well travelled and accessible to both your team as well as stakeholders of the business. Near a thoroughfare to the kitchen could be a good place! We’re lucky enough to have designed our office with a wall next to the dev team, but on the way to the kitchen/games room. First Impressions Last Keep your wall well groomed and fairly aligned. Design principles equally apply to useability of a wall as well as to your tech product. Some tips we have are: Use high quality and well sized cards and write on them with a medium point Sharpie so it’s easy to read from a few arms lengths away (If you have bad handwriting - try our opensource automated Trello Card Printer e-Claire Keep your backlogs groomed and in streams representing the product or initiatives. This makes it easy to locate cards in the backlog; Colour code your cards to initiatives and features. For example, Green = Feature, Red = Bug, Blue = Machine Learning & Customer Recommendations… This provides a great birds eye view on the progress of initiatives or the ratio of bugs to features we’re working on. We’ve taken this one step further by synchronising our colours to the labels in Trello! Make your cards stand out. We have a matte black wall – so our cards really pop from the wall and are easy to differentiate! An Apple a Day Keeps the Doctor Away At every standup, make sure you annotate your cards so that you can keep track. Our cards have a tracking element which we use to gage the health of a story. We aim to break tasks down into cards that represent 1 – 4 days in work. As soon as a card goes beyond that time, it’s in the red and we initiate a process to ensure we’re delivering the card the best way possible. When a card is blocked, we’ll place a big pink note over it – so we’re aware there is a blocker that needs to be resolved before the card can commence. Start today! The physical wall is a great addition to a team’s workflow. I highly recommend you go out, kit up and set out a wall with your team’s progress and then check the impact it has on the way your team interacts with its stakeholders. The transparency on progress the wall facilitates is reason enough alone to do this – and once you do you’ll discover a whole new world this old world technology starts to expose… but we’ll keep you hanging till the next article to find out! Does your team use a physical card wall? Is it working? We’d love to hear your thoughts in the comments below!", "date": "2016-02-10"},
{"website": "Kogan", "title": "Hacking the WiFi Spy Tank - Part 2", "author": ["Michael Cooper"], "link": "https://devblog.kogan.com/blog/hacking-the-wifi-spy-tank-part-2", "abstract": "Following on from my previous post , this will go deeper into the inner workings of the iSpy tank. If you run an NMap scan on the tank, you will find that other than the ports found in the last article, it also has port 80 and 21 open. Port 80 There is a web interface on port 80, and when you connect it greets you with a password prompt. After a little bit of searching on the internet, I found the username and password is \"HAPPYCOW\". Inside, it appears that this tank is just a WiFi router! To get into the administration interface, click on “Settings” in the top right corner. Port 21 The other port, 21, is an FTP interface, so lets take a look at that. Anonymous login works! To find out how it functions as a tank ontop of a WiFi router, we need to find the extra software it runs. A good place to start is the startup or \"init\" scripts. The one for the tank is at /etc/init.d/rcS # start web server\nboa\niwpriv wlan0 set_mib led_type=3\nifconfig wlan0 down;ifconfig wlan0 up\nuvc_stream -r `flash get1 UVC_RESOLUTION` -f  `flash get1 UVC_FRAME_RATE`  -p `flash get1 UVC_PORT` -m MJPG -l 3 -b -d /dev/video0\nuart_bridge 1 192.168.1.188 `flash get1 UART_SERVER_PORT` &\n#spook -c /etc/spook.conf -p 7070 & Looking at the bottom of the file, there are two interesting programs it appears to be starting: uvc_stream is an open-source mjpeg streamer, found here: leoz/mjpg-streamer . uart_bridge I assume is the daemon behind port 8150 that controls the motors. The name implies that the motor control is done by a separate chip and this is just a bridge to it. Opening the covers Inside the tank, it appears to be made up of two boards, one is the wifi router (the smaller one on top), and another larger one that takes care of power and controlling the motors. View fullsize The markings of the WiFi chip say mine has a RTL8196EU chip. Reset to factory default If you mess up the settings, and need to reset it (which happens to all of us, right?) here’s how to do it! WARNING: This involves touching a LIVE circuit, If done incorrectly you could fry the whole thing, short the battery and/or electrocute yourself! Dont do this unless you know what you are doing and understand the risks! IMPORTANT: This WILL void your warranty for sure! Unfortunately the \"Reset\" button on the bottom of the tank does not reset the thing to factory defaults. To do that you will need to open up the tank, and find the reset pin on the smaller WiFi board. To help locate the pin, there is a German blog post that documents the pinout of the WiFi board: http://meinpb.blogspot.com.au/2013/08/wlc-240-wifi.html To reset the board back to factory defaults, pull pin 7 down for 15 seconds (short pin 7 and 9 with something conductive), then release. The pins are numbered the same as the table in the post, when looking down on the board from above. When it comes back online, it will have a new SSID along the lines of HAPPYCOW_00:13:35 Joining your home WiFi network To find out how to get the tank to join your own WiFi network, see part 3 !", "date": "2015-05-14"},
{"website": "Kogan", "title": "React Meetup - April 2015", "author": ["Choon Ken Ding"], "link": "https://devblog.kogan.com/blog/react-meetup-april-2015", "abstract": "Yesterday we hosted the second React Meetup in Melbourne, what a great turn out! It was great to see such a big community for a new technology. Here is a link to the talk Michael and I did:", "date": "2015-04-22"},
{"website": "Kogan", "title": "Hacking the WiFi Spy Tank", "author": ["Michael Cooper"], "link": "https://devblog.kogan.com/blog/hacking-the-wifi-spy-tank", "abstract": "Here at Kogan, we sell a Remote Control Spy Tank , aka “WiFi Spy Tank”, \"I-Spy Tank\", “EYE SPY” or “Instant Spy Tank”. It's a fun little remote control car that is supposed to be driven from your iPhone, iPad or Android device by joining its wifi network. That wasn't enough for me, I wanted to be able to drive it around my house while I was not home. The first step is to reverse engineer how the app talks to the tank, and that is what this post is about. Disclaimer: Hacking (or in this case, Telnetting) into the WiFi Spy Tank may void your warranty. Wireshark To discover how the iPad app works, I used Wireshark to see all the communications between the iPad and the Tank. Unfortunately, I couldnt get my laptop's in-built WiFi adaptor to go into monitor or promiscuous mode, so I used the TP-LINK WN722N . Video From the screenshot, you can see that there appears to be an MJPEG http stream on port 8196. To test this out, you can use VLC to open the MJPEG stream on http://10.10.1.1:9876/, and you should be ale to see the camera! Motors The motor control is a bit more tricky, as it's not a standard interface. After looking at the packets a bit more, I can see there is also a small amount of traffic on port 8150, which is probably the way it controls the motors. The data is a stream of small packets (2-4 bytes at a time). The first packet is 't1' in ASCII, then a 'KK' and '1020'. From this, we can assume that 't1' is the hello, 'KK' is a no-op, and '1020' is both motors off. Driving the tank around a bit yields these codes: Stop    = 1020\nForward = 1121\nBack    = 1222 From here you can see the basics of the protocol, which is sets of two numbers: \"{motor_number}{direction}\" . Left motor = 1\nRight motor = 2\nCamera tilt = 3\n\nForward = 1\nBack = 2\nStop = 0 To test this out, you can use the telnet command: $ telnet 10.10.1.1 8150 Send the command “t1” and press enter. Then tell it to drive forward with “1121” and press enter. It should move forward! $ telnet 10.10.1.1 8150\nTrying 10.10.1.1...\nConnected to 10.10.1.1.\nEscape character is '^]'.\nt1\n1121 Python app To put all this together, I wrote a python GTK application to control the tank. The code is available here: https://github.com/mic159/TankRemote", "date": "2015-02-05"},
{"website": "Kogan", "title": "Top 3 lessons from CssConfAu", "author": ["Choon Ken Ding"], "link": "https://devblog.kogan.com/blog/top-3-lessons-from-cssconfau", "abstract": "We recently got the opportunity to attend cssconf in Melbourne, followed by Decompress (a day of lightning talks and hacking). I found it to be a really rewarding experience. The material presented was really relevant to my work and I got to pick the brains of some of the speakers during break time :) Here's a list of my top 3 takeaways from the CSS Conf AU by category: CSS & Usability & Accessibility 4 1/2 of theming css - Depending on your needs. Everyone is responsible for \"UX\" - that includes devs! Every hour you spend making the web faster, more accessible and easier to use, turns into days of time saved by real people SVGs are what we should eat for breakfast every morning Pick your colors wisely, because they might increase/decrease your accessibility Web Page performance: Cram your initial view into the first 14kb Eliminate any non-critical resources blocking your critical path Be responsible with your responsive design Perceived performance > Actual performance Animations: Improves your user experience Animate exclusively on opacity and transforms Perceived speed > Actual speed Animations help infer context from revealed information Use animations instead of gifs! After following all the speakers on Twitter, I happened upon a couple of good resources unbeknownst to me, and I’ve added them to my Easter break reading/review/code list of resources: Front End Guidelines by Benjamin de Cock https://github.com/bendc/frontend-guidelines CSS Guidelines by Harry Roberts http://cssguidelin.es/ Stray articles I’ve missed from Filament Group http://www.filamentgroup.com/lab/ A guide to SVG Animations by Sara Soueidan (+ her other articles) https://css-tricks.com/guide-svg-animations-smil/ Like what we do? Join us then.", "date": "2015-04-02"},
{"website": "Kogan", "title": "Tips for writing unit tests for Django middleware", "author": ["Nam Ngo"], "link": "https://devblog.kogan.com/blog/writing-unit-tests-for-django-middleware", "abstract": "Django framework provides developers with great testing tools and it's dead easy to write tests for views using Django's test client. It has extensive documentation on how to use django.test.Client to write automated tests. However, we often want to write tests for components that we have no control over when using django.test.Client . An example of that is Django Middleware which is used to add business logic either before or after view processing. django.test.Client has no public API for developers to access the internal request object. Here is a simple example of a middleware class that creates a stash from data saved in the session. class Stash(object):\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n\nclass StashMiddleware(object):\n    \"\"\"\n    Reconstructs the stash object from session\n    and attach it to the request object\n    \"\"\"\n    def process_request(self, request):\n        stashed_data = request.session.get('stashed_data', None)\n        # Instatiate the stash from data in session\n        if stashed_data is None:\n            stash = Stash()\n        else:\n            stash = Stash(**stashed_data)\n        # Attach the stash to request object\n        setattr(request, 'stash', stash)\n        return None Let's analyze what needs to be tested: 1. Assert that if the stashed data exists in the session, it should be set as an attribute of the request 2. Assert that if the stashed data doesn't exist in the session, an empty stash is created and attached to the request object 3. Assert that all attributes of the stash can be accessed How about dependencies? What do we need in order to write this test? - StashMiddleware class (this can be easily imported) - request object as an argument in process_request(). This one is a bit harder to obtain, and since we are writing a unit test, let's just mock it. We are now ready to write the test from django.test import TestCase\nfrom mock import Mock\nfrom bugfreeapp.middleware import StashMiddleware, Stash\n\nclass StashMiddlewareTest(TestCase):\n\n    def setUp(self):\n        self.middleware = StashMiddleware()\n        self.request = Mock()\n        self.request.session = {} This sets up an instance of StashMiddleware and mocks a request. I'm using Michael Foord's mock library to assist me with this. Since we know session is a dictionary like object, we can mock it with an empty dictionary. def test_process_request_without_stash(self):\n        self.assertIsNone(self.middleware.process_request(self.request))\n        self.assertIsInstance(self.request.stash, Stash)\n\n    def test_process_request_with_stash(self):\n        data = {'foo': 'bar'}\n        self.request.session = {'stashed_data': data}\n        self.assertIsNone(self.middleware.process_request(self.request))\n        self.assertIsInstance(self.request.stash, Stash)\n        self.assertEqual(self.request.stash.foo, 'bar') The first test asserts that (without stashed data in the session): - process_request returns None - Stash object has been attached to request The second test asserts that: - process_request returns None - Dictionary containing data in session is unpacked and used to create a Stash object. - Stash attributes can be accessed In both cases, we assert for a return value of process_request . This might sound like a redandunt thing to test for but it actually helps us to identify regressions. Knowing that process_request returns None, we don't have to worry about this middleware skipping the subsequent middlewares. Tips Not all tests can be written with django.test.client.Client . Keep your dependencies for unit tests as light as possible, use mocks. Write unit tests that run fast. Don't test ORM or network calls, try using mock.patch instead Revisit your code if you have a hard time trying to set up dependencies, that normally indicates that the code is too coupled. Like the sound of how we work? Check out our Careers Page !", "date": "2015-01-24"},
{"website": "Kogan", "title": "Continuously Improving our Process - Retrospectives", "author": ["Claire Pitchford"], "link": "https://devblog.kogan.com/blog/retrospectives", "abstract": "Like many agile based teams we regularly run retrospectives to gauge as a team how we are going and think of what and how to improve. We have one every two weeks, they are time boxed to one hour and are held standing up (like we do for nearly all of our team meetings). We have the typical ‘happy’ column, a ‘sad’ column and a ‘puzzling’ column. Everyone brainstorms on post-its, we group it together and vote on what we would like to discuss. The team is prompted by being next to our wall and running through some of the achievements/big events in the past two weeks. I ask probing questions such as: What has slowed down your progress? What has enabled you? From when you pick up a story to when you deploy it to production, what obstacles do you have? What areas have been inefficient and where is there unnecessary work or rework? There is one key part that distinguishes a productive retrospective from a time waster - the actions and improvements that are an output of the retro. One thing that is tempting is to discuss the particular details of an issue that has just occurred - which can often turn into a bit of a whinge and sometimes even a blame game (especially if the people involved are not present at the retro). Then a tactical action is thought of to address the latest symptom and the team moves on to the next highest voted item. It is far more valuable for the team to discuss the patterns and root cause of the issue. To do this, the team should discuss what process was followed and give other examples of that process in play. This is to remove the emotion and raise it up to a more high level discussion about repeatable systemic issues. Once we understand the root cause we explore what is within our control to improve. That way, when we begin to think of actions to improve the situation we are thinking of changes/tweaks to our process rather than a quick fix or bandaid. Then, once we have an action to improve our process, we run through a few hypothetical situations to ensure we have a shared understanding of what our new improvement looks like. We’ll often look at a few tasks in our backlog to give us some examples and we run through how things will play out with our new improvement. The next retrospective the first thing we discuss are our actions from the last retro: Has the action been implemented? if not, why not? Is the issue still occurring? if yes, why? Did the action improve the issue? if not, why not? By beginning our retrospective with the previously agreed actions, it reinforces to the team the purpose of our retrospectives, which is, to share examples of our teams anchors and engines, to discuss their root cause and to action improvements to our process.", "date": "2015-01-27"},
{"website": "Kogan", "title": "Testing auto_now DateTime Fields in Django", "author": ["Dylan Leigh"], "link": "https://devblog.kogan.com/blog/testing-auto-now-datetime-fields-in-django", "abstract": "Django's auto_now_add and auto_now field arguments provide a convenient way to create a field which tracks when an object was created and last modified. For example: class BlogPost(models.Model):\n      title   = models.CharField()\n      author  = models.ForeignKey(\"author\")\n      body    = models.TextField()\n      created = models.DateTimeField(auto_now_add=True)\n      edited  = models.DateTimeField(auto_now=True)\n      ... Unfortunately, they can make writing unit tests which depend on these creation or modification times difficult, as there is no simple way to set these fields to a specific time for testing. The problem Although auto_now fields can be be changed in code, as they will update themselves afterwards with the present date and time, they can effectively never be set to another time for testing. For example, if your Django-powered blog is set to prevent commenting on posts a month after it was last edited, you may wish to create a post object to test the block. The following example will not work: def test_no_comment(self):\n      blog_post = BlogPostFactory()\n\n      blog_post.edited = datetime.now() - timedelta(days=60)\n      # Django will replace this change with now()\n\n      self.assertFalse(blog_post.can_comment()) Even changes to an auto_now field in a factory or using the update() function won't last; Django will still overwrite the change with the current time. The easiest way to fix this for testing? Fake the current time. The solution: Mock Time The auto_now field uses django.utils.timezone.now to obtain the current time. We can mock.patch() this function to return a false time when the factory creates the object for testing: import mock\n   ...\n   def test_no_comment(self):\n\n      # make \"now\" 2 months ago\n      testtime = datetime.now() - timedelta(days=60)\n\n      with mock.patch('django.utils.timezone.now') as mock_now:\n         mock_now.return_value = testtime\n\n         blog_post = BlogPostFactory()\n\n      # out of the with statement - now is now the real now\n      self.assertFalse(blog_post.can_comment()) Once you need to return to the present, get out of the with statement and then you can test the long-ago-updated object in the present time. Other Solutions An alternative solution is to use a fixture instead; however fixtures should generally be avoided as they have to be manually updated as your models change and can lead to tests incorrectly passing or failing - see this blog post for more details. Another alternative is to create your own version of save() for the object which can be overridden directly. However this requires more complex code than using mock.patch() - and all that extra code will end up in production, not in the test as in the example above.", "date": "2015-02-24"},
{"website": "Kogan", "title": "Going too deep with Django Sessions", "author": ["Michael Cooper"], "link": "https://devblog.kogan.com/blog/duplicate-emails", "abstract": "The other day I was battling with some weird behaviour where a key in a session was updated, but sometimes it would revert after a while. The key in question was a flag to say that a customer had been sent an email about abandoning their cart, and when the key reverted they ended up getting duplicate emails. To achieve this, we have an offline Celery task that looks over all sessions in the DB and checks a flag on the cart to know if the email had already been sent. from django.contrib.sessions.backends.db import SessionStore\n\ndef update_flag(session, cart):\n    cart.email_sent = True\n    session_store = SessionStore(session_key=session.pk)\n    session_store['cart'] = cart.serialise()\n    session_store.save()\n\ndef find_sessions_for_email():\n    for session in SessionStore.objects.all().iterator():\n        cart = Cart.from_session(session)\n        if check_time(session) and not cart.email_sent:\n            yield session, cart I made a test session, forced the email send, and checked the session in the DB and the flag was correctly set. I then searched the entire codebase for references to this flag, and found absolutely nothing else touched this flag except this code. When I dug into the cases where duplicate emails were sent, I noticed that all of them came back to the site after the first email and started browsing again. But why? Why would browsing the site cause the flag to change state? And why wasn't everyone effected? A big clue is that we are using Django's django.contrib.sessions.backends.cached_db session engine in production. The problem was we were directly importing django.contrib.sessions.backends.db instead of the backend we had in the settings! When we used db instead of cached_db it was updating the flag in the DB, but not in the Redis cache. When the user browsed the site again during the cache's TTL, it would have potentially re-saved the cached version to the database, clobbering the version in the database.", "date": "2015-12-23"},
{"website": "Kogan", "title": "Webpack Your Things", "author": ["Simon Knox"], "link": "https://devblog.kogan.com/blog/webpack-your-things", "abstract": "We very recently finished migrating our front-end build process to Webpack. As with any reasonably sized codebase, it's always a little more complex than the 3 line examples in how-to guides. This post will list some of the higher-level things I learned during this undertaking, the next one in the Webpack Series will detail some specific quirks and solutions. Resources I was largely able to do this by leveraging the hard work of some clever heroes. This very conveniently timed blog post covered a lot of what was needed. JLongster also has some very good tips, helpful for more than just backend apps. Regarding documentation, the widely-cited Webpack How-to gives a pretty concise overview of most things you will need. And of course, the docs have a lot of information. Sometimes too much. But usually most things you need are listed there. Occasionally something isn't, which brings me to lesson one. Lesson 1: cd node_modules One of the biggest things I learned in this undertaking isn't limited just to Webpack, and helped fix a few other things. Previously I had treated the node_modules folder as a black box - just npm install and be on my way. This is fine for everyday usage, but when you hit barriers or bugs sometimes you need to do some digging. Rather than throwing random inputs at a black box to measure the effect, you can just crack it open. A good example of this is the CommonsChunkPlugin, which is documented thusly:  If omitted and options.async or options.children is set all chunks are used, elsewise options.filename is used as chunk name  — chunk.name definition I found this sentence somewhat confusing, but easy to clarify by reading the code that checks this. If/else and some variable assignments and straightforward things to follow. And the nature of Webpack modules means they are generally quite small, if all else fails just console.log everything. Note this doesn't necessarily mean you must always open the box and understand the internal implementation of libs you are using. But it is reassuring to know that you can. Lesson 2: Use Tables For visualising data, never for layout. The quite excellent webpack analyse tool provides a tonne of useful data to improve your module situation. The crazy tree-view animations look awesome, and are animated and zoomable. But they can quickly spiral out of control into a meaningless ball of branches. There are fortunately table views for all these pages as well. While repeatedly processing a bunch of lists to try minimising file sizes isn't the most romantic task, you can sort and group a lot more easily. Conversely, the Chunks tree view stays parseable for a longer time (as you will have fewer chunks than modules). It can give a quick overview if any of your chunks are ballooning out of control, and the accompanying table used for more automated analysis. Lesson 3: Measuring Victory As with any code change, the best measure of success is not breaking anything. In this case our ideal outcome was the change was completely invisible to end users, assuming everything deploys and the site still works. Beyond that it was meant to simplify frontend development for all of our devs, again a success. We have simple build & watch tasks without global Node package requirements (except npm). We were able to deploy our first React component by adding a single line to process JSX. So we don't (yet) have a quantifiable metric for the success of this adventure into the world of Webpack. But as someone who formerly complained about writing build tasks, it has been fun. In the next post I will drill down into a few specific issues encountered and how they were fixed, and some other useful features and tricks. If you have any advice or thoughts, we'd love to work with you - careers .", "date": "2015-05-20"},
{"website": "Kogan", "title": "Catches when Expecting Exceptions in Django Unit Tests", "author": ["Dylan Leigh"], "link": "https://devblog.kogan.com/blog/catches-when-expecting-exceptions-in-django-unit-tests", "abstract": "To cover all bases when writing a suite of unit tests, you need to test for the exceptional cases. However, handling exceptions can break the usual flow of the test case and confuse Django. Example scenario: unique_together For example, we have an ecommerce site with many products serving multiple countries , which may have different national languages . Our products may have a description written in different languages, but only one description per (product, language) pair. We can set up a unique_together constraint to enforce that unique pairing: class Description(models.Model):\n    product = models.ForeignKey(\"Product\")\n    language = models.ForeignKey(\"countries.Language\")\n\n    class Meta:\n        unique_together = (\"product\", \"language\")\n\n    subtitle = models.CharField(...)\n    body = models.CharField(...)\n    ... Developer chooses AssertRaises() If the unique_together rule is violated, Django will raise an IntegrityError. A unit test can verify that this occurs using assertRaises () on a lambda function: def test_unique_product_description(self):\n   desc1 = DescriptionFactory(self.prod1, self.lang1)\n   self.assertRaises(IntegrityError, lambda:\n      desc2 = DescriptionFactory(self.prod1, self.lang1) The assertion passes, but the test will fail with a new exception. A wild TransactionManagementError appears! Raising the exception when creating a new object will break the current database transaction, causing further queries to be invalid. The next code that accesses the DB - probably the test teardown - will cause a TransactionManagementError to be thrown: Traceback (most recent call last):\nFile \".../test_....py\", line 29, in tearDown\n   ...\nFile ...\n   ...\nFile \".../django/db/backends/__init.py, line 386, in validate_no_broken_transaction\nAn error occurred in the current transaction.\nTransactionManagementError: An error occurred in the current transaction.\nYou can't execute queries until the end of the 'atomic' block. Developer used transaction.atomic. It's super effective! Wrapping the test (or just the assertion) in its own transaction will prevent the TransactionManagementError from occurring, as only the inner transaction will be affected by the IntegrityError : def test_unique_product_description(self):\n   desc1 = DescriptionFactory(self.prod1, self.lang1)\n   with transaction.atomic():\n       self.assertRaises(IntegrityError, lambda:\n          desc2 = DescriptionFactory(self.prod1, self.lang1) You don't have to catch 'em all: Another solution Another way to fix this issue is to subclass your test from TransactionTestCase instead of the usual TestCase . Despite the name, TransactionTestCase doesn't use DB transactions to reset between tests; instead it truncates the tables. This may make the test slower for some cases, but will be more convenient if you are dealing with many IntegrityErrors in the one test. See the Django Documentation for more details on the difference between the two classes.", "date": "2015-08-26"},
{"website": "Kogan", "title": "eClaire - printing our trello cards for our physical wall", "author": ["Claire Pitchford"], "link": "https://devblog.kogan.com/blog/automated-trello-card-printing", "abstract": "View Code on GitHub We ♥ our Physical Wall There is nothing that creates visibility, collaboration and flexibility of process in a team like a physical wall displaying the process and the work. But... they are not without their limitations. They lack the detail and the reporting aspect that you really need an online tool to fulfill and that's why... We ♥ Trello Trello is the closest app to a physical wall that we have ever come across. But... in all it's virtue it does lack the transparency and in your face impact that a physical wall gives. So we continue to hold our face to face meetings at our physical wall. Why eClaire was born Therefore, both a physical wall and trello are 'must haves' for our team however having both is not without it's own pain. We have to keep the two in sync and for months we were typing a card in trello and writing a physical card for the wall. When you're creating and delivering cards as quickly as our team does - this overhead starts to wear thin. We ♥ hack days So one hack day we decided to build eClaire - a trello label printer. The requirement was simple; I want to create a card in trello and it create a card for the physical wall too. We built a solution that has saved us countless amounts of hours while raising the bar of how aesthetically pleasing our physical wall looks. You create a card in trello, label it how you'd like and then add the label \"PrintMe\". A sticky label is then printed out from the eClaire printer with the title and the relevant labels tagged at the bottom. Oh, and did I mention it is printed out in my own hand writing? As a nice to have you can upload your own font to not lose the feel that handwriting gives you on your physical wall. We have our own Kogan.com index cards, so we stick it on the appropriate coloured card and tack it to the wall. Another added bonus is that the card contains a QR Code which links to the Trello URL for the card. Now we can scan a card with our phone, open the card in the Trello app and get context on the card whilst at the wall -- quick & easy!! This means we've been able to capitalise on all the value of a physical wall without the inefficiencies! -- View Code on Github", "date": "2015-12-23"},
{"website": "Kogan", "title": "Anatomy 101 - Does Django Scale?", "author": ["Goran Stefkovski"], "link": "https://devblog.kogan.com/blog/anatomy-101-does-django-scale", "abstract": "How Django is used by Australia’s largest online retailer. I’m often asked about the choice of Django (and Python) for the base technology stack of kogan.com.  People are often surprised that Australia’s largest online retailer is not built in Java, .NET (your typical ‘enterprise-y’ stack) or using an out-of-box enterprise commerce product.  I thought it would be good to give people context behind the technology that we use at kogan.com, how it came into being and where it is going. First there was one About 6 years ago when Kogan was still a very young business I was co-developing the first Django powered version of kogan.com to replace the initial PHP implementation.  To put this period into perspective, AWS and Rackspace weren’t de facto things back then - they too were in the very early days of their cloud services. So why Django?  I first used Django and Python in a final year software engineering project at University and was impressed with the URL routing, ORM, documentation, testing framework and of course the admin interface which comes ‘batteries included’!  These elements made building dynamic websites much more pleasurable than using PHP or ASP (with which I had built sites in previously).  Moreover, there were also many open-source Django apps that could be leveraged to handle functionality such as a blog or the implementation of a particular payment engine. In about a month of work, we had put together most of the Kogan website.  We set a single instance (app) up on a shared cloud hosting environment with Apache sitting in front of Django. Memcache was used to cache the views and MySQL as the database.  Being less experienced at the time - we thought that this set up would do the business well for quite some time! The Slashdot Effect Just as we were nearing completion of the website, a current affairs TV show caught wind of the disruption Kogan was causing in the TV industry and filmed a segment on the business.  Ruslan called me and let me know that segment was going on air the next day.  The site we were building was much better than the older site - so we opted to fast track it into production — overnight, I had managed to tie down the ‘last 10%’ and test the site in production the next day.  A few hours from the air time — the new site had received its first order! When the current affairs show aired and announced the staggering low prices of the Kogan TVs to Australia - we were fortunate enough to experience the phenomenon that was known as the ‘Slashdot effect’.  Website traffic skyrocketed and the site was being crushed.  We were quickly working with the hosting provider to recover availability on this single machine.  In a very short period of time, we were able to add in an open-source Django app that wrote the server HTML responses to file.  We set up a list of ‘safe’ urls (like the homepage and list views) to cache in this way and modified the Apache config to try for those files first before hitting the dynamic urls of the site.   This meant that despite not having the smoothest experience across the entire site, most of the visitors to the site were now able to discover more about the brand and offering.  The setup, architecture and community around Django meant we were able to iterate and deliver in extremely short turn-around. Scaling out of the box With the rapid growth of the business, it quickly became apparent that a single shared hosting instance would not cut it!   Scaling is typically a difficult problem and has the potential for complicated re-writes.  Fortunately Django was designed in a way that made itis possible to horizontally scale the application servers with little effort.  The ‘state’ of our Django web app was only relevant between the HTTP request & response cycle - all persistent data (cart, sessions, orders, etc) were stored in the database.  This meant that web requests could be safely distributed to many app instances, and the load distributed by DNS round robin.  We acquired more shared hosting accounts on various web servers with our provider and configured the DNS so that we could add or remove app servers as needed. This solution held fast for quite some time with very little maintenance effort, and we even got in control of the ‘digg effect’ spikes that would occasionally hit us from TV exposure.  However, the business and the Internet was growing so quickly that we needed new features, complex integrations with external systems and new visual styles and UX and so it came time to redesign and rebuild to what is the current kogan.com platform. Coming up in Part 2, we’ll go over the current kogan.com architecture and where we see it heading.", "date": "2014-12-16"},
{"website": "Kogan", "title": "A hidden gem in Django 1.7: ManifestStaticFilesStorage", "author": ["Nam Ngo"], "link": "https://devblog.kogan.com/blog/a-hidden-gem-in-django-1-7-manifeststaticfilesstorage", "abstract": "The biggest change in Django 1.7 was the built-in schema migration support which everyone is aware of, however 1.7 also shipped with lots of other great additions, ManifestStaticFilesStorage - the new static files storage backend was one of them. Static file caching is everywhere Before explaining what ManifestStaticFilesStorage is and how it works, this is the overview of why we need it at Kogan.com: In order to deliver the content to our customers as fast as possible, we cache the downloaded static files by using max-age request headers. This allows our customers to download the content once and the subsequent requests to static files will be served from a cache. As shown on the diagram, if we were to use normal static file names like base.css , the content of the file would be cached in the CDN as well as on the browser and we would have a hard time trying to invalidate these caches. We cache-bust the content by appending a md5 hash of the content of the file to the file name. When we deploy a new base.css , {% static %} template tag will turn base.css into base.d1833e.css and the browser will then request a new file. {% static %} template tag is able to translate base.css into base.d1833e.css thanks to static files storage backend. This setting is named STATICFILES_STORAGE in Django. Before ManifestStaticFilesStorage Our Django app was previously configured to use CachedStaticFilesStorage which resulted in placing file mappings in the CACHES backend, for us it was Redis . Django adds these mappings during collectstatic when it gathers all statics and puts them in one place. This solution has coupled static assets deployment with code deployment resulting in a number of issues: Running collectstatic as part of code deployment --> slow deploys Extra load on Redis App servers were sometimes out of sync as we deploy them in batch. When we start the deployment, Redis would be updated with the new keys, the first batch of App servers would get the new code, but the other half still had old code. ManifestStaticFilesStorage to the rescue ManifestStaticFilesStorage has helped us to decouple the static compilation stage from deployments by allowing Django to read static file mappings from staticfiles.json on a filesystem. staticfiles.json is an artifact file produced by collectstatic with ManifestStaticFilesStorage as a backend. We can now include this staticfiles.json into our code package and deploy it to a single app server without affecting the others. Where is staticfiles.json located? By default staticfiles.json will reside in STATIC_ROOT which is the directory where all static files are collected in. We host all our static assets on an S3 bucket which means staticfiles.json by default would end up being synced to S3. However, we wanted it to live in the code directory so we could package it and ship it to each app server. As a result of this, ManifestStaticFilesStorage will look for staticfiles.json in STATIC_ROOT in order to read the mappings. We had to overwrite this behaviour, so we subclassed ManifestStaticFilesStorage : from django.contrib.staticfiles.storage import ManifestStaticFilesStorage\nfrom django.conf import settings\n\nclass KoganManifestStaticFilesStorage(ManifestStaticFilesStorage):\n\n    def read_manifest(self):\n        \"\"\"\n        Looks up staticfiles.json in Project directory\n        \"\"\"\n        manifest_location = os.path.abspath(\n            os.path.join(settings.PROJECT_ROOT, self.manifest_name)\n        )\n        try:\n            with open(manifest_location) as manifest:\n                return manifest.read().decode('utf-8')\n        except IOError:\n            return None With the above change, Django static template tag will now read the mappings from staticfiles.json that resides in project root directory. Thanks Django Thanks to Django 1.7, we've not only gotten a better schema migration system but also improved our deployment process. And not to mention ManifestStaticFilesStorage addition was only 40-50 lines of code (as of the day this blog post was published).", "date": "2015-09-28"},
{"website": "Kogan", "title": "ReactJS + Flux Meetup", "author": ["Sean Kozer"], "link": "https://devblog.kogan.com/blog/react-flux-meetup-demo", "abstract": "Yesterday we had the pleasure of hosting the first ReactJS Melbourne meetup and we're absolutely thrilled with the turn out! For those who missed out, here are the slides from the Kogan.com prototype demo we presented yesterday: We'd like to thank everyone who came, and we look forward to seeing what you create with these tools! View fullsize Here are some snaps from the event: Like the sound of how we work? Check out our Careers Page !", "date": "2015-03-04"},
{"website": "Kogan", "title": "Building a Sales Radar for our Dashboard", "author": ["Sean Kozer"], "link": "https://devblog.kogan.com/blog/building-a-sales-radar-for-our-dashboard", "abstract": "Every month, our dev team hosts a \"hack day\" to explore creative ideas and learn new skills. This month, we focussed on building a custom module for our dashboard. We've found our dashboard to be immensely valuable over the last 12 months and have spent a great deal of effort on adding to it and improving it. Currently, our dashboard provides insights into all the important site health and infrastructure metrics, but not a lot is dedicated to monitoring customer behaviour. My goal for this hack day was to build a display to visualise customer purchase behaviour across Australia. The idea was to overlay a heatmap of sales across a map, cycling through the last 24 hours of data. The visualisation was partly inspired by the Bureau of Meteorology's national weather radar loop . The new dashboard How we built it It's always a challenge to co-ordinate a team working on a single product.\nThere is always a risk of team members stepping on eachothers' toes, resulting in code rewrites and critical time lost. To combat this, I split the team up to work on separate components, which meant communication was now more important than ever. Each team was responsible for one component: Design and frontend dev. Sourcing live, aggregated sales data. Processing and feeding sales data to the frontend. Prior to commencing development, we sketched out several ideas until we had an agreed vision.\nThen the real work began. The final dashboard mockup To start with the data, we grouped the number of hourly sales by postcode, and mapped them to lat/long co-ordinates. For the map visualisation, we shortlisted two libraries: Google Maps LeafletJS LeafletJS was the standout because of its flexibility and plugin options. What did we learn? The insights from the visualisation didn't come as much of a surprise to our team. The culture at Kogan.com is very deep-rooted in data and analytics, so the visualisation came close to our initial hypothesis. Majority of purchases are clustered in the major cities. Regional sales, which account for a high portion of total sales, appeared very thin when overlayed on a map. I found that having the team work on separate components significantly reduced the need for rewrites.\nHowever, we cut it very close to the deadline, as we delivered functional prototypes throughout the day less frequently than desired. A major takeaway is this project motivated the team to think about new ways of exploring our data for future hack days. What's next Similar dashboard for page views Use Kibana to drill down further into the data", "date": "2016-01-11"},
{"website": "Kogan", "title": "Frontend Testing with PhantomJS CasperJS Mocha and Chai", "author": ["Choon Ken Ding"], "link": "https://devblog.kogan.com/blog/frontend-testing-with-phantomjs-casperjs-mocha-and-chai", "abstract": "Let’s face it. Front end testing, or in fact, any sort of testing that involves you (the developer/tester) manually going through each scenario can be a gruelling process. This post isn’t about the importance of front end testing, because it's 2015. I’m going to write about testing the UI and simulating user actions in the browser using PhantomJS, CasperJS, Mocha and Chai. Before I proceed any further, here is a brief introduction to what each framework/library does: Mocha Mocha is a feature-rich JavaScript test framework running on node.js and the browser, making asynchronous testing simple and fun. Mocha tests run serially, allowing for flexible and accurate reporting, while mapping uncaught exceptions to the correct test cases. Chai Chai is a BDD / TDD assertion library for node and the browser that can be delightfully paired with any javascript testing framework. PhantomJS PhantomJS is a headless WebKit scriptable with a JavaScript API. It has fast and native support for various web standards: DOM handling, CSS selector, JSON, Canvas, and SVG. CasperJS CasperJS is an open source navigation scripting and testing utility written in Javascript for the PhantomJS WebKit headless browser and SlimerJS (Gecko). It eases the process of defining a full navigation scenario and provides useful high-level functions, methods and syntactic sugar. CasperJS provides us with some really neat functions to work with. Particularly: casper.start() : Configures and starts Casper casper.waitFor() : Waits until a function returns true to process any next step. casper.waitUntilVisible() : Waits until an element matching the provided selector expression is visible in the remote DOM to process a next step. Uses waitFor(). I find this particularly useful especially when we are interacting with DOM elements that have animation. casper.capture() : Proxy method for PhantomJS’ WebPage#render. Adds a clipRect parameter for automatically setting page clipRect settings and reverts it back once done. There are many more useful functions here . CasperJS provides built in testing functionality as well, but we opted for the Mocha testing framework and Chai assertion library because we liked it better. Let’s Get Started! First get Mocha and Chai installed (if you opt to use Mocha and Chai instead of just using casper’s built in testing utility): npm install mocha npm install chai Then install phantomJS: - npm install -g phantomjs There are a couple of other ways to install phantomjs shown here . Get casperjs running: - npm install -g casperjs Finally get mocha-casperjs and casper-chai installed if you want to use Mocha and Chai with casperjs. npm install -g mocha-casperjs npm install -g casper-chai Note from the future: I recommend installing the npm packages locally and specifying the test commands within package.json instead. Alternatively, if you’re happy with just phantomjs and casperjs, using the Chrome extension tool here can help you get off to a good start. Resurrectio allows you to record a sequence of browser actions and produces the corresponding CasperJS script. Ahem. Let’s really get started now. Let’s write some code that does all of the following in sequence: Loads a page Waits for a selector to appear Clicks on an element and asserts that the redirected page has the correct url. If successful, capture a screenshot. describe('Home page', function() {\n \tbefore(function() {\n\t\tcasper.start('http://localhost:8000');\n\t});\n\tit('should have an element in DOM', function(){\n\t\tcasper.waitForSelector('#correctElement', function() {\n\t\t\t'#correctElement'.should.be.inDOM;\n\t\t});\n\t});\n\tit('should bring you to another page on click', function() {\n\t\tcasper.thenClick('#destroyEverything', function(){\n\t\t\tthis.echo('Clicked on Destory everything');\n\t\t});\n\t\tcasper.waitFor(function check() {\n\t\t\treturn this.evaluate(function() {\n\t\t\t\treturn /urlthatwewant/.test(document.location.pathname);\n\t\t\t});\n\t\t}, function then() {\n\t\t\t// Succeeded\n\t\t\tthis.echo('->Succeeded in loading the another page');\n\t\t\tthis.capture(anotherPage.png');\n\t\t}, function timeout() {\n\t\t\tthis.echo('Failed to load page').exit();\n\t\t});\n\t});\n}); Running the above is simple. mocha-casperjs <filename>.js Here are some things we learnt while setting up frontend tests for Kogan.com . Keeping it DRY Many of the functions can be reused. Fortunately, PhantomJS allows you to import/require modules using the CommonJS syntax. A very reusable module would be the login module (simplified version) (as shown below) module.exports = function(email, password) { \n  describe('Logging in', function() {\n    it('Filling in form fields and clicking should log the user in', function() {\n    \tcasper.then(function(){\n    \t\tthis.fillSelectors('form.form-email', {\n    \t\t\t'input#email' : email,\n    \t\t\t'input#password' : password\n    \t\t});\n\n  \t\t\tthis.echo('Filling in details');\n  \t\t});\n\n      casper.thenClick('#loginButton', function() {\n        this.capture('filledIn.png');\n        this.echo('->Clicked on Login');\n      });   \n\n  \t});\n\n  }); \n}; Config File Having a config file with all your valid/invalid data can also help with making things DRY-er. /**\n      config file\n      **/\n      module.exports = {\n      URL: {\n      dev: ‘http://localhost:8000’\n      }\n      }; Putting them all together, we could make the code a lot cleaner, and easier to understand. Pretty sweet huh? var config = require('./config');\nvar login = require('./functions/login);\nvar doSomething1 = require('./functions/doSomething1);\nvar doSomething2 = require('./functions/doSomething2);\n\n\ndescribe('Yet Another Page', function() {\n  before(function() {\n    casper.start(config.URL.dev);\n  });\n\n  login();\n\n  doSomething1();\n\n  doSomething2(1);\n\n}); Like the sound of how we work? Check out our Careers Page ! References: - PhantomJS - CasperJS - Mocha", "date": "2015-02-17"}
]